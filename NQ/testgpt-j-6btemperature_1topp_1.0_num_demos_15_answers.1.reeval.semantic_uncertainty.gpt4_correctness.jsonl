{"question": "what is the location at the end of minority report", "answer": ["dumptonwell"], "scores": [[0.0005296349991112947, 0.009419902227818966, 0.006070134229958057, 0.00032424271921627223]], "normalized_score": [0.001770202536135912], "tokens": [["\u0120d", "um", "pton", "well"]], "entropy": [[7.096961975097656, 5.3163533210754395, 4.1760759353637695, 4.697162628173828]], "avg_entropy": [5.321638464927673], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "an undisclosed location", "score_of_biggest_cluster": 0.001770202536135912, "score_of_first_answer": 0.001770202536135912, "max_score_of_first_answer": 0.009419902227818966, "min_score_of_first_answer": 0.00032424271921627223, "avg_score": 0.001770202536135912, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.003846617068141091, "std_of_first_answer": 0.003846617068141091, "avg_entropy_of_first_answer": 5.321638464927673, "lex_sim": 1.0}
{"question": "who sings the song rock you like a hurricane", "answer": ["Alicia Keys"], "scores": [[0.007050392217934132, 0.9071322083473206]], "normalized_score": [0.07997274398803711], "tokens": [["\u0120Alicia", "\u0120Keys"]], "entropy": [[6.311304092407227, 0.6961355209350586]], "avg_entropy": [3.5037198066711426], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "German rock band Scorpions", "score_of_biggest_cluster": 0.07997274398803711, "score_of_first_answer": 0.07997274398803711, "max_score_of_first_answer": 0.9071322083473206, "min_score_of_first_answer": 0.007050392217934132, "avg_score": 0.07997274398803711, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4500409080646932, "std_of_first_answer": 0.4500409080646932, "avg_entropy_of_first_answer": 3.5037198066711426, "lex_sim": 1.0}
{"question": "avatar the last airbender season 3 episode 12", "answer": ["Mankodra/Shaolin"], "scores": [[0.003793333424255252, 0.062004007399082184, 0.01793384738266468, 0.05618467926979065, 0.0007949610590003431, 0.00604937644675374, 0.16370026767253876, 0.7424329519271851]], "normalized_score": [0.02469942905008793], "tokens": [["\u0120M", "ank", "od", "ra", "/", "Sh", "a", "olin"]], "entropy": [[5.978438854217529, 5.560317039489746, 4.425714492797852, 4.441047668457031, 2.3726372718811035, 5.7510986328125, 4.999178409576416, 1.8880157470703125]], "avg_entropy": [4.427056014537811], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Western Air Temple", "score_of_biggest_cluster": 0.02469942905008793, "score_of_first_answer": 0.02469942905008793, "max_score_of_first_answer": 0.7424329519271851, "min_score_of_first_answer": 0.0007949610590003431, "avg_score": 0.02469942905008793, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2363275929720364, "std_of_first_answer": 0.2363275929720364, "avg_entropy_of_first_answer": 4.427056014537811, "lex_sim": 1.0}
{"question": "product-market fit means being in a good market with a product that can satisfy that market", "answer": ["market fit"], "scores": [[0.019866788759827614, 0.4095121920108795]], "normalized_score": [0.0901980772614479], "tokens": [["\u0120market", "\u0120fit"]], "entropy": [[6.260490417480469, 3.406301975250244]], "avg_entropy": [4.8333961963653564], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mark Andreessen", "score_of_biggest_cluster": 0.0901980772614479, "score_of_first_answer": 0.0901980772614479, "max_score_of_first_answer": 0.4095121920108795, "min_score_of_first_answer": 0.019866788759827614, "avg_score": 0.0901980772614479, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19482270162552595, "std_of_first_answer": 0.19482270162552595, "avg_entropy_of_first_answer": 4.8333961963653564, "lex_sim": 1.0}
{"question": "which animal on earth has the longest life span", "answer": ["blue whale"], "scores": [[0.012062960304319859, 0.861015796661377]], "normalized_score": [0.10191366821527481], "tokens": [["\u0120blue", "\u0120whale"]], "entropy": [[6.320643424987793, 0.8196486830711365]], "avg_entropy": [3.5701460540294647], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "tortoise", "score_of_biggest_cluster": 0.10191366821527481, "score_of_first_answer": 0.10191366821527481, "max_score_of_first_answer": 0.861015796661377, "min_score_of_first_answer": 0.012062960304319859, "avg_score": 0.10191366821527481, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42447641817852855, "std_of_first_answer": 0.42447641817852855, "avg_entropy_of_first_answer": 3.5701460540294647, "lex_sim": 1.0}
{"question": "who wrote hallelujah i just love her so", "answer": ["Leonard Cohen"], "scores": [[0.30073267221450806, 0.9711116552352905]], "normalized_score": [0.5404118895530701], "tokens": [["\u0120Leonard", "\u0120Cohen"]], "entropy": [[4.777875900268555, 0.3017195165157318]], "avg_entropy": [2.5397977083921432], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ray Charles", "score_of_biggest_cluster": 0.5404118895530701, "score_of_first_answer": 0.5404118895530701, "max_score_of_first_answer": 0.9711116552352905, "min_score_of_first_answer": 0.30073267221450806, "avg_score": 0.5404118895530701, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33518949151039124, "std_of_first_answer": 0.33518949151039124, "avg_entropy_of_first_answer": 2.5397977083921432, "lex_sim": 1.0}
{"question": "what is the most common cause of right ventricular heart failure", "answer": ["dilated cardiomyopathy"], "scores": [[0.0299412552267313, 0.8954070806503296, 0.9328070878982544, 0.9896535277366638, 0.9938801527023315, 0.9857274889945984]], "normalized_score": [0.5379922986030579], "tokens": [["\u0120dil", "ated", "\u0120card", "i", "omy", "opathy"]], "entropy": [[4.772833824157715, 0.3970664143562317, 0.47775256633758545, 0.07810347527265549, 0.04076255485415459, 0.11211146414279938]], "avg_entropy": [0.9797717165201902], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "left-sided heart failure", "score_of_biggest_cluster": 0.5379922986030579, "score_of_first_answer": 0.5379922986030579, "max_score_of_first_answer": 0.9938801527023315, "min_score_of_first_answer": 0.0299412552267313, "avg_score": 0.5379922986030579, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34824754130149793, "std_of_first_answer": 0.34824754130149793, "avg_entropy_of_first_answer": 0.9797717165201902, "lex_sim": 1.0}
{"question": "vapor pressure of water at 100c in torr", "answer": ["100 torr"], "scores": [[0.08877776563167572, 0.2812979221343994, 0.9300211071968079]], "normalized_score": [0.2853127419948578], "tokens": [["\u0120100", "\u0120tor", "r"]], "entropy": [[4.307068824768066, 3.302016258239746, 0.2819478511810303]], "avg_entropy": [2.6303443113962808], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "759.9625", "score_of_biggest_cluster": 0.2853127419948578, "score_of_first_answer": 0.2853127419948578, "max_score_of_first_answer": 0.9300211071968079, "min_score_of_first_answer": 0.08877776563167572, "avg_score": 0.2853127419948578, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3598759263389751, "std_of_first_answer": 0.3598759263389751, "avg_entropy_of_first_answer": 2.6303443113962808, "lex_sim": 1.0}
{"question": "when does middle-earth shadow of mordor come out", "answer": ["April 7, 3011"], "scores": [[0.05550349876284599, 0.007726259529590607, 0.14667530357837677, 0.005519713740795851, 0.04544464126229286]], "normalized_score": [0.02751750499010086], "tokens": [["\u0120April", "\u01207", ",", "\u0120301", "1"]], "entropy": [[3.990060329437256, 3.3081579208374023, 1.0507538318634033, 2.6534552574157715, 1.7706557512283325]], "avg_entropy": [2.554616618156433], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "September 2014", "score_of_biggest_cluster": 0.02751750499010086, "score_of_first_answer": 0.02751750499010086, "max_score_of_first_answer": 0.14667530357837677, "min_score_of_first_answer": 0.005519713740795851, "avg_score": 0.02751750499010086, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.051262247840409225, "std_of_first_answer": 0.051262247840409225, "avg_entropy_of_first_answer": 2.554616618156433, "lex_sim": 1.0}
{"question": "what is on a mcchicken sandwich from mcdonalds", "answer": ["chicken, lettuce and some other stuff"], "scores": [[0.15633291006088257, 0.08710741996765137, 0.42088744044303894, 0.10174127668142319, 0.0024795932695269585, 0.10649007558822632, 0.5482583045959473]], "normalized_score": [0.09760946780443192], "tokens": [["\u0120chicken", ",", "\u0120lettuce", "\u0120and", "\u0120some", "\u0120other", "\u0120stuff"]], "entropy": [[4.979928016662598, 1.788109302520752, 3.1815223693847656, 0.48098230361938477, 2.7814230918884277, 3.928948402404785, 2.6099295616149902]], "avg_entropy": [2.8215490068708147], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "a breaded chicken patty", "score_of_biggest_cluster": 0.09760946780443192, "score_of_first_answer": 0.09760946780443192, "max_score_of_first_answer": 0.5482583045959473, "min_score_of_first_answer": 0.0024795932695269585, "avg_score": 0.09760946780443192, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18595787360927768, "std_of_first_answer": 0.18595787360927768, "avg_entropy_of_first_answer": 2.8215490068708147, "lex_sim": 1.0}
{"question": "what was the initial effect of the transition from command to market economies in eastern europe", "answer": ["rise of entrepreneurship"], "scores": [[0.000940871424973011, 0.36199527978897095, 0.00663967477157712]], "normalized_score": [0.013125822879374027], "tokens": [["\u0120rise", "\u0120of", "\u0120entrepreneurship"]], "entropy": [[7.020301818847656, 1.794680118560791, 4.992836952209473]], "avg_entropy": [4.602606296539307], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Inequality of opportunity", "score_of_biggest_cluster": 0.013125822879374027, "score_of_first_answer": 0.013125822879374027, "max_score_of_first_answer": 0.36199527978897095, "min_score_of_first_answer": 0.000940871424973011, "avg_score": 0.013125822879374027, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.16887548607576658, "std_of_first_answer": 0.16887548607576658, "avg_entropy_of_first_answer": 4.602606296539307, "lex_sim": 1.0}
{"question": "what engine is in a holden v8 supercar", "answer": ["Holden 4.6-liter engine"], "scores": [[0.04054414480924606, 0.009413103573024273, 0.8014715909957886, 0.04816214740276337, 0.20966219902038574, 0.26187413930892944, 0.02600986696779728]], "normalized_score": [0.0800362080335617], "tokens": [["\u0120Holden", "\u01204", ".", "6", "-", "liter", "\u0120engine"]], "entropy": [[4.041769027709961, 2.5520896911621094, 1.287308931350708, 2.042862892150879, 2.745633125305176, 0.8140805959701538, 2.455447196960449]], "avg_entropy": [2.277027351515634], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "V8-engine", "score_of_biggest_cluster": 0.0800362080335617, "score_of_first_answer": 0.0800362080335617, "max_score_of_first_answer": 0.8014715909957886, "min_score_of_first_answer": 0.009413103573024273, "avg_score": 0.0800362080335617, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2620701939471384, "std_of_first_answer": 0.2620701939471384, "avg_entropy_of_first_answer": 2.277027351515634, "lex_sim": 1.0}
{"question": "who is the oldest wrestler that is still wrestling", "answer": ["86 years old"], "scores": [[0.013750521466135979, 0.3176230788230896, 0.8914130926132202]], "normalized_score": [0.15731500089168549], "tokens": [["\u012086", "\u0120years", "\u0120old"]], "entropy": [[6.24312686920166, 1.601169228553772, 0.6124614477157593]], "avg_entropy": [2.8189191818237305], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dory Funk Jr.", "score_of_biggest_cluster": 0.15731500089168549, "score_of_first_answer": 0.15731500089168549, "max_score_of_first_answer": 0.8914130926132202, "min_score_of_first_answer": 0.013750521466135979, "avg_score": 0.15731500089168549, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3639085812906014, "std_of_first_answer": 0.3639085812906014, "avg_entropy_of_first_answer": 2.8189191818237305, "lex_sim": 1.0}
{"question": "what is the meaning of lord shiva lingam", "answer": ["A phallus."], "scores": [[0.030637843534350395, 0.11893361806869507, 0.5070521831512451, 0.9938509464263916, 0.057635847479104996, 0.05916959047317505, 0.055647555738687515, 0.14512087404727936, 0.8791460990905762, 0.6324499249458313, 0.012765385210514069, 0.015328997746109962, 0.8890387415885925, 0.03800345957279205, 0.003894606838002801, 0.9726130962371826, 0.5256773233413696, 0.01553739421069622, 0.8995324373245239, 0.0634804219007492, 0.5049965381622314, 0.9322502613067627, 0.9900515675544739, 0.0048552751541137695, 0.02155194617807865, 0.9643425941467285, 0.0013132381718605757, 0.10948155075311661, 0.06054031476378441, 0.9309674501419067, 0.006484667770564556, 0.23492059111595154, 0.007907262071967125, 0.00289855501614511, 0.019167795777320862, 0.49805665016174316, 0.1032448261976242, 0.11087125539779663, 0.1277245283126831, 0.40832996368408203, 0.8802141547203064, 0.24387486279010773, 0.07077554613351822, 0.0311236921697855, 0.09637697041034698, 0.8579874038696289, 0.41846486926078796, 0.004054300021380186, 0.4965496063232422, 0.7067159414291382]], "normalized_score": [0.10845920443534851], "tokens": [["\u0120A", "\u0120ph", "all", "us", ".", "\u0120The", "\u0120male", "\u0120sex", "\u0120organ", ".", "\u0120Shiva", "\u0120Ling", "am", ",", "\u0120refers", "\u0120to", "\u0120the", "\u0120Ling", "am", "\u0120or", "\u0120ph", "all", "us", "\u0120on", "\u0120Lord", "\u0120Shiva", ";", "\u0120a", "\u0120representation", "\u0120of", "\u0120fertility", ".", "\u0120A", "\u0120major", "\u0120icon", "\u0120of", "\u0120Shiva", "\u0120worship", "\u0120in", "\u0120Hindu", "ism", ",", "\u0120and", "\u0120of", "\u0120Sha", "ivism", "\u0120in", "\u0120South", "\u0120India", "."]], "entropy": [[4.897238731384277, 4.65875244140625, 0.7211486101150513, 0.06305462121963501, 2.660494327545166, 2.320873260498047, 4.955726146697998, 2.8555805683135986, 0.6896366477012634, 1.3768157958984375, 1.2239725589752197, 2.2923951148986816, 0.5750730037689209, 2.3658664226531982, 4.424331188201904, 0.237638920545578, 2.1716794967651367, 4.554795265197754, 0.43413424491882324, 2.941645860671997, 2.9066085815429688, 0.2682097256183624, 0.06549452245235443, 2.854825019836426, 1.7454640865325928, 0.27123722434043884, 1.8467496633529663, 3.89943528175354, 4.673031330108643, 0.5039379000663757, 2.6112122535705566, 1.906188726425171, 1.7248446941375732, 5.086280822753906, 4.209900856018066, 1.8038136959075928, 1.932004451751709, 2.862513542175293, 2.0341219902038574, 2.2196710109710693, 0.7861844301223755, 1.440394639968872, 3.7427332401275635, 3.9922189712524414, 3.520902395248413, 0.5200794339179993, 1.9819560050964355, 1.4178621768951416, 1.505088210105896, 1.1986548900604248]], "avg_entropy": [2.239049459993839], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "reminds of the Omnipotent Lord", "score_of_biggest_cluster": 0.10845920443534851, "score_of_first_answer": 0.10845920443534851, "max_score_of_first_answer": 0.9938509464263916, "min_score_of_first_answer": 0.0013132381718605757, "avg_score": 0.10845920443534851, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36349395131805684, "std_of_first_answer": 0.36349395131805684, "avg_entropy_of_first_answer": 2.239049459993839, "lex_sim": 1.0}
{"question": "where did they live in sex and the city", "answer": ["New York, USA"], "scores": [[0.264843225479126, 0.9196829199790955, 0.03375539928674698, 0.18746256828308105]], "normalized_score": [0.19813954830169678], "tokens": [["\u0120New", "\u0120York", ",", "\u0120USA"]], "entropy": [[5.406070709228516, 0.3979876637458801, 1.0511492490768433, 4.155292510986328]], "avg_entropy": [2.752625033259392], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "New York City", "score_of_biggest_cluster": 0.19813954830169678, "score_of_first_answer": 0.19813954830169678, "max_score_of_first_answer": 0.9196829199790955, "min_score_of_first_answer": 0.03375539928674698, "avg_score": 0.19813954830169678, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.33845646730543677, "std_of_first_answer": 0.33845646730543677, "avg_entropy_of_first_answer": 2.752625033259392, "lex_sim": 1.0}
{"question": "last team to win fa cup outside top flight", "answer": ["2011"], "scores": [[0.0016478251200169325]], "normalized_score": [0.0016478251200169325], "tokens": [["\u01202011"]], "entropy": [[5.560590744018555]], "avg_entropy": [5.560590744018555], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Arsenal", "score_of_biggest_cluster": 0.0016478251200169325, "score_of_first_answer": 0.0016478251200169325, "max_score_of_first_answer": 0.0016478251200169325, "min_score_of_first_answer": 0.0016478251200169325, "avg_score": 0.0016478251200169325, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.560590744018555, "lex_sim": 1.0}
{"question": "who sang the superbowl national anthem in 2017", "answer": ["the national anthem by the us national football team"], "scores": [[0.038200560957193375, 0.14330163598060608, 0.35079628229141235, 0.00619416031986475, 0.07072149217128754, 0.003849980654194951, 0.17059391736984253, 0.2957983911037445, 0.4239918887615204]], "normalized_score": [0.07433395832777023], "tokens": [["\u0120the", "\u0120national", "\u0120anthem", "\u0120by", "\u0120the", "\u0120us", "\u0120national", "\u0120football", "\u0120team"]], "entropy": [[5.703368186950684, 5.894216060638428, 3.600494861602783, 3.0771849155426025, 6.221717834472656, 5.886521339416504, 2.6053967475891113, 2.7319133281707764, 1.983603596687317]], "avg_entropy": [4.189379652341207], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Luke Bryan", "score_of_biggest_cluster": 0.07433395832777023, "score_of_first_answer": 0.07433395832777023, "max_score_of_first_answer": 0.4239918887615204, "min_score_of_first_answer": 0.003849980654194951, "avg_score": 0.07433395832777023, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1473185247638919, "std_of_first_answer": 0.1473185247638919, "avg_entropy_of_first_answer": 4.189379652341207, "lex_sim": 1.0}
{"question": "when did the eagles win last super bowl", "answer": ["2014"], "scores": [[0.018830955028533936]], "normalized_score": [0.018830953165888786], "tokens": [["\u01202014"]], "entropy": [[4.767884254455566]], "avg_entropy": [4.767884254455566], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2017", "score_of_biggest_cluster": 0.018830953165888786, "score_of_first_answer": 0.018830953165888786, "max_score_of_first_answer": 0.018830955028533936, "min_score_of_first_answer": 0.018830955028533936, "avg_score": 0.018830953165888786, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.767884254455566, "lex_sim": 1.0}
{"question": "who was the bad guy in silence of the lambs", "answer": ["Liam Neeson"], "scores": [[0.0052361804991960526, 0.8711702227592468, 0.9879322648048401, 0.9936724901199341]], "normalized_score": [0.258685439825058], "tokens": [["\u0120Liam", "\u0120N", "ees", "on"]], "entropy": [[6.481252670288086, 0.8818522691726685, 0.08929223567247391, 0.05016382411122322]], "avg_entropy": [1.8756402498111129], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Buffalo Bill", "score_of_biggest_cluster": 0.258685439825058, "score_of_first_answer": 0.258685439825058, "max_score_of_first_answer": 0.9936724901199341, "min_score_of_first_answer": 0.0052361804991960526, "avg_score": 0.258685439825058, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4124024754403287, "std_of_first_answer": 0.4124024754403287, "avg_entropy_of_first_answer": 1.8756402498111129, "lex_sim": 1.0}
{"question": "when are the next commonwealth games going to be held", "answer": ["2012"], "scores": [[0.0015792546328157187]], "normalized_score": [0.0015792546328157187], "tokens": [["\u01202012"]], "entropy": [[4.4382219314575195]], "avg_entropy": [4.4382219314575195], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2022", "score_of_biggest_cluster": 0.0015792546328157187, "score_of_first_answer": 0.0015792546328157187, "max_score_of_first_answer": 0.0015792546328157187, "min_score_of_first_answer": 0.0015792546328157187, "avg_score": 0.0015792546328157187, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.4382219314575195, "lex_sim": 1.0}
{"question": "who plays the beast on the new beauty and the beast", "answer": ["James Earl Jones"], "scores": [[0.02366696298122406, 0.02340649999678135, 0.9928487539291382]], "normalized_score": [0.08193208277225494], "tokens": [["\u0120James", "\u0120Earl", "\u0120Jones"]], "entropy": [[3.9639225006103516, 1.3370095491409302, 0.07046498358249664]], "avg_entropy": [1.7904656777779262], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dan Stevens", "score_of_biggest_cluster": 0.08193208277225494, "score_of_first_answer": 0.08193208277225494, "max_score_of_first_answer": 0.9928487539291382, "min_score_of_first_answer": 0.02340649999678135, "avg_score": 0.08193208277225494, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.456938081807763, "std_of_first_answer": 0.456938081807763, "avg_entropy_of_first_answer": 1.7904656777779262, "lex_sim": 1.0}
{"question": "who said some are born great in twelfth night", "answer": ["Richard III"], "scores": [[0.013530203141272068, 0.4024403393268585]], "normalized_score": [0.07379093021154404], "tokens": [["\u0120Richard", "\u0120III"]], "entropy": [[5.517450332641602, 3.178570032119751]], "avg_entropy": [4.348010182380676], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Malvolio", "score_of_biggest_cluster": 0.07379093021154404, "score_of_first_answer": 0.07379093021154404, "max_score_of_first_answer": 0.4024403393268585, "min_score_of_first_answer": 0.013530203141272068, "avg_score": 0.07379093021154404, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19445506809279323, "std_of_first_answer": 0.19445506809279323, "avg_entropy_of_first_answer": 4.348010182380676, "lex_sim": 1.0}
{"question": "when is the next deadpool movie being released", "answer": ["April 14"], "scores": [[0.054994918406009674, 0.022360796108841896]], "normalized_score": [0.03506750613451004], "tokens": [["\u0120April", "\u012014"]], "entropy": [[4.256021976470947, 3.3041722774505615]], "avg_entropy": [3.7800971269607544], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "May 18, 2018", "score_of_biggest_cluster": 0.03506750613451004, "score_of_first_answer": 0.03506750613451004, "max_score_of_first_answer": 0.054994918406009674, "min_score_of_first_answer": 0.022360796108841896, "avg_score": 0.03506750613451004, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01631706114858389, "std_of_first_answer": 0.01631706114858389, "avg_entropy_of_first_answer": 3.7800971269607544, "lex_sim": 1.0}
{"question": "a legislative act passed by congress is an example of", "answer": ["law"], "scores": [[0.05080972611904144]], "normalized_score": [0.05080972984433174], "tokens": [["\u0120law"]], "entropy": [[5.99245548248291]], "avg_entropy": [5.99245548248291], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "statutory law", "score_of_biggest_cluster": 0.05080972984433174, "score_of_first_answer": 0.05080972984433174, "max_score_of_first_answer": 0.05080972611904144, "min_score_of_first_answer": 0.05080972611904144, "avg_score": 0.05080972984433174, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.99245548248291, "lex_sim": 1.0}
{"question": "seven union territories of india and their capital", "answer": ["Delhi"], "scores": [[0.35065290331840515]], "normalized_score": [0.35065293312072754], "tokens": [["\u0120Delhi"]], "entropy": [[3.89585018157959]], "avg_entropy": [3.89585018157959], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chandigarh \u2014 Chandigarh", "score_of_biggest_cluster": 0.35065293312072754, "score_of_first_answer": 0.35065293312072754, "max_score_of_first_answer": 0.35065290331840515, "min_score_of_first_answer": 0.35065290331840515, "avg_score": 0.35065293312072754, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.89585018157959, "lex_sim": 1.0}
{"question": "who sings i want to rock and roll", "answer": ["Ozzy Osbourne"], "scores": [[0.015245500952005386, 0.9186025857925415, 0.8549132347106934, 0.9747322201728821]], "normalized_score": [0.32867690920829773], "tokens": [["\u0120O", "zzy", "\u0120Os", "bourne"]], "entropy": [[5.168948650360107, 0.5063557624816895, 0.8333510756492615, 0.1835193634033203]], "avg_entropy": [1.6730437129735947], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kiss", "score_of_biggest_cluster": 0.32867690920829773, "score_of_first_answer": 0.32867690920829773, "max_score_of_first_answer": 0.9747322201728821, "min_score_of_first_answer": 0.015245500952005386, "avg_score": 0.32867690920829773, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3923705318941499, "std_of_first_answer": 0.3923705318941499, "avg_entropy_of_first_answer": 1.6730437129735947, "lex_sim": 1.0}
{"question": "when did they start adding zinc to pennies", "answer": ["1798"], "scores": [[0.029111994430422783, 0.035786766558885574]], "normalized_score": [0.03227730095386505], "tokens": [["\u012017", "98"]], "entropy": [[4.952917098999023, 3.089534282684326]], "avg_entropy": [4.021225690841675], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1864", "score_of_biggest_cluster": 0.03227730095386505, "score_of_first_answer": 0.03227730095386505, "max_score_of_first_answer": 0.035786766558885574, "min_score_of_first_answer": 0.029111994430422783, "avg_score": 0.03227730095386505, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0033373860642313957, "std_of_first_answer": 0.0033373860642313957, "avg_entropy_of_first_answer": 4.021225690841675, "lex_sim": 1.0}
{"question": "who plays the judge in drop dead diva", "answer": ["Jessica Biel"], "scores": [[0.005049499683082104, 0.4047180116176605, 0.9920787215232849]], "normalized_score": [0.1265656054019928], "tokens": [["\u0120Jessica", "\u0120B", "iel"]], "entropy": [[6.558938980102539, 2.5458853244781494, 0.07236762344837189]], "avg_entropy": [3.059063976009687], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lex Medlin", "score_of_biggest_cluster": 0.1265656054019928, "score_of_first_answer": 0.1265656054019928, "max_score_of_first_answer": 0.9920787215232849, "min_score_of_first_answer": 0.005049499683082104, "avg_score": 0.1265656054019928, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4053742040440763, "std_of_first_answer": 0.4053742040440763, "avg_entropy_of_first_answer": 3.059063976009687, "lex_sim": 1.0}
{"question": "where will the first round of march madness be played", "answer": ["Oklahoma City"], "scores": [[0.02446862868964672, 0.5644731521606445]], "normalized_score": [0.11752399057149887], "tokens": [["\u0120Oklahoma", "\u0120City"]], "entropy": [[5.6876935958862305, 1.5206751823425293]], "avg_entropy": [3.60418438911438], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wichita State University", "score_of_biggest_cluster": 0.11752399057149887, "score_of_first_answer": 0.11752399057149887, "max_score_of_first_answer": 0.5644731521606445, "min_score_of_first_answer": 0.02446862868964672, "avg_score": 0.11752399057149887, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2700022617354989, "std_of_first_answer": 0.2700022617354989, "avg_entropy_of_first_answer": 3.60418438911438, "lex_sim": 1.0}
{"question": "who sings will you still love me when i'm not young and beautiful", "answer": ["Celine Dion"], "scores": [[0.01304436381906271, 0.9441490173339844, 0.9869310259819031]], "normalized_score": [0.2299235314130783], "tokens": [["\u0120C", "eline", "\u0120Dion"]], "entropy": [[5.98180627822876, 0.3628804683685303, 0.10269954800605774]], "avg_entropy": [2.1491287648677826], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lana Del Rey", "score_of_biggest_cluster": 0.2299235314130783, "score_of_first_answer": 0.2299235314130783, "max_score_of_first_answer": 0.9869310259819031, "min_score_of_first_answer": 0.01304436381906271, "avg_score": 0.2299235314130783, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4493503219834623, "std_of_first_answer": 0.4493503219834623, "avg_entropy_of_first_answer": 2.1491287648677826, "lex_sim": 1.0}
{"question": "when is chinese new year and what year is it", "answer": ["the year of the Monkey"], "scores": [[0.04453130066394806, 0.34554800391197205, 0.9385546445846558, 0.9748955965042114, 0.012060025706887245]], "normalized_score": [0.17619268596172333], "tokens": [["\u0120the", "\u0120year", "\u0120of", "\u0120the", "\u0120Monkey"]], "entropy": [[3.708698272705078, 3.4633023738861084, 0.5350309610366821, 0.24241094291210175, 3.400479316711426]], "avg_entropy": [2.269984373450279], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the year of the Dog", "score_of_biggest_cluster": 0.17619268596172333, "score_of_first_answer": 0.17619268596172333, "max_score_of_first_answer": 0.9748955965042114, "min_score_of_first_answer": 0.012060025706887245, "avg_score": 0.17619268596172333, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4196301076691477, "std_of_first_answer": 0.4196301076691477, "avg_entropy_of_first_answer": 2.269984373450279, "lex_sim": 1.0}
{"question": "where are antibodies made and by what type of lymphocyte", "answer": ["antibody made by natural killer cells"], "scores": [[0.01846460811793804, 0.04483591765165329, 0.9150872230529785, 0.002155908150598407, 0.9769774675369263, 0.5914153456687927]], "normalized_score": [0.09903892874717712], "tokens": [["\u0120antibody", "\u0120made", "\u0120by", "\u0120natural", "\u0120killer", "\u0120cells"]], "entropy": [[3.3589487075805664, 3.7641966342926025, 0.53675377368927, 2.488283634185791, 0.1971648633480072, 1.373974323272705]], "avg_entropy": [1.953220322728157], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "B cells", "score_of_biggest_cluster": 0.09903892874717712, "score_of_first_answer": 0.09903892874717712, "max_score_of_first_answer": 0.9769774675369263, "min_score_of_first_answer": 0.002155908150598407, "avg_score": 0.09903892874717712, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42054555242739616, "std_of_first_answer": 0.42054555242739616, "avg_entropy_of_first_answer": 1.953220322728157, "lex_sim": 1.0}
{"question": "who played joey's agent estelle on friends", "answer": ["Estelle Getty"], "scores": [[0.20888128876686096, 0.9862716197967529, 0.13138537108898163]], "normalized_score": [0.3002486228942871], "tokens": [["\u0120Est", "elle", "\u0120Getty"]], "entropy": [[5.229239463806152, 0.09755172580480576, 3.741241931915283]], "avg_entropy": [3.0226777071754136], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "June Gable", "score_of_biggest_cluster": 0.3002486228942871, "score_of_first_answer": 0.3002486228942871, "max_score_of_first_answer": 0.9862716197967529, "min_score_of_first_answer": 0.13138537108898163, "avg_score": 0.3002486228942871, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3860299128111918, "std_of_first_answer": 0.3860299128111918, "avg_entropy_of_first_answer": 3.0226777071754136, "lex_sim": 1.0}
{"question": "who plays whitey bulger's girlfriend in black mass", "answer": ["Julia Sweeney"], "scores": [[0.01055128313601017, 0.012966425158083439]], "normalized_score": [0.011696681380271912], "tokens": [["\u0120Julia", "\u0120Sweeney"]], "entropy": [[6.160201072692871, 2.661712884902954]], "avg_entropy": [4.410956978797913], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "actress Dakota Johnson", "score_of_biggest_cluster": 0.011696681380271912, "score_of_first_answer": 0.011696681380271912, "max_score_of_first_answer": 0.012966425158083439, "min_score_of_first_answer": 0.01055128313601017, "avg_score": 0.011696681380271912, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0012075710110366344, "std_of_first_answer": 0.0012075710110366344, "avg_entropy_of_first_answer": 4.410956978797913, "lex_sim": 1.0}
{"question": "what is the main objective of criminal law", "answer": ["the punishment, not the crime"], "scores": [[0.027041984722018242, 0.14204491674900055, 0.0038821138441562653, 0.08873702585697174, 0.8428131937980652, 0.47294941544532776]], "normalized_score": [0.08988707512617111], "tokens": [["\u0120the", "\u0120punishment", ",", "\u0120not", "\u0120the", "\u0120crime"]], "entropy": [[4.41603946685791, 3.655374526977539, 1.3421722650527954, 4.603606224060059, 1.1618692874908447, 3.122011184692383]], "avg_entropy": [3.050178825855255], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Incapacitation", "score_of_biggest_cluster": 0.08988707512617111, "score_of_first_answer": 0.08988707512617111, "max_score_of_first_answer": 0.8428131937980652, "min_score_of_first_answer": 0.0038821138441562653, "avg_score": 0.08988707512617111, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30223556759421333, "std_of_first_answer": 0.30223556759421333, "avg_entropy_of_first_answer": 3.050178825855255, "lex_sim": 1.0}
{"question": "who does claire from mcleods daughters end up with", "answer": ["jackie"], "scores": [[0.005249050911515951, 0.06512380391359329]], "normalized_score": [0.018488861620426178], "tokens": [["\u0120jack", "ie"]], "entropy": [[6.831418037414551, 3.503446578979492]], "avg_entropy": [5.1674323081970215], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alex Ryan", "score_of_biggest_cluster": 0.018488861620426178, "score_of_first_answer": 0.018488861620426178, "max_score_of_first_answer": 0.06512380391359329, "min_score_of_first_answer": 0.005249050911515951, "avg_score": 0.018488861620426178, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02993737650103867, "std_of_first_answer": 0.02993737650103867, "avg_entropy_of_first_answer": 5.1674323081970215, "lex_sim": 1.0}
{"question": "who sponsored the phoenix open before waste management", "answer": ["Waste Management"], "scores": [[0.18309564888477325, 0.9622430801391602]], "normalized_score": [0.4197410047054291], "tokens": [["\u0120Waste", "\u0120Management"]], "entropy": [[6.1395344734191895, 0.30162960290908813]], "avg_entropy": [3.220582038164139], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Friedman Billings Ramsey", "score_of_biggest_cluster": 0.4197410047054291, "score_of_first_answer": 0.4197410047054291, "max_score_of_first_answer": 0.9622430801391602, "min_score_of_first_answer": 0.18309564888477325, "avg_score": 0.4197410047054291, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38957371562719345, "std_of_first_answer": 0.38957371562719345, "avg_entropy_of_first_answer": 3.220582038164139, "lex_sim": 1.0}
{"question": "who became king of erebor after thorin dies", "answer": ["Bilbo Baggins"], "scores": [[0.09433022141456604, 0.9970404505729675, 0.6567158699035645, 0.995879054069519]], "normalized_score": [0.49800869822502136], "tokens": [["\u0120Bil", "bo", "\u0120Bagg", "ins"]], "entropy": [[4.2724809646606445, 0.03341821953654289, 1.4961918592453003, 0.04539502412080765]], "avg_entropy": [1.4618715168908238], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "D\u00e1in", "score_of_biggest_cluster": 0.49800869822502136, "score_of_first_answer": 0.49800869822502136, "max_score_of_first_answer": 0.9970404505729675, "min_score_of_first_answer": 0.09433022141456604, "avg_score": 0.49800869822502136, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36868071447886536, "std_of_first_answer": 0.36868071447886536, "avg_entropy_of_first_answer": 1.4618715168908238, "lex_sim": 1.0}
{"question": "which church was given a letter in the book of revelation", "answer": ["Saint Peter\u2019s"], "scores": [[0.02068359963595867, 0.1709742546081543, 0.09052176028490067, 0.9988144636154175, 0.9993282556533813]], "normalized_score": [0.19994036853313446], "tokens": [["\u0120Saint", "\u0120Peter", "\u00e2\u0122", "\u013b", "s"]], "entropy": [[5.174270153045654, 3.6980035305023193, 1.4773777723312378, 0.010899991728365421, 0.008971168659627438]], "avg_entropy": [2.0739045232534408], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Philadelphia", "score_of_biggest_cluster": 0.19994036853313446, "score_of_first_answer": 0.19994036853313446, "max_score_of_first_answer": 0.9993282556533813, "min_score_of_first_answer": 0.02068359963595867, "avg_score": 0.19994036853313446, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4459075021097885, "std_of_first_answer": 0.4459075021097885, "avg_entropy_of_first_answer": 2.0739045232534408, "lex_sim": 1.0}
{"question": "when did marley die in a christmas carol", "answer": ["December 30, 1983"], "scores": [[0.531995952129364, 0.0004953799652867019, 0.18939413130283356, 0.02250596694648266]], "normalized_score": [0.03255574777722359], "tokens": [["\u0120December", "\u012030", ",", "\u01201983"]], "entropy": [[3.186089277267456, 0.5994707345962524, 0.8478997945785522, 2.3109638690948486]], "avg_entropy": [1.7361059188842773], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "December 24, 1836", "score_of_biggest_cluster": 0.03255574777722359, "score_of_first_answer": 0.03255574777722359, "max_score_of_first_answer": 0.531995952129364, "min_score_of_first_answer": 0.0004953799652867019, "avg_score": 0.03255574777722359, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2126422721445018, "std_of_first_answer": 0.2126422721445018, "avg_entropy_of_first_answer": 1.7361059188842773, "lex_sim": 1.0}
{"question": "when was the cat and mouse act introduced", "answer": ["1940"], "scores": [[0.009571989066898823]], "normalized_score": [0.009571988135576248], "tokens": [["\u01201940"]], "entropy": [[6.0729594230651855]], "avg_entropy": [6.0729594230651855], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1913", "score_of_biggest_cluster": 0.009571988135576248, "score_of_first_answer": 0.009571988135576248, "max_score_of_first_answer": 0.009571989066898823, "min_score_of_first_answer": 0.009571989066898823, "avg_score": 0.009571988135576248, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.0729594230651855, "lex_sim": 1.0}
{"question": "who won the mvp for the national league", "answer": ["Ben Shenton"], "scores": [[0.00695238821208477, 0.012631038203835487, 0.03964703157544136]], "normalized_score": [0.01515634823590517], "tokens": [["\u0120Ben", "\u0120She", "nton"]], "entropy": [[6.362466812133789, 4.633247375488281, 3.847276449203491]], "avg_entropy": [4.9476635456085205], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stanton, Giancarlo", "score_of_biggest_cluster": 0.01515634823590517, "score_of_first_answer": 0.01515634823590517, "max_score_of_first_answer": 0.03964703157544136, "min_score_of_first_answer": 0.00695238821208477, "avg_score": 0.01515634823590517, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.014263592613677274, "std_of_first_answer": 0.014263592613677274, "avg_entropy_of_first_answer": 4.9476635456085205, "lex_sim": 1.0}
{"question": "the plane of earth's orbit is called the", "answer": ["ecliptic"], "scores": [[0.36144599318504333, 0.9867587685585022, 0.9939671754837036, 0.9839417338371277]], "normalized_score": [0.768509030342102], "tokens": [["\u0120e", "cl", "ipt", "ic"]], "entropy": [[4.141388893127441, 0.132114440202713, 0.05712157115340233, 0.12815721333026886]], "avg_entropy": [1.1146955294534564], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ecliptic", "score_of_biggest_cluster": 0.768509030342102, "score_of_first_answer": 0.768509030342102, "max_score_of_first_answer": 0.9939671754837036, "min_score_of_first_answer": 0.36144599318504333, "avg_score": 0.768509030342102, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2714268394207463, "std_of_first_answer": 0.2714268394207463, "avg_entropy_of_first_answer": 1.1146955294534564, "lex_sim": 1.0}
{"question": "what is the center of heredity in a cell", "answer": ["telomere"], "scores": [[0.00492695951834321, 0.948432445526123, 0.7117913365364075]], "normalized_score": [0.1492723524570465], "tokens": [["\u0120tel", "ome", "re"]], "entropy": [[3.187528133392334, 0.3259880244731903, 0.6133504509925842]], "avg_entropy": [1.3756222029527028], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "nucleus", "score_of_biggest_cluster": 0.1492723524570465, "score_of_first_answer": 0.1492723524570465, "max_score_of_first_answer": 0.948432445526123, "min_score_of_first_answer": 0.00492695951834321, "avg_score": 0.1492723524570465, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4008129051049179, "std_of_first_answer": 0.4008129051049179, "avg_entropy_of_first_answer": 1.3756222029527028, "lex_sim": 1.0}
{"question": "what happened at the 1939 worlds fair in regards to television", "answer": ["television was introduced"], "scores": [[0.021433228626847267, 0.45659399032592773, 0.08545441180467606]], "normalized_score": [0.09421443194150925], "tokens": [["\u0120television", "\u0120was", "\u0120introduced"]], "entropy": [[5.535564422607422, 3.0618247985839844, 2.537713050842285]], "avg_entropy": [3.711700757344564], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Television demonstrations are held", "score_of_biggest_cluster": 0.09421443194150925, "score_of_first_answer": 0.09421443194150925, "max_score_of_first_answer": 0.45659399032592773, "min_score_of_first_answer": 0.021433228626847267, "avg_score": 0.09421443194150925, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1918356319476955, "std_of_first_answer": 0.1918356319476955, "avg_entropy_of_first_answer": 3.711700757344564, "lex_sim": 1.0}
{"question": "where did the term hot rod come from", "answer": ["American car, used car"], "scores": [[0.031756941229104996, 0.08364460617303848, 0.008539720438420773, 0.0056246924214065075, 0.0065118917264044285]], "normalized_score": [0.015272322110831738], "tokens": [["\u0120American", "\u0120car", ",", "\u0120used", "\u0120car"]], "entropy": [[6.3043670654296875, 4.345341682434082, 3.2697455883026123, 6.010871410369873, 2.8637890815734863]], "avg_entropy": [4.558822965621948], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "southern California", "score_of_biggest_cluster": 0.015272322110831738, "score_of_first_answer": 0.015272322110831738, "max_score_of_first_answer": 0.08364460617303848, "min_score_of_first_answer": 0.0056246924214065075, "avg_score": 0.015272322110831738, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02982769123340166, "std_of_first_answer": 0.02982769123340166, "avg_entropy_of_first_answer": 4.558822965621948, "lex_sim": 1.0}
{"question": "what stations in seattle are owned by sinclair", "answer": ["KTSM and KPLU in Seattle"], "scores": [[0.4332035779953003, 0.0020356825552880764, 0.623274028301239, 0.06517020612955093, 0.6881436109542847, 0.01227474957704544, 0.1045537143945694, 0.00046440097503364086, 0.5377901196479797]], "normalized_score": [0.05839927867054939], "tokens": [["\u0120K", "TS", "M", "\u0120and", "\u0120K", "PL", "U", "\u0120in", "\u0120Seattle"]], "entropy": [[3.4184927940368652, 1.967679738998413, 1.558577299118042, 2.1216931343078613, 1.767409324645996, 4.5855536460876465, 2.351390838623047, 0.45797455310821533, 2.8151137828826904]], "avg_entropy": [2.338209456867642], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "KOMO-TV", "score_of_biggest_cluster": 0.05839927867054939, "score_of_first_answer": 0.05839927867054939, "max_score_of_first_answer": 0.6881436109542847, "min_score_of_first_answer": 0.00046440097503364086, "avg_score": 0.05839927867054939, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27447860448909617, "std_of_first_answer": 0.27447860448909617, "avg_entropy_of_first_answer": 2.338209456867642, "lex_sim": 1.0}
{"question": "when did indoor toilets become common in america", "answer": ["early 1900's"], "scores": [[0.016339611262083054, 0.4891252815723419, 0.2996633052825928]], "normalized_score": [0.13379251956939697], "tokens": [["\u0120early", "\u01201900", "'s"]], "entropy": [[4.790313243865967, 2.011920690536499, 1.2201107740402222]], "avg_entropy": [2.6741149028142295], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "After World War I", "score_of_biggest_cluster": 0.13379251956939697, "score_of_first_answer": 0.13379251956939697, "max_score_of_first_answer": 0.4891252815723419, "min_score_of_first_answer": 0.016339611262083054, "avg_score": 0.13379251956939697, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19427770676241485, "std_of_first_answer": 0.19427770676241485, "avg_entropy_of_first_answer": 2.6741149028142295, "lex_sim": 1.0}
{"question": "to aru kagaku no railgun s episode 3", "answer": ["who are you going to the rock and roll hall of fame"], "scores": [[0.010560823604464531, 0.048893433064222336, 0.37282511591911316, 0.0039035321678966284, 0.8837451338768005, 0.008406056091189384, 0.0016370660159736872, 0.19947291910648346, 0.9746791124343872, 0.9326051473617554, 0.9935365319252014, 0.9759830832481384]], "normalized_score": [0.1040312796831131], "tokens": [["\u0120who", "\u0120are", "\u0120you", "\u0120going", "\u0120to", "\u0120the", "\u0120rock", "\u0120and", "\u0120roll", "\u0120hall", "\u0120of", "\u0120fame"]], "entropy": [[6.4844746589660645, 4.245687484741211, 2.489518642425537, 2.9031617641448975, 0.7543385028839111, 4.384587287902832, 6.322076797485352, 2.9729087352752686, 0.25472816824913025, 0.6075538396835327, 0.0665515810251236, 0.20421311259269714]], "avg_entropy": [2.6408167146146297], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Project Radio Noise", "score_of_biggest_cluster": 0.1040312796831131, "score_of_first_answer": 0.1040312796831131, "max_score_of_first_answer": 0.9935365319252014, "min_score_of_first_answer": 0.0016370660159736872, "avg_score": 0.1040312796831131, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4364790269907127, "std_of_first_answer": 0.4364790269907127, "avg_entropy_of_first_answer": 2.6408167146146297, "lex_sim": 1.0}
{"question": "how many players on the line of scrimmage in american football", "answer": ["32"], "scores": [[0.0019973788876086473]], "normalized_score": [0.0019973788876086473], "tokens": [["\u012032"]], "entropy": [[2.9383416175842285]], "avg_entropy": [2.9383416175842285], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "at least seven players", "score_of_biggest_cluster": 0.0019973788876086473, "score_of_first_answer": 0.0019973788876086473, "max_score_of_first_answer": 0.0019973788876086473, "min_score_of_first_answer": 0.0019973788876086473, "avg_score": 0.0019973788876086473, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.9383416175842285, "lex_sim": 1.0}
{"question": "who grows the most coffee in the world", "answer": ["Ethiopia"], "scores": [[0.3211803734302521]], "normalized_score": [0.3211803734302521], "tokens": [["\u0120Ethiopia"]], "entropy": [[4.085268020629883]], "avg_entropy": [4.085268020629883], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brazil", "score_of_biggest_cluster": 0.3211803734302521, "score_of_first_answer": 0.3211803734302521, "max_score_of_first_answer": 0.3211803734302521, "min_score_of_first_answer": 0.3211803734302521, "avg_score": 0.3211803734302521, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.085268020629883, "lex_sim": 1.0}
{"question": "who has the most super bowls in nfl history", "answer": ["Tom Brady"], "scores": [[0.13392779231071472, 0.9569880366325378]], "normalized_score": [0.358004629611969], "tokens": [["\u0120Tom", "\u0120Brady"]], "entropy": [[5.140202522277832, 0.39773547649383545]], "avg_entropy": [2.7689689993858337], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pittsburgh Steelers", "score_of_biggest_cluster": 0.358004629611969, "score_of_first_answer": 0.358004629611969, "max_score_of_first_answer": 0.9569880366325378, "min_score_of_first_answer": 0.13392779231071472, "avg_score": 0.358004629611969, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41153012216091156, "std_of_first_answer": 0.41153012216091156, "avg_entropy_of_first_answer": 2.7689689993858337, "lex_sim": 1.0}
{"question": "where did the last name edwards come from", "answer": ["British noble family whose name came from Edward the First."], "scores": [[0.003954207058995962, 0.01345379464328289, 0.575015127658844, 0.008733472786843777, 0.2636238932609558, 0.041615039110183716, 0.9273349642753601, 0.11676362156867981, 0.654769241809845, 0.0036192997358739376, 0.043348681181669235]], "normalized_score": [0.05941973254084587], "tokens": [["\u0120British", "\u0120noble", "\u0120family", "\u0120whose", "\u0120name", "\u0120came", "\u0120from", "\u0120Edward", "\u0120the", "\u0120First", "."]], "entropy": [[5.6855387687683105, 3.767190456390381, 1.9089581966400146, 1.889181137084961, 3.894444704055786, 2.6136560440063477, 0.43170544505119324, 3.8801112174987793, 1.7082629203796387, 3.5310211181640625, 1.966059684753418]], "avg_entropy": [2.8432845175266266], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "England", "score_of_biggest_cluster": 0.05941973254084587, "score_of_first_answer": 0.05941973254084587, "max_score_of_first_answer": 0.9273349642753601, "min_score_of_first_answer": 0.0036192997358739376, "avg_score": 0.05941973254084587, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3114276336631478, "std_of_first_answer": 0.3114276336631478, "avg_entropy_of_first_answer": 2.8432845175266266, "lex_sim": 1.0}
{"question": "los angeles stadium at hollywood park opening date", "answer": ["2007"], "scores": [[0.008558237925171852]], "normalized_score": [0.008558237925171852], "tokens": [["\u01202007"]], "entropy": [[5.109478950500488]], "avg_entropy": [5.109478950500488], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2020", "score_of_biggest_cluster": 0.008558237925171852, "score_of_first_answer": 0.008558237925171852, "max_score_of_first_answer": 0.008558237925171852, "min_score_of_first_answer": 0.008558237925171852, "avg_score": 0.008558237925171852, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.109478950500488, "lex_sim": 1.0}
{"question": "another name for the torso is the blank region of the body", "answer": ["Torso"], "scores": [[0.014844057150185108, 0.9912325739860535]], "normalized_score": [0.12130092084407806], "tokens": [["\u0120Tor", "so"]], "entropy": [[6.379046440124512, 0.09377888590097427]], "avg_entropy": [3.236412663012743], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "trunk", "score_of_biggest_cluster": 0.12130092084407806, "score_of_first_answer": 0.12130092084407806, "max_score_of_first_answer": 0.9912325739860535, "min_score_of_first_answer": 0.014844057150185108, "avg_score": 0.12130092084407806, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4881942584179342, "std_of_first_answer": 0.4881942584179342, "avg_entropy_of_first_answer": 3.236412663012743, "lex_sim": 1.0}
{"question": "how long was dark side of the moon number 1", "answer": ["7 years"], "scores": [[0.019819093868136406, 0.3546912372112274]], "normalized_score": [0.08384307473897934], "tokens": [["\u01207", "\u0120years"]], "entropy": [[5.334088325500488, 2.7511820793151855]], "avg_entropy": [4.042635202407837], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "a week", "score_of_biggest_cluster": 0.08384307473897934, "score_of_first_answer": 0.08384307473897934, "max_score_of_first_answer": 0.3546912372112274, "min_score_of_first_answer": 0.019819093868136406, "avg_score": 0.08384307473897934, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1674360716715455, "std_of_first_answer": 0.1674360716715455, "avg_entropy_of_first_answer": 4.042635202407837, "lex_sim": 1.0}
{"question": "where is the tennessee titans football stadium located", "answer": ["Nissan Stadium"], "scores": [[0.7260379791259766, 0.9512590765953064]], "normalized_score": [0.8310536742210388], "tokens": [["\u0120Nissan", "\u0120Stadium"]], "entropy": [[2.1239876747131348, 0.3454497158527374]], "avg_entropy": [1.234718695282936], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nashville, Tennessee", "score_of_biggest_cluster": 0.8310536742210388, "score_of_first_answer": 0.8310536742210388, "max_score_of_first_answer": 0.9512590765953064, "min_score_of_first_answer": 0.7260379791259766, "avg_score": 0.8310536742210388, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.11261054873466492, "std_of_first_answer": 0.11261054873466492, "avg_entropy_of_first_answer": 1.234718695282936, "lex_sim": 1.0}
{"question": "when does the heart develop and begin pumping blood", "answer": ["In the womb"], "scores": [[0.027576854452490807, 0.423399418592453, 0.1108689159154892]], "normalized_score": [0.10898537933826447], "tokens": [["\u0120In", "\u0120the", "\u0120womb"]], "entropy": [[5.0760393142700195, 2.258732318878174, 4.053830146789551]], "avg_entropy": [3.7962005933125815], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "by week 4 of development", "score_of_biggest_cluster": 0.10898537933826447, "score_of_first_answer": 0.10898537933826447, "max_score_of_first_answer": 0.423399418592453, "min_score_of_first_answer": 0.027576854452490807, "avg_score": 0.10898537933826447, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1703879184842654, "std_of_first_answer": 0.1703879184842654, "avg_entropy_of_first_answer": 3.7962005933125815, "lex_sim": 1.0}
{"question": "what's the average lifespan of a basset hound", "answer": ["6 years"], "scores": [[0.02481897920370102, 0.5012207627296448]], "normalized_score": [0.11153380572795868], "tokens": [["\u01206", "\u0120years"]], "entropy": [[3.336022138595581, 1.6332507133483887]], "avg_entropy": [2.484636425971985], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Median 10\u201312 years", "score_of_biggest_cluster": 0.11153380572795868, "score_of_first_answer": 0.11153380572795868, "max_score_of_first_answer": 0.5012207627296448, "min_score_of_first_answer": 0.02481897920370102, "avg_score": 0.11153380572795868, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23820089176297188, "std_of_first_answer": 0.23820089176297188, "avg_entropy_of_first_answer": 2.484636425971985, "lex_sim": 1.0}
{"question": "capital of georgia the former soviet republic 7 letters", "answer": ["Tbilisi"], "scores": [[0.18200518190860748, 0.7290369868278503, 0.9975360631942749]], "normalized_score": [0.5096288323402405], "tokens": [["\u0120T", "bil", "isi"]], "entropy": [[4.993581771850586, 0.8739053606987, 0.021290279924869537]], "avg_entropy": [1.9629258041580517], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tbilisi", "score_of_biggest_cluster": 0.5096288323402405, "score_of_first_answer": 0.5096288323402405, "max_score_of_first_answer": 0.9975360631942749, "min_score_of_first_answer": 0.18200518190860748, "avg_score": 0.5096288323402405, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3393500598257081, "std_of_first_answer": 0.3393500598257081, "avg_entropy_of_first_answer": 1.9629258041580517, "lex_sim": 1.0}
{"question": "who is the ceo of crate and barrel", "answer": ["Mike"], "scores": [[0.010935894213616848]], "normalized_score": [0.010935891419649124], "tokens": [["\u0120Mike"]], "entropy": [[6.421086311340332]], "avg_entropy": [6.421086311340332], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Neela Montgomery", "score_of_biggest_cluster": 0.010935891419649124, "score_of_first_answer": 0.010935891419649124, "max_score_of_first_answer": 0.010935894213616848, "min_score_of_first_answer": 0.010935894213616848, "avg_score": 0.010935891419649124, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.421086311340332, "lex_sim": 1.0}
{"question": "who plays lindsay denton in line of duty", "answer": ["Idris Elba"], "scores": [[0.003411122364923358, 0.8786019682884216, 0.9772511124610901, 0.9991292357444763]], "normalized_score": [0.23258371651172638], "tokens": [["\u0120Id", "ris", "\u0120El", "ba"]], "entropy": [[5.661255836486816, 0.7717697620391846, 0.20947399735450745, 0.010047142393887043]], "avg_entropy": [1.6631366845685989], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Keeley Clare Julia Hawes", "score_of_biggest_cluster": 0.23258371651172638, "score_of_first_answer": 0.23258371651172638, "max_score_of_first_answer": 0.9991292357444763, "min_score_of_first_answer": 0.003411122364923358, "avg_score": 0.23258371651172638, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41310675319278495, "std_of_first_answer": 0.41310675319278495, "avg_entropy_of_first_answer": 1.6631366845685989, "lex_sim": 1.0}
{"question": "what is the name given to the common currency to the european union", "answer": ["the Euro"], "scores": [[0.06559726595878601, 0.2253565639257431]], "normalized_score": [0.1215844452381134], "tokens": [["\u0120the", "\u0120Euro"]], "entropy": [[2.349768877029419, 1.264643907546997]], "avg_entropy": [1.807206392288208], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "euro", "score_of_biggest_cluster": 0.1215844452381134, "score_of_first_answer": 0.1215844452381134, "max_score_of_first_answer": 0.2253565639257431, "min_score_of_first_answer": 0.06559726595878601, "avg_score": 0.1215844452381134, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.07987964898347855, "std_of_first_answer": 0.07987964898347855, "avg_entropy_of_first_answer": 1.807206392288208, "lex_sim": 1.0}
{"question": "who wrote if she knew what she wants", "answer": ["Sylvia Plath"], "scores": [[0.02032427303493023, 0.8888736367225647, 0.9948486089706421]], "normalized_score": [0.2619413137435913], "tokens": [["\u0120Sylvia", "\u0120Pl", "ath"]], "entropy": [[6.504811763763428, 0.9252504110336304, 0.048941150307655334]], "avg_entropy": [2.493001108368238], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jules Shear", "score_of_biggest_cluster": 0.2619413137435913, "score_of_first_answer": 0.2619413137435913, "max_score_of_first_answer": 0.9948486089706421, "min_score_of_first_answer": 0.02032427303493023, "avg_score": 0.2619413137435913, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43656568466410073, "std_of_first_answer": 0.43656568466410073, "avg_entropy_of_first_answer": 2.493001108368238, "lex_sim": 1.0}
{"question": "what is the general structure of an url", "answer": ["u"], "scores": [[0.008967380970716476]], "normalized_score": [0.008967382833361626], "tokens": [["\u0120u"]], "entropy": [[4.465670108795166]], "avg_entropy": [4.465670108795166], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "a protocol", "score_of_biggest_cluster": 0.008967382833361626, "score_of_first_answer": 0.008967382833361626, "max_score_of_first_answer": 0.008967380970716476, "min_score_of_first_answer": 0.008967380970716476, "avg_score": 0.008967382833361626, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.465670108795166, "lex_sim": 1.0}
{"question": "where was the rise of the planet of the apes filmed", "answer": ["Argentina"], "scores": [[0.004277245607227087]], "normalized_score": [0.004277246538549662], "tokens": [["\u0120Argentina"]], "entropy": [[5.904241561889648]], "avg_entropy": [5.904241561889648], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Oahu, Hawaii", "score_of_biggest_cluster": 0.004277246538549662, "score_of_first_answer": 0.004277246538549662, "max_score_of_first_answer": 0.004277245607227087, "min_score_of_first_answer": 0.004277245607227087, "avg_score": 0.004277246538549662, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.904241561889648, "lex_sim": 1.0}
{"question": "who wrote ain't living long like this", "answer": ["Tom Petty and the heartbreakers"], "scores": [[0.00448355870321393, 0.2717331647872925, 0.0590677410364151, 0.8455208539962769, 0.2664036452770233, 0.9827850461006165]], "normalized_score": [0.1586255580186844], "tokens": [["\u0120Tom", "\u0120Petty", "\u0120and", "\u0120the", "\u0120heart", "breakers"]], "entropy": [[6.37781286239624, 2.7373828887939453, 0.5131827592849731, 0.8793320655822754, 1.20798921585083, 0.1790114790201187]], "avg_entropy": [1.9824518784880638], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rodney Crowell", "score_of_biggest_cluster": 0.1586255580186844, "score_of_first_answer": 0.1586255580186844, "max_score_of_first_answer": 0.9827850461006165, "min_score_of_first_answer": 0.00448355870321393, "avg_score": 0.1586255580186844, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3752656234251323, "std_of_first_answer": 0.3752656234251323, "avg_entropy_of_first_answer": 1.9824518784880638, "lex_sim": 1.0}
{"question": "how many house of representative do we have in nigeria", "answer": ["9"], "scores": [[0.014377741143107414]], "normalized_score": [0.014377741143107414], "tokens": [["\u01209"]], "entropy": [[5.000850677490234]], "avg_entropy": [5.000850677490234], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "360", "score_of_biggest_cluster": 0.014377741143107414, "score_of_first_answer": 0.014377741143107414, "max_score_of_first_answer": 0.014377741143107414, "min_score_of_first_answer": 0.014377741143107414, "avg_score": 0.014377741143107414, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.000850677490234, "lex_sim": 1.0}
{"question": "who was the french chef given credit for developing the classic kitchen\u200b brigade", "answer": ["Louis XV"], "scores": [[0.03035544604063034, 0.015989573672413826]], "normalized_score": [0.02203112468123436], "tokens": [["\u0120Louis", "\u0120XV"]], "entropy": [[4.300257682800293, 5.240931510925293]], "avg_entropy": [4.770594596862793], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Georges Auguste Escoffier", "score_of_biggest_cluster": 0.02203112468123436, "score_of_first_answer": 0.02203112468123436, "max_score_of_first_answer": 0.03035544604063034, "min_score_of_first_answer": 0.015989573672413826, "avg_score": 0.02203112468123436, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.007182936184108257, "std_of_first_answer": 0.007182936184108257, "avg_entropy_of_first_answer": 4.770594596862793, "lex_sim": 1.0}
{"question": "who won french open 2017 tennis tournament in men's double category", "answer": ["Stanislas Wawrinka"], "scores": [[0.08081527799367905, 0.25556740164756775, 0.9921782612800598, 0.9550543427467346, 0.9989166259765625, 0.9909942746162415, 0.9946938157081604]], "normalized_score": [0.568835437297821], "tokens": [["\u0120Stan", "isl", "as", "\u0120W", "aw", "r", "inka"]], "entropy": [[4.3299336433410645, 1.7730801105499268, 0.07321646064519882, 0.4113829433917999, 0.013179304078221321, 0.08487407863140106, 0.04515893757343292]], "avg_entropy": [0.9615464968872922], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ryan Harrison", "score_of_biggest_cluster": 0.568835437297821, "score_of_first_answer": 0.568835437297821, "max_score_of_first_answer": 0.9989166259765625, "min_score_of_first_answer": 0.08081527799367905, "avg_score": 0.568835437297821, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3727953760194429, "std_of_first_answer": 0.3727953760194429, "avg_entropy_of_first_answer": 0.9615464968872922, "lex_sim": 1.0}
{"question": "who plays the prosecutor on law and order svu", "answer": ["tom sizemore"], "scores": [[0.0004912182921543717, 0.06750896573066711, 0.6661625504493713, 0.9863844513893127, 0.9994490742683411]], "normalized_score": [0.11684351414442062], "tokens": [["\u0120tom", "\u0120s", "iz", "em", "ore"]], "entropy": [[5.730707168579102, 3.440434455871582, 1.8978124856948853, 0.14197972416877747, 0.006455000955611467]], "avg_entropy": [2.2434777670539914], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ra\u00fal Eduardo Esparza", "score_of_biggest_cluster": 0.11684351414442062, "score_of_first_answer": 0.11684351414442062, "max_score_of_first_answer": 0.9994490742683411, "min_score_of_first_answer": 0.0004912182921543717, "avg_score": 0.11684351414442062, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43370659491066427, "std_of_first_answer": 0.43370659491066427, "avg_entropy_of_first_answer": 2.2434777670539914, "lex_sim": 1.0}
{"question": "who was charlie writing to in perks of being a wallflower movie", "answer": ["Charlie Brown"], "scores": [[0.2526383101940155, 0.058763254433870316]], "normalized_score": [0.12184355407953262], "tokens": [["\u0120Charlie", "\u0120Brown"]], "entropy": [[5.605504989624023, 4.838753700256348]], "avg_entropy": [5.2221293449401855], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "an unknown recipient", "score_of_biggest_cluster": 0.12184355407953262, "score_of_first_answer": 0.12184355407953262, "max_score_of_first_answer": 0.2526383101940155, "min_score_of_first_answer": 0.058763254433870316, "avg_score": 0.12184355407953262, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0969375278800726, "std_of_first_answer": 0.0969375278800726, "avg_entropy_of_first_answer": 5.2221293449401855, "lex_sim": 1.0}
{"question": "who bowled the last over of the 2007 world twenty20 that india won", "answer": ["Anil Kumble"], "scores": [[0.019821930676698685, 0.5487116575241089, 0.8290375471115112, 0.9936233162879944]], "normalized_score": [0.307660311460495], "tokens": [["\u0120An", "il", "\u0120K", "umble"]], "entropy": [[5.596269130706787, 2.2755210399627686, 0.6221460700035095, 0.05905131995677948]], "avg_entropy": [2.138246890157461], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pakistan", "score_of_biggest_cluster": 0.307660311460495, "score_of_first_answer": 0.307660311460495, "max_score_of_first_answer": 0.9936233162879944, "min_score_of_first_answer": 0.019821930676698685, "avg_score": 0.307660311460495, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3696671526148215, "std_of_first_answer": 0.3696671526148215, "avg_entropy_of_first_answer": 2.138246890157461, "lex_sim": 1.0}
{"question": "when was the last time minnesota vikings was in the super bowl", "answer": ["1995"], "scores": [[0.005262837745249271]], "normalized_score": [0.005262836813926697], "tokens": [["\u01201995"]], "entropy": [[4.754265785217285]], "avg_entropy": [4.754265785217285], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1969", "score_of_biggest_cluster": 0.005262836813926697, "score_of_first_answer": 0.005262836813926697, "max_score_of_first_answer": 0.005262837745249271, "min_score_of_first_answer": 0.005262837745249271, "avg_score": 0.005262836813926697, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.754265785217285, "lex_sim": 1.0}
{"question": "who plays the dad in drake and josh", "answer": ["Josh Duhamel- Drake"], "scores": [[0.11750670522451401, 0.41647520661354065, 0.983169674873352, 0.9965445399284363, 0.00010761232260847464, 0.059438567608594894]], "normalized_score": [0.08212042599916458], "tokens": [["\u0120Josh", "\u0120Du", "ham", "el", "-", "\u0120Drake"]], "entropy": [[5.62891960144043, 3.112976551055908, 0.15434636175632477, 0.033617082983255386, 0.23375508189201355, 7.105820655822754]], "avg_entropy": [2.7115725558251143], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jonathan Goldstein", "score_of_biggest_cluster": 0.08212042599916458, "score_of_first_answer": 0.08212042599916458, "max_score_of_first_answer": 0.9965445399284363, "min_score_of_first_answer": 0.00010761232260847464, "avg_score": 0.08212042599916458, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4177160146412986, "std_of_first_answer": 0.4177160146412986, "avg_entropy_of_first_answer": 2.7115725558251143, "lex_sim": 1.0}
{"question": "when was last time bayern munich won champions league", "answer": ["2009"], "scores": [[0.005603764671832323]], "normalized_score": [0.005603764671832323], "tokens": [["\u01202009"]], "entropy": [[5.128795623779297]], "avg_entropy": [5.128795623779297], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2012\u201313", "score_of_biggest_cluster": 0.005603764671832323, "score_of_first_answer": 0.005603764671832323, "max_score_of_first_answer": 0.005603764671832323, "min_score_of_first_answer": 0.005603764671832323, "avg_score": 0.005603764671832323, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.128795623779297, "lex_sim": 1.0}
{"question": "public companies are part of what economic sector", "answer": ["industri"], "scores": [[0.0029823307413607836, 0.8999235033988953]], "normalized_score": [0.05180608108639717], "tokens": [["\u0120indust", "ri"]], "entropy": [[5.522592544555664, 0.6976894736289978]], "avg_entropy": [3.110141009092331], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "public sector", "score_of_biggest_cluster": 0.05180608108639717, "score_of_first_answer": 0.05180608108639717, "max_score_of_first_answer": 0.8999235033988953, "min_score_of_first_answer": 0.0029823307413607836, "avg_score": 0.05180608108639717, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44847058632876724, "std_of_first_answer": 0.44847058632876724, "avg_entropy_of_first_answer": 3.110141009092331, "lex_sim": 1.0}
{"question": "when did the soviet union start and end", "answer": ["August 1945 to 1991"], "scores": [[0.024764737114310265, 0.04011833667755127, 0.34591445326805115, 0.4787018299102783]], "normalized_score": [0.11325377225875854], "tokens": [["\u0120August", "\u01201945", "\u0120to", "\u01201991"]], "entropy": [[4.441399097442627, 3.3371100425720215, 2.3575439453125, 2.487118721008301]], "avg_entropy": [3.1557929515838623], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in Eurasia", "score_of_biggest_cluster": 0.11325377225875854, "score_of_first_answer": 0.11325377225875854, "max_score_of_first_answer": 0.4787018299102783, "min_score_of_first_answer": 0.024764737114310265, "avg_score": 0.11325377225875854, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19572477332258056, "std_of_first_answer": 0.19572477332258056, "avg_entropy_of_first_answer": 3.1557929515838623, "lex_sim": 1.0}
{"question": "when did the jurassic park movies come out", "answer": ["2001, 2005, 2010, 2013, 2015"], "scores": [[0.1302807778120041, 0.19571036100387573, 0.13753046095371246, 0.7782715559005737, 0.022374404594302177, 0.2589885890483856, 0.1590949296951294, 0.42623230814933777, 0.5250053405761719]], "normalized_score": [0.2021220326423645], "tokens": [["\u01202001", ",", "\u01202005", ",", "\u01202010", ",", "\u01202013", ",", "\u01202015"]], "entropy": [[3.621690273284912, 1.5711514949798584, 3.4292197227478027, 0.7957121729850769, 2.6254851818084717, 0.9622646570205688, 1.8153660297393799, 1.015953540802002, 1.7505208253860474]], "avg_entropy": [1.9541515443060133], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jurassic Park (1993)", "score_of_biggest_cluster": 0.2021220326423645, "score_of_first_answer": 0.2021220326423645, "max_score_of_first_answer": 0.7782715559005737, "min_score_of_first_answer": 0.022374404594302177, "avg_score": 0.2021220326423645, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22595564770743287, "std_of_first_answer": 0.22595564770743287, "avg_entropy_of_first_answer": 1.9541515443060133, "lex_sim": 1.0}
{"question": "where is the new stadium being built in las vegas", "answer": ["Mandalay Bay"], "scores": [[0.05132844299077988, 0.9995092153549194, 0.9536928534507751]], "normalized_score": [0.36575013399124146], "tokens": [["\u0120Mandal", "ay", "\u0120Bay"]], "entropy": [[5.751694679260254, 0.006889105308800936, 0.31896504759788513]], "avg_entropy": [2.025849610722313], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paradise, Nevada", "score_of_biggest_cluster": 0.36575013399124146, "score_of_first_answer": 0.36575013399124146, "max_score_of_first_answer": 0.9995092153549194, "min_score_of_first_answer": 0.05132844299077988, "avg_score": 0.36575013399124146, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.436578546498413, "std_of_first_answer": 0.436578546498413, "avg_entropy_of_first_answer": 2.025849610722313, "lex_sim": 1.0}
{"question": "what disney cartoon character's middle name is fauntleroy", "answer": ["fauntleroy"], "scores": [[0.005747534800320864, 0.7741088271141052, 0.8834619522094727, 0.9725390076637268]], "normalized_score": [0.24865353107452393], "tokens": [["\u0120f", "aunt", "ler", "oy"]], "entropy": [[5.240121364593506, 1.4515583515167236, 0.8029279112815857, 0.24217654764652252]], "avg_entropy": [1.9341960437595844], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Donald Duck", "score_of_biggest_cluster": 0.24865353107452393, "score_of_first_answer": 0.24865353107452393, "max_score_of_first_answer": 0.9725390076637268, "min_score_of_first_answer": 0.005747534800320864, "avg_score": 0.24865353107452393, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3836269990486257, "std_of_first_answer": 0.3836269990486257, "avg_entropy_of_first_answer": 1.9341960437595844, "lex_sim": 1.0}
{"question": "who won the fountain of youth stakes at gulfstream park", "answer": ["maejig"], "scores": [[0.002219610847532749, 0.011755529791116714, 0.014611446298658848, 0.0024546512868255377]], "normalized_score": [0.00553095992654562], "tokens": [["\u0120m", "ae", "j", "ig"]], "entropy": [[6.871655464172363, 3.739154815673828, 5.592246055603027, 3.45328950881958]], "avg_entropy": [4.9140864610672], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gunnevera", "score_of_biggest_cluster": 0.00553095992654562, "score_of_first_answer": 0.00553095992654562, "max_score_of_first_answer": 0.014611446298658848, "min_score_of_first_answer": 0.002219610847532749, "avg_score": 0.00553095992654562, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.005517001251607755, "std_of_first_answer": 0.005517001251607755, "avg_entropy_of_first_answer": 4.9140864610672, "lex_sim": 1.0}
{"question": "who gave the famous objective co relative theory", "answer": ["Karl E. Popper"], "scores": [[0.025559309870004654, 0.005624521989375353, 0.6783729195594788, 0.24207335710525513, 0.9951453804969788]], "normalized_score": [0.11862805485725403], "tokens": [["\u0120Karl", "\u0120E", ".", "\u0120Po", "pper"]], "entropy": [[5.7965826988220215, 1.815744161605835, 2.0601773262023926, 3.080981731414795, 0.03237878531217575]], "avg_entropy": [2.557172940671444], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "T.S. Eliot", "score_of_biggest_cluster": 0.11862805485725403, "score_of_first_answer": 0.11862805485725403, "max_score_of_first_answer": 0.9951453804969788, "min_score_of_first_answer": 0.005624521989375353, "avg_score": 0.11862805485725403, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3877632290907206, "std_of_first_answer": 0.3877632290907206, "avg_entropy_of_first_answer": 2.557172940671444, "lex_sim": 1.0}
{"question": "who sang the song you got a friend in me", "answer": ["The Beatles"], "scores": [[0.0132508659735322, 0.22413252294063568]], "normalized_score": [0.05449724942445755], "tokens": [["\u0120The", "\u0120Beatles"]], "entropy": [[6.091524124145508, 3.984969139099121]], "avg_entropy": [5.0382466316223145], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lyle Lovett", "score_of_biggest_cluster": 0.05449724942445755, "score_of_first_answer": 0.05449724942445755, "max_score_of_first_answer": 0.22413252294063568, "min_score_of_first_answer": 0.0132508659735322, "avg_score": 0.05449724942445755, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10544082848355174, "std_of_first_answer": 0.10544082848355174, "avg_entropy_of_first_answer": 5.0382466316223145, "lex_sim": 1.0}
{"question": "who has won the most f1 grand prix", "answer": ["Lewis Hamilton"], "scores": [[0.24529512226581573, 0.960188627243042]], "normalized_score": [0.48531389236450195], "tokens": [["\u0120Lewis", "\u0120Hamilton"]], "entropy": [[2.7649025917053223, 0.30812984704971313]], "avg_entropy": [1.5365162193775177], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Michael Schumacher", "score_of_biggest_cluster": 0.48531389236450195, "score_of_first_answer": 0.48531389236450195, "max_score_of_first_answer": 0.960188627243042, "min_score_of_first_answer": 0.24529512226581573, "avg_score": 0.48531389236450195, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35744675248861313, "std_of_first_answer": 0.35744675248861313, "avg_entropy_of_first_answer": 1.5365162193775177, "lex_sim": 1.0}
{"question": "time of the state of the nation address", "answer": ["1600"], "scores": [[0.008576628752052784]], "normalized_score": [0.008576629683375359], "tokens": [["\u01201600"]], "entropy": [[4.784728050231934]], "avg_entropy": [4.784728050231934], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "after 9pm\u00a0ET", "score_of_biggest_cluster": 0.008576629683375359, "score_of_first_answer": 0.008576629683375359, "max_score_of_first_answer": 0.008576628752052784, "min_score_of_first_answer": 0.008576628752052784, "avg_score": 0.008576629683375359, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.784728050231934, "lex_sim": 1.0}
{"question": "who built the tower of london in 1066", "answer": ["William the Conqueror"], "scores": [[0.40441200137138367, 0.5564452409744263, 0.6289258003234863, 0.9891859889030457]], "normalized_score": [0.6116893887519836], "tokens": [["\u0120William", "\u0120the", "\u0120Conquer", "or"]], "entropy": [[3.5823216438293457, 2.676562786102295, 1.5823557376861572, 0.07366235554218292]], "avg_entropy": [1.9787256307899952], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "William the Conqueror", "score_of_biggest_cluster": 0.6116893887519836, "score_of_first_answer": 0.6116893887519836, "max_score_of_first_answer": 0.9891859889030457, "min_score_of_first_answer": 0.40441200137138367, "avg_score": 0.6116893887519836, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.21473625657922424, "std_of_first_answer": 0.21473625657922424, "avg_entropy_of_first_answer": 1.9787256307899952, "lex_sim": 1.0}
{"question": "which term is used to identify having official power to act", "answer": ["prince"], "scores": [[0.004581301007419825]], "normalized_score": [0.004581301007419825], "tokens": [["\u0120prince"]], "entropy": [[6.497046947479248]], "avg_entropy": [6.497046947479248], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "authority", "score_of_biggest_cluster": 0.004581301007419825, "score_of_first_answer": 0.004581301007419825, "max_score_of_first_answer": 0.004581301007419825, "min_score_of_first_answer": 0.004581301007419825, "avg_score": 0.004581301007419825, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.497046947479248, "lex_sim": 1.0}
{"question": "when was the first super bowl the eagles ever won", "answer": ["Super Bowl XVII"], "scores": [[0.06553849577903748, 0.7800256013870239, 0.16661150753498077, 0.5099977850914001]], "normalized_score": [0.25672587752342224], "tokens": [["\u0120Super", "\u0120Bowl", "\u0120XV", "II"]], "entropy": [[4.882444381713867, 0.8296692371368408, 3.341017246246338, 1.6045923233032227]], "avg_entropy": [2.664430797100067], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2017", "score_of_biggest_cluster": 0.25672587752342224, "score_of_first_answer": 0.25672587752342224, "max_score_of_first_answer": 0.7800256013870239, "min_score_of_first_answer": 0.06553849577903748, "avg_score": 0.25672587752342224, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2834340361271589, "std_of_first_answer": 0.2834340361271589, "avg_entropy_of_first_answer": 2.664430797100067, "lex_sim": 1.0}
{"question": "who is mostly responsible for writing the declaration of independence", "answer": ["Thomas Jefferson"], "scores": [[0.6926987767219543, 0.9774231314659119]], "normalized_score": [0.8228364586830139], "tokens": [["\u0120Thomas", "\u0120Jefferson"]], "entropy": [[1.7445592880249023, 0.2256709337234497]], "avg_entropy": [0.985115110874176], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Thomas Jefferson", "score_of_biggest_cluster": 0.8228364586830139, "score_of_first_answer": 0.8228364586830139, "max_score_of_first_answer": 0.9774231314659119, "min_score_of_first_answer": 0.6926987767219543, "avg_score": 0.8228364586830139, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.14236217737197876, "std_of_first_answer": 0.14236217737197876, "avg_entropy_of_first_answer": 0.985115110874176, "lex_sim": 1.0}
{"question": "what are the zords in the power rangers movie", "answer": ["zords"], "scores": [[0.040570616722106934, 0.8082083463668823]], "normalized_score": [0.1810787320137024], "tokens": [["\u0120z", "ords"]], "entropy": [[5.344735622406006, 0.8413990139961243]], "avg_entropy": [3.093067318201065], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "animal-themed Ninja Zords", "score_of_biggest_cluster": 0.1810787320137024, "score_of_first_answer": 0.1810787320137024, "max_score_of_first_answer": 0.8082083463668823, "min_score_of_first_answer": 0.040570616722106934, "avg_score": 0.1810787320137024, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3838188648223877, "std_of_first_answer": 0.3838188648223877, "avg_entropy_of_first_answer": 3.093067318201065, "lex_sim": 1.0}
{"question": "what is the top speed of thrust ssc", "answer": ["about 25,000ft/second"], "scores": [[0.010325267910957336, 0.00742198433727026, 0.3571982979774475, 0.9248852133750916, 0.010854030027985573, 0.7156452536582947, 0.0884004682302475]], "normalized_score": [0.07788462191820145], "tokens": [["\u0120about", "\u012025", ",", "000", "ft", "/", "second"]], "entropy": [[5.6760101318359375, 5.0262250900268555, 3.4210758209228516, 0.6088181734085083, 3.659090995788574, 1.122261643409729, 1.46587073802948]], "avg_entropy": [2.997050370488848], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1,228\u00a0km/h", "score_of_biggest_cluster": 0.07788462191820145, "score_of_first_answer": 0.07788462191820145, "max_score_of_first_answer": 0.9248852133750916, "min_score_of_first_answer": 0.00742198433727026, "avg_score": 0.07788462191820145, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35140051492625346, "std_of_first_answer": 0.35140051492625346, "avg_entropy_of_first_answer": 2.997050370488848, "lex_sim": 1.0}
{"question": "new movie of ajay devgan and sonakshi sinha", "answer": ["Raanjhanaa."], "scores": [[0.004745763726532459, 0.3258586823940277, 0.9966999888420105, 0.9635782837867737, 0.8945680856704712, 0.9430919885635376, 0.0005812751478515565]], "normalized_score": [0.13279777765274048], "tokens": [["\u0120Ra", "an", "j", "h", "ana", "a", "."]], "entropy": [[6.964575290679932, 2.2685859203338623, 0.03414199501276016, 0.2612660825252533, 0.7024545669555664, 0.3320457637310028, 0.4317587614059448]], "avg_entropy": [1.5706897686634744], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Action Jackson", "score_of_biggest_cluster": 0.13279777765274048, "score_of_first_answer": 0.13279777765274048, "max_score_of_first_answer": 0.9966999888420105, "min_score_of_first_answer": 0.0005812751478515565, "avg_score": 0.13279777765274048, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42797008683498167, "std_of_first_answer": 0.42797008683498167, "avg_entropy_of_first_answer": 1.5706897686634744, "lex_sim": 1.0}
{"question": "who inaugurated 'world teachers' day'", "answer": ["Vladimir Dmitrievich Lenin"], "scores": [[0.03753093630075455, 0.0013813382247462869, 0.8719201683998108, 0.6864264607429504, 0.9410953521728516, 0.9471033215522766]], "normalized_score": [0.1738995909690857], "tokens": [["\u0120Vladimir", "\u0120Dmit", "ri", "ev", "ich", "\u0120Lenin"]], "entropy": [[6.256193161010742, 1.7923836708068848, 0.7591185569763184, 1.7127339839935303, 0.43234536051750183, 0.4123135805130005]], "avg_entropy": [1.8941813856363297], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "UNESCO", "score_of_biggest_cluster": 0.1738995909690857, "score_of_first_answer": 0.1738995909690857, "max_score_of_first_answer": 0.9471033215522766, "min_score_of_first_answer": 0.0013813382247462869, "avg_score": 0.1738995909690857, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40635902741169705, "std_of_first_answer": 0.40635902741169705, "avg_entropy_of_first_answer": 1.8941813856363297, "lex_sim": 1.0}
{"question": "when were birth certificates first issued in the united states", "answer": ["1887"], "scores": [[0.15336008369922638, 0.04890545830130577]], "normalized_score": [0.08660338819026947], "tokens": [["\u012018", "87"]], "entropy": [[3.7303903102874756, 3.5827853679656982]], "avg_entropy": [3.656587839126587], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1902", "score_of_biggest_cluster": 0.08660338819026947, "score_of_first_answer": 0.08660338819026947, "max_score_of_first_answer": 0.15336008369922638, "min_score_of_first_answer": 0.04890545830130577, "avg_score": 0.08660338819026947, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.052227312698960304, "std_of_first_answer": 0.052227312698960304, "avg_entropy_of_first_answer": 3.656587839126587, "lex_sim": 1.0}
{"question": "sending money home to the native country is an example of", "answer": ["voluntary tax evasion"], "scores": [[0.010164867155253887, 0.026878440752625465, 0.038732945919036865]], "normalized_score": [0.02195475809276104], "tokens": [["\u0120voluntary", "\u0120tax", "\u0120evasion"]], "entropy": [[6.4082489013671875, 4.4716620445251465, 1.9590976238250732]], "avg_entropy": [4.279669523239136], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "international capital flows", "score_of_biggest_cluster": 0.02195475809276104, "score_of_first_answer": 0.02195475809276104, "max_score_of_first_answer": 0.038732945919036865, "min_score_of_first_answer": 0.010164867155253887, "avg_score": 0.02195475809276104, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.011718968286921937, "std_of_first_answer": 0.011718968286921937, "avg_entropy_of_first_answer": 4.279669523239136, "lex_sim": 1.0}
{"question": "who won the marathon in new york city", "answer": ["Ina Boudreau"], "scores": [[0.009409131482243538, 0.03093474544584751, 0.09870164096355438, 0.02365446649491787, 0.12112953513860703]], "normalized_score": [0.03829100355505943], "tokens": [["\u0120In", "a", "\u0120B", "oud", "reau"]], "entropy": [[5.223250865936279, 4.532320976257324, 5.254140377044678, 4.094221115112305, 4.08350133895874]], "avg_entropy": [4.637486934661865], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Geoffrey Kamworor", "score_of_biggest_cluster": 0.03829100355505943, "score_of_first_answer": 0.03829100355505943, "max_score_of_first_answer": 0.12112953513860703, "min_score_of_first_answer": 0.009409131482243538, "avg_score": 0.03829100355505943, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04451418425002157, "std_of_first_answer": 0.04451418425002157, "avg_entropy_of_first_answer": 4.637486934661865, "lex_sim": 1.0}
{"question": "when was the last time the seattle seahawks won the superbowl", "answer": ["Super Bowl XLVIII"], "scores": [[0.1749361753463745, 0.4358438551425934, 0.23661306500434875, 0.8431165814399719, 0.942379355430603]], "normalized_score": [0.4278312921524048], "tokens": [["\u0120Super", "\u0120Bowl", "\u0120XL", "V", "III"]], "entropy": [[4.080275058746338, 1.1864978075027466, 3.509369373321533, 0.7238052487373352, 0.3189260959625244]], "avg_entropy": [1.9637747168540955], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2013", "score_of_biggest_cluster": 0.4278312921524048, "score_of_first_answer": 0.4278312921524048, "max_score_of_first_answer": 0.942379355430603, "min_score_of_first_answer": 0.1749361753463745, "avg_score": 0.4278312921524048, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3127464666894056, "std_of_first_answer": 0.3127464666894056, "avg_entropy_of_first_answer": 1.9637747168540955, "lex_sim": 1.0}
{"question": "who's winning the election in virginia for governor", "answer": ["Rick Perry"], "scores": [[0.0016877283342182636, 0.0958896353840828]], "normalized_score": [0.012721462175250053], "tokens": [["\u0120Rick", "\u0120Perry"]], "entropy": [[3.5953123569488525, 5.010583877563477]], "avg_entropy": [4.3029481172561646], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ralph Northam", "score_of_biggest_cluster": 0.012721462175250053, "score_of_first_answer": 0.012721462175250053, "max_score_of_first_answer": 0.0958896353840828, "min_score_of_first_answer": 0.0016877283342182636, "avg_score": 0.012721462175250053, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.047100953524932265, "std_of_first_answer": 0.047100953524932265, "avg_entropy_of_first_answer": 4.3029481172561646, "lex_sim": 1.0}
{"question": "what nfl team has the most expensive super bowl ring", "answer": ["Washington Redskins"], "scores": [[0.006375914439558983, 0.7706656455993652]], "normalized_score": [0.07009778171777725], "tokens": [["\u0120Washington", "\u0120Redskins"]], "entropy": [[4.002939224243164, 1.1500991582870483]], "avg_entropy": [2.576519191265106], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the New England Patriots", "score_of_biggest_cluster": 0.07009778171777725, "score_of_first_answer": 0.07009778171777725, "max_score_of_first_answer": 0.7706656455993652, "min_score_of_first_answer": 0.006375914439558983, "avg_score": 0.07009778171777725, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3821448655799031, "std_of_first_answer": 0.3821448655799031, "avg_entropy_of_first_answer": 2.576519191265106, "lex_sim": 1.0}
{"question": "when does zay come to girl meets world", "answer": ["season 2"], "scores": [[0.0020339535549283028, 0.21283425390720367]], "normalized_score": [0.020806126296520233], "tokens": [["\u0120season", "\u01202"]], "entropy": [[4.996442794799805, 2.4726784229278564]], "avg_entropy": [3.7345606088638306], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "May\u00a013,\u00a02015", "score_of_biggest_cluster": 0.020806126296520233, "score_of_first_answer": 0.020806126296520233, "max_score_of_first_answer": 0.21283425390720367, "min_score_of_first_answer": 0.0020339535549283028, "avg_score": 0.020806126296520233, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.10540015017613769, "std_of_first_answer": 0.10540015017613769, "avg_entropy_of_first_answer": 3.7345606088638306, "lex_sim": 1.0}
{"question": "who plays at the prudential center in newark", "answer": ["Peter Cappelli"], "scores": [[0.006528683006763458, 0.009591896086931229, 0.009663298726081848, 0.18422798812389374]], "normalized_score": [0.018272703513503075], "tokens": [["\u0120Peter", "\u0120C", "app", "elli"]], "entropy": [[6.975088119506836, 6.185728073120117, 3.0902609825134277, 3.4257287979125977]], "avg_entropy": [4.919201493263245], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Seton Hall Pirates", "score_of_biggest_cluster": 0.018272703513503075, "score_of_first_answer": 0.018272703513503075, "max_score_of_first_answer": 0.18422798812389374, "min_score_of_first_answer": 0.006528683006763458, "avg_score": 0.018272703513503075, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07606200293758175, "std_of_first_answer": 0.07606200293758175, "avg_entropy_of_first_answer": 4.919201493263245, "lex_sim": 1.0}
{"question": "what is the meaning of the name sinead", "answer": ["female name"], "scores": [[0.011164727620780468, 0.21807868778705597]], "normalized_score": [0.049343593418598175], "tokens": [["\u0120female", "\u0120name"]], "entropy": [[6.397410869598389, 3.539363384246826]], "avg_entropy": [4.968387126922607], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "God forgave/God gratified", "score_of_biggest_cluster": 0.049343593418598175, "score_of_first_answer": 0.049343593418598175, "max_score_of_first_answer": 0.21807868778705597, "min_score_of_first_answer": 0.011164727620780468, "avg_score": 0.049343593418598175, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10345698008313775, "std_of_first_answer": 0.10345698008313775, "avg_entropy_of_first_answer": 4.968387126922607, "lex_sim": 1.0}
{"question": "when will miraculous season 2 episode 11 be released", "answer": ["this week"], "scores": [[0.019286366179585457, 0.16574344038963318]], "normalized_score": [0.056538376957178116], "tokens": [["\u0120this", "\u0120week"]], "entropy": [[4.618867874145508, 3.269479274749756]], "avg_entropy": [3.944173574447632], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "30\u00a0March\u00a02018", "score_of_biggest_cluster": 0.056538376957178116, "score_of_first_answer": 0.056538376957178116, "max_score_of_first_answer": 0.16574344038963318, "min_score_of_first_answer": 0.019286366179585457, "avg_score": 0.056538376957178116, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07322853710502386, "std_of_first_answer": 0.07322853710502386, "avg_entropy_of_first_answer": 3.944173574447632, "lex_sim": 1.0}
{"question": "who played young clark kent in 1978 superman", "answer": ["Marlon Brando"], "scores": [[0.020296059548854828, 0.9978955984115601, 0.8980752229690552, 0.9967233538627625]], "normalized_score": [0.36694082617759705], "tokens": [["\u0120Mar", "lon", "\u0120Brand", "o"]], "entropy": [[2.3181097507476807, 0.021744899451732635, 0.5863773226737976, 0.03453334793448448]], "avg_entropy": [0.7401913302019238], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jeff East", "score_of_biggest_cluster": 0.36694082617759705, "score_of_first_answer": 0.36694082617759705, "max_score_of_first_answer": 0.9978955984115601, "min_score_of_first_answer": 0.020296059548854828, "avg_score": 0.36694082617759705, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4107389921257785, "std_of_first_answer": 0.4107389921257785, "avg_entropy_of_first_answer": 0.7401913302019238, "lex_sim": 1.0}
{"question": "when did the international space station go into space", "answer": ["on October 8, 2000"], "scores": [[0.008552913554012775, 0.22855474054813385, 0.08050062507390976, 0.2754874527454376, 0.5006719827651978]], "normalized_score": [0.11676476150751114], "tokens": [["\u0120on", "\u0120October", "\u01208", ",", "\u01202000"]], "entropy": [[4.483107089996338, 3.5301146507263184, 3.348538875579834, 1.0103214979171753, 1.3201749324798584]], "avg_entropy": [2.7384514093399046], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1998", "score_of_biggest_cluster": 0.11676476150751114, "score_of_first_answer": 0.11676476150751114, "max_score_of_first_answer": 0.5006719827651978, "min_score_of_first_answer": 0.008552913554012775, "avg_score": 0.11676476150751114, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17093330898487394, "std_of_first_answer": 0.17093330898487394, "avg_entropy_of_first_answer": 2.7384514093399046, "lex_sim": 1.0}
{"question": "who took part in the american civil war", "answer": ["only three (2 southern, 1 northern)"], "scores": [[0.0017837926279753447, 0.006945640780031681, 0.005441954825073481, 0.012368084862828255, 0.005620070733129978, 0.2547471225261688, 0.9007073044776917, 0.9022683501243591, 0.9490206241607666]], "normalized_score": [0.045992232859134674], "tokens": [["\u0120only", "\u0120three", "\u0120(", "2", "\u0120southern", ",", "\u01201", "\u0120northern", ")"]], "entropy": [[5.00701904296875, 4.942261695861816, 4.138418674468994, 5.8481645584106445, 5.059152126312256, 2.2087960243225098, 0.5791089534759521, 0.5550605058670044, 0.3959156274795532]], "avg_entropy": [3.1926552454630532], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "nationalists of the Union", "score_of_biggest_cluster": 0.045992232859134674, "score_of_first_answer": 0.045992232859134674, "max_score_of_first_answer": 0.9490206241607666, "min_score_of_first_answer": 0.0017837926279753447, "avg_score": 0.045992232859134674, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4170078872523891, "std_of_first_answer": 0.4170078872523891, "avg_entropy_of_first_answer": 3.1926552454630532, "lex_sim": 1.0}
{"question": "what happened to the west wing of the white house during a christmas party in 1929", "answer": ["it was being painted red, yellow, green and blue"], "scores": [[0.03644868731498718, 0.29851168394088745, 0.0048920162953436375, 0.09211655706167221, 0.03717847168445587, 0.03301212191581726, 0.010463816113770008, 0.5757901072502136, 0.12758010625839233, 0.32068753242492676, 0.6927520036697388]], "normalized_score": [0.08131422847509384], "tokens": [["\u0120it", "\u0120was", "\u0120being", "\u0120painted", "\u0120red", ",", "\u0120yellow", ",", "\u0120green", "\u0120and", "\u0120blue"]], "entropy": [[5.268084526062012, 2.3485965728759766, 4.603200912475586, 3.848278760910034, 1.8296042680740356, 1.4865461587905884, 1.0948493480682373, 0.8218327164649963, 1.6610931158065796, 1.121335744857788, 1.5219595432281494]], "avg_entropy": [2.3277619697830896], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "electrical fire", "score_of_biggest_cluster": 0.08131422847509384, "score_of_first_answer": 0.08131422847509384, "max_score_of_first_answer": 0.6927520036697388, "min_score_of_first_answer": 0.0048920162953436375, "avg_score": 0.08131422847509384, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22967040037937408, "std_of_first_answer": 0.22967040037937408, "avg_entropy_of_first_answer": 2.3277619697830896, "lex_sim": 1.0}
{"question": "when did 10 shilling note go out of circulation", "answer": ["1885"], "scores": [[0.10108670592308044, 0.004176673479378223]], "normalized_score": [0.02054765820503235], "tokens": [["\u012018", "85"]], "entropy": [[4.900543212890625, 3.4944396018981934]], "avg_entropy": [4.197491407394409], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "22 November 1970", "score_of_biggest_cluster": 0.02054765820503235, "score_of_first_answer": 0.02054765820503235, "max_score_of_first_answer": 0.10108670592308044, "min_score_of_first_answer": 0.004176673479378223, "avg_score": 0.02054765820503235, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04845501622185111, "std_of_first_answer": 0.04845501622185111, "avg_entropy_of_first_answer": 4.197491407394409, "lex_sim": 1.0}
{"question": "when does season 8 of vampire diaries come out", "answer": ["2014"], "scores": [[0.008078867569565773]], "normalized_score": [0.008078869432210922], "tokens": [["\u01202014"]], "entropy": [[3.899775505065918]], "avg_entropy": [3.899775505065918], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "October 21, 2016", "score_of_biggest_cluster": 0.008078869432210922, "score_of_first_answer": 0.008078869432210922, "max_score_of_first_answer": 0.008078867569565773, "min_score_of_first_answer": 0.008078867569565773, "avg_score": 0.008078869432210922, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.899775505065918, "lex_sim": 1.0}
{"question": "who is the best director in bollywood 2017", "answer": ["rajkummar singh"], "scores": [[0.010673384182155132, 0.8031437397003174, 0.7229505777359009, 0.0447809211909771, 0.9367712140083313, 0.1425771564245224, 0.9902100563049316]], "normalized_score": [0.23248068988323212], "tokens": [["\u0120r", "aj", "k", "um", "mar", "\u0120sing", "h"]], "entropy": [[4.59257698059082, 1.1459927558898926, 1.3379387855529785, 0.6683254241943359, 0.42101046442985535, 3.9274866580963135, 0.083322674036026]], "avg_entropy": [1.739521963255746], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nitesh Tiwari", "score_of_biggest_cluster": 0.23248068988323212, "score_of_first_answer": 0.23248068988323212, "max_score_of_first_answer": 0.9902100563049316, "min_score_of_first_answer": 0.010673384182155132, "avg_score": 0.23248068988323212, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40423335016784384, "std_of_first_answer": 0.40423335016784384, "avg_entropy_of_first_answer": 1.739521963255746, "lex_sim": 1.0}
{"question": "what episode does goku give up against cell", "answer": ["Battle in the Solar Empire"], "scores": [[0.0035086756106466055, 0.005557957570999861, 0.5892750024795532, 0.005176452454179525, 0.007743817754089832]], "normalized_score": [0.013572905212640762], "tokens": [["\u0120Battle", "\u0120in", "\u0120the", "\u0120Solar", "\u0120Empire"]], "entropy": [[5.930934906005859, 1.4776805639266968, 2.8204116821289062, 4.751967430114746, 2.065992832183838]], "avg_entropy": [3.4093974828720093], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "165", "score_of_biggest_cluster": 0.013572905212640762, "score_of_first_answer": 0.013572905212640762, "max_score_of_first_answer": 0.5892750024795532, "min_score_of_first_answer": 0.0035086756106466055, "avg_score": 0.013572905212640762, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23351521111232262, "std_of_first_answer": 0.23351521111232262, "avg_entropy_of_first_answer": 3.4093974828720093, "lex_sim": 1.0}
{"question": "what are the 3 layers of the meninges", "answer": ["1. Dura, 2. White, 3. Pia"], "scores": [[0.012952908873558044, 0.4573354721069336, 0.10014693439006805, 0.9829652905464172, 0.269452840089798, 0.8802236318588257, 0.9866753220558167, 0.005052432883530855, 0.6460826992988586, 0.9501790404319763, 0.9915658235549927, 0.06819325685501099, 0.9025298357009888]], "normalized_score": [0.2608068287372589], "tokens": [["\u01201", ".", "\u0120D", "ura", ",", "\u01202", ".", "\u0120White", ",", "\u01203", ".", "\u0120P", "ia"]], "entropy": [[3.7876336574554443, 2.277489423751831, 4.276588439941406, 0.09653959423303604, 2.8772480487823486, 0.9898030161857605, 0.139009028673172, 3.8933820724487305, 1.764817476272583, 0.2664335072040558, 0.08959850668907166, 3.6566414833068848, 0.4212653338909149]], "avg_entropy": [1.8874191991411722], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the arachnoid mater", "score_of_biggest_cluster": 0.2608068287372589, "score_of_first_answer": 0.2608068287372589, "max_score_of_first_answer": 0.9915658235549927, "min_score_of_first_answer": 0.005052432883530855, "avg_score": 0.2608068287372589, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4002104628153116, "std_of_first_answer": 0.4002104628153116, "avg_entropy_of_first_answer": 1.8874191991411722, "lex_sim": 1.0}
{"question": "most assists in an nba all star game", "answer": ["Stephen Curry (25)"], "scores": [[0.010840579867362976, 0.8033422827720642, 0.02682686224579811, 0.007406739518046379, 0.8327237963676453]], "normalized_score": [0.06787818670272827], "tokens": [["\u0120Stephen", "\u0120Curry", "\u0120(", "25", ")"]], "entropy": [[6.0042924880981445, 1.4294946193695068, 1.1786601543426514, 4.732410430908203, 0.9890389442443848]], "avg_entropy": [2.866779327392578], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "60", "score_of_biggest_cluster": 0.06787818670272827, "score_of_first_answer": 0.06787818670272827, "max_score_of_first_answer": 0.8327237963676453, "min_score_of_first_answer": 0.007406739518046379, "avg_score": 0.06787818670272827, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3935564222699198, "std_of_first_answer": 0.3935564222699198, "avg_entropy_of_first_answer": 2.866779327392578, "lex_sim": 1.0}
{"question": "when does it's a wonderful life comes on", "answer": ["The day after tomorrow"], "scores": [[0.019532151520252228, 0.030705083161592484, 0.432341992855072, 0.5654464364051819]], "normalized_score": [0.11003853380680084], "tokens": [["\u0120The", "\u0120day", "\u0120after", "\u0120tomorrow"]], "entropy": [[5.468254089355469, 6.444833278656006, 2.7387685775756836, 2.232241153717041]], "avg_entropy": [4.22102427482605], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Christmas season", "score_of_biggest_cluster": 0.11003853380680084, "score_of_first_answer": 0.11003853380680084, "max_score_of_first_answer": 0.5654464364051819, "min_score_of_first_answer": 0.019532151520252228, "avg_score": 0.11003853380680084, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24154923423282973, "std_of_first_answer": 0.24154923423282973, "avg_entropy_of_first_answer": 4.22102427482605, "lex_sim": 1.0}
{"question": "do you need a permit or license to own a gun in florida", "answer": ["no, only a license to fire a gun"], "scores": [[0.13828161358833313, 0.03723074123263359, 0.035066138952970505, 0.3252795338630676, 0.23280306160449982, 0.5601558089256287, 0.004838435910642147, 0.2457141876220703, 0.9213578701019287]], "normalized_score": [0.12665729224681854], "tokens": [["\u0120no", ",", "\u0120only", "\u0120a", "\u0120license", "\u0120to", "\u0120fire", "\u0120a", "\u0120gun"]], "entropy": [[3.718517303466797, 0.7586830854415894, 3.5301783084869385, 3.7612273693084717, 3.676785945892334, 1.7507160902023315, 2.053654432296753, 2.0278594493865967, 0.4877295196056366]], "avg_entropy": [2.41837238934305], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "No", "score_of_biggest_cluster": 0.12665729224681854, "score_of_first_answer": 0.12665729224681854, "max_score_of_first_answer": 0.9213578701019287, "min_score_of_first_answer": 0.004838435910642147, "avg_score": 0.12665729224681854, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2808650773821588, "std_of_first_answer": 0.2808650773821588, "avg_entropy_of_first_answer": 2.41837238934305, "lex_sim": 1.0}
{"question": "who sings too much time on my hands lyrics", "answer": ["The Script"], "scores": [[0.022237278521060944, 0.024520941078662872]], "normalized_score": [0.023351209238171577], "tokens": [["\u0120The", "\u0120Script"]], "entropy": [[6.757596969604492, 5.393123626708984]], "avg_entropy": [6.075360298156738], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tommy Shaw", "score_of_biggest_cluster": 0.023351209238171577, "score_of_first_answer": 0.023351209238171577, "max_score_of_first_answer": 0.024520941078662872, "min_score_of_first_answer": 0.022237278521060944, "avg_score": 0.023351209238171577, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0011418312788009644, "std_of_first_answer": 0.0011418312788009644, "avg_entropy_of_first_answer": 6.075360298156738, "lex_sim": 1.0}
{"question": "when did billy don't be a hero come out", "answer": ["December 15, 2006"], "scores": [[0.02134493552148342, 0.01999334804713726, 0.35518795251846313, 0.020853359252214432]], "normalized_score": [0.04216514527797699], "tokens": [["\u0120December", "\u012015", ",", "\u01202006"]], "entropy": [[4.756165504455566, 3.1944580078125, 1.0907423496246338, 3.3754005432128906]], "avg_entropy": [3.1041916012763977], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1974", "score_of_biggest_cluster": 0.04216514527797699, "score_of_first_answer": 0.04216514527797699, "max_score_of_first_answer": 0.35518795251846313, "min_score_of_first_answer": 0.01999334804713726, "avg_score": 0.04216514527797699, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1448251124576735, "std_of_first_answer": 0.1448251124576735, "avg_entropy_of_first_answer": 3.1041916012763977, "lex_sim": 1.0}
{"question": "when is season 4 of flash coming out", "answer": ["2014"], "scores": [[0.01386913750320673]], "normalized_score": [0.013869140297174454], "tokens": [["\u01202014"]], "entropy": [[4.593863487243652]], "avg_entropy": [4.593863487243652], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "October 10, 2017", "score_of_biggest_cluster": 0.013869140297174454, "score_of_first_answer": 0.013869140297174454, "max_score_of_first_answer": 0.01386913750320673, "min_score_of_first_answer": 0.01386913750320673, "avg_score": 0.013869140297174454, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.593863487243652, "lex_sim": 1.0}
{"question": "who did america declare war on in ww1", "answer": ["France"], "scores": [[0.019415233284235]], "normalized_score": [0.01941523142158985], "tokens": [["\u0120France"]], "entropy": [[2.069459915161133]], "avg_entropy": [2.069459915161133], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Austria-Hungary", "score_of_biggest_cluster": 0.01941523142158985, "score_of_first_answer": 0.01941523142158985, "max_score_of_first_answer": 0.019415233284235, "min_score_of_first_answer": 0.019415233284235, "avg_score": 0.01941523142158985, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.069459915161133, "lex_sim": 1.0}
{"question": "who has the most catches in nfl history", "answer": ["hall of fame"], "scores": [[0.00471928296610713, 0.982613742351532, 0.35558927059173584]], "normalized_score": [0.11814150959253311], "tokens": [["\u0120hall", "\u0120of", "\u0120fame"]], "entropy": [[4.5756683349609375, 0.16156062483787537, 0.7677872776985168]], "avg_entropy": [1.83500541249911], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jerry Rice", "score_of_biggest_cluster": 0.11814150959253311, "score_of_first_answer": 0.11814150959253311, "max_score_of_first_answer": 0.982613742351532, "min_score_of_first_answer": 0.00471928296610713, "avg_score": 0.11814150959253311, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4044951599901032, "std_of_first_answer": 0.4044951599901032, "avg_entropy_of_first_answer": 1.83500541249911, "lex_sim": 1.0}
{"question": "who has won the most games in nfl 2017", "answer": ["Tom Brady"], "scores": [[0.31982630491256714, 0.9900936484336853]], "normalized_score": [0.562723696231842], "tokens": [["\u0120Tom", "\u0120Brady"]], "entropy": [[3.7944648265838623, 0.10373611003160477]], "avg_entropy": [1.9491004683077335], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dallas Cowboys", "score_of_biggest_cluster": 0.562723696231842, "score_of_first_answer": 0.562723696231842, "max_score_of_first_answer": 0.9900936484336853, "min_score_of_first_answer": 0.31982630491256714, "avg_score": 0.562723696231842, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3351336717605591, "std_of_first_answer": 0.3351336717605591, "avg_entropy_of_first_answer": 1.9491004683077335, "lex_sim": 1.0}
{"question": "who sang smoke gets in your eyes first", "answer": ["Prince"], "scores": [[0.004031132906675339]], "normalized_score": [0.004031133837997913], "tokens": [["\u0120Prince"]], "entropy": [[5.842124938964844]], "avg_entropy": [5.842124938964844], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gertrude Niesen", "score_of_biggest_cluster": 0.004031133837997913, "score_of_first_answer": 0.004031133837997913, "max_score_of_first_answer": 0.004031132906675339, "min_score_of_first_answer": 0.004031132906675339, "avg_score": 0.004031133837997913, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.842124938964844, "lex_sim": 1.0}
{"question": "guardians of the galaxy blue guy in prison", "answer": ["David Tennant"], "scores": [[0.008348565548658371, 0.12147991359233856, 0.9661468863487244]], "normalized_score": [0.09932375699281693], "tokens": [["\u0120David", "\u0120Tenn", "ant"]], "entropy": [[5.885698318481445, 5.288751125335693, 0.27925437688827515]], "avg_entropy": [3.8179012735684714], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Michael Rooker", "score_of_biggest_cluster": 0.09932375699281693, "score_of_first_answer": 0.09932375699281693, "max_score_of_first_answer": 0.9661468863487244, "min_score_of_first_answer": 0.008348565548658371, "avg_score": 0.09932375699281693, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4273482342829821, "std_of_first_answer": 0.4273482342829821, "avg_entropy_of_first_answer": 3.8179012735684714, "lex_sim": 1.0}
{"question": "with a land area of 54 314 square miles where does wisconsin rank among the 50 states", "answer": ["5th"], "scores": [[0.0233137346804142, 0.6185904741287231]], "normalized_score": [0.12009020894765854], "tokens": [["\u01205", "th"]], "entropy": [[4.573789596557617, 0.8516812324523926]], "avg_entropy": [2.712735414505005], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "25", "score_of_biggest_cluster": 0.12009020894765854, "score_of_first_answer": 0.12009020894765854, "max_score_of_first_answer": 0.6185904741287231, "min_score_of_first_answer": 0.0233137346804142, "avg_score": 0.12009020894765854, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2976383697241545, "std_of_first_answer": 0.2976383697241545, "avg_entropy_of_first_answer": 2.712735414505005, "lex_sim": 1.0}
{"question": "of which country is sofia the capital city", "answer": ["Bulgaria"], "scores": [[0.7947974801063538]], "normalized_score": [0.7947974801063538], "tokens": [["\u0120Bulgaria"]], "entropy": [[1.4712531566619873]], "avg_entropy": [1.4712531566619873], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bulgaria", "score_of_biggest_cluster": 0.7947974801063538, "score_of_first_answer": 0.7947974801063538, "max_score_of_first_answer": 0.7947974801063538, "min_score_of_first_answer": 0.7947974801063538, "avg_score": 0.7947974801063538, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.4712531566619873, "lex_sim": 1.0}
{"question": "how many farmers are there in the usa", "answer": ["7,638,000"], "scores": [[0.01153239794075489, 0.07255543023347855, 0.00988146010786295, 0.08363305032253265, 0.9403673410415649, 0.20842579007148743]], "normalized_score": [0.07167035341262817], "tokens": [["\u01207", ",", "6", "38", ",", "000"]], "entropy": [[5.473778247833252, 2.6303207874298096, 5.767647743225098, 2.734530210494995, 0.2837614417076111, 5.4222917556762695]], "avg_entropy": [3.718721697727839], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "3.2 million", "score_of_biggest_cluster": 0.07167035341262817, "score_of_first_answer": 0.07167035341262817, "max_score_of_first_answer": 0.9403673410415649, "min_score_of_first_answer": 0.00988146010786295, "avg_score": 0.07167035341262817, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3283800236012095, "std_of_first_answer": 0.3283800236012095, "avg_entropy_of_first_answer": 3.718721697727839, "lex_sim": 1.0}
{"question": "what year did the price is right first air", "answer": ["1987"], "scores": [[0.020467707887291908]], "normalized_score": [0.02046770602464676], "tokens": [["\u01201987"]], "entropy": [[4.584424018859863]], "avg_entropy": [4.584424018859863], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "November 26, 1956", "score_of_biggest_cluster": 0.02046770602464676, "score_of_first_answer": 0.02046770602464676, "max_score_of_first_answer": 0.020467707887291908, "min_score_of_first_answer": 0.020467707887291908, "avg_score": 0.02046770602464676, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.584424018859863, "lex_sim": 1.0}
{"question": "what is the oasis in ready player one", "answer": ["oasis"], "scores": [[0.035109881311655045, 0.9712292551994324]], "normalized_score": [0.1846611648797989], "tokens": [["\u0120o", "asis"]], "entropy": [[6.641228675842285, 0.24517148733139038]], "avg_entropy": [3.4432000815868378], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "a virtual reality simulator", "score_of_biggest_cluster": 0.1846611648797989, "score_of_first_answer": 0.1846611648797989, "max_score_of_first_answer": 0.9712292551994324, "min_score_of_first_answer": 0.035109881311655045, "avg_score": 0.1846611648797989, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46805968694388866, "std_of_first_answer": 0.46805968694388866, "avg_entropy_of_first_answer": 3.4432000815868378, "lex_sim": 1.0}
{"question": "premier league players to score 5 goals in one game", "answer": ["five"], "scores": [[0.012586661614477634]], "normalized_score": [0.012586659751832485], "tokens": [["\u0120five"]], "entropy": [[5.9480977058410645]], "avg_entropy": [5.9480977058410645], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alan Shearer", "score_of_biggest_cluster": 0.012586659751832485, "score_of_first_answer": 0.012586659751832485, "max_score_of_first_answer": 0.012586661614477634, "min_score_of_first_answer": 0.012586661614477634, "avg_score": 0.012586659751832485, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.9480977058410645, "lex_sim": 1.0}
{"question": "when did the smoking ban in public places start", "answer": ["November 13 2006"], "scores": [[0.035713374614715576, 0.004461321514099836, 0.0009972614934667945]], "normalized_score": [0.005416279658675194], "tokens": [["\u0120November", "\u012013", "\u01202006"]], "entropy": [[4.420307636260986, 3.542891502380371, 1.0903068780899048]], "avg_entropy": [3.017835338910421], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "August 2, 1990", "score_of_biggest_cluster": 0.005416279658675194, "score_of_first_answer": 0.005416279658675194, "max_score_of_first_answer": 0.035713374614715576, "min_score_of_first_answer": 0.0009972614934667945, "avg_score": 0.005416279658675194, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.015613025349501528, "std_of_first_answer": 0.015613025349501528, "avg_entropy_of_first_answer": 3.017835338910421, "lex_sim": 1.0}
{"question": "when do love and hip hop new york come back on", "answer": ["May 22, 2014; May 22, 2016"], "scores": [[0.033583808690309525, 0.023290978744626045, 0.11584039777517319, 0.049820300191640854, 0.00220563099719584, 0.07150615751743317, 0.0572715625166893, 0.9575940370559692, 0.07299885898828506]], "normalized_score": [0.05214477330446243], "tokens": [["\u0120May", "\u012022", ",", "\u01202014", ";", "\u0120May", "\u012022", ",", "\u01202016"]], "entropy": [[5.261168003082275, 3.65928316116333, 1.3171710968017578, 3.173872709274292, 1.0901519060134888, 5.023834228515625, 3.352691173553467, 0.30925512313842773, 2.1370630264282227]], "avg_entropy": [2.813832269774543], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "November\u00a013,\u00a02017", "score_of_biggest_cluster": 0.05214477330446243, "score_of_first_answer": 0.05214477330446243, "max_score_of_first_answer": 0.9575940370559692, "min_score_of_first_answer": 0.00220563099719584, "avg_score": 0.05214477330446243, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28585578972078923, "std_of_first_answer": 0.28585578972078923, "avg_entropy_of_first_answer": 2.813832269774543, "lex_sim": 1.0}
{"question": "who plays dorian tyrell when he puts on the mask", "answer": ["Alan Rickman"], "scores": [[0.0094927828758955, 0.5723620653152466, 0.9951434135437012]], "normalized_score": [0.17551597952842712], "tokens": [["\u0120Alan", "\u0120Rick", "man"]], "entropy": [[5.405265808105469, 3.05767560005188, 0.05256752669811249]], "avg_entropy": [2.838502978285154], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Peter Greene", "score_of_biggest_cluster": 0.17551597952842712, "score_of_first_answer": 0.17551597952842712, "max_score_of_first_answer": 0.9951434135437012, "min_score_of_first_answer": 0.0094927828758955, "avg_score": 0.17551597952842712, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40374263849628855, "std_of_first_answer": 0.40374263849628855, "avg_entropy_of_first_answer": 2.838502978285154, "lex_sim": 1.0}
{"question": "how many us states are commonwealths and which states are they", "answer": ["28 states are commonwealthsI was not surprised to receive this letter from Michael Brown today:"], "scores": [[0.007778559345752001, 0.02786233089864254, 0.2565784156322479, 0.8357413411140442, 0.9961276650428772, 0.9252529740333557, 6.864394208605518e-07, 0.005619170609861612, 0.09219957888126373, 0.0004889616975560784, 0.13144132494926453, 0.008319742046296597, 0.18781527876853943, 0.0008997658151201904, 0.14231239259243011, 0.0009873142698779702, 0.0018545163329690695, 0.014669216237962246, 0.04318777099251747]], "normalized_score": [0.016954606398940086], "tokens": [["\u012028", "\u0120states", "\u0120are", "\u0120common", "wealth", "s", "I", "\u0120was", "\u0120not", "\u0120surprised", "\u0120to", "\u0120receive", "\u0120this", "\u0120letter", "\u0120from", "\u0120Michael", "\u0120Brown", "\u0120today", ":"]], "entropy": [[5.069632530212402, 0.7962785959243774, 2.0804293155670166, 1.0283703804016113, 0.03299189358949661, 0.4498917758464813, 1.6227158308029175, 4.134402275085449, 4.837856292724609, 2.8354952335357666, 2.912520408630371, 2.3311755657196045, 3.197230339050293, 3.700106382369995, 3.0265679359436035, 4.434416770935059, 5.9487152099609375, 3.391099452972412, 2.1037912368774414]], "avg_entropy": [2.8386151276920972], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kentucky", "score_of_biggest_cluster": 0.016954606398940086, "score_of_first_answer": 0.016954606398940086, "max_score_of_first_answer": 0.9961276650428772, "min_score_of_first_answer": 6.864394208605518e-07, "avg_score": 0.016954606398940086, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32318841233802503, "std_of_first_answer": 0.32318841233802503, "avg_entropy_of_first_answer": 2.8386151276920972, "lex_sim": 1.0}
{"question": "where was the last world cup held and who won", "answer": ["2010 world cup \u2013 Australia"], "scores": [[0.0011365876998752356, 0.05522957816720009, 0.9919094443321228, 0.0022987150587141514, 0.007447540760040283]], "normalized_score": [0.01605272851884365], "tokens": [["\u01202010", "\u0120world", "\u0120cup", "\u0120\u00e2\u0122\u0135", "\u0120Australia"]], "entropy": [[2.625779628753662, 2.577253818511963, 0.07475368678569794, 3.112762689590454, 4.095111846923828]], "avg_entropy": [2.497132334113121], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Germany", "score_of_biggest_cluster": 0.01605272851884365, "score_of_first_answer": 0.01605272851884365, "max_score_of_first_answer": 0.9919094443321228, "min_score_of_first_answer": 0.0011365876998752356, "avg_score": 0.01605272851884365, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39066984527891546, "std_of_first_answer": 0.39066984527891546, "avg_entropy_of_first_answer": 2.497132334113121, "lex_sim": 1.0}
{"question": "who played the girl in my two dads", "answer": ["Emma Roberts"], "scores": [[0.029288778081536293, 0.2816404402256012]], "normalized_score": [0.0908234715461731], "tokens": [["\u0120Emma", "\u0120Roberts"]], "entropy": [[6.267382621765137, 1.8329856395721436]], "avg_entropy": [4.05018413066864], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Staci Keanan", "score_of_biggest_cluster": 0.0908234715461731, "score_of_first_answer": 0.0908234715461731, "max_score_of_first_answer": 0.2816404402256012, "min_score_of_first_answer": 0.029288778081536293, "avg_score": 0.0908234715461731, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12617583107203245, "std_of_first_answer": 0.12617583107203245, "avg_entropy_of_first_answer": 4.05018413066864, "lex_sim": 1.0}
{"question": "how long is a prime minister term in uk", "answer": ["5 years"], "scores": [[0.5118287801742554, 0.9148611426353455]], "normalized_score": [0.6842895746231079], "tokens": [["\u01205", "\u0120years"]], "entropy": [[2.5323219299316406, 0.5202732682228088]], "avg_entropy": [1.5262975990772247], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "At Her Majesty's pleasure", "score_of_biggest_cluster": 0.6842895746231079, "score_of_first_answer": 0.6842895746231079, "max_score_of_first_answer": 0.9148611426353455, "min_score_of_first_answer": 0.5118287801742554, "avg_score": 0.6842895746231079, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20151618123054504, "std_of_first_answer": 0.20151618123054504, "avg_entropy_of_first_answer": 1.5262975990772247, "lex_sim": 1.0}
{"question": "when does the good doctor episode 8 air", "answer": ["June 8"], "scores": [[0.023638572543859482, 0.038756027817726135]], "normalized_score": [0.030267762020230293], "tokens": [["\u0120June", "\u01208"]], "entropy": [[4.124510765075684, 3.56844162940979]], "avg_entropy": [3.846476197242737], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "November\u00a020,\u00a02017", "score_of_biggest_cluster": 0.030267762020230293, "score_of_first_answer": 0.030267762020230293, "max_score_of_first_answer": 0.038756027817726135, "min_score_of_first_answer": 0.023638572543859482, "avg_score": 0.030267762020230293, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.007558727636933327, "std_of_first_answer": 0.007558727636933327, "avg_entropy_of_first_answer": 3.846476197242737, "lex_sim": 1.0}
{"question": "when did the song here comes the boom come out", "answer": ["1962"], "scores": [[0.008872204460203648]], "normalized_score": [0.008872203528881073], "tokens": [["\u01201962"]], "entropy": [[5.977243423461914]], "avg_entropy": [5.977243423461914], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "May 2002", "score_of_biggest_cluster": 0.008872203528881073, "score_of_first_answer": 0.008872203528881073, "max_score_of_first_answer": 0.008872204460203648, "min_score_of_first_answer": 0.008872204460203648, "avg_score": 0.008872203528881073, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.977243423461914, "lex_sim": 1.0}
{"question": "when did rachel have her baby on friends", "answer": ["April 2, 2009"], "scores": [[0.04475325345993042, 0.040618933737277985, 0.0714699774980545, 0.04333673045039177]], "normalized_score": [0.04871169850230217], "tokens": [["\u0120April", "\u01202", ",", "\u01202009"]], "entropy": [[5.303971290588379, 3.7836363315582275, 0.8136123418807983, 3.5536694526672363]], "avg_entropy": [3.3637223541736603], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "May\u00a016,\u00a02002", "score_of_biggest_cluster": 0.04871169850230217, "score_of_first_answer": 0.04871169850230217, "max_score_of_first_answer": 0.0714699774980545, "min_score_of_first_answer": 0.040618933737277985, "avg_score": 0.04871169850230217, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.012458770479773793, "std_of_first_answer": 0.012458770479773793, "avg_entropy_of_first_answer": 3.3637223541736603, "lex_sim": 1.0}
{"question": "who plays jack skellington in nightmare before christmas", "answer": ["Mike Myers"], "scores": [[0.003806306514889002, 0.19523116946220398]], "normalized_score": [0.027260035276412964], "tokens": [["\u0120Mike", "\u0120Myers"]], "entropy": [[4.483357906341553, 4.61543083190918]], "avg_entropy": [4.549394369125366], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Danny Elfman", "score_of_biggest_cluster": 0.027260035276412964, "score_of_first_answer": 0.027260035276412964, "max_score_of_first_answer": 0.19523116946220398, "min_score_of_first_answer": 0.003806306514889002, "avg_score": 0.027260035276412964, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09571243147365749, "std_of_first_answer": 0.09571243147365749, "avg_entropy_of_first_answer": 4.549394369125366, "lex_sim": 1.0}
{"question": "when was the south asian association for regional co-operation (saarc) formed", "answer": ["1997"], "scores": [[0.011722688563168049]], "normalized_score": [0.011722690425813198], "tokens": [["\u01201997"]], "entropy": [[4.293369770050049]], "avg_entropy": [4.293369770050049], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "December 1985", "score_of_biggest_cluster": 0.011722690425813198, "score_of_first_answer": 0.011722690425813198, "max_score_of_first_answer": 0.011722688563168049, "min_score_of_first_answer": 0.011722688563168049, "avg_score": 0.011722690425813198, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.293369770050049, "lex_sim": 1.0}
{"question": "who presides over the highest criminal court in a district", "answer": ["district judge"], "scores": [[0.018845602869987488, 0.2978200316429138]], "normalized_score": [0.074917271733284], "tokens": [["\u0120district", "\u0120judge"]], "entropy": [[4.341637134552002, 1.8601714372634888]], "avg_entropy": [3.1009042859077454], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "District Judge", "score_of_biggest_cluster": 0.074917271733284, "score_of_first_answer": 0.074917271733284, "max_score_of_first_answer": 0.2978200316429138, "min_score_of_first_answer": 0.018845602869987488, "avg_score": 0.074917271733284, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.13948721438646317, "std_of_first_answer": 0.13948721438646317, "avg_entropy_of_first_answer": 3.1009042859077454, "lex_sim": 1.0}
{"question": "who sings theme tune to orange is the new black", "answer": ["Celine Dion"], "scores": [[0.0033515661489218473, 0.8392747044563293, 0.9919546246528625]], "normalized_score": [0.14078225195407867], "tokens": [["\u0120C", "eline", "\u0120Dion"]], "entropy": [[6.739128112792969, 1.0657134056091309, 0.07767379283905029]], "avg_entropy": [2.62750510374705], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Spektor", "score_of_biggest_cluster": 0.14078225195407867, "score_of_first_answer": 0.14078225195407867, "max_score_of_first_answer": 0.9919546246528625, "min_score_of_first_answer": 0.0033515661489218473, "avg_score": 0.14078225195407867, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43453866449530526, "std_of_first_answer": 0.43453866449530526, "avg_entropy_of_first_answer": 2.62750510374705, "lex_sim": 1.0}
{"question": "the fertile crescent is located between what two bodies of water", "answer": ["Mediterranean Sea and Persian Gulf"], "scores": [[0.08424585312604904, 0.8188486099243164, 0.8492456078529358, 0.24127496778964996, 0.9355199337005615]], "normalized_score": [0.42098841071128845], "tokens": [["\u0120Mediterranean", "\u0120Sea", "\u0120and", "\u0120Persian", "\u0120Gulf"]], "entropy": [[3.124030113220215, 0.6987917423248291, 0.7601882219314575, 2.3739070892333984, 0.35354146361351013]], "avg_entropy": [1.462091726064682], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tigris and Euphrates rivers", "score_of_biggest_cluster": 0.42098841071128845, "score_of_first_answer": 0.42098841071128845, "max_score_of_first_answer": 0.9355199337005615, "min_score_of_first_answer": 0.08424585312604904, "avg_score": 0.42098841071128845, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3510764393503712, "std_of_first_answer": 0.3510764393503712, "avg_entropy_of_first_answer": 1.462091726064682, "lex_sim": 1.0}
{"question": "when was the first ford f 150 made", "answer": ["1952"], "scores": [[0.013237962499260902]], "normalized_score": [0.013237964361906052], "tokens": [["\u01201952"]], "entropy": [[5.094905853271484]], "avg_entropy": [5.094905853271484], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1975", "score_of_biggest_cluster": 0.013237964361906052, "score_of_first_answer": 0.013237964361906052, "max_score_of_first_answer": 0.013237962499260902, "min_score_of_first_answer": 0.013237962499260902, "avg_score": 0.013237964361906052, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.094905853271484, "lex_sim": 1.0}
{"question": "who introduced the first chrismas tree to the uk", "answer": ["Joseph Henry"], "scores": [[0.0231811311095953, 0.00269983964972198]], "normalized_score": [0.007911087945103645], "tokens": [["\u0120Joseph", "\u0120Henry"]], "entropy": [[5.806850433349609, 5.8181233406066895]], "avg_entropy": [5.812486886978149], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charlotte of Mecklenburg-Strelitz", "score_of_biggest_cluster": 0.007911087945103645, "score_of_first_answer": 0.007911087945103645, "max_score_of_first_answer": 0.0231811311095953, "min_score_of_first_answer": 0.00269983964972198, "avg_score": 0.007911087945103645, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01024064572993666, "std_of_first_answer": 0.01024064572993666, "avg_entropy_of_first_answer": 5.812486886978149, "lex_sim": 1.0}
{"question": "who wrote somewhere there's a place for us", "answer": ["Neil Young"], "scores": [[0.005684459116309881, 0.10490750521421432]], "normalized_score": [0.024420123547315598], "tokens": [["\u0120Neil", "\u0120Young"]], "entropy": [[6.182616710662842, 3.464773654937744]], "avg_entropy": [4.823695182800293], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stephen Sondheim", "score_of_biggest_cluster": 0.024420123547315598, "score_of_first_answer": 0.024420123547315598, "max_score_of_first_answer": 0.10490750521421432, "min_score_of_first_answer": 0.005684459116309881, "avg_score": 0.024420123547315598, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04961152304895222, "std_of_first_answer": 0.04961152304895222, "avg_entropy_of_first_answer": 4.823695182800293, "lex_sim": 1.0}
{"question": "how many games does a team have to win for the world series", "answer": ["7 to reach the series"], "scores": [[0.20943158864974976, 0.0037541084457188845, 0.0052475230768322945, 0.7616458535194397, 0.0889759361743927]], "normalized_score": [0.04889964684844017], "tokens": [["\u01207", "\u0120to", "\u0120reach", "\u0120the", "\u0120series"]], "entropy": [[3.803190231323242, 0.48427659273147583, 2.225998878479004, 1.5368033647537231, 3.2763068675994873]], "avg_entropy": [2.2653151869773867], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "best-of-seven", "score_of_biggest_cluster": 0.04889964684844017, "score_of_first_answer": 0.04889964684844017, "max_score_of_first_answer": 0.7616458535194397, "min_score_of_first_answer": 0.0037541084457188845, "avg_score": 0.04889964684844017, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28402416120130053, "std_of_first_answer": 0.28402416120130053, "avg_entropy_of_first_answer": 2.2653151869773867, "lex_sim": 1.0}
{"question": "what's the name of the last mission impossible movie", "answer": ["Mission Impossible 3"], "scores": [[0.33261561393737793, 0.7563967704772949, 0.13829533755779266]], "normalized_score": [0.3264625072479248], "tokens": [["\u0120Mission", "\u0120Impossible", "\u01203"]], "entropy": [[4.978518486022949, 1.2489240169525146, 2.7996065616607666]], "avg_entropy": [3.0090163548787436], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mission: Impossible \u2013 Fallout", "score_of_biggest_cluster": 0.3264625072479248, "score_of_first_answer": 0.3264625072479248, "max_score_of_first_answer": 0.7563967704772949, "min_score_of_first_answer": 0.13829533755779266, "avg_score": 0.3264625072479248, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2580698053343298, "std_of_first_answer": 0.2580698053343298, "avg_entropy_of_first_answer": 3.0090163548787436, "lex_sim": 1.0}
{"question": "when did the movie peter pan come out", "answer": ["1934"], "scores": [[0.0032265817280858755]], "normalized_score": [0.003226581960916519], "tokens": [["\u01201934"]], "entropy": [[4.650580883026123]], "avg_entropy": [4.650580883026123], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2003", "score_of_biggest_cluster": 0.003226581960916519, "score_of_first_answer": 0.003226581960916519, "max_score_of_first_answer": 0.0032265817280858755, "min_score_of_first_answer": 0.0032265817280858755, "avg_score": 0.003226581960916519, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.650580883026123, "lex_sim": 1.0}
{"question": "how many hoops are used in a game of croquet", "answer": ["nine"], "scores": [[0.009449361823499203]], "normalized_score": [0.009449362754821777], "tokens": [["\u0120nine"]], "entropy": [[3.7390940189361572]], "avg_entropy": [3.7390940189361572], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "six", "score_of_biggest_cluster": 0.009449362754821777, "score_of_first_answer": 0.009449362754821777, "max_score_of_first_answer": 0.009449361823499203, "min_score_of_first_answer": 0.009449361823499203, "avg_score": 0.009449362754821777, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.7390940189361572, "lex_sim": 1.0}
{"question": "when does the next episode of izombie air", "answer": ["September 23"], "scores": [[0.027477649971842766, 0.029179193079471588]], "normalized_score": [0.028315644711256027], "tokens": [["\u0120September", "\u012023"]], "entropy": [[4.197943210601807, 3.5491433143615723]], "avg_entropy": [3.8735432624816895], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "May\u00a07,\u00a02018", "score_of_biggest_cluster": 0.028315644711256027, "score_of_first_answer": 0.028315644711256027, "max_score_of_first_answer": 0.029179193079471588, "min_score_of_first_answer": 0.027477649971842766, "avg_score": 0.028315644711256027, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0008507715538144112, "std_of_first_answer": 0.0008507715538144112, "avg_entropy_of_first_answer": 3.8735432624816895, "lex_sim": 1.0}
{"question": "how many countries are a part of opec", "answer": ["16 member countries"], "scores": [[0.02624627575278282, 0.0015278542414307594, 0.5525888204574585]], "normalized_score": [0.028087761253118515], "tokens": [["\u012016", "\u0120member", "\u0120countries"]], "entropy": [[4.045494079589844, 0.2270629107952118, 1.6714041233062744]], "avg_entropy": [1.9813203712304432], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "14", "score_of_biggest_cluster": 0.028087761253118515, "score_of_first_answer": 0.028087761253118515, "max_score_of_first_answer": 0.5525888204574585, "min_score_of_first_answer": 0.0015278542414307594, "avg_score": 0.028087761253118515, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2541468655052185, "std_of_first_answer": 0.2541468655052185, "avg_entropy_of_first_answer": 1.9813203712304432, "lex_sim": 1.0}
{"question": "when did the movie princess bride come out", "answer": ["July 20, 2008"], "scores": [[0.03093799017369747, 0.031165311112999916, 0.3818075954914093, 0.3043254613876343]], "normalized_score": [0.10288134217262268], "tokens": [["\u0120July", "\u012020", ",", "\u01202008"]], "entropy": [[4.193864345550537, 3.7751402854919434, 1.120039939880371, 2.5498454570770264]], "avg_entropy": [2.9097225069999695], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "September 25, 1987", "score_of_biggest_cluster": 0.10288134217262268, "score_of_first_answer": 0.10288134217262268, "max_score_of_first_answer": 0.3818075954914093, "min_score_of_first_answer": 0.03093799017369747, "avg_score": 0.10288134217262268, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.15839432625271274, "std_of_first_answer": 0.15839432625271274, "avg_entropy_of_first_answer": 2.9097225069999695, "lex_sim": 1.0}
{"question": "who has the power to approve or veto legislation constitution", "answer": ["the president"], "scores": [[0.25525593757629395, 0.6293545961380005]], "normalized_score": [0.40080732107162476], "tokens": [["\u0120the", "\u0120president"]], "entropy": [[3.174031972885132, 1.907146692276001]], "avg_entropy": [2.5405893325805664], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the President", "score_of_biggest_cluster": 0.40080732107162476, "score_of_first_answer": 0.40080732107162476, "max_score_of_first_answer": 0.6293545961380005, "min_score_of_first_answer": 0.25525593757629395, "avg_score": 0.40080732107162476, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.18704932928085327, "std_of_first_answer": 0.18704932928085327, "avg_entropy_of_first_answer": 2.5405893325805664, "lex_sim": 1.0}
{"question": "who stepped out of the dithyrambic chorus to become the first actor", "answer": ["William Shakespeare"], "scores": [[0.04950949177145958, 0.32993367314338684]], "normalized_score": [0.1278078705072403], "tokens": [["\u0120William", "\u0120Shakespeare"]], "entropy": [[6.097072124481201, 4.2941131591796875]], "avg_entropy": [5.195592641830444], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Thespis", "score_of_biggest_cluster": 0.1278078705072403, "score_of_first_answer": 0.1278078705072403, "max_score_of_first_answer": 0.32993367314338684, "min_score_of_first_answer": 0.04950949177145958, "avg_score": 0.1278078705072403, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14021209068596363, "std_of_first_answer": 0.14021209068596363, "avg_entropy_of_first_answer": 5.195592641830444, "lex_sim": 1.0}
{"question": "what type of planet is neptune known as", "answer": ["Planet Neptune"], "scores": [[0.030292905867099762, 0.5051202774047852]], "normalized_score": [0.12369947135448456], "tokens": [["\u0120Planet", "\u0120Neptune"]], "entropy": [[4.694129943847656, 2.9157137870788574]], "avg_entropy": [3.804921865463257], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "giant", "score_of_biggest_cluster": 0.12369947135448456, "score_of_first_answer": 0.12369947135448456, "max_score_of_first_answer": 0.5051202774047852, "min_score_of_first_answer": 0.030292905867099762, "avg_score": 0.12369947135448456, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2374136857688427, "std_of_first_answer": 0.2374136857688427, "avg_entropy_of_first_answer": 3.804921865463257, "lex_sim": 1.0}
{"question": "who is the owner of reading football club", "answer": ["phil"], "scores": [[0.00019995550974272192]], "normalized_score": [0.00019995543698314577], "tokens": [["\u0120phil"]], "entropy": [[6.847467422485352]], "avg_entropy": [6.847467422485352], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Xiu Li Dai", "score_of_biggest_cluster": 0.00019995543698314577, "score_of_first_answer": 0.00019995543698314577, "max_score_of_first_answer": 0.00019995550974272192, "min_score_of_first_answer": 0.00019995550974272192, "avg_score": 0.00019995543698314577, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.847467422485352, "lex_sim": 1.0}
{"question": "who were the original voices of the flintstones", "answer": ["Jim Henson"], "scores": [[0.004429475869983435, 0.046288713812828064, 0.8997560143470764]], "normalized_score": [0.056926872581243515], "tokens": [["\u0120Jim", "\u0120H", "enson"]], "entropy": [[4.846820831298828, 3.6486294269561768, 0.6462516784667969]], "avg_entropy": [3.047233978907267], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Verna Felton", "score_of_biggest_cluster": 0.056926872581243515, "score_of_first_answer": 0.056926872581243515, "max_score_of_first_answer": 0.8997560143470764, "min_score_of_first_answer": 0.004429475869983435, "avg_score": 0.056926872581243515, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41254874989828594, "std_of_first_answer": 0.41254874989828594, "avg_entropy_of_first_answer": 3.047233978907267, "lex_sim": 1.0}
{"question": "where does the president get authority to issue executive orders", "answer": ["from the constitution"], "scores": [[0.05737895518541336, 0.6974916458129883, 0.6389280557632446]], "normalized_score": [0.29461023211479187], "tokens": [["\u0120from", "\u0120the", "\u0120constitution"]], "entropy": [[4.288677215576172, 1.3662452697753906, 2.1380250453948975]], "avg_entropy": [2.597649176915487], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Constitution", "score_of_biggest_cluster": 0.29461023211479187, "score_of_first_answer": 0.29461023211479187, "max_score_of_first_answer": 0.6974916458129883, "min_score_of_first_answer": 0.05737895518541336, "avg_score": 0.29461023211479187, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2889393068073625, "std_of_first_answer": 0.2889393068073625, "avg_entropy_of_first_answer": 2.597649176915487, "lex_sim": 1.0}
{"question": "who is the book of galatians written to", "answer": ["Titus"], "scores": [[0.0026838534977287054]], "normalized_score": [0.0026838532648980618], "tokens": [["\u0120Titus"]], "entropy": [[3.519895553588867]], "avg_entropy": [3.519895553588867], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the churches of Galatia", "score_of_biggest_cluster": 0.0026838532648980618, "score_of_first_answer": 0.0026838532648980618, "max_score_of_first_answer": 0.0026838534977287054, "min_score_of_first_answer": 0.0026838534977287054, "avg_score": 0.0026838532648980618, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.519895553588867, "lex_sim": 1.0}
{"question": "what is the meaning of the word autumn", "answer": ["autumn"], "scores": [[0.08925235271453857]], "normalized_score": [0.08925236016511917], "tokens": [["\u0120autumn"]], "entropy": [[6.070025444030762]], "avg_entropy": [6.070025444030762], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "fall", "score_of_biggest_cluster": 0.08925236016511917, "score_of_first_answer": 0.08925236016511917, "max_score_of_first_answer": 0.08925235271453857, "min_score_of_first_answer": 0.08925235271453857, "avg_score": 0.08925236016511917, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.070025444030762, "lex_sim": 1.0}
{"question": "new york heart association functional classification of angina", "answer": ["Classes I to IV"], "scores": [[0.00209347833879292, 0.46034863591194153, 0.11687147617340088, 0.6717017889022827]], "normalized_score": [0.0932631716132164], "tokens": [["\u0120Classes", "\u0120I", "\u0120to", "\u0120IV"]], "entropy": [[4.465002059936523, 2.2794291973114014, 1.8623387813568115, 0.9093131422996521]], "avg_entropy": [2.379020795226097], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "II", "score_of_biggest_cluster": 0.0932631716132164, "score_of_first_answer": 0.0932631716132164, "max_score_of_first_answer": 0.6717017889022827, "min_score_of_first_answer": 0.00209347833879292, "avg_score": 0.0932631716132164, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2671645628480747, "std_of_first_answer": 0.2671645628480747, "avg_entropy_of_first_answer": 2.379020795226097, "lex_sim": 1.0}
{"question": "which is the fastest train in india and what is its speed", "answer": ["112 km/hr"], "scores": [[0.013480491936206818, 0.1784631609916687, 0.4232674837112427, 0.49671250581741333]], "normalized_score": [0.1499662548303604], "tokens": [["\u0120112", "\u0120km", "/", "hr"]], "entropy": [[5.836485862731934, 2.8813109397888184, 1.3555606603622437, 1.15230131149292]], "avg_entropy": [2.806414693593979], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gatiman", "score_of_biggest_cluster": 0.1499662548303604, "score_of_first_answer": 0.1499662548303604, "max_score_of_first_answer": 0.49671250581741333, "min_score_of_first_answer": 0.013480491936206818, "avg_score": 0.1499662548303604, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19288335402075993, "std_of_first_answer": 0.19288335402075993, "avg_entropy_of_first_answer": 2.806414693593979, "lex_sim": 1.0}
{"question": "what group of animals do octopus belong to", "answer": ["cephalopods"], "scores": [[0.22583316266536713, 0.9967523813247681, 0.9943608641624451, 0.9905597567558289, 0.8065748810768127]], "normalized_score": [0.708743155002594], "tokens": [["\u0120c", "ep", "hal", "op", "ods"]], "entropy": [[3.118212938308716, 0.02748110517859459, 0.057312335819005966, 0.07769445329904556, 0.6761472225189209]], "avg_entropy": [0.7913696110248566], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "class Cephalopoda", "score_of_biggest_cluster": 0.708743155002594, "score_of_first_answer": 0.708743155002594, "max_score_of_first_answer": 0.9967523813247681, "min_score_of_first_answer": 0.22583316266536713, "avg_score": 0.708743155002594, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.29748001511180594, "std_of_first_answer": 0.29748001511180594, "avg_entropy_of_first_answer": 0.7913696110248566, "lex_sim": 1.0}
{"question": "where did the jungle book story come from", "answer": ["Rudyard Kipling."], "scores": [[0.04604234918951988, 0.9957877993583679, 0.9220303297042847, 0.9848222732543945, 0.0015875381650403142]], "normalized_score": [0.14589159190654755], "tokens": [["\u0120Rud", "yard", "\u0120Ki", "pling", "."]], "entropy": [[5.030686378479004, 0.03219541907310486, 0.5214808583259583, 0.10504211485385895, 0.2808119058609009]], "avg_entropy": [1.1940433353185653], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "English author Rudyard Kipling", "score_of_biggest_cluster": 0.14589159190654755, "score_of_first_answer": 0.14589159190654755, "max_score_of_first_answer": 0.9957877993583679, "min_score_of_first_answer": 0.0015875381650403142, "avg_score": 0.14589159190654755, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.46323031893779226, "std_of_first_answer": 0.46323031893779226, "avg_entropy_of_first_answer": 1.1940433353185653, "lex_sim": 1.0}
{"question": "when does congress declare war for the first time", "answer": ["on March 1, 1798"], "scores": [[0.01745499297976494, 0.07762143015861511, 0.07674623280763626, 0.08077213168144226, 0.40206682682037354, 0.5690203905105591]], "normalized_score": [0.1114998534321785], "tokens": [["\u0120on", "\u0120March", "\u01201", ",", "\u012017", "98"]], "entropy": [[4.565877914428711, 3.7015976905822754, 2.1467556953430176, 0.633293628692627, 2.9680087566375732, 1.7975165843963623]], "avg_entropy": [2.635508378346761], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "June 18, 1812", "score_of_biggest_cluster": 0.1114998534321785, "score_of_first_answer": 0.1114998534321785, "max_score_of_first_answer": 0.5690203905105591, "min_score_of_first_answer": 0.01745499297976494, "avg_score": 0.1114998534321785, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2060014352199259, "std_of_first_answer": 0.2060014352199259, "avg_entropy_of_first_answer": 2.635508378346761, "lex_sim": 1.0}
{"question": "what type of plate boundary is associated with iceland and its volcanic eruptions", "answer": ["oceanic"], "scores": [[0.0213067214936018, 0.5607857704162598]], "normalized_score": [0.10930923372507095], "tokens": [["\u0120ocean", "ic"]], "entropy": [[5.732850074768066, 1.8986573219299316]], "avg_entropy": [3.815753698348999], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "divergent tectonic plate boundary", "score_of_biggest_cluster": 0.10930923372507095, "score_of_first_answer": 0.10930923372507095, "max_score_of_first_answer": 0.5607857704162598, "min_score_of_first_answer": 0.0213067214936018, "avg_score": 0.10930923372507095, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.269739524461329, "std_of_first_answer": 0.269739524461329, "avg_entropy_of_first_answer": 3.815753698348999, "lex_sim": 1.0}
{"question": "who played bat masterson in the movie tombstone", "answer": ["Mark-Paul Gosselaar"], "scores": [[0.003789315465837717, 0.0022846763022243977, 0.7225691080093384, 0.889982283115387, 0.9975892305374146, 0.9968307614326477, 0.9993187189102173]], "normalized_score": [0.17741449177265167], "tokens": [["\u0120Mark", "-", "Paul", "\u0120Gos", "sel", "a", "ar"]], "entropy": [[3.2291574478149414, 5.902298927307129, 1.8603153228759766, 0.9723514318466187, 0.023382138460874557, 0.028248541057109833, 0.006439418997615576]], "avg_entropy": [1.717456175480038], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mason Alan Dinehart", "score_of_biggest_cluster": 0.17741449177265167, "score_of_first_answer": 0.17741449177265167, "max_score_of_first_answer": 0.9993187189102173, "min_score_of_first_answer": 0.0022846763022243977, "avg_score": 0.17741449177265167, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42469467472731304, "std_of_first_answer": 0.42469467472731304, "avg_entropy_of_first_answer": 1.717456175480038, "lex_sim": 1.0}
{"question": "what is the corporate tax rate in great britain", "answer": ["a corporation or a person is taxed to pay their income or gains to the state, at a rate equal to the tax rate of the business."], "scores": [[0.002405767561867833, 0.016171691939234734, 0.005269312299787998, 0.2446376085281372, 0.28100845217704773, 0.05068172141909599, 0.2577867805957794, 0.016400691121816635, 0.10321103036403656, 0.04994046688079834, 0.09491872787475586, 0.026187842711806297, 0.03423513099551201, 0.25968247652053833, 0.8645138740539551, 0.07669148594141006, 0.06397343426942825, 0.01663452573120594, 0.5054119229316711, 0.5871658325195312, 0.006697292905300856, 0.9833438992500305, 0.5319061875343323, 0.034426018595695496, 0.5454409718513489, 0.13523712754249573, 0.5109615325927734, 0.014092160388827324, 0.16990292072296143, 0.016396090388298035, 0.514570951461792, 0.014802379533648491, 0.022896965965628624, 0.36324363946914673, 0.5622692704200745, 0.01221294142305851, 0.4811820387840271, 0.5744745135307312, 0.07812449336051941, 0.5563560128211975, 0.16303251683712006, 0.4190022349357605, 0.8743507862091064, 0.6680055856704712, 0.04653719812631607, 0.0950273647904396, 0.8720691800117493, 0.3253159821033478, 0.9759057760238647, 0.6742421984672546]], "normalized_score": [0.12215105444192886], "tokens": [["\u0120a", "\u0120corporation", "\u0120or", "\u0120a", "\u0120person", "\u0120is", "\u0120taxed", "\u0120to", "\u0120pay", "\u0120their", "\u0120income", "\u0120or", "\u0120gains", "\u0120to", "\u0120the", "\u0120state", ",", "\u0120at", "\u0120a", "\u0120rate", "\u0120equal", "\u0120to", "\u0120the", "\u0120tax", "\u0120rate", "\u0120of", "\u0120the", "\u0120business", ".", "\u0120a", "\u0120corporation", "\u0120with", "\u0120an", "\u0120income", "\u0120of", "\u01201", "\u0120million", "\u0120pounds", "\u0120would", "\u0120pay", "\u0120a", "\u0120corporation", "\u0120tax", "\u0120of", "\u01201", ",", "000", ",", "000", "\u0120pounds"]], "entropy": [[3.208981513977051, 5.730483531951904, 2.9389727115631104, 3.863276958465576, 3.758727788925171, 2.4521002769470215, 3.145655632019043, 2.902599334716797, 3.5085296630859375, 3.480362892150879, 3.9476120471954346, 2.4403843879699707, 3.335561752319336, 2.8573265075683594, 0.8337050080299377, 2.144012451171875, 2.459620952606201, 4.684685707092285, 2.3925790786743164, 2.3240654468536377, 3.2158737182617188, 0.14508390426635742, 2.3339171409606934, 4.003260135650635, 2.442577362060547, 3.4043543338775635, 2.409942388534546, 3.980466842651367, 2.993436336517334, 2.883558511734009, 2.396663188934326, 2.885136604309082, 2.6942365169525146, 3.0443222522735596, 2.2875468730926514, 2.8105568885803223, 2.3024039268493652, 2.2230775356292725, 3.184096574783325, 1.538238763809204, 3.565795421600342, 2.254347324371338, 0.6493716239929199, 1.0662975311279297, 3.923213243484497, 2.2152652740478516, 1.0802357196807861, 1.2443184852600098, 0.21310964226722717, 1.812122106552124]], "avg_entropy": [2.672241396307945], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "20%", "score_of_biggest_cluster": 0.12215105444192886, "score_of_first_answer": 0.12215105444192886, "max_score_of_first_answer": 0.9833438992500305, "min_score_of_first_answer": 0.002405767561867833, "avg_score": 0.12215105444192886, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29742858286277624, "std_of_first_answer": 0.29742858286277624, "avg_entropy_of_first_answer": 2.672241396307945, "lex_sim": 1.0}
{"question": "who was the first president of the constituent assembly", "answer": ["Gouverneur Morris"], "scores": [[0.01572680100798607, 0.43298768997192383, 0.9908949136734009, 0.9965409636497498, 0.983442485332489]], "normalized_score": [0.366502970457077], "tokens": [["\u0120G", "ouver", "ne", "ur", "\u0120Morris"]], "entropy": [[5.484936714172363, 3.2651407718658447, 0.09265902638435364, 0.037740278989076614, 0.1775462031364441]], "avg_entropy": [1.8116045989096166], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dr. Rajendra Prasad", "score_of_biggest_cluster": 0.366502970457077, "score_of_first_answer": 0.366502970457077, "max_score_of_first_answer": 0.9965409636497498, "min_score_of_first_answer": 0.01572680100798607, "avg_score": 0.366502970457077, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.397775906546094, "std_of_first_answer": 0.397775906546094, "avg_entropy_of_first_answer": 1.8116045989096166, "lex_sim": 1.0}
{"question": "who was originally cast as phoebe on charmed", "answer": ["Annet Basten"], "scores": [[0.12720219790935516, 0.018209164962172508, 0.00016096439503598958, 0.09851115196943283]], "normalized_score": [0.013843629509210587], "tokens": [["\u0120An", "net", "\u0120Bast", "en"]], "entropy": [[5.096122741699219, 2.644428014755249, 1.1010006666183472, 2.6061246395111084]], "avg_entropy": [2.861919015645981], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lori Rom", "score_of_biggest_cluster": 0.013843629509210587, "score_of_first_answer": 0.013843629509210587, "max_score_of_first_answer": 0.12720219790935516, "min_score_of_first_answer": 0.00016096439503598958, "avg_score": 0.013843629509210587, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.053203053494334906, "std_of_first_answer": 0.053203053494334906, "avg_entropy_of_first_answer": 2.861919015645981, "lex_sim": 1.0}
{"question": "what features of muscle contraction can be determined from an emg (electromyogram)", "answer": ["EMG"], "scores": [[0.007746520917862654, 0.9714870452880859]], "normalized_score": [0.08675047755241394], "tokens": [["\u0120EM", "G"]], "entropy": [[6.646071434020996, 0.17866650223731995]], "avg_entropy": [3.412368968129158], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "electric potential generated", "score_of_biggest_cluster": 0.08675047755241394, "score_of_first_answer": 0.08675047755241394, "max_score_of_first_answer": 0.9714870452880859, "min_score_of_first_answer": 0.007746520917862654, "avg_score": 0.08675047755241394, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.48187026218511164, "std_of_first_answer": 0.48187026218511164, "avg_entropy_of_first_answer": 3.412368968129158, "lex_sim": 1.0}
{"question": "pace maker is associated with which body organ", "answer": ["Atrioventricular node"], "scores": [[0.004780398681759834, 0.06921850144863129, 0.9663978815078735, 0.9993231296539307, 0.9975097179412842, 0.7073257565498352]], "normalized_score": [0.24670659005641937], "tokens": [["\u0120At", "ri", "ov", "entric", "ular", "\u0120node"]], "entropy": [[2.835482597351074, 1.4065155982971191, 0.26435786485671997, 0.006189859006553888, 0.023578904569149017, 1.463718295097351]], "avg_entropy": [0.9999738531963279], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "heart", "score_of_biggest_cluster": 0.24670659005641937, "score_of_first_answer": 0.24670659005641937, "max_score_of_first_answer": 0.9993231296539307, "min_score_of_first_answer": 0.004780398681759834, "avg_score": 0.24670659005641937, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.42735071982276435, "std_of_first_answer": 0.42735071982276435, "avg_entropy_of_first_answer": 0.9999738531963279, "lex_sim": 1.0}
{"question": "what role does the president play in foreign affairs", "answer": ["Commander in Chief"], "scores": [[0.18379655480384827, 0.7684741020202637, 0.9132986664772034]], "normalized_score": [0.5052734613418579], "tokens": [["\u0120Commander", "\u0120in", "\u0120Chief"]], "entropy": [[4.96751070022583, 0.7320618033409119, 0.35588210821151733]], "avg_entropy": [2.018484870592753], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "negotiates treaties with foreign nations", "score_of_biggest_cluster": 0.5052734613418579, "score_of_first_answer": 0.5052734613418579, "max_score_of_first_answer": 0.9132986664772034, "min_score_of_first_answer": 0.18379655480384827, "avg_score": 0.5052734613418579, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3153473075876519, "std_of_first_answer": 0.3153473075876519, "avg_entropy_of_first_answer": 2.018484870592753, "lex_sim": 1.0}
{"question": "who has the most votes for the all star game", "answer": ["Miley cyrus has 15.824 votes"], "scores": [[0.01207718439400196, 0.01710747368633747, 0.0678449422121048, 0.9553605318069458, 0.0025042174383997917, 0.004382065963000059, 0.053918223828077316, 0.044941890984773636, 0.0011612636735662818, 0.3139737546443939]], "normalized_score": [0.02578337863087654], "tokens": [["\u0120M", "iley", "\u0120c", "yrus", "\u0120has", "\u012015", ".", "8", "24", "\u0120votes"]], "entropy": [[5.295633316040039, 1.6790962219238281, 0.7684409618377686, 0.3096262812614441, 0.3212532699108124, 4.17508602142334, 3.1610591411590576, 4.986982822418213, 1.7789112329483032, 3.8038315773010254]], "avg_entropy": [2.627992084622383], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "fan voting", "score_of_biggest_cluster": 0.02578337863087654, "score_of_first_answer": 0.02578337863087654, "max_score_of_first_answer": 0.9553605318069458, "min_score_of_first_answer": 0.0011612636735662818, "avg_score": 0.02578337863087654, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2835904775425749, "std_of_first_answer": 0.2835904775425749, "avg_entropy_of_first_answer": 2.627992084622383, "lex_sim": 1.0}
{"question": "who has the oldest team in the nba", "answer": ["the boston Celtics"], "scores": [[0.1686045378446579, 0.0074282013811171055, 0.7821671962738037, 0.4235689342021942]], "normalized_score": [0.14272308349609375], "tokens": [["\u0120the", "\u0120b", "oston", "\u0120Celtics"]], "entropy": [[4.04251766204834, 5.039834976196289, 1.3788549900054932, 3.405681848526001]], "avg_entropy": [3.4667223691940308], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sacramento Kings", "score_of_biggest_cluster": 0.14272308349609375, "score_of_first_answer": 0.14272308349609375, "max_score_of_first_answer": 0.7821671962738037, "min_score_of_first_answer": 0.0074282013811171055, "avg_score": 0.14272308349609375, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2925566316913471, "std_of_first_answer": 0.2925566316913471, "avg_entropy_of_first_answer": 3.4667223691940308, "lex_sim": 1.0}
{"question": "what is a home run with a girl", "answer": ["home run with a girl"], "scores": [[0.011859019286930561, 0.9082403779029846, 0.2726227641105652, 0.831886351108551, 0.5102956295013428]], "normalized_score": [0.2625061571598053], "tokens": [["\u0120home", "\u0120run", "\u0120with", "\u0120a", "\u0120girl"]], "entropy": [[5.997604846954346, 0.7651568651199341, 1.5966041088104248, 1.3075014352798462, 3.346951484680176]], "avg_entropy": [2.6027637481689454], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\"full\" sexual intercourse", "score_of_biggest_cluster": 0.2625061571598053, "score_of_first_answer": 0.2625061571598053, "max_score_of_first_answer": 0.9082403779029846, "min_score_of_first_answer": 0.011859019286930561, "avg_score": 0.2625061571598053, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3366461408711188, "std_of_first_answer": 0.3366461408711188, "avg_entropy_of_first_answer": 2.6027637481689454, "lex_sim": 1.0}
{"question": "who carried florida by 537 votes in 2000", "answer": ["GW Bush."], "scores": [[0.0015375345246866345, 0.8743546009063721, 0.0018032463267445564]], "normalized_score": [0.01343349926173687], "tokens": [["\u0120GW", "\u0120Bush", "."]], "entropy": [[2.2471446990966797, 0.7184457778930664, 0.3251734673976898]], "avg_entropy": [1.096921314795812], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George W. Bush", "score_of_biggest_cluster": 0.01343349926173687, "score_of_first_answer": 0.01343349926173687, "max_score_of_first_answer": 0.8743546009063721, "min_score_of_first_answer": 0.0015375345246866345, "avg_score": 0.01343349926173687, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4113872963452966, "std_of_first_answer": 0.4113872963452966, "avg_entropy_of_first_answer": 1.096921314795812, "lex_sim": 1.0}
{"question": "where is the majority of phosphorous stored in the environment", "answer": ["sea floor"], "scores": [[0.009920896962285042, 0.05656421557068825]], "normalized_score": [0.02368897758424282], "tokens": [["\u0120sea", "\u0120floor"]], "entropy": [[4.721136093139648, 1.844376564025879]], "avg_entropy": [3.2827563285827637], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "minerals", "score_of_biggest_cluster": 0.02368897758424282, "score_of_first_answer": 0.02368897758424282, "max_score_of_first_answer": 0.05656421557068825, "min_score_of_first_answer": 0.009920896962285042, "avg_score": 0.02368897758424282, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.023321659304201603, "std_of_first_answer": 0.023321659304201603, "avg_entropy_of_first_answer": 3.2827563285827637, "lex_sim": 1.0}
{"question": "who sings oh what a night late december back in 63", "answer": ["Chris whitley and jimi hendrix"], "scores": [[0.00563920009881258, 0.0009648881386965513, 0.8403002619743347, 0.007386704441159964, 0.018355267122387886, 0.037887800484895706, 0.6129406094551086, 0.9825538992881775, 0.9687929749488831]], "normalized_score": [0.06208445131778717], "tokens": [["\u0120Chris", "\u0120whit", "ley", "\u0120and", "\u0120j", "imi", "\u0120he", "nd", "rix"]], "entropy": [[6.246003150939941, 5.338717937469482, 0.8691615462303162, 0.39131176471710205, 6.154508590698242, 3.387683868408203, 2.6989080905914307, 0.1696484237909317, 0.20977121591567993]], "avg_entropy": [2.8295238431957035], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Four Seasons", "score_of_biggest_cluster": 0.06208445131778717, "score_of_first_answer": 0.06208445131778717, "max_score_of_first_answer": 0.9825538992881775, "min_score_of_first_answer": 0.0009648881386965513, "avg_score": 0.06208445131778717, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42765952913721805, "std_of_first_answer": 0.42765952913721805, "avg_entropy_of_first_answer": 2.8295238431957035, "lex_sim": 1.0}
{"question": "in the texas legislature the house has members and the senate has", "answer": ["the house has 72 members, and the senate has 35 members"], "scores": [[0.02254096418619156, 0.13674727082252502, 0.516528308391571, 0.00573939410969615, 0.7380465865135193, 0.08105508238077164, 0.13930609822273254, 0.9482564926147461, 0.919238269329071, 0.9413834810256958, 0.1063791885972023, 0.2515251636505127]], "normalized_score": [0.18561096489429474], "tokens": [["\u0120the", "\u0120house", "\u0120has", "\u012072", "\u0120members", ",", "\u0120and", "\u0120the", "\u0120senate", "\u0120has", "\u012035", "\u0120members"]], "entropy": [[4.610263824462891, 4.644218444824219, 2.225360155105591, 4.489615440368652, 1.2836670875549316, 1.332244873046875, 1.6347813606262207, 0.45584720373153687, 0.5995004177093506, 0.4721788167953491, 3.4261817932128906, 1.1285597085952759]], "avg_entropy": [2.1918682605028152], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "member", "score_of_biggest_cluster": 0.18561096489429474, "score_of_first_answer": 0.18561096489429474, "max_score_of_first_answer": 0.9482564926147461, "min_score_of_first_answer": 0.00573939410969615, "avg_score": 0.18561096489429474, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.369267750810625, "std_of_first_answer": 0.369267750810625, "avg_entropy_of_first_answer": 2.1918682605028152, "lex_sim": 1.0}
{"question": "love is not love that alters when it alteration finds meaning", "answer": ["in what condition is one's skin while getting a tattoo removed"], "scores": [[0.0013301291037350893, 0.026427755132317543, 0.0028973461594432592, 0.5336223840713501, 0.0029180976562201977, 0.16080500185489655, 0.0059388307854533195, 0.005343674216419458, 0.0034790809731930494, 0.6681457757949829, 0.5465582609176636, 0.011314941570162773]], "normalized_score": [0.021817883476614952], "tokens": [["\u0120in", "\u0120what", "\u0120condition", "\u0120is", "\u0120one", "'s", "\u0120skin", "\u0120while", "\u0120getting", "\u0120a", "\u0120tattoo", "\u0120removed"]], "entropy": [[5.963382720947266, 5.2188310623168945, 4.350630760192871, 2.0509042739868164, 3.7464025020599365, 4.59264612197876, 5.141246318817139, 3.411375045776367, 5.020268440246582, 2.378535032272339, 2.7258942127227783, 0.8093688488006592]], "avg_entropy": [3.7841237783432007], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sonnet 116", "score_of_biggest_cluster": 0.021817883476614952, "score_of_first_answer": 0.021817883476614952, "max_score_of_first_answer": 0.6681457757949829, "min_score_of_first_answer": 0.0013301291037350893, "avg_score": 0.021817883476614952, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24726311212817959, "std_of_first_answer": 0.24726311212817959, "avg_entropy_of_first_answer": 3.7841237783432007, "lex_sim": 1.0}
{"question": "which mirror is used in vehicles for rear view", "answer": ["Mirror"], "scores": [[0.004054403398185968]], "normalized_score": [0.004054403398185968], "tokens": [["\u0120Mirror"]], "entropy": [[4.510863304138184]], "avg_entropy": [4.510863304138184], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "rear-view mirror", "score_of_biggest_cluster": 0.004054403398185968, "score_of_first_answer": 0.004054403398185968, "max_score_of_first_answer": 0.004054403398185968, "min_score_of_first_answer": 0.004054403398185968, "avg_score": 0.004054403398185968, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.510863304138184, "lex_sim": 1.0}
{"question": "where does the show the path take place", "answer": ["west sussex"], "scores": [[0.001763340551406145, 0.003875122405588627, 0.9884490370750427]], "normalized_score": [0.018902767449617386], "tokens": [["\u0120west", "\u0120sus", "sex"]], "entropy": [[6.887618064880371, 3.934952735900879, 0.10964524745941162]], "avg_entropy": [3.6440720160802207], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Upstate New York", "score_of_biggest_cluster": 0.018902767449617386, "score_of_first_answer": 0.018902767449617386, "max_score_of_first_answer": 0.9884490370750427, "min_score_of_first_answer": 0.001763340551406145, "avg_score": 0.018902767449617386, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4646311460352737, "std_of_first_answer": 0.4646311460352737, "avg_entropy_of_first_answer": 3.6440720160802207, "lex_sim": 1.0}
{"question": "when was the minimum wage established in the united states", "answer": ["August 25, 2009 - the federal minimum wage is now $7.25"], "scores": [[0.02448314055800438, 0.05608183518052101, 0.6014084815979004, 0.0863242819905281, 0.0028556676115840673, 0.04126182571053505, 0.03707163408398628, 0.9318438172340393, 0.9711942672729492, 0.4976619780063629, 0.030522236600518227, 0.8730016350746155, 0.981221616268158, 0.9998055100440979, 0.9961338043212891]], "normalized_score": [0.1727827489376068], "tokens": [["\u0120August", "\u012025", ",", "\u01202009", "\u0120-", "\u0120the", "\u0120federal", "\u0120minimum", "\u0120wage", "\u0120is", "\u0120now", "\u0120$", "7", ".", "25"]], "entropy": [[4.219484806060791, 3.0071005821228027, 0.9562708139419556, 1.913987159729004, 0.7248145937919617, 4.920070648193359, 4.342020034790039, 0.4953247606754303, 0.16351830959320068, 2.1128430366516113, 1.577820897102356, 0.8580813407897949, 0.12422871589660645, 0.0024766381829977036, 0.034824129194021225]], "avg_entropy": [1.6968577644477287], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1938", "score_of_biggest_cluster": 0.1727827489376068, "score_of_first_answer": 0.1727827489376068, "max_score_of_first_answer": 0.9998055100440979, "min_score_of_first_answer": 0.0028556676115840673, "avg_score": 0.1727827489376068, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42918448095451994, "std_of_first_answer": 0.42918448095451994, "avg_entropy_of_first_answer": 1.6968577644477287, "lex_sim": 1.0}
{"question": "the length and direction of a straight line between two locations or positions", "answer": ["a line"], "scores": [[0.03892366588115692, 0.11334758251905441]], "normalized_score": [0.06642216444015503], "tokens": [["\u0120a", "\u0120line"]], "entropy": [[6.240340232849121, 3.425187587738037]], "avg_entropy": [4.832763910293579], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "displacement", "score_of_biggest_cluster": 0.06642216444015503, "score_of_first_answer": 0.06642216444015503, "max_score_of_first_answer": 0.11334758251905441, "min_score_of_first_answer": 0.03892366588115692, "avg_score": 0.06642216444015503, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.037211958318948746, "std_of_first_answer": 0.037211958318948746, "avg_entropy_of_first_answer": 4.832763910293579, "lex_sim": 1.0}
{"question": "what is the ethnic background of the shib sibs", "answer": ["a mixture of Chinese, English, Irish and Scottish"], "scores": [[0.0014908694429323077, 0.0854383111000061, 0.9265252947807312, 0.08453424274921417, 0.6044920086860657, 0.061456236988306046, 0.7515127062797546, 0.10378200560808182, 0.19847522675991058, 0.2762216627597809]], "normalized_score": [0.13182450830936432], "tokens": [["\u0120a", "\u0120mixture", "\u0120of", "\u0120Chinese", ",", "\u0120English", ",", "\u0120Irish", "\u0120and", "\u0120Scottish"]], "entropy": [[5.497554302215576, 5.564701080322266, 0.4172152578830719, 5.634992599487305, 1.1277046203613281, 3.9084646701812744, 0.763799786567688, 3.1793692111968994, 0.9142184257507324, 3.4398629665374756]], "avg_entropy": [3.0447882920503617], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Japanese", "score_of_biggest_cluster": 0.13182450830936432, "score_of_first_answer": 0.13182450830936432, "max_score_of_first_answer": 0.9265252947807312, "min_score_of_first_answer": 0.0014908694429323077, "avg_score": 0.13182450830936432, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3124224741861823, "std_of_first_answer": 0.3124224741861823, "avg_entropy_of_first_answer": 3.0447882920503617, "lex_sim": 1.0}
{"question": "who wrote it's a long long way to pasadena", "answer": ["Frank Pierson and Sylvia Plath"], "scores": [[0.15068291127681732, 0.0010294863022863865, 0.910345196723938, 0.03409857302904129, 0.0015631478745490313, 0.25651195645332336, 0.9263168573379517]], "normalized_score": [0.056280288845300674], "tokens": [["\u0120Frank", "\u0120Pi", "erson", "\u0120and", "\u0120Sylvia", "\u0120Pl", "ath"]], "entropy": [[5.742598533630371, 3.8248825073242188, 0.5787487030029297, 0.42486196756362915, 4.1592817306518555, 3.548292636871338, 0.418800950050354]], "avg_entropy": [2.6710667184420993], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Young", "score_of_biggest_cluster": 0.056280288845300674, "score_of_first_answer": 0.056280288845300674, "max_score_of_first_answer": 0.9263168573379517, "min_score_of_first_answer": 0.0010294863022863865, "avg_score": 0.056280288845300674, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38423868245396386, "std_of_first_answer": 0.38423868245396386, "avg_entropy_of_first_answer": 2.6710667184420993, "lex_sim": 1.0}
{"question": "three act puccini opera first performed in 1900", "answer": ["Tosca"], "scores": [[0.3206537365913391, 0.9991862177848816]], "normalized_score": [0.5660325288772583], "tokens": [["\u0120Tos", "ca"]], "entropy": [[3.2479100227355957, 0.009762600064277649]], "avg_entropy": [1.6288363113999367], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tosca", "score_of_biggest_cluster": 0.5660325288772583, "score_of_first_answer": 0.5660325288772583, "max_score_of_first_answer": 0.9991862177848816, "min_score_of_first_answer": 0.3206537365913391, "avg_score": 0.5660325288772583, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.33926624059677124, "std_of_first_answer": 0.33926624059677124, "avg_entropy_of_first_answer": 1.6288363113999367, "lex_sim": 1.0}
{"question": "when did the prr first begin its passenger service between pittsburgh and philadelphia", "answer": ["December 14, 1971"], "scores": [[0.018951693549752235, 0.02394881844520569, 0.4744397699832916, 0.025518322363495827]], "normalized_score": [0.04841626062989235], "tokens": [["\u0120December", "\u012014", ",", "\u01201971"]], "entropy": [[5.33383321762085, 3.9849748611450195, 1.0680465698242188, 4.805548667907715]], "avg_entropy": [3.7981008291244507], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "by 1854", "score_of_biggest_cluster": 0.04841626062989235, "score_of_first_answer": 0.04841626062989235, "max_score_of_first_answer": 0.4744397699832916, "min_score_of_first_answer": 0.018951693549752235, "avg_score": 0.04841626062989235, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19557807049513767, "std_of_first_answer": 0.19557807049513767, "avg_entropy_of_first_answer": 3.7981008291244507, "lex_sim": 1.0}
{"question": "where was the italian episode of everybody loves raymond filmed", "answer": ["France, Italy, India, USA"], "scores": [[0.00495165167376399, 0.009450076147913933, 0.11926688998937607, 0.3342808485031128, 0.0019199727103114128, 0.41606900095939636, 0.030943403020501137]], "normalized_score": [0.033373862504959106], "tokens": [["\u0120France", ",", "\u0120Italy", ",", "\u0120India", ",", "\u0120USA"]], "entropy": [[6.001771926879883, 0.2611466646194458, 5.754014492034912, 1.1492424011230469, 4.152646541595459, 1.0912892818450928, 4.354343891143799]], "avg_entropy": [3.252065028463091], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Anguillara Sabazia outside of Rome", "score_of_biggest_cluster": 0.033373862504959106, "score_of_first_answer": 0.033373862504959106, "max_score_of_first_answer": 0.41606900095939636, "min_score_of_first_answer": 0.0019199727103114128, "avg_score": 0.033373862504959106, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.160383757950673, "std_of_first_answer": 0.160383757950673, "avg_entropy_of_first_answer": 3.252065028463091, "lex_sim": 1.0}
{"question": "suffix applied to the end of the name of enzymes", "answer": ["the"], "scores": [[0.004967911634594202]], "normalized_score": [0.004967911634594202], "tokens": [["\u0120the"]], "entropy": [[4.2820258140563965]], "avg_entropy": [4.2820258140563965], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "-ase", "score_of_biggest_cluster": 0.004967911634594202, "score_of_first_answer": 0.004967911634594202, "max_score_of_first_answer": 0.004967911634594202, "min_score_of_first_answer": 0.004967911634594202, "avg_score": 0.004967911634594202, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.2820258140563965, "lex_sim": 1.0}
{"question": "by 1914 which of the following african countries were still independent", "answer": ["africa"], "scores": [[0.0007759081781841815, 0.6013144850730896]], "normalized_score": [0.021600112318992615], "tokens": [["\u0120af", "rica"]], "entropy": [[4.293168544769287, 1.6592388153076172]], "avg_entropy": [2.976203680038452], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Liberia", "score_of_biggest_cluster": 0.021600112318992615, "score_of_first_answer": 0.021600112318992615, "max_score_of_first_answer": 0.6013144850730896, "min_score_of_first_answer": 0.0007759081781841815, "avg_score": 0.021600112318992615, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3002692884474527, "std_of_first_answer": 0.3002692884474527, "avg_entropy_of_first_answer": 2.976203680038452, "lex_sim": 1.0}
{"question": "who played major hochstetter in hogan's heroes", "answer": ["Gene Hackman"], "scores": [[0.00553283141925931, 0.8635411262512207, 0.9951516389846802]], "normalized_score": [0.1681537628173828], "tokens": [["\u0120Gene", "\u0120Hack", "man"]], "entropy": [[5.965605735778809, 0.7330785393714905, 0.03808193653821945]], "avg_entropy": [2.245588737229506], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Howard Caine", "score_of_biggest_cluster": 0.1681537628173828, "score_of_first_answer": 0.1681537628173828, "max_score_of_first_answer": 0.9951516389846802, "min_score_of_first_answer": 0.00553283141925931, "avg_score": 0.1681537628173828, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4387918951633779, "std_of_first_answer": 0.4387918951633779, "avg_entropy_of_first_answer": 2.245588737229506, "lex_sim": 1.0}
{"question": "where did the river thames start and end", "answer": ["From its source near the town of thrimpsley, it flows south through thames meadow in the county of lancashire before entering london harbor."], "scores": [[0.004490111488848925, 0.028513850644230843, 0.9246569275856018, 0.05126115307211876, 0.10195614397525787, 0.07786186039447784, 0.9679533243179321, 0.0011119851842522621, 0.001920100417919457, 0.0714397206902504, 0.18248142302036285, 0.2719082832336426, 0.2395104467868805, 0.5812993049621582, 0.0709955096244812, 0.23571930825710297, 0.014193383045494556, 0.41697636246681213, 0.2450428605079651, 0.3631606996059418, 0.023016972467303276, 0.21198201179504395, 0.4553196132183075, 0.9475400447845459, 0.06176989898085594, 0.3481190502643585, 0.8477767109870911, 0.9983885288238525, 0.044673383235931396, 0.16972492635250092, 0.007881023921072483, 0.9808903932571411, 0.0036011175252497196, 0.21689361333847046, 0.09041725099086761, 0.1353103220462799, 0.5904673337936401, 0.004772464279085398, 0.033614303916692734, 0.07176149636507034, 0.5157672166824341, 0.36723753809928894]], "normalized_score": [0.10608769208192825], "tokens": [["\u0120From", "\u0120its", "\u0120source", "\u0120near", "\u0120the", "\u0120town", "\u0120of", "\u0120th", "rim", "ps", "ley", ",", "\u0120it", "\u0120flows", "\u0120south", "\u0120through", "\u0120th", "ames", "\u0120me", "adow", "\u0120in", "\u0120the", "\u0120county", "\u0120of", "\u0120l", "anc", "ash", "ire", "\u0120before", "\u0120entering", "\u0120l", "ondon", "\u0120harbor", ".", "\u0120it", "\u0120then", "\u0120flows", "\u0120northwest", "ward", "\u0120to", "\u0120the", "\u0120sea"]], "entropy": [[5.302535057067871, 3.898622512817383, 0.554959774017334, 1.9761288166046143, 5.301782131195068, 4.238100528717041, 0.3575282692909241, 5.321877479553223, 1.7910597324371338, 2.815089225769043, 3.2694573402404785, 2.2674660682678223, 3.621683120727539, 1.9812228679656982, 2.795397996902466, 2.9395575523376465, 4.547121047973633, 2.870363473892212, 3.970205783843994, 1.18929123878479, 1.9137089252471924, 4.6039018630981445, 3.4153363704681396, 0.46560412645339966, 5.306984901428223, 1.4617191553115845, 0.5572965741157532, 0.015167204663157463, 1.9303641319274902, 3.2078447341918945, 1.0708369016647339, 0.14412783086299896, 3.0113916397094727, 2.1999599933624268, 2.259854793548584, 3.2661187648773193, 2.20544695854187, 2.8143258094787598, 2.4910902976989746, 2.108664035797119, 2.814948558807373, 2.7810373306274414]], "avg_entropy": [2.6441709742925705], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Thames Estuary, North Sea", "score_of_biggest_cluster": 0.10608769208192825, "score_of_first_answer": 0.10608769208192825, "max_score_of_first_answer": 0.9983885288238525, "min_score_of_first_answer": 0.0011119851842522621, "avg_score": 0.10608769208192825, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3140659959539053, "std_of_first_answer": 0.3140659959539053, "avg_entropy_of_first_answer": 2.6441709742925705, "lex_sim": 1.0}
{"question": "who does the voice of the beast in the new movie", "answer": ["Daniel Radcliffe of Harry Potter fame"], "scores": [[0.04252306744456291, 0.33496519923210144, 0.9821954369544983, 3.4228880394948646e-05, 0.6133016347885132, 0.9845433831214905, 0.6147921681404114]], "normalized_score": [0.10856582224369049], "tokens": [["\u0120Daniel", "\u0120Rad", "cliffe", "\u0120of", "\u0120Harry", "\u0120Potter", "\u0120fame"]], "entropy": [[2.8794989585876465, 3.179932117462158, 0.1001574844121933, 0.18973678350448608, 2.0333847999572754, 0.11150233447551727, 1.2366183996200562]], "avg_entropy": [1.3901186968599046], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dan Stevens", "score_of_biggest_cluster": 0.10856582224369049, "score_of_first_answer": 0.10856582224369049, "max_score_of_first_answer": 0.9845433831214905, "min_score_of_first_answer": 3.4228880394948646e-05, "avg_score": 0.10856582224369049, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37398409796109855, "std_of_first_answer": 0.37398409796109855, "avg_entropy_of_first_answer": 1.3901186968599046, "lex_sim": 1.0}
{"question": "who are the co hosts on the real", "answer": ["Drew and Carson"], "scores": [[0.007807828485965729, 0.504566490650177, 0.001300595817156136]], "normalized_score": [0.017239728942513466], "tokens": [["\u0120Drew", "\u0120and", "\u0120Carson"]], "entropy": [[6.22991943359375, 2.807072162628174, 4.359882831573486]], "avg_entropy": [4.465624809265137], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tamar Braxton", "score_of_biggest_cluster": 0.017239728942513466, "score_of_first_answer": 0.017239728942513466, "max_score_of_first_answer": 0.504566490650177, "min_score_of_first_answer": 0.001300595817156136, "avg_score": 0.017239728942513466, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23572301858425662, "std_of_first_answer": 0.23572301858425662, "avg_entropy_of_first_answer": 4.465624809265137, "lex_sim": 1.0}
{"question": "the world's tallest jain statue is located in which state of india", "answer": ["Kerala"], "scores": [[0.04612371698021889]], "normalized_score": [0.046123720705509186], "tokens": [["\u0120Kerala"]], "entropy": [[3.7724790573120117]], "avg_entropy": [3.7724790573120117], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Maharashtra", "score_of_biggest_cluster": 0.046123720705509186, "score_of_first_answer": 0.046123720705509186, "max_score_of_first_answer": 0.04612371698021889, "min_score_of_first_answer": 0.04612371698021889, "avg_score": 0.046123720705509186, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.7724790573120117, "lex_sim": 1.0}
{"question": "who played cosette in les miserables on broadway", "answer": ["Celine"], "scores": [[0.007859586738049984, 0.3258386552333832]], "normalized_score": [0.050605908036231995], "tokens": [["\u0120C", "eline"]], "entropy": [[5.329678535461426, 2.4770824909210205]], "avg_entropy": [3.903380513191223], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Samantha Hill", "score_of_biggest_cluster": 0.050605908036231995, "score_of_first_answer": 0.050605908036231995, "max_score_of_first_answer": 0.3258386552333832, "min_score_of_first_answer": 0.007859586738049984, "avg_score": 0.050605908036231995, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1589895342476666, "std_of_first_answer": 0.1589895342476666, "avg_entropy_of_first_answer": 3.903380513191223, "lex_sim": 1.0}
{"question": "who plays addison shepherd on grey's anatomy", "answer": ["Jennifer Morrison"], "scores": [[0.01739359088242054, 0.501073956489563]], "normalized_score": [0.09335672110319138], "tokens": [["\u0120Jennifer", "\u0120Morrison"]], "entropy": [[5.350226402282715, 2.4384660720825195]], "avg_entropy": [3.894346237182617], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kathleen Erin Walsh", "score_of_biggest_cluster": 0.09335672110319138, "score_of_first_answer": 0.09335672110319138, "max_score_of_first_answer": 0.501073956489563, "min_score_of_first_answer": 0.01739359088242054, "avg_score": 0.09335672110319138, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24184018280357122, "std_of_first_answer": 0.24184018280357122, "avg_entropy_of_first_answer": 3.894346237182617, "lex_sim": 1.0}
{"question": "which was the first village of karnataka which declared independence", "answer": ["Hampi"], "scores": [[0.018077732995152473, 0.9267354011535645]], "normalized_score": [0.12943445146083832], "tokens": [["\u0120Hamp", "i"]], "entropy": [[5.66589879989624, 0.5593441724777222]], "avg_entropy": [3.112621486186981], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hyderabad", "score_of_biggest_cluster": 0.12943445146083832, "score_of_first_answer": 0.12943445146083832, "max_score_of_first_answer": 0.9267354011535645, "min_score_of_first_answer": 0.018077732995152473, "avg_score": 0.12943445146083832, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.454328834079206, "std_of_first_answer": 0.454328834079206, "avg_entropy_of_first_answer": 3.112621486186981, "lex_sim": 1.0}
{"question": "what percentage of the earth's surface is water", "answer": ["78.09%"], "scores": [[0.02705712802708149, 0.260850727558136, 0.0418347530066967, 0.6485836505889893]], "normalized_score": [0.11763707548379898], "tokens": [["\u012078", ".", "09", "%"]], "entropy": [[3.7664811611175537, 1.1651744842529297, 3.470554828643799, 1.6237356662750244]], "avg_entropy": [2.5064865350723267], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "78%", "score_of_biggest_cluster": 0.11763707548379898, "score_of_first_answer": 0.11763707548379898, "max_score_of_first_answer": 0.6485836505889893, "min_score_of_first_answer": 0.02705712802708149, "avg_score": 0.11763707548379898, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2509509520127534, "std_of_first_answer": 0.2509509520127534, "avg_entropy_of_first_answer": 2.5064865350723267, "lex_sim": 1.0}
{"question": "ganglion axons forming the optic nerve run to the", "answer": ["Lateral Geniculate Nuclei"], "scores": [[0.002812829101458192, 0.38171645998954773, 0.23802226781845093, 0.9934588074684143, 0.9708232283592224, 0.7356343269348145, 0.9841943383216858, 0.19149726629257202]], "normalized_score": [0.2765108644962311], "tokens": [["\u0120L", "ateral", "\u0120Gen", "ic", "ulate", "\u0120N", "ucle", "i"]], "entropy": [[4.400270462036133, 3.3489487171173096, 1.8114510774612427, 0.0569436252117157, 0.2149582952260971, 1.2045496702194214, 0.12171231955289841, 0.5513602495193481]], "avg_entropy": [1.4637743020430207], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "pretectal nucleus", "score_of_biggest_cluster": 0.2765108644962311, "score_of_first_answer": 0.2765108644962311, "max_score_of_first_answer": 0.9934588074684143, "min_score_of_first_answer": 0.002812829101458192, "avg_score": 0.2765108644962311, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37899798547887503, "std_of_first_answer": 0.37899798547887503, "avg_entropy_of_first_answer": 1.4637743020430207, "lex_sim": 1.0}
{"question": "who died in the first episode of stranger things", "answer": ["Jack Torrance"], "scores": [[0.008456445299088955, 0.18638837337493896, 0.8463224172592163]], "normalized_score": [0.11008146405220032], "tokens": [["\u0120Jack", "\u0120Tor", "rance"]], "entropy": [[6.450924873352051, 3.387705087661743, 0.9038501977920532]], "avg_entropy": [3.580826719601949], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Benny", "score_of_biggest_cluster": 0.11008146405220032, "score_of_first_answer": 0.11008146405220032, "max_score_of_first_answer": 0.8463224172592163, "min_score_of_first_answer": 0.008456445299088955, "avg_score": 0.11008146405220032, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36043062212287336, "std_of_first_answer": 0.36043062212287336, "avg_entropy_of_first_answer": 3.580826719601949, "lex_sim": 1.0}
{"question": "what is the first book of percy jackson", "answer": ["The Red Badge of Courage (1895)"], "scores": [[0.3800630271434784, 0.0270395465195179, 0.4565180242061615, 0.9775716662406921, 0.9965483546257019, 0.0020529695320874453, 0.4506441354751587, 0.7602279186248779, 0.9639526605606079]], "normalized_score": [0.24429155886173248], "tokens": [["\u0120The", "\u0120Red", "\u0120Badge", "\u0120of", "\u0120Courage", "\u0120(", "18", "95", ")"]], "entropy": [[4.552903175354004, 5.255003929138184, 2.8824729919433594, 0.12342173606157303, 0.030600760132074356, 0.1526479572057724, 3.5115206241607666, 1.0950289964675903, 0.2882905602455139]], "avg_entropy": [1.9879878589676485], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Lightning Thief", "score_of_biggest_cluster": 0.24429155886173248, "score_of_first_answer": 0.24429155886173248, "max_score_of_first_answer": 0.9965483546257019, "min_score_of_first_answer": 0.0020529695320874453, "avg_score": 0.24429155886173248, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36795745904104776, "std_of_first_answer": 0.36795745904104776, "avg_entropy_of_first_answer": 1.9879878589676485, "lex_sim": 1.0}
{"question": "what song did the titanic band play when it sank", "answer": ["My sweet darling's gone"], "scores": [[0.03341585397720337, 0.03263380005955696, 0.04373469203710556, 0.001980588771402836, 0.11449369788169861]], "normalized_score": [0.025515517219901085], "tokens": [["\u0120My", "\u0120sweet", "\u0120darling", "'s", "\u0120gone"]], "entropy": [[6.750718116760254, 5.053610801696777, 4.7454962730407715, 3.890690326690674, 4.179080486297607]], "avg_entropy": [4.923919200897217], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\"Autumn\"", "score_of_biggest_cluster": 0.025515517219901085, "score_of_first_answer": 0.025515517219901085, "max_score_of_first_answer": 0.11449369788169861, "min_score_of_first_answer": 0.001980588771402836, "avg_score": 0.025515517219901085, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.03733213977458331, "std_of_first_answer": 0.03733213977458331, "avg_entropy_of_first_answer": 4.923919200897217, "lex_sim": 1.0}
{"question": "who plays dusty in the movie pure country", "answer": ["Dustin Yellin"], "scores": [[0.007368944585323334, 0.00900054071098566, 0.24132171273231506, 0.9354333281517029]], "normalized_score": [0.06220434606075287], "tokens": [["\u0120Dustin", "\u0120Y", "ell", "in"]], "entropy": [[6.176724433898926, 5.443874359130859, 2.4587810039520264, 0.432738721370697]], "avg_entropy": [3.628029629588127], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George Strait", "score_of_biggest_cluster": 0.06220434606075287, "score_of_first_answer": 0.06220434606075287, "max_score_of_first_answer": 0.9354333281517029, "min_score_of_first_answer": 0.007368944585323334, "avg_score": 0.06220434606075287, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3799738338975342, "std_of_first_answer": 0.3799738338975342, "avg_entropy_of_first_answer": 3.628029629588127, "lex_sim": 1.0}
{"question": "what kind of trees have heart shaped leaves", "answer": ["palm tree"], "scores": [[0.00829989556223154, 0.12592023611068726]], "normalized_score": [0.032328393310308456], "tokens": [["\u0120palm", "\u0120tree"]], "entropy": [[6.051235675811768, 1.2769956588745117]], "avg_entropy": [3.6641156673431396], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Linden", "score_of_biggest_cluster": 0.032328393310308456, "score_of_first_answer": 0.032328393310308456, "max_score_of_first_answer": 0.12592023611068726, "min_score_of_first_answer": 0.00829989556223154, "avg_score": 0.032328393310308456, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05881017027422786, "std_of_first_answer": 0.05881017027422786, "avg_entropy_of_first_answer": 3.6641156673431396, "lex_sim": 1.0}
{"question": "what was the book wealth of nations about", "answer": ["economics"], "scores": [[0.017221221700310707]], "normalized_score": [0.017221225425601006], "tokens": [["\u0120economics"]], "entropy": [[5.390175819396973]], "avg_entropy": [5.390175819396973], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "what builds nations' wealth", "score_of_biggest_cluster": 0.017221225425601006, "score_of_first_answer": 0.017221225425601006, "max_score_of_first_answer": 0.017221221700310707, "min_score_of_first_answer": 0.017221221700310707, "avg_score": 0.017221225425601006, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.390175819396973, "lex_sim": 1.0}
{"question": "when was the last time university of michigan won a national championship", "answer": ["last year"], "scores": [[0.03662745654582977, 0.6321547627449036]], "normalized_score": [0.15216509997844696], "tokens": [["\u0120last", "\u0120year"]], "entropy": [[5.215553283691406, 1.9328724145889282]], "avg_entropy": [3.5742128491401672], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1997", "score_of_biggest_cluster": 0.15216509997844696, "score_of_first_answer": 0.15216509997844696, "max_score_of_first_answer": 0.6321547627449036, "min_score_of_first_answer": 0.03662745654582977, "avg_score": 0.15216509997844696, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2977636530995369, "std_of_first_answer": 0.2977636530995369, "avg_entropy_of_first_answer": 3.5742128491401672, "lex_sim": 1.0}
{"question": "when were 7 books removed from the bible", "answer": ["in the first century AD, by the Rabbinites"], "scores": [[0.05095640569925308, 0.4170268774032593, 0.022078795358538628, 0.7659944295883179, 0.0552593469619751, 0.030702197924256325, 0.1606615036725998, 0.4914361536502838, 0.007414732128381729, 0.008106672205030918, 0.5206427574157715]], "normalized_score": [0.08419232070446014], "tokens": [["\u0120in", "\u0120the", "\u0120first", "\u0120century", "\u0120AD", ",", "\u0120by", "\u0120the", "\u0120Rabb", "in", "ites"]], "entropy": [[5.426414489746094, 4.089420318603516, 4.775883674621582, 1.3416763544082642, 1.8301551342010498, 0.8590549826622009, 4.274102210998535, 3.226703643798828, 4.6943864822387695, 1.0274441242218018, 3.068166971206665]], "avg_entropy": [3.1466734897006643], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1546", "score_of_biggest_cluster": 0.08419232070446014, "score_of_first_answer": 0.08419232070446014, "max_score_of_first_answer": 0.7659944295883179, "min_score_of_first_answer": 0.007414732128381729, "avg_score": 0.08419232070446014, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25658979863096565, "std_of_first_answer": 0.25658979863096565, "avg_entropy_of_first_answer": 3.1466734897006643, "lex_sim": 1.0}
{"question": "what are the joyful mysteries of the holy rosary", "answer": ["seven"], "scores": [[0.01158824935555458]], "normalized_score": [0.01158824935555458], "tokens": [["\u0120seven"]], "entropy": [[5.051804065704346]], "avg_entropy": [5.051804065704346], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Nativity", "score_of_biggest_cluster": 0.01158824935555458, "score_of_first_answer": 0.01158824935555458, "max_score_of_first_answer": 0.01158824935555458, "min_score_of_first_answer": 0.01158824935555458, "avg_score": 0.01158824935555458, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.051804065704346, "lex_sim": 1.0}
{"question": "who played the hobbits in the lord of the rings", "answer": ["Ian Holm"], "scores": [[0.08815491199493408, 0.28756949305534363, 0.9711266160011292]], "normalized_score": [0.290907621383667], "tokens": [["\u0120Ian", "\u0120Hol", "m"]], "entropy": [[4.019905090332031, 1.3657597303390503, 0.20791630446910858]], "avg_entropy": [1.8645270417133968], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dominic Monaghan", "score_of_biggest_cluster": 0.290907621383667, "score_of_first_answer": 0.290907621383667, "max_score_of_first_answer": 0.9711266160011292, "min_score_of_first_answer": 0.08815491199493408, "avg_score": 0.290907621383667, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3781027998875177, "std_of_first_answer": 0.3781027998875177, "avg_entropy_of_first_answer": 1.8645270417133968, "lex_sim": 1.0}
{"question": "when was penicillin first introduced to the public", "answer": ["1928"], "scores": [[0.3335639536380768]], "normalized_score": [0.3335639536380768], "tokens": [["\u01201928"]], "entropy": [[3.5571751594543457]], "avg_entropy": [3.5571751594543457], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "After World War II", "score_of_biggest_cluster": 0.3335639536380768, "score_of_first_answer": 0.3335639536380768, "max_score_of_first_answer": 0.3335639536380768, "min_score_of_first_answer": 0.3335639536380768, "avg_score": 0.3335639536380768, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.5571751594543457, "lex_sim": 1.0}
{"question": "where does the band imagine dragons come from", "answer": ["Dragons"], "scores": [[0.02026110142469406]], "normalized_score": [0.02026110142469406], "tokens": [["\u0120Dragons"]], "entropy": [[6.959897518157959]], "avg_entropy": [6.959897518157959], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Las Vegas, Nevada", "score_of_biggest_cluster": 0.02026110142469406, "score_of_first_answer": 0.02026110142469406, "max_score_of_first_answer": 0.02026110142469406, "min_score_of_first_answer": 0.02026110142469406, "avg_score": 0.02026110142469406, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.959897518157959, "lex_sim": 1.0}
{"question": "the tenure of the office of the lokayukt of up", "answer": ["2 years"], "scores": [[0.017931034788489342, 0.490991473197937]], "normalized_score": [0.09382956475019455], "tokens": [["\u01202", "\u0120years"]], "entropy": [[6.524439811706543, 2.441812753677368]], "avg_entropy": [4.483126282691956], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "eight years", "score_of_biggest_cluster": 0.09382956475019455, "score_of_first_answer": 0.09382956475019455, "max_score_of_first_answer": 0.490991473197937, "min_score_of_first_answer": 0.017931034788489342, "avg_score": 0.09382956475019455, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23653021920472383, "std_of_first_answer": 0.23653021920472383, "avg_entropy_of_first_answer": 4.483126282691956, "lex_sim": 1.0}
{"question": "what is the current population of bora bora", "answer": ["11,300"], "scores": [[0.017537064850330353, 0.7048414349555969, 0.0069786361418664455]], "normalized_score": [0.04418480768799782], "tokens": [["\u012011", ",", "300"]], "entropy": [[5.216073513031006, 1.593408465385437, 4.4722394943237305]], "avg_entropy": [3.7605738242467246], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "10,605", "score_of_biggest_cluster": 0.04418480768799782, "score_of_first_answer": 0.04418480768799782, "max_score_of_first_answer": 0.7048414349555969, "min_score_of_first_answer": 0.0069786361418664455, "avg_score": 0.04418480768799782, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32651548602898905, "std_of_first_answer": 0.32651548602898905, "avg_entropy_of_first_answer": 3.7605738242467246, "lex_sim": 1.0}
{"question": "when was rosencrantz and guildenstern are dead written", "answer": ["1814"], "scores": [[0.0595257394015789, 0.04695921763777733]], "normalized_score": [0.05287042632699013], "tokens": [["\u012018", "14"]], "entropy": [[4.452498912811279, 3.9861011505126953]], "avg_entropy": [4.219300031661987], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1966", "score_of_biggest_cluster": 0.05287042632699013, "score_of_first_answer": 0.05287042632699013, "max_score_of_first_answer": 0.0595257394015789, "min_score_of_first_answer": 0.04695921763777733, "avg_score": 0.05287042632699013, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.006283260881900787, "std_of_first_answer": 0.006283260881900787, "avg_entropy_of_first_answer": 4.219300031661987, "lex_sim": 1.0}
{"question": "who sings you can't get what you want", "answer": ["Billy Joel"], "scores": [[0.033890895545482635, 0.88954758644104]], "normalized_score": [0.1736305207014084], "tokens": [["\u0120Billy", "\u0120Joel"]], "entropy": [[6.033986568450928, 0.7384345531463623]], "avg_entropy": [3.386210560798645], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Rolling Stones", "score_of_biggest_cluster": 0.1736305207014084, "score_of_first_answer": 0.1736305207014084, "max_score_of_first_answer": 0.88954758644104, "min_score_of_first_answer": 0.033890895545482635, "avg_score": 0.1736305207014084, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4278283454477787, "std_of_first_answer": 0.4278283454477787, "avg_entropy_of_first_answer": 3.386210560798645, "lex_sim": 1.0}
{"question": "who sings somebody's watching me with michael jackson", "answer": ["somebody's watching me"], "scores": [[0.0014196686679497361, 0.48845937848091125, 0.9318403005599976, 0.9087837934494019]], "normalized_score": [0.15566980838775635], "tokens": [["\u0120somebody", "'s", "\u0120watching", "\u0120me"]], "entropy": [[5.2338128089904785, 1.8163727521896362, 0.47887516021728516, 0.5911200642585754]], "avg_entropy": [2.030045196413994], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jermaine Jackson", "score_of_biggest_cluster": 0.15566980838775635, "score_of_first_answer": 0.15566980838775635, "max_score_of_first_answer": 0.9318403005599976, "min_score_of_first_answer": 0.0014196686679497361, "avg_score": 0.15566980838775635, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37914300934211714, "std_of_first_answer": 0.37914300934211714, "avg_entropy_of_first_answer": 2.030045196413994, "lex_sim": 1.0}
{"question": "who invented the chip in. debit cards", "answer": ["Peter Brennan"], "scores": [[0.005555222742259502, 0.0030795747879892588]], "normalized_score": [0.0041361479088664055], "tokens": [["\u0120Peter", "\u0120Brennan"]], "entropy": [[5.467272758483887, 5.07688045501709]], "avg_entropy": [5.272076606750488], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Helmut Gr\u00f6ttrup", "score_of_biggest_cluster": 0.0041361479088664055, "score_of_first_answer": 0.0041361479088664055, "max_score_of_first_answer": 0.005555222742259502, "min_score_of_first_answer": 0.0030795747879892588, "avg_score": 0.0041361479088664055, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0012378239771351218, "std_of_first_answer": 0.0012378239771351218, "avg_entropy_of_first_answer": 5.272076606750488, "lex_sim": 1.0}
{"question": "who played raquel in only fools and horses", "answer": ["Raquel Welch"], "scores": [[0.5086807608604431, 0.9964249730110168, 0.9691991806030273]], "normalized_score": [0.7890435457229614], "tokens": [["\u0120Ra", "quel", "\u0120Welch"]], "entropy": [[3.885514497756958, 0.03600962460041046, 0.3128988444805145]], "avg_entropy": [1.4114743222792943], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tessa Peake-Jones", "score_of_biggest_cluster": 0.7890435457229614, "score_of_first_answer": 0.7890435457229614, "max_score_of_first_answer": 0.9964249730110168, "min_score_of_first_answer": 0.5086807608604431, "avg_score": 0.7890435457229614, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22378384292308945, "std_of_first_answer": 0.22378384292308945, "avg_entropy_of_first_answer": 1.4114743222792943, "lex_sim": 1.0}
{"question": "when does madea's family funeral come out", "answer": ["June 21, 2019"], "scores": [[0.05180646851658821, 0.04183868691325188, 0.10344248265028, 0.22435173392295837]], "normalized_score": [0.08421657234430313], "tokens": [["\u0120June", "\u012021", ",", "\u01202019"]], "entropy": [[4.61489200592041, 3.704704761505127, 0.9615223407745361, 2.9585983753204346]], "avg_entropy": [3.059929370880127], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2018", "score_of_biggest_cluster": 0.08421657234430313, "score_of_first_answer": 0.08421657234430313, "max_score_of_first_answer": 0.22435173392295837, "min_score_of_first_answer": 0.04183868691325188, "avg_score": 0.08421657234430313, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07257004816949184, "std_of_first_answer": 0.07257004816949184, "avg_entropy_of_first_answer": 3.059929370880127, "lex_sim": 1.0}
{"question": "is it marley and me or marley and i", "answer": ["Marley and Me"], "scores": [[0.5232464671134949, 0.9947370886802673, 0.859593391418457, 0.6138538718223572]], "normalized_score": [0.7239243388175964], "tokens": [["\u0120Mar", "ley", "\u0120and", "\u0120Me"]], "entropy": [[3.018826484680176, 0.05209021270275116, 0.7079362869262695, 1.1676033735275269]], "avg_entropy": [1.2366140894591808], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marley & Me", "score_of_biggest_cluster": 0.7239243388175964, "score_of_first_answer": 0.7239243388175964, "max_score_of_first_answer": 0.9947370886802673, "min_score_of_first_answer": 0.5232464671134949, "avg_score": 0.7239243388175964, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.18830927300385536, "std_of_first_answer": 0.18830927300385536, "avg_entropy_of_first_answer": 1.2366140894591808, "lex_sim": 1.0}
{"question": "when does the champions league quarter finals start", "answer": ["September 20, 2015"], "scores": [[0.049166563898324966, 0.04011448845267296, 0.047903336584568024, 0.0795845314860344]], "normalized_score": [0.052365027368068695], "tokens": [["\u0120September", "\u012020", ",", "\u01202015"]], "entropy": [[4.544861316680908, 3.625028610229492, 0.853383481502533, 2.819878101348877]], "avg_entropy": [2.9607878774404526], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "16 March 2018", "score_of_biggest_cluster": 0.052365027368068695, "score_of_first_answer": 0.052365027368068695, "max_score_of_first_answer": 0.0795845314860344, "min_score_of_first_answer": 0.04011448845267296, "avg_score": 0.052365027368068695, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.015064521913980474, "std_of_first_answer": 0.015064521913980474, "avg_entropy_of_first_answer": 2.9607878774404526, "lex_sim": 1.0}
{"question": "when does the stock market open in hong kong", "answer": ["Monday, January 25"], "scores": [[0.03804725781083107, 0.12394366413354874, 0.04674100503325462, 0.06402966380119324]], "normalized_score": [0.06129242479801178], "tokens": [["\u0120Monday", ",", "\u0120January", "\u012025"]], "entropy": [[4.752251625061035, 1.9922637939453125, 3.9674184322357178, 3.467729330062866]], "avg_entropy": [3.544915795326233], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "09:15", "score_of_biggest_cluster": 0.06129242479801178, "score_of_first_answer": 0.06129242479801178, "max_score_of_first_answer": 0.12394366413354874, "min_score_of_first_answer": 0.03804725781083107, "avg_score": 0.06129242479801178, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.033520231774024134, "std_of_first_answer": 0.033520231774024134, "avg_entropy_of_first_answer": 3.544915795326233, "lex_sim": 1.0}
{"question": "what type of fuel goes in a zippo", "answer": ["flint"], "scores": [[0.03334573283791542, 0.8461058139801025]], "normalized_score": [0.16797028481960297], "tokens": [["\u0120fl", "int"]], "entropy": [[5.548008918762207, 0.5247107744216919]], "avg_entropy": [3.0363598465919495], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "lighter fluid", "score_of_biggest_cluster": 0.16797028481960297, "score_of_first_answer": 0.16797028481960297, "max_score_of_first_answer": 0.8461058139801025, "min_score_of_first_answer": 0.03334573283791542, "avg_score": 0.16797028481960297, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40638004057109356, "std_of_first_answer": 0.40638004057109356, "avg_entropy_of_first_answer": 3.0363598465919495, "lex_sim": 1.0}
{"question": "what is the name of the main artery which takes blood from the heart to the body", "answer": ["Aorta"], "scores": [[0.16950860619544983, 0.9933449029922485, 0.8658877611160278]], "normalized_score": [0.526321530342102], "tokens": [["\u0120A", "ort", "a"]], "entropy": [[2.978140354156494, 0.05971241742372513, 0.48665764927864075]], "avg_entropy": [1.1748368069529533], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The aorta", "score_of_biggest_cluster": 0.526321530342102, "score_of_first_answer": 0.526321530342102, "max_score_of_first_answer": 0.9933449029922485, "min_score_of_first_answer": 0.16950860619544983, "avg_score": 0.526321530342102, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3620766484757699, "std_of_first_answer": 0.3620766484757699, "avg_entropy_of_first_answer": 1.1748368069529533, "lex_sim": 1.0}
{"question": "who did the dominican republic gain its independence from", "answer": ["Haiti"], "scores": [[0.03789316117763519]], "normalized_score": [0.03789316490292549], "tokens": [["\u0120Haiti"]], "entropy": [[2.6778712272644043]], "avg_entropy": [2.6778712272644043], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Haiti", "score_of_biggest_cluster": 0.03789316490292549, "score_of_first_answer": 0.03789316490292549, "max_score_of_first_answer": 0.03789316117763519, "min_score_of_first_answer": 0.03789316117763519, "avg_score": 0.03789316490292549, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.6778712272644043, "lex_sim": 1.0}
{"question": "what is the meaning of the harp in ireland", "answer": ["harp"], "scores": [[0.11767934262752533, 0.9215244054794312]], "normalized_score": [0.32930895686149597], "tokens": [["\u0120har", "p"]], "entropy": [[5.843312740325928, 0.515514612197876]], "avg_entropy": [3.179413676261902], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the arms of Ireland", "score_of_biggest_cluster": 0.32930895686149597, "score_of_first_answer": 0.32930895686149597, "max_score_of_first_answer": 0.9215244054794312, "min_score_of_first_answer": 0.11767934262752533, "avg_score": 0.32930895686149597, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4019225314259529, "std_of_first_answer": 0.4019225314259529, "avg_entropy_of_first_answer": 3.179413676261902, "lex_sim": 1.0}
{"question": "what is the system of government in japan", "answer": ["constitutional monarchy"], "scores": [[0.08409187197685242, 0.7590221166610718]], "normalized_score": [0.2526412308216095], "tokens": [["\u0120constitutional", "\u0120monarchy"]], "entropy": [[4.893139839172363, 1.2836346626281738]], "avg_entropy": [3.0883872509002686], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "constitutional monarchy", "score_of_biggest_cluster": 0.2526412308216095, "score_of_first_answer": 0.2526412308216095, "max_score_of_first_answer": 0.7590221166610718, "min_score_of_first_answer": 0.08409187197685242, "avg_score": 0.2526412308216095, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3374651223421097, "std_of_first_answer": 0.3374651223421097, "avg_entropy_of_first_answer": 3.0883872509002686, "lex_sim": 1.0}
{"question": "who won battle of the sexes tennis game", "answer": ["Caroline Wozniacki"], "scores": [[0.00470293452963233, 0.9398659467697144, 0.9779030680656433, 0.9923946857452393, 0.9955480098724365, 0.9808727502822876]], "normalized_score": [0.40149644017219543], "tokens": [["\u0120Caroline", "\u0120W", "oz", "ni", "ack", "i"]], "entropy": [[5.025265693664551, 0.5764201879501343, 0.19987857341766357, 0.0805312842130661, 0.03583215922117233, 0.14585667848587036]], "avg_entropy": [1.0106307628254096], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Billie Jean King", "score_of_biggest_cluster": 0.40149644017219543, "score_of_first_answer": 0.40149644017219543, "max_score_of_first_answer": 0.9955480098724365, "min_score_of_first_answer": 0.00470293452963233, "avg_score": 0.40149644017219543, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3629256581387, "std_of_first_answer": 0.3629256581387, "avg_entropy_of_first_answer": 1.0106307628254096, "lex_sim": 1.0}
{"question": "who won in the brown vs board of education case", "answer": ["the courts overrode the board's decision and found the state constitution trumped the nebraska state law"], "scores": [[0.14074485003948212, 0.01049408596009016, 0.0009556517470628023, 0.282926082611084, 0.9983137845993042, 0.7109518647193909, 0.1497858464717865, 0.06269456446170807, 0.5274292230606079, 0.10634788125753403, 0.00864200759679079, 0.231161966919899, 0.019773585721850395, 0.03000670298933983, 0.06809933483600616, 0.9934324026107788, 0.5285987257957458, 0.0008392216986976564, 0.2790588438510895, 0.13543696701526642, 0.26910296082496643]], "normalized_score": [0.087024986743927], "tokens": [["\u0120the", "\u0120courts", "\u0120over", "ro", "de", "\u0120the", "\u0120board", "'s", "\u0120decision", "\u0120and", "\u0120found", "\u0120the", "\u0120state", "\u0120constitution", "\u0120trump", "ed", "\u0120the", "\u0120ne", "braska", "\u0120state", "\u0120law"]], "entropy": [[5.448960781097412, 4.704275131225586, 1.8273024559020996, 3.2665207386016846, 0.018112869933247566, 2.149415969848633, 4.454429626464844, 1.7973759174346924, 3.180187702178955, 1.546184778213501, 4.329611778259277, 2.3994674682617188, 4.8120341300964355, 3.683833599090576, 4.427122592926025, 0.06849950551986694, 2.7134313583374023, 4.494663715362549, 3.472594976425171, 3.482459545135498, 2.3246045112609863]], "avg_entropy": [3.0762423405512456], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "defendants", "score_of_biggest_cluster": 0.087024986743927, "score_of_first_answer": 0.087024986743927, "max_score_of_first_answer": 0.9983137845993042, "min_score_of_first_answer": 0.0008392216986976564, "avg_score": 0.087024986743927, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30417602560157025, "std_of_first_answer": 0.30417602560157025, "avg_entropy_of_first_answer": 3.0762423405512456, "lex_sim": 1.0}
{"question": "when does the 14th season of grey's anatomy come out", "answer": ["Fall 2011"], "scores": [[0.023376408964395523, 0.06390846520662308]], "normalized_score": [0.03865165263414383], "tokens": [["\u0120Fall", "\u01202011"]], "entropy": [[4.484760284423828, 2.414064884185791]], "avg_entropy": [3.4494125843048096], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "September 28, 2017", "score_of_biggest_cluster": 0.03865165263414383, "score_of_first_answer": 0.03865165263414383, "max_score_of_first_answer": 0.06390846520662308, "min_score_of_first_answer": 0.023376408964395523, "avg_score": 0.03865165263414383, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.020266028121113777, "std_of_first_answer": 0.020266028121113777, "avg_entropy_of_first_answer": 3.4494125843048096, "lex_sim": 1.0}
{"question": "when do the walking dead comics come out", "answer": ["February 6th, 2009"], "scores": [[0.021428842097520828, 0.010961754247546196, 0.7508482336997986, 0.030835658311843872, 0.07140916585922241]], "normalized_score": [0.05222129076719284], "tokens": [["\u0120February", "\u01206", "th", ",", "\u01202009"]], "entropy": [[5.064750671386719, 2.3450069427490234, 0.9375238418579102, 0.7295568585395813, 3.4919052124023438]], "avg_entropy": [2.5137487053871155], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "October\u00a02003\u00a0\u2013 present", "score_of_biggest_cluster": 0.05222129076719284, "score_of_first_answer": 0.05222129076719284, "max_score_of_first_answer": 0.7508482336997986, "min_score_of_first_answer": 0.010961754247546196, "avg_score": 0.05222129076719284, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2876060749690264, "std_of_first_answer": 0.2876060749690264, "avg_entropy_of_first_answer": 2.5137487053871155, "lex_sim": 1.0}
{"question": "where did zendaya place in dancing with the stars", "answer": ["16th place"], "scores": [[0.0054623642936348915, 0.857640266418457, 0.06581570953130722]], "normalized_score": [0.06755721569061279], "tokens": [["\u012016", "th", "\u0120place"]], "entropy": [[4.860218048095703, 0.5009738206863403, 0.7056639194488525]], "avg_entropy": [2.022285262743632], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Runner-up", "score_of_biggest_cluster": 0.06755721569061279, "score_of_first_answer": 0.06755721569061279, "max_score_of_first_answer": 0.857640266418457, "min_score_of_first_answer": 0.0054623642936348915, "avg_score": 0.06755721569061279, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.388277654383328, "std_of_first_answer": 0.388277654383328, "avg_entropy_of_first_answer": 2.022285262743632, "lex_sim": 1.0}
{"question": "what tectonic setting is responsible for the folded mountains of pennsylvania and the high himalaya", "answer": ["ophiolite complex"], "scores": [[0.06639125198125839, 0.39868173003196716, 0.962069571018219, 0.9368639588356018, 0.012262691743671894]], "normalized_score": [0.19644512236118317], "tokens": [["\u0120o", "ph", "iol", "ite", "\u0120complex"]], "entropy": [[5.516373157501221, 1.0580053329467773, 0.28993791341781616, 0.2895981967449188, 1.2299036979675293]], "avg_entropy": [1.6767636597156526], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "a convergent plate boundary", "score_of_biggest_cluster": 0.19644512236118317, "score_of_first_answer": 0.19644512236118317, "max_score_of_first_answer": 0.962069571018219, "min_score_of_first_answer": 0.012262691743671894, "avg_score": 0.19644512236118317, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40925937131561796, "std_of_first_answer": 0.40925937131561796, "avg_entropy_of_first_answer": 1.6767636597156526, "lex_sim": 1.0}
{"question": "when is episode 12 of miraculous ladybug season 2 coming out", "answer": ["June 6"], "scores": [[0.03536021709442139, 0.0419902540743351]], "normalized_score": [0.03853290528059006], "tokens": [["\u0120June", "\u01206"]], "entropy": [[4.628436088562012, 3.584075927734375]], "avg_entropy": [4.106256008148193], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "30\u00a0March\u00a02018", "score_of_biggest_cluster": 0.03853290528059006, "score_of_first_answer": 0.03853290528059006, "max_score_of_first_answer": 0.0419902540743351, "min_score_of_first_answer": 0.03536021709442139, "avg_score": 0.03853290528059006, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0033150184899568558, "std_of_first_answer": 0.0033150184899568558, "avg_entropy_of_first_answer": 4.106256008148193, "lex_sim": 1.0}
{"question": "how many episodes of corrie has there been", "answer": ["16 episodes"], "scores": [[0.009220804087817669, 0.01084616407752037]], "normalized_score": [0.010000514797866344], "tokens": [["\u012016", "\u0120episodes"]], "entropy": [[5.661327362060547, 0.48586609959602356]], "avg_entropy": [3.073596730828285], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "9,436", "score_of_biggest_cluster": 0.010000514797866344, "score_of_first_answer": 0.010000514797866344, "max_score_of_first_answer": 0.01084616407752037, "min_score_of_first_answer": 0.009220804087817669, "avg_score": 0.010000514797866344, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0008126799948513508, "std_of_first_answer": 0.0008126799948513508, "avg_entropy_of_first_answer": 3.073596730828285, "lex_sim": 1.0}
{"question": "when did australia 2 win the america's cup", "answer": ["2007"], "scores": [[0.015627548098564148]], "normalized_score": [0.015627548098564148], "tokens": [["\u01202007"]], "entropy": [[5.077404499053955]], "avg_entropy": [5.077404499053955], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1983", "score_of_biggest_cluster": 0.015627548098564148, "score_of_first_answer": 0.015627548098564148, "max_score_of_first_answer": 0.015627548098564148, "min_score_of_first_answer": 0.015627548098564148, "avg_score": 0.015627548098564148, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.077404499053955, "lex_sim": 1.0}
{"question": "where is the protien made in the cell", "answer": ["outside the nucleus"], "scores": [[0.0015716779744252563, 0.3458278179168701, 0.14191803336143494]], "normalized_score": [0.042568378150463104], "tokens": [["\u0120outside", "\u0120the", "\u0120nucleus"]], "entropy": [[5.915969371795654, 1.74905526638031, 1.5791611671447754]], "avg_entropy": [3.0813952684402466], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in the cell nucleus", "score_of_biggest_cluster": 0.042568378150463104, "score_of_first_answer": 0.042568378150463104, "max_score_of_first_answer": 0.3458278179168701, "min_score_of_first_answer": 0.0015716779744252563, "avg_score": 0.042568378150463104, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14133828198723677, "std_of_first_answer": 0.14133828198723677, "avg_entropy_of_first_answer": 3.0813952684402466, "lex_sim": 1.0}
{"question": "who is the present president of assam sahitya sabha", "answer": ["Jogesh Patnaik"], "scores": [[0.011921555735170841, 0.02643773891031742, 0.22958408296108246, 0.10998693853616714, 0.5652568936347961, 0.9987583160400391]], "normalized_score": [0.1284569799900055], "tokens": [["\u0120J", "og", "esh", "\u0120Pat", "na", "ik"]], "entropy": [[6.0870466232299805, 3.9989871978759766, 1.3925230503082275, 2.9493017196655273, 1.827944278717041, 0.013408161699771881]], "avg_entropy": [2.7115351719160876], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dhrubajyoti Bora", "score_of_biggest_cluster": 0.1284569799900055, "score_of_first_answer": 0.1284569799900055, "max_score_of_first_answer": 0.9987583160400391, "min_score_of_first_answer": 0.011921555735170841, "avg_score": 0.1284569799900055, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35453142333252635, "std_of_first_answer": 0.35453142333252635, "avg_entropy_of_first_answer": 2.7115351719160876, "lex_sim": 1.0}
{"question": "who did the 49ers beat in the super bowls", "answer": ["Packers"], "scores": [[0.00544271245598793]], "normalized_score": [0.005442712921649218], "tokens": [["\u0120Packers"]], "entropy": [[3.7817137241363525]], "avg_entropy": [3.7817137241363525], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "San Diego Chargers", "score_of_biggest_cluster": 0.005442712921649218, "score_of_first_answer": 0.005442712921649218, "max_score_of_first_answer": 0.00544271245598793, "min_score_of_first_answer": 0.00544271245598793, "avg_score": 0.005442712921649218, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.7817137241363525, "lex_sim": 1.0}
{"question": "a single period of precession of earth's axis is completed in about", "answer": ["25,800 years"], "scores": [[0.10627803951501846, 0.6783459186553955, 0.4132540225982666, 0.8160346150398254]], "normalized_score": [0.3948708772659302], "tokens": [["\u012025", ",", "800", "\u0120years"]], "entropy": [[4.575348377227783, 1.4116320610046387, 2.416659116744995, 0.7926301956176758]], "avg_entropy": [2.299067437648773], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "approximately 26,000 years", "score_of_biggest_cluster": 0.3948708772659302, "score_of_first_answer": 0.3948708772659302, "max_score_of_first_answer": 0.8160346150398254, "min_score_of_first_answer": 0.10627803951501846, "avg_score": 0.3948708772659302, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2711911693286648, "std_of_first_answer": 0.2711911693286648, "avg_entropy_of_first_answer": 2.299067437648773, "lex_sim": 1.0}
{"question": "when did the last volcano erupt in iceland", "answer": ["1875"], "scores": [[0.036905840039253235, 0.02101387456059456]], "normalized_score": [0.02784842438995838], "tokens": [["\u012018", "75"]], "entropy": [[5.4564104080200195, 2.8969225883483887]], "avg_entropy": [4.176666498184204], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2010", "score_of_biggest_cluster": 0.02784842438995838, "score_of_first_answer": 0.02784842438995838, "max_score_of_first_answer": 0.036905840039253235, "min_score_of_first_answer": 0.02101387456059456, "avg_score": 0.02784842438995838, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.007945982739329338, "std_of_first_answer": 0.007945982739329338, "avg_entropy_of_first_answer": 4.176666498184204, "lex_sim": 1.0}
{"question": "what class of ship is the carnival glory", "answer": ["cruise ship"], "scores": [[0.15536613762378693, 0.7028661966323853]], "normalized_score": [0.3304566442966461], "tokens": [["\u0120cruise", "\u0120ship"]], "entropy": [[5.793582916259766, 1.0352897644042969]], "avg_entropy": [3.4144363403320312], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Conquest", "score_of_biggest_cluster": 0.3304566442966461, "score_of_first_answer": 0.3304566442966461, "max_score_of_first_answer": 0.7028661966323853, "min_score_of_first_answer": 0.15536613762378693, "avg_score": 0.3304566442966461, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27375002950429916, "std_of_first_answer": 0.27375002950429916, "avg_entropy_of_first_answer": 3.4144363403320312, "lex_sim": 1.0}
{"question": "who was the oldest man elected president of usa", "answer": ["zelazny in 1981"], "scores": [[0.0010947032133117318, 0.043762966990470886, 0.1523723006248474, 0.8640969395637512, 0.0033778753131628036, 0.017018023878335953]], "normalized_score": [0.026703648269176483], "tokens": [["\u0120z", "el", "az", "ny", "\u0120in", "\u01201981"]], "entropy": [[4.445550918579102, 4.539567947387695, 4.39573335647583, 0.7812892198562622, 1.894822120666504, 4.922115802764893]], "avg_entropy": [3.496513227621714], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ronald Reagan", "score_of_biggest_cluster": 0.026703648269176483, "score_of_first_answer": 0.026703648269176483, "max_score_of_first_answer": 0.8640969395637512, "min_score_of_first_answer": 0.0010947032133117318, "avg_score": 0.026703648269176483, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31012834753411156, "std_of_first_answer": 0.31012834753411156, "avg_entropy_of_first_answer": 3.496513227621714, "lex_sim": 1.0}
{"question": "who win road march in trinidad and tobago", "answer": ["Trinbago giant rats"], "scores": [[0.103936105966568, 0.8832170963287354, 0.9111189842224121, 0.01698981039226055, 0.01631454937160015]], "normalized_score": [0.11831356585025787], "tokens": [["\u0120Trin", "b", "ago", "\u0120giant", "\u0120rats"]], "entropy": [[5.647408962249756, 0.6163666248321533, 0.32661962509155273, 4.4074907302856445, 4.820375442504883]], "avg_entropy": [3.163652276992798], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Superblue", "score_of_biggest_cluster": 0.11831356585025787, "score_of_first_answer": 0.11831356585025787, "max_score_of_first_answer": 0.9111189842224121, "min_score_of_first_answer": 0.01631454937160015, "avg_score": 0.11831356585025787, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4184184948423497, "std_of_first_answer": 0.4184184948423497, "avg_entropy_of_first_answer": 3.163652276992798, "lex_sim": 1.0}
{"question": "who sings god gave rock and roll to you", "answer": ["Elton John"], "scores": [[0.09784206748008728, 0.9770525693893433, 0.9060850739479065]], "normalized_score": [0.44245675206184387], "tokens": [["\u0120El", "ton", "\u0120John"]], "entropy": [[4.792485237121582, 0.19610357284545898, 0.4866746664047241]], "avg_entropy": [1.8250878254572551], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the British band Argent", "score_of_biggest_cluster": 0.44245675206184387, "score_of_first_answer": 0.44245675206184387, "max_score_of_first_answer": 0.9770525693893433, "min_score_of_first_answer": 0.09784206748008728, "avg_score": 0.44245675206184387, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39879042805587545, "std_of_first_answer": 0.39879042805587545, "avg_entropy_of_first_answer": 1.8250878254572551, "lex_sim": 1.0}
{"question": "who was the buccaneers qb when they won the superbowl", "answer": ["Dan Marino"], "scores": [[0.01184485387057066, 0.8377837538719177]], "normalized_score": [0.0996164008975029], "tokens": [["\u0120Dan", "\u0120Marino"]], "entropy": [[5.434321403503418, 1.1683562994003296]], "avg_entropy": [3.301338851451874], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brad Johnson", "score_of_biggest_cluster": 0.0996164008975029, "score_of_first_answer": 0.0996164008975029, "max_score_of_first_answer": 0.8377837538719177, "min_score_of_first_answer": 0.01184485387057066, "avg_score": 0.0996164008975029, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41296945000067353, "std_of_first_answer": 0.41296945000067353, "avg_entropy_of_first_answer": 3.301338851451874, "lex_sim": 1.0}
{"question": "who played amy grant i i can only imagine", "answer": ["Sarah Jessica Parker"], "scores": [[0.0024417415261268616, 0.32701754570007324, 0.9518763422966003]], "normalized_score": [0.09126069396734238], "tokens": [["\u0120Sarah", "\u0120Jessica", "\u0120Parker"]], "entropy": [[4.45847225189209, 3.3643922805786133, 0.3874795436859131]], "avg_entropy": [2.736781358718872], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nicole DuPort", "score_of_biggest_cluster": 0.09126069396734238, "score_of_first_answer": 0.09126069396734238, "max_score_of_first_answer": 0.9518763422966003, "min_score_of_first_answer": 0.0024417415261268616, "avg_score": 0.09126069396734238, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3940141042990738, "std_of_first_answer": 0.3940141042990738, "avg_entropy_of_first_answer": 2.736781358718872, "lex_sim": 1.0}
{"question": "when does season 8 of shameless come back", "answer": ["September 17"], "scores": [[0.12182025611400604, 0.025526581332087517]], "normalized_score": [0.05576426908373833], "tokens": [["\u0120September", "\u012017"]], "entropy": [[4.241684436798096, 3.603989362716675]], "avg_entropy": [3.9228368997573853], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "November 5, 2017", "score_of_biggest_cluster": 0.05576426908373833, "score_of_first_answer": 0.05576426908373833, "max_score_of_first_answer": 0.12182025611400604, "min_score_of_first_answer": 0.025526581332087517, "avg_score": 0.05576426908373833, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04814683739095926, "std_of_first_answer": 0.04814683739095926, "avg_entropy_of_first_answer": 3.9228368997573853, "lex_sim": 1.0}
{"question": "who will take the throne after the queen dies", "answer": ["Prince William"], "scores": [[0.26283034682273865, 0.36777421832084656]], "normalized_score": [0.31090548634529114], "tokens": [["\u0120Prince", "\u0120William"]], "entropy": [[4.631279945373535, 2.135573387145996]], "avg_entropy": [3.3834266662597656], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charles, Prince of Wales", "score_of_biggest_cluster": 0.31090548634529114, "score_of_first_answer": 0.31090548634529114, "max_score_of_first_answer": 0.36777421832084656, "min_score_of_first_answer": 0.26283034682273865, "avg_score": 0.31090548634529114, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.052471935749053955, "std_of_first_answer": 0.052471935749053955, "avg_entropy_of_first_answer": 3.3834266662597656, "lex_sim": 1.0}
{"question": "vikram samvat calender is official in which country", "answer": ["india"], "scores": [[0.04299166053533554, 0.9794493913650513]], "normalized_score": [0.20520271360874176], "tokens": [["\u0120ind", "ia"]], "entropy": [[1.477504014968872, 0.13438086211681366]], "avg_entropy": [0.8059424385428429], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nepal", "score_of_biggest_cluster": 0.20520271360874176, "score_of_first_answer": 0.20520271360874176, "max_score_of_first_answer": 0.9794493913650513, "min_score_of_first_answer": 0.04299166053533554, "avg_score": 0.20520271360874176, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46822886541485786, "std_of_first_answer": 0.46822886541485786, "avg_entropy_of_first_answer": 0.8059424385428429, "lex_sim": 1.0}
{"question": "when's the last time the philadelphia eagles played the new england patriots", "answer": ["September 21, 2012"], "scores": [[0.04500407725572586, 0.03592368960380554, 0.3551812767982483, 0.04240487143397331]], "normalized_score": [0.07024650275707245], "tokens": [["\u0120September", "\u012021", ",", "\u01202012"]], "entropy": [[5.111966609954834, 3.901470184326172, 1.122491717338562, 3.6260948181152344]], "avg_entropy": [3.4405058324337006], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in Super Bowl LII", "score_of_biggest_cluster": 0.07024650275707245, "score_of_first_answer": 0.07024650275707245, "max_score_of_first_answer": 0.3551812767982483, "min_score_of_first_answer": 0.03592368960380554, "avg_score": 0.07024650275707245, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1360366671339443, "std_of_first_answer": 0.1360366671339443, "avg_entropy_of_first_answer": 3.4405058324337006, "lex_sim": 1.0}
{"question": "when did john steinbeck write of mice and me", "answer": ["1939"], "scores": [[0.04579414799809456]], "normalized_score": [0.04579414427280426], "tokens": [["\u01201939"]], "entropy": [[4.2857770919799805]], "avg_entropy": [4.2857770919799805], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1937", "score_of_biggest_cluster": 0.04579414427280426, "score_of_first_answer": 0.04579414427280426, "max_score_of_first_answer": 0.04579414799809456, "min_score_of_first_answer": 0.04579414799809456, "avg_score": 0.04579414427280426, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.2857770919799805, "lex_sim": 1.0}
{"question": "who was the winner of the first indianapolis 500", "answer": ["james croney"], "scores": [[0.0005242312327027321, 0.11718877404928207, 0.10054437071084976, 0.06721631437540054]], "normalized_score": [0.02538401260972023], "tokens": [["\u0120j", "ames", "\u0120cr", "oney"]], "entropy": [[5.101834297180176, 2.258511543273926, 5.141221523284912, 3.625723361968994]], "avg_entropy": [4.031822681427002], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ray Harroun", "score_of_biggest_cluster": 0.02538401260972023, "score_of_first_answer": 0.02538401260972023, "max_score_of_first_answer": 0.11718877404928207, "min_score_of_first_answer": 0.0005242312327027321, "avg_score": 0.02538401260972023, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.044684673010714056, "std_of_first_answer": 0.044684673010714056, "avg_entropy_of_first_answer": 4.031822681427002, "lex_sim": 1.0}
{"question": "who talks for belle in beauty and the beast", "answer": ["Tim Rice"], "scores": [[0.0046064951457083225, 0.157411128282547]], "normalized_score": [0.02692793309688568], "tokens": [["\u0120Tim", "\u0120Rice"]], "entropy": [[5.4762983322143555, 4.219475746154785]], "avg_entropy": [4.84788703918457], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Julie Nathanson", "score_of_biggest_cluster": 0.02692793309688568, "score_of_first_answer": 0.02692793309688568, "max_score_of_first_answer": 0.157411128282547, "min_score_of_first_answer": 0.0046064951457083225, "avg_score": 0.02692793309688568, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07640231656841934, "std_of_first_answer": 0.07640231656841934, "avg_entropy_of_first_answer": 4.84788703918457, "lex_sim": 1.0}
{"question": "what was going on in the world in 1900", "answer": ["World War I"], "scores": [[0.1301305890083313, 0.7176453471183777, 0.5724141001701355]], "normalized_score": [0.376703679561615], "tokens": [["\u0120World", "\u0120War", "\u0120I"]], "entropy": [[5.244823455810547, 1.708942174911499, 1.2008476257324219]], "avg_entropy": [2.7182044188181558], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Galveston hurricane", "score_of_biggest_cluster": 0.376703679561615, "score_of_first_answer": 0.376703679561615, "max_score_of_first_answer": 0.7176453471183777, "min_score_of_first_answer": 0.1301305890083313, "avg_score": 0.376703679561615, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24986227547121087, "std_of_first_answer": 0.24986227547121087, "avg_entropy_of_first_answer": 2.7182044188181558, "lex_sim": 1.0}
{"question": "when did fender start making amps in mexico", "answer": ["Fender Mexico began in 1973."], "scores": [[0.010540900751948357, 0.9676387906074524, 0.21770910918712616, 0.05670006573200226, 0.07633600383996964, 0.007726369891315699, 0.08863598853349686]], "normalized_score": [0.06779459863901138], "tokens": [["\u0120F", "ender", "\u0120Mexico", "\u0120began", "\u0120in", "\u01201973", "."]], "entropy": [[5.64430046081543, 0.24343979358673096, 4.423306941986084, 3.3348312377929688, 2.769571542739868, 4.773099899291992, 1.096848726272583]], "avg_entropy": [3.1836283717836653], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1996", "score_of_biggest_cluster": 0.06779459863901138, "score_of_first_answer": 0.06779459863901138, "max_score_of_first_answer": 0.9676387906074524, "min_score_of_first_answer": 0.007726369891315699, "avg_score": 0.06779459863901138, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31860971950501127, "std_of_first_answer": 0.31860971950501127, "avg_entropy_of_first_answer": 3.1836283717836653, "lex_sim": 1.0}
{"question": "what was the religion in the virginia colony", "answer": ["christianity"], "scores": [[0.03477395325899124, 0.9807727336883545, 0.6952125430107117]], "normalized_score": [0.2872852385044098], "tokens": [["\u0120christ", "ian", "ity"]], "entropy": [[4.406763553619385, 0.1447984278202057, 0.8060033321380615]], "avg_entropy": [1.785855104525884], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Anglican church", "score_of_biggest_cluster": 0.2872852385044098, "score_of_first_answer": 0.2872852385044098, "max_score_of_first_answer": 0.9807727336883545, "min_score_of_first_answer": 0.03477395325899124, "avg_score": 0.2872852385044098, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3961814183071712, "std_of_first_answer": 0.3961814183071712, "avg_entropy_of_first_answer": 1.785855104525884, "lex_sim": 1.0}
{"question": "who is president of india in present time", "answer": ["Narendra Modi"], "scores": [[0.19278982281684875, 0.9349307417869568]], "normalized_score": [0.42455288767814636], "tokens": [["\u0120Narendra", "\u0120Modi"]], "entropy": [[4.356748104095459, 0.43993350863456726]], "avg_entropy": [2.398340806365013], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ram Nath Kovind", "score_of_biggest_cluster": 0.42455288767814636, "score_of_first_answer": 0.42455288767814636, "max_score_of_first_answer": 0.9349307417869568, "min_score_of_first_answer": 0.19278982281684875, "avg_score": 0.42455288767814636, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.371070459485054, "std_of_first_answer": 0.371070459485054, "avg_entropy_of_first_answer": 2.398340806365013, "lex_sim": 1.0}
{"question": "who sang the american anthem at the super bowl", "answer": ["a"], "scores": [[0.00327457208186388]], "normalized_score": [0.00327457208186388], "tokens": [["\u0120a"]], "entropy": [[6.252558708190918]], "avg_entropy": [6.252558708190918], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pink", "score_of_biggest_cluster": 0.00327457208186388, "score_of_first_answer": 0.00327457208186388, "max_score_of_first_answer": 0.00327457208186388, "min_score_of_first_answer": 0.00327457208186388, "avg_score": 0.00327457208186388, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.252558708190918, "lex_sim": 1.0}
{"question": "who plays max voice in a goofy movie", "answer": ["Max Landis"], "scores": [[0.12208301573991776, 0.016326988115906715, 0.9971252083778381]], "normalized_score": [0.12572944164276123], "tokens": [["\u0120Max", "\u0120Land", "is"]], "entropy": [[5.64930534362793, 5.772179126739502, 0.030523892492055893]], "avg_entropy": [3.8173361209531627], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jason Marsden", "score_of_biggest_cluster": 0.12572944164276123, "score_of_first_answer": 0.12572944164276123, "max_score_of_first_answer": 0.9971252083778381, "min_score_of_first_answer": 0.016326988115906715, "avg_score": 0.12572944164276123, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4395513274470738, "std_of_first_answer": 0.4395513274470738, "avg_entropy_of_first_answer": 3.8173361209531627, "lex_sim": 1.0}
{"question": "when was the last time oklahoma won a national championship in football", "answer": ["March 12, 1968"], "scores": [[0.00790743064135313, 0.01774684153497219, 0.5397622585296631, 0.0038113403134047985]], "normalized_score": [0.02317976951599121], "tokens": [["\u0120March", "\u012012", ",", "\u01201968"]], "entropy": [[4.949134826660156, 3.8107283115386963, 1.4232648611068726, 3.8414764404296875]], "avg_entropy": [3.506151109933853], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2003", "score_of_biggest_cluster": 0.02317976951599121, "score_of_first_answer": 0.02317976951599121, "max_score_of_first_answer": 0.5397622585296631, "min_score_of_first_answer": 0.0038113403134047985, "avg_score": 0.02317976951599121, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22952679996327494, "std_of_first_answer": 0.22952679996327494, "avg_entropy_of_first_answer": 3.506151109933853, "lex_sim": 1.0}
{"question": "who played the judge in good will hunting", "answer": ["Jack Nicholson"], "scores": [[0.033692460507154465, 0.8396937847137451]], "normalized_score": [0.1682003289461136], "tokens": [["\u0120Jack", "\u0120Nicholson"]], "entropy": [[5.023392677307129, 1.0726454257965088]], "avg_entropy": [3.048019051551819], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "James P. Flynn", "score_of_biggest_cluster": 0.1682003289461136, "score_of_first_answer": 0.1682003289461136, "max_score_of_first_answer": 0.8396937847137451, "min_score_of_first_answer": 0.033692460507154465, "avg_score": 0.1682003289461136, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4030006621032953, "std_of_first_answer": 0.4030006621032953, "avg_entropy_of_first_answer": 3.048019051551819, "lex_sim": 1.0}
{"question": "when did come fly with me come out", "answer": ["November 21, 1994"], "scores": [[0.013288031332194805, 0.03571964055299759, 0.3013070523738861, 0.004532225430011749]], "normalized_score": [0.02837410755455494], "tokens": [["\u0120November", "\u012021", ",", "\u01201994"]], "entropy": [[6.182344436645508, 3.8848042488098145, 1.2077829837799072, 3.961455821990967]], "avg_entropy": [3.809096872806549], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "25 December 2010", "score_of_biggest_cluster": 0.02837410755455494, "score_of_first_answer": 0.02837410755455494, "max_score_of_first_answer": 0.3013070523738861, "min_score_of_first_answer": 0.004532225430011749, "avg_score": 0.02837410755455494, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12326785463939494, "std_of_first_answer": 0.12326785463939494, "avg_entropy_of_first_answer": 3.809096872806549, "lex_sim": 1.0}
{"question": "who played gareth in four weddings and a funeral", "answer": ["Eddie Hemmings"], "scores": [[0.03184214234352112, 0.00024237878096755594, 0.19193047285079956, 0.9955293536186218]], "normalized_score": [0.0348476767539978], "tokens": [["\u0120Eddie", "\u0120Hem", "m", "ings"]], "entropy": [[5.35051155090332, 2.1077332496643066, 2.0266647338867188, 0.04832800477743149]], "avg_entropy": [2.3833093848079443], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Simon Callow", "score_of_biggest_cluster": 0.0348476767539978, "score_of_first_answer": 0.0348476767539978, "max_score_of_first_answer": 0.9955293536186218, "min_score_of_first_answer": 0.00024237878096755594, "avg_score": 0.0348476767539978, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4053109538257254, "std_of_first_answer": 0.4053109538257254, "avg_entropy_of_first_answer": 2.3833093848079443, "lex_sim": 1.0}
{"question": "who has won the most olympic medals in curling", "answer": ["Norway"], "scores": [[0.017176298424601555]], "normalized_score": [0.017176300287246704], "tokens": [["\u0120Norway"]], "entropy": [[5.147168159484863]], "avg_entropy": [5.147168159484863], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Torger Nerg\u00e5rd", "score_of_biggest_cluster": 0.017176300287246704, "score_of_first_answer": 0.017176300287246704, "max_score_of_first_answer": 0.017176298424601555, "min_score_of_first_answer": 0.017176298424601555, "avg_score": 0.017176300287246704, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.147168159484863, "lex_sim": 1.0}
{"question": "who abandoned traditional harmony and created the twelve-tone system for composition", "answer": ["Arnold Schoenberg"], "scores": [[0.35846781730651855, 0.9476918578147888, 0.8747348189353943, 0.9797973036766052]], "normalized_score": [0.7345688343048096], "tokens": [["\u0120Arnold", "\u0120Sch", "oen", "berg"]], "entropy": [[3.500142812728882, 0.43658646941185, 0.7523622512817383, 0.14856621623039246]], "avg_entropy": [1.2094144374132156], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Arnold Schoenberg", "score_of_biggest_cluster": 0.7345688343048096, "score_of_first_answer": 0.7345688343048096, "max_score_of_first_answer": 0.9797973036766052, "min_score_of_first_answer": 0.35846781730651855, "avg_score": 0.7345688343048096, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2521356941699139, "std_of_first_answer": 0.2521356941699139, "avg_entropy_of_first_answer": 1.2094144374132156, "lex_sim": 1.0}
{"question": "who owns the dower house in the archers", "answer": ["Margaret Thatcher"], "scores": [[0.001315219677053392, 0.09903934597969055]], "normalized_score": [0.011413084343075752], "tokens": [["\u0120Margaret", "\u0120Thatcher"]], "entropy": [[7.121537208557129, 6.031923770904541]], "avg_entropy": [6.576730489730835], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lilian Bellamy", "score_of_biggest_cluster": 0.011413084343075752, "score_of_first_answer": 0.011413084343075752, "max_score_of_first_answer": 0.09903934597969055, "min_score_of_first_answer": 0.001315219677053392, "avg_score": 0.011413084343075752, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04886206315131858, "std_of_first_answer": 0.04886206315131858, "avg_entropy_of_first_answer": 6.576730489730835, "lex_sim": 1.0}
{"question": "what happens when an air mass is pushed up and over a mountain range", "answer": ["a warm air mass is forced over a mountain and the air cools"], "scores": [[0.031923700124025345, 0.01176415104418993, 0.12626981735229492, 0.5225167870521545, 0.26791831851005554, 0.06290929019451141, 0.221487358212471, 0.6064062714576721, 0.3012138307094574, 0.01475997269153595, 0.08822499960660934, 0.30468517541885376, 0.30263781547546387, 0.9929190278053284]], "normalized_score": [0.14607694745063782], "tokens": [["\u0120a", "\u0120warm", "\u0120air", "\u0120mass", "\u0120is", "\u0120forced", "\u0120over", "\u0120a", "\u0120mountain", "\u0120and", "\u0120the", "\u0120air", "\u0120cool", "s"]], "entropy": [[4.940956115722656, 5.120707035064697, 2.25972318649292, 2.8765945434570312, 3.6775906085968018, 3.982832431793213, 2.4736390113830566, 1.4605867862701416, 2.035491943359375, 0.7830297350883484, 4.416592597961426, 3.8126347064971924, 3.3611884117126465, 0.06994718313217163]], "avg_entropy": [2.94796530689512], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Orographic lift", "score_of_biggest_cluster": 0.14607694745063782, "score_of_first_answer": 0.14607694745063782, "max_score_of_first_answer": 0.9929190278053284, "min_score_of_first_answer": 0.01176415104418993, "avg_score": 0.14607694745063782, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26589984997014127, "std_of_first_answer": 0.26589984997014127, "avg_entropy_of_first_answer": 2.94796530689512, "lex_sim": 1.0}
{"question": "how long is a whale shark in meters", "answer": ["17.7 meters"], "scores": [[0.004705898463726044, 0.29864034056663513, 0.04814204201102257, 0.25813740491867065]], "normalized_score": [0.06464599072933197], "tokens": [["\u012017", ".", "7", "\u0120meters"]], "entropy": [[4.9051008224487305, 2.50363826751709, 1.7796496152877808, 1.917100191116333]], "avg_entropy": [2.7763722240924835], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "12.65\u00a0m", "score_of_biggest_cluster": 0.06464599072933197, "score_of_first_answer": 0.06464599072933197, "max_score_of_first_answer": 0.29864034056663513, "min_score_of_first_answer": 0.004705898463726044, "avg_score": 0.06464599072933197, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12772030503768458, "std_of_first_answer": 0.12772030503768458, "avg_entropy_of_first_answer": 2.7763722240924835, "lex_sim": 1.0}
{"question": "when did the cubs won a world series", "answer": ["1904"], "scores": [[0.005315079353749752]], "normalized_score": [0.005315078888088465], "tokens": [["\u01201904"]], "entropy": [[4.111730575561523]], "avg_entropy": [4.111730575561523], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1908", "score_of_biggest_cluster": 0.005315078888088465, "score_of_first_answer": 0.005315078888088465, "max_score_of_first_answer": 0.005315079353749752, "min_score_of_first_answer": 0.005315079353749752, "avg_score": 0.005315078888088465, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.111730575561523, "lex_sim": 1.0}
{"question": "who won the most on who wants to be a millionaire", "answer": ["David Blaine"], "scores": [[0.015426946803927422, 0.21954303979873657, 0.9767285585403442]], "normalized_score": [0.14900168776512146], "tokens": [["\u0120David", "\u0120Bl", "aine"]], "entropy": [[6.641097068786621, 4.855747222900391, 0.1699100285768509]], "avg_entropy": [3.888918106754621], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kevin Olmstead", "score_of_biggest_cluster": 0.14900168776512146, "score_of_first_answer": 0.14900168776512146, "max_score_of_first_answer": 0.9767285585403442, "min_score_of_first_answer": 0.015426946803927422, "avg_score": 0.14900168776512146, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41353410150104586, "std_of_first_answer": 0.41353410150104586, "avg_entropy_of_first_answer": 3.888918106754621, "lex_sim": 1.0}
{"question": "who played ice queen in chronicles of narnia", "answer": ["Helen Mirren"], "scores": [[0.04096098989248276, 0.9127399921417236, 0.995124876499176]], "normalized_score": [0.33383485674858093], "tokens": [["\u0120Helen", "\u0120Mir", "ren"]], "entropy": [[5.7195515632629395, 0.7511566281318665, 0.05234432965517044]], "avg_entropy": [2.1743508403499923], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tilda Swinton", "score_of_biggest_cluster": 0.33383485674858093, "score_of_first_answer": 0.33383485674858093, "max_score_of_first_answer": 0.995124876499176, "min_score_of_first_answer": 0.04096098989248276, "avg_score": 0.33383485674858093, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4316910701515949, "std_of_first_answer": 0.4316910701515949, "avg_entropy_of_first_answer": 2.1743508403499923, "lex_sim": 1.0}
{"question": "when does the new gotham season come out", "answer": ["January 2012"], "scores": [[0.03581947460770607, 0.0005726325907744467]], "normalized_score": [0.004528950899839401], "tokens": [["\u0120January", "\u01202012"]], "entropy": [[3.8229570388793945, 2.7680509090423584]], "avg_entropy": [3.2955039739608765], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "September\u00a021,\u00a02017", "score_of_biggest_cluster": 0.004528950899839401, "score_of_first_answer": 0.004528950899839401, "max_score_of_first_answer": 0.03581947460770607, "min_score_of_first_answer": 0.0005726325907744467, "avg_score": 0.004528950899839401, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01762342100846581, "std_of_first_answer": 0.01762342100846581, "avg_entropy_of_first_answer": 3.2955039739608765, "lex_sim": 1.0}
{"question": "number 4 in roman numerals on clock faces", "answer": ["IV"], "scores": [[0.3456089496612549]], "normalized_score": [0.3456089496612549], "tokens": [["\u0120IV"]], "entropy": [[3.8148484230041504]], "avg_entropy": [3.8148484230041504], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "IV", "score_of_biggest_cluster": 0.3456089496612549, "score_of_first_answer": 0.3456089496612549, "max_score_of_first_answer": 0.3456089496612549, "min_score_of_first_answer": 0.3456089496612549, "avg_score": 0.3456089496612549, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.8148484230041504, "lex_sim": 1.0}
{"question": "when was the national flag of india adopted", "answer": ["Independent Day, 16th August 1947"], "scores": [[0.009604169055819511, 0.0024605279322713614, 0.09720233827829361, 0.014921237714588642, 0.1498727947473526, 0.2557351589202881, 0.14929547905921936]], "normalized_score": [0.0410410612821579], "tokens": [["\u0120Independent", "\u0120Day", ",", "\u012016", "th", "\u0120August", "\u01201947"]], "entropy": [[4.3734612464904785, 0.626662015914917, 1.704206109046936, 4.561115264892578, 3.1411471366882324, 2.8853538036346436, 1.691455602645874]], "avg_entropy": [2.711914454187666], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "15 August 1947", "score_of_biggest_cluster": 0.0410410612821579, "score_of_first_answer": 0.0410410612821579, "max_score_of_first_answer": 0.2557351589202881, "min_score_of_first_answer": 0.0024605279322713614, "avg_score": 0.0410410612821579, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08786345431960324, "std_of_first_answer": 0.08786345431960324, "avg_entropy_of_first_answer": 2.711914454187666, "lex_sim": 1.0}
{"question": "what side does us army go on acu", "answer": ["right"], "scores": [[0.039709318429231644]], "normalized_score": [0.03970932215452194], "tokens": [["\u0120right"]], "entropy": [[6.459593772888184]], "avg_entropy": [6.459593772888184], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the right", "score_of_biggest_cluster": 0.03970932215452194, "score_of_first_answer": 0.03970932215452194, "max_score_of_first_answer": 0.039709318429231644, "min_score_of_first_answer": 0.039709318429231644, "avg_score": 0.03970932215452194, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.459593772888184, "lex_sim": 1.0}
{"question": "who does the vocals for feel it still", "answer": ["Robbie Williams"], "scores": [[0.0011255007702857256, 0.2119147628545761]], "normalized_score": [0.015443773008883], "tokens": [["\u0120Robbie", "\u0120Williams"]], "entropy": [[6.82512092590332, 5.264134883880615]], "avg_entropy": [6.044627904891968], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Portugal. The Man", "score_of_biggest_cluster": 0.015443773008883, "score_of_first_answer": 0.015443773008883, "max_score_of_first_answer": 0.2119147628545761, "min_score_of_first_answer": 0.0011255007702857256, "avg_score": 0.015443773008883, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10539463104214519, "std_of_first_answer": 0.10539463104214519, "avg_entropy_of_first_answer": 6.044627904891968, "lex_sim": 1.0}
{"question": "where does arsenic and old lace take place", "answer": ["London, UK"], "scores": [[0.08137266337871552, 0.10605759173631668, 0.14601820707321167]], "normalized_score": [0.10801292210817337], "tokens": [["\u0120London", ",", "\u0120UK"]], "entropy": [[6.708219051361084, 1.2412196397781372, 2.3172850608825684]], "avg_entropy": [3.4222412506739297], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brooklyn, New York", "score_of_biggest_cluster": 0.10801292210817337, "score_of_first_answer": 0.10801292210817337, "max_score_of_first_answer": 0.14601820707321167, "min_score_of_first_answer": 0.08137266337871552, "avg_score": 0.10801292210817337, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.026635904727099726, "std_of_first_answer": 0.026635904727099726, "avg_entropy_of_first_answer": 3.4222412506739297, "lex_sim": 1.0}
{"question": "when is the new dumping ground coming out 2017", "answer": ["Oct 22, 2017"], "scores": [[0.00833064317703247, 0.0162640567868948, 0.11190564930438995, 0.9000973105430603]], "normalized_score": [0.060780204832553864], "tokens": [["\u0120Oct", "\u012022", ",", "\u01202017"]], "entropy": [[4.700637340545654, 3.455686569213867, 1.4915802478790283, 0.6263636350631714]], "avg_entropy": [2.5685669481754303], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "15\u00a0December\u00a02017", "score_of_biggest_cluster": 0.060780204832553864, "score_of_first_answer": 0.060780204832553864, "max_score_of_first_answer": 0.9000973105430603, "min_score_of_first_answer": 0.00833064317703247, "avg_score": 0.060780204832553864, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37228963350810335, "std_of_first_answer": 0.37228963350810335, "avg_entropy_of_first_answer": 2.5685669481754303, "lex_sim": 1.0}
{"question": "when does flash come back on season 4 episode 17", "answer": ["November 1"], "scores": [[0.03138136491179466, 0.03487343341112137]], "normalized_score": [0.033081360161304474], "tokens": [["\u0120November", "\u01201"]], "entropy": [[5.014220237731934, 3.789759874343872]], "avg_entropy": [4.401990056037903], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "May\u00a01,\u00a02018", "score_of_biggest_cluster": 0.033081360161304474, "score_of_first_answer": 0.033081360161304474, "max_score_of_first_answer": 0.03487343341112137, "min_score_of_first_answer": 0.03138136491179466, "avg_score": 0.033081360161304474, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.001746034249663353, "std_of_first_answer": 0.001746034249663353, "avg_entropy_of_first_answer": 4.401990056037903, "lex_sim": 1.0}
{"question": "when did the first episode of that 70s show air", "answer": ["1968"], "scores": [[0.028132783249020576]], "normalized_score": [0.028132786974310875], "tokens": [["\u01201968"]], "entropy": [[4.538985252380371]], "avg_entropy": [4.538985252380371], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "August 23, 1998", "score_of_biggest_cluster": 0.028132786974310875, "score_of_first_answer": 0.028132786974310875, "max_score_of_first_answer": 0.028132783249020576, "min_score_of_first_answer": 0.028132783249020576, "avg_score": 0.028132786974310875, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.538985252380371, "lex_sim": 1.0}
{"question": "who was suite judy blue eyes written about", "answer": ["Jennifer Love Hewitt"], "scores": [[0.0016961463261395693, 0.09166113287210464, 0.8451604843139648, 0.9714941382408142]], "normalized_score": [0.10629355162382126], "tokens": [["\u0120Jennifer", "\u0120Love", "\u0120Hew", "itt"]], "entropy": [[3.734830856323242, 3.175703287124634, 1.0448217391967773, 0.23232169449329376]], "avg_entropy": [2.0469193942844868], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "singer/songwriter Judy Collins", "score_of_biggest_cluster": 0.10629355162382126, "score_of_first_answer": 0.10629355162382126, "max_score_of_first_answer": 0.9714941382408142, "min_score_of_first_answer": 0.0016961463261395693, "avg_score": 0.10629355162382126, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4342998327377887, "std_of_first_answer": 0.4342998327377887, "avg_entropy_of_first_answer": 2.0469193942844868, "lex_sim": 1.0}
{"question": "why does cooling water run through the condenser", "answer": ["because it's colder"], "scores": [[0.04266415536403656, 0.35543033480644226, 0.3001349866390228, 0.024154921993613243]], "normalized_score": [0.10239647328853607], "tokens": [["\u0120because", "\u0120it", "'s", "\u0120colder"]], "entropy": [[3.9380288124084473, 3.095813751220703, 2.993515729904175, 3.850576162338257]], "avg_entropy": [3.4694836139678955], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "condense the steam", "score_of_biggest_cluster": 0.10239647328853607, "score_of_first_answer": 0.10239647328853607, "max_score_of_first_answer": 0.35543033480644226, "min_score_of_first_answer": 0.024154921993613243, "avg_score": 0.10239647328853607, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14862336509305646, "std_of_first_answer": 0.14862336509305646, "avg_entropy_of_first_answer": 3.4694836139678955, "lex_sim": 1.0}
{"question": "what was the lowest barometric pressure for hurricane irma", "answer": ["963 mb"], "scores": [[0.39648279547691345, 0.013155341148376465, 0.2003518044948578, 0.7633291482925415]], "normalized_score": [0.16805750131607056], "tokens": [["\u01209", "63", "\u0120m", "b"]], "entropy": [[3.5816397666931152, 4.369379043579102, 2.1160874366760254, 0.7240725755691528]], "avg_entropy": [2.6977947056293488], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "914 mbar (hPa)", "score_of_biggest_cluster": 0.16805750131607056, "score_of_first_answer": 0.16805750131607056, "max_score_of_first_answer": 0.7633291482925415, "min_score_of_first_answer": 0.013155341148376465, "avg_score": 0.16805750131607056, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27779604657507107, "std_of_first_answer": 0.27779604657507107, "avg_entropy_of_first_answer": 2.6977947056293488, "lex_sim": 1.0}
{"question": "when does the nba season end this year", "answer": ["June 30"], "scores": [[0.21342945098876953, 0.4122035503387451]], "normalized_score": [0.2966081202030182], "tokens": [["\u0120June", "\u012030"]], "entropy": [[3.9571428298950195, 2.4062914848327637]], "avg_entropy": [3.1817171573638916], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "April 11, 2018", "score_of_biggest_cluster": 0.2966081202030182, "score_of_first_answer": 0.2966081202030182, "max_score_of_first_answer": 0.4122035503387451, "min_score_of_first_answer": 0.21342945098876953, "avg_score": 0.2966081202030182, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09938704967498779, "std_of_first_answer": 0.09938704967498779, "avg_entropy_of_first_answer": 3.1817171573638916, "lex_sim": 1.0}
{"question": "who wrote the book the origin of species", "answer": ["Charles Darwin"], "scores": [[0.9521757364273071, 0.9510514140129089]], "normalized_score": [0.9516134262084961], "tokens": [["\u0120Charles", "\u0120Darwin"]], "entropy": [[0.3377034068107605, 0.3181726336479187]], "avg_entropy": [0.3279380202293396], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charles Darwin", "score_of_biggest_cluster": 0.9516134262084961, "score_of_first_answer": 0.9516134262084961, "max_score_of_first_answer": 0.9521757364273071, "min_score_of_first_answer": 0.9510514140129089, "avg_score": 0.9516134262084961, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0005621612071990967, "std_of_first_answer": 0.0005621612071990967, "avg_entropy_of_first_answer": 0.3279380202293396, "lex_sim": 1.0}
{"question": "which hormone is released from the ruptured follicle or corpus luteum", "answer": ["androstenedione"], "scores": [[0.004050322342664003, 0.382230281829834, 0.9110669493675232, 0.9288328289985657]], "normalized_score": [0.1902504414319992], "tokens": [["\u0120and", "rost", "ened", "ione"]], "entropy": [[3.6224465370178223, 1.3179881572723389, 0.4338398575782776, 0.39699071645736694]], "avg_entropy": [1.4428163170814514], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "estrogen", "score_of_biggest_cluster": 0.1902504414319992, "score_of_first_answer": 0.1902504414319992, "max_score_of_first_answer": 0.9288328289985657, "min_score_of_first_answer": 0.004050322342664003, "avg_score": 0.1902504414319992, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3872725235587355, "std_of_first_answer": 0.3872725235587355, "avg_entropy_of_first_answer": 1.4428163170814514, "lex_sim": 1.0}
{"question": "tad the lost explorer and the secret of king midas english cast", "answer": ["lost"], "scores": [[0.0050083645619452]], "normalized_score": [0.005008365493267775], "tokens": [["\u0120lost"]], "entropy": [[7.104203224182129]], "avg_entropy": [7.104203224182129], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ariel Winter as Sara Lavrof", "score_of_biggest_cluster": 0.005008365493267775, "score_of_first_answer": 0.005008365493267775, "max_score_of_first_answer": 0.0050083645619452, "min_score_of_first_answer": 0.0050083645619452, "avg_score": 0.005008365493267775, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 7.104203224182129, "lex_sim": 1.0}
{"question": "who played sonny in lemony snicket's a series of unfortunate events", "answer": ["Jack Nicholson"], "scores": [[0.022457607090473175, 0.1641673892736435]], "normalized_score": [0.060719083994627], "tokens": [["\u0120Jack", "\u0120Nicholson"]], "entropy": [[5.684913158416748, 3.992605686187744]], "avg_entropy": [4.838759422302246], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Presley Smith", "score_of_biggest_cluster": 0.060719083994627, "score_of_first_answer": 0.060719083994627, "max_score_of_first_answer": 0.1641673892736435, "min_score_of_first_answer": 0.022457607090473175, "avg_score": 0.060719083994627, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07085489109158516, "std_of_first_answer": 0.07085489109158516, "avg_entropy_of_first_answer": 4.838759422302246, "lex_sim": 1.0}
{"question": "where does route 66 start on the west coast", "answer": ["San Jose"], "scores": [[0.05915128439664841, 0.022070223465561867]], "normalized_score": [0.03613145649433136], "tokens": [["\u0120San", "\u0120Jose"]], "entropy": [[4.533246040344238, 1.5284607410430908]], "avg_entropy": [3.0308533906936646], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in Santa Monica", "score_of_biggest_cluster": 0.03613145649433136, "score_of_first_answer": 0.03613145649433136, "max_score_of_first_answer": 0.05915128439664841, "min_score_of_first_answer": 0.022070223465561867, "avg_score": 0.03613145649433136, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01854053046554327, "std_of_first_answer": 0.01854053046554327, "avg_entropy_of_first_answer": 3.0308533906936646, "lex_sim": 1.0}
{"question": "who represents the arizona 9th district in the house", "answer": ["Rub\u00e9n K. Arizmendi"], "scores": [[0.09908031672239304, 0.009647625498473644, 0.552060067653656, 0.08233486860990524, 0.12671014666557312, 0.44009798765182495, 0.711948812007904, 0.9709630608558655]], "normalized_score": [0.18967002630233765], "tokens": [["\u0120Rub", "\u00c3\u00a9n", "\u0120K", ".", "\u0120Ari", "z", "m", "endi"]], "entropy": [[5.33791446685791, 0.07955954968929291, 2.1721103191375732, 4.062610626220703, 4.051025867462158, 0.7864198684692383, 1.3129756450653076, 0.16532333195209503]], "avg_entropy": [2.245992459356785], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kyrsten Sinema", "score_of_biggest_cluster": 0.18967002630233765, "score_of_first_answer": 0.18967002630233765, "max_score_of_first_answer": 0.9709630608558655, "min_score_of_first_answer": 0.009647625498473644, "avg_score": 0.18967002630233765, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32810113606758057, "std_of_first_answer": 0.32810113606758057, "avg_entropy_of_first_answer": 2.245992459356785, "lex_sim": 1.0}
{"question": "who is doing 2018 super bowl half time show", "answer": ["Beyonce"], "scores": [[0.027610044926404953, 0.8245393633842468]], "normalized_score": [0.15088263154029846], "tokens": [["\u0120Beyon", "ce"]], "entropy": [[5.796257019042969, 0.6322044134140015]], "avg_entropy": [3.214230716228485], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Justin Timberlake", "score_of_biggest_cluster": 0.15088263154029846, "score_of_first_answer": 0.15088263154029846, "max_score_of_first_answer": 0.8245393633842468, "min_score_of_first_answer": 0.027610044926404953, "avg_score": 0.15088263154029846, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39846465922892094, "std_of_first_answer": 0.39846465922892094, "avg_entropy_of_first_answer": 3.214230716228485, "lex_sim": 1.0}
{"question": "who sang never gonna let you go", "answer": ["Elton John"], "scores": [[0.2644279897212982, 0.9511216878890991, 0.8972015976905823]], "normalized_score": [0.60880446434021], "tokens": [["\u0120El", "ton", "\u0120John"]], "entropy": [[4.4290666580200195, 0.3904838562011719, 0.5299983620643616]], "avg_entropy": [1.783182958761851], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joe Pizzulo and Leeza Miller", "score_of_biggest_cluster": 0.60880446434021, "score_of_first_answer": 0.60880446434021, "max_score_of_first_answer": 0.9511216878890991, "min_score_of_first_answer": 0.2644279897212982, "avg_score": 0.60880446434021, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31177948941392186, "std_of_first_answer": 0.31177948941392186, "avg_entropy_of_first_answer": 1.783182958761851, "lex_sim": 1.0}
{"question": "where was robin hood prince of thieves made", "answer": ["England"], "scores": [[0.10243341326713562]], "normalized_score": [0.10243342071771622], "tokens": [["\u0120England"]], "entropy": [[5.602060317993164]], "avg_entropy": [5.602060317993164], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the United Kingdom", "score_of_biggest_cluster": 0.10243342071771622, "score_of_first_answer": 0.10243342071771622, "max_score_of_first_answer": 0.10243341326713562, "min_score_of_first_answer": 0.10243341326713562, "avg_score": 0.10243342071771622, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.602060317993164, "lex_sim": 1.0}
{"question": "where does some like it hot take place", "answer": ["Thailand"], "scores": [[0.0032461010850965977]], "normalized_score": [0.0032461003866046667], "tokens": [["\u0120Thailand"]], "entropy": [[6.031371116638184]], "avg_entropy": [6.031371116638184], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chicago", "score_of_biggest_cluster": 0.0032461003866046667, "score_of_first_answer": 0.0032461003866046667, "max_score_of_first_answer": 0.0032461010850965977, "min_score_of_first_answer": 0.0032461010850965977, "avg_score": 0.0032461003866046667, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.031371116638184, "lex_sim": 1.0}
{"question": "when did fortnite battle royal come out on xbox", "answer": ["Nov, 2017"], "scores": [[0.012634683400392532, 0.023135215044021606, 0.5671293139457703]], "normalized_score": [0.05493384599685669], "tokens": [["\u0120Nov", ",", "\u01202017"]], "entropy": [[4.248498916625977, 3.4218428134918213, 1.7877581119537354]], "avg_entropy": [3.1526999473571777], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "September 2017", "score_of_biggest_cluster": 0.05493384599685669, "score_of_first_answer": 0.05493384599685669, "max_score_of_first_answer": 0.5671293139457703, "min_score_of_first_answer": 0.012634683400392532, "avg_score": 0.05493384599685669, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2589517621549489, "std_of_first_answer": 0.2589517621549489, "avg_entropy_of_first_answer": 3.1526999473571777, "lex_sim": 1.0}
{"question": "what is an open university grade 3 pass", "answer": ["GCSE"], "scores": [[0.005764976143836975, 0.9910915493965149]], "normalized_score": [0.07558848708868027], "tokens": [["\u0120GC", "SE"]], "entropy": [[6.116115570068359, 0.09055928885936737]], "avg_entropy": [3.1033374294638634], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "55\u201369%", "score_of_biggest_cluster": 0.07558848708868027, "score_of_first_answer": 0.07558848708868027, "max_score_of_first_answer": 0.9910915493965149, "min_score_of_first_answer": 0.005764976143836975, "avg_score": 0.07558848708868027, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.49266328662633896, "std_of_first_answer": 0.49266328662633896, "avg_entropy_of_first_answer": 3.1033374294638634, "lex_sim": 1.0}
{"question": "who plays frankie\u2019s sister teddy on grace and frankie", "answer": ["Natalie"], "scores": [[0.00914353970438242]], "normalized_score": [0.00914353784173727], "tokens": [["\u0120Natalie"]], "entropy": [[6.6715593338012695]], "avg_entropy": [6.6715593338012695], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Talia Shire", "score_of_biggest_cluster": 0.00914353784173727, "score_of_first_answer": 0.00914353784173727, "max_score_of_first_answer": 0.00914353970438242, "min_score_of_first_answer": 0.00914353970438242, "avg_score": 0.00914353784173727, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.6715593338012695, "lex_sim": 1.0}
{"question": "where is fight or flight in the brain", "answer": ["right amygdala"], "scores": [[0.016023147851228714, 0.17798173427581787]], "normalized_score": [0.05340250954031944], "tokens": [["\u0120right", "\u0120amygdala"]], "entropy": [[5.701069355010986, 3.6925227642059326]], "avg_entropy": [4.6967960596084595], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the adrenal medulla", "score_of_biggest_cluster": 0.05340250954031944, "score_of_first_answer": 0.05340250954031944, "max_score_of_first_answer": 0.17798173427581787, "min_score_of_first_answer": 0.016023147851228714, "avg_score": 0.05340250954031944, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08097929321229458, "std_of_first_answer": 0.08097929321229458, "avg_entropy_of_first_answer": 4.6967960596084595, "lex_sim": 1.0}
{"question": "who is given credit for inventing the aqua lung", "answer": ["Dr. John Parnell"], "scores": [[0.03214866667985916, 0.7272952198982239, 0.0895196795463562, 0.15717674791812897, 0.06579921394586563, 0.873266875743866]], "normalized_score": [0.16321422159671783], "tokens": [["\u0120Dr", ".", "\u0120John", "\u0120P", "arn", "ell"]], "entropy": [[5.973306655883789, 2.2439231872558594, 5.626528263092041, 4.941352844238281, 2.0179967880249023, 0.7474780082702637]], "avg_entropy": [3.591764291127523], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jacques Cousteau", "score_of_biggest_cluster": 0.16321422159671783, "score_of_first_answer": 0.16321422159671783, "max_score_of_first_answer": 0.873266875743866, "min_score_of_first_answer": 0.03214866667985916, "avg_score": 0.16321422159671783, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34132017163966655, "std_of_first_answer": 0.34132017163966655, "avg_entropy_of_first_answer": 3.591764291127523, "lex_sim": 1.0}
{"question": "who sings the theme song for degrassi the next generation", "answer": ["the theme song for degrassi the next generation is"], "scores": [[0.01070298533886671, 0.043791014701128006, 0.7443951368331909, 0.1923825740814209, 0.5579448938369751, 0.9933236241340637, 0.9966287016868591, 0.7000488042831421, 0.9763596653938293, 0.9873714447021484, 0.6926851868629456]], "normalized_score": [0.36911892890930176], "tokens": [["\u0120the", "\u0120theme", "\u0120song", "\u0120for", "\u0120de", "grass", "i", "\u0120the", "\u0120next", "\u0120generation", "\u0120is"]], "entropy": [[6.91986083984375, 6.191591262817383, 1.3132548332214355, 2.2495694160461426, 2.0719685554504395, 0.05273619294166565, 0.04005174711346626, 1.4573986530303955, 0.2505991458892822, 0.10037549585103989, 1.3998167514801025]], "avg_entropy": [2.0042929903350095], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jakalope", "score_of_biggest_cluster": 0.36911892890930176, "score_of_first_answer": 0.36911892890930176, "max_score_of_first_answer": 0.9966287016868591, "min_score_of_first_answer": 0.01070298533886671, "avg_score": 0.36911892890930176, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3642908296441583, "std_of_first_answer": 0.3642908296441583, "avg_entropy_of_first_answer": 2.0042929903350095, "lex_sim": 1.0}
{"question": "what is the most common blood type in sweden", "answer": ["O-"], "scores": [[0.27342498302459717, 0.11424513906240463]], "normalized_score": [0.17674125730991364], "tokens": [["\u0120O", "-"]], "entropy": [[2.291280508041382, 1.796890139579773]], "avg_entropy": [2.0440853238105774], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A+", "score_of_biggest_cluster": 0.17674125730991364, "score_of_first_answer": 0.17674125730991364, "max_score_of_first_answer": 0.27342498302459717, "min_score_of_first_answer": 0.11424513906240463, "avg_score": 0.17674125730991364, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07958992198109627, "std_of_first_answer": 0.07958992198109627, "avg_entropy_of_first_answer": 2.0440853238105774, "lex_sim": 1.0}
{"question": "where is nuclear energy used most in the world", "answer": ["China"], "scores": [[0.11603367328643799]], "normalized_score": [0.11603367328643799], "tokens": [["\u0120China"]], "entropy": [[4.125974655151367]], "avg_entropy": [4.125974655151367], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "United States", "score_of_biggest_cluster": 0.11603367328643799, "score_of_first_answer": 0.11603367328643799, "max_score_of_first_answer": 0.11603367328643799, "min_score_of_first_answer": 0.11603367328643799, "avg_score": 0.11603367328643799, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.125974655151367, "lex_sim": 1.0}
{"question": "what is the name for the ch3coo- ion", "answer": ["molecule"], "scores": [[0.0005015945644117892]], "normalized_score": [0.0005015946226194501], "tokens": [["\u0120molecule"]], "entropy": [[7.50235652923584]], "avg_entropy": [7.50235652923584], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "polyatomic anion", "score_of_biggest_cluster": 0.0005015946226194501, "score_of_first_answer": 0.0005015946226194501, "max_score_of_first_answer": 0.0005015945644117892, "min_score_of_first_answer": 0.0005015945644117892, "avg_score": 0.0005015946226194501, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 7.50235652923584, "lex_sim": 1.0}
{"question": "which animal is the carrier of the h1n1 virus", "answer": ["The H1N1 virus (influenza A) that causes the \"H1N1\" type of a flu has no more than an estimated 0.0001 percent chance of becoming a major pandemic because each infected host will pass on the"], "scores": [[0.028935125097632408, 0.10922234505414963, 0.8477369546890259, 0.9535079002380371, 0.9893156290054321, 0.5622208118438721, 0.009326518513262272, 0.04317910596728325, 0.9992853999137878, 0.9969647526741028, 0.5258227586746216, 0.15194442868232727, 0.0031195306219160557, 0.2316378653049469, 0.46095407009124756, 0.03352440521121025, 0.014701012521982193, 0.7506335377693176, 0.9720067977905273, 0.9962764382362366, 0.6126501560211182, 0.003990465775132179, 0.7568246126174927, 0.0007432835409417748, 0.5128957629203796, 0.017229653894901276, 0.009051303379237652, 0.004246716387569904, 0.4303587079048157, 0.016050759702920914, 0.04007316380739212, 0.06822437047958374, 0.9649952054023743, 0.045301653444767, 0.19508661329746246, 0.42205867171287537, 0.9293169975280762, 0.15153171122074127, 0.41205012798309326, 0.012689601629972458, 0.5868194103240967, 0.9994308352470398, 0.01449268776923418, 0.002218318171799183, 0.01773933880031109, 0.022870877757668495, 0.0470350906252861, 0.10738987475633621, 0.13561953604221344, 0.7804757356643677]], "normalized_score": [0.10822208225727081], "tokens": [["\u0120The", "\u0120H", "1", "N", "1", "\u0120virus", "\u0120(", "inf", "lu", "enza", "\u0120A", ")", "\u0120that", "\u0120causes", "\u0120the", "\u0120\"", "H", "1", "N", "1", "\"", "\u0120type", "\u0120of", "\u0120a", "\u0120flu", "\u0120has", "\u0120no", "\u0120more", "\u0120than", "\u0120an", "\u0120estimated", "\u01200", ".", "0001", "\u0120percent", "\u0120chance", "\u0120of", "\u0120becoming", "\u0120a", "\u0120major", "\u0120pand", "emic", "\u0120because", "\u0120each", "\u0120infected", "\u0120host", "\u0120will", "\u0120pass", "\u0120on", "\u0120the"]], "entropy": [[4.914921760559082, 5.282727241516113, 1.06038498878479, 0.3007463216781616, 0.10358701646327972, 2.3826866149902344, 2.305269479751587, 3.9721384048461914, 0.007898837327957153, 0.022261181846261024, 1.6406874656677246, 1.4058277606964111, 1.8420759439468384, 3.2534520626068115, 2.453338146209717, 2.778503894805908, 1.7146919965744019, 1.288474202156067, 0.17494484782218933, 0.040992673486471176, 1.6162264347076416, 2.643782138824463, 1.340545415878296, 1.9118694067001343, 2.6557202339172363, 3.1511483192443848, 3.357440948486328, 4.2956647872924805, 4.026869773864746, 3.8042712211608887, 3.808748722076416, 4.377232074737549, 0.23568150401115417, 2.025707960128784, 2.775125741958618, 2.4113821983337402, 0.4221492111682892, 4.018783092498779, 2.852828025817871, 1.9182623624801636, 1.842171311378479, 0.0058181616477668285, 2.7150750160217285, 2.5415470600128174, 3.4475040435791016, 1.333626389503479, 4.323555946350098, 3.95524263381958, 1.386850357055664, 1.2775369882583618]], "avg_entropy": [2.2684795664530246], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pig", "score_of_biggest_cluster": 0.10822208225727081, "score_of_first_answer": 0.10822208225727081, "max_score_of_first_answer": 0.9994308352470398, "min_score_of_first_answer": 0.0007432835409417748, "avg_score": 0.10822208225727081, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3771687619349599, "std_of_first_answer": 0.3771687619349599, "avg_entropy_of_first_answer": 2.2684795664530246, "lex_sim": 1.0}
{"question": "mention the chemical change that proinsulin undergo to be able to act as mature insulin", "answer": ["conversion of proinsulin to insulin (proinsulin to insulin)"], "scores": [[0.035108279436826706, 0.39247533679008484, 0.7683482766151428, 0.9001032710075378, 0.9944466352462769, 0.8844318389892578, 0.7722302079200745, 0.004548551980406046, 0.08846157789230347, 0.5130888819694519, 0.9926119446754456, 0.48530468344688416, 0.7520620226860046, 0.7468109130859375]], "normalized_score": [0.3466309905052185], "tokens": [["\u0120conversion", "\u0120of", "\u0120pro", "ins", "ulin", "\u0120to", "\u0120insulin", "\u0120(", "pro", "ins", "ulin", "\u0120to", "\u0120insulin", ")"]], "entropy": [[5.92824125289917, 1.3940989971160889, 1.5947113037109375, 0.6973891258239746, 0.04444608464837074, 0.6770095229148865, 1.0439773797988892, 0.5449889898300171, 5.3511962890625, 2.449993133544922, 0.061510682106018066, 3.067209482192993, 1.3309080600738525, 1.3486583232879639]], "avg_entropy": [1.823881330500756], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "post translational modification", "score_of_biggest_cluster": 0.3466309905052185, "score_of_first_answer": 0.3466309905052185, "max_score_of_first_answer": 0.9944466352462769, "min_score_of_first_answer": 0.004548551980406046, "avg_score": 0.3466309905052185, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3361694331970425, "std_of_first_answer": 0.3361694331970425, "avg_entropy_of_first_answer": 1.823881330500756, "lex_sim": 1.0}
{"question": "when did earth's atmosphere change due to living organisms", "answer": ["450 million years ago"], "scores": [[0.012303568422794342, 0.7200090289115906, 0.9753784537315369, 0.9497110843658447]], "normalized_score": [0.30097708106040955], "tokens": [["\u0120450", "\u0120million", "\u0120years", "\u0120ago"]], "entropy": [[5.841292381286621, 1.4378299713134766, 0.18256337940692902, 0.2917129695415497]], "avg_entropy": [1.938349675387144], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "around 2.45\u00a0billion years ago", "score_of_biggest_cluster": 0.30097708106040955, "score_of_first_answer": 0.30097708106040955, "max_score_of_first_answer": 0.9753784537315369, "min_score_of_first_answer": 0.012303568422794342, "avg_score": 0.30097708106040955, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3893687596392045, "std_of_first_answer": 0.3893687596392045, "avg_entropy_of_first_answer": 1.938349675387144, "lex_sim": 1.0}
{"question": "who sang the end of the world as we know it", "answer": ["Elton john"], "scores": [[0.05705522745847702, 0.8700842261314392, 0.08163030445575714]], "normalized_score": [0.15942977368831635], "tokens": [["\u0120El", "ton", "\u0120john"]], "entropy": [[5.7467474937438965, 0.985649049282074, 0.508861780166626]], "avg_entropy": [2.413752774397532], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "R.E.M.", "score_of_biggest_cluster": 0.15942977368831635, "score_of_first_answer": 0.15942977368831635, "max_score_of_first_answer": 0.8700842261314392, "min_score_of_first_answer": 0.05705522745847702, "avg_score": 0.15942977368831635, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3776064491119387, "std_of_first_answer": 0.3776064491119387, "avg_entropy_of_first_answer": 2.413752774397532, "lex_sim": 1.0}
{"question": "what is the most current adobe flash player version", "answer": ["20"], "scores": [[0.06651817262172699]], "normalized_score": [0.06651816517114639], "tokens": [["\u012020"]], "entropy": [[3.980586051940918]], "avg_entropy": [3.980586051940918], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "28.0.0.137", "score_of_biggest_cluster": 0.06651816517114639, "score_of_first_answer": 0.06651816517114639, "max_score_of_first_answer": 0.06651817262172699, "min_score_of_first_answer": 0.06651817262172699, "avg_score": 0.06651816517114639, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.980586051940918, "lex_sim": 1.0}
{"question": "what is the minimum wage in france per hour", "answer": ["945.00 euros"], "scores": [[0.023413412272930145, 0.006033923476934433, 0.050937287509441376, 0.1976574808359146, 0.3025723099708557]], "normalized_score": [0.05330507084727287], "tokens": [["\u01209", "45", ".", "00", "\u0120euros"]], "entropy": [[4.854392051696777, 3.6878457069396973, 2.905930519104004, 4.231024742126465, 2.201200485229492]], "avg_entropy": [3.576078701019287], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "11.16", "score_of_biggest_cluster": 0.05330507084727287, "score_of_first_answer": 0.05330507084727287, "max_score_of_first_answer": 0.3025723099708557, "min_score_of_first_answer": 0.006033923476934433, "avg_score": 0.05330507084727287, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1152172437553061, "std_of_first_answer": 0.1152172437553061, "avg_entropy_of_first_answer": 3.576078701019287, "lex_sim": 1.0}
{"question": "who sings far from the home i love in fiddler on the roof", "answer": ["Danny Boy"], "scores": [[0.009513957425951958, 0.021025873720645905]], "normalized_score": [0.01414352748543024], "tokens": [["\u0120Danny", "\u0120Boy"]], "entropy": [[5.653733730316162, 2.7204768657684326]], "avg_entropy": [4.187105298042297], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hodel", "score_of_biggest_cluster": 0.01414352748543024, "score_of_first_answer": 0.01414352748543024, "max_score_of_first_answer": 0.021025873720645905, "min_score_of_first_answer": 0.009513957425951958, "avg_score": 0.01414352748543024, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.005755958147346973, "std_of_first_answer": 0.005755958147346973, "avg_entropy_of_first_answer": 4.187105298042297, "lex_sim": 1.0}
{"question": "when was i don't like mondays released", "answer": ["released in 2012"], "scores": [[0.0024413003120571375, 0.318338543176651, 0.022301794961094856]], "normalized_score": [0.02587915025651455], "tokens": [["\u0120released", "\u0120in", "\u01202012"]], "entropy": [[5.0072431564331055, 3.2577767372131348, 4.461289882659912]], "avg_entropy": [4.242103258768718], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1979", "score_of_biggest_cluster": 0.02587915025651455, "score_of_first_answer": 0.02587915025651455, "max_score_of_first_answer": 0.318338543176651, "min_score_of_first_answer": 0.0024413003120571375, "avg_score": 0.02587915025651455, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14446193792876041, "std_of_first_answer": 0.14446193792876041, "avg_entropy_of_first_answer": 4.242103258768718, "lex_sim": 1.0}
{"question": "what is the name of india 29 state", "answer": ["West Bengal"], "scores": [[0.01558695174753666, 0.9191933274269104]], "normalized_score": [0.1196971982717514], "tokens": [["\u0120West", "\u0120Bengal"]], "entropy": [[4.435054302215576, 0.5581709146499634]], "avg_entropy": [2.4966126084327698], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rajasthan", "score_of_biggest_cluster": 0.1196971982717514, "score_of_first_answer": 0.1196971982717514, "max_score_of_first_answer": 0.9191933274269104, "min_score_of_first_answer": 0.01558695174753666, "avg_score": 0.1196971982717514, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45180318783968687, "std_of_first_answer": 0.45180318783968687, "avg_entropy_of_first_answer": 2.4966126084327698, "lex_sim": 1.0}
{"question": "what does the c stand for chrysler 300c", "answer": ["300c"], "scores": [[0.08899645507335663, 0.14349694550037384]], "normalized_score": [0.11300759762525558], "tokens": [["\u0120300", "c"]], "entropy": [[4.836041450500488, 3.4072487354278564]], "avg_entropy": [4.121645092964172], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "coupe", "score_of_biggest_cluster": 0.11300759762525558, "score_of_first_answer": 0.11300759762525558, "max_score_of_first_answer": 0.14349694550037384, "min_score_of_first_answer": 0.08899645507335663, "avg_score": 0.11300759762525558, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.027250245213508606, "std_of_first_answer": 0.027250245213508606, "avg_entropy_of_first_answer": 4.121645092964172, "lex_sim": 1.0}
{"question": "who played ethel on the doris day show", "answer": ["Doris"], "scores": [[0.19883909821510315, 0.9937019348144531]], "normalized_score": [0.444507360458374], "tokens": [["\u0120Dor", "is"]], "entropy": [[4.729220390319824, 0.05427035689353943]], "avg_entropy": [2.391745373606682], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Carol Worthington", "score_of_biggest_cluster": 0.444507360458374, "score_of_first_answer": 0.444507360458374, "max_score_of_first_answer": 0.9937019348144531, "min_score_of_first_answer": 0.19883909821510315, "avg_score": 0.444507360458374, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.397431418299675, "std_of_first_answer": 0.397431418299675, "avg_entropy_of_first_answer": 2.391745373606682, "lex_sim": 1.0}
{"question": "who lives in the blue house in balamory", "answer": ["Dr. Dickory McCallum"], "scores": [[0.0036416028160601854, 0.794510006904602, 0.001200478058308363, 0.006442217621952295, 0.003698470303788781, 0.15176720917224884, 0.5005258917808533]], "normalized_score": [0.025105716660618782], "tokens": [["\u0120Dr", ".", "\u0120Dick", "ory", "\u0120McC", "all", "um"]], "entropy": [[6.719921588897705, 1.8762683868408203, 6.5882463455200195, 5.269280433654785, 5.0646209716796875, 4.441372871398926, 1.5919628143310547]], "avg_entropy": [4.507381916046143], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Edie McCredie", "score_of_biggest_cluster": 0.025105716660618782, "score_of_first_answer": 0.025105716660618782, "max_score_of_first_answer": 0.794510006904602, "min_score_of_first_answer": 0.001200478058308363, "avg_score": 0.025105716660618782, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2926762671415982, "std_of_first_answer": 0.2926762671415982, "avg_entropy_of_first_answer": 4.507381916046143, "lex_sim": 1.0}
{"question": "what was icq as related to instant messaging", "answer": ["Internet chat"], "scores": [[0.04884300380945206, 0.1362336426973343]], "normalized_score": [0.08157242834568024], "tokens": [["\u0120Internet", "\u0120chat"]], "entropy": [[5.1195573806762695, 3.946895122528076]], "avg_entropy": [4.533226251602173], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "an instant messaging client", "score_of_biggest_cluster": 0.08157242834568024, "score_of_first_answer": 0.08157242834568024, "max_score_of_first_answer": 0.1362336426973343, "min_score_of_first_answer": 0.04884300380945206, "avg_score": 0.08157242834568024, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.043695319443941116, "std_of_first_answer": 0.043695319443941116, "avg_entropy_of_first_answer": 4.533226251602173, "lex_sim": 1.0}
{"question": "the group that officially elects the president of the united states is called", "answer": ["Electoral College"], "scores": [[0.16333907842636108, 0.908477246761322]], "normalized_score": [0.3852140009403229], "tokens": [["\u0120Electoral", "\u0120College"]], "entropy": [[2.330052375793457, 0.3328346908092499]], "avg_entropy": [1.3314435333013535], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the U.S. Electoral College", "score_of_biggest_cluster": 0.3852140009403229, "score_of_first_answer": 0.3852140009403229, "max_score_of_first_answer": 0.908477246761322, "min_score_of_first_answer": 0.16333907842636108, "avg_score": 0.3852140009403229, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.37256908416748047, "std_of_first_answer": 0.37256908416748047, "avg_entropy_of_first_answer": 1.3314435333013535, "lex_sim": 1.0}
{"question": "when does season 8 for blue bloods start", "answer": ["September 24, 2012"], "scores": [[0.10988105088472366, 0.04912701994180679, 0.10683566331863403, 0.028067663311958313]], "normalized_score": [0.06342952698469162], "tokens": [["\u0120September", "\u012024", ",", "\u01202012"]], "entropy": [[4.178832054138184, 3.5014991760253906, 0.9157722592353821, 2.65484881401062]], "avg_entropy": [2.812738075852394], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "September 29, 2017", "score_of_biggest_cluster": 0.06342952698469162, "score_of_first_answer": 0.06342952698469162, "max_score_of_first_answer": 0.10988105088472366, "min_score_of_first_answer": 0.028067663311958313, "avg_score": 0.06342952698469162, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.03568257531211903, "std_of_first_answer": 0.03568257531211903, "avg_entropy_of_first_answer": 2.812738075852394, "lex_sim": 1.0}
{"question": "where is the light between two oceans filmed", "answer": ["the english channel"], "scores": [[0.06316334754228592, 0.011100114323198795, 0.9280790090560913]], "normalized_score": [0.08665477484464645], "tokens": [["\u0120the", "\u0120english", "\u0120channel"]], "entropy": [[6.278362274169922, 6.2112202644348145, 0.638474702835083]], "avg_entropy": [4.3760190804799395], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Australia", "score_of_biggest_cluster": 0.08665477484464645, "score_of_first_answer": 0.08665477484464645, "max_score_of_first_answer": 0.9280790090560913, "min_score_of_first_answer": 0.011100114323198795, "avg_score": 0.08665477484464645, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42053404871671424, "std_of_first_answer": 0.42053404871671424, "avg_entropy_of_first_answer": 4.3760190804799395, "lex_sim": 1.0}
{"question": "in which sea pearl is found in india", "answer": ["Banda Aceh"], "scores": [[0.0037542206700891256, 0.4109911024570465, 0.15688110888004303, 0.9913694262504578]], "normalized_score": [0.1244628056883812], "tokens": [["\u0120B", "anda", "\u0120Ace", "h"]], "entropy": [[5.117612838745117, 3.3256986141204834, 2.9784913063049316, 0.08138647675514221]], "avg_entropy": [2.8757973089814186], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Indian Ocean", "score_of_biggest_cluster": 0.1244628056883812, "score_of_first_answer": 0.1244628056883812, "max_score_of_first_answer": 0.9913694262504578, "min_score_of_first_answer": 0.0037542206700891256, "avg_score": 0.1244628056883812, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37603649718699533, "std_of_first_answer": 0.37603649718699533, "avg_entropy_of_first_answer": 2.8757973089814186, "lex_sim": 1.0}
{"question": "what is the official symbol of the carnival of quebec", "answer": ["maple leaf"], "scores": [[0.010417833924293518, 0.9167824387550354]], "normalized_score": [0.09772864729166031], "tokens": [["\u0120maple", "\u0120leaf"]], "entropy": [[6.000081539154053, 0.45869362354278564]], "avg_entropy": [3.229387581348419], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bonhomme Carnaval", "score_of_biggest_cluster": 0.09772864729166031, "score_of_first_answer": 0.09772864729166031, "max_score_of_first_answer": 0.9167824387550354, "min_score_of_first_answer": 0.010417833924293518, "avg_score": 0.09772864729166031, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45318230241537094, "std_of_first_answer": 0.45318230241537094, "avg_entropy_of_first_answer": 3.229387581348419, "lex_sim": 1.0}
{"question": "who did puerto rico belong to before the u.s", "answer": ["spanish colony"], "scores": [[0.06690312176942825, 0.7865662574768066, 0.03027862124145031]], "normalized_score": [0.11679903417825699], "tokens": [["\u0120sp", "anish", "\u0120colony"]], "entropy": [[3.9450907707214355, 0.5315014123916626, 2.358429431915283]], "avg_entropy": [2.278340538342794], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ta\u00edno", "score_of_biggest_cluster": 0.11679903417825699, "score_of_first_answer": 0.11679903417825699, "max_score_of_first_answer": 0.7865662574768066, "min_score_of_first_answer": 0.03027862124145031, "avg_score": 0.11679903417825699, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3482060966435384, "std_of_first_answer": 0.3482060966435384, "avg_entropy_of_first_answer": 2.278340538342794, "lex_sim": 1.0}
{"question": "what is best thing i never had song about", "answer": ["My Baby Loves Me by B.B."], "scores": [[0.008256293833255768, 0.024045361205935478, 0.24490082263946533, 0.9975947737693787, 0.735549807548523, 0.09064685553312302, 0.005855526775121689, 0.4985806941986084, 0.27617013454437256, 0.9199962615966797, 0.8977763652801514]], "normalized_score": [0.16296911239624023], "tokens": [["\u0120My", "\u0120Baby", "\u0120L", "oves", "\u0120Me", "\u0120by", "\u0120B", ".", "B", ".", "\u0120King"]], "entropy": [[7.252233028411865, 6.02908182144165, 2.655949592590332, 0.022049613296985626, 1.818176031112671, 0.7150133848190308, 5.542158126831055, 2.3088831901550293, 3.9953579902648926, 0.5295832753181458, 0.5712553262710571]], "avg_entropy": [2.8581583073193375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "revenge and karma", "score_of_biggest_cluster": 0.16296911239624023, "score_of_first_answer": 0.16296911239624023, "max_score_of_first_answer": 0.9975947737693787, "min_score_of_first_answer": 0.005855526775121689, "avg_score": 0.16296911239624023, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3783582909649069, "std_of_first_answer": 0.3783582909649069, "avg_entropy_of_first_answer": 2.8581583073193375, "lex_sim": 1.0}
{"question": "what is one element a topographic map shows", "answer": ["elevation"], "scores": [[0.2263842076063156]], "normalized_score": [0.2263842076063156], "tokens": [["\u0120elevation"]], "entropy": [[4.113176345825195]], "avg_entropy": [4.113176345825195], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "relief", "score_of_biggest_cluster": 0.2263842076063156, "score_of_first_answer": 0.2263842076063156, "max_score_of_first_answer": 0.2263842076063156, "min_score_of_first_answer": 0.2263842076063156, "avg_score": 0.2263842076063156, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.113176345825195, "lex_sim": 1.0}
{"question": "where is the left anterior descending artery located", "answer": ["It is in the heart of the left ventricle."], "scores": [[0.0006068015354685485, 0.31232279539108276, 0.1473027765750885, 0.7308465838432312, 0.21930751204490662, 0.02426818199455738, 0.7396023869514465, 0.5406003594398499, 0.8181813359260559, 0.9548912644386292, 0.9880408644676208, 0.266069233417511]], "normalized_score": [0.21339558064937592], "tokens": [["\u0120It", "\u0120is", "\u0120in", "\u0120the", "\u0120heart", "\u0120of", "\u0120the", "\u0120left", "\u0120vent", "ric", "le", "."]], "entropy": [[4.213159561157227, 3.161856174468994, 3.343862295150757, 1.0648385286331177, 2.9095892906188965, 1.3694931268692017, 1.2749847173690796, 2.629727840423584, 0.9360517263412476, 0.22108791768550873, 0.09520828723907471, 1.6978126764297485]], "avg_entropy": [1.9098060118655364], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the left coronary artery", "score_of_biggest_cluster": 0.21339558064937592, "score_of_first_answer": 0.21339558064937592, "max_score_of_first_answer": 0.9880408644676208, "min_score_of_first_answer": 0.0006068015354685485, "avg_score": 0.21339558064937592, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3442378226963158, "std_of_first_answer": 0.3442378226963158, "avg_entropy_of_first_answer": 1.9098060118655364, "lex_sim": 1.0}
{"question": "how many stores does lowes have in canada", "answer": ["12,000"], "scores": [[0.011157683096826077, 0.05532419681549072, 0.5141584277153015]], "normalized_score": [0.06821218878030777], "tokens": [["\u012012", ",", "000"]], "entropy": [[5.199876308441162, 3.81961727142334, 3.4823262691497803]], "avg_entropy": [4.167273283004761], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "62", "score_of_biggest_cluster": 0.06821218878030777, "score_of_first_answer": 0.06821218878030777, "max_score_of_first_answer": 0.5141584277153015, "min_score_of_first_answer": 0.011157683096826077, "avg_score": 0.06821218878030777, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2274225830343696, "std_of_first_answer": 0.2274225830343696, "avg_entropy_of_first_answer": 4.167273283004761, "lex_sim": 1.0}
{"question": "who said have you no sense of decency", "answer": ["George Bernard Shaw"], "scores": [[0.021745294332504272, 0.45131364464759827, 0.9831284284591675]], "normalized_score": [0.21288809180259705], "tokens": [["\u0120George", "\u0120Bernard", "\u0120Shaw"]], "entropy": [[6.107909202575684, 1.8293585777282715, 0.17075297236442566]], "avg_entropy": [2.7026735842227936], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joseph Nye Welch", "score_of_biggest_cluster": 0.21288809180259705, "score_of_first_answer": 0.21288809180259705, "max_score_of_first_answer": 0.9831284284591675, "min_score_of_first_answer": 0.021745294332504272, "avg_score": 0.21288809180259705, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39322222476697194, "std_of_first_answer": 0.39322222476697194, "avg_entropy_of_first_answer": 2.7026735842227936, "lex_sim": 1.0}
{"question": "love yourself by justin bieber is about who", "answer": ["Who?"], "scores": [[0.013921638019382954, 0.23989184200763702]], "normalized_score": [0.05779002979397774], "tokens": [["\u0120Who", "?"]], "entropy": [[5.807002544403076, 2.592316150665283]], "avg_entropy": [4.19965934753418], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rihanna", "score_of_biggest_cluster": 0.05779002979397774, "score_of_first_answer": 0.05779002979397774, "max_score_of_first_answer": 0.23989184200763702, "min_score_of_first_answer": 0.013921638019382954, "avg_score": 0.05779002979397774, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11298510199412704, "std_of_first_answer": 0.11298510199412704, "avg_entropy_of_first_answer": 4.19965934753418, "lex_sim": 1.0}
{"question": "who has been appointed as the election commissioner of india", "answer": ["N. V. Subba Rao"], "scores": [[0.07475490123033524, 0.38803380727767944, 0.12580592930316925, 0.8873118162155151, 0.020230969414114952, 0.29190564155578613, 0.5401813387870789]], "normalized_score": [0.1939663589000702], "tokens": [["\u0120N", ".", "\u0120V", ".", "\u0120Sub", "ba", "\u0120Rao"]], "entropy": [[5.387743949890137, 3.12729549407959, 3.4399476051330566, 0.6330872774124146, 2.345078945159912, 1.8386809825897217, 1.4623994827270508]], "avg_entropy": [2.604890533855983], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Om Prakash Rawat", "score_of_biggest_cluster": 0.1939663589000702, "score_of_first_answer": 0.1939663589000702, "max_score_of_first_answer": 0.8873118162155151, "min_score_of_first_answer": 0.020230969414114952, "avg_score": 0.1939663589000702, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2834346593583855, "std_of_first_answer": 0.2834346593583855, "avg_entropy_of_first_answer": 2.604890533855983, "lex_sim": 1.0}
{"question": "what is the membrane on the surface of the stomach called", "answer": ["mucoid"], "scores": [[0.006433221977204084, 0.9294006824493408, 0.19524221122264862]], "normalized_score": [0.10529353469610214], "tokens": [["\u0120mu", "co", "id"]], "entropy": [[4.257952690124512, 0.49206554889678955, 2.386105537414551]], "avg_entropy": [2.378707925478617], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the visceral membrane", "score_of_biggest_cluster": 0.10529353469610214, "score_of_first_answer": 0.10529353469610214, "max_score_of_first_answer": 0.9294006824493408, "min_score_of_first_answer": 0.006433221977204084, "avg_score": 0.10529353469610214, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3981214817928185, "std_of_first_answer": 0.3981214817928185, "avg_entropy_of_first_answer": 2.378707925478617, "lex_sim": 1.0}
{"question": "which river separates the bronx in new york city from manhattan island", "answer": ["Bronx River"], "scores": [[0.2575453519821167, 0.303171843290329]], "normalized_score": [0.2794288694858551], "tokens": [["\u0120Bronx", "\u0120River"]], "entropy": [[2.6975326538085938, 1.5125460624694824]], "avg_entropy": [2.105039358139038], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Harlem River", "score_of_biggest_cluster": 0.2794288694858551, "score_of_first_answer": 0.2794288694858551, "max_score_of_first_answer": 0.303171843290329, "min_score_of_first_answer": 0.2575453519821167, "avg_score": 0.2794288694858551, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02281324565410614, "std_of_first_answer": 0.02281324565410614, "avg_entropy_of_first_answer": 2.105039358139038, "lex_sim": 1.0}
{"question": "when did korea separate into north and south", "answer": ["1948"], "scores": [[0.2957911193370819]], "normalized_score": [0.2957911193370819], "tokens": [["\u01201948"]], "entropy": [[3.5612359046936035]], "avg_entropy": [3.5612359046936035], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1945", "score_of_biggest_cluster": 0.2957911193370819, "score_of_first_answer": 0.2957911193370819, "max_score_of_first_answer": 0.2957911193370819, "min_score_of_first_answer": 0.2957911193370819, "avg_score": 0.2957911193370819, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.5612359046936035, "lex_sim": 1.0}
{"question": "who has the talismans in sailor moon s", "answer": ["Sailor Senshi"], "scores": [[0.3353134095668793, 0.047171298414468765, 0.9989330172538757]], "normalized_score": [0.25093141198158264], "tokens": [["\u0120Sailor", "\u0120Sens", "hi"]], "entropy": [[4.800066947937012, 2.2159852981567383, 0.01203235425055027]], "avg_entropy": [2.3426948667814336], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Haruka and Michiru", "score_of_biggest_cluster": 0.25093141198158264, "score_of_first_answer": 0.25093141198158264, "max_score_of_first_answer": 0.9989330172538757, "min_score_of_first_answer": 0.047171298414468765, "avg_score": 0.25093141198158264, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3985065494197824, "std_of_first_answer": 0.3985065494197824, "avg_entropy_of_first_answer": 2.3426948667814336, "lex_sim": 1.0}
{"question": "where does the saskatchewan river start and end", "answer": ["it starts at lake basin and ends at lake mckay"], "scores": [[0.014197823591530323, 0.5850833058357239, 0.2465560883283615, 0.18059523403644562, 0.002056561643257737, 0.49197253584861755, 0.839185893535614, 0.7489748597145081, 0.2577492296695709, 0.010196915827691555, 0.09272686392068863, 0.13472704589366913]], "normalized_score": [0.1185721829533577], "tokens": [["\u0120it", "\u0120starts", "\u0120at", "\u0120lake", "\u0120basin", "\u0120and", "\u0120ends", "\u0120at", "\u0120lake", "\u0120m", "ck", "ay"]], "entropy": [[5.401983261108398, 2.0002224445343018, 1.4955660104751587, 3.078841209411621, 4.796319961547852, 3.2829360961914062, 1.139115333557129, 1.1699461936950684, 3.451993465423584, 5.550388336181641, 3.4613733291625977, 1.0932953357696533]], "avg_entropy": [2.9934984147548676], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "central Saskatchewan", "score_of_biggest_cluster": 0.1185721829533577, "score_of_first_answer": 0.1185721829533577, "max_score_of_first_answer": 0.839185893535614, "min_score_of_first_answer": 0.002056561643257737, "avg_score": 0.1185721829533577, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.281963300473562, "std_of_first_answer": 0.281963300473562, "avg_entropy_of_first_answer": 2.9934984147548676, "lex_sim": 1.0}
{"question": "who won britain's next top model 2016", "answer": ["Kate Humble"], "scores": [[0.00622810423374176, 0.015133611857891083, 0.7656604051589966]], "normalized_score": [0.04163369908928871], "tokens": [["\u0120Kate", "\u0120H", "umble"]], "entropy": [[6.77875280380249, 5.209423542022705, 1.3881142139434814]], "avg_entropy": [4.4587635199228925], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chloe Keenan", "score_of_biggest_cluster": 0.04163369908928871, "score_of_first_answer": 0.04163369908928871, "max_score_of_first_answer": 0.7656604051589966, "min_score_of_first_answer": 0.00622810423374176, "avg_score": 0.04163369908928871, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3559193409276744, "std_of_first_answer": 0.3559193409276744, "avg_entropy_of_first_answer": 4.4587635199228925, "lex_sim": 1.0}
{"question": "what is the population of fayetteville north carolina", "answer": ["about 115,000"], "scores": [[0.00818201806396246, 0.0026135342195630074, 0.8093944191932678, 0.8552422523498535]], "normalized_score": [0.06202753633260727], "tokens": [["\u0120about", "\u0120115", ",", "000"]], "entropy": [[6.031712532043457, 5.379760265350342, 1.0287244319915771, 1.2228096723556519]], "avg_entropy": [3.415751725435257], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "204,408 in 2013", "score_of_biggest_cluster": 0.06202753633260727, "score_of_first_answer": 0.06202753633260727, "max_score_of_first_answer": 0.8552422523498535, "min_score_of_first_answer": 0.0026135342195630074, "avg_score": 0.06202753633260727, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41378259021109076, "std_of_first_answer": 0.41378259021109076, "avg_entropy_of_first_answer": 3.415751725435257, "lex_sim": 1.0}
{"question": "who hung the lanterns in the old north church", "answer": ["Mary, mother of God"], "scores": [[0.012101881206035614, 0.06813478469848633, 0.46200376749038696, 0.9965994954109192, 0.1515982300043106]], "normalized_score": [0.141911119222641], "tokens": [["\u0120Mary", ",", "\u0120mother", "\u0120of", "\u0120God"]], "entropy": [[6.510214805603027, 5.249344348907471, 2.603382110595703, 0.03765043988823891, 1.5234652757644653]], "avg_entropy": [3.184811396151781], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Robert Newman", "score_of_biggest_cluster": 0.141911119222641, "score_of_first_answer": 0.141911119222641, "max_score_of_first_answer": 0.9965994954109192, "min_score_of_first_answer": 0.012101881206035614, "avg_score": 0.141911119222641, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3641188348414297, "std_of_first_answer": 0.3641188348414297, "avg_entropy_of_first_answer": 3.184811396151781, "lex_sim": 1.0}
{"question": "input that tells the computer which task to execute is", "answer": ["hello"], "scores": [[0.00677523436024785]], "normalized_score": [0.006775235757231712], "tokens": [["\u0120hello"]], "entropy": [[6.698598861694336]], "avg_entropy": [6.698598861694336], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "source code", "score_of_biggest_cluster": 0.006775235757231712, "score_of_first_answer": 0.006775235757231712, "max_score_of_first_answer": 0.00677523436024785, "min_score_of_first_answer": 0.00677523436024785, "avg_score": 0.006775235757231712, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.698598861694336, "lex_sim": 1.0}
{"question": "who wins america's next top model cycle 20", "answer": ["Tyra Banks"], "scores": [[0.03646694868803024, 0.991549015045166, 0.8894293904304504]], "normalized_score": [0.31801068782806396], "tokens": [["\u0120Ty", "ra", "\u0120Banks"]], "entropy": [[6.872000694274902, 0.08627964556217194, 0.7777858972549438]], "avg_entropy": [2.5786887456973395], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jourdan Miller", "score_of_biggest_cluster": 0.31801068782806396, "score_of_first_answer": 0.31801068782806396, "max_score_of_first_answer": 0.991549015045166, "min_score_of_first_answer": 0.03646694868803024, "avg_score": 0.31801068782806396, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42819454278421654, "std_of_first_answer": 0.42819454278421654, "avg_entropy_of_first_answer": 2.5786887456973395, "lex_sim": 1.0}
{"question": "civil war what did the north fight for", "answer": ["abolishment of slavery"], "scores": [[0.0036125436890870333, 0.4113311171531677, 0.9654335975646973, 0.9472483396530151]], "normalized_score": [0.1919984519481659], "tokens": [["\u0120abol", "ishment", "\u0120of", "\u0120slavery"]], "entropy": [[4.93451452255249, 0.8666462898254395, 0.23343199491500854, 0.3693717420101166]], "avg_entropy": [1.6009911373257637], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "maintaining the republic", "score_of_biggest_cluster": 0.1919984519481659, "score_of_first_answer": 0.1919984519481659, "max_score_of_first_answer": 0.9654335975646973, "min_score_of_first_answer": 0.0036125436890870333, "avg_score": 0.1919984519481659, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4012753280188309, "std_of_first_answer": 0.4012753280188309, "avg_entropy_of_first_answer": 1.6009911373257637, "lex_sim": 1.0}
{"question": "who made the nuclear model of an atom", "answer": ["Ernest O. Lawrence"], "scores": [[0.23532508313655853, 0.08336036652326584, 0.4262502193450928, 0.981696367263794]], "normalized_score": [0.3010006248950958], "tokens": [["\u0120Ernest", "\u0120O", ".", "\u0120Lawrence"]], "entropy": [[4.541301727294922, 2.2464373111724854, 3.75596022605896, 0.18526917695999146]], "avg_entropy": [2.6822421103715897], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ernest Rutherford", "score_of_biggest_cluster": 0.3010006248950958, "score_of_first_answer": 0.3010006248950958, "max_score_of_first_answer": 0.981696367263794, "min_score_of_first_answer": 0.08336036652326584, "avg_score": 0.3010006248950958, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3400107738305626, "std_of_first_answer": 0.3400107738305626, "avg_entropy_of_first_answer": 2.6822421103715897, "lex_sim": 1.0}
{"question": "where is the deepest lake in the us located", "answer": ["Death Valley"], "scores": [[0.011919171549379826, 0.7164826393127441]], "normalized_score": [0.09241148084402084], "tokens": [["\u0120Death", "\u0120Valley"]], "entropy": [[3.0566208362579346, 1.4323816299438477]], "avg_entropy": [2.244501233100891], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Oregon", "score_of_biggest_cluster": 0.09241148084402084, "score_of_first_answer": 0.09241148084402084, "max_score_of_first_answer": 0.7164826393127441, "min_score_of_first_answer": 0.011919171549379826, "avg_score": 0.09241148084402084, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35228173388168216, "std_of_first_answer": 0.35228173388168216, "avg_entropy_of_first_answer": 2.244501233100891, "lex_sim": 1.0}
{"question": "who sang gonna sit right down and write myself a letter", "answer": ["Johnny Cash"], "scores": [[0.04965797811746597, 0.6695236563682556]], "normalized_score": [0.18233811855316162], "tokens": [["\u0120Johnny", "\u0120Cash"]], "entropy": [[5.77065372467041, 2.532160758972168]], "avg_entropy": [4.151407241821289], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fats Waller", "score_of_biggest_cluster": 0.18233811855316162, "score_of_first_answer": 0.18233811855316162, "max_score_of_first_answer": 0.6695236563682556, "min_score_of_first_answer": 0.04965797811746597, "avg_score": 0.18233811855316162, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3099328391253948, "std_of_first_answer": 0.3099328391253948, "avg_entropy_of_first_answer": 4.151407241821289, "lex_sim": 1.0}
{"question": "whats the difference between tomato paste and tomato puree", "answer": ["tomato puree is extracted from tomato paste."], "scores": [[0.1757015436887741, 0.3316067159175873, 0.9816734790802002, 0.6107550263404846, 0.00289530330337584, 0.7047544121742249, 0.09390980750322342, 0.3600771129131317, 0.09821075946092606, 0.016674796119332314, 0.0019485661759972572, 0.03853583708405495, 0.3686997592449188, 0.07163647562265396, 0.040933284908533096, 0.009808328934013844, 0.07069550454616547, 0.009060095995664597, 0.5401158928871155, 0.4640132486820221]], "normalized_score": [0.08612805604934692], "tokens": [["\u0120tomato", "\u0120pure", "e", "\u0120is", "\u0120extracted", "\u0120from", "\u0120tomato", "\u0120paste", ".", "\u0120it", "\u0120will", "\u0120always", "\u0120be", "\u0120a", "\u0120solid", "\u0120state", "\u0120at", "\u0120all", "\u0120times", "."]], "entropy": [[4.132866859436035, 1.299066424369812, 0.14206360280513763, 1.602553367614746, 4.331923007965088, 1.61422860622406, 2.567922592163086, 3.1655313968658447, 1.4302170276641846, 1.562455654144287, 2.1995038986206055, 3.301846981048584, 2.1642637252807617, 4.511587619781494, 4.681995391845703, 3.4877028465270996, 2.9403131008148193, 1.1098544597625732, 1.528775691986084, 1.8251852989196777]], "avg_entropy": [2.479992877691984], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "consistency", "score_of_biggest_cluster": 0.08612805604934692, "score_of_first_answer": 0.08612805604934692, "max_score_of_first_answer": 0.9816734790802002, "min_score_of_first_answer": 0.0019485661759972572, "avg_score": 0.08612805604934692, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27687655671902345, "std_of_first_answer": 0.27687655671902345, "avg_entropy_of_first_answer": 2.479992877691984, "lex_sim": 1.0}
{"question": "who played bat masterson in the tv series", "answer": ["bat masterson"], "scores": [[0.01561847422271967, 0.4329495429992676, 0.9932330250740051]], "normalized_score": [0.18867264688014984], "tokens": [["\u0120bat", "\u0120masters", "on"]], "entropy": [[5.622442245483398, 2.3170313835144043, 0.07123731076717377]], "avg_entropy": [2.670236979921659], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gene Barry", "score_of_biggest_cluster": 0.18867264688014984, "score_of_first_answer": 0.18867264688014984, "max_score_of_first_answer": 0.9932330250740051, "min_score_of_first_answer": 0.01561847422271967, "avg_score": 0.18867264688014984, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4005292347893013, "std_of_first_answer": 0.4005292347893013, "avg_entropy_of_first_answer": 2.670236979921659, "lex_sim": 1.0}
{"question": "where does blood go when it leaves the pulmonary artery", "answer": ["circulatory system"], "scores": [[0.019987361505627632, 0.4578516185283661, 0.9666814208030701]], "normalized_score": [0.20681776106357574], "tokens": [["\u0120circ", "ulatory", "\u0120system"]], "entropy": [[4.406094074249268, 0.9364503622055054, 0.2107006311416626]], "avg_entropy": [1.8510816891988118], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "lungs", "score_of_biggest_cluster": 0.20681776106357574, "score_of_first_answer": 0.20681776106357574, "max_score_of_first_answer": 0.9666814208030701, "min_score_of_first_answer": 0.019987361505627632, "avg_score": 0.20681776106357574, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38684802028273735, "std_of_first_answer": 0.38684802028273735, "avg_entropy_of_first_answer": 1.8510816891988118, "lex_sim": 1.0}
{"question": "who validated the civil rights movement by proclaiming we shall overcome", "answer": ["Martin Luther King, Jr."], "scores": [[0.72222501039505, 0.9894300699234009, 0.9799956679344177, 0.16891950368881226, 0.9557889699935913, 0.8051734566688538]], "normalized_score": [0.670711100101471], "tokens": [["\u0120Martin", "\u0120Luther", "\u0120King", ",", "\u0120Jr", "."]], "entropy": [[1.69148588180542, 0.09298425167798996, 0.1345236897468567, 1.2296439409255981, 0.36749228835105896, 0.5500491857528687]], "avg_entropy": [0.6776965397099654], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "President Lyndon Johnson", "score_of_biggest_cluster": 0.670711100101471, "score_of_first_answer": 0.670711100101471, "max_score_of_first_answer": 0.9894300699234009, "min_score_of_first_answer": 0.16891950368881226, "avg_score": 0.670711100101471, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28623483095857355, "std_of_first_answer": 0.28623483095857355, "avg_entropy_of_first_answer": 0.6776965397099654, "lex_sim": 1.0}
{"question": "what is the common name for gravitational force", "answer": ["Mass"], "scores": [[0.0021577775478363037]], "normalized_score": [0.00215777731500566], "tokens": [["\u0120Mass"]], "entropy": [[3.5391273498535156]], "avg_entropy": [3.5391273498535156], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gravity", "score_of_biggest_cluster": 0.00215777731500566, "score_of_first_answer": 0.00215777731500566, "max_score_of_first_answer": 0.0021577775478363037, "min_score_of_first_answer": 0.0021577775478363037, "avg_score": 0.00215777731500566, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.5391273498535156, "lex_sim": 1.0}
{"question": "who does the voice of salem the cat", "answer": ["John Williams"], "scores": [[0.03600594773888588, 0.012518309988081455]], "normalized_score": [0.021230490878224373], "tokens": [["\u0120John", "\u0120Williams"]], "entropy": [[5.642437934875488, 4.100407600402832]], "avg_entropy": [4.87142276763916], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nicholas \"Nick\" Bakay", "score_of_biggest_cluster": 0.021230490878224373, "score_of_first_answer": 0.021230490878224373, "max_score_of_first_answer": 0.03600594773888588, "min_score_of_first_answer": 0.012518309988081455, "avg_score": 0.021230490878224373, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.011743818875402212, "std_of_first_answer": 0.011743818875402212, "avg_entropy_of_first_answer": 4.87142276763916, "lex_sim": 1.0}
{"question": "when is season 7 race to the edge coming out", "answer": ["2014"], "scores": [[0.004509622231125832]], "normalized_score": [0.004509623162448406], "tokens": [["\u01202014"]], "entropy": [[4.4239020347595215]], "avg_entropy": [4.4239020347595215], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "February 16, 2018", "score_of_biggest_cluster": 0.004509623162448406, "score_of_first_answer": 0.004509623162448406, "max_score_of_first_answer": 0.004509622231125832, "min_score_of_first_answer": 0.004509622231125832, "avg_score": 0.004509623162448406, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.4239020347595215, "lex_sim": 1.0}
{"question": "in 1945 which party came into power in england", "answer": ["Labour"], "scores": [[0.5197412967681885]], "normalized_score": [0.5197412967681885], "tokens": [["\u0120Labour"]], "entropy": [[2.6572823524475098]], "avg_entropy": [2.6572823524475098], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Labour Party", "score_of_biggest_cluster": 0.5197412967681885, "score_of_first_answer": 0.5197412967681885, "max_score_of_first_answer": 0.5197412967681885, "min_score_of_first_answer": 0.5197412967681885, "avg_score": 0.5197412967681885, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.6572823524475098, "lex_sim": 1.0}
{"question": "what is the job of the whip in congress", "answer": ["whip"], "scores": [[0.09559271484613419]], "normalized_score": [0.09559271484613419], "tokens": [["\u0120whip"]], "entropy": [[5.4985551834106445]], "avg_entropy": [5.4985551834106445], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "to ensure party discipline", "score_of_biggest_cluster": 0.09559271484613419, "score_of_first_answer": 0.09559271484613419, "max_score_of_first_answer": 0.09559271484613419, "min_score_of_first_answer": 0.09559271484613419, "avg_score": 0.09559271484613419, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.4985551834106445, "lex_sim": 1.0}
{"question": "what is the account number of airtel payment bank", "answer": ["477"], "scores": [[0.014804650098085403, 0.003768072696402669]], "normalized_score": [0.007468936033546925], "tokens": [["\u01204", "77"]], "entropy": [[5.944392204284668, 5.991054058074951]], "avg_entropy": [5.96772313117981], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Your Airtel mobile number", "score_of_biggest_cluster": 0.007468936033546925, "score_of_first_answer": 0.007468936033546925, "max_score_of_first_answer": 0.014804650098085403, "min_score_of_first_answer": 0.003768072696402669, "avg_score": 0.007468936033546925, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.005518288700841367, "std_of_first_answer": 0.005518288700841367, "avg_entropy_of_first_answer": 5.96772313117981, "lex_sim": 1.0}
{"question": "who scored the most goals in premier league season", "answer": ["Carlos Tevez for Manchester United"], "scores": [[0.0017675330163910985, 0.6027116775512695, 0.9999488592147827, 0.0003298980591353029, 0.47187691926956177, 0.20936821401119232]], "normalized_score": [0.05711660534143448], "tokens": [["\u0120Carlos", "\u0120Te", "vez", "\u0120for", "\u0120Manchester", "\u0120United"]], "entropy": [[4.83390998840332, 1.9888837337493896, 0.0007499475032091141, 0.785990834236145, 3.1494972705841064, 1.7892506122589111]], "avg_entropy": [2.0913803977891803], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alan Shearer", "score_of_biggest_cluster": 0.05711660534143448, "score_of_first_answer": 0.05711660534143448, "max_score_of_first_answer": 0.9999488592147827, "min_score_of_first_answer": 0.0003298980591353029, "avg_score": 0.05711660534143448, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35559734560971556, "std_of_first_answer": 0.35559734560971556, "avg_entropy_of_first_answer": 2.0913803977891803, "lex_sim": 1.0}
{"question": "what kind of dog is nana in snow dogs", "answer": ["golden retriever"], "scores": [[0.014450774528086185, 0.8500185608863831, 0.9960765242576599]], "normalized_score": [0.23042918741703033], "tokens": [["\u0120golden", "\u0120retri", "ever"]], "entropy": [[5.31220006942749, 0.8304340243339539, 0.036228179931640625]], "avg_entropy": [2.059620757897695], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "a Border Collie", "score_of_biggest_cluster": 0.23042918741703033, "score_of_first_answer": 0.23042918741703033, "max_score_of_first_answer": 0.9960765242576599, "min_score_of_first_answer": 0.014450774528086185, "avg_score": 0.23042918741703033, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4324472436412225, "std_of_first_answer": 0.4324472436412225, "avg_entropy_of_first_answer": 2.059620757897695, "lex_sim": 1.0}
{"question": "nickname given to railroad executives due to shady practices of their businesses", "answer": ["Boss"], "scores": [[0.011106782592833042]], "normalized_score": [0.011106782592833042], "tokens": [["\u0120Boss"]], "entropy": [[6.922606468200684]], "avg_entropy": [6.922606468200684], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Robber baron", "score_of_biggest_cluster": 0.011106782592833042, "score_of_first_answer": 0.011106782592833042, "max_score_of_first_answer": 0.011106782592833042, "min_score_of_first_answer": 0.011106782592833042, "avg_score": 0.011106782592833042, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.922606468200684, "lex_sim": 1.0}
{"question": "where was the flag placed on the moon", "answer": ["the moon"], "scores": [[0.08119761198759079, 0.20139719545841217]], "normalized_score": [0.12787874042987823], "tokens": [["\u0120the", "\u0120moon"]], "entropy": [[5.12821102142334, 4.644071578979492]], "avg_entropy": [4.886141300201416], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hadley\u2013Apennine", "score_of_biggest_cluster": 0.12787874042987823, "score_of_first_answer": 0.12787874042987823, "max_score_of_first_answer": 0.20139719545841217, "min_score_of_first_answer": 0.08119761198759079, "avg_score": 0.12787874042987823, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06009979173541069, "std_of_first_answer": 0.06009979173541069, "avg_entropy_of_first_answer": 4.886141300201416, "lex_sim": 1.0}
{"question": "who plays the saint of killers on preacher", "answer": ["Chris Pine"], "scores": [[0.0099791269749403, 0.1759933978319168]], "normalized_score": [0.041907764971256256], "tokens": [["\u0120Chris", "\u0120Pine"]], "entropy": [[6.496243476867676, 4.034130573272705]], "avg_entropy": [5.26518702507019], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Graham McTavish", "score_of_biggest_cluster": 0.041907764971256256, "score_of_first_answer": 0.041907764971256256, "max_score_of_first_answer": 0.1759933978319168, "min_score_of_first_answer": 0.0099791269749403, "avg_score": 0.041907764971256256, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08300713542848825, "std_of_first_answer": 0.08300713542848825, "avg_entropy_of_first_answer": 5.26518702507019, "lex_sim": 1.0}
{"question": "whose new political ideas of natural rights influenced the declaration of independence", "answer": ["Alexander Hamilton"], "scores": [[0.0037440808955579996, 0.9492012858390808]], "normalized_score": [0.059614475816488266], "tokens": [["\u0120Alexander", "\u0120Hamilton"]], "entropy": [[2.661973237991333, 0.475744366645813]], "avg_entropy": [1.568858802318573], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Francis Hutcheson", "score_of_biggest_cluster": 0.059614475816488266, "score_of_first_answer": 0.059614475816488266, "max_score_of_first_answer": 0.9492012858390808, "min_score_of_first_answer": 0.0037440808955579996, "avg_score": 0.059614475816488266, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4727286024717614, "std_of_first_answer": 0.4727286024717614, "avg_entropy_of_first_answer": 1.568858802318573, "lex_sim": 1.0}
{"question": "mount everest is part of what mountain range", "answer": ["Himalayan mountain range."], "scores": [[0.7541183829307556, 0.14145663380622864, 0.0329522080719471, 0.9227034449577332, 0.002704108599573374]], "normalized_score": [0.09741071611642838], "tokens": [["\u0120Himal", "ayan", "\u0120mountain", "\u0120range", "."]], "entropy": [[1.6901850700378418, 0.9502518773078918, 1.1265240907669067, 0.47162026166915894, 0.15663594007492065]], "avg_entropy": [0.879043447971344], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Himalayas", "score_of_biggest_cluster": 0.09741071611642838, "score_of_first_answer": 0.09741071611642838, "max_score_of_first_answer": 0.9227034449577332, "min_score_of_first_answer": 0.002704108599573374, "avg_score": 0.09741071611642838, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.38826910507835066, "std_of_first_answer": 0.38826910507835066, "avg_entropy_of_first_answer": 0.879043447971344, "lex_sim": 1.0}
{"question": "ranjit sagar dam has builded over which river", "answer": ["Hooghly River"], "scores": [[0.025547366589307785, 0.9657842516899109, 0.9747957587242126, 0.10617148131132126]], "normalized_score": [0.2247951775789261], "tokens": [["\u0120Hoo", "gh", "ly", "\u0120River"]], "entropy": [[4.67067289352417, 0.28760457038879395, 0.15795759856700897, 0.8932214975357056]], "avg_entropy": [1.5023641400039196], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ravi River", "score_of_biggest_cluster": 0.2247951775789261, "score_of_first_answer": 0.2247951775789261, "max_score_of_first_answer": 0.9747957587242126, "min_score_of_first_answer": 0.025547366589307785, "avg_score": 0.2247951775789261, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4531239907241964, "std_of_first_answer": 0.4531239907241964, "avg_entropy_of_first_answer": 1.5023641400039196, "lex_sim": 1.0}
{"question": "who played booster in jingle all the way", "answer": ["Jonathan Taylor Thomas"], "scores": [[0.008514570072293282, 0.09592519700527191, 0.9893390536308289]], "normalized_score": [0.09314229339361191], "tokens": [["\u0120Jonathan", "\u0120Taylor", "\u0120Thomas"]], "entropy": [[6.502293586730957, 4.171985626220703, 0.09832291305065155]], "avg_entropy": [3.590867375334104], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Curtis Armstrong", "score_of_biggest_cluster": 0.09314229339361191, "score_of_first_answer": 0.09314229339361191, "max_score_of_first_answer": 0.9893390536308289, "min_score_of_first_answer": 0.008514570072293282, "avg_score": 0.09314229339361191, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44320118387066554, "std_of_first_answer": 0.44320118387066554, "avg_entropy_of_first_answer": 3.590867375334104, "lex_sim": 1.0}
{"question": "name the process of fusion of an egg with a sperm", "answer": ["fertilization"], "scores": [[0.44047626852989197, 0.9090865254402161]], "normalized_score": [0.6327962279319763], "tokens": [["\u0120fertil", "ization"]], "entropy": [[3.859286308288574, 0.35718297958374023]], "avg_entropy": [2.1082346439361572], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "fertilization", "score_of_biggest_cluster": 0.6327962279319763, "score_of_first_answer": 0.6327962279319763, "max_score_of_first_answer": 0.9090865254402161, "min_score_of_first_answer": 0.44047626852989197, "avg_score": 0.6327962279319763, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23430512845516205, "std_of_first_answer": 0.23430512845516205, "avg_entropy_of_first_answer": 2.1082346439361572, "lex_sim": 1.0}
{"question": "who sang the theme song for the man with the golden gun", "answer": ["James Horner"], "scores": [[0.006188713014125824, 0.5868045687675476, 0.9590222835540771]], "normalized_score": [0.1515796184539795], "tokens": [["\u0120James", "\u0120Hor", "ner"]], "entropy": [[6.367034912109375, 2.6100058555603027, 0.27782416343688965]], "avg_entropy": [3.0849549770355225], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lulu", "score_of_biggest_cluster": 0.1515796184539795, "score_of_first_answer": 0.1515796184539795, "max_score_of_first_answer": 0.9590222835540771, "min_score_of_first_answer": 0.006188713014125824, "avg_score": 0.1515796184539795, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39208171091426236, "std_of_first_answer": 0.39208171091426236, "avg_entropy_of_first_answer": 3.0849549770355225, "lex_sim": 1.0}
{"question": "who was the captain of the mayflower which brought the pilgrims to plymouth", "answer": ["Edward Winslow"], "scores": [[0.042401015758514404, 0.7106704115867615, 0.9812519550323486]], "normalized_score": [0.30922529101371765], "tokens": [["\u0120Edward", "\u0120Wins", "low"]], "entropy": [[3.2555062770843506, 2.0886268615722656, 0.160995751619339]], "avg_entropy": [1.8350429634253185], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Master Christopher Jones", "score_of_biggest_cluster": 0.30922529101371765, "score_of_first_answer": 0.30922529101371765, "max_score_of_first_answer": 0.9812519550323486, "min_score_of_first_answer": 0.042401015758514404, "avg_score": 0.30922529101371765, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3945798670357297, "std_of_first_answer": 0.3945798670357297, "avg_entropy_of_first_answer": 1.8350429634253185, "lex_sim": 1.0}
{"question": "who played shaggy in the new scooby doo movie", "answer": ["shaggy"], "scores": [[0.14110559225082397, 0.993285596370697, 0.9843430519104004]], "normalized_score": [0.5167195796966553], "tokens": [["\u0120sh", "ag", "gy"]], "entropy": [[4.415816307067871, 0.06761021912097931, 0.11840307712554932]], "avg_entropy": [1.5339432011048], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Matthew Lyn Lillard", "score_of_biggest_cluster": 0.5167195796966553, "score_of_first_answer": 0.5167195796966553, "max_score_of_first_answer": 0.993285596370697, "min_score_of_first_answer": 0.14110559225082397, "avg_score": 0.5167195796966553, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3996304045123454, "std_of_first_answer": 0.3996304045123454, "avg_entropy_of_first_answer": 1.5339432011048, "lex_sim": 1.0}
{"question": "what percentage of global man-made co2 emissions comes from worldwide aviation", "answer": ["0.16%"], "scores": [[0.10640524327754974, 0.9061706066131592, 0.008246156387031078, 0.6779661774635315]], "normalized_score": [0.1523730307817459], "tokens": [["\u01200", ".", "16", "%"]], "entropy": [[4.437676906585693, 0.44612976908683777, 4.304092884063721, 1.4000154733657837]], "avg_entropy": [2.646978758275509], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "4 percent cumulative effect", "score_of_biggest_cluster": 0.1523730307817459, "score_of_first_answer": 0.1523730307817459, "max_score_of_first_answer": 0.9061706066131592, "min_score_of_first_answer": 0.008246156387031078, "avg_score": 0.1523730307817459, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37772445567554447, "std_of_first_answer": 0.37772445567554447, "avg_entropy_of_first_answer": 2.646978758275509, "lex_sim": 1.0}
{"question": "when did the ship hector arrived in pictou", "answer": ["The arrival of the ship \"Hector\" in Pictou Harbour on December 2,1765."], "scores": [[0.008638318628072739, 0.002423110418021679, 0.7073303461074829, 0.7797199487686157, 0.18661142885684967, 0.013813645578920841, 0.6442791819572449, 0.9821162223815918, 0.9363784193992615, 0.3091675043106079, 0.9408029913902283, 0.9930114150047302, 0.021319640800356865, 0.22821272909641266, 0.026030002161860466, 0.039066459983587265, 0.8530269265174866, 0.013898670673370361, 0.007525438908487558, 0.26028427481651306]], "normalized_score": [0.12256401777267456], "tokens": [["\u0120The", "\u0120arrival", "\u0120of", "\u0120the", "\u0120ship", "\u0120\"", "H", "ector", "\"", "\u0120in", "\u0120Pict", "ou", "\u0120Harbour", "\u0120on", "\u0120December", "\u01202", ",", "17", "65", "."]], "entropy": [[4.690341949462891, 6.40335750579834, 1.4939866065979004, 1.6379153728485107, 5.083581447601318, 1.8533152341842651, 1.515034556388855, 0.12459011375904083, 0.4641436040401459, 2.7869999408721924, 0.4594864845275879, 0.07192008197307587, 2.997694253921509, 2.618483543395996, 3.40210223197937, 3.5239081382751465, 0.6137632131576538, 2.9616756439208984, 3.883720874786377, 2.0976810455322266]], "avg_entropy": [2.434185092151165], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1773", "score_of_biggest_cluster": 0.12256401777267456, "score_of_first_answer": 0.12256401777267456, "max_score_of_first_answer": 0.9930114150047302, "min_score_of_first_answer": 0.002423110418021679, "avg_score": 0.12256401777267456, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39076545173885124, "std_of_first_answer": 0.39076545173885124, "avg_entropy_of_first_answer": 2.434185092151165, "lex_sim": 1.0}
{"question": "when did the government change the retirement age", "answer": ["60"], "scores": [[0.016513507813215256]], "normalized_score": [0.016513505950570107], "tokens": [["\u012060"]], "entropy": [[5.504434108734131]], "avg_entropy": [5.504434108734131], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2015", "score_of_biggest_cluster": 0.016513505950570107, "score_of_first_answer": 0.016513505950570107, "max_score_of_first_answer": 0.016513507813215256, "min_score_of_first_answer": 0.016513507813215256, "avg_score": 0.016513505950570107, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.504434108734131, "lex_sim": 1.0}
{"question": "who has the most trophies in la liga", "answer": ["puyol"], "scores": [[0.01005484163761139, 0.013580571860074997, 0.9722638726234436]], "normalized_score": [0.05101437121629715], "tokens": [["\u0120p", "uy", "ol"]], "entropy": [[4.301941394805908, 2.156773567199707, 0.21318423748016357]], "avg_entropy": [2.223966399828593], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Real Madrid", "score_of_biggest_cluster": 0.05101437121629715, "score_of_first_answer": 0.05101437121629715, "max_score_of_first_answer": 0.9722638726234436, "min_score_of_first_answer": 0.01005484163761139, "avg_score": 0.05101437121629715, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45276095253294324, "std_of_first_answer": 0.45276095253294324, "avg_entropy_of_first_answer": 2.223966399828593, "lex_sim": 1.0}
{"question": "where was the remake of wake in fright filmed", "answer": ["London"], "scores": [[0.020777305588126183]], "normalized_score": [0.020777305588126183], "tokens": [["\u0120London"]], "entropy": [[6.731810569763184]], "avg_entropy": [6.731810569763184], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Broken", "score_of_biggest_cluster": 0.020777305588126183, "score_of_first_answer": 0.020777305588126183, "max_score_of_first_answer": 0.020777305588126183, "min_score_of_first_answer": 0.020777305588126183, "avg_score": 0.020777305588126183, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.731810569763184, "lex_sim": 1.0}
{"question": "who sings good night on the beatles white album", "answer": ["Paul McCartney and Stuart Sutcliffe"], "scores": [[0.26589342951774597, 0.6297817230224609, 0.03349274396896362, 0.005151238292455673, 0.7157849669456482, 0.9022830128669739]], "normalized_score": [0.1628599315881729], "tokens": [["\u0120Paul", "\u0120McCartney", "\u0120and", "\u0120Stuart", "\u0120Sut", "cliffe"]], "entropy": [[4.546988487243652, 2.0651984214782715, 0.3410147428512573, 3.0149152278900146, 2.3773961067199707, 0.6437140703201294]], "avg_entropy": [2.164871176083883], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ringo Starr", "score_of_biggest_cluster": 0.1628599315881729, "score_of_first_answer": 0.1628599315881729, "max_score_of_first_answer": 0.9022830128669739, "min_score_of_first_answer": 0.005151238292455673, "avg_score": 0.1628599315881729, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3437914558615868, "std_of_first_answer": 0.3437914558615868, "avg_entropy_of_first_answer": 2.164871176083883, "lex_sim": 1.0}
{"question": "what is a another name for the water cycle", "answer": ["hydrologic cycle"], "scores": [[0.354018896818161, 0.6750115156173706, 0.9935587048530579, 0.9775615334510803]], "normalized_score": [0.6940948367118835], "tokens": [["\u0120hyd", "rolog", "ic", "\u0120cycle"]], "entropy": [[3.63743257522583, 0.8637707829475403, 0.06562622636556625, 0.17618906497955322]], "avg_entropy": [1.1857546623796225], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the hydrological cycle", "score_of_biggest_cluster": 0.6940948367118835, "score_of_first_answer": 0.6940948367118835, "max_score_of_first_answer": 0.9935587048530579, "min_score_of_first_answer": 0.354018896818161, "avg_score": 0.6940948367118835, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.26150018901021127, "std_of_first_answer": 0.26150018901021127, "avg_entropy_of_first_answer": 1.1857546623796225, "lex_sim": 1.0}
{"question": "a 58-story skyscraper in san francisco is tilting and sinking", "answer": ["555 Fremont St."], "scores": [[0.008317089639604092, 0.3277912139892578, 0.9995960593223572, 0.15233555436134338, 0.4399547278881073]], "normalized_score": [0.1787806749343872], "tokens": [["\u0120555", "\u0120Frem", "ont", "\u0120St", "."]], "entropy": [[5.8630571365356445, 3.2246479988098145, 0.0048864055424928665, 1.5695013999938965, 1.4927868843078613]], "avg_entropy": [2.430975965037942], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Millennium Tower", "score_of_biggest_cluster": 0.1787806749343872, "score_of_first_answer": 0.1787806749343872, "max_score_of_first_answer": 0.9995960593223572, "min_score_of_first_answer": 0.008317089639604092, "avg_score": 0.1787806749343872, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3406001024928193, "std_of_first_answer": 0.3406001024928193, "avg_entropy_of_first_answer": 2.430975965037942, "lex_sim": 1.0}
{"question": "known as the punk poet who used poetry in their music", "answer": ["Chuck D"], "scores": [[0.005758273880928755, 0.7704420685768127]], "normalized_score": [0.06660643219947815], "tokens": [["\u0120Chuck", "\u0120D"]], "entropy": [[4.903178691864014, 1.044377088546753]], "avg_entropy": [2.9737778902053833], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Cooper Clarke", "score_of_biggest_cluster": 0.06660643219947815, "score_of_first_answer": 0.06660643219947815, "max_score_of_first_answer": 0.7704420685768127, "min_score_of_first_answer": 0.005758273880928755, "avg_score": 0.06660643219947815, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.382341897347942, "std_of_first_answer": 0.382341897347942, "avg_entropy_of_first_answer": 2.9737778902053833, "lex_sim": 1.0}
{"question": "who is considered to be the greatest english composer of the baroque period", "answer": ["John Sebastian Bach"], "scores": [[0.12929758429527283, 0.009892493486404419, 0.8659356832504272]], "normalized_score": [0.10346511006355286], "tokens": [["\u0120John", "\u0120Sebastian", "\u0120Bach"]], "entropy": [[3.9425973892211914, 4.822291374206543, 0.8185557723045349]], "avg_entropy": [3.1944815119107566], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Henry Purcell", "score_of_biggest_cluster": 0.10346511006355286, "score_of_first_answer": 0.10346511006355286, "max_score_of_first_answer": 0.8659356832504272, "min_score_of_first_answer": 0.009892493486404419, "avg_score": 0.10346511006355286, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37855033531218374, "std_of_first_answer": 0.37855033531218374, "avg_entropy_of_first_answer": 3.1944815119107566, "lex_sim": 1.0}
{"question": "what is the number of cities in texas", "answer": ["3.6 million"], "scores": [[0.006658540107309818, 0.010002147406339645, 0.026018673554062843, 0.8324520587921143]], "normalized_score": [0.03465605899691582], "tokens": [["\u01203", ".", "6", "\u0120million"]], "entropy": [[5.0864715576171875, 0.9860488176345825, 3.657935619354248, 0.7869729399681091]], "avg_entropy": [2.629357233643532], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "961", "score_of_biggest_cluster": 0.03465605899691582, "score_of_first_answer": 0.03465605899691582, "max_score_of_first_answer": 0.8324520587921143, "min_score_of_first_answer": 0.006658540107309818, "avg_score": 0.03465605899691582, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35437763408663797, "std_of_first_answer": 0.35437763408663797, "avg_entropy_of_first_answer": 2.629357233643532, "lex_sim": 1.0}
{"question": "super robot monkey team hyperforce go fighting games", "answer": ["fighting games (2D)"], "scores": [[0.01592414453625679, 0.5971764922142029, 0.005460615269839764, 0.00989939458668232, 0.040281910449266434, 0.30328547954559326]], "normalized_score": [0.042953163385391235], "tokens": [["\u0120fighting", "\u0120games", "\u0120(", "2", "D", ")"]], "entropy": [[6.211825370788574, 1.4737467765808105, 0.9005389213562012, 6.504482269287109, 1.884333848953247, 3.324173927307129]], "avg_entropy": [3.3831835190455117], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Galactic Smash", "score_of_biggest_cluster": 0.042953163385391235, "score_of_first_answer": 0.042953163385391235, "max_score_of_first_answer": 0.5971764922142029, "min_score_of_first_answer": 0.005460615269839764, "avg_score": 0.042953163385391235, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22103272306282976, "std_of_first_answer": 0.22103272306282976, "avg_entropy_of_first_answer": 3.3831835190455117, "lex_sim": 1.0}
{"question": "who owns the delano hotel in las vegas", "answer": ["Steven H Davis"], "scores": [[0.00034793451777659357, 0.014278985559940338, 0.0015199272893369198]], "normalized_score": [0.00196188036352396], "tokens": [["\u0120Steven", "\u0120H", "\u0120Davis"]], "entropy": [[4.574403285980225, 5.410269737243652, 4.168549537658691]], "avg_entropy": [4.7177408536275225], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "MGM Resorts International", "score_of_biggest_cluster": 0.00196188036352396, "score_of_first_answer": 0.00196188036352396, "max_score_of_first_answer": 0.014278985559940338, "min_score_of_first_answer": 0.00034793451777659357, "avg_score": 0.00196188036352396, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.006309087961456999, "std_of_first_answer": 0.006309087961456999, "avg_entropy_of_first_answer": 4.7177408536275225, "lex_sim": 1.0}
{"question": "who is considered as architect of india constitution", "answer": ["B C Roy as per convention of indian constitution"], "scores": [[0.05656147748231888, 0.0011198128340765834, 0.13656209409236908, 5.0898048357339576e-05, 0.39537155628204346, 0.003375366097316146, 0.18977124989032745, 0.14748632907867432, 0.3623725473880768, 0.0585954450070858]], "normalized_score": [0.028464432805776596], "tokens": [["\u0120B", "\u0120C", "\u0120Roy", "\u0120as", "\u0120per", "\u0120convention", "\u0120of", "\u0120ind", "ian", "\u0120constitution"]], "entropy": [[5.35350227355957, 0.9404661655426025, 4.380163669586182, 0.6715314388275146, 4.007532119750977, 4.361952304840088, 2.700033664703369, 5.563573837280273, 1.0576897859573364, 4.718942642211914]], "avg_entropy": [3.3755387902259826], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "B. R. Ambedkar", "score_of_biggest_cluster": 0.028464432805776596, "score_of_first_answer": 0.028464432805776596, "max_score_of_first_answer": 0.39537155628204346, "min_score_of_first_answer": 5.0898048357339576e-05, "avg_score": 0.028464432805776596, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13714961437868517, "std_of_first_answer": 0.13714961437868517, "avg_entropy_of_first_answer": 3.3755387902259826, "lex_sim": 1.0}
{"question": "who has a ring of power in lotr", "answer": ["Sauron"], "scores": [[0.09480660408735275, 0.964360773563385]], "normalized_score": [0.30237025022506714], "tokens": [["\u0120Saur", "on"]], "entropy": [[4.089237213134766, 0.3088483214378357]], "avg_entropy": [2.1990427672863007], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sauron", "score_of_biggest_cluster": 0.30237025022506714, "score_of_first_answer": 0.30237025022506714, "max_score_of_first_answer": 0.964360773563385, "min_score_of_first_answer": 0.09480660408735275, "avg_score": 0.30237025022506714, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.43477708473801613, "std_of_first_answer": 0.43477708473801613, "avg_entropy_of_first_answer": 2.1990427672863007, "lex_sim": 1.0}
{"question": "when was the last time new zealand had an earthquake", "answer": ["March 3, 2011"], "scores": [[0.02657630667090416, 0.02114752307534218, 0.21354873478412628, 0.33799266815185547]], "normalized_score": [0.07980674505233765], "tokens": [["\u0120March", "\u01203", ",", "\u01202011"]], "entropy": [[5.136802673339844, 3.617554187774658, 1.0316379070281982, 3.005411148071289]], "avg_entropy": [3.1978514790534973], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "11 Jul 2017", "score_of_biggest_cluster": 0.07980674505233765, "score_of_first_answer": 0.07980674505233765, "max_score_of_first_answer": 0.33799266815185547, "min_score_of_first_answer": 0.02114752307534218, "avg_score": 0.07980674505233765, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1334315544440091, "std_of_first_answer": 0.1334315544440091, "avg_entropy_of_first_answer": 3.1978514790534973, "lex_sim": 1.0}
{"question": "who was the actor that played ben stone on law and order", "answer": ["Ben Stone"], "scores": [[0.2603312134742737, 0.1622193306684494]], "normalized_score": [0.2055012285709381], "tokens": [["\u0120Ben", "\u0120Stone"]], "entropy": [[4.479509353637695, 4.4595136642456055]], "avg_entropy": [4.46951150894165], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Michael Moriarty", "score_of_biggest_cluster": 0.2055012285709381, "score_of_first_answer": 0.2055012285709381, "max_score_of_first_answer": 0.2603312134742737, "min_score_of_first_answer": 0.1622193306684494, "avg_score": 0.2055012285709381, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.04905594140291214, "std_of_first_answer": 0.04905594140291214, "avg_entropy_of_first_answer": 4.46951150894165, "lex_sim": 1.0}
{"question": "when does sam realize he is jim in ghost whisperer", "answer": ["the end of the first episode"], "scores": [[0.035130903124809265, 0.1332082748413086, 0.5702359080314636, 0.5592358112335205, 0.34234780073165894, 0.4113014042377472]], "normalized_score": [0.243827685713768], "tokens": [["\u0120the", "\u0120end", "\u0120of", "\u0120the", "\u0120first", "\u0120episode"]], "entropy": [[5.3216400146484375, 4.332339286804199, 1.0251151323318481, 2.1395838260650635, 2.5255141258239746, 1.9216434955596924]], "avg_entropy": [2.8776393135388694], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Leap of Faith", "score_of_biggest_cluster": 0.243827685713768, "score_of_first_answer": 0.243827685713768, "max_score_of_first_answer": 0.5702359080314636, "min_score_of_first_answer": 0.035130903124809265, "avg_score": 0.243827685713768, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20076869424042731, "std_of_first_answer": 0.20076869424042731, "avg_entropy_of_first_answer": 2.8776393135388694, "lex_sim": 1.0}
{"question": "where did the tea come from in the boston tea party", "answer": ["England"], "scores": [[0.02595396898686886]], "normalized_score": [0.025953972712159157], "tokens": [["\u0120England"]], "entropy": [[3.674956798553467]], "avg_entropy": [3.674956798553467], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "England", "score_of_biggest_cluster": 0.025953972712159157, "score_of_first_answer": 0.025953972712159157, "max_score_of_first_answer": 0.02595396898686886, "min_score_of_first_answer": 0.02595396898686886, "avg_score": 0.025953972712159157, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.674956798553467, "lex_sim": 1.0}
{"question": "the boards name on ed edd and eddy", "answer": ["eddie ralph"], "scores": [[0.02531639114022255, 0.29331913590431213, 0.0075376881286501884, 0.27785176038742065]], "normalized_score": [0.06279837340116501], "tokens": [["\u0120ed", "die", "\u0120r", "alph"]], "entropy": [[7.094651222229004, 3.607985019683838, 3.9924354553222656, 4.363552093505859]], "avg_entropy": [4.764655947685242], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Plank", "score_of_biggest_cluster": 0.06279837340116501, "score_of_first_answer": 0.06279837340116501, "max_score_of_first_answer": 0.29331913590431213, "min_score_of_first_answer": 0.0075376881286501884, "avg_score": 0.06279837340116501, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1348368550002956, "std_of_first_answer": 0.1348368550002956, "avg_entropy_of_first_answer": 4.764655947685242, "lex_sim": 1.0}
{"question": "what kind of national legislature did the constitution created", "answer": ["bicameral congress, where the senate is also part of it"], "scores": [[0.22067750990390778, 0.9967329502105713, 0.9965527057647705, 0.9997475743293762, 0.008717392571270466, 0.01130831427872181, 0.01550429966300726, 0.21514087915420532, 0.1498386114835739, 0.31454864144325256, 0.010080639272928238, 0.019096164032816887, 0.9616960287094116, 0.12222868949174881]], "normalized_score": [0.1156805157661438], "tokens": [["\u0120b", "ic", "am", "eral", "\u0120congress", ",", "\u0120where", "\u0120the", "\u0120senate", "\u0120is", "\u0120also", "\u0120part", "\u0120of", "\u0120it"]], "entropy": [[3.8463242053985596, 0.03168103098869324, 0.02441941574215889, 0.0036057624965906143, 1.2644102573394775, 0.7947142720222473, 4.881270408630371, 3.222531795501709, 3.0466110706329346, 2.9320976734161377, 4.595635414123535, 3.302058219909668, 0.30189043283462524, 1.3957266807556152]], "avg_entropy": [2.1173554742708802], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "bicameral Congress", "score_of_biggest_cluster": 0.1156805157661438, "score_of_first_answer": 0.1156805157661438, "max_score_of_first_answer": 0.9997475743293762, "min_score_of_first_answer": 0.008717392571270466, "avg_score": 0.1156805157661438, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4077313894684066, "std_of_first_answer": 0.4077313894684066, "avg_entropy_of_first_answer": 2.1173554742708802, "lex_sim": 1.0}
{"question": "who designed the garden city of new earswick", "answer": ["Herbert Edward Frere David Gascoyne"], "scores": [[0.004088983405381441, 0.001648535719141364, 0.0015281324740499258, 0.09397830814123154, 0.0002103668375639245, 0.302897572517395, 0.017432939261198044, 0.9929351210594177, 0.995658278465271]], "normalized_score": [0.0216913390904665], "tokens": [["\u0120Herbert", "\u0120Edward", "\u0120Fre", "re", "\u0120David", "\u0120G", "asc", "oy", "ne"]], "entropy": [[5.167879581451416, 5.574346542358398, 5.019728660583496, 3.2615652084350586, 1.7452279329299927, 3.2500972747802734, 1.9000422954559326, 0.07035661488771439, 0.03748849779367447]], "avg_entropy": [2.891859178741773], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "planner Raymond Unwin", "score_of_biggest_cluster": 0.0216913390904665, "score_of_first_answer": 0.0216913390904665, "max_score_of_first_answer": 0.995658278465271, "min_score_of_first_answer": 0.0002103668375639245, "avg_score": 0.0216913390904665, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39897272539452927, "std_of_first_answer": 0.39897272539452927, "avg_entropy_of_first_answer": 2.891859178741773, "lex_sim": 1.0}
{"question": "the lowest temperature at which a lubricant will flow is called the", "answer": ["pour point"], "scores": [[0.03438899293541908, 0.969096302986145]], "normalized_score": [0.18255476653575897], "tokens": [["\u0120pour", "\u0120point"]], "entropy": [[5.202060699462891, 0.17895056307315826]], "avg_entropy": [2.6905056312680244], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "pour point", "score_of_biggest_cluster": 0.18255476653575897, "score_of_first_answer": 0.18255476653575897, "max_score_of_first_answer": 0.969096302986145, "min_score_of_first_answer": 0.03438899293541908, "avg_score": 0.18255476653575897, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.46735365502536297, "std_of_first_answer": 0.46735365502536297, "avg_entropy_of_first_answer": 2.6905056312680244, "lex_sim": 1.0}
{"question": "who sings gimme some lovin in days of thunder", "answer": ["Bon Iver"], "scores": [[0.0036180943716317415, 0.0901123508810997, 0.958121120929718]], "normalized_score": [0.06785184144973755], "tokens": [["\u0120Bon", "\u0120I", "ver"]], "entropy": [[6.16073751449585, 1.3047648668289185, 0.40889665484428406]], "avg_entropy": [2.6247996787230172], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Spencer Davis Group", "score_of_biggest_cluster": 0.06785184144973755, "score_of_first_answer": 0.06785184144973755, "max_score_of_first_answer": 0.958121120929718, "min_score_of_first_answer": 0.0036180943716317415, "avg_score": 0.06785184144973755, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4310190133727007, "std_of_first_answer": 0.4310190133727007, "avg_entropy_of_first_answer": 2.6247996787230172, "lex_sim": 1.0}
{"question": "who was the king of england in 1756", "answer": ["George II"], "scores": [[0.6819299459457397, 0.6496943831443787]], "normalized_score": [0.665617048740387], "tokens": [["\u0120George", "\u0120II"]], "entropy": [[1.88362717628479, 1.3561650514602661]], "avg_entropy": [1.619896113872528], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George II", "score_of_biggest_cluster": 0.665617048740387, "score_of_first_answer": 0.665617048740387, "max_score_of_first_answer": 0.6819299459457397, "min_score_of_first_answer": 0.6496943831443787, "avg_score": 0.665617048740387, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.016117781400680542, "std_of_first_answer": 0.016117781400680542, "avg_entropy_of_first_answer": 1.619896113872528, "lex_sim": 1.0}
{"question": "where did the california gold rush take place", "answer": ["In California"], "scores": [[0.012017716653645039, 0.4278005361557007]], "normalized_score": [0.07170206308364868], "tokens": [["\u0120In", "\u0120California"]], "entropy": [[4.2549638748168945, 2.847053289413452]], "avg_entropy": [3.5510085821151733], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Northern California", "score_of_biggest_cluster": 0.07170206308364868, "score_of_first_answer": 0.07170206308364868, "max_score_of_first_answer": 0.4278005361557007, "min_score_of_first_answer": 0.012017716653645039, "avg_score": 0.07170206308364868, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.20789140975102782, "std_of_first_answer": 0.20789140975102782, "avg_entropy_of_first_answer": 3.5510085821151733, "lex_sim": 1.0}
{"question": "wal-mart is a prime example of which type organization", "answer": ["company"], "scores": [[0.0069312104023993015]], "normalized_score": [0.006931211333721876], "tokens": [["\u0120company"]], "entropy": [[6.245990753173828]], "avg_entropy": [6.245990753173828], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "multinational retail corporation", "score_of_biggest_cluster": 0.006931211333721876, "score_of_first_answer": 0.006931211333721876, "max_score_of_first_answer": 0.0069312104023993015, "min_score_of_first_answer": 0.0069312104023993015, "avg_score": 0.006931211333721876, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.245990753173828, "lex_sim": 1.0}
{"question": "how long did it take to build the great pyramid of egypt", "answer": ["26 years"], "scores": [[0.012367439456284046, 0.9190104603767395]], "normalized_score": [0.10661052167415619], "tokens": [["\u012026", "\u0120years"]], "entropy": [[4.535881996154785, 0.583308219909668]], "avg_entropy": [2.5595951080322266], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "20-year period", "score_of_biggest_cluster": 0.10661052167415619, "score_of_first_answer": 0.10661052167415619, "max_score_of_first_answer": 0.9190104603767395, "min_score_of_first_answer": 0.012367439456284046, "avg_score": 0.10661052167415619, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45332151046022773, "std_of_first_answer": 0.45332151046022773, "avg_entropy_of_first_answer": 2.5595951080322266, "lex_sim": 1.0}
{"question": "who do the characters represent in 8 mile", "answer": ["two characters representing two sides of a town"], "scores": [[0.013608844950795174, 0.08400632441043854, 0.04071588069200516, 0.2057678997516632, 0.03581617400050163, 0.5865175724029541, 0.22961083054542542, 0.0047373659908771515]], "normalized_score": [0.062018413096666336], "tokens": [["\u0120two", "\u0120characters", "\u0120representing", "\u0120two", "\u0120sides", "\u0120of", "\u0120a", "\u0120town"]], "entropy": [[7.324489593505859, 5.593879699707031, 3.0974597930908203, 4.843602657318115, 4.82811164855957, 1.4779601097106934, 2.249739646911621, 4.778965473175049]], "avg_entropy": [4.274276077747345], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "loosely on Eminem", "score_of_biggest_cluster": 0.062018413096666336, "score_of_first_answer": 0.062018413096666336, "max_score_of_first_answer": 0.5865175724029541, "min_score_of_first_answer": 0.0047373659908771515, "avg_score": 0.062018413096666336, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18338326254758777, "std_of_first_answer": 0.18338326254758777, "avg_entropy_of_first_answer": 4.274276077747345, "lex_sim": 1.0}
{"question": "when was the first case of alzheimer's diagnosed", "answer": ["April 4, 1906"], "scores": [[0.003578160423785448, 0.10597104579210281, 0.3846573531627655, 0.7592700719833374]], "normalized_score": [0.10258394479751587], "tokens": [["\u0120April", "\u01204", ",", "\u01201906"]], "entropy": [[5.309886932373047, 4.242071628570557, 1.2325494289398193, 1.6087008714675903]], "avg_entropy": [3.0983022153377533], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1901", "score_of_biggest_cluster": 0.10258394479751587, "score_of_first_answer": 0.10258394479751587, "max_score_of_first_answer": 0.7592700719833374, "min_score_of_first_answer": 0.003578160423785448, "avg_score": 0.10258394479751587, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2927857881572319, "std_of_first_answer": 0.2927857881572319, "avg_entropy_of_first_answer": 3.0983022153377533, "lex_sim": 1.0}
{"question": "when did the subway open in new york", "answer": ["March 25, 1870"], "scores": [[0.005686075426638126, 0.03690708056092262, 0.2672160565853119, 0.0031625812407583]], "normalized_score": [0.020521387457847595], "tokens": [["\u0120March", "\u012025", ",", "\u01201870"]], "entropy": [[5.175715923309326, 3.7435808181762695, 1.0873810052871704, 2.893451452255249]], "avg_entropy": [3.225032299757004], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "October 27, 1904", "score_of_biggest_cluster": 0.020521387457847595, "score_of_first_answer": 0.020521387457847595, "max_score_of_first_answer": 0.2672160565853119, "min_score_of_first_answer": 0.0031625812407583, "avg_score": 0.020521387457847595, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10991024853905387, "std_of_first_answer": 0.10991024853905387, "avg_entropy_of_first_answer": 3.225032299757004, "lex_sim": 1.0}
{"question": "authorities involved in formulating accounting standard in india", "answer": ["standard setting bodies"], "scores": [[0.002775990404188633, 0.29627689719200134, 0.2742782533168793]], "normalized_score": [0.06087452918291092], "tokens": [["\u0120standard", "\u0120setting", "\u0120bodies"]], "entropy": [[5.635516166687012, 4.050509452819824, 2.4532291889190674]], "avg_entropy": [4.046418269475301], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "government department", "score_of_biggest_cluster": 0.06087452918291092, "score_of_first_answer": 0.06087452918291092, "max_score_of_first_answer": 0.29627689719200134, "min_score_of_first_answer": 0.002775990404188633, "avg_score": 0.06087452918291092, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1334750086405377, "std_of_first_answer": 0.1334750086405377, "avg_entropy_of_first_answer": 4.046418269475301, "lex_sim": 1.0}
{"question": "who played morticia in the addams family tv show", "answer": ["Barbara Eden"], "scores": [[0.007466799579560757, 0.20597590506076813]], "normalized_score": [0.03921709954738617], "tokens": [["\u0120Barbara", "\u0120Eden"]], "entropy": [[5.319273471832275, 3.0818912982940674]], "avg_entropy": [4.200582385063171], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Carolyn Sue Jones", "score_of_biggest_cluster": 0.03921709954738617, "score_of_first_answer": 0.03921709954738617, "max_score_of_first_answer": 0.20597590506076813, "min_score_of_first_answer": 0.007466799579560757, "avg_score": 0.03921709954738617, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09925455274060369, "std_of_first_answer": 0.09925455274060369, "avg_entropy_of_first_answer": 4.200582385063171, "lex_sim": 1.0}
{"question": "fast and furious 7 red car abu dhabi", "answer": ["2014"], "scores": [[0.002422282239422202]], "normalized_score": [0.002422282239422202], "tokens": [["\u01202014"]], "entropy": [[6.121095657348633]], "avg_entropy": [6.121095657348633], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Lykan Hypersport", "score_of_biggest_cluster": 0.002422282239422202, "score_of_first_answer": 0.002422282239422202, "max_score_of_first_answer": 0.002422282239422202, "min_score_of_first_answer": 0.002422282239422202, "avg_score": 0.002422282239422202, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.121095657348633, "lex_sim": 1.0}
{"question": "where was the first colony in north america located", "answer": ["The first colony established by the Dutch in Virginia was at Jamestown, which was settled in 1616."], "scores": [[0.007817240431904793, 0.04198073595762253, 0.3215949833393097, 0.01327265053987503, 0.2946892976760864, 0.6566359996795654, 0.0533171109855175, 0.38598424196243286, 0.035657547414302826, 0.4188200533390045, 0.07550061494112015, 0.4217115640640259, 0.9957770109176636, 0.9989503026008606, 0.18927623331546783, 0.1781705766916275, 0.49247637391090393, 0.05117775872349739, 0.710310697555542, 0.9274551272392273, 0.007382717449218035, 0.6016935706138611]], "normalized_score": [0.1655610203742981], "tokens": [["\u0120The", "\u0120first", "\u0120colony", "\u0120established", "\u0120by", "\u0120the", "\u0120Dutch", "\u0120in", "\u0120Virginia", "\u0120was", "\u0120at", "\u0120Jam", "est", "own", ",", "\u0120which", "\u0120was", "\u0120settled", "\u0120in", "\u012016", "16", "."]], "entropy": [[3.282470464706421, 4.374871253967285, 3.3861196041107178, 1.8155314922332764, 1.84159517288208, 2.525362014770508, 4.45142936706543, 2.435413360595703, 3.288135051727295, 2.3644819259643555, 3.7292706966400146, 2.8160030841827393, 0.04096516594290733, 0.013781838119029999, 2.0659310817718506, 3.3830296993255615, 2.005890369415283, 3.4696731567382812, 1.284111738204956, 0.46548891067504883, 1.4663763046264648, 1.2897616624832153]], "avg_entropy": [2.3543497007340193], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Virginia", "score_of_biggest_cluster": 0.1655610203742981, "score_of_first_answer": 0.1655610203742981, "max_score_of_first_answer": 0.9989503026008606, "min_score_of_first_answer": 0.007382717449218035, "avg_score": 0.1655610203742981, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32629190896674337, "std_of_first_answer": 0.32629190896674337, "avg_entropy_of_first_answer": 2.3543497007340193, "lex_sim": 1.0}
{"question": "who wrote the country song i can only imagine", "answer": ["Eddy Arnold"], "scores": [[0.008412685245275497, 0.279980331659317, 0.41711679100990295]], "normalized_score": [0.09941224008798599], "tokens": [["\u0120Ed", "dy", "\u0120Arnold"]], "entropy": [[5.988636016845703, 3.7354817390441895, 3.7199978828430176]], "avg_entropy": [4.481371879577637], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bart Millard", "score_of_biggest_cluster": 0.09941224008798599, "score_of_first_answer": 0.09941224008798599, "max_score_of_first_answer": 0.41711679100990295, "min_score_of_first_answer": 0.008412685245275497, "avg_score": 0.09941224008798599, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.16983470432254633, "std_of_first_answer": 0.16983470432254633, "avg_entropy_of_first_answer": 4.481371879577637, "lex_sim": 1.0}
{"question": "who are the stars in dancing with the stars", "answer": ["Kevin Bacon"], "scores": [[0.0072057354263961315, 0.09004795551300049]], "normalized_score": [0.025472763925790787], "tokens": [["\u0120Kevin", "\u0120Bacon"]], "entropy": [[6.5598602294921875, 3.404019832611084]], "avg_entropy": [4.981940031051636], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jennie Finch Daigle", "score_of_biggest_cluster": 0.025472763925790787, "score_of_first_answer": 0.025472763925790787, "max_score_of_first_answer": 0.09004795551300049, "min_score_of_first_answer": 0.0072057354263961315, "avg_score": 0.025472763925790787, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04142111004330218, "std_of_first_answer": 0.04142111004330218, "avg_entropy_of_first_answer": 4.981940031051636, "lex_sim": 1.0}
{"question": "where did the idea of a unicorn come from", "answer": ["from european art"], "scores": [[0.04633575305342674, 0.0009708869620226324, 0.8882436156272888, 0.6024556756019592, 0.006294948514550924]], "normalized_score": [0.04326202720403671], "tokens": [["\u0120from", "\u0120euro", "pe", "an", "\u0120art"]], "entropy": [[6.02877950668335, 3.678980588912964, 0.7002216577529907, 1.271711826324463, 3.1136300563812256]], "avg_entropy": [2.9586647272109987], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Greek writers of natural history", "score_of_biggest_cluster": 0.04326202720403671, "score_of_first_answer": 0.04326202720403671, "max_score_of_first_answer": 0.8882436156272888, "min_score_of_first_answer": 0.0009708869620226324, "avg_score": 0.04326202720403671, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3680065464993622, "std_of_first_answer": 0.3680065464993622, "avg_entropy_of_first_answer": 2.9586647272109987, "lex_sim": 1.0}
{"question": "when did ole miss beat alabama in football", "answer": ["November 02, 2008"], "scores": [[0.04562999680638313, 0.00011782570072682574, 0.7621342539787292, 0.014895456843078136]], "normalized_score": [0.01571788638830185], "tokens": [["\u0120November", "\u012002", ",", "\u01202008"]], "entropy": [[5.258957862854004, 3.7033658027648926, 1.3604187965393066, 3.902679681777954]], "avg_entropy": [3.5563555359840393], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "October 3, 1970", "score_of_biggest_cluster": 0.01571788638830185, "score_of_first_answer": 0.01571788638830185, "max_score_of_first_answer": 0.7621342539787292, "min_score_of_first_answer": 0.00011782570072682574, "avg_score": 0.01571788638830185, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.321679922001677, "std_of_first_answer": 0.321679922001677, "avg_entropy_of_first_answer": 3.5563555359840393, "lex_sim": 1.0}
{"question": "total number of mna in pakistan national assembly", "answer": ["342"], "scores": [[0.044130802154541016]], "normalized_score": [0.04413079842925072], "tokens": [["\u0120342"]], "entropy": [[5.534278869628906]], "avg_entropy": [5.534278869628906], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "332 members", "score_of_biggest_cluster": 0.04413079842925072, "score_of_first_answer": 0.04413079842925072, "max_score_of_first_answer": 0.044130802154541016, "min_score_of_first_answer": 0.044130802154541016, "avg_score": 0.04413079842925072, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.534278869628906, "lex_sim": 1.0}
{"question": "who won season 8 of america's next top model", "answer": ["Ashley Graham"], "scores": [[0.017459221184253693, 0.3513733446598053]], "normalized_score": [0.07832435518503189], "tokens": [["\u0120Ashley", "\u0120Graham"]], "entropy": [[6.832469463348389, 3.8479671478271484]], "avg_entropy": [5.3402183055877686], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jaslene Gonzalez", "score_of_biggest_cluster": 0.07832435518503189, "score_of_first_answer": 0.07832435518503189, "max_score_of_first_answer": 0.3513733446598053, "min_score_of_first_answer": 0.017459221184253693, "avg_score": 0.07832435518503189, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1669570617377758, "std_of_first_answer": 0.1669570617377758, "avg_entropy_of_first_answer": 5.3402183055877686, "lex_sim": 1.0}
{"question": "who played cruella de vil in 101 dalmatians", "answer": ["Glenn Close"], "scores": [[0.08581230044364929, 0.9921237826347351]], "normalized_score": [0.291781485080719], "tokens": [["\u0120Glenn", "\u0120Close"]], "entropy": [[5.8925933837890625, 0.08788160979747772]], "avg_entropy": [2.99023749679327], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Glenn Close", "score_of_biggest_cluster": 0.291781485080719, "score_of_first_answer": 0.291781485080719, "max_score_of_first_answer": 0.9921237826347351, "min_score_of_first_answer": 0.08581230044364929, "avg_score": 0.291781485080719, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4531557410955429, "std_of_first_answer": 0.4531557410955429, "avg_entropy_of_first_answer": 2.99023749679327, "lex_sim": 1.0}
{"question": "who invented the submarine during the civil war", "answer": ["Joseph P. Grady (1850\u20131922)"], "scores": [[0.01852327398955822, 0.029672518372535706, 0.6149841547012329, 0.004318949766457081, 0.28749901056289673, 0.002600126201286912, 0.14083930850028992, 0.014083381742238998, 0.17726856470108032, 0.770598828792572, 0.038893405348062515, 0.9275869727134705]], "normalized_score": [0.06849710643291473], "tokens": [["\u0120Joseph", "\u0120P", ".", "\u0120Gr", "ady", "\u0120(", "18", "50", "\u00e2\u0122\u0135", "19", "22", ")"]], "entropy": [[5.445918083190918, 5.355781555175781, 3.108889102935791, 4.104308128356934, 3.020730972290039, 0.41692355275154114, 5.813798427581787, 4.305791854858398, 1.578061819076538, 0.8708552718162537, 3.7215170860290527, 0.35724321007728577]], "avg_entropy": [3.1749849220116935], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Horace Lawson Hunley", "score_of_biggest_cluster": 0.06849710643291473, "score_of_first_answer": 0.06849710643291473, "max_score_of_first_answer": 0.9275869727134705, "min_score_of_first_answer": 0.002600126201286912, "avg_score": 0.06849710643291473, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3169945857327232, "std_of_first_answer": 0.3169945857327232, "avg_entropy_of_first_answer": 3.1749849220116935, "lex_sim": 1.0}
{"question": "third president of the united states of america", "answer": ["John Quincy Adams"], "scores": [[0.0924219861626625, 0.07897897064685822, 0.9348315596580505]], "normalized_score": [0.1896735429763794], "tokens": [["\u0120John", "\u0120Quincy", "\u0120Adams"]], "entropy": [[3.6914963722229004, 1.0390595197677612, 0.42017000913619995]], "avg_entropy": [1.7169086337089539], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Thomas Jefferson", "score_of_biggest_cluster": 0.1896735429763794, "score_of_first_answer": 0.1896735429763794, "max_score_of_first_answer": 0.9348315596580505, "min_score_of_first_answer": 0.07897897064685822, "avg_score": 0.1896735429763794, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40032185083449495, "std_of_first_answer": 0.40032185083449495, "avg_entropy_of_first_answer": 1.7169086337089539, "lex_sim": 1.0}
{"question": "where did hope on bold and beautiful go", "answer": ["USA"], "scores": [[0.0016265377635136247]], "normalized_score": [0.0016265374142676592], "tokens": [["\u0120USA"]], "entropy": [[7.184600830078125]], "avg_entropy": [7.184600830078125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Milan, Italy", "score_of_biggest_cluster": 0.0016265374142676592, "score_of_first_answer": 0.0016265374142676592, "max_score_of_first_answer": 0.0016265377635136247, "min_score_of_first_answer": 0.0016265377635136247, "avg_score": 0.0016265374142676592, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 7.184600830078125, "lex_sim": 1.0}
{"question": "who scored the most points in a single game in the nba", "answer": ["Kareem Abdul-Jabbar"], "scores": [[0.05468284711241722, 0.9969268441200256, 0.9087051749229431, 0.45103394985198975, 0.9535404443740845, 0.974450409412384, 0.9502795338630676]], "normalized_score": [0.5707453489303589], "tokens": [["\u0120Kare", "em", "\u0120Abdul", "-", "J", "ab", "bar"]], "entropy": [[4.269675254821777, 0.024276191368699074, 0.6414613127708435, 1.4581403732299805, 0.2944023311138153, 0.1645478904247284, 0.3403272032737732]], "avg_entropy": [1.0275472224290882], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wilt Chamberlain", "score_of_biggest_cluster": 0.5707453489303589, "score_of_first_answer": 0.5707453489303589, "max_score_of_first_answer": 0.9969268441200256, "min_score_of_first_answer": 0.05468284711241722, "avg_score": 0.5707453489303589, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33609122728711227, "std_of_first_answer": 0.33609122728711227, "avg_entropy_of_first_answer": 1.0275472224290882, "lex_sim": 1.0}
{"question": "who wrote lyrics for phantom of the opera", "answer": ["Andrew Lloyd Webber"], "scores": [[0.5594595670700073, 0.8819288015365601, 0.715945303440094, 0.9990301132202148]], "normalized_score": [0.7707528471946716], "tokens": [["\u0120Andrew", "\u0120Lloyd", "\u0120Web", "ber"]], "entropy": [[3.3236818313598633, 0.8489564657211304, 1.2319254875183105, 0.010421976447105408]], "avg_entropy": [1.3537464402616024], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charles Hart", "score_of_biggest_cluster": 0.7707528471946716, "score_of_first_answer": 0.7707528471946716, "max_score_of_first_answer": 0.9990301132202148, "min_score_of_first_answer": 0.5594595670700073, "avg_score": 0.7707528471946716, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.16641377389547413, "std_of_first_answer": 0.16641377389547413, "avg_entropy_of_first_answer": 1.3537464402616024, "lex_sim": 1.0}
{"question": "who wrote the song going to kansas city", "answer": ["Gary Moore"], "scores": [[0.012209467589855194, 0.04882284626364708]], "normalized_score": [0.02441517449915409], "tokens": [["\u0120Gary", "\u0120Moore"]], "entropy": [[6.2284016609191895, 5.146887302398682]], "avg_entropy": [5.6876444816589355], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jerry Leiber and Mike Stoller", "score_of_biggest_cluster": 0.02441517449915409, "score_of_first_answer": 0.02441517449915409, "max_score_of_first_answer": 0.04882284626364708, "min_score_of_first_answer": 0.012209467589855194, "avg_score": 0.02441517449915409, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.018306689336895943, "std_of_first_answer": 0.018306689336895943, "avg_entropy_of_first_answer": 5.6876444816589355, "lex_sim": 1.0}
{"question": "who has the most rushing yards in a super bowl", "answer": ["Terrell Davis"], "scores": [[0.010596140287816525, 0.6917407512664795, 0.3395939767360687]], "normalized_score": [0.13552424311637878], "tokens": [["\u0120Ter", "rell", "\u0120Davis"]], "entropy": [[5.581399917602539, 1.0695991516113281, 0.901681661605835]], "avg_entropy": [2.5175602436065674], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Timmy Smith", "score_of_biggest_cluster": 0.13552424311637878, "score_of_first_answer": 0.13552424311637878, "max_score_of_first_answer": 0.6917407512664795, "min_score_of_first_answer": 0.010596140287816525, "avg_score": 0.13552424311637878, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2781296476795343, "std_of_first_answer": 0.2781296476795343, "avg_entropy_of_first_answer": 2.5175602436065674, "lex_sim": 1.0}
{"question": "which material is the heaviest in term of density", "answer": ["air"], "scores": [[0.003074649255722761]], "normalized_score": [0.0030746490228921175], "tokens": [["\u0120air"]], "entropy": [[5.429646015167236]], "avg_entropy": [5.429646015167236], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Osmium", "score_of_biggest_cluster": 0.0030746490228921175, "score_of_first_answer": 0.0030746490228921175, "max_score_of_first_answer": 0.003074649255722761, "min_score_of_first_answer": 0.003074649255722761, "avg_score": 0.0030746490228921175, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.429646015167236, "lex_sim": 1.0}
{"question": "when did last podcast on the left start", "answer": ["January 1"], "scores": [[0.06797797977924347, 0.11108759045600891]], "normalized_score": [0.08689942210912704], "tokens": [["\u0120January", "\u01201"]], "entropy": [[4.896646499633789, 3.747800827026367]], "avg_entropy": [4.322223663330078], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2011", "score_of_biggest_cluster": 0.08689942210912704, "score_of_first_answer": 0.08689942210912704, "max_score_of_first_answer": 0.11108759045600891, "min_score_of_first_answer": 0.06797797977924347, "avg_score": 0.08689942210912704, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02155480533838272, "std_of_first_answer": 0.02155480533838272, "avg_entropy_of_first_answer": 4.322223663330078, "lex_sim": 1.0}
{"question": "striking the funny bone is actually compression of the", "answer": ["zygomatic bone"], "scores": [[0.0019303079461678863, 0.9343149065971375, 0.9691756963729858, 0.08722744882106781]], "normalized_score": [0.11112043261528015], "tokens": [["\u0120z", "yg", "omatic", "\u0120bone"]], "entropy": [[6.200626373291016, 0.45540404319763184, 0.21612690389156342, 1.891899824142456]], "avg_entropy": [2.1910142861306667], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The ulnar nerve", "score_of_biggest_cluster": 0.11112043261528015, "score_of_first_answer": 0.11112043261528015, "max_score_of_first_answer": 0.9691756963729858, "min_score_of_first_answer": 0.0019303079461678863, "avg_score": 0.11112043261528015, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4547516788466212, "std_of_first_answer": 0.4547516788466212, "avg_entropy_of_first_answer": 2.1910142861306667, "lex_sim": 1.0}
{"question": "what language is the olympic anthem sang in", "answer": ["Ode to the Athlete"], "scores": [[0.003889173036441207, 0.44304487109184265, 0.7229865193367004, 0.13095639646053314, 0.002466433448716998, 0.9626525044441223]], "normalized_score": [0.08537882566452026], "tokens": [["\u0120O", "de", "\u0120to", "\u0120the", "\u0120Ath", "lete"]], "entropy": [[3.497593402862549, 3.9066483974456787, 1.454303503036499, 0.9050097465515137, 4.897873878479004, 0.26564985513687134]], "avg_entropy": [2.487846463918686], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Greek", "score_of_biggest_cluster": 0.08537882566452026, "score_of_first_answer": 0.08537882566452026, "max_score_of_first_answer": 0.9626525044441223, "min_score_of_first_answer": 0.002466433448716998, "avg_score": 0.08537882566452026, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3667604413046254, "std_of_first_answer": 0.3667604413046254, "avg_entropy_of_first_answer": 2.487846463918686, "lex_sim": 1.0}
{"question": "winner of the women's royal rumble match", "answer": ["The Undertaker"], "scores": [[0.010174822993576527, 0.13519294559955597, 0.9906514883041382]], "normalized_score": [0.11086657643318176], "tokens": [["\u0120The", "\u0120Under", "taker"]], "entropy": [[5.536533355712891, 4.952906131744385, 0.09017495810985565]], "avg_entropy": [3.526538148522377], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Asuka", "score_of_biggest_cluster": 0.11086657643318176, "score_of_first_answer": 0.11086657643318176, "max_score_of_first_answer": 0.9906514883041382, "min_score_of_first_answer": 0.010174822993576527, "avg_score": 0.11086657643318176, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43573352464355886, "std_of_first_answer": 0.43573352464355886, "avg_entropy_of_first_answer": 3.526538148522377, "lex_sim": 1.0}
{"question": "where did they film the show the crossing", "answer": ["I live in Maryland"], "scores": [[0.003121706424281001, 0.01028282381594181, 0.6583577990531921, 0.005260948091745377]], "normalized_score": [0.018260281533002853], "tokens": [["\u0120I", "\u0120live", "\u0120in", "\u0120Maryland"]], "entropy": [[6.8096418380737305, 3.6318535804748535, 1.9584404230117798, 5.485398292541504]], "avg_entropy": [4.471333533525467], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "British Columbia, Canada", "score_of_biggest_cluster": 0.018260281533002853, "score_of_first_answer": 0.018260281533002853, "max_score_of_first_answer": 0.6583577990531921, "min_score_of_first_answer": 0.003121706424281001, "avg_score": 0.018260281533002853, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28239512262753425, "std_of_first_answer": 0.28239512262753425, "avg_entropy_of_first_answer": 4.471333533525467, "lex_sim": 1.0}
{"question": "who played taylor on the bold and beautiful", "answer": ["Taylor Kitsch"], "scores": [[0.4055785834789276, 0.04192705824971199, 0.9307813048362732]], "normalized_score": [0.25107625126838684], "tokens": [["\u0120Taylor", "\u0120Kits", "ch"]], "entropy": [[4.558770179748535, 4.96047306060791, 0.43105900287628174]], "avg_entropy": [3.316767414410909], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hunter Tylo", "score_of_biggest_cluster": 0.25107625126838684, "score_of_first_answer": 0.25107625126838684, "max_score_of_first_answer": 0.9307813048362732, "min_score_of_first_answer": 0.04192705824971199, "avg_score": 0.25107625126838684, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36486560722144434, "std_of_first_answer": 0.36486560722144434, "avg_entropy_of_first_answer": 3.316767414410909, "lex_sim": 1.0}
{"question": "who plays the voice of sulley in monsters inc", "answer": ["Sulley"], "scores": [[0.08504816144704819, 0.9994192123413086]], "normalized_score": [0.29154545068740845], "tokens": [["\u0120Sul", "ley"]], "entropy": [[4.575072288513184, 0.0071705966256558895]], "avg_entropy": [2.2911214425694197], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Goodman", "score_of_biggest_cluster": 0.29154545068740845, "score_of_first_answer": 0.29154545068740845, "max_score_of_first_answer": 0.9994192123413086, "min_score_of_first_answer": 0.08504816144704819, "avg_score": 0.29154545068740845, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4571855254471302, "std_of_first_answer": 0.4571855254471302, "avg_entropy_of_first_answer": 2.2911214425694197, "lex_sim": 1.0}
{"question": "who did dwight howard play for last year", "answer": ["dwight howard, national basketball association"], "scores": [[0.01907661184668541, 0.9947081804275513, 0.5527350902557373, 0.9597584009170532, 0.005494019016623497, 0.0004222566494718194, 0.5269542932510376, 0.7390781044960022]], "normalized_score": [0.09882134199142456], "tokens": [["\u0120dw", "ight", "\u0120how", "ard", ",", "\u0120national", "\u0120basketball", "\u0120association"]], "entropy": [[5.956958770751953, 0.0598469078540802, 3.0647480487823486, 0.18721221387386322, 0.8529460430145264, 5.775875091552734, 2.374250650405884, 1.4046523571014404]], "avg_entropy": [2.4595612604171038], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Atlanta Hawks", "score_of_biggest_cluster": 0.09882134199142456, "score_of_first_answer": 0.09882134199142456, "max_score_of_first_answer": 0.9947081804275513, "min_score_of_first_answer": 0.0004222566494718194, "avg_score": 0.09882134199142456, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.393238506149457, "std_of_first_answer": 0.393238506149457, "avg_entropy_of_first_answer": 2.4595612604171038, "lex_sim": 1.0}
{"question": "who plays rooster's girlfriend on the ranch", "answer": ["Kristen Bell"], "scores": [[0.007201751694083214, 0.758311927318573]], "normalized_score": [0.0738997533917427], "tokens": [["\u0120Kristen", "\u0120Bell"]], "entropy": [[6.718204498291016, 1.4463627338409424]], "avg_entropy": [4.082283616065979], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Maggie Lawson", "score_of_biggest_cluster": 0.0738997533917427, "score_of_first_answer": 0.0738997533917427, "max_score_of_first_answer": 0.758311927318573, "min_score_of_first_answer": 0.007201751694083214, "avg_score": 0.0738997533917427, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3755550878122449, "std_of_first_answer": 0.3755550878122449, "avg_entropy_of_first_answer": 4.082283616065979, "lex_sim": 1.0}
{"question": "who had a baby at 100 in the bible", "answer": ["Rebecca of Cana"], "scores": [[0.0034393828827887774, 0.06355181336402893, 0.013855828903615475, 0.8964276909828186]], "normalized_score": [0.04059186205267906], "tokens": [["\u0120Rebecca", "\u0120of", "\u0120Can", "a"]], "entropy": [[5.380372047424316, 5.3563408851623535, 4.30231237411499, 0.7755109667778015]], "avg_entropy": [3.9536340683698654], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sarah", "score_of_biggest_cluster": 0.04059186205267906, "score_of_first_answer": 0.04059186205267906, "max_score_of_first_answer": 0.8964276909828186, "min_score_of_first_answer": 0.0034393828827887774, "avg_score": 0.04059186205267906, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3771799251662039, "std_of_first_answer": 0.3771799251662039, "avg_entropy_of_first_answer": 3.9536340683698654, "lex_sim": 1.0}
{"question": "when was the latest version of chrome released", "answer": ["version 67.0.3396.97"], "scores": [[0.03067268431186676, 0.02131836861371994, 0.20890915393829346, 0.9327265024185181, 0.8774356842041016, 0.8249009847640991, 0.964108407497406, 0.9826858043670654, 0.0067408038303256035]], "normalized_score": [0.20313626527786255], "tokens": [["\u0120version", "\u012067", ".", "0", ".", "33", "96", ".", "97"]], "entropy": [[5.033187389373779, 4.733353137969971, 1.2956666946411133, 0.5490162372589111, 0.4969366192817688, 0.921511709690094, 0.2687172591686249, 0.11197572201490402, 2.42403507232666]], "avg_entropy": [1.7593777601917584], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2018-01-22", "score_of_biggest_cluster": 0.20313626527786255, "score_of_first_answer": 0.20313626527786255, "max_score_of_first_answer": 0.9826858043670654, "min_score_of_first_answer": 0.0067408038303256035, "avg_score": 0.20313626527786255, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4278529595076478, "std_of_first_answer": 0.4278529595076478, "avg_entropy_of_first_answer": 1.7593777601917584, "lex_sim": 1.0}
{"question": "who's playing in the sec championship 2017", "answer": ["Oklahoma"], "scores": [[0.013685830868780613]], "normalized_score": [0.013685830868780613], "tokens": [["\u0120Oklahoma"]], "entropy": [[5.4528374671936035]], "avg_entropy": [5.4528374671936035], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Auburn Tigers", "score_of_biggest_cluster": 0.013685830868780613, "score_of_first_answer": 0.013685830868780613, "max_score_of_first_answer": 0.013685830868780613, "min_score_of_first_answer": 0.013685830868780613, "avg_score": 0.013685830868780613, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.4528374671936035, "lex_sim": 1.0}
{"question": "who does the voice of stewie family guy", "answer": ["Peter Cullen"], "scores": [[0.07595980167388916, 0.05917609483003616]], "normalized_score": [0.06704480201005936], "tokens": [["\u0120Peter", "\u0120Cullen"]], "entropy": [[3.885129928588867, 4.279766082763672]], "avg_entropy": [4.0824480056762695], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Seth MacFarlane", "score_of_biggest_cluster": 0.06704480201005936, "score_of_first_answer": 0.06704480201005936, "max_score_of_first_answer": 0.07595980167388916, "min_score_of_first_answer": 0.05917609483003616, "avg_score": 0.06704480201005936, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.008391853421926498, "std_of_first_answer": 0.008391853421926498, "avg_entropy_of_first_answer": 4.0824480056762695, "lex_sim": 1.0}
{"question": "what grade was arnold from hey arnold in", "answer": ["a+"], "scores": [[0.02185117080807686, 0.021870819851756096]], "normalized_score": [0.021860990673303604], "tokens": [["\u0120a", "+"]], "entropy": [[5.450709342956543, 4.022523880004883]], "avg_entropy": [4.736616611480713], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "fourth", "score_of_biggest_cluster": 0.021860990673303604, "score_of_first_answer": 0.021860990673303604, "max_score_of_first_answer": 0.021870819851756096, "min_score_of_first_answer": 0.02185117080807686, "avg_score": 0.021860990673303604, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 9.824521839618683e-06, "std_of_first_answer": 9.824521839618683e-06, "avg_entropy_of_first_answer": 4.736616611480713, "lex_sim": 1.0}
{"question": "where was the tv show high chaparral filmed", "answer": ["California"], "scores": [[0.03561621904373169]], "normalized_score": [0.03561621531844139], "tokens": [["\u0120California"]], "entropy": [[6.187567234039307]], "avg_entropy": [6.187567234039307], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "California", "score_of_biggest_cluster": 0.03561621531844139, "score_of_first_answer": 0.03561621531844139, "max_score_of_first_answer": 0.03561621904373169, "min_score_of_first_answer": 0.03561621904373169, "avg_score": 0.03561621531844139, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.187567234039307, "lex_sim": 1.0}
{"question": "who sold out jesus for 30 pieces of silver", "answer": ["Judas"], "scores": [[0.905379593372345, 0.9986768364906311]], "normalized_score": [0.9508846402168274], "tokens": [["\u0120Jud", "as"]], "entropy": [[0.778462290763855, 0.016686147078871727]], "avg_entropy": [0.39757421892136335], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Judas Iscariot", "score_of_biggest_cluster": 0.9508846402168274, "score_of_first_answer": 0.9508846402168274, "max_score_of_first_answer": 0.9986768364906311, "min_score_of_first_answer": 0.905379593372345, "avg_score": 0.9508846402168274, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.046648621559143066, "std_of_first_answer": 0.046648621559143066, "avg_entropy_of_first_answer": 0.39757421892136335, "lex_sim": 1.0}
{"question": "who built the first temple for god in jerusalem", "answer": ["Jebusites"], "scores": [[0.0026579583063721657, 0.6999051570892334, 0.2966282367706299]], "normalized_score": [0.08202255517244339], "tokens": [["\u0120Jeb", "us", "ites"]], "entropy": [[3.1124794483184814, 1.7109891176223755, 1.5040820837020874]], "avg_entropy": [2.1091835498809814], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Solomon", "score_of_biggest_cluster": 0.08202255517244339, "score_of_first_answer": 0.08202255517244339, "max_score_of_first_answer": 0.6999051570892334, "min_score_of_first_answer": 0.0026579583063721657, "avg_score": 0.08202255517244339, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2858135474770139, "std_of_first_answer": 0.2858135474770139, "avg_entropy_of_first_answer": 2.1091835498809814, "lex_sim": 1.0}
{"question": "when is if loving you is wrong coming back season 4", "answer": ["\"I'm coming back\" air date: May 17, 2013 ; season 4 airs in the UK on Wednesdays at 9pm."], "scores": [[0.00247211754322052, 0.0503043569624424, 0.2583301067352295, 0.05714018642902374, 0.49498289823532104, 0.4279394745826721, 0.0031335277017205954, 0.7817478775978088, 0.14990666508674622, 0.05459580570459366, 0.02744663693010807, 0.37159526348114014, 0.051104191690683365, 0.00011952924978686497, 0.17033833265304565, 0.7120286822319031, 0.03362978994846344, 0.05505965277552605, 0.2594778537750244, 0.09422402828931808, 0.5558335185050964, 0.01626608334481716, 0.97275710105896, 0.9793459177017212, 0.5248327851295471, 0.5874543786048889, 0.523551344871521, 0.0816684439778328]], "normalized_score": [0.10403594374656677], "tokens": [["\u0120\"", "I", "'m", "\u0120coming", "\u0120back", "\"", "\u0120air", "\u0120date", ":", "\u0120May", "\u012017", ",", "\u01202013", "\u0120;", "\u0120season", "\u01204", "\u0120airs", "\u0120in", "\u0120the", "\u0120UK", "\u0120on", "\u0120Wed", "nes", "days", "\u0120at", "\u01209", "pm", "."]], "entropy": [[4.802449703216553, 5.9376935958862305, 4.241783142089844, 5.140261650085449, 1.8624188899993896, 2.8672714233398438, 2.8194150924682617, 1.604617953300476, 3.913435459136963, 4.167262077331543, 3.552244186401367, 1.7017055749893188, 2.6114487648010254, 1.1491010189056396, 5.146917819976807, 1.3506147861480713, 3.6244027614593506, 3.1701583862304688, 3.9534013271331787, 2.440760612487793, 2.4211690425872803, 3.8073806762695312, 0.23151099681854248, 0.17118540406227112, 2.0823593139648438, 1.3793439865112305, 1.5199742317199707, 2.5119895935058594]], "avg_entropy": [2.8636527668152536], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "September\u00a019,\u00a02017", "score_of_biggest_cluster": 0.10403594374656677, "score_of_first_answer": 0.10403594374656677, "max_score_of_first_answer": 0.9793459177017212, "min_score_of_first_answer": 0.00011952924978686497, "avg_score": 0.10403594374656677, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30191386500036066, "std_of_first_answer": 0.30191386500036066, "avg_entropy_of_first_answer": 2.8636527668152536, "lex_sim": 1.0}
{"question": "internet based test of english as a foreign language test", "answer": ["TOEFL"], "scores": [[0.205447718501091, 0.9386664628982544, 0.864837646484375]], "normalized_score": [0.5504472851753235], "tokens": [["\u0120TO", "E", "FL"]], "entropy": [[5.219490051269531, 0.4843476414680481, 0.5111965537071228]], "avg_entropy": [2.071678082148234], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "TOEFL", "score_of_biggest_cluster": 0.5504472851753235, "score_of_first_answer": 0.5504472851753235, "max_score_of_first_answer": 0.9386664628982544, "min_score_of_first_answer": 0.205447718501091, "avg_score": 0.5504472851753235, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.32962192119562067, "std_of_first_answer": 0.32962192119562067, "avg_entropy_of_first_answer": 2.071678082148234, "lex_sim": 1.0}
{"question": "who donates a christmas tree to trafalgar square", "answer": ["the mayor of london"], "scores": [[0.07455760985612869, 0.012892442755401134, 0.6510966420173645, 0.5150940418243408, 0.9825663566589355]], "normalized_score": [0.1995925009250641], "tokens": [["\u0120the", "\u0120mayor", "\u0120of", "\u0120l", "ondon"]], "entropy": [[6.13277530670166, 5.288694381713867, 0.9894403219223022, 2.037710428237915, 0.11226202547550201]], "avg_entropy": [2.912176492810249], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Oslo, Norway", "score_of_biggest_cluster": 0.1995925009250641, "score_of_first_answer": 0.1995925009250641, "max_score_of_first_answer": 0.9825663566589355, "min_score_of_first_answer": 0.012892442755401134, "avg_score": 0.1995925009250641, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3633967457250715, "std_of_first_answer": 0.3633967457250715, "avg_entropy_of_first_answer": 2.912176492810249, "lex_sim": 1.0}
{"question": "where does hydrogen peroxide come from in the body", "answer": ["cells"], "scores": [[0.021166278049349785]], "normalized_score": [0.021166279911994934], "tokens": [["\u0120cells"]], "entropy": [[5.4502363204956055]], "avg_entropy": [5.4502363204956055], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "nearly all living cells", "score_of_biggest_cluster": 0.021166279911994934, "score_of_first_answer": 0.021166279911994934, "max_score_of_first_answer": 0.021166278049349785, "min_score_of_first_answer": 0.021166278049349785, "avg_score": 0.021166279911994934, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.4502363204956055, "lex_sim": 1.0}
{"question": "who plays mr wilson in dennis the menace", "answer": ["Dennis Finch"], "scores": [[0.09600190073251724, 0.001137366285547614]], "normalized_score": [0.010449371300637722], "tokens": [["\u0120Dennis", "\u0120Finch"]], "entropy": [[5.5747785568237305, 4.534495830535889]], "avg_entropy": [5.05463719367981], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joseph Sherrard Kearns", "score_of_biggest_cluster": 0.010449371300637722, "score_of_first_answer": 0.010449371300637722, "max_score_of_first_answer": 0.09600190073251724, "min_score_of_first_answer": 0.001137366285547614, "avg_score": 0.010449371300637722, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.047432267223484814, "std_of_first_answer": 0.047432267223484814, "avg_entropy_of_first_answer": 5.05463719367981, "lex_sim": 1.0}
{"question": "when was the last time an american won the new york marathon", "answer": ["1967"], "scores": [[0.0448944978415966]], "normalized_score": [0.0448944978415966], "tokens": [["\u01201967"]], "entropy": [[5.283693313598633]], "avg_entropy": [5.283693313598633], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2009", "score_of_biggest_cluster": 0.0448944978415966, "score_of_first_answer": 0.0448944978415966, "max_score_of_first_answer": 0.0448944978415966, "min_score_of_first_answer": 0.0448944978415966, "avg_score": 0.0448944978415966, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.283693313598633, "lex_sim": 1.0}
{"question": "where is the ucla usc game being played", "answer": ["campus of ucla"], "scores": [[0.0067373416386544704, 0.20119810104370117, 0.2808663249015808, 0.9109108448028564, 0.9941309094429016]], "normalized_score": [0.20300482213497162], "tokens": [["\u0120campus", "\u0120of", "\u0120u", "cl", "a"]], "entropy": [[5.971407890319824, 3.3116965293884277, 3.0586068630218506, 0.541154682636261, 0.059314750134944916]], "avg_entropy": [2.5884361431002616], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "UCLA", "score_of_biggest_cluster": 0.20300482213497162, "score_of_first_answer": 0.20300482213497162, "max_score_of_first_answer": 0.9941309094429016, "min_score_of_first_answer": 0.0067373416386544704, "avg_score": 0.20300482213497162, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3978365068687621, "std_of_first_answer": 0.3978365068687621, "avg_entropy_of_first_answer": 2.5884361431002616, "lex_sim": 1.0}
{"question": "who warned europe to stay out of the americas", "answer": ["Marco il Bello"], "scores": [[0.013962797820568085, 0.0005620767478831112, 0.02302066795527935, 0.8355666399002075]], "normalized_score": [0.019711364060640335], "tokens": [["\u0120Marco", "\u0120il", "\u0120Bell", "o"]], "entropy": [[5.1043291091918945, 1.5958880186080933, 6.0390214920043945, 1.0783395767211914]], "avg_entropy": [3.4543945491313934], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Monroe Doctrine", "score_of_biggest_cluster": 0.019711364060640335, "score_of_first_answer": 0.019711364060640335, "max_score_of_first_answer": 0.8355666399002075, "min_score_of_first_answer": 0.0005620767478831112, "avg_score": 0.019711364060640335, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35648128132701645, "std_of_first_answer": 0.35648128132701645, "avg_entropy_of_first_answer": 3.4543945491313934, "lex_sim": 1.0}
{"question": "the inner sanctuary of the parthenon where the statue of athena is located in known as the", "answer": ["cella"], "scores": [[0.036236125975847244, 0.8445166349411011]], "normalized_score": [0.1749342978000641], "tokens": [["\u0120cell", "a"]], "entropy": [[5.2241597175598145, 0.9377651214599609]], "avg_entropy": [3.0809624195098877], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "cella", "score_of_biggest_cluster": 0.1749342978000641, "score_of_first_answer": 0.1749342978000641, "max_score_of_first_answer": 0.8445166349411011, "min_score_of_first_answer": 0.036236125975847244, "avg_score": 0.1749342978000641, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4041402544826269, "std_of_first_answer": 0.4041402544826269, "avg_entropy_of_first_answer": 3.0809624195098877, "lex_sim": 1.0}
{"question": "where did they get the tree for rockefeller center", "answer": ["Rockefeller Center"], "scores": [[0.02296760305762291, 0.4898950457572937]], "normalized_score": [0.10607410967350006], "tokens": [["\u0120Rockefeller", "\u0120Center"]], "entropy": [[6.510012626647949, 2.82828426361084]], "avg_entropy": [4.6691484451293945], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "State College, PA", "score_of_biggest_cluster": 0.10607410967350006, "score_of_first_answer": 0.10607410967350006, "max_score_of_first_answer": 0.4898950457572937, "min_score_of_first_answer": 0.02296760305762291, "avg_score": 0.10607410967350006, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2334637213498354, "std_of_first_answer": 0.2334637213498354, "avg_entropy_of_first_answer": 4.6691484451293945, "lex_sim": 1.0}
{"question": "when was the mission san antonio de valero built", "answer": ["in 1535"], "scores": [[0.03118472546339035, 0.025102397426962852, 0.01984441466629505]], "normalized_score": [0.02495160512626171], "tokens": [["\u0120in", "\u012015", "35"]], "entropy": [[4.198878765106201, 3.5489282608032227, 4.1981000900268555]], "avg_entropy": [3.9819690386454263], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the 18th century", "score_of_biggest_cluster": 0.02495160512626171, "score_of_first_answer": 0.02495160512626171, "max_score_of_first_answer": 0.03118472546339035, "min_score_of_first_answer": 0.01984441466629505, "avg_score": 0.02495160512626171, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.004633737944310762, "std_of_first_answer": 0.004633737944310762, "avg_entropy_of_first_answer": 3.9819690386454263, "lex_sim": 1.0}
{"question": "who plays young agent o in mib 3", "answer": ["James McAvoy"], "scores": [[0.014334720559418201, 0.14098472893238068, 0.7095164060592651, 0.9991747736930847]], "normalized_score": [0.19455453753471375], "tokens": [["\u0120James", "\u0120Mc", "Av", "oy"]], "entropy": [[6.583530426025391, 5.0958147048950195, 0.8576581478118896, 0.009085457772016525]], "avg_entropy": [3.136522184126079], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alice Eve", "score_of_biggest_cluster": 0.19455453753471375, "score_of_first_answer": 0.19455453753471375, "max_score_of_first_answer": 0.9991747736930847, "min_score_of_first_answer": 0.014334720559418201, "avg_score": 0.19455453753471375, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4041076683180315, "std_of_first_answer": 0.4041076683180315, "avg_entropy_of_first_answer": 3.136522184126079, "lex_sim": 1.0}
{"question": "who proposed the first accurate model of dna", "answer": ["Francis Crick and James Watson"], "scores": [[0.3324350416660309, 0.9165825247764587, 0.9993883371353149, 0.16170433163642883, 0.8944516181945801, 0.9043154120445251]], "normalized_score": [0.584388792514801], "tokens": [["\u0120Francis", "\u0120Cr", "ick", "\u0120and", "\u0120James", "\u0120Watson"]], "entropy": [[2.130704879760742, 0.6462820172309875, 0.007750032469630241, 0.7592541575431824, 0.6949825286865234, 0.41077983379364014]], "avg_entropy": [0.774958908247451], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Watson", "score_of_biggest_cluster": 0.584388792514801, "score_of_first_answer": 0.584388792514801, "max_score_of_first_answer": 0.9993883371353149, "min_score_of_first_answer": 0.16170433163642883, "avg_score": 0.584388792514801, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3268411124526353, "std_of_first_answer": 0.3268411124526353, "avg_entropy_of_first_answer": 0.774958908247451, "lex_sim": 1.0}
{"question": "who unveiled the new coat of arms on 27 april 2000", "answer": ["Elizabeth II of Canada"], "scores": [[0.024427268654108047, 0.8840617537498474, 0.04924245551228523, 0.004725820850580931]], "normalized_score": [0.04734712839126587], "tokens": [["\u0120Elizabeth", "\u0120II", "\u0120of", "\u0120Canada"]], "entropy": [[4.913529396057129, 0.85683673620224, 0.6614665985107422, 1.9072421789169312]], "avg_entropy": [2.0847687274217606], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "South Africa", "score_of_biggest_cluster": 0.04734712839126587, "score_of_first_answer": 0.04734712839126587, "max_score_of_first_answer": 0.8840617537498474, "min_score_of_first_answer": 0.004725820850580931, "avg_score": 0.04734712839126587, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3718292671769415, "std_of_first_answer": 0.3718292671769415, "avg_entropy_of_first_answer": 2.0847687274217606, "lex_sim": 1.0}
{"question": "where did the term spirits for alcohol come from", "answer": ["15th century Germany"], "scores": [[0.004177059978246689, 0.3526369333267212, 0.850688099861145, 0.0191169586032629]], "normalized_score": [0.06995954364538193], "tokens": [["\u012015", "th", "\u0120century", "\u0120Germany"]], "entropy": [[6.122096061706543, 3.4869444370269775, 0.8930047750473022, 2.6202661991119385]], "avg_entropy": [3.2805778682231903], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Middle Eastern alchemy", "score_of_biggest_cluster": 0.06995954364538193, "score_of_first_answer": 0.06995954364538193, "max_score_of_first_answer": 0.850688099861145, "min_score_of_first_answer": 0.004177059978246689, "avg_score": 0.06995954364538193, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3436049262550087, "std_of_first_answer": 0.3436049262550087, "avg_entropy_of_first_answer": 3.2805778682231903, "lex_sim": 1.0}
{"question": "who has the highest corporate taxes in the world", "answer": ["Ireland"], "scores": [[0.052544109523296356]], "normalized_score": [0.05254410579800606], "tokens": [["\u0120Ireland"]], "entropy": [[4.408937454223633]], "avg_entropy": [4.408937454223633], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Argentina", "score_of_biggest_cluster": 0.05254410579800606, "score_of_first_answer": 0.05254410579800606, "max_score_of_first_answer": 0.052544109523296356, "min_score_of_first_answer": 0.052544109523296356, "avg_score": 0.05254410579800606, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.408937454223633, "lex_sim": 1.0}
{"question": "blood alcohol concentration means the parts of alcohol in the blood in relation to what", "answer": ["per cent"], "scores": [[0.004410372115671635, 0.14694827795028687]], "normalized_score": [0.02545774169266224], "tokens": [["\u0120per", "\u0120cent"]], "entropy": [[5.684392929077148, 3.6525185108184814]], "avg_entropy": [4.668455719947815], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "volume of blood", "score_of_biggest_cluster": 0.02545774169266224, "score_of_first_answer": 0.02545774169266224, "max_score_of_first_answer": 0.14694827795028687, "min_score_of_first_answer": 0.004410372115671635, "avg_score": 0.02545774169266224, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07126895291730762, "std_of_first_answer": 0.07126895291730762, "avg_entropy_of_first_answer": 4.668455719947815, "lex_sim": 1.0}
{"question": "what is the definition of the name thomas", "answer": ["the letter T, a, and m, the letter H being the fourth letter"], "scores": [[0.021889397874474525, 0.0038028110284358263, 0.37975648045539856, 0.06918351352214813, 0.033438555896282196, 0.022608516737818718, 0.22779712080955505, 0.02093893475830555, 0.13559748232364655, 0.09196627140045166, 0.09210453182458878, 0.19608081877231598, 0.00595823023468256, 0.15970787405967712, 0.007162272464483976, 0.7087442278862]], "normalized_score": [0.05569813773036003], "tokens": [["\u0120the", "\u0120letter", "\u0120T", ",", "\u0120a", ",", "\u0120and", "\u0120m", ",", "\u0120the", "\u0120letter", "\u0120H", "\u0120being", "\u0120the", "\u0120fourth", "\u0120letter"]], "entropy": [[5.988207817077637, 6.620617389678955, 3.015786647796631, 2.5640459060668945, 4.151763916015625, 5.1663126945495605, 3.5003561973571777, 2.973240852355957, 1.4647356271743774, 4.282103538513184, 4.519632816314697, 3.2346367835998535, 1.3271796703338623, 3.3197736740112305, 5.1173295974731445, 1.7343895435333252]], "avg_entropy": [3.686257041990757], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\"twin\"", "score_of_biggest_cluster": 0.05569813773036003, "score_of_first_answer": 0.05569813773036003, "max_score_of_first_answer": 0.7087442278862, "min_score_of_first_answer": 0.0038028110284358263, "avg_score": 0.05569813773036003, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17836144471244933, "std_of_first_answer": 0.17836144471244933, "avg_entropy_of_first_answer": 3.686257041990757, "lex_sim": 1.0}
{"question": "who is responsible for introducing the principle of uniformitarianism", "answer": ["Charles Lyell"], "scores": [[0.48728299140930176, 0.9043079018592834, 0.9992738366127014]], "normalized_score": [0.7607828378677368], "tokens": [["\u0120Charles", "\u0120Ly", "ell"]], "entropy": [[3.257646083831787, 0.5003384351730347, 0.007844246923923492]], "avg_entropy": [1.2552762553095818], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "James Hutton", "score_of_biggest_cluster": 0.7607828378677368, "score_of_first_answer": 0.7607828378677368, "max_score_of_first_answer": 0.9992738366127014, "min_score_of_first_answer": 0.48728299140930176, "avg_score": 0.7607828378677368, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22237678992384308, "std_of_first_answer": 0.22237678992384308, "avg_entropy_of_first_answer": 1.2552762553095818, "lex_sim": 1.0}
{"question": "where does the water come from to fill lake eyre", "answer": ["wind and tectonic force"], "scores": [[0.0016499387566000223, 0.05417920649051666, 0.0015326255233958364, 0.5963393449783325, 0.9632589221000671, 0.0016102107474580407]], "normalized_score": [0.022411750629544258], "tokens": [["\u0120wind", "\u0120and", "\u0120t", "ect", "onic", "\u0120force"]], "entropy": [[5.466439247131348, 1.7884893417358398, 2.625518798828125, 1.9804526567459106, 0.19043992459774017, 2.3668766021728516]], "avg_entropy": [2.403036095201969], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Warburton River", "score_of_biggest_cluster": 0.022411750629544258, "score_of_first_answer": 0.022411750629544258, "max_score_of_first_answer": 0.9632589221000671, "min_score_of_first_answer": 0.0015326255233958364, "avg_score": 0.022411750629544258, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3763427384862189, "std_of_first_answer": 0.3763427384862189, "avg_entropy_of_first_answer": 2.403036095201969, "lex_sim": 1.0}
{"question": "when did korn's follow the leader come out", "answer": ["September 21, 1994"], "scores": [[0.028099430724978447, 0.0259423665702343, 0.2668691575527191, 0.04493453726172447]], "normalized_score": [0.05437465384602547], "tokens": [["\u0120September", "\u012021", ",", "\u01201994"]], "entropy": [[4.683720588684082, 4.159023284912109, 1.0147428512573242, 3.6906094551086426]], "avg_entropy": [3.3870240449905396], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "August 18, 1998", "score_of_biggest_cluster": 0.05437465384602547, "score_of_first_answer": 0.05437465384602547, "max_score_of_first_answer": 0.2668691575527191, "min_score_of_first_answer": 0.0259423665702343, "avg_score": 0.05437465384602547, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10153830959041572, "std_of_first_answer": 0.10153830959041572, "avg_entropy_of_first_answer": 3.3870240449905396, "lex_sim": 1.0}
{"question": "who began the age of exploration in portugal", "answer": ["Prince Henry, The Navigator"], "scores": [[0.23659582436084747, 0.9792467951774597, 0.012490669265389442, 0.03377837687730789, 0.9645833373069763, 0.9883826375007629]], "normalized_score": [0.21292731165885925], "tokens": [["\u0120Prince", "\u0120Henry", ",", "\u0120The", "\u0120Nav", "igator"]], "entropy": [[3.654484987258911, 0.1727716028690338, 0.6101710796356201, 2.0646109580993652, 0.34150218963623047, 0.0861281007528305]], "avg_entropy": [1.1549448197086651], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "King Dinis of Portugal", "score_of_biggest_cluster": 0.21292731165885925, "score_of_first_answer": 0.21292731165885925, "max_score_of_first_answer": 0.9883826375007629, "min_score_of_first_answer": 0.012490669265389442, "avg_score": 0.21292731165885925, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44735011379261524, "std_of_first_answer": 0.44735011379261524, "avg_entropy_of_first_answer": 1.1549448197086651, "lex_sim": 1.0}
{"question": "who plays jill bigelow in line of duty", "answer": ["Melissa Leon"], "scores": [[0.04004388675093651, 0.0175186675041914]], "normalized_score": [0.026486137881875038], "tokens": [["\u0120Melissa", "\u0120Leon"]], "entropy": [[4.764605522155762, 3.637693166732788]], "avg_entropy": [4.201149344444275], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Polly Walker", "score_of_biggest_cluster": 0.026486137881875038, "score_of_first_answer": 0.026486137881875038, "max_score_of_first_answer": 0.04004388675093651, "min_score_of_first_answer": 0.0175186675041914, "avg_score": 0.026486137881875038, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.011262609623372555, "std_of_first_answer": 0.011262609623372555, "avg_entropy_of_first_answer": 4.201149344444275, "lex_sim": 1.0}
{"question": "who is the all time leading scorer in ncaa tournament history", "answer": ["Michael Jordan"], "scores": [[0.06592804193496704, 0.6550871133804321]], "normalized_score": [0.20781871676445007], "tokens": [["\u0120Michael", "\u0120Jordan"]], "entropy": [[5.039735794067383, 2.5955615043640137]], "avg_entropy": [3.8176486492156982], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pete Maravich", "score_of_biggest_cluster": 0.20781871676445007, "score_of_first_answer": 0.20781871676445007, "max_score_of_first_answer": 0.6550871133804321, "min_score_of_first_answer": 0.06592804193496704, "avg_score": 0.20781871676445007, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29457953572273254, "std_of_first_answer": 0.29457953572273254, "avg_entropy_of_first_answer": 3.8176486492156982, "lex_sim": 1.0}
{"question": "what is the first step in the evolution of the eye", "answer": ["make two eyes"], "scores": [[0.0006268936558626592, 0.009316952899098396, 0.33283287286758423]], "normalized_score": [0.012480482459068298], "tokens": [["\u0120make", "\u0120two", "\u0120eyes"]], "entropy": [[6.411520004272461, 3.8541760444641113, 4.21270751953125]], "avg_entropy": [4.8261345227559405], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "photoreceptor proteins that sense light", "score_of_biggest_cluster": 0.012480482459068298, "score_of_first_answer": 0.012480482459068298, "max_score_of_first_answer": 0.33283287286758423, "min_score_of_first_answer": 0.0006268936558626592, "avg_score": 0.012480482459068298, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1545958459362582, "std_of_first_answer": 0.1545958459362582, "avg_entropy_of_first_answer": 4.8261345227559405, "lex_sim": 1.0}
{"question": "who was the famous scientist that ran the research lab moseley went to in manchester", "answer": ["Charles Robert Mayer"], "scores": [[0.06000678241252899, 0.005513698793947697, 0.008616539649665356]], "normalized_score": [0.014179431833326817], "tokens": [["\u0120Charles", "\u0120Robert", "\u0120Mayer"]], "entropy": [[4.843537330627441, 2.2443816661834717, 3.047642707824707]], "avg_entropy": [3.3785205682118735], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sir Ernest Rutherford", "score_of_biggest_cluster": 0.014179431833326817, "score_of_first_answer": 0.014179431833326817, "max_score_of_first_answer": 0.06000678241252899, "min_score_of_first_answer": 0.005513698793947697, "avg_score": 0.014179431833326817, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.024989066130929393, "std_of_first_answer": 0.024989066130929393, "avg_entropy_of_first_answer": 3.3785205682118735, "lex_sim": 1.0}
{"question": "what does it mean to fuse the spine", "answer": ["back"], "scores": [[0.0014682674081996083]], "normalized_score": [0.0014682675246149302], "tokens": [["\u0120back"]], "entropy": [[6.408474922180176]], "avg_entropy": [6.408474922180176], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "joins two or more vertebrae", "score_of_biggest_cluster": 0.0014682675246149302, "score_of_first_answer": 0.0014682675246149302, "max_score_of_first_answer": 0.0014682674081996083, "min_score_of_first_answer": 0.0014682674081996083, "avg_score": 0.0014682675246149302, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.408474922180176, "lex_sim": 1.0}
{"question": "where are trigger points located in the body", "answer": ["Trigger muscles: neck muscles, shoulders, wrist, head/neck area, hips, legs"], "scores": [[0.021954916417598724, 0.0007264905143529177, 0.0029178590048104525, 0.04791325330734253, 0.014011681079864502, 0.6003043055534363, 0.08294238150119781, 0.8341423869132996, 0.005083783529698849, 0.5636980533599854, 0.004128800705075264, 0.015516647137701511, 0.34652179479599, 0.044123563915491104, 0.40564799308776855, 0.031434472650289536, 0.6954099535942078, 0.04050454497337341]], "normalized_score": [0.04711410030722618], "tokens": [["\u0120Trigger", "\u0120muscles", ":", "\u0120neck", "\u0120muscles", ",", "\u0120shoulders", ",", "\u0120wrist", ",", "\u0120head", "/", "neck", "\u0120area", ",", "\u0120hips", ",", "\u0120legs"]], "entropy": [[5.958705425262451, 1.5428745746612549, 1.2595750093460083, 5.784381866455078, 1.2328336238861084, 1.9155561923980713, 4.107366561889648, 0.8962598443031311, 3.8454489707946777, 1.871185541152954, 4.034266471862793, 1.655238389968872, 3.102141857147217, 2.3416595458984375, 1.4396941661834717, 4.703832149505615, 1.2311129570007324, 3.906402111053467]], "avg_entropy": [2.8238075143761106], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "muscles", "score_of_biggest_cluster": 0.04711410030722618, "score_of_first_answer": 0.04711410030722618, "max_score_of_first_answer": 0.8341423869132996, "min_score_of_first_answer": 0.0007264905143529177, "avg_score": 0.04711410030722618, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27615378991764117, "std_of_first_answer": 0.27615378991764117, "avg_entropy_of_first_answer": 2.8238075143761106, "lex_sim": 1.0}
{"question": "actress who plays penelope garcia on criminal minds", "answer": ["Marg Helgenberger"], "scores": [[0.02426757849752903, 0.14918069541454315, 0.9985047578811646, 0.9955982565879822]], "normalized_score": [0.24493074417114258], "tokens": [["\u0120Marg", "\u0120Hel", "gen", "berger"]], "entropy": [[5.414053916931152, 1.117182970046997, 0.015453843399882317, 0.04897519573569298]], "avg_entropy": [1.6489164815284312], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kirsten Simone Vangsness", "score_of_biggest_cluster": 0.24493074417114258, "score_of_first_answer": 0.24493074417114258, "max_score_of_first_answer": 0.9985047578811646, "min_score_of_first_answer": 0.02426757849752903, "avg_score": 0.24493074417114258, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4573023584954714, "std_of_first_answer": 0.4573023584954714, "avg_entropy_of_first_answer": 1.6489164815284312, "lex_sim": 1.0}
{"question": "who plays colin on young and the restless", "answer": ["Kevin Connolly"], "scores": [[0.007508584298193455, 0.03337112069129944, 0.8780174851417542]], "normalized_score": [0.06036853790283203], "tokens": [["\u0120Kevin", "\u0120Conn", "olly"]], "entropy": [[5.962629318237305, 4.094923973083496, 0.8938557505607605]], "avg_entropy": [3.6504696806271872], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tristan Rogers", "score_of_biggest_cluster": 0.06036853790283203, "score_of_first_answer": 0.06036853790283203, "max_score_of_first_answer": 0.8780174851417542, "min_score_of_first_answer": 0.007508584298193455, "avg_score": 0.06036853790283203, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4044038270804622, "std_of_first_answer": 0.4044038270804622, "avg_entropy_of_first_answer": 3.6504696806271872, "lex_sim": 1.0}
{"question": "who pays medical bills in great britain where does the money come from to pay these bills", "answer": ["national health service"], "scores": [[0.011321896687150002, 0.3508259654045105, 0.8940920829772949]], "normalized_score": [0.15256834030151367], "tokens": [["\u0120national", "\u0120health", "\u0120service"]], "entropy": [[3.261378526687622, 1.0651408433914185, 0.6794265508651733]], "avg_entropy": [1.668648640314738], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "general taxation", "score_of_biggest_cluster": 0.15256834030151367, "score_of_first_answer": 0.15256834030151367, "max_score_of_first_answer": 0.8940920829772949, "min_score_of_first_answer": 0.011321896687150002, "avg_score": 0.15256834030151367, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3635754986520812, "std_of_first_answer": 0.3635754986520812, "avg_entropy_of_first_answer": 1.668648640314738, "lex_sim": 1.0}
{"question": "who played gino in a place to call home", "answer": ["gino savoldi"], "scores": [[0.007556193973869085, 0.9182031750679016, 0.011842071078717709, 0.3645245134830475, 0.7384164929389954]], "normalized_score": [0.1172032356262207], "tokens": [["\u0120g", "ino", "\u0120sav", "old", "i"]], "entropy": [[6.107569694519043, 0.6477581262588501, 5.3064069747924805, 1.9262170791625977, 0.8601080179214478]], "avg_entropy": [2.969611978530884], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Aldo Mignone", "score_of_biggest_cluster": 0.1172032356262207, "score_of_first_answer": 0.1172032356262207, "max_score_of_first_answer": 0.9182031750679016, "min_score_of_first_answer": 0.007556193973869085, "avg_score": 0.1172032356262207, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.371124977377056, "std_of_first_answer": 0.371124977377056, "avg_entropy_of_first_answer": 2.969611978530884, "lex_sim": 1.0}
{"question": "who is the most losing team in nfl history", "answer": ["the san francisco 49ers"], "scores": [[0.20637550950050354, 0.008691107854247093, 0.42218443751335144, 0.9770093560218811, 0.9655346870422363, 0.9835491180419922]], "normalized_score": [0.29816022515296936], "tokens": [["\u0120the", "\u0120san", "\u0120franc", "isco", "\u012049", "ers"]], "entropy": [[4.472230911254883, 5.937616348266602, 2.0834875106811523, 0.13213682174682617, 0.3311746418476105, 0.10986672341823578]], "avg_entropy": [2.177752159535885], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Arizona Cardinals", "score_of_biggest_cluster": 0.29816022515296936, "score_of_first_answer": 0.29816022515296936, "max_score_of_first_answer": 0.9835491180419922, "min_score_of_first_answer": 0.008691107854247093, "avg_score": 0.29816022515296936, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39975877445867286, "std_of_first_answer": 0.39975877445867286, "avg_entropy_of_first_answer": 2.177752159535885, "lex_sim": 1.0}
{"question": "who wrote the song after you've gone", "answer": ["after you've gone"], "scores": [[0.004333629738539457, 0.7045062780380249, 0.665070116519928, 0.9677685499191284]], "normalized_score": [0.21054449677467346], "tokens": [["\u0120after", "\u0120you", "'ve", "\u0120gone"]], "entropy": [[6.858515739440918, 1.8993310928344727, 1.5698885917663574, 0.23459921777248383]], "avg_entropy": [2.640583660453558], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Turner Layton", "score_of_biggest_cluster": 0.21054449677467346, "score_of_first_answer": 0.21054449677467346, "max_score_of_first_answer": 0.9677685499191284, "min_score_of_first_answer": 0.004333629738539457, "avg_score": 0.21054449677467346, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3550976858580534, "std_of_first_answer": 0.3550976858580534, "avg_entropy_of_first_answer": 2.640583660453558, "lex_sim": 1.0}
{"question": "who was elected the president of constituent assembly", "answer": ["Mohd Ali Salman"], "scores": [[0.004251623060554266, 0.014973372220993042, 0.18584811687469482, 0.0019717859104275703]], "normalized_score": [0.0123587129637599], "tokens": [["\u0120Moh", "d", "\u0120Ali", "\u0120Salman"]], "entropy": [[6.125295639038086, 0.5874225497245789, 3.9130818843841553, 3.8281760215759277]], "avg_entropy": [3.613494023680687], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rajendra Prasad", "score_of_biggest_cluster": 0.0123587129637599, "score_of_first_answer": 0.0123587129637599, "max_score_of_first_answer": 0.18584811687469482, "min_score_of_first_answer": 0.0019717859104275703, "avg_score": 0.0123587129637599, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07757059882133954, "std_of_first_answer": 0.07757059882133954, "avg_entropy_of_first_answer": 3.613494023680687, "lex_sim": 1.0}
{"question": "what type of artwork was created in the safavid empire", "answer": ["murals"], "scores": [[0.04159333556890488, 0.9922771453857422]], "normalized_score": [0.20315539836883545], "tokens": [["\u0120mur", "als"]], "entropy": [[5.694503307342529, 0.0805649384856224]], "avg_entropy": [2.887534122914076], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "architecture", "score_of_biggest_cluster": 0.20315539836883545, "score_of_first_answer": 0.20315539836883545, "max_score_of_first_answer": 0.9922771453857422, "min_score_of_first_answer": 0.04159333556890488, "avg_score": 0.20315539836883545, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.47534190490841866, "std_of_first_answer": 0.47534190490841866, "avg_entropy_of_first_answer": 2.887534122914076, "lex_sim": 1.0}
{"question": "when is the 5th round fa cup played", "answer": ["October 3"], "scores": [[0.039419520646333694, 0.02264656126499176]], "normalized_score": [0.029878366738557816], "tokens": [["\u0120October", "\u01203"]], "entropy": [[4.915220260620117, 3.672368049621582]], "avg_entropy": [4.29379415512085], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "29 January 2018", "score_of_biggest_cluster": 0.029878366738557816, "score_of_first_answer": 0.029878366738557816, "max_score_of_first_answer": 0.039419520646333694, "min_score_of_first_answer": 0.02264656126499176, "avg_score": 0.029878366738557816, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.008386479690670967, "std_of_first_answer": 0.008386479690670967, "avg_entropy_of_first_answer": 4.29379415512085, "lex_sim": 1.0}
{"question": "who plays the queen in victoria and abdul", "answer": ["Olivia Wilde"], "scores": [[0.051387351006269455, 0.03044312447309494]], "normalized_score": [0.03955238685011864], "tokens": [["\u0120Olivia", "\u0120Wilde"]], "entropy": [[5.169076919555664, 1.7187168598175049]], "avg_entropy": [3.4438968896865845], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Judi Dench", "score_of_biggest_cluster": 0.03955238685011864, "score_of_first_answer": 0.03955238685011864, "max_score_of_first_answer": 0.051387351006269455, "min_score_of_first_answer": 0.03044312447309494, "avg_score": 0.03955238685011864, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.010472113266587257, "std_of_first_answer": 0.010472113266587257, "avg_entropy_of_first_answer": 3.4438968896865845, "lex_sim": 1.0}
{"question": "who sings the song let me love you", "answer": ["kurt cobain"], "scores": [[0.005035721231251955, 0.01614706590771675, 0.17863716185092926, 0.9975008368492126]], "normalized_score": [0.06169643625617027], "tokens": [["\u0120k", "urt", "\u0120cob", "ain"]], "entropy": [[6.224617004394531, 4.148651599884033, 5.001575946807861, 0.027387872338294983]], "avg_entropy": [3.85055810585618], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "DJ Snake featuring Justin Bieber", "score_of_biggest_cluster": 0.06169643625617027, "score_of_first_answer": 0.06169643625617027, "max_score_of_first_answer": 0.9975008368492126, "min_score_of_first_answer": 0.005035721231251955, "avg_score": 0.06169643625617027, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4089043211083081, "std_of_first_answer": 0.4089043211083081, "avg_entropy_of_first_answer": 3.85055810585618, "lex_sim": 1.0}
{"question": "who sings in cry baby for johnny depp", "answer": ["johnny depp"], "scores": [[0.002604908775538206, 0.891889214515686, 0.9319201111793518, 0.9973106384277344]], "normalized_score": [0.21556496620178223], "tokens": [["\u0120john", "ny", "\u0120de", "pp"]], "entropy": [[6.386678218841553, 0.7487059831619263, 0.5837623476982117, 0.02238989621400833]], "avg_entropy": [1.9353841114789248], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "James Intveld", "score_of_biggest_cluster": 0.21556496620178223, "score_of_first_answer": 0.21556496620178223, "max_score_of_first_answer": 0.9973106384277344, "min_score_of_first_answer": 0.002604908775538206, "avg_score": 0.21556496620178223, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40780547452328975, "std_of_first_answer": 0.40780547452328975, "avg_entropy_of_first_answer": 1.9353841114789248, "lex_sim": 1.0}
{"question": "when is the next scandal episode coming out", "answer": ["April 25"], "scores": [[0.05709047615528107, 0.017950890585780144]], "normalized_score": [0.03201289474964142], "tokens": [["\u0120April", "\u012025"]], "entropy": [[4.855295181274414, 3.4741053581237793]], "avg_entropy": [4.164700269699097], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "February\u00a01,\u00a02018", "score_of_biggest_cluster": 0.03201289474964142, "score_of_first_answer": 0.03201289474964142, "max_score_of_first_answer": 0.05709047615528107, "min_score_of_first_answer": 0.017950890585780144, "avg_score": 0.03201289474964142, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01956979278475046, "std_of_first_answer": 0.01956979278475046, "avg_entropy_of_first_answer": 4.164700269699097, "lex_sim": 1.0}
{"question": "when did the the regulatory reform (fire safety) order 2005 first come into effect", "answer": ["2004"], "scores": [[0.019983572885394096]], "normalized_score": [0.019983572885394096], "tokens": [["\u01202004"]], "entropy": [[3.8717846870422363]], "avg_entropy": [3.8717846870422363], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1 October 2006", "score_of_biggest_cluster": 0.019983572885394096, "score_of_first_answer": 0.019983572885394096, "max_score_of_first_answer": 0.019983572885394096, "min_score_of_first_answer": 0.019983572885394096, "avg_score": 0.019983572885394096, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.8717846870422363, "lex_sim": 1.0}
{"question": "what was the code name of the us program to develop an atomic bomb", "answer": ["blue ribbon"], "scores": [[0.008371719159185886, 0.029204724356532097]], "normalized_score": [0.015636298805475235], "tokens": [["\u0120blue", "\u0120ribbon"]], "entropy": [[5.708264350891113, 4.273090839385986]], "avg_entropy": [4.99067759513855], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Manhattan", "score_of_biggest_cluster": 0.015636298805475235, "score_of_first_answer": 0.015636298805475235, "max_score_of_first_answer": 0.029204724356532097, "min_score_of_first_answer": 0.008371719159185886, "avg_score": 0.015636298805475235, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.010416502598673105, "std_of_first_answer": 0.010416502598673105, "avg_entropy_of_first_answer": 4.99067759513855, "lex_sim": 1.0}
{"question": "where did the beer hall putsch take place", "answer": ["putsch beer hall"], "scores": [[0.005466463975608349, 0.9915103912353516, 0.04169095680117607, 0.7714852094650269]], "normalized_score": [0.11490615457296371], "tokens": [["\u0120puts", "ch", "\u0120beer", "\u0120hall"]], "entropy": [[5.093728065490723, 0.07923270761966705, 2.5700535774230957, 1.4163875579833984]], "avg_entropy": [2.289850477129221], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Munich, Bavaria", "score_of_biggest_cluster": 0.11490615457296371, "score_of_first_answer": 0.11490615457296371, "max_score_of_first_answer": 0.9915103912353516, "min_score_of_first_answer": 0.005466463975608349, "avg_score": 0.11490615457296371, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.43614413104146116, "std_of_first_answer": 0.43614413104146116, "avg_entropy_of_first_answer": 2.289850477129221, "lex_sim": 1.0}
{"question": "who gets the most punishments on impractical jokers", "answer": ["Jim Carrey"], "scores": [[0.007219396997243166, 0.2144278585910797, 0.9994804263114929]], "normalized_score": [0.11566060781478882], "tokens": [["\u0120Jim", "\u0120Car", "rey"]], "entropy": [[7.01478385925293, 4.47178840637207, 0.006718091666698456]], "avg_entropy": [3.8310967857638993], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sal", "score_of_biggest_cluster": 0.11566060781478882, "score_of_first_answer": 0.11566060781478882, "max_score_of_first_answer": 0.9994804263114929, "min_score_of_first_answer": 0.007219396997243166, "avg_score": 0.11566060781478882, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42737244112993955, "std_of_first_answer": 0.42737244112993955, "avg_entropy_of_first_answer": 3.8310967857638993, "lex_sim": 1.0}
{"question": "when does canada switch to daylight savings time", "answer": ["spring, 2 march"], "scores": [[0.0677453801035881, 0.022576503455638885, 0.009053844958543777, 0.03522540628910065]], "normalized_score": [0.02642751857638359], "tokens": [["\u0120spring", ",", "\u01202", "\u0120march"]], "entropy": [[3.954075336456299, 1.754077672958374, 5.22955322265625, 2.796400547027588]], "avg_entropy": [3.4335266947746277], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the second Sunday of March", "score_of_biggest_cluster": 0.02642751857638359, "score_of_first_answer": 0.02642751857638359, "max_score_of_first_answer": 0.0677453801035881, "min_score_of_first_answer": 0.009053844958543777, "avg_score": 0.02642751857638359, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02175183910387391, "std_of_first_answer": 0.02175183910387391, "avg_entropy_of_first_answer": 3.4335266947746277, "lex_sim": 1.0}
{"question": "where do they grow hops in the us", "answer": ["the west coast states, especially washington and oregon"], "scores": [[0.03698458895087242, 0.025631219148635864, 0.5758072137832642, 0.003144016256555915, 0.017133453860878944, 0.05770086869597435, 0.004038507118821144, 0.994211733341217, 0.19169649481773376, 0.39987850189208984, 0.998229444026947]], "normalized_score": [0.07644835859537125], "tokens": [["\u0120the", "\u0120west", "\u0120coast", "\u0120states", ",", "\u0120especially", "\u0120washing", "ton", "\u0120and", "\u0120o", "regon"]], "entropy": [[5.215691089630127, 3.9701273441314697, 1.0567119121551514, 0.833691418170929, 1.1912319660186768, 4.327527046203613, 3.048353672027588, 0.05408675596117973, 1.4230914115905762, 2.194429874420166, 0.014063151553273201]], "avg_entropy": [2.1208186947147953], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yakima (Washington)", "score_of_biggest_cluster": 0.07644835859537125, "score_of_first_answer": 0.07644835859537125, "max_score_of_first_answer": 0.998229444026947, "min_score_of_first_answer": 0.003144016256555915, "avg_score": 0.07644835859537125, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37247597022052503, "std_of_first_answer": 0.37247597022052503, "avg_entropy_of_first_answer": 2.1208186947147953, "lex_sim": 1.0}
{"question": "how many jimmy johns are there in the us", "answer": ["700"], "scores": [[0.0023994003422558308]], "normalized_score": [0.002399400109425187], "tokens": [["\u0120700"]], "entropy": [[5.349564075469971]], "avg_entropy": [5.349564075469971], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "more than 2,500", "score_of_biggest_cluster": 0.002399400109425187, "score_of_first_answer": 0.002399400109425187, "max_score_of_first_answer": 0.0023994003422558308, "min_score_of_first_answer": 0.0023994003422558308, "avg_score": 0.002399400109425187, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.349564075469971, "lex_sim": 1.0}
{"question": "who plays nikko in the wizard of oz", "answer": ["Russell Crowe"], "scores": [[0.01698135770857334, 0.6236785054206848, 0.9986881613731384]], "normalized_score": [0.219510018825531], "tokens": [["\u0120Russell", "\u0120Cro", "we"]], "entropy": [[6.067344665527344, 2.0395705699920654, 0.016186125576496124]], "avg_entropy": [2.707700453698635], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Patrick Walshe", "score_of_biggest_cluster": 0.219510018825531, "score_of_first_answer": 0.219510018825531, "max_score_of_first_answer": 0.9986881613731384, "min_score_of_first_answer": 0.01698135770857334, "avg_score": 0.219510018825531, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40448347301230797, "std_of_first_answer": 0.40448347301230797, "avg_entropy_of_first_answer": 2.707700453698635, "lex_sim": 1.0}
{"question": "who was the actor who played grizzly adams", "answer": ["Jason Mewes"], "scores": [[0.00785717461258173, 0.026204511523246765, 0.9916988611221313]], "normalized_score": [0.0588853694498539], "tokens": [["\u0120Jason", "\u0120Mew", "es"]], "entropy": [[5.436546325683594, 4.413952350616455, 0.0797470360994339]], "avg_entropy": [3.310081904133161], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\"Dan\" Haggerty", "score_of_biggest_cluster": 0.0588853694498539, "score_of_first_answer": 0.0588853694498539, "max_score_of_first_answer": 0.9916988611221313, "min_score_of_first_answer": 0.00785717461258173, "avg_score": 0.0588853694498539, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45952395995965717, "std_of_first_answer": 0.45952395995965717, "avg_entropy_of_first_answer": 3.310081904133161, "lex_sim": 1.0}
{"question": "what part of dc is howard university in", "answer": ["U.S. of A."], "scores": [[0.02545052394270897, 0.12177868187427521, 0.9516640305519104, 0.9478857517242432, 0.0019673018250614405, 0.6250319480895996, 0.4804188311100006]], "normalized_score": [0.1492745727300644], "tokens": [["\u0120U", ".", "S", ".", "\u0120of", "\u0120A", "."]], "entropy": [[6.135174751281738, 3.29156231880188, 0.4221198558807373, 0.36089015007019043, 2.8533687591552734, 2.3525216579437256, 1.033595323562622]], "avg_entropy": [2.3498904023851668], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in northwest Washington", "score_of_biggest_cluster": 0.1492745727300644, "score_of_first_answer": 0.1492745727300644, "max_score_of_first_answer": 0.9516640305519104, "min_score_of_first_answer": 0.0019673018250614405, "avg_score": 0.1492745727300644, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3816918695380565, "std_of_first_answer": 0.3816918695380565, "avg_entropy_of_first_answer": 2.3498904023851668, "lex_sim": 1.0}
{"question": "nba record for most double doubles in a season", "answer": ["26"], "scores": [[0.025713833048939705]], "normalized_score": [0.025713836774230003], "tokens": [["\u012026"]], "entropy": [[5.246420860290527]], "avg_entropy": [5.246420860290527], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tim Duncan", "score_of_biggest_cluster": 0.025713836774230003, "score_of_first_answer": 0.025713836774230003, "max_score_of_first_answer": 0.025713833048939705, "min_score_of_first_answer": 0.025713833048939705, "avg_score": 0.025713836774230003, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.246420860290527, "lex_sim": 1.0}
{"question": "who sang how long has this been going on song", "answer": ["How Long Does It Take To Get To Sainte Marie du Mont"], "scores": [[0.01754695735871792, 0.6320664286613464, 0.007094354834407568, 0.515536367893219, 0.9287417531013489, 0.214970663189888, 0.2859102487564087, 0.861611545085907, 0.5794811248779297, 0.04514314979314804, 0.129157155752182, 0.20093396306037903, 0.597417950630188, 0.8135689496994019]], "normalized_score": [0.2208886444568634], "tokens": [["\u0120How", "\u0120Long", "\u0120Does", "\u0120It", "\u0120Take", "\u0120To", "\u0120Get", "\u0120To", "\u0120S", "ain", "te", "\u0120Marie", "\u0120du", "\u0120Mont"]], "entropy": [[6.876117706298828, 1.3155603408813477, 2.2186169624328613, 2.1279163360595703, 0.47168081998825073, 2.123361587524414, 2.1967031955718994, 0.7187581062316895, 1.5966155529022217, 3.3367843627929688, 2.2007603645324707, 1.974661111831665, 2.033325433731079, 1.2619106769561768]], "avg_entropy": [2.175198039838246], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ace", "score_of_biggest_cluster": 0.2208886444568634, "score_of_first_answer": 0.2208886444568634, "max_score_of_first_answer": 0.9287417531013489, "min_score_of_first_answer": 0.007094354834407568, "avg_score": 0.2208886444568634, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31454450749011315, "std_of_first_answer": 0.31454450749011315, "avg_entropy_of_first_answer": 2.175198039838246, "lex_sim": 1.0}
{"question": "the man in the high castle episode 1 season 1", "answer": ["Lord Edam"], "scores": [[0.01083295326679945, 0.1265760064125061, 0.009363070130348206]], "normalized_score": [0.02341560646891594], "tokens": [["\u0120Lord", "\u0120Ed", "am"]], "entropy": [[6.786208152770996, 5.511785507202148, 2.8572587966918945]], "avg_entropy": [5.051750818888347], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The New World", "score_of_biggest_cluster": 0.02341560646891594, "score_of_first_answer": 0.02341560646891594, "max_score_of_first_answer": 0.1265760064125061, "min_score_of_first_answer": 0.009363070130348206, "avg_score": 0.02341560646891594, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05491153222311311, "std_of_first_answer": 0.05491153222311311, "avg_entropy_of_first_answer": 5.051750818888347, "lex_sim": 1.0}
{"question": "when did the celebrities enter the big brother house", "answer": ["Big Brother season 13"], "scores": [[0.016660332679748535, 0.879909336566925, 0.04360831528902054, 0.051869556307792664]], "normalized_score": [0.07588411122560501], "tokens": [["\u0120Big", "\u0120Brother", "\u0120season", "\u012013"]], "entropy": [[5.198562145233154, 0.9770601987838745, 5.078566551208496, 3.512106418609619]], "avg_entropy": [3.691573828458786], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "February\u00a07,\u00a02018", "score_of_biggest_cluster": 0.07588411122560501, "score_of_first_answer": 0.07588411122560501, "max_score_of_first_answer": 0.879909336566925, "min_score_of_first_answer": 0.016660332679748535, "avg_score": 0.07588411122560501, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3650584071327346, "std_of_first_answer": 0.3650584071327346, "avg_entropy_of_first_answer": 3.691573828458786, "lex_sim": 1.0}
{"question": "who were the the continent of the americas named after logically", "answer": ["North America"], "scores": [[0.46460336446762085, 0.4449393153190613]], "normalized_score": [0.45466503500938416], "tokens": [["\u0120North", "\u0120America"]], "entropy": [[3.4904932975769043, 1.4473979473114014]], "avg_entropy": [2.468945622444153], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Amerigo Vespucci", "score_of_biggest_cluster": 0.45466503500938416, "score_of_first_answer": 0.45466503500938416, "max_score_of_first_answer": 0.46460336446762085, "min_score_of_first_answer": 0.4449393153190613, "avg_score": 0.45466503500938416, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.009832024574279785, "std_of_first_answer": 0.009832024574279785, "avg_entropy_of_first_answer": 2.468945622444153, "lex_sim": 1.0}
{"question": "how do you spell padawan from star wars", "answer": ["p A D A W A N"], "scores": [[0.06157218664884567, 0.011720898561179638, 0.670112669467926, 0.8500986099243164, 0.5195446014404297, 0.8802062273025513, 0.909978449344635]], "normalized_score": [0.28965821862220764], "tokens": [["\u0120p", "\u0120A", "\u0120D", "\u0120A", "\u0120W", "\u0120A", "\u0120N"]], "entropy": [[5.104055404663086, 3.8838653564453125, 1.675427794456482, 0.8325871229171753, 1.9303096532821655, 0.5900698900222778, 0.48255297541618347]], "avg_entropy": [2.0712668853146687], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Padawan", "score_of_biggest_cluster": 0.28965821862220764, "score_of_first_answer": 0.28965821862220764, "max_score_of_first_answer": 0.909978449344635, "min_score_of_first_answer": 0.011720898561179638, "avg_score": 0.28965821862220764, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3529337625550377, "std_of_first_answer": 0.3529337625550377, "avg_entropy_of_first_answer": 2.0712668853146687, "lex_sim": 1.0}
{"question": "who ran the fastest 40 yard dash in the nfl", "answer": ["Michael Jackson"], "scores": [[0.10301180183887482, 0.003014535643160343]], "normalized_score": [0.01762194186449051], "tokens": [["\u0120Michael", "\u0120Jackson"]], "entropy": [[5.214686393737793, 4.116369724273682]], "avg_entropy": [4.665528059005737], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jakeem Grant", "score_of_biggest_cluster": 0.01762194186449051, "score_of_first_answer": 0.01762194186449051, "max_score_of_first_answer": 0.10301180183887482, "min_score_of_first_answer": 0.003014535643160343, "avg_score": 0.01762194186449051, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04999863309785724, "std_of_first_answer": 0.04999863309785724, "avg_entropy_of_first_answer": 4.665528059005737, "lex_sim": 1.0}
{"question": "who's the original singer of help me make it through the night", "answer": ["Amy"], "scores": [[0.015041804872453213]], "normalized_score": [0.015041806735098362], "tokens": [["\u0120Amy"]], "entropy": [[6.2264227867126465]], "avg_entropy": [6.2264227867126465], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kris Kristofferson", "score_of_biggest_cluster": 0.015041806735098362, "score_of_first_answer": 0.015041806735098362, "max_score_of_first_answer": 0.015041804872453213, "min_score_of_first_answer": 0.015041804872453213, "avg_score": 0.015041806735098362, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.2264227867126465, "lex_sim": 1.0}
{"question": "how many senators are there in the us senate", "answer": ["fifty"], "scores": [[0.0051058074459433556]], "normalized_score": [0.0051058074459433556], "tokens": [["\u0120fifty"]], "entropy": [[3.4173426628112793]], "avg_entropy": [3.4173426628112793], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "100", "score_of_biggest_cluster": 0.0051058074459433556, "score_of_first_answer": 0.0051058074459433556, "max_score_of_first_answer": 0.0051058074459433556, "min_score_of_first_answer": 0.0051058074459433556, "avg_score": 0.0051058074459433556, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.4173426628112793, "lex_sim": 1.0}
{"question": "when did the song the joker come out", "answer": ["1935 to 1951"], "scores": [[0.02757832407951355, 0.0006115223513916135, 0.0025554492603987455]], "normalized_score": [0.0035060306545346975], "tokens": [["\u01201935", "\u0120to", "\u01201951"]], "entropy": [[5.426830291748047, 0.38248297572135925, 4.272615432739258]], "avg_entropy": [3.360642900069555], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1973", "score_of_biggest_cluster": 0.0035060306545346975, "score_of_first_answer": 0.0035060306545346975, "max_score_of_first_answer": 0.02757832407951355, "min_score_of_first_answer": 0.0006115223513916135, "avg_score": 0.0035060306545346975, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.012279755297437467, "std_of_first_answer": 0.012279755297437467, "avg_entropy_of_first_answer": 3.360642900069555, "lex_sim": 1.0}
{"question": "who sings the song you'll never find another love like mine", "answer": ["The Darkness"], "scores": [[0.01661684922873974, 0.0076945167966187]], "normalized_score": [0.011307459324598312], "tokens": [["\u0120The", "\u0120Darkness"]], "entropy": [[6.232938766479492, 5.017223358154297]], "avg_entropy": [5.6250810623168945], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lou Rawls", "score_of_biggest_cluster": 0.011307459324598312, "score_of_first_answer": 0.011307459324598312, "max_score_of_first_answer": 0.01661684922873974, "min_score_of_first_answer": 0.0076945167966187, "avg_score": 0.011307459324598312, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.004461166216060519, "std_of_first_answer": 0.004461166216060519, "avg_entropy_of_first_answer": 5.6250810623168945, "lex_sim": 1.0}
{"question": "what is the average salary for a us congressman", "answer": ["about $174,869"], "scores": [[0.012297878041863441, 0.7575483322143555, 0.5580042600631714, 0.825505256652832, 0.0021298278588801622, 0.022975929081439972]], "normalized_score": [0.07709672302007675], "tokens": [["\u0120about", "\u0120$", "174", ",", "8", "69"]], "entropy": [[1.850421667098999, 1.7987172603607178, 2.8659846782684326, 0.775905966758728, 0.8432945609092712, 4.003877639770508]], "avg_entropy": [2.0230336288611093], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "$174,000", "score_of_biggest_cluster": 0.07709672302007675, "score_of_first_answer": 0.07709672302007675, "max_score_of_first_answer": 0.825505256652832, "min_score_of_first_answer": 0.0021298278588801622, "avg_score": 0.07709672302007675, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3597317873833181, "std_of_first_answer": 0.3597317873833181, "avg_entropy_of_first_answer": 2.0230336288611093, "lex_sim": 1.0}
{"question": "what city was zeus the patron god of", "answer": ["Mount Olympus"], "scores": [[0.005649229045957327, 0.8857545852661133]], "normalized_score": [0.07073775678873062], "tokens": [["\u0120Mount", "\u0120Olympus"]], "entropy": [[3.7693753242492676, 0.8601980805397034]], "avg_entropy": [2.3147867023944855], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Olympia", "score_of_biggest_cluster": 0.07073775678873062, "score_of_first_answer": 0.07073775678873062, "max_score_of_first_answer": 0.8857545852661133, "min_score_of_first_answer": 0.005649229045957327, "avg_score": 0.07073775678873062, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.440052678110078, "std_of_first_answer": 0.440052678110078, "avg_entropy_of_first_answer": 2.3147867023944855, "lex_sim": 1.0}
{"question": "who was the 19th c. artist who was both painter and graphic designer", "answer": ["William Powell"], "scores": [[0.05916747823357582, 0.0032069797161966562]], "normalized_score": [0.013774937018752098], "tokens": [["\u0120William", "\u0120Powell"]], "entropy": [[5.20623779296875, 2.779742479324341]], "avg_entropy": [3.9929901361465454], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ren\u00e9 Georges Hermann-Paul", "score_of_biggest_cluster": 0.013774937018752098, "score_of_first_answer": 0.013774937018752098, "max_score_of_first_answer": 0.05916747823357582, "min_score_of_first_answer": 0.0032069797161966562, "avg_score": 0.013774937018752098, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.027980249258689582, "std_of_first_answer": 0.027980249258689582, "avg_entropy_of_first_answer": 3.9929901361465454, "lex_sim": 1.0}
{"question": "who developed a set of postulates to prove that specific microorganisms cause disease", "answer": ["bernard lesh"], "scores": [[0.0003733064513653517, 0.22012418508529663, 0.8277086019515991, 0.04449155181646347, 0.024867849424481392]], "normalized_score": [0.037610165774822235], "tokens": [["\u0120b", "ern", "ard", "\u0120l", "esh"]], "entropy": [[3.641451597213745, 3.6842384338378906, 1.1028658151626587, 3.6453027725219727, 4.929444313049316]], "avg_entropy": [3.400660586357117], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Robert Koch", "score_of_biggest_cluster": 0.037610165774822235, "score_of_first_answer": 0.037610165774822235, "max_score_of_first_answer": 0.8277086019515991, "min_score_of_first_answer": 0.0003733064513653517, "avg_score": 0.037610165774822235, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31188578264422767, "std_of_first_answer": 0.31188578264422767, "avg_entropy_of_first_answer": 3.400660586357117, "lex_sim": 1.0}
{"question": "number of degree of freedom for plane mechanism", "answer": ["3"], "scores": [[0.14025212824344635]], "normalized_score": [0.14025212824344635], "tokens": [["\u01203"]], "entropy": [[3.2949235439300537]], "avg_entropy": [3.2949235439300537], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "six", "score_of_biggest_cluster": 0.14025212824344635, "score_of_first_answer": 0.14025212824344635, "max_score_of_first_answer": 0.14025212824344635, "min_score_of_first_answer": 0.14025212824344635, "avg_score": 0.14025212824344635, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.2949235439300537, "lex_sim": 1.0}
{"question": "who had created the second bank of the united states", "answer": ["Thomas Jefferson"], "scores": [[0.09965486824512482, 0.8182637095451355]], "normalized_score": [0.2855590283870697], "tokens": [["\u0120Thomas", "\u0120Jefferson"]], "entropy": [[4.490392684936523, 1.5396349430084229]], "avg_entropy": [3.015013813972473], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "President James Madison", "score_of_biggest_cluster": 0.2855590283870697, "score_of_first_answer": 0.2855590283870697, "max_score_of_first_answer": 0.8182637095451355, "min_score_of_first_answer": 0.09965486824512482, "avg_score": 0.2855590283870697, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35930442065000534, "std_of_first_answer": 0.35930442065000534, "avg_entropy_of_first_answer": 3.015013813972473, "lex_sim": 1.0}
{"question": "who wrote old flames cant hold a candle to you", "answer": ["William Shakespeare"], "scores": [[0.006417085416615009, 0.050772011280059814]], "normalized_score": [0.018050160259008408], "tokens": [["\u0120William", "\u0120Shakespeare"]], "entropy": [[6.62285852432251, 3.6867356300354004]], "avg_entropy": [5.154797077178955], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pebe Sebert", "score_of_biggest_cluster": 0.018050160259008408, "score_of_first_answer": 0.018050160259008408, "max_score_of_first_answer": 0.050772011280059814, "min_score_of_first_answer": 0.006417085416615009, "avg_score": 0.018050160259008408, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.022177462931722403, "std_of_first_answer": 0.022177462931722403, "avg_entropy_of_first_answer": 5.154797077178955, "lex_sim": 1.0}
{"question": "who sang for lee bowman in my dream is yours", "answer": ["Marillee"], "scores": [[0.005184815730899572, 0.00020864035468548536, 0.002197049092501402]], "normalized_score": [0.0013345162151381373], "tokens": [["\u0120Mar", "il", "lee"]], "entropy": [[6.681052207946777, 0.7558697462081909, 1.0229218006134033]], "avg_entropy": [2.819947918256124], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hal Derwin", "score_of_biggest_cluster": 0.0013345162151381373, "score_of_first_answer": 0.0013345162151381373, "max_score_of_first_answer": 0.005184815730899572, "min_score_of_first_answer": 0.00020864035468548536, "avg_score": 0.0013345162151381373, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0020451253742873893, "std_of_first_answer": 0.0020451253742873893, "avg_entropy_of_first_answer": 2.819947918256124, "lex_sim": 1.0}
{"question": "how many casinos are in atlantic city new jersey", "answer": ["4"], "scores": [[0.05092129111289978]], "normalized_score": [0.05092128738760948], "tokens": [["\u01204"]], "entropy": [[4.798968315124512]], "avg_entropy": [4.798968315124512], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "eleven", "score_of_biggest_cluster": 0.05092128738760948, "score_of_first_answer": 0.05092128738760948, "max_score_of_first_answer": 0.05092129111289978, "min_score_of_first_answer": 0.05092129111289978, "avg_score": 0.05092128738760948, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.798968315124512, "lex_sim": 1.0}
{"question": "who produced a change is gonna come", "answer": ["Chris Martin and David Moyes"], "scores": [[0.0051589831709861755, 0.08438099920749664, 0.01554971281439066, 0.04187242314219475, 0.042186446487903595, 0.06672104448080063]], "normalized_score": [0.03045433759689331], "tokens": [["\u0120Chris", "\u0120Martin", "\u0120and", "\u0120David", "\u0120Moy", "es"]], "entropy": [[6.86684513092041, 5.810321807861328, 0.5345720052719116, 5.174461364746094, 5.185269832611084, 3.3899145126342773]], "avg_entropy": [4.493564109007518], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hugo & Luigi", "score_of_biggest_cluster": 0.03045433759689331, "score_of_first_answer": 0.03045433759689331, "max_score_of_first_answer": 0.08438099920749664, "min_score_of_first_answer": 0.0051589831709861755, "avg_score": 0.03045433759689331, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02726937457834532, "std_of_first_answer": 0.02726937457834532, "avg_entropy_of_first_answer": 4.493564109007518, "lex_sim": 1.0}
{"question": "who played mr thatcher in when calls the heart", "answer": ["Alistair McGowan"], "scores": [[0.008628644049167633, 0.21190056204795837, 0.9981697797775269, 0.18168212473392487, 0.9152734279632568]], "normalized_score": [0.19789206981658936], "tokens": [["\u0120Al", "ist", "air", "\u0120McG", "owan"]], "entropy": [[5.5501861572265625, 3.466672897338867, 0.021476928144693375, 4.529203414916992, 0.5350756645202637]], "avg_entropy": [2.820523012429476], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Garwin Sanford", "score_of_biggest_cluster": 0.19789206981658936, "score_of_first_answer": 0.19789206981658936, "max_score_of_first_answer": 0.9981697797775269, "min_score_of_first_answer": 0.008628644049167633, "avg_score": 0.19789206981658936, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4097808841166854, "std_of_first_answer": 0.4097808841166854, "avg_entropy_of_first_answer": 2.820523012429476, "lex_sim": 1.0}
{"question": "who sang it going to take a miracle", "answer": ["Elton John"], "scores": [[0.012131400406360626, 0.916724443435669, 0.9153229594230652]], "normalized_score": [0.21672452986240387], "tokens": [["\u0120El", "ton", "\u0120John"]], "entropy": [[6.323354721069336, 0.6483086347579956, 0.45883017778396606]], "avg_entropy": [2.4768311778704324], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Royalettes", "score_of_biggest_cluster": 0.21672452986240387, "score_of_first_answer": 0.21672452986240387, "max_score_of_first_answer": 0.916724443435669, "min_score_of_first_answer": 0.012131400406360626, "avg_score": 0.21672452986240387, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42609930114593486, "std_of_first_answer": 0.42609930114593486, "avg_entropy_of_first_answer": 2.4768311778704324, "lex_sim": 1.0}
{"question": "when does the day of the dead end", "answer": ["Halloween 2012"], "scores": [[0.011690154671669006, 0.0001432975404895842]], "normalized_score": [0.0012942829634994268], "tokens": [["\u0120Halloween", "\u01202012"]], "entropy": [[3.5061492919921875, 0.5428796410560608]], "avg_entropy": [2.024514466524124], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "November 2", "score_of_biggest_cluster": 0.0012942829634994268, "score_of_first_answer": 0.0012942829634994268, "max_score_of_first_answer": 0.011690154671669006, "min_score_of_first_answer": 0.0001432975404895842, "avg_score": 0.0012942829634994268, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.005773428565589711, "std_of_first_answer": 0.005773428565589711, "avg_entropy_of_first_answer": 2.024514466524124, "lex_sim": 1.0}
{"question": "what is the scientific name for the calf muscle", "answer": ["M. Longissimus dorsi"], "scores": [[0.03984155133366585, 0.7536785006523132, 0.030566049739718437, 0.6623806953430176, 0.9928674697875977, 0.5534765720367432, 0.9966444969177246]], "normalized_score": [0.3185664713382721], "tokens": [["\u0120M", ".", "\u0120Long", "iss", "imus", "\u0120d", "orsi"]], "entropy": [[3.844243049621582, 1.2013251781463623, 4.70683479309082, 1.7329641580581665, 0.06862971186637878, 1.5590652227401733, 0.032577600330114365]], "avg_entropy": [1.877948530550514], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "gastrocnemius", "score_of_biggest_cluster": 0.3185664713382721, "score_of_first_answer": 0.3185664713382721, "max_score_of_first_answer": 0.9966444969177246, "min_score_of_first_answer": 0.030566049739718437, "avg_score": 0.3185664713382721, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.373252501706514, "std_of_first_answer": 0.373252501706514, "avg_entropy_of_first_answer": 1.877948530550514, "lex_sim": 1.0}
{"question": "the chinese dragons are protectors of how many seas diggy", "answer": ["9"], "scores": [[0.03230547159910202]], "normalized_score": [0.03230547159910202], "tokens": [["\u01209"]], "entropy": [[4.626457691192627]], "avg_entropy": [4.626457691192627], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Four Seas", "score_of_biggest_cluster": 0.03230547159910202, "score_of_first_answer": 0.03230547159910202, "max_score_of_first_answer": 0.03230547159910202, "min_score_of_first_answer": 0.03230547159910202, "avg_score": 0.03230547159910202, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.626457691192627, "lex_sim": 1.0}
{"question": "what is the word for clarified butter in the balkans and middle east", "answer": ["garipan"], "scores": [[0.006462620105594397, 0.013010811060667038, 0.11289825290441513]], "normalized_score": [0.021173855289816856], "tokens": [["\u0120gar", "ip", "an"]], "entropy": [[4.862024784088135, 3.549588680267334, 3.124601364135742]], "avg_entropy": [3.845404942830404], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "smen", "score_of_biggest_cluster": 0.021173855289816856, "score_of_first_answer": 0.021173855289816856, "max_score_of_first_answer": 0.11289825290441513, "min_score_of_first_answer": 0.006462620105594397, "avg_score": 0.021173855289816856, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.048704236379917214, "std_of_first_answer": 0.048704236379917214, "avg_entropy_of_first_answer": 3.845404942830404, "lex_sim": 1.0}
{"question": "what is an example of a tricyclic antidepressant", "answer": ["nortriptyline"], "scores": [[0.10670335590839386, 0.9898063540458679, 0.9590398073196411, 0.9979571104049683]], "normalized_score": [0.5638573169708252], "tokens": [["\u0120nort", "ript", "y", "line"]], "entropy": [[4.428877830505371, 0.09119656682014465, 0.27787718176841736, 0.02175287716090679]], "avg_entropy": [1.20492611406371], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Amineptine", "score_of_biggest_cluster": 0.5638573169708252, "score_of_first_answer": 0.5638573169708252, "max_score_of_first_answer": 0.9979571104049683, "min_score_of_first_answer": 0.10670335590839386, "avg_score": 0.5638573169708252, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.37940818704382145, "std_of_first_answer": 0.37940818704382145, "avg_entropy_of_first_answer": 1.20492611406371, "lex_sim": 1.0}
{"question": "the creation of human beings in the kumulipo happens during which w\u0101 or period of creation", "answer": ["Big Island Period"], "scores": [[0.006562550086528063, 0.0009042047313414514, 0.03377944603562355]], "normalized_score": [0.005852355156093836], "tokens": [["\u0120Big", "\u0120Island", "\u0120Period"]], "entropy": [[6.279396057128906, 0.5113419890403748, 3.3367388248443604]], "avg_entropy": [3.3758256236712136], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "In the ninth w\u0101", "score_of_biggest_cluster": 0.005852355156093836, "score_of_first_answer": 0.005852355156093836, "max_score_of_first_answer": 0.03377944603562355, "min_score_of_first_answer": 0.0009042047313414514, "avg_score": 0.005852355156093836, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.014350988305406502, "std_of_first_answer": 0.014350988305406502, "avg_entropy_of_first_answer": 3.3758256236712136, "lex_sim": 1.0}
{"question": "where was the first high level language operating system created", "answer": ["FORTH"], "scores": [[0.006032509729266167, 0.07242454588413239]], "normalized_score": [0.02090219035744667], "tokens": [["\u0120FOR", "TH"]], "entropy": [[5.600205421447754, 0.434257447719574]], "avg_entropy": [3.017231434583664], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "at IBM", "score_of_biggest_cluster": 0.02090219035744667, "score_of_first_answer": 0.02090219035744667, "max_score_of_first_answer": 0.07242454588413239, "min_score_of_first_answer": 0.006032509729266167, "avg_score": 0.02090219035744667, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.03319601807743311, "std_of_first_answer": 0.03319601807743311, "avg_entropy_of_first_answer": 3.017231434583664, "lex_sim": 1.0}
{"question": "who played john clark sr on nypd blue", "answer": ["Steve Carell"], "scores": [[0.006688547320663929, 0.25128665566444397, 0.9958174824714661]], "normalized_score": [0.11872997134923935], "tokens": [["\u0120Steve", "\u0120Care", "ll"]], "entropy": [[5.418539047241211, 4.026835918426514, 0.039313819259405136]], "avg_entropy": [3.1615629283090434], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joe Spano", "score_of_biggest_cluster": 0.11872997134923935, "score_of_first_answer": 0.11872997134923935, "max_score_of_first_answer": 0.9958174824714661, "min_score_of_first_answer": 0.006688547320663929, "avg_score": 0.11872997134923935, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42065166864756975, "std_of_first_answer": 0.42065166864756975, "avg_entropy_of_first_answer": 3.1615629283090434, "lex_sim": 1.0}
{"question": "where does the electron transport chain pumps protons", "answer": ["in the cytostome"], "scores": [[0.025106701999902725, 0.808151364326477, 0.0016796335112303495, 0.007765877526253462, 0.7402117848396301]], "normalized_score": [0.045541636645793915], "tokens": [["\u0120in", "\u0120the", "\u0120cyt", "ost", "ome"]], "entropy": [[4.372835636138916, 1.2714872360229492, 2.1983277797698975, 0.4397570490837097, 1.6603044271469116]], "avg_entropy": [1.9885424256324769], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "mitochondrial membrane", "score_of_biggest_cluster": 0.045541636645793915, "score_of_first_answer": 0.045541636645793915, "max_score_of_first_answer": 0.808151364326477, "min_score_of_first_answer": 0.0016796335112303495, "avg_score": 0.045541636645793915, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37432375874530066, "std_of_first_answer": 0.37432375874530066, "avg_entropy_of_first_answer": 1.9885424256324769, "lex_sim": 1.0}
{"question": "where was the summer olympics held in 2012", "answer": ["Summer Games"], "scores": [[0.003674651961773634, 0.10229600965976715]], "normalized_score": [0.019388195127248764], "tokens": [["\u0120Summer", "\u0120Games"]], "entropy": [[1.165477991104126, 2.8382039070129395]], "avg_entropy": [2.0018409490585327], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "London", "score_of_biggest_cluster": 0.019388195127248764, "score_of_first_answer": 0.019388195127248764, "max_score_of_first_answer": 0.10229600965976715, "min_score_of_first_answer": 0.003674651961773634, "avg_score": 0.019388195127248764, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04931067884899676, "std_of_first_answer": 0.04931067884899676, "avg_entropy_of_first_answer": 2.0018409490585327, "lex_sim": 1.0}
{"question": "who is the king and queen of the netherlands", "answer": ["King Willem Alexander and Queen Maxima"], "scores": [[0.05688783898949623, 0.8361586928367615, 0.9921302199363708, 0.25559598207473755, 0.634405791759491, 0.9594288468360901, 0.7231993079185486, 0.9965594410896301]], "normalized_score": [0.5193336606025696], "tokens": [["\u0120King", "\u0120Wil", "lem", "\u0120Alexander", "\u0120and", "\u0120Queen", "\u0120Max", "ima"]], "entropy": [[4.0373663902282715, 1.1106981039047241, 0.07547502964735031, 1.528892159461975, 1.4933569431304932, 0.3330231308937073, 0.7216367721557617, 0.03469761461019516]], "avg_entropy": [1.1668932680040598], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Queen M\u00e1xima of the Netherlands", "score_of_biggest_cluster": 0.5193336606025696, "score_of_first_answer": 0.5193336606025696, "max_score_of_first_answer": 0.9965594410896301, "min_score_of_first_answer": 0.05688783898949623, "avg_score": 0.5193336606025696, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3302286937132524, "std_of_first_answer": 0.3302286937132524, "avg_entropy_of_first_answer": 1.1668932680040598, "lex_sim": 1.0}
{"question": "who plays elsa's aunt in once upon a time", "answer": ["Marion Bailey"], "scores": [[0.0026163652073591948, 0.036660946905612946]], "normalized_score": [0.009793796576559544], "tokens": [["\u0120Marion", "\u0120Bailey"]], "entropy": [[6.210821628570557, 1.7422683238983154]], "avg_entropy": [3.976544976234436], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Elizabeth Mitchell", "score_of_biggest_cluster": 0.009793796576559544, "score_of_first_answer": 0.009793796576559544, "max_score_of_first_answer": 0.036660946905612946, "min_score_of_first_answer": 0.0026163652073591948, "avg_score": 0.009793796576559544, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.017022290849126875, "std_of_first_answer": 0.017022290849126875, "avg_entropy_of_first_answer": 3.976544976234436, "lex_sim": 1.0}
{"question": "who wrote the song stop the world and let me off", "answer": ["U2"], "scores": [[0.0015479830326512456, 0.9635494947433472]], "normalized_score": [0.03862070292234421], "tokens": [["\u0120U", "2"]], "entropy": [[6.395284652709961, 0.31218841671943665]], "avg_entropy": [3.353736534714699], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "W. S. Stevenson", "score_of_biggest_cluster": 0.03862070292234421, "score_of_first_answer": 0.03862070292234421, "max_score_of_first_answer": 0.9635494947433472, "min_score_of_first_answer": 0.0015479830326512456, "avg_score": 0.03862070292234421, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.48100075585534796, "std_of_first_answer": 0.48100075585534796, "avg_entropy_of_first_answer": 3.353736534714699, "lex_sim": 1.0}
{"question": "when did athens emerges as wealthiest greek city state", "answer": ["545-478 BC"], "scores": [[0.044998686760663986, 0.004658527206629515, 0.12794993817806244, 0.04134047031402588, 0.47029227018356323]], "normalized_score": [0.05539192259311676], "tokens": [["\u01205", "45", "-", "478", "\u0120BC"]], "entropy": [[5.696930408477783, 3.1451456546783447, 2.296128273010254, 4.328560829162598, 1.7522605657577515]], "avg_entropy": [3.443805146217346], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the late 6th century BCE", "score_of_biggest_cluster": 0.05539192259311676, "score_of_first_answer": 0.05539192259311676, "max_score_of_first_answer": 0.47029227018356323, "min_score_of_first_answer": 0.004658527206629515, "avg_score": 0.05539192259311676, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.17105045941112937, "std_of_first_answer": 0.17105045941112937, "avg_entropy_of_first_answer": 3.443805146217346, "lex_sim": 1.0}
{"question": "where was thomas and the magic railroad filmed", "answer": ["Canada"], "scores": [[0.005034139845520258]], "normalized_score": [0.005034140311181545], "tokens": [["\u0120Canada"]], "entropy": [[6.3017096519470215]], "avg_entropy": [6.3017096519470215], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Isle of Man", "score_of_biggest_cluster": 0.005034140311181545, "score_of_first_answer": 0.005034140311181545, "max_score_of_first_answer": 0.005034139845520258, "min_score_of_first_answer": 0.005034139845520258, "avg_score": 0.005034140311181545, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.3017096519470215, "lex_sim": 1.0}
{"question": "who owned the colts when they left baltimore", "answer": ["John Mecom Sr."], "scores": [[0.030240438878536224, 0.010707472451031208, 0.08274690806865692, 0.5731049180030823, 0.018165355548262596, 0.6489518880844116]], "normalized_score": [0.07521185278892517], "tokens": [["\u0120John", "\u0120M", "ec", "om", "\u0120Sr", "."]], "entropy": [[4.762430191040039, 4.5903472900390625, 3.699361801147461, 2.6678004264831543, 0.9623168706893921, 0.8424580097198486]], "avg_entropy": [2.9207857648531594], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Robert Irsay", "score_of_biggest_cluster": 0.07521185278892517, "score_of_first_answer": 0.07521185278892517, "max_score_of_first_answer": 0.6489518880844116, "min_score_of_first_answer": 0.010707472451031208, "avg_score": 0.07521185278892517, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2731754757403526, "std_of_first_answer": 0.2731754757403526, "avg_entropy_of_first_answer": 2.9207857648531594, "lex_sim": 1.0}
{"question": "where did the potter's wheel first develop", "answer": ["Italy"], "scores": [[0.023712944239377975]], "normalized_score": [0.023712942376732826], "tokens": [["\u0120Italy"]], "entropy": [[4.951753616333008]], "avg_entropy": [4.951753616333008], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in the Near East", "score_of_biggest_cluster": 0.023712942376732826, "score_of_first_answer": 0.023712942376732826, "max_score_of_first_answer": 0.023712944239377975, "min_score_of_first_answer": 0.023712944239377975, "avg_score": 0.023712942376732826, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.951753616333008, "lex_sim": 1.0}
{"question": "when was the immigration act passed in canada", "answer": ["January 24, 2002"], "scores": [[0.03823189064860344, 0.00301471259444952, 0.3202720284461975, 0.0034312002826482058]], "normalized_score": [0.018865104764699936], "tokens": [["\u0120January", "\u012024", ",", "\u01202002"]], "entropy": [[4.285946846008301, 0.8839216232299805, 1.1352219581604004, 3.3954973220825195]], "avg_entropy": [2.4251469373703003], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1923", "score_of_biggest_cluster": 0.018865104764699936, "score_of_first_answer": 0.018865104764699936, "max_score_of_first_answer": 0.3202720284461975, "min_score_of_first_answer": 0.00301471259444952, "avg_score": 0.018865104764699936, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13300339876384384, "std_of_first_answer": 0.13300339876384384, "avg_entropy_of_first_answer": 2.4251469373703003, "lex_sim": 1.0}
{"question": "what proposition made the insurance commissioner an elected position", "answer": ["1873"], "scores": [[0.005661355797201395, 0.015938544645905495]], "normalized_score": [0.009499146603047848], "tokens": [["\u012018", "73"]], "entropy": [[6.038341522216797, 4.5548553466796875]], "avg_entropy": [5.296598434448242], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Proposition 103", "score_of_biggest_cluster": 0.009499146603047848, "score_of_first_answer": 0.009499146603047848, "max_score_of_first_answer": 0.015938544645905495, "min_score_of_first_answer": 0.005661355797201395, "avg_score": 0.009499146603047848, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.00513859442435205, "std_of_first_answer": 0.00513859442435205, "avg_entropy_of_first_answer": 5.296598434448242, "lex_sim": 1.0}
{"question": "the winner of the best actor award in the 64th national film festival 2017", "answer": ["Adil Hussain"], "scores": [[0.011744054965674877, 0.5966470241546631, 0.5321311354637146]], "normalized_score": [0.1550665646791458], "tokens": [["\u0120Ad", "il", "\u0120Hussain"]], "entropy": [[6.589120864868164, 2.3092472553253174, 3.040998697280884]], "avg_entropy": [3.9797889391581216], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Akshay Kumar", "score_of_biggest_cluster": 0.1550665646791458, "score_of_first_answer": 0.1550665646791458, "max_score_of_first_answer": 0.5966470241546631, "min_score_of_first_answer": 0.011744054965674877, "avg_score": 0.1550665646791458, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26184738914297867, "std_of_first_answer": 0.26184738914297867, "avg_entropy_of_first_answer": 3.9797889391581216, "lex_sim": 1.0}
{"question": "how many ceos of fortune 500 companies are female", "answer": ["1 in 4"], "scores": [[0.040070030838251114, 0.08389351516962051, 0.09611202776432037]], "normalized_score": [0.06861860305070877], "tokens": [["\u01201", "\u0120in", "\u01204"]], "entropy": [[5.600720405578613, 1.7710442543029785, 3.4162302017211914]], "avg_entropy": [3.5959982872009277], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "32", "score_of_biggest_cluster": 0.06861860305070877, "score_of_first_answer": 0.06861860305070877, "max_score_of_first_answer": 0.09611202776432037, "min_score_of_first_answer": 0.040070030838251114, "avg_score": 0.06861860305070877, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.024061253427289313, "std_of_first_answer": 0.024061253427289313, "avg_entropy_of_first_answer": 3.5959982872009277, "lex_sim": 1.0}
{"question": "who played big enos in smokey and the bandit", "answer": ["Enos Barnes"], "scores": [[0.09251673519611359, 0.9336794018745422, 0.00653857970610261]], "normalized_score": [0.08266095817089081], "tokens": [["\u0120En", "os", "\u0120Barnes"]], "entropy": [[5.6237616539001465, 0.3469216525554657, 6.064432621002197]], "avg_entropy": [4.011705309152603], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pat McCormick", "score_of_biggest_cluster": 0.08266095817089081, "score_of_first_answer": 0.08266095817089081, "max_score_of_first_answer": 0.9336794018745422, "min_score_of_first_answer": 0.00653857970610261, "avg_score": 0.08266095817089081, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41826851810850996, "std_of_first_answer": 0.41826851810850996, "avg_entropy_of_first_answer": 4.011705309152603, "lex_sim": 1.0}
{"question": "when did amnesia the dark descent come out", "answer": ["August 23, 2009"], "scores": [[0.05356185510754585, 0.02098716050386429, 0.19386020302772522, 0.05022907629609108]], "normalized_score": [0.05751926079392433], "tokens": [["\u0120August", "\u012023", ",", "\u01202009"]], "entropy": [[4.277681350708008, 3.8719887733459473, 0.9650380611419678, 2.863760471343994]], "avg_entropy": [2.9946171641349792], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "8 September 2010", "score_of_biggest_cluster": 0.05751926079392433, "score_of_first_answer": 0.05751926079392433, "max_score_of_first_answer": 0.19386020302772522, "min_score_of_first_answer": 0.02098716050386429, "avg_score": 0.05751926079392433, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06714067489659117, "std_of_first_answer": 0.06714067489659117, "avg_entropy_of_first_answer": 2.9946171641349792, "lex_sim": 1.0}
{"question": "who made delhi as capital for the first time", "answer": ["Prithviraj Chauhan"], "scores": [[0.003941187169402838, 0.4355379343032837, 0.8638265132904053, 0.9916236996650696, 0.9172497987747192, 0.9735758304595947]], "normalized_score": [0.33091309666633606], "tokens": [["\u0120Pr", "ith", "vir", "aj", "\u0120Chau", "han"]], "entropy": [[5.526113510131836, 1.9588141441345215, 0.6259074807167053, 0.05970961973071098, 0.5576773881912231, 0.21547943353652954]], "avg_entropy": [1.4906169294069211], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Pandavas", "score_of_biggest_cluster": 0.33091309666633606, "score_of_first_answer": 0.33091309666633606, "max_score_of_first_answer": 0.9916236996650696, "min_score_of_first_answer": 0.003941187169402838, "avg_score": 0.33091309666633606, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36247504596352376, "std_of_first_answer": 0.36247504596352376, "avg_entropy_of_first_answer": 1.4906169294069211, "lex_sim": 1.0}
{"question": "what was the biggest tank in world war 2", "answer": ["HMS Dreadnought"], "scores": [[0.0017180925933644176, 0.058054521679878235, 0.9915492534637451, 0.8433126211166382]], "normalized_score": [0.09556443989276886], "tokens": [["\u0120HMS", "\u0120Dread", "n", "ought"]], "entropy": [[4.192023277282715, 5.81831169128418, 0.0894748717546463, 0.47973519563674927]], "avg_entropy": [2.6448862589895725], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Panzerkampfwagen VIII Maus", "score_of_biggest_cluster": 0.09556443989276886, "score_of_first_answer": 0.09556443989276886, "max_score_of_first_answer": 0.9915492534637451, "min_score_of_first_answer": 0.0017180925933644176, "avg_score": 0.09556443989276886, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4473000717416535, "std_of_first_answer": 0.4473000717416535, "avg_entropy_of_first_answer": 2.6448862589895725, "lex_sim": 1.0}
{"question": "which state was returned to spain after the revolutionary war", "answer": ["Puerto Rico"], "scores": [[0.15585128962993622, 0.9865346550941467]], "normalized_score": [0.3921131193637848], "tokens": [["\u0120Puerto", "\u0120Rico"]], "entropy": [[4.210464954376221, 0.12163036316633224]], "avg_entropy": [2.1660476587712765], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Florida", "score_of_biggest_cluster": 0.3921131193637848, "score_of_first_answer": 0.3921131193637848, "max_score_of_first_answer": 0.9865346550941467, "min_score_of_first_answer": 0.15585128962993622, "avg_score": 0.3921131193637848, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41534168273210526, "std_of_first_answer": 0.41534168273210526, "avg_entropy_of_first_answer": 2.1660476587712765, "lex_sim": 1.0}
{"question": "who has won the most masters green jackets", "answer": ["Roger Federer"], "scores": [[0.023335207253694534, 0.7815845608711243, 0.9978955984115601]], "normalized_score": [0.26304149627685547], "tokens": [["\u0120Roger", "\u0120Fed", "erer"]], "entropy": [[6.46583366394043, 1.8182673454284668, 0.021627837792038918]], "avg_entropy": [2.7685762823869786], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jack Nicklaus", "score_of_biggest_cluster": 0.26304149627685547, "score_of_first_answer": 0.26304149627685547, "max_score_of_first_answer": 0.9978955984115601, "min_score_of_first_answer": 0.023335207253694534, "avg_score": 0.26304149627685547, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41786501053129077, "std_of_first_answer": 0.41786501053129077, "avg_entropy_of_first_answer": 2.7685762823869786, "lex_sim": 1.0}
{"question": "who was in charge of the revolutionary war", "answer": ["George Washington"], "scores": [[0.7700687050819397, 0.9843391180038452]], "normalized_score": [0.8706369996070862], "tokens": [["\u0120George", "\u0120Washington"]], "entropy": [[1.5820547342300415, 0.15382680296897888]], "avg_entropy": [0.8679407685995102], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "General George Washington", "score_of_biggest_cluster": 0.8706369996070862, "score_of_first_answer": 0.8706369996070862, "max_score_of_first_answer": 0.9843391180038452, "min_score_of_first_answer": 0.7700687050819397, "avg_score": 0.8706369996070862, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.10713520646095276, "std_of_first_answer": 0.10713520646095276, "avg_entropy_of_first_answer": 0.8679407685995102, "lex_sim": 1.0}
{"question": "where do the elves go on the boat in lord of the rings", "answer": ["elves keep to the forest."], "scores": [[0.006238373927772045, 0.002295910380780697, 0.05498122796416283, 0.22221307456493378, 0.009077503345906734, 0.022643567994236946, 0.002620767569169402, 0.004463258199393749, 0.013326384127140045, 0.008521897718310356, 0.5232107639312744, 0.8061497211456299, 0.028239503502845764, 0.0728258267045021, 0.5394774675369263, 0.08113595098257065, 0.45106241106987, 0.41695547103881836]], "normalized_score": [0.04372422397136688], "tokens": [["\u0120elves", "\u0120keep", "\u0120to", "\u0120the", "\u0120forest", ".", "\u0120If", "\u0120attacked", "\u0120or", "\u0120frightened", ",", "\u0120they", "\u0120can", "\u0120hide", "\u0120in", "\u0120a", "\u0120tree", "."]], "entropy": [[5.775581359863281, 3.9908993244171143, 4.908044815063477, 1.5270088911056519, 4.840306282043457, 1.7131601572036743, 3.252842426300049, 2.6150529384613037, 2.0789475440979004, 5.193804740905762, 1.423687219619751, 1.3818628787994385, 4.227327823638916, 4.497081756591797, 2.3466460704803467, 3.0114660263061523, 3.1832058429718018, 2.273954153060913]], "avg_entropy": [3.2356044583850436], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Valinor", "score_of_biggest_cluster": 0.04372422397136688, "score_of_first_answer": 0.04372422397136688, "max_score_of_first_answer": 0.8061497211456299, "min_score_of_first_answer": 0.002295910380780697, "avg_score": 0.04372422397136688, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2433082156034855, "std_of_first_answer": 0.2433082156034855, "avg_entropy_of_first_answer": 3.2356044583850436, "lex_sim": 1.0}
{"question": "what color pages in the emergency response guidebook (erg) list compounds by name", "answer": ["Purple"], "scores": [[0.019950343295931816]], "normalized_score": [0.019950345158576965], "tokens": [["\u0120Purple"]], "entropy": [[4.649487018585205]], "avg_entropy": [4.649487018585205], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "blue", "score_of_biggest_cluster": 0.019950345158576965, "score_of_first_answer": 0.019950345158576965, "max_score_of_first_answer": 0.019950343295931816, "min_score_of_first_answer": 0.019950343295931816, "avg_score": 0.019950345158576965, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.649487018585205, "lex_sim": 1.0}
{"question": "when did frank sinatra first sing new york new york", "answer": ["1939"], "scores": [[0.06276393681764603]], "normalized_score": [0.06276393681764603], "tokens": [["\u01201939"]], "entropy": [[5.199783802032471]], "avg_entropy": [5.199783802032471], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1980", "score_of_biggest_cluster": 0.06276393681764603, "score_of_first_answer": 0.06276393681764603, "max_score_of_first_answer": 0.06276393681764603, "min_score_of_first_answer": 0.06276393681764603, "avg_score": 0.06276393681764603, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.199783802032471, "lex_sim": 1.0}
{"question": "who won the battle of stirling bridge 1297", "answer": ["earl sir john fowler"], "scores": [[0.00010074391320813447, 0.9219171404838562, 0.003207322210073471, 0.07832768559455872, 0.022365152835845947, 0.032188817858695984]], "normalized_score": [0.01600322686135769], "tokens": [["\u0120ear", "l", "\u0120sir", "\u0120john", "\u0120f", "owler"]], "entropy": [[3.400604248046875, 0.4673827588558197, 1.8955391645431519, 5.431960105895996, 4.37333345413208, 3.8531670570373535]], "avg_entropy": [3.2369977980852127], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Scottish victory", "score_of_biggest_cluster": 0.01600322686135769, "score_of_first_answer": 0.01600322686135769, "max_score_of_first_answer": 0.9219171404838562, "min_score_of_first_answer": 0.00010074391320813447, "avg_score": 0.01600322686135769, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3344185754955366, "std_of_first_answer": 0.3344185754955366, "avg_entropy_of_first_answer": 3.2369977980852127, "lex_sim": 1.0}
{"question": "when did the last state became a state", "answer": ["Alaska achieved statehood on January 3, 1959"], "scores": [[0.015957556664943695, 0.006428965367376804, 0.8785850405693054, 0.9958204030990601, 0.3916248083114624, 0.6646919250488281, 0.9611282348632812, 0.4871548116207123, 0.9590741395950317]], "normalized_score": [0.2797451317310333], "tokens": [["\u0120Alaska", "\u0120achieved", "\u0120state", "hood", "\u0120on", "\u0120January", "\u01203", ",", "\u01201959"]], "entropy": [[5.740943908691406, 1.762198805809021, 0.7802271842956543, 0.033231817185878754, 1.4102007150650024, 1.3020453453063965, 0.2791394293308258, 0.9819328784942627, 0.2599439024925232]], "avg_entropy": [1.3944293318523302], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hawaii", "score_of_biggest_cluster": 0.2797451317310333, "score_of_first_answer": 0.2797451317310333, "max_score_of_first_answer": 0.9958204030990601, "min_score_of_first_answer": 0.006428965367376804, "avg_score": 0.2797451317310333, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37226469769902837, "std_of_first_answer": 0.37226469769902837, "avg_entropy_of_first_answer": 1.3944293318523302, "lex_sim": 1.0}
{"question": "who has won more national championships michigan or ohio state", "answer": ["michigan"], "scores": [[0.09733504801988602, 0.9940322041511536, 0.9915328621864319]], "normalized_score": [0.45778223872184753], "tokens": [["\u0120m", "ich", "igan"]], "entropy": [[2.622645139694214, 0.06428524851799011, 0.07459139078855515]], "avg_entropy": [0.9205072596669197], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Michigan", "score_of_biggest_cluster": 0.45778223872184753, "score_of_first_answer": 0.45778223872184753, "max_score_of_first_answer": 0.9940322041511536, "min_score_of_first_answer": 0.09733504801988602, "avg_score": 0.45778223872184753, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4221192258361227, "std_of_first_answer": 0.4221192258361227, "avg_entropy_of_first_answer": 0.9205072596669197, "lex_sim": 1.0}
{"question": "when did the vikings first sail and reach britain", "answer": ["987 A.D."], "scores": [[0.034304309636354446, 0.04872913286089897, 0.06930150836706161, 0.990725576877594, 0.9741937518119812, 0.7530200481414795]], "normalized_score": [0.20935380458831787], "tokens": [["\u01209", "87", "\u0120A", ".", "D", "."]], "entropy": [[3.6920979022979736, 3.201686143875122, 1.882490634918213, 0.09582731127738953, 0.16562357544898987, 0.7130783796310425]], "avg_entropy": [1.625133991241455], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "6 January 793", "score_of_biggest_cluster": 0.20935380458831787, "score_of_first_answer": 0.20935380458831787, "max_score_of_first_answer": 0.990725576877594, "min_score_of_first_answer": 0.034304309636354446, "avg_score": 0.20935380458831787, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43453130332035317, "std_of_first_answer": 0.43453130332035317, "avg_entropy_of_first_answer": 1.625133991241455, "lex_sim": 1.0}
{"question": "where does the red river start and end in louisiana", "answer": ["the red river is an arm of the mississippi"], "scores": [[0.10441523045301437, 0.10242940485477448, 0.9173397421836853, 0.1728394478559494, 0.0347861684858799, 0.06771578639745712, 0.9805702567100525, 0.9269592761993408, 0.3427034020423889, 0.9871930480003357, 0.9731903672218323]], "normalized_score": [0.28946253657341003], "tokens": [["\u0120the", "\u0120red", "\u0120river", "\u0120is", "\u0120an", "\u0120arm", "\u0120of", "\u0120the", "\u0120miss", "iss", "ippi"]], "entropy": [[4.90822172164917, 3.868640899658203, 0.727590799331665, 3.203217029571533, 2.7164742946624756, 4.067690849304199, 0.18069583177566528, 0.5962215065956116, 3.169135808944702, 0.09763748943805695, 0.1466258466243744]], "avg_entropy": [2.1529229161414234], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Texas Panhandle", "score_of_biggest_cluster": 0.28946253657341003, "score_of_first_answer": 0.28946253657341003, "max_score_of_first_answer": 0.9871930480003357, "min_score_of_first_answer": 0.0347861684858799, "avg_score": 0.28946253657341003, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41529806226938193, "std_of_first_answer": 0.41529806226938193, "avg_entropy_of_first_answer": 2.1529229161414234, "lex_sim": 1.0}
{"question": "when was the first star wars film released", "answer": ["Star Wars Episode IV: A New Hope (1977)"], "scores": [[0.09114730358123779, 0.9578601717948914, 0.44886767864227295, 0.8276870846748352, 0.828057050704956, 0.9725305438041687, 0.9895042181015015, 0.9959384202957153, 0.10662597417831421, 0.8587342500686646, 0.9866893887519836]], "normalized_score": [0.5762290358543396], "tokens": [["\u0120Star", "\u0120Wars", "\u0120Episode", "\u0120IV", ":", "\u0120A", "\u0120New", "\u0120Hope", "\u0120(", "1977", ")"]], "entropy": [[2.9645771980285645, 0.25626903772354126, 2.1018900871276855, 0.8360021114349365, 0.8769814968109131, 0.18180888891220093, 0.07310683280229568, 0.04201968386769295, 0.9896489977836609, 1.0837514400482178, 0.11543801426887512]], "avg_entropy": [0.865590344437144], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1977", "score_of_biggest_cluster": 0.5762290358543396, "score_of_first_answer": 0.5762290358543396, "max_score_of_first_answer": 0.9959384202957153, "min_score_of_first_answer": 0.09114730358123779, "avg_score": 0.5762290358543396, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.33385997911886633, "std_of_first_answer": 0.33385997911886633, "avg_entropy_of_first_answer": 0.865590344437144, "lex_sim": 1.0}
{"question": "who has the best batting average in test cricket", "answer": ["Adam Gilchrist"], "scores": [[0.011312580667436123, 0.8769305348396301, 0.9861395359039307]], "normalized_score": [0.21387255191802979], "tokens": [["\u0120Adam", "\u0120Gil", "christ"]], "entropy": [[4.014090538024902, 0.9319934844970703, 0.13995541632175446]], "avg_entropy": [1.6953464796145756], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Australia's Sir Donald Bradman", "score_of_biggest_cluster": 0.21387255191802979, "score_of_first_answer": 0.21387255191802979, "max_score_of_first_answer": 0.9861395359039307, "min_score_of_first_answer": 0.011312580667436123, "avg_score": 0.21387255191802979, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43608213308381505, "std_of_first_answer": 0.43608213308381505, "avg_entropy_of_first_answer": 1.6953464796145756, "lex_sim": 1.0}
{"question": "who invented the transtheoretical model of behavior change", "answer": ["Edward K. Brecher"], "scores": [[0.0189865343272686, 0.00365346553735435, 0.6075828075408936, 0.002963888691738248, 0.678970992565155]], "normalized_score": [0.038520678877830505], "tokens": [["\u0120Edward", "\u0120K", ".", "\u0120Bre", "cher"]], "entropy": [[4.87337589263916, 2.0973644256591797, 2.853292465209961, 5.527632713317871, 1.8555150032043457]], "avg_entropy": [3.4414361000061033], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "colleagues", "score_of_biggest_cluster": 0.038520678877830505, "score_of_first_answer": 0.038520678877830505, "max_score_of_first_answer": 0.678970992565155, "min_score_of_first_answer": 0.002963888691738248, "avg_score": 0.038520678877830505, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3118299336860979, "std_of_first_answer": 0.3118299336860979, "avg_entropy_of_first_answer": 3.4414361000061033, "lex_sim": 1.0}
{"question": "who had the best strike rate among batsmen who scored over 200 runs in the world cup", "answer": ["ken batty"], "scores": [[0.0003047912905458361, 0.10257751494646072, 0.03265251964330673, 0.08276975899934769]], "normalized_score": [0.01704946905374527], "tokens": [["\u0120k", "en", "\u0120bat", "ty"]], "entropy": [[4.704929351806641, 4.292778015136719, 4.752102851867676, 4.071815490722656]], "avg_entropy": [4.455406427383423], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chris Gayle", "score_of_biggest_cluster": 0.01704946905374527, "score_of_first_answer": 0.01704946905374527, "max_score_of_first_answer": 0.10257751494646072, "min_score_of_first_answer": 0.0003047912905458361, "avg_score": 0.01704946905374527, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04038884923254969, "std_of_first_answer": 0.04038884923254969, "avg_entropy_of_first_answer": 4.455406427383423, "lex_sim": 1.0}
{"question": "when did the ottoman empire surrender in ww1", "answer": ["October 30, 1914"], "scores": [[0.20779891312122345, 0.06330990046262741, 0.2828029990196228, 0.05706188827753067]], "normalized_score": [0.12070806324481964], "tokens": [["\u0120October", "\u012030", ",", "\u01201914"]], "entropy": [[4.1966705322265625, 2.7826242446899414, 1.03401517868042, 2.310783863067627]], "avg_entropy": [2.5810234546661377], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1918", "score_of_biggest_cluster": 0.12070806324481964, "score_of_first_answer": 0.12070806324481964, "max_score_of_first_answer": 0.2828029990196228, "min_score_of_first_answer": 0.05706188827753067, "avg_score": 0.12070806324481964, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09630668635559786, "std_of_first_answer": 0.09630668635559786, "avg_entropy_of_first_answer": 2.5810234546661377, "lex_sim": 1.0}
{"question": "what is the name of season 6 of american horror story", "answer": ["Season 6"], "scores": [[0.026338811963796616, 0.5289207696914673]], "normalized_score": [0.11803027242422104], "tokens": [["\u0120Season", "\u01206"]], "entropy": [[7.056003570556641, 2.059502601623535]], "avg_entropy": [4.557753086090088], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Roanoke", "score_of_biggest_cluster": 0.11803027242422104, "score_of_first_answer": 0.11803027242422104, "max_score_of_first_answer": 0.5289207696914673, "min_score_of_first_answer": 0.026338811963796616, "avg_score": 0.11803027242422104, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25129097886383533, "std_of_first_answer": 0.25129097886383533, "avg_entropy_of_first_answer": 4.557753086090088, "lex_sim": 1.0}
{"question": "where does the papillary layer of the skin lie", "answer": ["epidermis"], "scores": [[0.24093741178512573, 0.9994527697563171, 0.9273989200592041]], "normalized_score": [0.6067051291465759], "tokens": [["\u0120ep", "ider", "mis"]], "entropy": [[4.56132698059082, 0.006569047458469868, 0.3223946988582611]], "avg_entropy": [1.6300969089691837], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "adjacent to the epidermis", "score_of_biggest_cluster": 0.6067051291465759, "score_of_first_answer": 0.6067051291465759, "max_score_of_first_answer": 0.9994527697563171, "min_score_of_first_answer": 0.24093741178512573, "avg_score": 0.6067051291465759, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34185225981673256, "std_of_first_answer": 0.34185225981673256, "avg_entropy_of_first_answer": 1.6300969089691837, "lex_sim": 1.0}
{"question": "the temperature of gas is proportional to the", "answer": ["number of electrons in the gas"], "scores": [[0.024631522595882416, 0.9393025636672974, 0.07430995255708694, 0.37592390179634094, 0.55330890417099, 0.19169992208480835]], "normalized_score": [0.20230449736118317], "tokens": [["\u0120number", "\u0120of", "\u0120electrons", "\u0120in", "\u0120the", "\u0120gas"]], "entropy": [[4.925148010253906, 0.47877660393714905, 3.9325358867645264, 2.1403515338897705, 1.7241430282592773, 3.5093324184417725]], "avg_entropy": [2.785047913591067], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "volume", "score_of_biggest_cluster": 0.20230449736118317, "score_of_first_answer": 0.20230449736118317, "max_score_of_first_answer": 0.9393025636672974, "min_score_of_first_answer": 0.024631522595882416, "avg_score": 0.20230449736118317, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3150737970354805, "std_of_first_answer": 0.3150737970354805, "avg_entropy_of_first_answer": 2.785047913591067, "lex_sim": 1.0}
{"question": "where did the race in grease take place", "answer": ["New Jersey"], "scores": [[0.022160885855555534, 0.20650336146354675]], "normalized_score": [0.06764833629131317], "tokens": [["\u0120New", "\u0120Jersey"]], "entropy": [[6.840643405914307, 1.7656277418136597]], "avg_entropy": [4.303135573863983], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Thunder Road", "score_of_biggest_cluster": 0.06764833629131317, "score_of_first_answer": 0.06764833629131317, "max_score_of_first_answer": 0.20650336146354675, "min_score_of_first_answer": 0.022160885855555534, "avg_score": 0.06764833629131317, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09217123780399561, "std_of_first_answer": 0.09217123780399561, "avg_entropy_of_first_answer": 4.303135573863983, "lex_sim": 1.0}
{"question": "what drug does the mom use in requiem for a dream", "answer": ["morphine"], "scores": [[0.02379574254155159]], "normalized_score": [0.02379574254155159], "tokens": [["\u0120morphine"]], "entropy": [[6.003674030303955]], "avg_entropy": [6.003674030303955], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "weight-loss amphetamines", "score_of_biggest_cluster": 0.02379574254155159, "score_of_first_answer": 0.02379574254155159, "max_score_of_first_answer": 0.02379574254155159, "min_score_of_first_answer": 0.02379574254155159, "avg_score": 0.02379574254155159, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.003674030303955, "lex_sim": 1.0}
{"question": "who sang let me tell you about the birds and the bees", "answer": ["Neil Young"], "scores": [[0.014981815591454506, 0.31875842809677124]], "normalized_score": [0.06910555809736252], "tokens": [["\u0120Neil", "\u0120Young"]], "entropy": [[6.044806480407715, 1.9157347679138184]], "avg_entropy": [3.9802706241607666], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jewel Akens", "score_of_biggest_cluster": 0.06910555809736252, "score_of_first_answer": 0.06910555809736252, "max_score_of_first_answer": 0.31875842809677124, "min_score_of_first_answer": 0.014981815591454506, "avg_score": 0.06910555809736252, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.15188830625265837, "std_of_first_answer": 0.15188830625265837, "avg_entropy_of_first_answer": 3.9802706241607666, "lex_sim": 1.0}
{"question": "most of new zealands land supports which economic activity", "answer": ["agriculture"], "scores": [[0.2676647901535034]], "normalized_score": [0.2676647901535034], "tokens": [["\u0120agriculture"]], "entropy": [[3.990750551223755]], "avg_entropy": [3.990750551223755], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "agriculture", "score_of_biggest_cluster": 0.2676647901535034, "score_of_first_answer": 0.2676647901535034, "max_score_of_first_answer": 0.2676647901535034, "min_score_of_first_answer": 0.2676647901535034, "avg_score": 0.2676647901535034, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.990750551223755, "lex_sim": 1.0}
{"question": "what is the most famous building in rennes", "answer": ["rue du la couronne"], "scores": [[0.002302836626768112, 0.5340825915336609, 0.21950821578502655, 0.006485634949058294, 0.0025329533964395523, 0.3397972881793976]], "normalized_score": [0.03385999798774719], "tokens": [["\u0120r", "ue", "\u0120du", "\u0120la", "\u0120cour", "onne"]], "entropy": [[4.568001747131348, 2.479444742202759, 3.634045124053955, 5.558147430419922, 5.032487869262695, 2.904283046722412]], "avg_entropy": [4.029401659965515], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Parlement de Bretagne", "score_of_biggest_cluster": 0.03385999798774719, "score_of_first_answer": 0.03385999798774719, "max_score_of_first_answer": 0.5340825915336609, "min_score_of_first_answer": 0.002302836626768112, "avg_score": 0.03385999798774719, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20229799116004368, "std_of_first_answer": 0.20229799116004368, "avg_entropy_of_first_answer": 4.029401659965515, "lex_sim": 1.0}
{"question": "phase change from gas to solid is called", "answer": ["solidification"], "scores": [[0.1726924180984497, 0.6746297478675842]], "normalized_score": [0.3413260281085968], "tokens": [["\u0120solid", "ification"]], "entropy": [[3.8540377616882324, 1.4852100610733032]], "avg_entropy": [2.669623911380768], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Deposition", "score_of_biggest_cluster": 0.3413260281085968, "score_of_first_answer": 0.3413260281085968, "max_score_of_first_answer": 0.6746297478675842, "min_score_of_first_answer": 0.1726924180984497, "avg_score": 0.3413260281085968, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.25096866488456726, "std_of_first_answer": 0.25096866488456726, "avg_entropy_of_first_answer": 2.669623911380768, "lex_sim": 1.0}
{"question": "what state did they film daddy's home 2", "answer": ["Louisiana"], "scores": [[0.04696621000766754]], "normalized_score": [0.04696620628237724], "tokens": [["\u0120Louisiana"]], "entropy": [[3.8011159896850586]], "avg_entropy": [3.8011159896850586], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Great Barrington, Massachusetts", "score_of_biggest_cluster": 0.04696620628237724, "score_of_first_answer": 0.04696620628237724, "max_score_of_first_answer": 0.04696621000766754, "min_score_of_first_answer": 0.04696621000766754, "avg_score": 0.04696620628237724, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.8011159896850586, "lex_sim": 1.0}
{"question": "where was the salvation army's christmas collection kettle first introduced", "answer": ["Australia"], "scores": [[0.0006945483619347215]], "normalized_score": [0.0006945483619347215], "tokens": [["\u0120Australia"]], "entropy": [[6.137282848358154]], "avg_entropy": [6.137282848358154], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "San Francisco", "score_of_biggest_cluster": 0.0006945483619347215, "score_of_first_answer": 0.0006945483619347215, "max_score_of_first_answer": 0.0006945483619347215, "min_score_of_first_answer": 0.0006945483619347215, "avg_score": 0.0006945483619347215, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.137282848358154, "lex_sim": 1.0}
{"question": "which country has the smallest population in europe", "answer": ["Portugal"], "scores": [[0.012341609224677086]], "normalized_score": [0.012341611087322235], "tokens": [["\u0120Portugal"]], "entropy": [[4.302508354187012]], "avg_entropy": [4.302508354187012], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Vatican City", "score_of_biggest_cluster": 0.012341611087322235, "score_of_first_answer": 0.012341611087322235, "max_score_of_first_answer": 0.012341609224677086, "min_score_of_first_answer": 0.012341609224677086, "avg_score": 0.012341611087322235, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.302508354187012, "lex_sim": 1.0}
{"question": "when was son of a preacher man released", "answer": ["December 12, 2011"], "scores": [[0.020878104493021965, 0.018824797123670578, 0.3788697421550751, 0.014100385829806328]], "normalized_score": [0.038065843284130096], "tokens": [["\u0120December", "\u012012", ",", "\u01202011"]], "entropy": [[4.832241535186768, 4.119902610778809, 1.18973970413208, 3.7150039672851562]], "avg_entropy": [3.464221954345703], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "late 1968", "score_of_biggest_cluster": 0.038065843284130096, "score_of_first_answer": 0.038065843284130096, "max_score_of_first_answer": 0.3788697421550751, "min_score_of_first_answer": 0.014100385829806328, "avg_score": 0.038065843284130096, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.15630889529951603, "std_of_first_answer": 0.15630889529951603, "avg_entropy_of_first_answer": 3.464221954345703, "lex_sim": 1.0}
{"question": "who signed the largest on the declaration of independence", "answer": ["George Washington"], "scores": [[0.3474777042865753, 0.9046520590782166]], "normalized_score": [0.5606660842895508], "tokens": [["\u0120George", "\u0120Washington"]], "entropy": [[2.629786252975464, 0.65522301197052]], "avg_entropy": [1.642504632472992], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Hancock", "score_of_biggest_cluster": 0.5606660842895508, "score_of_first_answer": 0.5606660842895508, "max_score_of_first_answer": 0.9046520590782166, "min_score_of_first_answer": 0.3474777042865753, "avg_score": 0.5606660842895508, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2785871773958206, "std_of_first_answer": 0.2785871773958206, "avg_entropy_of_first_answer": 1.642504632472992, "lex_sim": 1.0}
{"question": "what is the salary of the ceo of microsoft", "answer": ["$1.5 billions"], "scores": [[0.6064251661300659, 0.05214620754122734, 0.5093234181404114, 0.12446358054876328, 0.0012165935477241874]], "normalized_score": [0.07541128247976303], "tokens": [["\u0120$", "1", ".", "5", "\u0120billions"]], "entropy": [[2.989875316619873, 5.074717044830322, 2.002927303314209, 3.816403388977051, 2.1695361137390137]], "avg_entropy": [3.2106918334960937], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "$84.3 million", "score_of_biggest_cluster": 0.07541128247976303, "score_of_first_answer": 0.07541128247976303, "max_score_of_first_answer": 0.6064251661300659, "min_score_of_first_answer": 0.0012165935477241874, "avg_score": 0.07541128247976303, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2492815588209629, "std_of_first_answer": 0.2492815588209629, "avg_entropy_of_first_answer": 3.2106918334960937, "lex_sim": 1.0}
{"question": "who is the present rajya sabha speaker of india", "answer": ["ramnath singh"], "scores": [[0.004319092258810997, 0.31076622009277344, 0.9805958271026611, 0.2738780379295349, 0.9841238856315613]], "normalized_score": [0.20416656136512756], "tokens": [["\u0120ram", "n", "ath", "\u0120sing", "h"]], "entropy": [[5.460535049438477, 3.5517096519470215, 0.15617966651916504, 3.315542221069336, 0.13247498869895935]], "avg_entropy": [2.5232883155345918], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Venkaiah Naidu", "score_of_biggest_cluster": 0.20416656136512756, "score_of_first_answer": 0.20416656136512756, "max_score_of_first_answer": 0.9841238856315613, "min_score_of_first_answer": 0.004319092258810997, "avg_score": 0.20416656136512756, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39935248563249304, "std_of_first_answer": 0.39935248563249304, "avg_entropy_of_first_answer": 2.5232883155345918, "lex_sim": 1.0}
{"question": "who is the actor that plays sneaky pete", "answer": ["Daniel Stern"], "scores": [[0.007249058689922094, 0.015170629136264324]], "normalized_score": [0.010486788116395473], "tokens": [["\u0120Daniel", "\u0120Stern"]], "entropy": [[5.588890075683594, 4.2919921875]], "avg_entropy": [4.940441131591797], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Giovanni Ribisi", "score_of_biggest_cluster": 0.010486788116395473, "score_of_first_answer": 0.010486788116395473, "max_score_of_first_answer": 0.015170629136264324, "min_score_of_first_answer": 0.007249058689922094, "avg_score": 0.010486788116395473, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.003960785223171115, "std_of_first_answer": 0.003960785223171115, "avg_entropy_of_first_answer": 4.940441131591797, "lex_sim": 1.0}
{"question": "who was the book of philippians written to", "answer": ["Paul, also known as the apostle to the gentiles"], "scores": [[0.2943505644798279, 0.051585786044597626, 0.016752516850829124, 0.7034232020378113, 0.98954176902771, 0.261077344417572, 0.8424827456474304, 0.6161636710166931, 0.9964606165885925, 0.5423085689544678, 0.9930058717727661]], "normalized_score": [0.35928353667259216], "tokens": [["\u0120Paul", ",", "\u0120also", "\u0120known", "\u0120as", "\u0120the", "\u0120apostle", "\u0120to", "\u0120the", "\u0120gent", "iles"]], "entropy": [[3.958887815475464, 2.3741140365600586, 4.628302097320557, 1.4795485734939575, 0.09155702590942383, 3.85711407661438, 0.9022478461265564, 1.3938305377960205, 0.043402351438999176, 2.5507113933563232, 0.05246683955192566]], "avg_entropy": [1.9392893266948787], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the church at Philippi", "score_of_biggest_cluster": 0.35928353667259216, "score_of_first_answer": 0.35928353667259216, "max_score_of_first_answer": 0.9964606165885925, "min_score_of_first_answer": 0.016752516850829124, "avg_score": 0.35928353667259216, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35392485438364363, "std_of_first_answer": 0.35392485438364363, "avg_entropy_of_first_answer": 1.9392893266948787, "lex_sim": 1.0}
{"question": "why the hole in the ozone layer over antarctica", "answer": ["the ozone layer is recovering due to natural variability"], "scores": [[0.029258813709020615, 0.0630391389131546, 0.6263901591300964, 0.4437200427055359, 0.011944844387471676, 0.014652957208454609, 0.9929524064064026, 0.006925925612449646, 0.012810925953090191]], "normalized_score": [0.058402955532073975], "tokens": [["\u0120the", "\u0120ozone", "\u0120layer", "\u0120is", "\u0120recovering", "\u0120due", "\u0120to", "\u0120natural", "\u0120variability"]], "entropy": [[6.614861488342285, 3.821112632751465, 1.864058256149292, 2.9844393730163574, 5.0223236083984375, 1.7302671670913696, 0.07191039621829987, 4.100532531738281, 4.03200101852417]], "avg_entropy": [3.3601673858033285], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "bromine", "score_of_biggest_cluster": 0.058402955532073975, "score_of_first_answer": 0.058402955532073975, "max_score_of_first_answer": 0.9929524064064026, "min_score_of_first_answer": 0.006925925612449646, "avg_score": 0.058402955532073975, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3402612082737784, "std_of_first_answer": 0.3402612082737784, "avg_entropy_of_first_answer": 3.3601673858033285, "lex_sim": 1.0}
{"question": "what is the meaning of the name comanche", "answer": ["a tribe native to and the name of one of my grandmothers who was part of"], "scores": [[0.03640270605683327, 0.2570294141769409, 0.004858984146267176, 0.967987060546875, 0.0006820205016992986, 0.027445653453469276, 0.09795784950256348, 0.7209908962249756, 0.01593928039073944, 0.9505468010902405, 0.012266004458069801, 0.002090675523504615, 0.8015899062156677, 0.009280259720981121, 0.38972601294517517, 0.07339555770158768, 0.3321574628353119]], "normalized_score": [0.059796497225761414], "tokens": [["\u0120a", "\u0120tribe", "\u0120native", "\u0120to", "\u0120and", "\u0120the", "\u0120name", "\u0120of", "\u0120one", "\u0120of", "\u0120my", "\u0120grandmother", "s", "\u0120who", "\u0120was", "\u0120part", "\u0120of"]], "entropy": [[6.2431135177612305, 4.625570297241211, 1.7937198877334595, 0.20295396447181702, 2.4436886310577393, 4.781036376953125, 4.853971481323242, 1.5701847076416016, 2.004465103149414, 0.45429471135139465, 1.187464952468872, 4.378824234008789, 0.7581204175949097, 1.6221270561218262, 3.197777509689331, 3.940614700317383, 3.065434455871582]], "avg_entropy": [2.7719624708680546], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "enemy", "score_of_biggest_cluster": 0.059796497225761414, "score_of_first_answer": 0.059796497225761414, "max_score_of_first_answer": 0.967987060546875, "min_score_of_first_answer": 0.0006820205016992986, "avg_score": 0.059796497225761414, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34718771395385223, "std_of_first_answer": 0.34718771395385223, "avg_entropy_of_first_answer": 2.7719624708680546, "lex_sim": 1.0}
{"question": "to whom do the powers not given to the federal government by the constitution go", "answer": ["the constitution grants the federal government all powers not specifically prohibited by either the constitution or the law"], "scores": [[0.17130343616008759, 0.0053026494570076466, 0.01133027859032154, 0.2177295833826065, 0.3796435594558716, 0.9334152936935425, 0.04943164810538292, 0.6830652356147766, 0.6511220335960388, 0.32452139258384705, 0.10336319357156754, 0.5152167677879333, 0.005353786051273346, 0.8317952752113342, 0.7711679935455322, 0.9421809315681458, 0.4259586036205292, 0.027512075379490852]], "normalized_score": [0.16806189715862274], "tokens": [["\u0120the", "\u0120constitution", "\u0120grants", "\u0120the", "\u0120federal", "\u0120government", "\u0120all", "\u0120powers", "\u0120not", "\u0120specifically", "\u0120prohibited", "\u0120by", "\u0120either", "\u0120the", "\u0120constitution", "\u0120or", "\u0120the", "\u0120law"]], "entropy": [[4.173765182495117, 3.079695224761963, 1.9723749160766602, 3.38069224357605, 2.4094884395599365, 0.4392775893211365, 3.6021199226379395, 1.5013415813446045, 1.7485662698745728, 2.6077098846435547, 3.203594207763672, 1.5244152545928955, 0.7804713845252991, 1.1296088695526123, 1.5219860076904297, 0.3522271513938904, 2.6823134422302246, 3.7084109783172607]], "avg_entropy": [2.2121143639087677], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the states or the people", "score_of_biggest_cluster": 0.16806189715862274, "score_of_first_answer": 0.16806189715862274, "max_score_of_first_answer": 0.9421809315681458, "min_score_of_first_answer": 0.0053026494570076466, "avg_score": 0.16806189715862274, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33018956461029325, "std_of_first_answer": 0.33018956461029325, "avg_entropy_of_first_answer": 2.2121143639087677, "lex_sim": 1.0}
{"question": "who is responsible for establishing local licensing forum", "answer": ["ALF"], "scores": [[0.0009764460846781731, 0.12465821951627731]], "normalized_score": [0.011032767593860626], "tokens": [["\u0120AL", "F"]], "entropy": [[7.423155784606934, 4.5598368644714355]], "avg_entropy": [5.991496324539185], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "unitary authorities", "score_of_biggest_cluster": 0.011032767593860626, "score_of_first_answer": 0.011032767593860626, "max_score_of_first_answer": 0.12465821951627731, "min_score_of_first_answer": 0.0009764460846781731, "avg_score": 0.011032767593860626, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06184088671579957, "std_of_first_answer": 0.06184088671579957, "avg_entropy_of_first_answer": 5.991496324539185, "lex_sim": 1.0}
{"question": "who is the young man in hawthorne's my kinsman major molineux", "answer": ["Ben Walley"], "scores": [[0.01567315123975277, 0.0003384914598427713, 0.03394819423556328]], "normalized_score": [0.005647292826324701], "tokens": [["\u0120Ben", "\u0120Wal", "ley"]], "entropy": [[6.175105094909668, 5.1157307624816895, 3.7775356769561768]], "avg_entropy": [5.022790511449178], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Robin", "score_of_biggest_cluster": 0.005647292826324701, "score_of_first_answer": 0.005647292826324701, "max_score_of_first_answer": 0.03394819423556328, "min_score_of_first_answer": 0.0003384914598427713, "avg_score": 0.005647292826324701, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.013738595705920306, "std_of_first_answer": 0.013738595705920306, "avg_entropy_of_first_answer": 5.022790511449178, "lex_sim": 1.0}
{"question": "first day collection of mission china assamese film", "answer": ["\u0985\u09a8\u09cd\u09a4\u09b0\u09ac\u09be\u099c\u09af\u09bc\u09c0\u09a8\u09be\u09a4\u09bf\u0995 \u0997\u09cb\u099f\u09be\u09ae\u09be\ufffd"], "scores": [[0.004034389276057482, 0.15612547099590302, 0.02039557509124279, 0.9135567545890808, 0.279330849647522, 0.6796864867210388, 0.9999077320098877, 0.350129097700119, 0.9981091022491455, 0.4624063968658447, 0.8822511434555054, 0.2928125560283661, 0.3010959029197693, 0.021403424441814423, 0.8369803428649902, 0.5934640765190125, 0.983121931552887, 0.07018774002790451, 0.8239073157310486, 0.08574658632278442, 0.8716105818748474, 0.7911563515663147, 0.04923662170767784, 0.9999934434890747, 0.17038613557815552, 0.353133887052536, 0.15858754515647888, 0.2547520101070404, 0.4269042909145355, 0.45789268612861633, 0.013176028616726398, 0.5963997840881348, 0.16097891330718994, 0.4380697011947632, 0.4556906521320343, 0.20251503586769104, 0.9712309241294861, 0.07154456526041031, 0.13887637853622437, 1.0, 0.11593262106180191, 0.9574527144432068, 0.07552527636289597, 0.9122763276100159, 0.7062065601348877, 0.6344119906425476, 0.17272496223449707, 0.48093971610069275, 0.7374939322471619, 5.5463506214437075e-06]], "normalized_score": [0.23045016825199127], "tokens": [["\u0120", "\u00e0\u00a6", "\u0127", "\u00e0\u00a6", "\u00a8", "\u00e0", "\u00a7", "\u012f", "\u00e0\u00a6", "\u00a4", "\u00e0\u00a6", "\u00b0", "\u00e0\u00a6", "\u00ac", "\u00e0\u00a6", "\u00be", "\u00e0\u00a6", "\u013e", "\u00e0\u00a6", "\u00af", "\u00e0\u00a6", "\u00bc", "\u00e0", "\u00a7", "\u0122", "\u00e0\u00a6", "\u00a8", "\u00e0\u00a6", "\u00be", "\u00e0\u00a6", "\u00a4", "\u00e0\u00a6", "\u00bf", "\u00e0\u00a6", "\u0137", "\u0120", "\u00e0\u00a6", "\u0139", "\u00e0", "\u00a7", "\u012d", "\u00e0\u00a6", "\u0141", "\u00e0\u00a6", "\u00be", "\u00e0\u00a6", "\u00ae", "\u00e0\u00a6", "\u00be", "\u00ef\u00bf\u00bd"]], "entropy": [[6.795766830444336, 2.642622947692871, 2.958897113800049, 0.3765504062175751, 2.7029175758361816, 0.7440910339355469, 0.0011584219755604863, 1.2035529613494873, 0.01854104921221733, 1.6630710363388062, 0.5174268484115601, 2.5312697887420654, 1.2725006341934204, 2.0899438858032227, 0.4810428321361542, 1.6489108800888062, 0.09846757352352142, 1.7550976276397705, 0.7401013374328613, 1.7270323038101196, 0.4505994915962219, 0.732282817363739, 1.6941800117492676, 9.994317224482074e-05, 0.974120557308197, 1.9824199676513672, 2.7731268405914307, 1.9714723825454712, 2.4389472007751465, 1.6748192310333252, 2.800424575805664, 1.2952548265457153, 2.423210382461548, 1.8168847560882568, 2.193958282470703, 1.9060038328170776, 0.1660301238298416, 2.895667552947998, 0.41885173320770264, 9.150975301963626e-07, 1.7566088438034058, 0.22687190771102905, 2.043163776397705, 0.414201557636261, 1.141496181488037, 1.253974437713623, 2.367250442504883, 1.4454023838043213, 1.1511636972427368, 1.102209210395813]], "avg_entropy": [1.509593219046808], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u20b9 39.97 lakh", "score_of_biggest_cluster": 0.23045016825199127, "score_of_first_answer": 0.23045016825199127, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.5463506214437075e-06, "avg_score": 0.23045016825199127, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3460015048745526, "std_of_first_answer": 0.3460015048745526, "avg_entropy_of_first_answer": 1.509593219046808, "lex_sim": 0.0}
{"question": "nobel laureate who began career as accountant in calcutta", "answer": ["rajan harrison"], "scores": [[0.004394411575049162, 0.26288095116615295, 0.05557781085371971, 0.0034693763591349125, 0.08560729771852493]], "normalized_score": [0.028580166399478912], "tokens": [["\u0120r", "aj", "an", "\u0120h", "arrison"]], "entropy": [[6.100715160369873, 3.492135524749756, 2.619286060333252, 2.725630283355713, 2.923476219177246]], "avg_entropy": [3.572248649597168], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Amartya Sen", "score_of_biggest_cluster": 0.028580166399478912, "score_of_first_answer": 0.028580166399478912, "max_score_of_first_answer": 0.26288095116615295, "min_score_of_first_answer": 0.0034693763591349125, "avg_score": 0.028580166399478912, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09551754238115875, "std_of_first_answer": 0.09551754238115875, "avg_entropy_of_first_answer": 3.572248649597168, "lex_sim": 1.0}
{"question": "when does i'm a celebrity get me out of here start in the uk", "answer": ["October 5"], "scores": [[0.06234769895672798, 0.03576691448688507]], "normalized_score": [0.04722271487116814], "tokens": [["\u0120October", "\u01205"]], "entropy": [[4.261597633361816, 3.5669565200805664]], "avg_entropy": [3.9142770767211914], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2018", "score_of_biggest_cluster": 0.04722271487116814, "score_of_first_answer": 0.04722271487116814, "max_score_of_first_answer": 0.06234769895672798, "min_score_of_first_answer": 0.03576691448688507, "avg_score": 0.04722271487116814, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.013290392234921455, "std_of_first_answer": 0.013290392234921455, "avg_entropy_of_first_answer": 3.9142770767211914, "lex_sim": 1.0}
{"question": "who won the gold for the men's figure skating", "answer": ["Michael Chabon"], "scores": [[0.013183929026126862, 0.01889495924115181, 0.017762701958417892, 0.4096018373966217]], "normalized_score": [0.03669149428606033], "tokens": [["\u0120Michael", "\u0120Ch", "ab", "on"]], "entropy": [[5.561574935913086, 3.3851027488708496, 4.850473403930664, 1.6711969375610352]], "avg_entropy": [3.8670870065689087], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yuzuru Hanyu", "score_of_biggest_cluster": 0.03669149428606033, "score_of_first_answer": 0.03669149428606033, "max_score_of_first_answer": 0.4096018373966217, "min_score_of_first_answer": 0.013183929026126862, "avg_score": 0.03669149428606033, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17018221738221267, "std_of_first_answer": 0.17018221738221267, "avg_entropy_of_first_answer": 3.8670870065689087, "lex_sim": 1.0}
{"question": "what is the name of the chief justice of ghana", "answer": ["akunyama Samuel Otu"], "scores": [[0.0023566358722746372, 0.0032854266464710236, 0.023541029542684555, 0.009504602290689945, 0.0065779909491539, 0.005153735633939505, 0.7549906373023987]], "normalized_score": [0.012370863929390907], "tokens": [["\u0120a", "kun", "y", "ama", "\u0120Samuel", "\u0120Ot", "u"]], "entropy": [[6.033270835876465, 5.7886247634887695, 3.6576178073883057, 4.254646301269531, 5.933237075805664, 4.496602535247803, 1.308517575263977]], "avg_entropy": [4.496073842048645], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sophia Akuffo", "score_of_biggest_cluster": 0.012370863929390907, "score_of_first_answer": 0.012370863929390907, "max_score_of_first_answer": 0.7549906373023987, "min_score_of_first_answer": 0.0023566358722746372, "avg_score": 0.012370863929390907, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26133509448105463, "std_of_first_answer": 0.26133509448105463, "avg_entropy_of_first_answer": 4.496073842048645, "lex_sim": 1.0}
{"question": "who has hit a double century in the ranji trophy for jharkhand in 2016", "answer": ["Virat Kohli"], "scores": [[0.019589008763432503, 0.9346818327903748, 0.893525242805481, 0.9970526695251465]], "normalized_score": [0.3573759198188782], "tokens": [["\u0120Vir", "at", "\u0120Koh", "li"]], "entropy": [[5.762831687927246, 0.4485653042793274, 0.642430305480957, 0.029291803017258644]], "avg_entropy": [1.7207797751761973], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jaydev Shah", "score_of_biggest_cluster": 0.3573759198188782, "score_of_first_answer": 0.3573759198188782, "max_score_of_first_answer": 0.9970526695251465, "min_score_of_first_answer": 0.019589008763432503, "avg_score": 0.3573759198188782, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4010062764568241, "std_of_first_answer": 0.4010062764568241, "avg_entropy_of_first_answer": 1.7207797751761973, "lex_sim": 1.0}
{"question": "who has trained the most melbourne cup winners", "answer": ["Gary Coombes"], "scores": [[0.005991832353174686, 0.012498600408434868, 0.2904289960861206, 0.9991399049758911]], "normalized_score": [0.06827660650014877], "tokens": [["\u0120Gary", "\u0120Co", "omb", "es"]], "entropy": [[6.673048973083496, 5.853035926818848, 2.8626766204833984, 0.008951777592301369]], "avg_entropy": [3.849428324494511], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bart Cummings", "score_of_biggest_cluster": 0.06827660650014877, "score_of_first_answer": 0.06827660650014877, "max_score_of_first_answer": 0.9991399049758911, "min_score_of_first_answer": 0.005991832353174686, "avg_score": 0.06827660650014877, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4046810050364265, "std_of_first_answer": 0.4046810050364265, "avg_entropy_of_first_answer": 3.849428324494511, "lex_sim": 1.0}
{"question": "where does the river irk start and finish", "answer": ["Kirtlington reservoir"], "scores": [[0.0022513007279485464, 0.006172299850732088, 0.02246812917292118, 0.0018286658450961113]], "normalized_score": [0.004888161085546017], "tokens": [["\u0120K", "irt", "lington", "\u0120reservoir"]], "entropy": [[6.092413902282715, 5.7668633460998535, 1.520772933959961, 3.436229705810547]], "avg_entropy": [4.204069972038269], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "east of Royton", "score_of_biggest_cluster": 0.004888161085546017, "score_of_first_answer": 0.004888161085546017, "max_score_of_first_answer": 0.02246812917292118, "min_score_of_first_answer": 0.0018286658450961113, "avg_score": 0.004888161085546017, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.008421259046573731, "std_of_first_answer": 0.008421259046573731, "avg_entropy_of_first_answer": 4.204069972038269, "lex_sim": 1.0}
{"question": "which of the following helmiths feeds on blood", "answer": ["a vampire"], "scores": [[0.017568569630384445, 0.3587418794631958]], "normalized_score": [0.0793888047337532], "tokens": [["\u0120a", "\u0120vampire"]], "entropy": [[7.101614952087402, 4.826105117797852]], "avg_entropy": [5.963860034942627], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Necator americanus", "score_of_biggest_cluster": 0.0793888047337532, "score_of_first_answer": 0.0793888047337532, "max_score_of_first_answer": 0.3587418794631958, "min_score_of_first_answer": 0.017568569630384445, "avg_score": 0.0793888047337532, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17058665491640568, "std_of_first_answer": 0.17058665491640568, "avg_entropy_of_first_answer": 5.963860034942627, "lex_sim": 1.0}
{"question": "what country shares borders with both belarus and romania", "answer": ["Russia"], "scores": [[0.034639984369277954]], "normalized_score": [0.034639980643987656], "tokens": [["\u0120Russia"]], "entropy": [[3.495168924331665]], "avg_entropy": [3.495168924331665], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ukraine", "score_of_biggest_cluster": 0.034639980643987656, "score_of_first_answer": 0.034639980643987656, "max_score_of_first_answer": 0.034639984369277954, "min_score_of_first_answer": 0.034639984369277954, "avg_score": 0.034639980643987656, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.495168924331665, "lex_sim": 1.0}
{"question": "who wrote how do you do it by gerry and the pacemakers", "answer": ["Peter Green"], "scores": [[0.004418601281940937, 0.006436807569116354]], "normalized_score": [0.005333073437213898], "tokens": [["\u0120Peter", "\u0120Green"]], "entropy": [[5.681952476501465, 5.585789203643799]], "avg_entropy": [5.633870840072632], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mitch Murray", "score_of_biggest_cluster": 0.005333073437213898, "score_of_first_answer": 0.005333073437213898, "max_score_of_first_answer": 0.006436807569116354, "min_score_of_first_answer": 0.004418601281940937, "avg_score": 0.005333073437213898, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0010091031435877085, "std_of_first_answer": 0.0010091031435877085, "avg_entropy_of_first_answer": 5.633870840072632, "lex_sim": 1.0}
{"question": "who wrote the declaration of man and citizen", "answer": ["Thomas Jefferson"], "scores": [[0.2997805178165436, 0.9583674073219299]], "normalized_score": [0.536003589630127], "tokens": [["\u0120Thomas", "\u0120Jefferson"]], "entropy": [[4.062577247619629, 0.3189402222633362]], "avg_entropy": [2.1907587349414825], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "General Lafayette", "score_of_biggest_cluster": 0.536003589630127, "score_of_first_answer": 0.536003589630127, "max_score_of_first_answer": 0.9583674073219299, "min_score_of_first_answer": 0.2997805178165436, "avg_score": 0.536003589630127, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3292934447526932, "std_of_first_answer": 0.3292934447526932, "avg_entropy_of_first_answer": 2.1907587349414825, "lex_sim": 1.0}
{"question": "what is the breakdown product formed when one phosphate group is removed from atp", "answer": ["adenosine monophosphate"], "scores": [[0.2005041092634201, 0.9479103088378906, 0.9922279715538025, 0.11812414973974228, 0.9945940375328064, 0.9846339821815491, 0.9894824028015137]], "normalized_score": [0.5781282782554626], "tokens": [["\u0120ad", "enos", "ine", "\u0120mon", "oph", "osph", "ate"]], "entropy": [[3.333634376525879, 0.3033028542995453, 0.06640370935201645, 1.4933013916015625, 0.05116191506385803, 0.11730849742889404, 0.0812099501490593]], "avg_entropy": [0.7780460992029735], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "adenosine diphosphate (ADP)", "score_of_biggest_cluster": 0.5781282782554626, "score_of_first_answer": 0.5781282782554626, "max_score_of_first_answer": 0.9945940375328064, "min_score_of_first_answer": 0.11812414973974228, "avg_score": 0.5781282782554626, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3724847849360623, "std_of_first_answer": 0.3724847849360623, "avg_entropy_of_first_answer": 0.7780460992029735, "lex_sim": 1.0}
{"question": "who does the voice of mrs. wolowitz on the big bang theory", "answer": ["Julie Bowen"], "scores": [[0.012634186074137688, 0.7929884791374207]], "normalized_score": [0.1000937893986702], "tokens": [["\u0120Julie", "\u0120Bowen"]], "entropy": [[2.719651222229004, 1.1341028213500977]], "avg_entropy": [1.9268770217895508], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Carol Ann Susi", "score_of_biggest_cluster": 0.1000937893986702, "score_of_first_answer": 0.1000937893986702, "max_score_of_first_answer": 0.7929884791374207, "min_score_of_first_answer": 0.012634186074137688, "avg_score": 0.1000937893986702, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3901771465316415, "std_of_first_answer": 0.3901771465316415, "avg_entropy_of_first_answer": 1.9268770217895508, "lex_sim": 1.0}
{"question": "when did martial law in the philippines end", "answer": ["June 12, 1998"], "scores": [[0.05039495974779129, 0.350031316280365, 0.5518797039985657, 0.0026517037767916918]], "normalized_score": [0.07127968966960907], "tokens": [["\u0120June", "\u012012", ",", "\u01201998"]], "entropy": [[4.2891669273376465, 2.65972900390625, 1.4425150156021118, 1.5113494396209717]], "avg_entropy": [2.475690096616745], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1981", "score_of_biggest_cluster": 0.07127968966960907, "score_of_first_answer": 0.07127968966960907, "max_score_of_first_answer": 0.5518797039985657, "min_score_of_first_answer": 0.0026517037767916918, "avg_score": 0.07127968966960907, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2245293795361577, "std_of_first_answer": 0.2245293795361577, "avg_entropy_of_first_answer": 2.475690096616745, "lex_sim": 1.0}
{"question": "who won oscar for best director this month", "answer": ["George Roy Hill"], "scores": [[0.00348433549515903, 0.0008561221766285598, 0.8698133826255798]], "normalized_score": [0.013741282746195793], "tokens": [["\u0120George", "\u0120Roy", "\u0120Hill"]], "entropy": [[4.803134918212891, 2.9203977584838867, 1.0324450731277466]], "avg_entropy": [2.918659249941508], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Guillermo del Toro", "score_of_biggest_cluster": 0.013741282746195793, "score_of_first_answer": 0.013741282746195793, "max_score_of_first_answer": 0.8698133826255798, "min_score_of_first_answer": 0.0008561221766285598, "avg_score": 0.013741282746195793, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.409012312487058, "std_of_first_answer": 0.409012312487058, "avg_entropy_of_first_answer": 2.918659249941508, "lex_sim": 1.0}
{"question": "who is playing halftime at the pro bowl", "answer": ["Baltimore Ravens vs. San Francisco 49ers"], "scores": [[0.0029500650707632303, 0.9039974212646484, 0.06724511086940765, 0.11910707503557205, 0.33386388421058655, 0.8037990927696228, 0.9478954076766968, 0.9472667574882507]], "normalized_score": [0.2182445228099823], "tokens": [["\u0120Baltimore", "\u0120Ravens", "\u0120vs", ".", "\u0120San", "\u0120Francisco", "\u012049", "ers"]], "entropy": [[6.3144450187683105, 0.6011924147605896, 0.9867412447929382, 3.2129600048065186, 2.784910202026367, 0.7495793104171753, 0.4152260720729828, 0.3502952456474304]], "avg_entropy": [1.926918689161539], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jordan Fisher", "score_of_biggest_cluster": 0.2182445228099823, "score_of_first_answer": 0.2182445228099823, "max_score_of_first_answer": 0.9478954076766968, "min_score_of_first_answer": 0.0029500650707632303, "avg_score": 0.2182445228099823, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39705057632257734, "std_of_first_answer": 0.39705057632257734, "avg_entropy_of_first_answer": 1.926918689161539, "lex_sim": 1.0}
{"question": "the german princes who chose the holy roman empire were called", "answer": ["the electors"], "scores": [[0.09516934305429459, 0.020544372498989105]], "normalized_score": [0.044217582792043686], "tokens": [["\u0120the", "\u0120electors"]], "entropy": [[5.54823112487793, 6.032201766967773]], "avg_entropy": [5.790216445922852], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "prince-electors", "score_of_biggest_cluster": 0.044217582792043686, "score_of_first_answer": 0.044217582792043686, "max_score_of_first_answer": 0.09516934305429459, "min_score_of_first_answer": 0.020544372498989105, "avg_score": 0.044217582792043686, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03731248527765274, "std_of_first_answer": 0.03731248527765274, "avg_entropy_of_first_answer": 5.790216445922852, "lex_sim": 1.0}
{"question": "what is the democracy of the united states", "answer": ["direct"], "scores": [[0.08754777908325195]], "normalized_score": [0.08754776418209076], "tokens": [["\u0120direct"]], "entropy": [[5.185303688049316]], "avg_entropy": [5.185303688049316], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "federal republic", "score_of_biggest_cluster": 0.08754776418209076, "score_of_first_answer": 0.08754776418209076, "max_score_of_first_answer": 0.08754777908325195, "min_score_of_first_answer": 0.08754777908325195, "avg_score": 0.08754776418209076, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.185303688049316, "lex_sim": 1.0}
{"question": "what is the speed of the nascar pace car", "answer": ["220 mile per hour"], "scores": [[0.012108789756894112, 0.0030304144602268934, 0.5640432834625244, 0.9790281653404236]], "normalized_score": [0.06709307432174683], "tokens": [["\u0120220", "\u0120mile", "\u0120per", "\u0120hour"]], "entropy": [[5.514787197113037, 2.1776013374328613, 1.5627083778381348, 0.14485901594161987]], "avg_entropy": [2.3499889820814133], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "pit road speed", "score_of_biggest_cluster": 0.06709307432174683, "score_of_first_answer": 0.06709307432174683, "max_score_of_first_answer": 0.9790281653404236, "min_score_of_first_answer": 0.0030304144602268934, "avg_score": 0.06709307432174683, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.409204004914997, "std_of_first_answer": 0.409204004914997, "avg_entropy_of_first_answer": 2.3499889820814133, "lex_sim": 1.0}
{"question": "what episode does caroline get turned into a vampire", "answer": ["season 2 episode 9"], "scores": [[0.0493658147752285, 0.17356447875499725, 0.3913542628288269, 0.058682143688201904]], "normalized_score": [0.11843791604042053], "tokens": [["\u0120season", "\u01202", "\u0120episode", "\u01209"]], "entropy": [[5.909651279449463, 2.5916223526000977, 1.8104007244110107, 3.231656551361084]], "avg_entropy": [3.385832726955414], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brave New World", "score_of_biggest_cluster": 0.11843791604042053, "score_of_first_answer": 0.11843791604042053, "max_score_of_first_answer": 0.3913542628288269, "min_score_of_first_answer": 0.0493658147752285, "avg_score": 0.11843791604042053, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13778817087556036, "std_of_first_answer": 0.13778817087556036, "avg_entropy_of_first_answer": 3.385832726955414, "lex_sim": 1.0}
{"question": "who plays meredith quill in guardians of the galaxy 2", "answer": ["Jessica Stroup, Kelly Hu"], "scores": [[0.047434959560632706, 0.001474476419389248, 0.9189041256904602, 0.002364685060456395, 0.016326770186424255, 0.4322015643119812]], "normalized_score": [0.03199344500899315], "tokens": [["\u0120Jessica", "\u0120St", "roup", ",", "\u0120Kelly", "\u0120Hu"]], "entropy": [[5.528464317321777, 4.456091403961182, 0.6776323914527893, 0.1815621554851532, 6.245207786560059, 3.466229200363159]], "avg_entropy": [3.4258645425240197], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Laura Jane Haddock", "score_of_biggest_cluster": 0.03199344500899315, "score_of_first_answer": 0.03199344500899315, "max_score_of_first_answer": 0.9189041256904602, "min_score_of_first_answer": 0.001474476419389248, "avg_score": 0.03199344500899315, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3411384522847683, "std_of_first_answer": 0.3411384522847683, "avg_entropy_of_first_answer": 3.4258645425240197, "lex_sim": 1.0}
{"question": "when did the new day win the tag team titles", "answer": ["July 30th, 2012"], "scores": [[0.050118789076805115, 0.025151534005999565, 0.697055995464325, 0.1404910832643509, 0.044207774102687836]], "normalized_score": [0.0885922983288765], "tokens": [["\u0120July", "\u012030", "th", ",", "\u01202012"]], "entropy": [[4.997591495513916, 3.8613195419311523, 1.0042952299118042, 2.4435272216796875, 3.681793451309204]], "avg_entropy": [3.197705388069153], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2016", "score_of_biggest_cluster": 0.0885922983288765, "score_of_first_answer": 0.0885922983288765, "max_score_of_first_answer": 0.697055995464325, "min_score_of_first_answer": 0.025151534005999565, "avg_score": 0.0885922983288765, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25594694285938924, "std_of_first_answer": 0.25594694285938924, "avg_entropy_of_first_answer": 3.197705388069153, "lex_sim": 1.0}
{"question": "who won the ncaa women's championship last year 2017", "answer": ["Notre Dame"], "scores": [[0.02737828716635704, 0.9887596368789673]], "normalized_score": [0.16453129053115845], "tokens": [["\u0120Notre", "\u0120Dame"]], "entropy": [[5.661855697631836, 0.09427015483379364]], "avg_entropy": [2.878062926232815], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "South Carolina Gamecocks", "score_of_biggest_cluster": 0.16453129053115845, "score_of_first_answer": 0.16453129053115845, "max_score_of_first_answer": 0.9887596368789673, "min_score_of_first_answer": 0.02737828716635704, "avg_score": 0.16453129053115845, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4806906748563051, "std_of_first_answer": 0.4806906748563051, "avg_entropy_of_first_answer": 2.878062926232815, "lex_sim": 1.0}
{"question": "what is the emblematic rhythm of dominican republic", "answer": ["la pompe"], "scores": [[0.00480366637930274, 0.07473061978816986, 0.031852416694164276, 0.5728766918182373]], "normalized_score": [0.05059046670794487], "tokens": [["\u0120la", "\u0120p", "om", "pe"]], "entropy": [[6.272104263305664, 4.8346781730651855, 3.61946439743042, 2.191189765930176]], "avg_entropy": [4.229359149932861], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "merengue", "score_of_biggest_cluster": 0.05059046670794487, "score_of_first_answer": 0.05059046670794487, "max_score_of_first_answer": 0.5728766918182373, "min_score_of_first_answer": 0.00480366637930274, "avg_score": 0.05059046670794487, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23332162506814905, "std_of_first_answer": 0.23332162506814905, "avg_entropy_of_first_answer": 4.229359149932861, "lex_sim": 1.0}
{"question": "when was where have all the flowers gone written", "answer": ["November 2007"], "scores": [[0.00715103792026639, 0.000738825008738786]], "normalized_score": [0.0022985569667071104], "tokens": [["\u0120November", "\u01202007"]], "entropy": [[6.018787384033203, 3.826805830001831]], "avg_entropy": [4.922796607017517], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1955", "score_of_biggest_cluster": 0.0022985569667071104, "score_of_first_answer": 0.0022985569667071104, "max_score_of_first_answer": 0.00715103792026639, "min_score_of_first_answer": 0.000738825008738786, "avg_score": 0.0022985569667071104, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.003206106455763802, "std_of_first_answer": 0.003206106455763802, "avg_entropy_of_first_answer": 4.922796607017517, "lex_sim": 1.0}
{"question": "who played the first nfl thursday night football game", "answer": ["Terry Bradshaw"], "scores": [[0.01070152036845684, 0.8368869423866272, 0.9741827845573425]], "normalized_score": [0.2058658003807068], "tokens": [["\u0120Terry", "\u0120Brad", "shaw"]], "entropy": [[6.3338470458984375, 1.2359874248504639, 0.18918903172016144]], "avg_entropy": [2.5863411674896875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Denver Broncos", "score_of_biggest_cluster": 0.2058658003807068, "score_of_first_answer": 0.2058658003807068, "max_score_of_first_answer": 0.9741827845573425, "min_score_of_first_answer": 0.01070152036845684, "avg_score": 0.2058658003807068, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4255360862456936, "std_of_first_answer": 0.4255360862456936, "avg_entropy_of_first_answer": 2.5863411674896875, "lex_sim": 1.0}
{"question": "state the position of india in terms of population size", "answer": ["2nd"], "scores": [[0.0714176595211029, 0.8876696825027466]], "normalized_score": [0.25178420543670654], "tokens": [["\u01202", "nd"]], "entropy": [[4.9272541999816895, 0.551112711429596]], "avg_entropy": [2.7391834557056427], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "second", "score_of_biggest_cluster": 0.25178420543670654, "score_of_first_answer": 0.25178420543670654, "max_score_of_first_answer": 0.8876696825027466, "min_score_of_first_answer": 0.0714176595211029, "avg_score": 0.25178420543670654, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.40812601149082184, "std_of_first_answer": 0.40812601149082184, "avg_entropy_of_first_answer": 2.7391834557056427, "lex_sim": 1.0}
{"question": "when was the last time dallas cowboys won the super bowl", "answer": ["2011"], "scores": [[0.037650205194950104]], "normalized_score": [0.0376502089202404], "tokens": [["\u01202011"]], "entropy": [[4.706521511077881]], "avg_entropy": [4.706521511077881], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1995", "score_of_biggest_cluster": 0.0376502089202404, "score_of_first_answer": 0.0376502089202404, "max_score_of_first_answer": 0.037650205194950104, "min_score_of_first_answer": 0.037650205194950104, "avg_score": 0.0376502089202404, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.706521511077881, "lex_sim": 1.0}
{"question": "what domain has more individuals than all other domains combined do", "answer": ["Bacteria"], "scores": [[0.01332967821508646, 0.6035444736480713]], "normalized_score": [0.08969420939683914], "tokens": [["\u0120B", "acteria"]], "entropy": [[6.847383499145508, 2.539621114730835]], "avg_entropy": [4.693502306938171], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the com TLD", "score_of_biggest_cluster": 0.08969420939683914, "score_of_first_answer": 0.08969420939683914, "max_score_of_first_answer": 0.6035444736480713, "min_score_of_first_answer": 0.01332967821508646, "avg_score": 0.08969420939683914, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2951073977164924, "std_of_first_answer": 0.2951073977164924, "avg_entropy_of_first_answer": 4.693502306938171, "lex_sim": 1.0}
{"question": "who won the 1st battle of bull run", "answer": ["T.J. Simpson"], "scores": [[0.0080585777759552, 0.13943344354629517, 0.31643468141555786, 0.8681538105010986, 0.030914362519979477]], "normalized_score": [0.09906795620918274], "tokens": [["\u0120T", ".", "J", ".", "\u0120Simpson"]], "entropy": [[6.795132637023926, 4.372138023376465, 4.082080841064453, 1.1593356132507324, 5.575424671173096]], "avg_entropy": [4.396822357177735], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Confederate victory", "score_of_biggest_cluster": 0.09906795620918274, "score_of_first_answer": 0.09906795620918274, "max_score_of_first_answer": 0.8681538105010986, "min_score_of_first_answer": 0.0080585777759552, "avg_score": 0.09906795620918274, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3170909218776572, "std_of_first_answer": 0.3170909218776572, "avg_entropy_of_first_answer": 4.396822357177735, "lex_sim": 1.0}
{"question": "how many points are scored for a touchdown in american football", "answer": ["6 points"], "scores": [[0.05978315696120262, 0.03093363530933857]], "normalized_score": [0.04300360754132271], "tokens": [["\u01206", "\u0120points"]], "entropy": [[3.6657662391662598, 0.44290417432785034]], "avg_entropy": [2.054335206747055], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "six", "score_of_biggest_cluster": 0.04300360754132271, "score_of_first_answer": 0.04300360754132271, "max_score_of_first_answer": 0.05978315696120262, "min_score_of_first_answer": 0.03093363530933857, "avg_score": 0.04300360754132271, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.014424760825932026, "std_of_first_answer": 0.014424760825932026, "avg_entropy_of_first_answer": 2.054335206747055, "lex_sim": 1.0}
{"question": "what is the full form of cfc's", "answer": ["CFCs"], "scores": [[0.013642747886478901, 0.3042534291744232, 0.09878213703632355]], "normalized_score": [0.0742914229631424], "tokens": [["\u0120C", "FC", "s"]], "entropy": [[6.446874618530273, 4.387202262878418, 3.31791353225708]], "avg_entropy": [4.717330137888591], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chlorofluorocarbons", "score_of_biggest_cluster": 0.0742914229631424, "score_of_first_answer": 0.0742914229631424, "max_score_of_first_answer": 0.3042534291744232, "min_score_of_first_answer": 0.013642747886478901, "avg_score": 0.0742914229631424, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.12198439588568155, "std_of_first_answer": 0.12198439588568155, "avg_entropy_of_first_answer": 4.717330137888591, "lex_sim": 1.0}
{"question": "fictional character whose favorite song was blueberry hill", "answer": ["blueberry hill hill"], "scores": [[0.014292052946984768, 0.813862681388855, 0.901681125164032, 0.002254525665193796]], "normalized_score": [0.06973303109407425], "tokens": [["\u0120blue", "berry", "\u0120hill", "\u0120hill"]], "entropy": [[7.084329605102539, 1.5085468292236328, 0.886257529258728, 1.6679739952087402]], "avg_entropy": [2.78677698969841], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Richie Cunningham", "score_of_biggest_cluster": 0.06973303109407425, "score_of_first_answer": 0.06973303109407425, "max_score_of_first_answer": 0.901681125164032, "min_score_of_first_answer": 0.002254525665193796, "avg_score": 0.06973303109407425, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42590385819531784, "std_of_first_answer": 0.42590385819531784, "avg_entropy_of_first_answer": 2.78677698969841, "lex_sim": 1.0}
{"question": "sequel to a christmas story it runs in the family", "answer": ["it's in the blood!"], "scores": [[0.044058267027139664, 0.2915588915348053, 0.006035590078681707, 0.6711422801017761, 0.012191366404294968, 0.00040688426815904677]], "normalized_score": [0.025232981890439987], "tokens": [["\u0120it", "'s", "\u0120in", "\u0120the", "\u0120blood", "!!"]], "entropy": [[4.861686706542969, 2.8366425037384033, 2.8518452644348145, 2.227508544921875, 4.485368251800537, 0.828240156173706]], "avg_entropy": [3.0152152379353843], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "My Summer Story", "score_of_biggest_cluster": 0.025232981890439987, "score_of_first_answer": 0.025232981890439987, "max_score_of_first_answer": 0.6711422801017761, "min_score_of_first_answer": 0.00040688426815904677, "avg_score": 0.025232981890439987, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2457391363052609, "std_of_first_answer": 0.2457391363052609, "avg_entropy_of_first_answer": 3.0152152379353843, "lex_sim": 1.0}
{"question": "who ordered the forbidden city to be built", "answer": ["shambala"], "scores": [[0.004427088890224695, 0.0188671313226223, 0.2956850528717041]], "normalized_score": [0.029121778905391693], "tokens": [["\u0120sh", "amb", "ala"]], "entropy": [[6.189105987548828, 3.7286601066589355, 2.401888608932495]], "avg_entropy": [4.10655156771342], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Zhu Di", "score_of_biggest_cluster": 0.029121778905391693, "score_of_first_answer": 0.029121778905391693, "max_score_of_first_answer": 0.2956850528717041, "min_score_of_first_answer": 0.004427088890224695, "avg_score": 0.029121778905391693, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13402648084068222, "std_of_first_answer": 0.13402648084068222, "avg_entropy_of_first_answer": 4.10655156771342, "lex_sim": 1.0}
{"question": "who has the most olympic medals in figure skating", "answer": ["Aliona Savchenko [Ukrainian: \u0410\u043b\u044c\u0451\u043d\u0430 \u0429\u0435\u0440\u0431\u0438\u043d\u044f [Aliona Hyrbina]] (born 19 December 1984) is a Ukrainian triple-triple world champion figure"], "scores": [[0.023630432784557343, 0.5350924730300903, 0.7471462488174438, 0.8927803635597229, 0.0014677783474326134, 0.012326069176197052, 0.6301557421684265, 0.9872158169746399, 0.030377991497516632, 0.840861976146698, 0.9079616069793701, 0.954095184803009, 0.08555503934621811, 0.5950608253479004, 0.24284382164478302, 0.9316822290420532, 0.9719365239143372, 0.965497612953186, 0.0017563399160280824, 0.7383466958999634, 0.5246663689613342, 0.8574441075325012, 0.9856621623039246, 0.08825693279504776, 0.6719363927841187, 0.034614551812410355, 0.024265801534056664, 0.11794792115688324, 0.5665429830551147, 0.15172156691551208, 0.03308369964361191, 0.6746181845664978, 0.2357068508863449, 0.5628474354743958, 0.5948933362960815, 0.851965069770813, 0.016918612644076347, 0.06388745456933975, 0.06384797394275665, 0.7229197025299072, 0.9546264410018921, 0.8959253430366516, 0.9122348427772522, 0.0016896211309358478, 0.126874178647995, 0.13354206085205078, 0.9951456189155579, 0.015162358060479164, 0.8668094873428345, 0.3878970444202423]], "normalized_score": [0.20651626586914062], "tokens": [["\u0120Al", "iona", "\u0120Sav", "chenko", "\u0120[", "Uk", "rain", "ian", ":", "\u0120\u00d0", "\u0132", "\u00d0\u00bb", "\u00d1\u012e", "\u00d1", "\u0133", "\u00d0\u00bd", "\u00d0\u00b0", "\u0120\u00d0", "\u00a9", "\u00d0\u00b5", "\u00d1\u0122", "\u00d0", "\u00b1", "\u00d0\u00b8", "\u00d0\u00bd", "\u00d1\u0131", "\u0120[", "Al", "iona", "\u0120H", "yr", "b", "ina", "]]", "\u0120(", "born", "\u012019", "\u0120December", "\u01201984", ")", "\u0120is", "\u0120a", "\u0120Ukrainian", "\u0120triple", "-", "tri", "ple", "\u0120world", "\u0120champion", "\u0120figure"]], "entropy": [[5.297933578491211, 1.627589464187622, 1.6712877750396729, 0.6791963577270508, 1.7126916646957397, 6.458096504211426, 1.3196427822113037, 0.11464873701334, 3.7957301139831543, 0.9798468351364136, 0.5716639161109924, 0.29201996326446533, 0.9545198678970337, 1.1766529083251953, 1.6742554903030396, 0.37197235226631165, 0.19833555817604065, 0.2575643062591553, 0.9992803335189819, 1.1789220571517944, 1.527510643005371, 0.7757358551025391, 0.08969945460557938, 1.5508133172988892, 1.1362757682800293, 1.018390417098999, 2.853386402130127, 4.289159774780273, 2.0737037658691406, 4.034461975097656, 3.8308510780334473, 1.4190715551376343, 3.377352237701416, 1.4098293781280518, 1.803902506828308, 1.0309441089630127, 3.836822271347046, 2.5441975593566895, 2.8589372634887695, 0.8466876745223999, 0.33590543270111084, 0.3917733132839203, 0.5738120079040527, 2.9926838874816895, 2.4389071464538574, 2.3673954010009766, 0.041232749819755554, 3.370616912841797, 0.7843205332756042, 2.3634040355682373]], "avg_entropy": [1.7859926998615265], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Scott Moir", "score_of_biggest_cluster": 0.20651626586914062, "score_of_first_answer": 0.20651626586914062, "max_score_of_first_answer": 0.9951456189155579, "min_score_of_first_answer": 0.0014677783474326134, "avg_score": 0.20651626586914062, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3778518492513106, "std_of_first_answer": 0.3778518492513106, "avg_entropy_of_first_answer": 1.7859926998615265, "lex_sim": 1.0}
{"question": "what is the name of the protease which is released in the stomach", "answer": ["PEP"], "scores": [[0.0030775635968893766, 0.2599181830883026]], "normalized_score": [0.028282765299081802], "tokens": [["\u0120P", "EP"]], "entropy": [[2.4098453521728516, 3.0124893188476562]], "avg_entropy": [2.711167335510254], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pepsin", "score_of_biggest_cluster": 0.028282765299081802, "score_of_first_answer": 0.028282765299081802, "max_score_of_first_answer": 0.2599181830883026, "min_score_of_first_answer": 0.0030775635968893766, "avg_score": 0.028282765299081802, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12842030974570662, "std_of_first_answer": 0.12842030974570662, "avg_entropy_of_first_answer": 2.711167335510254, "lex_sim": 1.0}
{"question": "who plays david in alvin and the chipmunks", "answer": ["David Cross"], "scores": [[0.018925076350569725, 0.48111701011657715]], "normalized_score": [0.09542105346918106], "tokens": [["\u0120David", "\u0120Cross"]], "entropy": [[4.706121444702148, 3.3308544158935547]], "avg_entropy": [4.018487930297852], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jason Lee", "score_of_biggest_cluster": 0.09542105346918106, "score_of_first_answer": 0.09542105346918106, "max_score_of_first_answer": 0.48111701011657715, "min_score_of_first_answer": 0.018925076350569725, "avg_score": 0.09542105346918106, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2310959668830037, "std_of_first_answer": 0.2310959668830037, "avg_entropy_of_first_answer": 4.018487930297852, "lex_sim": 1.0}
{"question": "who sings the original windmills of your mind", "answer": ["Alanis Morissette"], "scores": [[0.010373475030064583, 0.6267528533935547, 0.5834091305732727, 0.8120899200439453, 0.9943411350250244]], "normalized_score": [0.314214825630188], "tokens": [["\u0120Alan", "is", "\u0120Mor", "iss", "ette"]], "entropy": [[6.610523700714111, 2.3175973892211914, 1.2734827995300293, 0.7475045919418335, 0.04595690965652466]], "avg_entropy": [2.199013078212738], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Noel Harrison", "score_of_biggest_cluster": 0.314214825630188, "score_of_first_answer": 0.314214825630188, "max_score_of_first_answer": 0.9943411350250244, "min_score_of_first_answer": 0.010373475030064583, "avg_score": 0.314214825630188, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33135842751982403, "std_of_first_answer": 0.33135842751982403, "avg_entropy_of_first_answer": 2.199013078212738, "lex_sim": 1.0}
{"question": "who was the first indian to be appointed as a judge in the international court of justice", "answer": ["Mahipal Singh Basoli"], "scores": [[0.014559033326804638, 0.006068950518965721, 0.1801227480173111, 0.0047322651371359825, 0.07517856359481812]], "normalized_score": [0.022417934611439705], "tokens": [["\u0120Mah", "ipal", "\u0120Singh", "\u0120Bas", "oli"]], "entropy": [[5.967740535736084, 2.280147075653076, 3.9806647300720215, 3.808088541030884, 3.5043678283691406]], "avg_entropy": [3.908201742172241], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sir Benegal Narsing Rau", "score_of_biggest_cluster": 0.022417934611439705, "score_of_first_answer": 0.022417934611439705, "max_score_of_first_answer": 0.1801227480173111, "min_score_of_first_answer": 0.0047322651371359825, "avg_score": 0.022417934611439705, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06725032401929454, "std_of_first_answer": 0.06725032401929454, "avg_entropy_of_first_answer": 3.908201742172241, "lex_sim": 1.0}
{"question": "when did italy enter the second world war", "answer": ["June 10, 1943"], "scores": [[0.04241204261779785, 0.39447084069252014, 0.3119643032550812, 0.31626278162002563]], "normalized_score": [0.20156462490558624], "tokens": [["\u0120June", "\u012010", ",", "\u01201943"]], "entropy": [[3.6427905559539795, 2.786999225616455, 1.1077393293380737, 1.570040225982666]], "avg_entropy": [2.2768923342227936], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "10 June 1940", "score_of_biggest_cluster": 0.20156462490558624, "score_of_first_answer": 0.20156462490558624, "max_score_of_first_answer": 0.39447084069252014, "min_score_of_first_answer": 0.04241204261779785, "avg_score": 0.20156462490558624, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13335580975078723, "std_of_first_answer": 0.13335580975078723, "avg_entropy_of_first_answer": 2.2768923342227936, "lex_sim": 1.0}
{"question": "the names of the beaches on d day", "answer": ["london, louisburg bay, florida bay, manhattan, hudson, la bre"], "scores": [[0.004773251246660948, 0.01809159480035305, 0.38662973046302795, 0.01173351053148508, 0.8204392194747925, 0.024197127670049667, 0.01134472992271185, 0.7877228856086731, 0.016984524205327034, 0.8025252819061279, 0.9847598671913147, 0.03709425777196884, 0.599392831325531, 0.01004218589514494, 0.7271087169647217, 0.3642679750919342, 0.009792093187570572, 0.1854093223810196, 0.9986013770103455, 0.19409798085689545, 0.0067782108671963215, 0.03745782747864723]], "normalized_score": [0.0872129425406456], "tokens": [["\u0120l", "ondon", ",", "\u0120lou", "is", "burg", "\u0120bay", ",", "\u0120fl", "or", "ida", "\u0120bay", ",", "\u0120man", "hattan", ",", "\u0120h", "ud", "son", ",", "\u0120la", "\u0120bre"]], "entropy": [[6.632180213928223, 5.1040544509887695, 3.154487371444702, 5.472297191619873, 1.1029531955718994, 1.871692419052124, 0.6601951122283936, 1.0249332189559937, 6.21405029296875, 1.2331103086471558, 0.12337925285100937, 2.228304386138916, 1.1297845840454102, 6.068216323852539, 1.3820867538452148, 2.082421064376831, 6.071272373199463, 2.588933229446411, 0.016648687422275543, 1.658919334411621, 6.179986000061035, 4.568490505218506]], "avg_entropy": [3.025836194103414], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Juno Beach", "score_of_biggest_cluster": 0.0872129425406456, "score_of_first_answer": 0.0872129425406456, "max_score_of_first_answer": 0.9986013770103455, "min_score_of_first_answer": 0.004773251246660948, "avg_score": 0.0872129425406456, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36300577735759115, "std_of_first_answer": 0.36300577735759115, "avg_entropy_of_first_answer": 3.025836194103414, "lex_sim": 1.0}
{"question": "what percent of the us population controls the wealth", "answer": ["33.1%"], "scores": [[0.009750345721840858, 0.14870622754096985, 0.026834329590201378, 0.5824015140533447]], "normalized_score": [0.0689946860074997], "tokens": [["\u012033", ".", "1", "%"]], "entropy": [[4.793797492980957, 1.3072476387023926, 2.664196014404297, 1.0137183666229248]], "avg_entropy": [2.444739878177643], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1%", "score_of_biggest_cluster": 0.0689946860074997, "score_of_first_answer": 0.0689946860074997, "max_score_of_first_answer": 0.5824015140533447, "min_score_of_first_answer": 0.009750345721840858, "avg_score": 0.0689946860074997, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23172305907238983, "std_of_first_answer": 0.23172305907238983, "avg_entropy_of_first_answer": 2.444739878177643, "lex_sim": 1.0}
{"question": "who plays sheila carter on the bold and the beautiful", "answer": ["Catherine Zeta Jones"], "scores": [[0.0033676933962851763, 0.3131328821182251, 0.9982522130012512, 0.7277966737747192]], "normalized_score": [0.16637109220027924], "tokens": [["\u0120Catherine", "\u0120Z", "eta", "\u0120Jones"]], "entropy": [[6.4210357666015625, 1.9735209941864014, 0.02122313529253006, 0.8859999179840088]], "avg_entropy": [2.3254449535161257], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kimberlin Brown", "score_of_biggest_cluster": 0.16637109220027924, "score_of_first_answer": 0.16637109220027924, "max_score_of_first_answer": 0.9982522130012512, "min_score_of_first_answer": 0.0033676933962851763, "avg_score": 0.16637109220027924, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38120104468185, "std_of_first_answer": 0.38120104468185, "avg_entropy_of_first_answer": 2.3254449535161257, "lex_sim": 1.0}
{"question": "describe the three phases of the normal blood clotting process", "answer": ["primary, secondary and tertiary"], "scores": [[0.020095448940992355, 0.6254702806472778, 0.9323875904083252, 0.2489510178565979, 0.4873141348361969, 0.9825718402862549]], "normalized_score": [0.3343474268913269], "tokens": [["\u0120primary", ",", "\u0120secondary", "\u0120and", "\u0120tert", "iary"]], "entropy": [[5.566793441772461, 1.6958847045898438, 0.5751144886016846, 0.6531412601470947, 3.149385452270508, 0.14839638769626617]], "avg_entropy": [1.9647859558463097], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "blood coagulation", "score_of_biggest_cluster": 0.3343474268913269, "score_of_first_answer": 0.3343474268913269, "max_score_of_first_answer": 0.9825718402862549, "min_score_of_first_answer": 0.020095448940992355, "avg_score": 0.3343474268913269, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34505834123297907, "std_of_first_answer": 0.34505834123297907, "avg_entropy_of_first_answer": 1.9647859558463097, "lex_sim": 1.0}
{"question": "who played ashley on the young and the restless", "answer": ["Ashley Greene"], "scores": [[0.5867109894752502, 0.06106340140104294]], "normalized_score": [0.18927907943725586], "tokens": [["\u0120Ashley", "\u0120Greene"]], "entropy": [[3.224709987640381, 3.6103529930114746]], "avg_entropy": [3.4175314903259277], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brenda Epperson", "score_of_biggest_cluster": 0.18927907943725586, "score_of_first_answer": 0.18927907943725586, "max_score_of_first_answer": 0.5867109894752502, "min_score_of_first_answer": 0.06106340140104294, "avg_score": 0.18927907943725586, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26282379403710365, "std_of_first_answer": 0.26282379403710365, "avg_entropy_of_first_answer": 3.4175314903259277, "lex_sim": 1.0}
{"question": "who created the pieta and also painted the ceiling of the sistine chapel", "answer": ["Paolo di franco scozzo"], "scores": [[0.0023502095136791468, 0.9447016716003418, 0.2501387596130371, 0.021622978150844574, 0.08249250799417496, 0.020777495577931404, 0.006517910398542881, 0.09500307589769363]], "normalized_score": [0.0434679351747036], "tokens": [["\u0120Pa", "olo", "\u0120di", "\u0120fr", "anco", "\u0120sc", "o", "zzo"]], "entropy": [[2.322375535964966, 0.33939284086227417, 3.711148738861084, 5.50128698348999, 1.1881937980651855, 2.6210312843322754, 3.4391162395477295, 2.4128079414367676]], "avg_entropy": [2.691919170320034], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Michelangelo", "score_of_biggest_cluster": 0.0434679351747036, "score_of_first_answer": 0.0434679351747036, "max_score_of_first_answer": 0.9447016716003418, "min_score_of_first_answer": 0.0023502095136791468, "avg_score": 0.0434679351747036, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2996825924170232, "std_of_first_answer": 0.2996825924170232, "avg_entropy_of_first_answer": 2.691919170320034, "lex_sim": 1.0}
{"question": "when does the new season of lost in space come out", "answer": ["November 10"], "scores": [[0.03529397025704384, 0.029520710930228233]], "normalized_score": [0.03227852284908295], "tokens": [["\u0120November", "\u012010"]], "entropy": [[4.297215938568115, 3.5267701148986816]], "avg_entropy": [3.9119930267333984], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "April 13, 2018", "score_of_biggest_cluster": 0.03227852284908295, "score_of_first_answer": 0.03227852284908295, "max_score_of_first_answer": 0.03529397025704384, "min_score_of_first_answer": 0.029520710930228233, "avg_score": 0.03227852284908295, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0028866296634078026, "std_of_first_answer": 0.0028866296634078026, "avg_entropy_of_first_answer": 3.9119930267333984, "lex_sim": 1.0}
{"question": "who sings the christmas song mary did you know", "answer": ["christmas did you know"], "scores": [[0.0042615048587322235, 0.3847343325614929, 0.07903555780649185, 0.9371991157531738, 0.9767535924911499]], "normalized_score": [0.16399578750133514], "tokens": [["\u0120christ", "mas", "\u0120did", "\u0120you", "\u0120know"]], "entropy": [[5.695043563842773, 1.7002731561660767, 3.6141350269317627, 0.46471554040908813, 0.2130947709083557]], "avg_entropy": [2.3374524116516113], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Michael English", "score_of_biggest_cluster": 0.16399578750133514, "score_of_first_answer": 0.16399578750133514, "max_score_of_first_answer": 0.9767535924911499, "min_score_of_first_answer": 0.0042615048587322235, "avg_score": 0.16399578750133514, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4127724301956545, "std_of_first_answer": 0.4127724301956545, "avg_entropy_of_first_answer": 2.3374524116516113, "lex_sim": 1.0}
{"question": "who are nominated for president of india 2017", "answer": ["Amitabh Bachan and Narendra Modi"], "scores": [[0.020085826516151428, 0.07784926891326904, 0.9815589785575867, 0.7753074765205383, 0.03407549485564232, 0.3690205514431, 0.2313496619462967, 0.9658687710762024]], "normalized_score": [0.20678867399692535], "tokens": [["\u0120Amit", "ab", "h", "\u0120Bach", "an", "\u0120and", "\u0120Narendra", "\u0120Modi"]], "entropy": [[5.049543380737305, 2.0999178886413574, 0.11450724303722382, 1.2171123027801514, 0.9760367274284363, 1.4361602067947388, 4.244194030761719, 0.24535390734672546]], "avg_entropy": [1.922853210940957], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Meira Kumar", "score_of_biggest_cluster": 0.20678867399692535, "score_of_first_answer": 0.20678867399692535, "max_score_of_first_answer": 0.9815589785575867, "min_score_of_first_answer": 0.020085826516151428, "avg_score": 0.20678867399692535, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.387710873097675, "std_of_first_answer": 0.387710873097675, "avg_entropy_of_first_answer": 1.922853210940957, "lex_sim": 1.0}
{"question": "who sang the them song for as told by ginger", "answer": ["The Beatles"], "scores": [[0.07433333992958069, 0.08924737572669983]], "normalized_score": [0.08144970238208771], "tokens": [["\u0120The", "\u0120Beatles"]], "entropy": [[6.451057434082031, 5.217380523681641]], "avg_entropy": [5.834218978881836], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Melissa Disney", "score_of_biggest_cluster": 0.08144970238208771, "score_of_first_answer": 0.08144970238208771, "max_score_of_first_answer": 0.08924737572669983, "min_score_of_first_answer": 0.07433333992958069, "avg_score": 0.08144970238208771, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.00745701789855957, "std_of_first_answer": 0.00745701789855957, "avg_entropy_of_first_answer": 5.834218978881836, "lex_sim": 1.0}
{"question": "how much of the world's diamonds does de beers own", "answer": ["about 10% of the world's diamond production."], "scores": [[0.040375951677560806, 0.038134071975946426, 0.5610306859016418, 0.028919924050569534, 0.5842834711074829, 0.7332479953765869, 0.9063921570777893, 0.05640305578708649, 0.27342188358306885, 0.02822403982281685]], "normalized_score": [0.1453973650932312], "tokens": [["\u0120about", "\u012010", "%", "\u0120of", "\u0120the", "\u0120world", "'s", "\u0120diamond", "\u0120production", "."]], "entropy": [[5.639442443847656, 4.421088218688965, 1.9257161617279053, 0.3548503816127777, 1.850248098373413, 1.4803684949874878, 0.5955320596694946, 2.107936382293701, 2.22320818901062, 0.7330760955810547]], "avg_entropy": [2.1331466525793075], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "33% in 2013", "score_of_biggest_cluster": 0.1453973650932312, "score_of_first_answer": 0.1453973650932312, "max_score_of_first_answer": 0.9063921570777893, "min_score_of_first_answer": 0.02822403982281685, "avg_score": 0.1453973650932312, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3227179205342392, "std_of_first_answer": 0.3227179205342392, "avg_entropy_of_first_answer": 2.1331466525793075, "lex_sim": 1.0}
{"question": "what type of car is a jeep considered", "answer": ["A4-1500"], "scores": [[0.010438136756420135, 0.042891085147857666, 0.00962051097303629, 0.0064824349246919155]], "normalized_score": [0.012926517054438591], "tokens": [["\u0120A", "4", "-", "1500"]], "entropy": [[4.613589286804199, 4.653974533081055, 3.148517608642578, 5.61643648147583]], "avg_entropy": [4.5081294775009155], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "off-road vehicles", "score_of_biggest_cluster": 0.012926517054438591, "score_of_first_answer": 0.012926517054438591, "max_score_of_first_answer": 0.042891085147857666, "min_score_of_first_answer": 0.0064824349246919155, "avg_score": 0.012926517054438591, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.014815275871716013, "std_of_first_answer": 0.014815275871716013, "avg_entropy_of_first_answer": 4.5081294775009155, "lex_sim": 1.0}
{"question": "who played the jewish man in coming to america", "answer": ["Al Shearing"], "scores": [[0.007470767013728619, 0.015242026187479496, 0.0037918128073215485]], "normalized_score": [0.00755819957703352], "tokens": [["\u0120Al", "\u0120She", "aring"]], "entropy": [[5.797389507293701, 3.7829413414001465, 3.306428909301758]], "avg_entropy": [4.295586585998535], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eddie Murphy", "score_of_biggest_cluster": 0.00755819957703352, "score_of_first_answer": 0.00755819957703352, "max_score_of_first_answer": 0.015242026187479496, "min_score_of_first_answer": 0.0037918128073215485, "avg_score": 0.00755819957703352, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.004773009295910071, "std_of_first_answer": 0.004773009295910071, "avg_entropy_of_first_answer": 4.295586585998535, "lex_sim": 1.0}
{"question": "atlantic ocean's shape is similar to which english alphabet", "answer": ["the ae"], "scores": [[0.023113515228033066, 0.06792012602090836, 0.17688320577144623]], "normalized_score": [0.06524047255516052], "tokens": [["\u0120the", "\u0120a", "e"]], "entropy": [[5.051987648010254, 4.965070724487305, 3.43210506439209]], "avg_entropy": [4.483054478963216], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "S", "score_of_biggest_cluster": 0.06524047255516052, "score_of_first_answer": 0.06524047255516052, "max_score_of_first_answer": 0.17688320577144623, "min_score_of_first_answer": 0.023113515228033066, "avg_score": 0.06524047255516052, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06457184013801417, "std_of_first_answer": 0.06457184013801417, "avg_entropy_of_first_answer": 4.483054478963216, "lex_sim": 1.0}
{"question": "when did sussex split into east and west", "answer": ["2005"], "scores": [[0.0032586536835879087]], "normalized_score": [0.003258652985095978], "tokens": [["\u01202005"]], "entropy": [[5.641320705413818]], "avg_entropy": [5.641320705413818], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1832", "score_of_biggest_cluster": 0.003258652985095978, "score_of_first_answer": 0.003258652985095978, "max_score_of_first_answer": 0.0032586536835879087, "min_score_of_first_answer": 0.0032586536835879087, "avg_score": 0.003258652985095978, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.641320705413818, "lex_sim": 1.0}
{"question": "who wrote he ain't heavy he's my brother lyrics", "answer": ["Elton John"], "scores": [[0.0369550995528698, 0.9524584412574768, 0.9038017392158508]], "normalized_score": [0.3168579041957855], "tokens": [["\u0120El", "ton", "\u0120John"]], "entropy": [[5.762036323547363, 0.42352330684661865, 0.6160107851028442]], "avg_entropy": [2.267190138498942], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bobby Scott", "score_of_biggest_cluster": 0.3168579041957855, "score_of_first_answer": 0.3168579041957855, "max_score_of_first_answer": 0.9524584412574768, "min_score_of_first_answer": 0.0369550995528698, "avg_score": 0.3168579041957855, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4205732780676173, "std_of_first_answer": 0.4205732780676173, "avg_entropy_of_first_answer": 2.267190138498942, "lex_sim": 1.0}
{"question": "who is the real killer in basic instinct 2", "answer": ["Leon Isaac Kennedy"], "scores": [[0.009965960867702961, 0.007147577591240406, 0.8452340364456177]], "normalized_score": [0.039193879812955856], "tokens": [["\u0120Leon", "\u0120Isaac", "\u0120Kennedy"]], "entropy": [[6.085865020751953, 5.209057807922363, 1.1905171871185303]], "avg_entropy": [4.161813338597615], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Catherine Tramell", "score_of_biggest_cluster": 0.039193879812955856, "score_of_first_answer": 0.039193879812955856, "max_score_of_first_answer": 0.8452340364456177, "min_score_of_first_answer": 0.007147577591240406, "avg_score": 0.039193879812955856, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.394415124495752, "std_of_first_answer": 0.394415124495752, "avg_entropy_of_first_answer": 4.161813338597615, "lex_sim": 1.0}
{"question": "which british city is farthest from the sea", "answer": ["Southampton"], "scores": [[0.02027730643749237]], "normalized_score": [0.02027730643749237], "tokens": [["\u0120Southampton"]], "entropy": [[4.996831893920898]], "avg_entropy": [4.996831893920898], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Coton in the Elms", "score_of_biggest_cluster": 0.02027730643749237, "score_of_first_answer": 0.02027730643749237, "max_score_of_first_answer": 0.02027730643749237, "min_score_of_first_answer": 0.02027730643749237, "avg_score": 0.02027730643749237, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.996831893920898, "lex_sim": 1.0}
{"question": "who were the three elves who got rings", "answer": ["Goldie Hawn, Farrah Fawcett, and Sandra Dee"], "scores": [[0.007678537629544735, 0.3375805616378784, 0.11795372515916824, 0.9962618947029114, 0.7626844644546509, 0.005476617719978094, 0.9959304928779602, 0.9231087565422058, 0.9886095523834229, 0.9600405693054199, 0.6326239109039307, 0.6461971998214722, 0.013655198737978935, 0.5063462853431702]], "normalized_score": [0.2469550520181656], "tokens": [["\u0120Gold", "ie", "\u0120H", "awn", ",", "\u0120Far", "rah", "\u0120Faw", "c", "ett", ",", "\u0120and", "\u0120Sandra", "\u0120Dee"]], "entropy": [[6.668857574462891, 2.8982391357421875, 2.4968061447143555, 0.04012319818139076, 1.1423680782318115, 5.304998397827148, 0.03487355262041092, 0.6150370836257935, 0.1054692342877388, 0.22552552819252014, 0.8756375312805176, 2.6039347648620605, 5.441435813903809, 1.5011520385742188]], "avg_entropy": [2.139604148321918], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "C\u00edrdan", "score_of_biggest_cluster": 0.2469550520181656, "score_of_first_answer": 0.2469550520181656, "max_score_of_first_answer": 0.9962618947029114, "min_score_of_first_answer": 0.005476617719978094, "avg_score": 0.2469550520181656, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38436627086472497, "std_of_first_answer": 0.38436627086472497, "avg_entropy_of_first_answer": 2.139604148321918, "lex_sim": 1.0}
{"question": "how many terms can a mayor serve in texas", "answer": ["2"], "scores": [[0.5262186527252197]], "normalized_score": [0.5262186527252197], "tokens": [["\u01202"]], "entropy": [[1.5446902513504028]], "avg_entropy": [1.5446902513504028], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "two four-year terms", "score_of_biggest_cluster": 0.5262186527252197, "score_of_first_answer": 0.5262186527252197, "max_score_of_first_answer": 0.5262186527252197, "min_score_of_first_answer": 0.5262186527252197, "avg_score": 0.5262186527252197, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.5446902513504028, "lex_sim": 1.0}
{"question": "what is bermuda competing in the winter olympics", "answer": ["nordic combined"], "scores": [[0.004981720354408026, 0.21988704800605774, 0.9934213757514954, 0.1606391966342926]], "normalized_score": [0.11498494446277618], "tokens": [["\u0120n", "ord", "ic", "\u0120combined"]], "entropy": [[4.921957015991211, 3.8836231231689453, 0.05273183435201645, 2.1533501148223877]], "avg_entropy": [2.75291552208364], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cross-country skiing", "score_of_biggest_cluster": 0.11498494446277618, "score_of_first_answer": 0.11498494446277618, "max_score_of_first_answer": 0.9934213757514954, "min_score_of_first_answer": 0.004981720354408026, "avg_score": 0.11498494446277618, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38265669372855876, "std_of_first_answer": 0.38265669372855876, "avg_entropy_of_first_answer": 2.75291552208364, "lex_sim": 1.0}
{"question": "who wrote the song what child is this", "answer": ["William Blake"], "scores": [[0.024028120562434196, 0.0018661305075511336]], "normalized_score": [0.006696238182485104], "tokens": [["\u0120William", "\u0120Blake"]], "entropy": [[6.611500263214111, 0.49659788608551025]], "avg_entropy": [3.554049074649811], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "William Chatterton Dix", "score_of_biggest_cluster": 0.006696238182485104, "score_of_first_answer": 0.006696238182485104, "max_score_of_first_answer": 0.024028120562434196, "min_score_of_first_answer": 0.0018661305075511336, "avg_score": 0.006696238182485104, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.011080995027441531, "std_of_first_answer": 0.011080995027441531, "avg_entropy_of_first_answer": 3.554049074649811, "lex_sim": 1.0}
{"question": "where is creatine phosphate found in the body", "answer": ["in the muscle of body tissues - Creatine Kinase"], "scores": [[0.07561781257390976, 0.5105320811271667, 0.28751492500305176, 0.0026630638167262077, 0.001979158027097583, 0.015728367492556572, 0.00027814641362056136, 0.014292680658400059, 0.9841510057449341, 0.020635290071368217, 0.9928638935089111]], "normalized_score": [0.034153614193201065], "tokens": [["\u0120in", "\u0120the", "\u0120muscle", "\u0120of", "\u0120body", "\u0120tissues", "\u0120-", "\u0120Creat", "ine", "\u0120Kin", "ase"]], "entropy": [[4.285263538360596, 2.1495230197906494, 2.967437982559204, 1.7355742454528809, 1.401303768157959, 1.77981698513031, 0.8215961456298828, 5.907349109649658, 0.10543061792850494, 2.8948631286621094, 0.06408863514661789]], "avg_entropy": [2.192022470588034], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "brain", "score_of_biggest_cluster": 0.034153614193201065, "score_of_first_answer": 0.034153614193201065, "max_score_of_first_answer": 0.9928638935089111, "min_score_of_first_answer": 0.00027814641362056136, "avg_score": 0.034153614193201065, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3737279654215746, "std_of_first_answer": 0.3737279654215746, "avg_entropy_of_first_answer": 2.192022470588034, "lex_sim": 1.0}
{"question": "where are the highest average incomes found in north america", "answer": ["Washington DC"], "scores": [[0.057283997535705566, 0.2469121664762497]], "normalized_score": [0.11892902851104736], "tokens": [["\u0120Washington", "\u0120DC"]], "entropy": [[4.989322185516357, 2.088656425476074]], "avg_entropy": [3.538989305496216], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Virginia", "score_of_biggest_cluster": 0.11892902851104736, "score_of_first_answer": 0.11892902851104736, "max_score_of_first_answer": 0.2469121664762497, "min_score_of_first_answer": 0.057283997535705566, "avg_score": 0.11892902851104736, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09481408447027206, "std_of_first_answer": 0.09481408447027206, "avg_entropy_of_first_answer": 3.538989305496216, "lex_sim": 1.0}
{"question": "when was the last time clemson won the national championship", "answer": ["2011"], "scores": [[0.05570754408836365]], "normalized_score": [0.05570754036307335], "tokens": [["\u01202011"]], "entropy": [[4.879249572753906]], "avg_entropy": [4.879249572753906], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2016", "score_of_biggest_cluster": 0.05570754036307335, "score_of_first_answer": 0.05570754036307335, "max_score_of_first_answer": 0.05570754408836365, "min_score_of_first_answer": 0.05570754408836365, "avg_score": 0.05570754036307335, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.879249572753906, "lex_sim": 1.0}
{"question": "what river flows through charleston west virginia\u2019s capital", "answer": ["gilmore"], "scores": [[0.002487163757905364, 0.09327977895736694, 0.09283918142318726]], "normalized_score": [0.027823245152831078], "tokens": [["\u0120g", "il", "more"]], "entropy": [[4.1031293869018555, 4.859134674072266, 3.9061357975006104]], "avg_entropy": [4.289466619491577], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kanawha River", "score_of_biggest_cluster": 0.027823245152831078, "score_of_first_answer": 0.027823245152831078, "max_score_of_first_answer": 0.09327977895736694, "min_score_of_first_answer": 0.002487163757905364, "avg_score": 0.027823245152831078, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.042696578313505845, "std_of_first_answer": 0.042696578313505845, "avg_entropy_of_first_answer": 4.289466619491577, "lex_sim": 1.0}
{"question": "new mexico was admitted to the union as the . . . state", "answer": ["union"], "scores": [[0.00942330900579691]], "normalized_score": [0.009423309937119484], "tokens": [["\u0120union"]], "entropy": [[5.34865665435791]], "avg_entropy": [5.34865665435791], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "47th", "score_of_biggest_cluster": 0.009423309937119484, "score_of_first_answer": 0.009423309937119484, "max_score_of_first_answer": 0.00942330900579691, "min_score_of_first_answer": 0.00942330900579691, "avg_score": 0.009423309937119484, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.34865665435791, "lex_sim": 1.0}
{"question": "what is the process to become a santero", "answer": ["ask me"], "scores": [[0.014752446673810482, 0.052077747881412506]], "normalized_score": [0.02771775983273983], "tokens": [["\u0120ask", "\u0120me"]], "entropy": [[6.842258453369141, 3.2155609130859375]], "avg_entropy": [5.028909683227539], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "a cleansing ritual", "score_of_biggest_cluster": 0.02771775983273983, "score_of_first_answer": 0.02771775983273983, "max_score_of_first_answer": 0.052077747881412506, "min_score_of_first_answer": 0.014752446673810482, "avg_score": 0.02771775983273983, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.018662650603801012, "std_of_first_answer": 0.018662650603801012, "avg_entropy_of_first_answer": 5.028909683227539, "lex_sim": 1.0}
{"question": "who is the owner of the mandalay bay in vegas", "answer": ["Steve Wynn"], "scores": [[0.0095229372382164, 0.9455390572547913, 0.9979239702224731]], "normalized_score": [0.20789751410484314], "tokens": [["\u0120Steve", "\u0120Wyn", "n"]], "entropy": [[3.7089579105377197, 0.42308831214904785, 0.02432350069284439]], "avg_entropy": [1.3854565744598706], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "MGM Resorts International", "score_of_biggest_cluster": 0.20789751410484314, "score_of_first_answer": 0.20789751410484314, "max_score_of_first_answer": 0.9979239702224731, "min_score_of_first_answer": 0.0095229372382164, "avg_score": 0.20789751410484314, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4540933528645429, "std_of_first_answer": 0.4540933528645429, "avg_entropy_of_first_answer": 1.3854565744598706, "lex_sim": 1.0}
{"question": "what episode does lori die on the walking dead", "answer": ["Season 4 Episode 13"], "scores": [[0.16240672767162323, 0.12189686298370361, 0.3129342496395111, 0.04128304868936539]], "normalized_score": [0.12646064162254333], "tokens": [["\u0120Season", "\u01204", "\u0120Episode", "\u012013"]], "entropy": [[5.078945159912109, 2.530208110809326, 1.7771092653274536, 3.2399742603302]], "avg_entropy": [3.1565591990947723], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\"Killer Within\"", "score_of_biggest_cluster": 0.12646064162254333, "score_of_first_answer": 0.12646064162254333, "max_score_of_first_answer": 0.3129342496395111, "min_score_of_first_answer": 0.04128304868936539, "avg_score": 0.12646064162254333, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09866571553421007, "std_of_first_answer": 0.09866571553421007, "avg_entropy_of_first_answer": 3.1565591990947723, "lex_sim": 1.0}
{"question": "a country having an island location and a country having continental location", "answer": ["Canada"], "scores": [[0.0394229032099247]], "normalized_score": [0.0394228994846344], "tokens": [["\u0120Canada"]], "entropy": [[5.532217979431152]], "avg_entropy": [5.532217979431152], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Australia", "score_of_biggest_cluster": 0.0394228994846344, "score_of_first_answer": 0.0394228994846344, "max_score_of_first_answer": 0.0394229032099247, "min_score_of_first_answer": 0.0394229032099247, "avg_score": 0.0394228994846344, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.532217979431152, "lex_sim": 1.0}
{"question": "what type of reaction occurs to form a dipeptide", "answer": ["condensation"], "scores": [[0.02681591548025608, 0.9397402405738831]], "normalized_score": [0.15874508023262024], "tokens": [["\u0120cond", "ensation"]], "entropy": [[5.834394454956055, 0.3502918481826782]], "avg_entropy": [3.0923431515693665], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "peptide bond", "score_of_biggest_cluster": 0.15874508023262024, "score_of_first_answer": 0.15874508023262024, "max_score_of_first_answer": 0.9397402405738831, "min_score_of_first_answer": 0.02681591548025608, "avg_score": 0.15874508023262024, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4564621625468135, "std_of_first_answer": 0.4564621625468135, "avg_entropy_of_first_answer": 3.0923431515693665, "lex_sim": 1.0}
{"question": "star wars episode ii attack of the clones characters", "answer": ["General Grievous"], "scores": [[0.01426154375076294, 0.7807619571685791, 0.9990856647491455, 0.999087929725647]], "normalized_score": [0.324692964553833], "tokens": [["\u0120General", "\u0120Gri", "ev", "ous"]], "entropy": [[5.962080955505371, 1.500511646270752, 0.010634327307343483, 0.010092725045979023]], "avg_entropy": [1.8708299135323614], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chancellor Palpatine / Darth Sidious", "score_of_biggest_cluster": 0.324692964553833, "score_of_first_answer": 0.324692964553833, "max_score_of_first_answer": 0.999087929725647, "min_score_of_first_answer": 0.01426154375076294, "avg_score": 0.324692964553833, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4048623157299988, "std_of_first_answer": 0.4048623157299988, "avg_entropy_of_first_answer": 1.8708299135323614, "lex_sim": 1.0}
{"question": "what is the ultimate objective of financial management", "answer": ["to make money"], "scores": [[0.11582046002149582, 0.12567465007305145, 0.2996704876422882]], "normalized_score": [0.16339007019996643], "tokens": [["\u0120to", "\u0120make", "\u0120money"]], "entropy": [[5.4655046463012695, 4.026313781738281, 2.981689929962158]], "avg_entropy": [4.157836119333903], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Profit maximization", "score_of_biggest_cluster": 0.16339007019996643, "score_of_first_answer": 0.16339007019996643, "max_score_of_first_answer": 0.2996704876422882, "min_score_of_first_answer": 0.11582046002149582, "avg_score": 0.16339007019996643, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0844409650513031, "std_of_first_answer": 0.0844409650513031, "avg_entropy_of_first_answer": 4.157836119333903, "lex_sim": 1.0}
{"question": "what type of dog was laika the spacedog", "answer": ["lama"], "scores": [[0.006080143619328737, 0.04395102709531784]], "normalized_score": [0.01634712517261505], "tokens": [["\u0120l", "ama"]], "entropy": [[5.790884494781494, 4.197409629821777]], "avg_entropy": [4.994147062301636], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "mongrel female", "score_of_biggest_cluster": 0.01634712517261505, "score_of_first_answer": 0.01634712517261505, "max_score_of_first_answer": 0.04395102709531784, "min_score_of_first_answer": 0.006080143619328737, "avg_score": 0.01634712517261505, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01893544173799455, "std_of_first_answer": 0.01893544173799455, "avg_entropy_of_first_answer": 4.994147062301636, "lex_sim": 1.0}
{"question": "who invented the printing press and in what year", "answer": ["Gutenberg in 1440"], "scores": [[0.365021288394928, 0.06108473986387253, 0.11928023397922516]], "normalized_score": [0.13854999840259552], "tokens": [["\u0120Gutenberg", "\u0120in", "\u01201440"]], "entropy": [[2.6535208225250244, 1.5198817253112793, 1.196454405784607]], "avg_entropy": [1.789952317873637], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the German Johannes Gutenberg", "score_of_biggest_cluster": 0.13854999840259552, "score_of_first_answer": 0.13854999840259552, "max_score_of_first_answer": 0.365021288394928, "min_score_of_first_answer": 0.06108473986387253, "avg_score": 0.13854999840259552, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13172058258704805, "std_of_first_answer": 0.13172058258704805, "avg_entropy_of_first_answer": 1.789952317873637, "lex_sim": 1.0}
{"question": "who defeated the last remaining roman army in europe", "answer": ["gaul"], "scores": [[0.007153867278248072, 0.41898736357688904]], "normalized_score": [0.05474833399057388], "tokens": [["\u0120g", "aul"]], "entropy": [[5.930058002471924, 2.296660900115967]], "avg_entropy": [4.113359451293945], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Goths", "score_of_biggest_cluster": 0.05474833399057388, "score_of_first_answer": 0.05474833399057388, "max_score_of_first_answer": 0.41898736357688904, "min_score_of_first_answer": 0.007153867278248072, "avg_score": 0.05474833399057388, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20591674814932048, "std_of_first_answer": 0.20591674814932048, "avg_entropy_of_first_answer": 4.113359451293945, "lex_sim": 1.0}
{"question": "what is the largest magnitude earthquake that has occurred in the pacific northwest", "answer": ["Great Central WA, USA, magnitude 7.1 (1979)"], "scores": [[0.0064140912145376205, 0.00604112446308136, 0.0032663154415786266, 0.04957311227917671, 0.043652165681123734, 0.26707151532173157, 0.1299050748348236, 0.39082083106040955, 0.966681718826294, 0.3881503641605377, 0.05207987129688263, 0.026795579120516777, 0.8382299542427063]], "normalized_score": [0.07285760343074799], "tokens": [["\u0120Great", "\u0120Central", "\u0120WA", ",", "\u0120USA", ",", "\u0120magnitude", "\u01207", ".", "1", "\u0120(", "1979", ")"]], "entropy": [[3.527902603149414, 4.2263031005859375, 3.6595425605773926, 4.156685829162598, 5.555637359619141, 2.657416343688965, 5.13723087310791, 1.94394052028656, 0.18850889801979065, 1.9376071691513062, 2.3040385246276855, 5.269614219665527, 0.930324912071228]], "avg_entropy": [3.1919040702856503], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The 1700 Cascadia earthquake", "score_of_biggest_cluster": 0.07285760343074799, "score_of_first_answer": 0.07285760343074799, "max_score_of_first_answer": 0.966681718826294, "min_score_of_first_answer": 0.0032663154415786266, "avg_score": 0.07285760343074799, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31184799699055504, "std_of_first_answer": 0.31184799699055504, "avg_entropy_of_first_answer": 3.1919040702856503, "lex_sim": 1.0}
{"question": "what is the minimum number of parties required to enter into a deed", "answer": ["3"], "scores": [[0.30587270855903625]], "normalized_score": [0.30587270855903625], "tokens": [["\u01203"]], "entropy": [[2.510594367980957]], "avg_entropy": [2.510594367980957], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "two parties", "score_of_biggest_cluster": 0.30587270855903625, "score_of_first_answer": 0.30587270855903625, "max_score_of_first_answer": 0.30587270855903625, "min_score_of_first_answer": 0.30587270855903625, "avg_score": 0.30587270855903625, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.510594367980957, "lex_sim": 1.0}
{"question": "what is the largest man made lake in the us", "answer": ["Lake Mead"], "scores": [[0.4713020324707031, 0.3445770740509033]], "normalized_score": [0.4029886722564697], "tokens": [["\u0120Lake", "\u0120Mead"]], "entropy": [[3.535597801208496, 3.375537395477295]], "avg_entropy": [3.4555675983428955], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lake Oahe", "score_of_biggest_cluster": 0.4029886722564697, "score_of_first_answer": 0.4029886722564697, "max_score_of_first_answer": 0.4713020324707031, "min_score_of_first_answer": 0.3445770740509033, "avg_score": 0.4029886722564697, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0633624792098999, "std_of_first_answer": 0.0633624792098999, "avg_entropy_of_first_answer": 3.4555675983428955, "lex_sim": 1.0}
{"question": "who played shmuel in the boy in the striped pyjamas", "answer": ["Daniel Day-Lewis"], "scores": [[0.16474507749080658, 0.23471805453300476, 0.3264460265636444, 0.9944275617599487]], "normalized_score": [0.33472326397895813], "tokens": [["\u0120Daniel", "\u0120Day", "-", "Lewis"]], "entropy": [[5.40281343460083, 2.5945723056793213, 0.9893801808357239, 0.043337732553482056]], "avg_entropy": [2.2575259134173393], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jack Scanlon", "score_of_biggest_cluster": 0.33472326397895813, "score_of_first_answer": 0.33472326397895813, "max_score_of_first_answer": 0.9944275617599487, "min_score_of_first_answer": 0.16474507749080658, "avg_score": 0.33472326397895813, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3308311811499646, "std_of_first_answer": 0.3308311811499646, "avg_entropy_of_first_answer": 2.2575259134173393, "lex_sim": 1.0}
{"question": "who was elected senator of illinois in 1858", "answer": ["Abraham Lincoln"], "scores": [[0.48951980471611023, 0.9927318692207336]], "normalized_score": [0.6971096992492676], "tokens": [["\u0120Abraham", "\u0120Lincoln"]], "entropy": [[2.363673686981201, 0.07597020268440247]], "avg_entropy": [1.2198219448328018], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stephen A. Douglas", "score_of_biggest_cluster": 0.6971096992492676, "score_of_first_answer": 0.6971096992492676, "max_score_of_first_answer": 0.9927318692207336, "min_score_of_first_answer": 0.48951980471611023, "avg_score": 0.6971096992492676, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2516060322523117, "std_of_first_answer": 0.2516060322523117, "avg_entropy_of_first_answer": 1.2198219448328018, "lex_sim": 1.0}
{"question": "when did jack mccoy join law and order", "answer": ["December 5th, 1986"], "scores": [[0.004346305504441261, 0.038645971566438675, 0.6223904490470886, 0.26034921407699585, 0.024833962321281433]], "normalized_score": [0.05834154039621353], "tokens": [["\u0120December", "\u01205", "th", ",", "\u01201986"]], "entropy": [[5.363899230957031, 4.083099365234375, 1.104304313659668, 2.665175676345825, 4.508862495422363]], "avg_entropy": [3.5450682163238527], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1994", "score_of_biggest_cluster": 0.05834154039621353, "score_of_first_answer": 0.05834154039621353, "max_score_of_first_answer": 0.6223904490470886, "min_score_of_first_answer": 0.004346305504441261, "avg_score": 0.05834154039621353, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23518741654462205, "std_of_first_answer": 0.23518741654462205, "avg_entropy_of_first_answer": 3.5450682163238527, "lex_sim": 1.0}
{"question": "published a treatise on perspective della pitture (or on painting) in 1435", "answer": ["1435"], "scores": [[0.0347449965775013, 0.4250067472457886]], "normalized_score": [0.12151894718408585], "tokens": [["\u012014", "35"]], "entropy": [[5.695712089538574, 2.8993892669677734]], "avg_entropy": [4.297550678253174], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Leon Battista Alberti", "score_of_biggest_cluster": 0.12151894718408585, "score_of_first_answer": 0.12151894718408585, "max_score_of_first_answer": 0.4250067472457886, "min_score_of_first_answer": 0.0347449965775013, "avg_score": 0.12151894718408585, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19513087533414364, "std_of_first_answer": 0.19513087533414364, "avg_entropy_of_first_answer": 4.297550678253174, "lex_sim": 1.0}
{"question": "which condition would most likely require nutrition delivered through tpn", "answer": ["End Stage liver disease"], "scores": [[0.018500322476029396, 0.5852693915367126, 0.005559684708714485, 0.5572115182876587]], "normalized_score": [0.07610292732715607], "tokens": [["\u0120End", "\u0120Stage", "\u0120liver", "\u0120disease"]], "entropy": [[6.545839309692383, 1.6207013130187988, 2.4730584621429443, 1.7212777137756348]], "avg_entropy": [3.09021919965744], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "bowel obstruction", "score_of_biggest_cluster": 0.07610292732715607, "score_of_first_answer": 0.07610292732715607, "max_score_of_first_answer": 0.5852693915367126, "min_score_of_first_answer": 0.005559684708714485, "avg_score": 0.07610292732715607, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2798185488150545, "std_of_first_answer": 0.2798185488150545, "avg_entropy_of_first_answer": 3.09021919965744, "lex_sim": 1.0}
{"question": "where does the sun hit the us first", "answer": ["California"], "scores": [[0.013306481763720512]], "normalized_score": [0.013306484557688236], "tokens": [["\u0120California"]], "entropy": [[5.546662330627441]], "avg_entropy": [5.546662330627441], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the summit of Cadillac Mountain", "score_of_biggest_cluster": 0.013306484557688236, "score_of_first_answer": 0.013306484557688236, "max_score_of_first_answer": 0.013306481763720512, "min_score_of_first_answer": 0.013306481763720512, "avg_score": 0.013306484557688236, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.546662330627441, "lex_sim": 1.0}
{"question": "a type of basic rock popular in the 1980s", "answer": ["Cretaceous oolite"], "scores": [[0.03433326259255409, 0.28526973724365234, 0.9197142720222473, 0.00048207506188191473, 0.09556332230567932, 0.8547607064247131]], "normalized_score": [0.08413545042276382], "tokens": [["\u0120C", "ret", "aceous", "\u0120o", "ol", "ite"]], "entropy": [[6.991965293884277, 4.092591285705566, 0.5669898986816406, 1.3201322555541992, 3.242525100708008, 0.6943826675415039]], "avg_entropy": [2.8180977503458657], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "alternative rock", "score_of_biggest_cluster": 0.08413545042276382, "score_of_first_answer": 0.08413545042276382, "max_score_of_first_answer": 0.9197142720222473, "min_score_of_first_answer": 0.00048207506188191473, "avg_score": 0.08413545042276382, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38051288141159045, "std_of_first_answer": 0.38051288141159045, "avg_entropy_of_first_answer": 2.8180977503458657, "lex_sim": 1.0}
{"question": "where are they building the new raiders stadium", "answer": ["Oakland,CA"], "scores": [[0.1445169746875763, 0.13793735206127167, 0.0038818016182631254]], "normalized_score": [0.04261324927210808], "tokens": [["\u0120Oakland", ",", "CA"]], "entropy": [[4.811882019042969, 1.9475414752960205, 1.3555749654769897]], "avg_entropy": [2.704999486605326], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paradise, Nevada", "score_of_biggest_cluster": 0.04261324927210808, "score_of_first_answer": 0.04261324927210808, "max_score_of_first_answer": 0.1445169746875763, "min_score_of_first_answer": 0.0038818016182631254, "avg_score": 0.04261324927210808, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06480092072624197, "std_of_first_answer": 0.06480092072624197, "avg_entropy_of_first_answer": 2.704999486605326, "lex_sim": 1.0}
{"question": "what is the main mineral in lithium batteries", "answer": ["lithium ion"], "scores": [[0.12915624678134918, 0.011733843944966793]], "normalized_score": [0.03892941027879715], "tokens": [["\u0120lithium", "\u0120ion"]], "entropy": [[2.3527772426605225, 1.0859649181365967]], "avg_entropy": [1.7193710803985596], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lithium", "score_of_biggest_cluster": 0.03892941027879715, "score_of_first_answer": 0.03892941027879715, "max_score_of_first_answer": 0.12915624678134918, "min_score_of_first_answer": 0.011733843944966793, "avg_score": 0.03892941027879715, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.058711201418191195, "std_of_first_answer": 0.058711201418191195, "avg_entropy_of_first_answer": 1.7193710803985596, "lex_sim": 1.0}
{"question": "when did ford change the f150 body style", "answer": ["2012"], "scores": [[0.004378670826554298]], "normalized_score": [0.004378669895231724], "tokens": [["\u01202012"]], "entropy": [[5.442450046539307]], "avg_entropy": [5.442450046539307], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the 2009 model year", "score_of_biggest_cluster": 0.004378669895231724, "score_of_first_answer": 0.004378669895231724, "max_score_of_first_answer": 0.004378670826554298, "min_score_of_first_answer": 0.004378670826554298, "avg_score": 0.004378669895231724, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.442450046539307, "lex_sim": 1.0}
{"question": "chandra and bhaga river meets at the place", "answer": ["Kaveri"], "scores": [[0.06571975350379944, 0.04827113822102547, 0.9795635342597961]], "normalized_score": [0.1459280252456665], "tokens": [["\u0120K", "aver", "i"]], "entropy": [[5.955679416656494, 3.873521566390991, 0.13592007756233215]], "avg_entropy": [3.3217070202032724], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tandi, in Lahaul", "score_of_biggest_cluster": 0.1459280252456665, "score_of_first_answer": 0.1459280252456665, "max_score_of_first_answer": 0.9795635342597961, "min_score_of_first_answer": 0.04827113822102547, "avg_score": 0.1459280252456665, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4349611012903382, "std_of_first_answer": 0.4349611012903382, "avg_entropy_of_first_answer": 3.3217070202032724, "lex_sim": 1.0}
{"question": "who has the best nba record this season", "answer": ["James Harden"], "scores": [[0.0360429510474205, 0.9610999226570129]], "normalized_score": [0.18612058460712433], "tokens": [["\u0120James", "\u0120Harden"]], "entropy": [[4.794041633605957, 0.39305579662323]], "avg_entropy": [2.5935487151145935], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Houston Rockets", "score_of_biggest_cluster": 0.18612058460712433, "score_of_first_answer": 0.18612058460712433, "max_score_of_first_answer": 0.9610999226570129, "min_score_of_first_answer": 0.0360429510474205, "avg_score": 0.18612058460712433, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4625284858047962, "std_of_first_answer": 0.4625284858047962, "avg_entropy_of_first_answer": 2.5935487151145935, "lex_sim": 1.0}
{"question": "who was the first coronated king of manipur at kangla", "answer": ["Anand Singh"], "scores": [[0.006941546685993671, 0.16476857662200928, 0.016593391075730324]], "normalized_score": [0.02667403407394886], "tokens": [["\u0120An", "and", "\u0120Singh"]], "entropy": [[6.414792537689209, 2.99121356010437, 3.710726499557495]], "avg_entropy": [4.372244199117024], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pakhangba", "score_of_biggest_cluster": 0.02667403407394886, "score_of_first_answer": 0.02667403407394886, "max_score_of_first_answer": 0.16476857662200928, "min_score_of_first_answer": 0.006941546685993671, "avg_score": 0.02667403407394886, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07223296807338957, "std_of_first_answer": 0.07223296807338957, "avg_entropy_of_first_answer": 4.372244199117024, "lex_sim": 1.0}
{"question": "who sings two pints of lager and a packet of crisps", "answer": ["Oasis"], "scores": [[0.0360674113035202, 0.6377556920051575]], "normalized_score": [0.15166474878787994], "tokens": [["\u0120O", "asis"]], "entropy": [[6.249481678009033, 1.3268907070159912]], "avg_entropy": [3.788186192512512], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Christian Henson", "score_of_biggest_cluster": 0.15166474878787994, "score_of_first_answer": 0.15166474878787994, "max_score_of_first_answer": 0.6377556920051575, "min_score_of_first_answer": 0.0360674113035202, "avg_score": 0.15166474878787994, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30084414035081863, "std_of_first_answer": 0.30084414035081863, "avg_entropy_of_first_answer": 3.788186192512512, "lex_sim": 1.0}
{"question": "who wrote there's a guy works down the chip shop lyrics", "answer": ["Robert Earl Keen"], "scores": [[0.005642831791192293, 0.09992383420467377, 0.6831192970275879]], "normalized_score": [0.07275914400815964], "tokens": [["\u0120Robert", "\u0120Earl", "\u0120Keen"]], "entropy": [[6.417120933532715, 5.927708148956299, 1.7514252662658691]], "avg_entropy": [4.698751449584961], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Philip Rambow", "score_of_biggest_cluster": 0.07275914400815964, "score_of_first_answer": 0.07275914400815964, "max_score_of_first_answer": 0.6831192970275879, "min_score_of_first_answer": 0.005642831791192293, "avg_score": 0.07275914400815964, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2996257325068736, "std_of_first_answer": 0.2996257325068736, "avg_entropy_of_first_answer": 4.698751449584961, "lex_sim": 1.0}
{"question": "what time is it in alaska time zone", "answer": ["13:30 UTC"], "scores": [[0.007391415070742369, 0.821597695350647, 0.06216200813651085, 0.004306338261812925]], "normalized_score": [0.03570713847875595], "tokens": [["\u012013", ":", "30", "\u0120UTC"]], "entropy": [[3.9966208934783936, 1.1651097536087036, 3.938983917236328, 1.1854225397109985]], "avg_entropy": [2.571534276008606], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "UTC\u221209:00", "score_of_biggest_cluster": 0.03570713847875595, "score_of_first_answer": 0.03570713847875595, "max_score_of_first_answer": 0.821597695350647, "min_score_of_first_answer": 0.004306338261812925, "avg_score": 0.03570713847875595, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34586812934394673, "std_of_first_answer": 0.34586812934394673, "avg_entropy_of_first_answer": 2.571534276008606, "lex_sim": 1.0}
{"question": "cat in the hat knows a lot about space movie", "answer": ["space"], "scores": [[0.013702468946576118]], "normalized_score": [0.013702468946576118], "tokens": [["\u0120space"]], "entropy": [[6.5911030769348145]], "avg_entropy": [6.5911030769348145], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Space is the Place", "score_of_biggest_cluster": 0.013702468946576118, "score_of_first_answer": 0.013702468946576118, "max_score_of_first_answer": 0.013702468946576118, "min_score_of_first_answer": 0.013702468946576118, "avg_score": 0.013702468946576118, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.5911030769348145, "lex_sim": 1.0}
{"question": "who did us fight in world war 1", "answer": ["France"], "scores": [[0.02120865136384964]], "normalized_score": [0.02120865136384964], "tokens": [["\u0120France"]], "entropy": [[4.484708786010742]], "avg_entropy": [4.484708786010742], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Germany", "score_of_biggest_cluster": 0.02120865136384964, "score_of_first_answer": 0.02120865136384964, "max_score_of_first_answer": 0.02120865136384964, "min_score_of_first_answer": 0.02120865136384964, "avg_score": 0.02120865136384964, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.484708786010742, "lex_sim": 1.0}
{"question": "vine with heart shaped leaves and white flowers", "answer": ["Vitis"], "scores": [[0.07397780567407608, 0.826757550239563]], "normalized_score": [0.24730895459651947], "tokens": [["\u0120V", "itis"]], "entropy": [[6.545304298400879, 1.0343776941299438]], "avg_entropy": [3.7898409962654114], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Antigonon leptopus", "score_of_biggest_cluster": 0.24730895459651947, "score_of_first_answer": 0.24730895459651947, "max_score_of_first_answer": 0.826757550239563, "min_score_of_first_answer": 0.07397780567407608, "avg_score": 0.24730895459651947, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37638987228274345, "std_of_first_answer": 0.37638987228274345, "avg_entropy_of_first_answer": 3.7898409962654114, "lex_sim": 1.0}
{"question": "who played skeletor in the movie masters of the universe", "answer": ["Eric Martin"], "scores": [[0.0077840364538133144, 0.004600887186825275]], "normalized_score": [0.00598443578928709], "tokens": [["\u0120Eric", "\u0120Martin"]], "entropy": [[6.001142978668213, 4.35739278793335]], "avg_entropy": [5.179267883300781], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Frank Langella", "score_of_biggest_cluster": 0.00598443578928709, "score_of_first_answer": 0.00598443578928709, "max_score_of_first_answer": 0.0077840364538133144, "min_score_of_first_answer": 0.004600887186825275, "avg_score": 0.00598443578928709, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0015915746334940195, "std_of_first_answer": 0.0015915746334940195, "avg_entropy_of_first_answer": 5.179267883300781, "lex_sim": 1.0}
{"question": "who is the book of acts written to", "answer": ["Acts of the apostles"], "scores": [[0.2464098036289215, 0.22372344136238098, 0.8962769508361816, 0.10356495529413223]], "normalized_score": [0.26745834946632385], "tokens": [["\u0120Acts", "\u0120of", "\u0120the", "\u0120apostles"]], "entropy": [[4.161396026611328, 1.4080638885498047, 0.873759925365448, 0.4494702219963074]], "avg_entropy": [1.723172515630722], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jewish audiences", "score_of_biggest_cluster": 0.26745834946632385, "score_of_first_answer": 0.26745834946632385, "max_score_of_first_answer": 0.8962769508361816, "min_score_of_first_answer": 0.10356495529413223, "avg_score": 0.26745834946632385, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31008116109979755, "std_of_first_answer": 0.31008116109979755, "avg_entropy_of_first_answer": 1.723172515630722, "lex_sim": 1.0}
{"question": "who stole the mona lisa from the louvre in 1911", "answer": ["the americans"], "scores": [[0.017547661438584328, 0.010682606138288975, 0.33126530051231384]], "normalized_score": [0.03959960117936134], "tokens": [["\u0120the", "\u0120americ", "ans"]], "entropy": [[5.871912002563477, 6.257950782775879, 0.6760977506637573]], "avg_entropy": [4.268653512001038], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Vincenzo Peruggia", "score_of_biggest_cluster": 0.03959960117936134, "score_of_first_answer": 0.03959960117936134, "max_score_of_first_answer": 0.33126530051231384, "min_score_of_first_answer": 0.010682606138288975, "avg_score": 0.03959960117936134, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14953228933443713, "std_of_first_answer": 0.14953228933443713, "avg_entropy_of_first_answer": 4.268653512001038, "lex_sim": 1.0}
{"question": "which domain of life are humans members of", "answer": ["mammals"], "scores": [[0.006917049176990986]], "normalized_score": [0.006917050573974848], "tokens": [["\u0120mammals"]], "entropy": [[5.040536880493164]], "avg_entropy": [5.040536880493164], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eukarya", "score_of_biggest_cluster": 0.006917050573974848, "score_of_first_answer": 0.006917050573974848, "max_score_of_first_answer": 0.006917049176990986, "min_score_of_first_answer": 0.006917049176990986, "avg_score": 0.006917050573974848, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.040536880493164, "lex_sim": 1.0}
{"question": "what is the term of an official in the house of representatives", "answer": ["term"], "scores": [[0.025122854858636856]], "normalized_score": [0.025122852995991707], "tokens": [["\u0120term"]], "entropy": [[4.485690116882324]], "avg_entropy": [4.485690116882324], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "two-year", "score_of_biggest_cluster": 0.025122852995991707, "score_of_first_answer": 0.025122852995991707, "max_score_of_first_answer": 0.025122854858636856, "min_score_of_first_answer": 0.025122854858636856, "avg_score": 0.025122852995991707, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.485690116882324, "lex_sim": 1.0}
{"question": "who wore number 7 for the green bay packers", "answer": ["hall of famer Reggie vanomalla"], "scores": [[0.0010777584975585341, 0.7694249749183655, 0.6300333738327026, 0.9379986524581909, 0.026202578097581863, 0.006795366294682026, 0.0027019991539418697, 0.029655907303094864]], "normalized_score": [0.04032514989376068], "tokens": [["\u0120hall", "\u0120of", "\u0120fam", "er", "\u0120Reggie", "\u0120van", "om", "alla"]], "entropy": [[6.147695541381836, 1.1350159645080566, 0.9318249821662903, 0.2912994623184204, 6.096500396728516, 3.7060041427612305, 3.9966468811035156, 4.464305877685547]], "avg_entropy": [3.3461616560816765], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Don Majkowski", "score_of_biggest_cluster": 0.04032514989376068, "score_of_first_answer": 0.04032514989376068, "max_score_of_first_answer": 0.9379986524581909, "min_score_of_first_answer": 0.0010777584975585341, "avg_score": 0.04032514989376068, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3788275902451139, "std_of_first_answer": 0.3788275902451139, "avg_entropy_of_first_answer": 3.3461616560816765, "lex_sim": 1.0}
{"question": "the cold dry winds that blow over northern india in winter are called", "answer": ["Bora"], "scores": [[0.0036312995944172144, 0.35767754912376404]], "normalized_score": [0.03603934869170189], "tokens": [["\u0120B", "ora"]], "entropy": [[5.6100969314575195, 3.3360419273376465]], "avg_entropy": [4.473069429397583], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Northeast Monsoon", "score_of_biggest_cluster": 0.03603934869170189, "score_of_first_answer": 0.03603934869170189, "max_score_of_first_answer": 0.35767754912376404, "min_score_of_first_answer": 0.0036312995944172144, "avg_score": 0.03603934869170189, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1770231247646734, "std_of_first_answer": 0.1770231247646734, "avg_entropy_of_first_answer": 4.473069429397583, "lex_sim": 1.0}
{"question": "what were the two causes of the dust bowl", "answer": ["droughts and overgrazing of grasslands and wheat fields"], "scores": [[0.011127852834761143, 0.921952486038208, 0.9775984287261963, 0.7596884965896606, 0.14250563085079193, 0.40449896454811096, 0.9997093081474304, 0.9750369191169739, 0.046252038329839706, 0.06766148656606674, 0.616364061832428, 0.0056627774611115456, 0.00402811961248517, 0.3956690728664398]], "normalized_score": [0.1603860855102539], "tokens": [["\u0120d", "rough", "ts", "\u0120and", "\u0120over", "g", "raz", "ing", "\u0120of", "\u0120grass", "lands", "\u0120and", "\u0120wheat", "\u0120fields"]], "entropy": [[6.140281677246094, 0.6013947129249573, 0.17741283774375916, 1.1941967010498047, 5.1649932861328125, 2.6820437908172607, 0.0036551093216985464, 0.15095680952072144, 0.8006401062011719, 3.251133918762207, 1.3172181844711304, 0.7452274560928345, 4.539333343505859, 2.2279722690582275]], "avg_entropy": [2.0711757287748958], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "severe drought", "score_of_biggest_cluster": 0.1603860855102539, "score_of_first_answer": 0.1603860855102539, "max_score_of_first_answer": 0.9997093081474304, "min_score_of_first_answer": 0.00402811961248517, "avg_score": 0.1603860855102539, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3964284896283258, "std_of_first_answer": 0.3964284896283258, "avg_entropy_of_first_answer": 2.0711757287748958, "lex_sim": 1.0}
{"question": "the area enclosed by hysteresis loop is a measure of", "answer": ["energy absorption"], "scores": [[0.020853430032730103, 0.004010031931102276]], "normalized_score": [0.009144558571279049], "tokens": [["\u0120energy", "\u0120absorption"]], "entropy": [[5.668440341949463, 2.931940793991089]], "avg_entropy": [4.300190567970276], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "energy loss", "score_of_biggest_cluster": 0.009144558571279049, "score_of_first_answer": 0.009144558571279049, "max_score_of_first_answer": 0.020853430032730103, "min_score_of_first_answer": 0.004010031931102276, "avg_score": 0.009144558571279049, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.008421699050813913, "std_of_first_answer": 0.008421699050813913, "avg_entropy_of_first_answer": 4.300190567970276, "lex_sim": 1.0}
{"question": "when was the suite life of zack and cody made", "answer": ["1996"], "scores": [[0.021920576691627502]], "normalized_score": [0.02192057855427265], "tokens": [["\u01201996"]], "entropy": [[4.710740566253662]], "avg_entropy": [4.710740566253662], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "March\u00a018,\u00a02005", "score_of_biggest_cluster": 0.02192057855427265, "score_of_first_answer": 0.02192057855427265, "max_score_of_first_answer": 0.021920576691627502, "min_score_of_first_answer": 0.021920576691627502, "avg_score": 0.02192057855427265, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.710740566253662, "lex_sim": 1.0}
{"question": "who is the prime minister of india full name", "answer": ["Narendra modi sprt"], "scores": [[0.7111336588859558, 0.052302781492471695, 0.9493988156318665, 0.001054057851433754, 0.00576363131403923]], "normalized_score": [0.046376410871744156], "tokens": [["\u0120Narendra", "\u0120mod", "i", "\u0120spr", "t"]], "entropy": [[1.969849705696106, 0.7283454537391663, 0.4193301796913147, 1.096923589706421, 3.843294382095337]], "avg_entropy": [1.611548662185669], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Narendra Modi", "score_of_biggest_cluster": 0.046376410871744156, "score_of_first_answer": 0.046376410871744156, "max_score_of_first_answer": 0.9493988156318665, "min_score_of_first_answer": 0.001054057851433754, "avg_score": 0.046376410871744156, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40457331918679645, "std_of_first_answer": 0.40457331918679645, "avg_entropy_of_first_answer": 1.611548662185669, "lex_sim": 1.0}
{"question": "georgia out of angus thongs and perfect snogging", "answer": ["who is in the group g4?"], "scores": [[0.006478077732026577, 0.2188449501991272, 0.025666754692792892, 0.4412372410297394, 0.009277022443711758, 0.01637975499033928, 0.007010930683463812, 0.07458508759737015]], "normalized_score": [0.032600268721580505], "tokens": [["\u0120who", "\u0120is", "\u0120in", "\u0120the", "\u0120group", "\u0120g", "4", "?"]], "entropy": [[6.5142598152160645, 4.450334072113037, 5.147388458251953, 4.625417232513428, 6.503758430480957, 5.256755352020264, 5.19023323059082, 2.9853291511535645]], "avg_entropy": [5.084184467792511], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Georgia Groome as Georgia Nicolson", "score_of_biggest_cluster": 0.032600268721580505, "score_of_first_answer": 0.032600268721580505, "max_score_of_first_answer": 0.4412372410297394, "min_score_of_first_answer": 0.006478077732026577, "avg_score": 0.032600268721580505, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14549682691964325, "std_of_first_answer": 0.14549682691964325, "avg_entropy_of_first_answer": 5.084184467792511, "lex_sim": 1.0}
{"question": "is the united states a country or nation", "answer": ["country"], "scores": [[0.18276269733905792]], "normalized_score": [0.18276269733905792], "tokens": [["\u0120country"]], "entropy": [[3.6543800830841064]], "avg_entropy": [3.6543800830841064], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "country", "score_of_biggest_cluster": 0.18276269733905792, "score_of_first_answer": 0.18276269733905792, "max_score_of_first_answer": 0.18276269733905792, "min_score_of_first_answer": 0.18276269733905792, "avg_score": 0.18276269733905792, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.6543800830841064, "lex_sim": 1.0}
{"question": "what was the outcome of the battle of san juan hill", "answer": ["The battle ended with a stalemate"], "scores": [[0.09860564768314362, 0.1614227592945099, 0.04920298233628273, 0.1840624064207077, 0.38030147552490234, 0.13906407356262207, 0.9988331198692322]], "normalized_score": [0.18569853901863098], "tokens": [["\u0120The", "\u0120battle", "\u0120ended", "\u0120with", "\u0120a", "\u0120stal", "emate"]], "entropy": [[5.734587669372559, 4.371629238128662, 1.8636316061019897, 1.9658663272857666, 2.735640525817871, 3.775473117828369, 0.010441416874527931]], "avg_entropy": [2.9224671287728206], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cuban-American victory", "score_of_biggest_cluster": 0.18569853901863098, "score_of_first_answer": 0.18569853901863098, "max_score_of_first_answer": 0.9988331198692322, "min_score_of_first_answer": 0.04920298233628273, "avg_score": 0.18569853901863098, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30605819985179034, "std_of_first_answer": 0.30605819985179034, "avg_entropy_of_first_answer": 2.9224671287728206, "lex_sim": 1.0}
{"question": "who brought the idea of castles to england", "answer": ["William the Conqueror"], "scores": [[0.46009618043899536, 0.693335235118866, 0.4852670133113861, 0.9889063835144043]], "normalized_score": [0.6255066990852356], "tokens": [["\u0120William", "\u0120the", "\u0120Conquer", "or"]], "entropy": [[3.3605690002441406, 1.9426367282867432, 2.019228458404541, 0.07587258517742157]], "avg_entropy": [1.8495766930282116], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Norman invaders", "score_of_biggest_cluster": 0.6255066990852356, "score_of_first_answer": 0.6255066990852356, "max_score_of_first_answer": 0.9889063835144043, "min_score_of_first_answer": 0.46009618043899536, "avg_score": 0.6255066990852356, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.21198195121291893, "std_of_first_answer": 0.21198195121291893, "avg_entropy_of_first_answer": 1.8495766930282116, "lex_sim": 1.0}
{"question": "only player to play for manchester united manchester city liverpool and everton", "answer": ["Roy Bailey"], "scores": [[0.01398093905299902, 0.009089246392250061]], "normalized_score": [0.011272805742919445], "tokens": [["\u0120Roy", "\u0120Bailey"]], "entropy": [[5.703117370605469, 3.9555373191833496]], "avg_entropy": [4.829327344894409], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Peter Andrew Beardsley", "score_of_biggest_cluster": 0.011272805742919445, "score_of_first_answer": 0.011272805742919445, "max_score_of_first_answer": 0.01398093905299902, "min_score_of_first_answer": 0.009089246392250061, "avg_score": 0.011272805742919445, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0024458463303744793, "std_of_first_answer": 0.0024458463303744793, "avg_entropy_of_first_answer": 4.829327344894409, "lex_sim": 1.0}
{"question": "who is the number one ranked golfer in the world right now", "answer": ["Rory McIlroy"], "scores": [[0.2620234787464142, 0.9797516465187073, 0.5982043147087097, 0.9771369099617004]], "normalized_score": [0.6223938465118408], "tokens": [["\u0120Rory", "\u0120Mc", "Il", "roy"]], "entropy": [[3.651583194732666, 0.19391873478889465, 2.0348448753356934, 0.20176300406455994]], "avg_entropy": [1.5205274522304535], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dustin Johnson", "score_of_biggest_cluster": 0.6223938465118408, "score_of_first_answer": 0.6223938465118408, "max_score_of_first_answer": 0.9797516465187073, "min_score_of_first_answer": 0.2620234787464142, "avg_score": 0.6223938465118408, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2988220218405868, "std_of_first_answer": 0.2988220218405868, "avg_entropy_of_first_answer": 1.5205274522304535, "lex_sim": 1.0}
{"question": "when was the $1 000 bill discontinued", "answer": ["1973"], "scores": [[0.010508695617318153]], "normalized_score": [0.010508697479963303], "tokens": [["\u01201973"]], "entropy": [[4.788453578948975]], "avg_entropy": [4.788453578948975], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1969", "score_of_biggest_cluster": 0.010508697479963303, "score_of_first_answer": 0.010508697479963303, "max_score_of_first_answer": 0.010508695617318153, "min_score_of_first_answer": 0.010508695617318153, "avg_score": 0.010508697479963303, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.788453578948975, "lex_sim": 1.0}
{"question": "who comes after the president if he dies", "answer": ["vice president"], "scores": [[0.3548714220523834, 0.8045520186424255]], "normalized_score": [0.5343337059020996], "tokens": [["\u0120vice", "\u0120president"]], "entropy": [[2.818664789199829, 0.7680746912956238]], "avg_entropy": [1.7933697402477264], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "vice president", "score_of_biggest_cluster": 0.5343337059020996, "score_of_first_answer": 0.5343337059020996, "max_score_of_first_answer": 0.8045520186424255, "min_score_of_first_answer": 0.3548714220523834, "avg_score": 0.5343337059020996, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.22484029829502106, "std_of_first_answer": 0.22484029829502106, "avg_entropy_of_first_answer": 1.7933697402477264, "lex_sim": 1.0}
{"question": "where are the first nations located in canada", "answer": ["northern territory"], "scores": [[0.013113893568515778, 0.030871646478772163]], "normalized_score": [0.02012082003057003], "tokens": [["\u0120northern", "\u0120territory"]], "entropy": [[5.342127323150635, 3.6442413330078125]], "avg_entropy": [4.493184328079224], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "south of the Arctic", "score_of_biggest_cluster": 0.02012082003057003, "score_of_first_answer": 0.02012082003057003, "max_score_of_first_answer": 0.030871646478772163, "min_score_of_first_answer": 0.013113893568515778, "avg_score": 0.02012082003057003, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.008878876455128193, "std_of_first_answer": 0.008878876455128193, "avg_entropy_of_first_answer": 4.493184328079224, "lex_sim": 1.0}
{"question": "where do peaches come from in the us", "answer": ["California"], "scores": [[0.0732056200504303]], "normalized_score": [0.0732056200504303], "tokens": [["\u0120California"]], "entropy": [[5.315254211425781]], "avg_entropy": [5.315254211425781], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "other states", "score_of_biggest_cluster": 0.0732056200504303, "score_of_first_answer": 0.0732056200504303, "max_score_of_first_answer": 0.0732056200504303, "min_score_of_first_answer": 0.0732056200504303, "avg_score": 0.0732056200504303, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.315254211425781, "lex_sim": 1.0}
{"question": "who won the battle of the first battle of bull run", "answer": ["Thomas J. Jackson"], "scores": [[0.004432282876223326, 0.12604756653308868, 0.7173353433609009, 0.19990797340869904]], "normalized_score": [0.09460814297199249], "tokens": [["\u0120Thomas", "\u0120J", ".", "\u0120Jackson"]], "entropy": [[6.502772331237793, 5.309610366821289, 2.3817827701568604, 4.5505781173706055]], "avg_entropy": [4.686185896396637], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Confederate victory", "score_of_biggest_cluster": 0.09460814297199249, "score_of_first_answer": 0.09460814297199249, "max_score_of_first_answer": 0.7173353433609009, "min_score_of_first_answer": 0.004432282876223326, "avg_score": 0.09460814297199249, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2720339448444311, "std_of_first_answer": 0.2720339448444311, "avg_entropy_of_first_answer": 4.686185896396637, "lex_sim": 1.0}
{"question": "the most important battles in the revolutionary war", "answer": ["the Revolutionary war"], "scores": [[0.15893088281154633, 0.05077451094985008, 0.039544593542814255]], "normalized_score": [0.068335622549057], "tokens": [["\u0120the", "\u0120Revolutionary", "\u0120war"]], "entropy": [[4.950675964355469, 4.509298324584961, 0.2961842119693756]], "avg_entropy": [3.252052833636602], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Philadelphia campaign", "score_of_biggest_cluster": 0.068335622549057, "score_of_first_answer": 0.068335622549057, "max_score_of_first_answer": 0.15893088281154633, "min_score_of_first_answer": 0.039544593542814255, "avg_score": 0.068335622549057, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0538279129207264, "std_of_first_answer": 0.0538279129207264, "avg_entropy_of_first_answer": 3.252052833636602, "lex_sim": 1.0}
{"question": "who was hope brady's mother on days of our lives", "answer": ["Carol Brady"], "scores": [[0.021557481959462166, 0.9541959762573242]], "normalized_score": [0.1434226781129837], "tokens": [["\u0120Carol", "\u0120Brady"]], "entropy": [[5.987924575805664, 0.4610854983329773]], "avg_entropy": [3.2245050370693207], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ciara Brady", "score_of_biggest_cluster": 0.1434226781129837, "score_of_first_answer": 0.1434226781129837, "max_score_of_first_answer": 0.9541959762573242, "min_score_of_first_answer": 0.021557481959462166, "avg_score": 0.1434226781129837, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.466319247148931, "std_of_first_answer": 0.466319247148931, "avg_entropy_of_first_answer": 3.2245050370693207, "lex_sim": 1.0}
{"question": "panic at the disco song about a wedding", "answer": ["You'll be singing in the rain"], "scores": [[0.011287066154181957, 0.03409444913268089, 0.360750675201416, 0.0061103422194719315, 0.2371395379304886, 0.5691309571266174, 0.5297130942344666]], "normalized_score": [0.09310448169708252], "tokens": [["\u0120You", "'ll", "\u0120be", "\u0120singing", "\u0120in", "\u0120the", "\u0120rain"]], "entropy": [[7.036979675292969, 4.703341007232666, 2.349973440170288, 2.9502930641174316, 3.2360754013061523, 1.3450396060943604, 2.610894203186035]], "avg_entropy": [3.4617994853428433], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "I Write Sins Not Tragedies", "score_of_biggest_cluster": 0.09310448169708252, "score_of_first_answer": 0.09310448169708252, "max_score_of_first_answer": 0.5691309571266174, "min_score_of_first_answer": 0.0061103422194719315, "avg_score": 0.09310448169708252, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22538724572782706, "std_of_first_answer": 0.22538724572782706, "avg_entropy_of_first_answer": 3.4617994853428433, "lex_sim": 1.0}
{"question": "who played santa in the santa clause movies", "answer": ["James Earl Jones"], "scores": [[0.01362935546785593, 0.31273287534713745, 0.998776376247406]], "normalized_score": [0.1620711386203766], "tokens": [["\u0120James", "\u0120Earl", "\u0120Jones"]], "entropy": [[5.252178192138672, 4.0387282371521, 0.014676559716463089]], "avg_entropy": [3.101860996335745], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tim Allen", "score_of_biggest_cluster": 0.1620711386203766, "score_of_first_answer": 0.1620711386203766, "max_score_of_first_answer": 0.998776376247406, "min_score_of_first_answer": 0.01362935546785593, "avg_score": 0.1620711386203766, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41239587955729246, "std_of_first_answer": 0.41239587955729246, "avg_entropy_of_first_answer": 3.101860996335745, "lex_sim": 1.0}
{"question": "when did gimme gimme gimme start", "answer": ["August 4"], "scores": [[0.033466972410678864, 0.022241514176130295]], "normalized_score": [0.027282889932394028], "tokens": [["\u0120August", "\u01204"]], "entropy": [[4.793021202087402, 3.884199619293213]], "avg_entropy": [4.338610410690308], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "8 January 1999", "score_of_biggest_cluster": 0.027282889932394028, "score_of_first_answer": 0.027282889932394028, "max_score_of_first_answer": 0.033466972410678864, "min_score_of_first_answer": 0.022241514176130295, "avg_score": 0.027282889932394028, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.005612729117274284, "std_of_first_answer": 0.005612729117274284, "avg_entropy_of_first_answer": 4.338610410690308, "lex_sim": 1.0}
{"question": "explorer who led an early voyage to the coast of newfoundland", "answer": ["John Cabot"], "scores": [[0.27416935563087463, 0.9267820119857788, 0.9938430786132812]], "normalized_score": [0.6320791244506836], "tokens": [["\u0120John", "\u0120Cab", "ot"]], "entropy": [[3.6445953845977783, 0.5864543318748474, 0.059692367911338806]], "avg_entropy": [1.4302473614613216], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jacques Cartier", "score_of_biggest_cluster": 0.6320791244506836, "score_of_first_answer": 0.6320791244506836, "max_score_of_first_answer": 0.9938430786132812, "min_score_of_first_answer": 0.27416935563087463, "avg_score": 0.6320791244506836, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32460758077051544, "std_of_first_answer": 0.32460758077051544, "avg_entropy_of_first_answer": 1.4302473614613216, "lex_sim": 1.0}
{"question": "who plays janie in the search for santa paws", "answer": ["Jennifer Lopez"], "scores": [[0.05957328900694847, 0.36570262908935547]], "normalized_score": [0.1476011723279953], "tokens": [["\u0120Jennifer", "\u0120Lopez"]], "entropy": [[6.441973686218262, 2.571157455444336]], "avg_entropy": [4.506565570831299], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "G. Hannelius", "score_of_biggest_cluster": 0.1476011723279953, "score_of_first_answer": 0.1476011723279953, "max_score_of_first_answer": 0.36570262908935547, "min_score_of_first_answer": 0.05957328900694847, "avg_score": 0.1476011723279953, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1530646700412035, "std_of_first_answer": 0.1530646700412035, "avg_entropy_of_first_answer": 4.506565570831299, "lex_sim": 1.0}
{"question": "what nfl player has the most nfl rings", "answer": ["Kurt Warner"], "scores": [[0.0040573906153440475, 0.9635962247848511]], "normalized_score": [0.06252748519182205], "tokens": [["\u0120Kurt", "\u0120Warner"]], "entropy": [[5.173072814941406, 0.34098848700523376]], "avg_entropy": [2.75703065097332], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charles Haley", "score_of_biggest_cluster": 0.06252748519182205, "score_of_first_answer": 0.06252748519182205, "max_score_of_first_answer": 0.9635962247848511, "min_score_of_first_answer": 0.0040573906153440475, "avg_score": 0.06252748519182205, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4797694170847535, "std_of_first_answer": 0.4797694170847535, "avg_entropy_of_first_answer": 2.75703065097332, "lex_sim": 1.0}
{"question": "who was originally cast to play indiana jones", "answer": ["Harrison Ford"], "scores": [[0.10580115765333176, 0.8997344970703125]], "normalized_score": [0.30853351950645447], "tokens": [["\u0120Harrison", "\u0120Ford"]], "entropy": [[5.452608108520508, 0.8650960922241211]], "avg_entropy": [3.1588521003723145], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tom Selleck", "score_of_biggest_cluster": 0.30853351950645447, "score_of_first_answer": 0.30853351950645447, "max_score_of_first_answer": 0.8997344970703125, "min_score_of_first_answer": 0.10580115765333176, "avg_score": 0.30853351950645447, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39696666970849037, "std_of_first_answer": 0.39696666970849037, "avg_entropy_of_first_answer": 3.1588521003723145, "lex_sim": 1.0}
{"question": "an edge that is between a vertex and itself is a", "answer": ["diagonal"], "scores": [[0.005019995849579573]], "normalized_score": [0.005019995383918285], "tokens": [["\u0120diagonal"]], "entropy": [[6.302391529083252]], "avg_entropy": [6.302391529083252], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "loop", "score_of_biggest_cluster": 0.005019995383918285, "score_of_first_answer": 0.005019995383918285, "max_score_of_first_answer": 0.005019995849579573, "min_score_of_first_answer": 0.005019995849579573, "avg_score": 0.005019995383918285, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.302391529083252, "lex_sim": 1.0}
{"question": "the gulf stream the world's fastest ocean current flows along the western side of this water body", "answer": ["east side"], "scores": [[0.012813837267458439, 0.01625629886984825]], "normalized_score": [0.014432796277105808], "tokens": [["\u0120east", "\u0120side"]], "entropy": [[5.674615859985352, 2.482959508895874]], "avg_entropy": [4.078787684440613], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Atlantic ocean", "score_of_biggest_cluster": 0.014432796277105808, "score_of_first_answer": 0.014432796277105808, "max_score_of_first_answer": 0.01625629886984825, "min_score_of_first_answer": 0.012813837267458439, "avg_score": 0.014432796277105808, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0017212308011949062, "std_of_first_answer": 0.0017212308011949062, "avg_entropy_of_first_answer": 4.078787684440613, "lex_sim": 1.0}
{"question": "what is the purse for the senior open", "answer": ["$1.1 Million"], "scores": [[0.1984698623418808, 0.09977208077907562, 0.2692081928253174, 0.061321988701820374, 0.04725655913352966]], "normalized_score": [0.1090872585773468], "tokens": [["\u0120$", "1", ".", "1", "\u0120Million"]], "entropy": [[5.500626564025879, 4.176396369934082, 1.2302825450897217, 2.94783353805542, 1.811266541481018]], "avg_entropy": [3.133281111717224], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "$2 million in 2011", "score_of_biggest_cluster": 0.1090872585773468, "score_of_first_answer": 0.1090872585773468, "max_score_of_first_answer": 0.2692081928253174, "min_score_of_first_answer": 0.04725655913352966, "avg_score": 0.1090872585773468, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08533278135118859, "std_of_first_answer": 0.08533278135118859, "avg_entropy_of_first_answer": 3.133281111717224, "lex_sim": 1.0}
{"question": "who dies at the end of den of thieves", "answer": ["Mr. Whistler"], "scores": [[0.004066913854330778, 0.728546142578125, 0.023838074877858162, 0.2625004053115845, 0.9290075302124023]], "normalized_score": [0.11148810386657715], "tokens": [["\u0120Mr", ".", "\u0120Wh", "ist", "ler"]], "entropy": [[6.943763732910156, 2.333831310272217, 6.421954154968262, 2.9880330562591553, 0.37605494260787964]], "avg_entropy": [3.8127274394035338], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Merrimen", "score_of_biggest_cluster": 0.11148810386657715, "score_of_first_answer": 0.11148810386657715, "max_score_of_first_answer": 0.9290075302124023, "min_score_of_first_answer": 0.004066913854330778, "avg_score": 0.11148810386657715, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3753443283800485, "std_of_first_answer": 0.3753443283800485, "avg_entropy_of_first_answer": 3.8127274394035338, "lex_sim": 1.0}
{"question": "puella magi madoka magica when does madoka become a magical girl", "answer": ["in chapter 3"], "scores": [[0.045969802886247635, 0.06291870027780533, 0.09729525446891785]], "normalized_score": [0.0655311793088913], "tokens": [["\u0120in", "\u0120chapter", "\u01203"]], "entropy": [[5.243071556091309, 2.0943398475646973, 3.307432174682617]], "avg_entropy": [3.548281192779541], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "My Very Best Friend", "score_of_biggest_cluster": 0.0655311793088913, "score_of_first_answer": 0.0655311793088913, "max_score_of_first_answer": 0.09729525446891785, "min_score_of_first_answer": 0.045969802886247635, "avg_score": 0.0655311793088913, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02135237322487459, "std_of_first_answer": 0.02135237322487459, "avg_entropy_of_first_answer": 3.548281192779541, "lex_sim": 1.0}
{"question": "who played tom on as the world turns", "answer": ["Paul Tomkowicz"], "scores": [[0.003204591339454055, 0.010381100699305534, 0.02716231718659401, 0.38099905848503113]], "normalized_score": [0.024222929030656815], "tokens": [["\u0120Paul", "\u0120Tom", "k", "owicz"]], "entropy": [[2.7470011711120605, 4.211711883544922, 2.9631102085113525, 1.8827635049819946]], "avg_entropy": [2.9511466920375824], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jason Kincaid (1984)", "score_of_biggest_cluster": 0.024222929030656815, "score_of_first_answer": 0.024222929030656815, "max_score_of_first_answer": 0.38099905848503113, "min_score_of_first_answer": 0.003204591339454055, "avg_score": 0.024222929030656815, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.15933334821862494, "std_of_first_answer": 0.15933334821862494, "avg_entropy_of_first_answer": 2.9511466920375824, "lex_sim": 1.0}
{"question": "who took control of jamestown and made new rules that made everyone work", "answer": ["Dutch West india company"], "scores": [[0.015939760953187943, 0.3149288594722748, 0.009302036836743355, 0.08164719492197037, 0.20397891104221344]], "normalized_score": [0.06000117212533951], "tokens": [["\u0120Dutch", "\u0120West", "\u0120ind", "ia", "\u0120company"]], "entropy": [[5.414874076843262, 2.97872257232666, 0.6022863388061523, 0.6425848603248596, 0.970459520816803]], "avg_entropy": [2.1217854738235475], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Smith", "score_of_biggest_cluster": 0.06000117212533951, "score_of_first_answer": 0.06000117212533951, "max_score_of_first_answer": 0.3149288594722748, "min_score_of_first_answer": 0.009302036836743355, "avg_score": 0.06000117212533951, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11787292582500267, "std_of_first_answer": 0.11787292582500267, "avg_entropy_of_first_answer": 2.1217854738235475, "lex_sim": 1.0}
{"question": "how many seasons of johnny bravo are there", "answer": ["two"], "scores": [[0.018636522814631462]], "normalized_score": [0.01863652467727661], "tokens": [["\u0120two"]], "entropy": [[3.913527488708496]], "avg_entropy": [3.913527488708496], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "four", "score_of_biggest_cluster": 0.01863652467727661, "score_of_first_answer": 0.01863652467727661, "max_score_of_first_answer": 0.018636522814631462, "min_score_of_first_answer": 0.018636522814631462, "avg_score": 0.01863652467727661, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.913527488708496, "lex_sim": 1.0}
{"question": "when did seattle slew win the triple crown", "answer": ["inauguration of their new stadium"], "scores": [[1.961578846021439e-06, 0.2997014820575714, 0.0011171848746016622, 0.12036430090665817, 0.3620029091835022]], "normalized_score": [0.00778620271012187], "tokens": [["\u0120inauguration", "\u0120of", "\u0120their", "\u0120new", "\u0120stadium"]], "entropy": [[5.425897598266602, 1.8341906070709229, 5.241453170776367, 4.417459487915039, 3.4233152866363525]], "avg_entropy": [4.068463230133057], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1977", "score_of_biggest_cluster": 0.00778620271012187, "score_of_first_answer": 0.00778620271012187, "max_score_of_first_answer": 0.3620029091835022, "min_score_of_first_answer": 1.961578846021439e-06, "avg_score": 0.00778620271012187, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1501194587899295, "std_of_first_answer": 0.1501194587899295, "avg_entropy_of_first_answer": 4.068463230133057, "lex_sim": 1.0}
{"question": "who was the spanish civil war fought between", "answer": ["Spain"], "scores": [[0.017399024218320847]], "normalized_score": [0.017399026080965996], "tokens": [["\u0120Spain"]], "entropy": [[5.430386066436768]], "avg_entropy": [5.430386066436768], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nationalists", "score_of_biggest_cluster": 0.017399026080965996, "score_of_first_answer": 0.017399026080965996, "max_score_of_first_answer": 0.017399024218320847, "min_score_of_first_answer": 0.017399024218320847, "avg_score": 0.017399026080965996, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.430386066436768, "lex_sim": 1.0}
{"question": "when did the age of new media began", "answer": ["August 26, 2005"], "scores": [[0.0045894840732216835, 0.007257893215864897, 0.442468523979187, 0.0670628771185875]], "normalized_score": [0.03153078630566597], "tokens": [["\u0120August", "\u012026", ",", "\u01202005"]], "entropy": [[5.005963325500488, 3.9529056549072266, 1.1789357662200928, 3.4301397800445557]], "avg_entropy": [3.391986131668091], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the 1980s", "score_of_biggest_cluster": 0.03153078630566597, "score_of_first_answer": 0.03153078630566597, "max_score_of_first_answer": 0.442468523979187, "min_score_of_first_answer": 0.0045894840732216835, "avg_score": 0.03153078630566597, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18192760025604315, "std_of_first_answer": 0.18192760025604315, "avg_entropy_of_first_answer": 3.391986131668091, "lex_sim": 1.0}
{"question": "when do we find out luke and leia are siblings", "answer": ["in a letter dated 5/5/1955"], "scores": [[0.14785684645175934, 0.04734940454363823, 0.0014063195558264852, 0.01160445250570774, 0.006135075353085995, 0.3409148156642914, 0.05278858169913292, 0.9805766940116882, 0.05611419677734375, 0.016384603455662727]], "normalized_score": [0.040326084941625595], "tokens": [["\u0120in", "\u0120a", "\u0120letter", "\u0120dated", "\u01205", "/", "5", "/", "19", "55"]], "entropy": [[5.347718238830566, 3.9214797019958496, 4.351700782775879, 3.122659683227539, 5.102168560028076, 3.2540934085845947, 3.496678352355957, 0.165815532207489, 4.806647300720215, 3.2582216262817383]], "avg_entropy": [3.6827183187007906], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Return of the Jedi", "score_of_biggest_cluster": 0.040326084941625595, "score_of_first_answer": 0.040326084941625595, "max_score_of_first_answer": 0.9805766940116882, "min_score_of_first_answer": 0.0014063195558264852, "avg_score": 0.040326084941625595, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2884960543581407, "std_of_first_answer": 0.2884960543581407, "avg_entropy_of_first_answer": 3.6827183187007906, "lex_sim": 1.0}
{"question": "what kind of book is where's waldo", "answer": ["A children's story with an adult twist"], "scores": [[0.07286889106035233, 0.27010679244995117, 0.8369072079658508, 0.10522253811359406, 0.0037270081229507923, 0.06751076877117157, 0.25477927923202515, 0.44461488723754883]], "normalized_score": [0.12210071086883545], "tokens": [["\u0120A", "\u0120children", "'s", "\u0120story", "\u0120with", "\u0120an", "\u0120adult", "\u0120twist"]], "entropy": [[4.470839023590088, 4.691927909851074, 0.6940481662750244, 1.2416083812713623, 1.4479079246520996, 3.5009570121765137, 4.710583209991455, 2.9587955474853516]], "avg_entropy": [2.964583396911621], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "puzzle", "score_of_biggest_cluster": 0.12210071086883545, "score_of_first_answer": 0.12210071086883545, "max_score_of_first_answer": 0.8369072079658508, "min_score_of_first_answer": 0.0037270081229507923, "avg_score": 0.12210071086883545, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2570657666661903, "std_of_first_answer": 0.2570657666661903, "avg_entropy_of_first_answer": 2.964583396911621, "lex_sim": 1.0}
{"question": "where does the us launch space shuttles from", "answer": ["Florida"], "scores": [[0.01847034879028797]], "normalized_score": [0.01847034879028797], "tokens": [["\u0120Florida"]], "entropy": [[3.548832893371582]], "avg_entropy": [3.548832893371582], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Florida", "score_of_biggest_cluster": 0.01847034879028797, "score_of_first_answer": 0.01847034879028797, "max_score_of_first_answer": 0.01847034879028797, "min_score_of_first_answer": 0.01847034879028797, "avg_score": 0.01847034879028797, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.548832893371582, "lex_sim": 1.0}
{"question": "where did the british empire control an entire continent", "answer": ["Africa"], "scores": [[0.1888113170862198]], "normalized_score": [0.1888113170862198], "tokens": [["\u0120Africa"]], "entropy": [[4.179150581359863]], "avg_entropy": [4.179150581359863], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Australia", "score_of_biggest_cluster": 0.1888113170862198, "score_of_first_answer": 0.1888113170862198, "max_score_of_first_answer": 0.1888113170862198, "min_score_of_first_answer": 0.1888113170862198, "avg_score": 0.1888113170862198, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.179150581359863, "lex_sim": 1.0}
{"question": "who sings war don't let me down", "answer": ["Don McLean"], "scores": [[0.032025083899497986, 0.3636607527732849, 0.7929170727729797]], "normalized_score": [0.20979967713356018], "tokens": [["\u0120Don", "\u0120Mc", "Lean"]], "entropy": [[5.276117324829102, 2.525757074356079, 1.230207085609436]], "avg_entropy": [3.010693828264872], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "American production duo The Chainsmokers", "score_of_biggest_cluster": 0.20979967713356018, "score_of_first_answer": 0.20979967713356018, "max_score_of_first_answer": 0.7929170727729797, "min_score_of_first_answer": 0.032025083899497986, "avg_score": 0.20979967713356018, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3114838722168608, "std_of_first_answer": 0.3114838722168608, "avg_entropy_of_first_answer": 3.010693828264872, "lex_sim": 1.0}
{"question": "when did toyota first come to the united states", "answer": ["1992"], "scores": [[0.008019757457077503]], "normalized_score": [0.008019757457077503], "tokens": [["\u01201992"]], "entropy": [[5.225224494934082]], "avg_entropy": [5.225224494934082], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1957", "score_of_biggest_cluster": 0.008019757457077503, "score_of_first_answer": 0.008019757457077503, "max_score_of_first_answer": 0.008019757457077503, "min_score_of_first_answer": 0.008019757457077503, "avg_score": 0.008019757457077503, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.225224494934082, "lex_sim": 1.0}
{"question": "what is the form of mozart symphony no 40", "answer": ["symphonie funebre"], "scores": [[0.13509859144687653, 0.22912487387657166, 0.14764894545078278, 0.8848195672035217, 0.13648495078086853, 0.9324558973312378, 0.9867298603057861]], "normalized_score": [0.33836737275123596], "tokens": [["\u0120sym", "ph", "on", "ie", "\u0120fun", "eb", "re"]], "entropy": [[5.28331184387207, 0.7040466070175171, 0.898912787437439, 0.6282557845115662, 2.9994921684265137, 0.38616037368774414, 0.10423345863819122]], "avg_entropy": [1.5720590033701487], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "G minor", "score_of_biggest_cluster": 0.33836737275123596, "score_of_first_answer": 0.33836737275123596, "max_score_of_first_answer": 0.9867298603057861, "min_score_of_first_answer": 0.13509859144687653, "avg_score": 0.33836737275123596, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3844303392694576, "std_of_first_answer": 0.3844303392694576, "avg_entropy_of_first_answer": 1.5720590033701487, "lex_sim": 1.0}
{"question": "who is the director of nightmare before christmas", "answer": ["Wes Craven"], "scores": [[0.09361837804317474, 0.941484808921814, 0.9991336464881897]], "normalized_score": [0.444903701543808], "tokens": [["\u0120Wes", "\u0120Cra", "ven"]], "entropy": [[4.3343825340271, 0.4184204339981079, 0.009620941244065762]], "avg_entropy": [1.5874746364230912], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Henry Selick", "score_of_biggest_cluster": 0.444903701543808, "score_of_first_answer": 0.444903701543808, "max_score_of_first_answer": 0.9991336464881897, "min_score_of_first_answer": 0.09361837804317474, "avg_score": 0.444903701543808, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41394561833794924, "std_of_first_answer": 0.41394561833794924, "avg_entropy_of_first_answer": 1.5874746364230912, "lex_sim": 1.0}
{"question": "what state courts can order a new trial", "answer": ["Michigan state courts"], "scores": [[0.02470577321946621, 0.0026864956598728895, 0.419001966714859]], "normalized_score": [0.030297046527266502], "tokens": [["\u0120Michigan", "\u0120state", "\u0120courts"]], "entropy": [[4.301302433013916, 0.30270642042160034, 2.010097026824951]], "avg_entropy": [2.2047019600868225], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "appellate court", "score_of_biggest_cluster": 0.030297046527266502, "score_of_first_answer": 0.030297046527266502, "max_score_of_first_answer": 0.419001966714859, "min_score_of_first_answer": 0.0026864956598728895, "avg_score": 0.030297046527266502, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19127435450709063, "std_of_first_answer": 0.19127435450709063, "avg_entropy_of_first_answer": 2.2047019600868225, "lex_sim": 1.0}
{"question": "what are the functions of the human skeleton", "answer": ["support"], "scores": [[0.19121231138706207]], "normalized_score": [0.19121231138706207], "tokens": [["\u0120support"]], "entropy": [[5.074528694152832]], "avg_entropy": [5.074528694152832], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "endocrine regulation", "score_of_biggest_cluster": 0.19121231138706207, "score_of_first_answer": 0.19121231138706207, "max_score_of_first_answer": 0.19121231138706207, "min_score_of_first_answer": 0.19121231138706207, "avg_score": 0.19121231138706207, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.074528694152832, "lex_sim": 1.0}
{"question": "what network is showing the monday night football game", "answer": ["NFL"], "scores": [[0.06935115903615952]], "normalized_score": [0.06935115903615952], "tokens": [["\u0120NFL"]], "entropy": [[3.203796863555908]], "avg_entropy": [3.203796863555908], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ESPN", "score_of_biggest_cluster": 0.06935115903615952, "score_of_first_answer": 0.06935115903615952, "max_score_of_first_answer": 0.06935115903615952, "min_score_of_first_answer": 0.06935115903615952, "avg_score": 0.06935115903615952, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.203796863555908, "lex_sim": 1.0}
{"question": "disagreements involving slavery and states' rights were two of the main causes of", "answer": ["the civil war"], "scores": [[0.5804580450057983, 0.26952579617500305, 0.9890296459197998]], "normalized_score": [0.5368589162826538], "tokens": [["\u0120the", "\u0120civil", "\u0120war"]], "entropy": [[2.2660648822784424, 1.6725475788116455, 0.08510708063840866]], "avg_entropy": [1.3412398472428322], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the American Civil War", "score_of_biggest_cluster": 0.5368589162826538, "score_of_first_answer": 0.5368589162826538, "max_score_of_first_answer": 0.9890296459197998, "min_score_of_first_answer": 0.26952579617500305, "avg_score": 0.5368589162826538, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2946363872515841, "std_of_first_answer": 0.2946363872515841, "avg_entropy_of_first_answer": 1.3412398472428322, "lex_sim": 1.0}
{"question": "how many languages in harry potter translated into", "answer": ["English"], "scores": [[0.07662850618362427]], "normalized_score": [0.07662850618362427], "tokens": [["\u0120English"]], "entropy": [[4.77277135848999]], "avg_entropy": [4.77277135848999], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "over 74", "score_of_biggest_cluster": 0.07662850618362427, "score_of_first_answer": 0.07662850618362427, "max_score_of_first_answer": 0.07662850618362427, "min_score_of_first_answer": 0.07662850618362427, "avg_score": 0.07662850618362427, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.77277135848999, "lex_sim": 1.0}
{"question": "where did the butchers in the slaughterhouse cases live", "answer": ["They live in the west side of town."], "scores": [[0.009481530636548996, 0.1302083134651184, 0.7399008274078369, 0.5110385417938232, 0.005237412638962269, 0.04775087535381317, 0.587505042552948, 0.13710933923721313, 0.28113502264022827]], "normalized_score": [0.11140766739845276], "tokens": [["\u0120They", "\u0120live", "\u0120in", "\u0120the", "\u0120west", "\u0120side", "\u0120of", "\u0120town", "."]], "entropy": [[6.329350471496582, 2.9259729385375977, 1.476867914199829, 3.6814615726470947, 6.193425178527832, 2.7148547172546387, 1.8190231323242188, 3.4299678802490234, 1.2889480590820312]], "avg_entropy": [3.3177635404798718], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "New Orleans", "score_of_biggest_cluster": 0.11140766739845276, "score_of_first_answer": 0.11140766739845276, "max_score_of_first_answer": 0.7399008274078369, "min_score_of_first_answer": 0.005237412638962269, "avg_score": 0.11140766739845276, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25913607516597864, "std_of_first_answer": 0.25913607516597864, "avg_entropy_of_first_answer": 3.3177635404798718, "lex_sim": 1.0}
{"question": "who sang the song good morning good morning", "answer": ["Good morning good morning"], "scores": [[0.01754697412252426, 0.5254172682762146, 0.15171010792255402, 0.9592469930648804]], "normalized_score": [0.19138717651367188], "tokens": [["\u0120Good", "\u0120morning", "\u0120good", "\u0120morning"]], "entropy": [[6.782829761505127, 1.0373268127441406, 3.3039817810058594, 0.3181084990501404]], "avg_entropy": [2.860561713576317], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gene Kelly", "score_of_biggest_cluster": 0.19138717651367188, "score_of_first_answer": 0.19138717651367188, "max_score_of_first_answer": 0.9592469930648804, "min_score_of_first_answer": 0.01754697412252426, "avg_score": 0.19138717651367188, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36595013325810627, "std_of_first_answer": 0.36595013325810627, "avg_entropy_of_first_answer": 2.860561713576317, "lex_sim": 1.0}
{"question": "where did they get the vibranium to make captain americas shield", "answer": ["Canada"], "scores": [[0.017171064391732216]], "normalized_score": [0.017171068117022514], "tokens": [["\u0120Canada"]], "entropy": [[6.495055198669434]], "avg_entropy": [6.495055198669434], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wakanda", "score_of_biggest_cluster": 0.017171068117022514, "score_of_first_answer": 0.017171068117022514, "max_score_of_first_answer": 0.017171064391732216, "min_score_of_first_answer": 0.017171064391732216, "avg_score": 0.017171068117022514, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.495055198669434, "lex_sim": 1.0}
{"question": "element named after fictional planet from which superman came", "answer": ["Krypton"], "scores": [[0.8260334730148315, 0.8841744661331177]], "normalized_score": [0.8546096682548523], "tokens": [["\u0120Krypt", "on"]], "entropy": [[1.1485605239868164, 0.39773720502853394]], "avg_entropy": [0.7731488645076752], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kryptonite", "score_of_biggest_cluster": 0.8546096682548523, "score_of_first_answer": 0.8546096682548523, "max_score_of_first_answer": 0.8841744661331177, "min_score_of_first_answer": 0.8260334730148315, "avg_score": 0.8546096682548523, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.029070496559143066, "std_of_first_answer": 0.029070496559143066, "avg_entropy_of_first_answer": 0.7731488645076752, "lex_sim": 1.0}
{"question": "right to property according to the constitution of india is a", "answer": ["Right to property"], "scores": [[0.05231591686606407, 0.5780662894248962, 0.2330871969461441]], "normalized_score": [0.1917387843132019], "tokens": [["\u0120Right", "\u0120to", "\u0120property"]], "entropy": [[5.9091644287109375, 1.8387157917022705, 2.4053850173950195]], "avg_entropy": [3.3844217459360757], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "constitutional right", "score_of_biggest_cluster": 0.1917387843132019, "score_of_first_answer": 0.1917387843132019, "max_score_of_first_answer": 0.5780662894248962, "min_score_of_first_answer": 0.05231591686606407, "avg_score": 0.1917387843132019, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21809841921965337, "std_of_first_answer": 0.21809841921965337, "avg_entropy_of_first_answer": 3.3844217459360757, "lex_sim": 1.0}
{"question": "where does the story the great gatsby take place", "answer": ["New York"], "scores": [[0.3235321044921875, 0.8477126359939575]], "normalized_score": [0.5237005352973938], "tokens": [["\u0120New", "\u0120York"]], "entropy": [[4.161621570587158, 0.6524767875671387]], "avg_entropy": [2.4070491790771484], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Long Island of 1922", "score_of_biggest_cluster": 0.5237005352973938, "score_of_first_answer": 0.5237005352973938, "max_score_of_first_answer": 0.8477126359939575, "min_score_of_first_answer": 0.3235321044921875, "avg_score": 0.5237005352973938, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.262090265750885, "std_of_first_answer": 0.262090265750885, "avg_entropy_of_first_answer": 2.4070491790771484, "lex_sim": 1.0}
{"question": "who sings the skye boat song on outlander", "answer": ["SHe would like to say hello"], "scores": [[0.004527865443378687, 0.005764143541455269, 0.0016446624649688601, 0.018479209393262863, 0.5138765573501587, 0.041795242577791214, 0.10813924670219421]], "normalized_score": [0.021067962050437927], "tokens": [["\u0120S", "He", "\u0120would", "\u0120like", "\u0120to", "\u0120say", "\u0120hello"]], "entropy": [[6.61060905456543, 4.092312812805176, 4.417539596557617, 4.09720516204834, 2.056055784225464, 4.323758602142334, 4.982484817504883]], "avg_entropy": [4.36856654712132], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bear McCreary", "score_of_biggest_cluster": 0.021067962050437927, "score_of_first_answer": 0.021067962050437927, "max_score_of_first_answer": 0.5138765573501587, "min_score_of_first_answer": 0.0016446624649688601, "avg_score": 0.021067962050437927, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.172816096098188, "std_of_first_answer": 0.172816096098188, "avg_entropy_of_first_answer": 4.36856654712132, "lex_sim": 1.0}
{"question": "when do you declare honors in contract bridge", "answer": ["first bid makes the announcement and then play"], "scores": [[0.002202121540904045, 0.01270572654902935, 0.0031540892086923122, 0.2656985819339752, 0.017451496794819832, 0.02206583134829998, 0.08901634067296982, 0.017183726653456688]], "normalized_score": [0.01851528510451317], "tokens": [["\u0120first", "\u0120bid", "\u0120makes", "\u0120the", "\u0120announcement", "\u0120and", "\u0120then", "\u0120play"]], "entropy": [[5.579387187957764, 5.357240676879883, 2.785120964050293, 3.78136944770813, 4.1754655838012695, 1.232431173324585, 4.211627006530762, 4.123356819152832]], "avg_entropy": [3.9057498574256897], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "any time after the auction", "score_of_biggest_cluster": 0.01851528510451317, "score_of_first_answer": 0.01851528510451317, "max_score_of_first_answer": 0.2656985819339752, "min_score_of_first_answer": 0.002202121540904045, "avg_score": 0.01851528510451317, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08420674309901376, "std_of_first_answer": 0.08420674309901376, "avg_entropy_of_first_answer": 3.9057498574256897, "lex_sim": 1.0}
{"question": "when do dwight and angela start dating again", "answer": ["january 2012"], "scores": [[0.0007379851886071265, 0.8878310918807983, 0.0013824617490172386]], "normalized_score": [0.009675583802163601], "tokens": [["\u0120jan", "uary", "\u01202012"]], "entropy": [[5.506595134735107, 0.7954664826393127, 3.4127750396728516]], "avg_entropy": [3.238278885682424], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "In \"A.A.R.M.\"", "score_of_biggest_cluster": 0.009675583802163601, "score_of_first_answer": 0.009675583802163601, "max_score_of_first_answer": 0.8878310918807983, "min_score_of_first_answer": 0.0007379851886071265, "avg_score": 0.009675583802163601, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4180278790747164, "std_of_first_answer": 0.4180278790747164, "avg_entropy_of_first_answer": 3.238278885682424, "lex_sim": 1.0}
{"question": "who was the president of pakistan during 1971 war", "answer": ["Yahya khan"], "scores": [[0.5868725180625916, 0.9790584444999695, 0.05418150871992111, 0.5558698177337646]], "normalized_score": [0.36269715428352356], "tokens": [["\u0120Yah", "ya", "\u0120k", "han"]], "entropy": [[2.07061767578125, 0.1549113392829895, 1.632521390914917, 2.6981425285339355]], "avg_entropy": [1.639048233628273], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "President Yahya Khan", "score_of_biggest_cluster": 0.36269715428352356, "score_of_first_answer": 0.36269715428352356, "max_score_of_first_answer": 0.9790584444999695, "min_score_of_first_answer": 0.05418150871992111, "avg_score": 0.36269715428352356, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3283203275119689, "std_of_first_answer": 0.3283203275119689, "avg_entropy_of_first_answer": 1.639048233628273, "lex_sim": 1.0}
{"question": "what was the first video game for nintendo", "answer": ["space invaders"], "scores": [[0.0022290993947535753, 0.8297926783561707]], "normalized_score": [0.043008025735616684], "tokens": [["\u0120space", "\u0120invaders"]], "entropy": [[5.829398155212402, 1.1770298480987549]], "avg_entropy": [3.5032140016555786], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Magnavox Odyssey", "score_of_biggest_cluster": 0.043008025735616684, "score_of_first_answer": 0.043008025735616684, "max_score_of_first_answer": 0.8297926783561707, "min_score_of_first_answer": 0.0022290993947535753, "avg_score": 0.043008025735616684, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41378178948070854, "std_of_first_answer": 0.41378178948070854, "avg_entropy_of_first_answer": 3.5032140016555786, "lex_sim": 1.0}
{"question": "who has climbed mount everest the number of times", "answer": ["7"], "scores": [[0.005828981287777424]], "normalized_score": [0.005828980356454849], "tokens": [["\u01207"]], "entropy": [[5.0322980880737305]], "avg_entropy": [5.0322980880737305], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Phurba Tashi Sherpa", "score_of_biggest_cluster": 0.005828980356454849, "score_of_first_answer": 0.005828980356454849, "max_score_of_first_answer": 0.005828981287777424, "min_score_of_first_answer": 0.005828981287777424, "avg_score": 0.005828980356454849, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.0322980880737305, "lex_sim": 1.0}
{"question": "who won the first celebrity big brother on channel 5", "answer": ["Simon Franks"], "scores": [[0.006069227587431669, 0.0008668542141094804]], "normalized_score": [0.0022937164176255465], "tokens": [["\u0120Simon", "\u0120Franks"]], "entropy": [[6.810598850250244, 3.544391393661499]], "avg_entropy": [5.177495121955872], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jack Dee", "score_of_biggest_cluster": 0.0022937164176255465, "score_of_first_answer": 0.0022937164176255465, "max_score_of_first_answer": 0.006069227587431669, "min_score_of_first_answer": 0.0008668542141094804, "avg_score": 0.0022937164176255465, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0026011866866610944, "std_of_first_answer": 0.0026011866866610944, "avg_entropy_of_first_answer": 5.177495121955872, "lex_sim": 1.0}
{"question": "who wrote the song fly me to the moon", "answer": ["Frank Sinatra"], "scores": [[0.2311682105064392, 0.14052067697048187, 0.9912737011909485]], "normalized_score": [0.31814172863960266], "tokens": [["\u0120Frank", "\u0120Sin", "atra"]], "entropy": [[4.942276477813721, 6.104047775268555, 0.07165128737688065]], "avg_entropy": [3.705991846819719], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bart Howard", "score_of_biggest_cluster": 0.31814172863960266, "score_of_first_answer": 0.31814172863960266, "max_score_of_first_answer": 0.9912737011909485, "min_score_of_first_answer": 0.14052067697048187, "avg_score": 0.31814172863960266, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3814822029524606, "std_of_first_answer": 0.3814822029524606, "avg_entropy_of_first_answer": 3.705991846819719, "lex_sim": 1.0}
{"question": "who was defeated in the french and indian war", "answer": ["Louis XIV of France"], "scores": [[0.031024163588881493, 0.29284805059432983, 0.0828540176153183, 0.942532479763031]], "normalized_score": [0.16320672631263733], "tokens": [["\u0120Louis", "\u0120XIV", "\u0120of", "\u0120France"]], "entropy": [[4.796514987945557, 2.9938058853149414, 1.1903811693191528, 0.39488863945007324]], "avg_entropy": [2.343897670507431], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the French", "score_of_biggest_cluster": 0.16320672631263733, "score_of_first_answer": 0.16320672631263733, "max_score_of_first_answer": 0.942532479763031, "min_score_of_first_answer": 0.031024163588881493, "avg_score": 0.16320672631263733, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3629152709053954, "std_of_first_answer": 0.3629152709053954, "avg_entropy_of_first_answer": 2.343897670507431, "lex_sim": 1.0}
{"question": "who headed the 7th central pay commission of india", "answer": ["T N Kiran Kumar"], "scores": [[0.023674247786402702, 0.029515212401747704, 0.006454881746321917, 0.9280878305435181, 0.5723032355308533]], "normalized_score": [0.07514241337776184], "tokens": [["\u0120T", "\u0120N", "\u0120Kir", "an", "\u0120Kumar"]], "entropy": [[5.3529157638549805, 2.722381114959717, 4.375467777252197, 0.38736677169799805, 1.1564031839370728]], "avg_entropy": [2.798906922340393], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Justice A.K Mathur", "score_of_biggest_cluster": 0.07514241337776184, "score_of_first_answer": 0.07514241337776184, "max_score_of_first_answer": 0.9280878305435181, "min_score_of_first_answer": 0.006454881746321917, "avg_score": 0.07514241337776184, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3751290985843473, "std_of_first_answer": 0.3751290985843473, "avg_entropy_of_first_answer": 2.798906922340393, "lex_sim": 1.0}
{"question": "who sings only love can break your heart", "answer": ["Elton john"], "scores": [[0.051212772727012634, 0.9033346772193909, 0.04991539567708969]], "normalized_score": [0.13217638432979584], "tokens": [["\u0120El", "ton", "\u0120john"]], "entropy": [[6.071381568908691, 0.6577937602996826, 0.38886845111846924]], "avg_entropy": [2.3726812601089478], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Neil Young", "score_of_biggest_cluster": 0.13217638432979584, "score_of_first_answer": 0.13217638432979584, "max_score_of_first_answer": 0.9033346772193909, "min_score_of_first_answer": 0.04991539567708969, "avg_score": 0.13217638432979584, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40200026166535224, "std_of_first_answer": 0.40200026166535224, "avg_entropy_of_first_answer": 2.3726812601089478, "lex_sim": 1.0}
{"question": "when were the winnie the pooh books written", "answer": ["1947"], "scores": [[0.004770522005856037]], "normalized_score": [0.0047705210745334625], "tokens": [["\u01201947"]], "entropy": [[4.978657245635986]], "avg_entropy": [4.978657245635986], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1924", "score_of_biggest_cluster": 0.0047705210745334625, "score_of_first_answer": 0.0047705210745334625, "max_score_of_first_answer": 0.004770522005856037, "min_score_of_first_answer": 0.004770522005856037, "avg_score": 0.0047705210745334625, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.978657245635986, "lex_sim": 1.0}
{"question": "where are they playing the superbowl at this year", "answer": ["New Orleans"], "scores": [[0.13926216959953308, 0.6695178151130676]], "normalized_score": [0.30534979701042175], "tokens": [["\u0120New", "\u0120Orleans"]], "entropy": [[4.942440986633301, 1.2879515886306763]], "avg_entropy": [3.1151962876319885], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Minneapolis, Minnesota", "score_of_biggest_cluster": 0.30534979701042175, "score_of_first_answer": 0.30534979701042175, "max_score_of_first_answer": 0.6695178151130676, "min_score_of_first_answer": 0.13926216959953308, "avg_score": 0.30534979701042175, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2651278227567673, "std_of_first_answer": 0.2651278227567673, "avg_entropy_of_first_answer": 3.1151962876319885, "lex_sim": 1.0}
{"question": "who developed the central processing unit (cpu)", "answer": ["Paul Rascia"], "scores": [[0.003253635950386524, 0.02386375516653061, 0.008842184208333492, 0.06978562474250793]], "normalized_score": [0.014794771559536457], "tokens": [["\u0120Paul", "\u0120R", "asc", "ia"]], "entropy": [[4.3611555099487305, 4.547510147094727, 2.1063895225524902, 4.255951881408691]], "avg_entropy": [3.8177517652511597], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John von Neumann", "score_of_biggest_cluster": 0.014794771559536457, "score_of_first_answer": 0.014794771559536457, "max_score_of_first_answer": 0.06978562474250793, "min_score_of_first_answer": 0.003253635950386524, "avg_score": 0.014794771559536457, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.026137954995941393, "std_of_first_answer": 0.026137954995941393, "avg_entropy_of_first_answer": 3.8177517652511597, "lex_sim": 1.0}
{"question": "who sings i'll sleep when i'm dead", "answer": ["Elvis"], "scores": [[0.014579590409994125]], "normalized_score": [0.01457958947867155], "tokens": [["\u0120Elvis"]], "entropy": [[6.106413841247559]], "avg_entropy": [6.106413841247559], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Warren Zevon", "score_of_biggest_cluster": 0.01457958947867155, "score_of_first_answer": 0.01457958947867155, "max_score_of_first_answer": 0.014579590409994125, "min_score_of_first_answer": 0.014579590409994125, "avg_score": 0.01457958947867155, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.106413841247559, "lex_sim": 1.0}
{"question": "who is emma dating in once upon a time", "answer": ["Emma Roberts"], "scores": [[0.41177496314048767, 0.021018121391534805]], "normalized_score": [0.09303083270788193], "tokens": [["\u0120Emma", "\u0120Roberts"]], "entropy": [[4.397465705871582, 1.8071167469024658]], "avg_entropy": [3.102291226387024], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hook", "score_of_biggest_cluster": 0.09303083270788193, "score_of_first_answer": 0.09303083270788193, "max_score_of_first_answer": 0.41177496314048767, "min_score_of_first_answer": 0.021018121391534805, "avg_score": 0.09303083270788193, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19537842087447643, "std_of_first_answer": 0.19537842087447643, "avg_entropy_of_first_answer": 3.102291226387024, "lex_sim": 1.0}
{"question": "when did the log flume closed at alton towers", "answer": ["1992"], "scores": [[0.004035511054098606]], "normalized_score": [0.004035511054098606], "tokens": [["\u01201992"]], "entropy": [[5.245157241821289]], "avg_entropy": [5.245157241821289], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "10\u00a0October\u00a02015", "score_of_biggest_cluster": 0.004035511054098606, "score_of_first_answer": 0.004035511054098606, "max_score_of_first_answer": 0.004035511054098606, "min_score_of_first_answer": 0.004035511054098606, "avg_score": 0.004035511054098606, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.245157241821289, "lex_sim": 1.0}
{"question": "baseball teams who have not won a world series", "answer": ["New York Yankees"], "scores": [[0.043169427663087845, 0.9583961963653564, 0.8010846376419067]], "normalized_score": [0.3212180435657501], "tokens": [["\u0120New", "\u0120York", "\u0120Yankees"]], "entropy": [[4.946282863616943, 0.3257288932800293, 0.9447882175445557]], "avg_entropy": [2.0722666581471763], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Washington Nationals", "score_of_biggest_cluster": 0.3212180435657501, "score_of_first_answer": 0.3212180435657501, "max_score_of_first_answer": 0.9583961963653564, "min_score_of_first_answer": 0.043169427663087845, "avg_score": 0.3212180435657501, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3995584271243046, "std_of_first_answer": 0.3995584271243046, "avg_entropy_of_first_answer": 2.0722666581471763, "lex_sim": 1.0}
{"question": "who played ricky in secret life of the american teenager", "answer": ["Ricky Gervais"], "scores": [[0.2084122896194458, 0.23991608619689941, 0.9898932576179504, 0.9952214360237122]], "normalized_score": [0.47111040353775024], "tokens": [["\u0120Ricky", "\u0120G", "erv", "ais"]], "entropy": [[5.5071821212768555, 3.8813419342041016, 0.086082324385643, 0.04728572443127632]], "avg_entropy": [2.380473026074469], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Daren Maxwell Kagasoff", "score_of_biggest_cluster": 0.47111040353775024, "score_of_first_answer": 0.47111040353775024, "max_score_of_first_answer": 0.9952214360237122, "min_score_of_first_answer": 0.2084122896194458, "avg_score": 0.47111040353775024, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38436261720077985, "std_of_first_answer": 0.38436261720077985, "avg_entropy_of_first_answer": 2.380473026074469, "lex_sim": 1.0}
{"question": "who rebuilt the temple after the babylonian captivity", "answer": ["Cyrus the great"], "scores": [[0.07916011661291122, 0.4826110303401947, 0.3670068383216858]], "normalized_score": [0.24113444983959198], "tokens": [["\u0120Cyrus", "\u0120the", "\u0120great"]], "entropy": [[3.8824801445007324, 1.4285928010940552, 1.1432595252990723]], "avg_entropy": [2.15144415696462], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Herod", "score_of_biggest_cluster": 0.24113444983959198, "score_of_first_answer": 0.24113444983959198, "max_score_of_first_answer": 0.4826110303401947, "min_score_of_first_answer": 0.07916011661291122, "avg_score": 0.24113444983959198, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.16963775267647396, "std_of_first_answer": 0.16963775267647396, "avg_entropy_of_first_answer": 2.15144415696462, "lex_sim": 1.0}
{"question": "who is the first president to be impeached", "answer": ["Andrew Johnson"], "scores": [[0.8522794246673584, 0.9446243643760681]], "normalized_score": [0.8972646594047546], "tokens": [["\u0120Andrew", "\u0120Johnson"]], "entropy": [[0.9078933596611023, 0.3946880102157593]], "avg_entropy": [0.6512906849384308], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Andrew Johnson", "score_of_biggest_cluster": 0.8972646594047546, "score_of_first_answer": 0.8972646594047546, "max_score_of_first_answer": 0.9446243643760681, "min_score_of_first_answer": 0.8522794246673584, "avg_score": 0.8972646594047546, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.04617246985435486, "std_of_first_answer": 0.04617246985435486, "avg_entropy_of_first_answer": 0.6512906849384308, "lex_sim": 1.0}
{"question": "most home runs by 2 teammates in a season", "answer": ["24"], "scores": [[0.008911896497011185]], "normalized_score": [0.008911894634366035], "tokens": [["\u012024"]], "entropy": [[5.905489921569824]], "avg_entropy": [5.905489921569824], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "115", "score_of_biggest_cluster": 0.008911894634366035, "score_of_first_answer": 0.008911894634366035, "max_score_of_first_answer": 0.008911896497011185, "min_score_of_first_answer": 0.008911896497011185, "avg_score": 0.008911894634366035, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.905489921569824, "lex_sim": 1.0}
{"question": "who wrote antigone and what are the dates of his birth and death", "answer": ["Sophocles and April 4, 495 BCE"], "scores": [[0.8975149989128113, 0.9993577599525452, 0.9931079149246216, 0.10478530079126358, 0.012190706096589565, 0.0642620250582695, 0.44786715507507324, 0.42874857783317566, 0.748035192489624, 0.06422632932662964]], "normalized_score": [0.2414921075105667], "tokens": [["\u0120Soph", "oc", "les", "\u0120and", "\u0120April", "\u01204", ",", "\u01204", "95", "\u0120BCE"]], "entropy": [[0.8240016102790833, 0.005790477152913809, 0.050739631056785583, 1.7568566799163818, 5.654536724090576, 3.6123390197753906, 1.3913025856018066, 3.284939765930176, 1.3204138278961182, 2.209770441055298]], "avg_entropy": [2.011069076275453], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "winter 406/5 BC", "score_of_biggest_cluster": 0.2414921075105667, "score_of_first_answer": 0.2414921075105667, "max_score_of_first_answer": 0.9993577599525452, "min_score_of_first_answer": 0.012190706096589565, "avg_score": 0.2414921075105667, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3857474904221461, "std_of_first_answer": 0.3857474904221461, "avg_entropy_of_first_answer": 2.011069076275453, "lex_sim": 1.0}
{"question": "who is the original singer of fool if you think it's over", "answer": ["Eliza Doolittle"], "scores": [[0.049974337220191956, 0.00527738593518734, 0.8140687346458435, 0.4811479151248932, 0.9985735416412354]], "normalized_score": [0.1594766527414322], "tokens": [["\u0120El", "iza", "\u0120D", "ool", "ittle"]], "entropy": [[5.975620269775391, 1.1145137548446655, 1.3385899066925049, 0.9694001078605652, 0.013223294168710709]], "avg_entropy": [1.8822694666683675], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chris Rea", "score_of_biggest_cluster": 0.1594766527414322, "score_of_first_answer": 0.1594766527414322, "max_score_of_first_answer": 0.9985735416412354, "min_score_of_first_answer": 0.00527738593518734, "avg_score": 0.1594766527414322, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39756409317917357, "std_of_first_answer": 0.39756409317917357, "avg_entropy_of_first_answer": 1.8822694666683675, "lex_sim": 1.0}
{"question": "who plays ser davos in game of thrones", "answer": ["Emilia Clarke"], "scores": [[0.08638278394937515, 0.9935446977615356, 0.9571152329444885]], "normalized_score": [0.4347032904624939], "tokens": [["\u0120Em", "ilia", "\u0120Clarke"]], "entropy": [[4.058321952819824, 0.05640926957130432, 0.41528812050819397]], "avg_entropy": [1.5100064476331074], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Liam Cunningham", "score_of_biggest_cluster": 0.4347032904624939, "score_of_first_answer": 0.4347032904624939, "max_score_of_first_answer": 0.9935446977615356, "min_score_of_first_answer": 0.08638278394937515, "avg_score": 0.4347032904624939, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4193175462729391, "std_of_first_answer": 0.4193175462729391, "avg_entropy_of_first_answer": 1.5100064476331074, "lex_sim": 1.0}
{"question": "natural resources can be divided into what two categories", "answer": ["natural and social"], "scores": [[0.039144255220890045, 0.6972520351409912, 0.006670206785202026]], "normalized_score": [0.056675996631383896], "tokens": [["\u0120natural", "\u0120and", "\u0120social"]], "entropy": [[5.7706685066223145, 1.6732103824615479, 3.5977039337158203]], "avg_entropy": [3.680527607599894], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Biotic", "score_of_biggest_cluster": 0.056675996631383896, "score_of_first_answer": 0.056675996631383896, "max_score_of_first_answer": 0.6972520351409912, "min_score_of_first_answer": 0.006670206785202026, "avg_score": 0.056675996631383896, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31816551867583925, "std_of_first_answer": 0.31816551867583925, "avg_entropy_of_first_answer": 3.680527607599894, "lex_sim": 1.0}
{"question": "what is the 180 degree line of longitude called", "answer": ["meridian"], "scores": [[0.43469753861427307, 0.9674009084701538]], "normalized_score": [0.6484803557395935], "tokens": [["\u0120mer", "idian"]], "entropy": [[3.440239191055298, 0.16289353370666504]], "avg_entropy": [1.8015663623809814], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "180th meridian or antimeridian", "score_of_biggest_cluster": 0.6484803557395935, "score_of_first_answer": 0.6484803557395935, "max_score_of_first_answer": 0.9674009084701538, "min_score_of_first_answer": 0.43469753861427307, "avg_score": 0.6484803557395935, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26635168492794037, "std_of_first_answer": 0.26635168492794037, "avg_entropy_of_first_answer": 1.8015663623809814, "lex_sim": 1.0}
{"question": "when boy meets girl waiting for a star to fall", "answer": ["twilight"], "scores": [[0.00025580721558071673]], "normalized_score": [0.0002558071573730558], "tokens": [["\u0120twilight"]], "entropy": [[6.810801029205322]], "avg_entropy": [6.810801029205322], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1988", "score_of_biggest_cluster": 0.0002558071573730558, "score_of_first_answer": 0.0002558071573730558, "max_score_of_first_answer": 0.00025580721558071673, "min_score_of_first_answer": 0.00025580721558071673, "avg_score": 0.0002558071573730558, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.810801029205322, "lex_sim": 1.0}
{"question": "when was the last time giants won superbowl", "answer": ["January 24, 2012"], "scores": [[0.0852656364440918, 0.035316821187734604, 0.3330089747905731, 0.029317881911993027]], "normalized_score": [0.07363530993461609], "tokens": [["\u0120January", "\u012024", ",", "\u01202012"]], "entropy": [[4.428552627563477, 4.022090911865234, 1.3133295774459839, 3.5225319862365723]], "avg_entropy": [3.3216262757778168], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2011", "score_of_biggest_cluster": 0.07363530993461609, "score_of_first_answer": 0.07363530993461609, "max_score_of_first_answer": 0.3330089747905731, "min_score_of_first_answer": 0.029317881911993027, "avg_score": 0.07363530993461609, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1244705493682826, "std_of_first_answer": 0.1244705493682826, "avg_entropy_of_first_answer": 3.3216262757778168, "lex_sim": 1.0}
{"question": "who is the presiding officer of legislative council in india", "answer": ["Dinesh Wagelkar"], "scores": [[0.0035745808854699135, 0.18779467046260834, 0.99637371301651, 0.010040906257927418, 0.0024161585606634617, 0.2628549635410309]], "normalized_score": [0.04027084633708], "tokens": [["\u0120D", "ines", "h", "\u0120Wag", "el", "kar"]], "entropy": [[5.750461578369141, 3.1689510345458984, 0.03823787719011307, 3.70149302482605, 1.408586859703064, 3.5159997940063477]], "avg_entropy": [2.9306216947734356], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chairman", "score_of_biggest_cluster": 0.04027084633708, "score_of_first_answer": 0.04027084633708, "max_score_of_first_answer": 0.99637371301651, "min_score_of_first_answer": 0.0024161585606634617, "avg_score": 0.04027084633708, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3513034291659643, "std_of_first_answer": 0.3513034291659643, "avg_entropy_of_first_answer": 2.9306216947734356, "lex_sim": 1.0}
{"question": "what is the enrollment at university of miami", "answer": ["3079"], "scores": [[0.012005871161818504, 0.0006565585499629378]], "normalized_score": [0.002807589014992118], "tokens": [["\u012030", "79"]], "entropy": [[5.645618438720703, 0.7896029949188232]], "avg_entropy": [3.217610716819763], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "16,801", "score_of_biggest_cluster": 0.002807589014992118, "score_of_first_answer": 0.002807589014992118, "max_score_of_first_answer": 0.012005871161818504, "min_score_of_first_answer": 0.0006565585499629378, "avg_score": 0.002807589014992118, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.005674656305927783, "std_of_first_answer": 0.005674656305927783, "avg_entropy_of_first_answer": 3.217610716819763, "lex_sim": 1.0}
{"question": "who has won the most world series in baseball", "answer": ["Babe Ruth"], "scores": [[0.04366292804479599, 0.9827083945274353]], "normalized_score": [0.20714227855205536], "tokens": [["\u0120Babe", "\u0120Ruth"]], "entropy": [[5.465987205505371, 0.17592753469944]], "avg_entropy": [2.8209573701024055], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the New York Yankees", "score_of_biggest_cluster": 0.20714227855205536, "score_of_first_answer": 0.20714227855205536, "max_score_of_first_answer": 0.9827083945274353, "min_score_of_first_answer": 0.04366292804479599, "avg_score": 0.20714227855205536, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46952273324131966, "std_of_first_answer": 0.46952273324131966, "avg_entropy_of_first_answer": 2.8209573701024055, "lex_sim": 1.0}
{"question": "when did cristiano ronaldo go to manchester united", "answer": ["December 6th 2009"], "scores": [[0.015704834833741188, 0.02619132027029991, 0.7315617203712463, 0.013000705279409885]], "normalized_score": [0.044473592191934586], "tokens": [["\u0120December", "\u01206", "th", "\u01202009"]], "entropy": [[5.053243637084961, 3.892643928527832, 0.9500836133956909, 2.2530927658081055]], "avg_entropy": [3.0372659862041473], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2003", "score_of_biggest_cluster": 0.044473592191934586, "score_of_first_answer": 0.044473592191934586, "max_score_of_first_answer": 0.7315617203712463, "min_score_of_first_answer": 0.013000705279409885, "avg_score": 0.044473592191934586, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3088911302280639, "std_of_first_answer": 0.3088911302280639, "avg_entropy_of_first_answer": 3.0372659862041473, "lex_sim": 1.0}
{"question": "who won the mens curling in the olympics 2018", "answer": ["Scotland"], "scores": [[0.031141942366957664]], "normalized_score": [0.031141944229602814], "tokens": [["\u0120Scotland"]], "entropy": [[4.414944648742676]], "avg_entropy": [4.414944648742676], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "United States", "score_of_biggest_cluster": 0.031141944229602814, "score_of_first_answer": 0.031141944229602814, "max_score_of_first_answer": 0.031141942366957664, "min_score_of_first_answer": 0.031141942366957664, "avg_score": 0.031141944229602814, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.414944648742676, "lex_sim": 1.0}
{"question": "when were the first general elections held in independent india", "answer": ["May 13, 1950"], "scores": [[0.010625296272337437, 0.034880779683589935, 0.421846866607666, 0.09195365011692047]], "normalized_score": [0.06157618388533592], "tokens": [["\u0120May", "\u012013", ",", "\u01201950"]], "entropy": [[4.360270977020264, 4.090131759643555, 1.1925684213638306, 2.983884811401367]], "avg_entropy": [3.156713992357254], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1951\u201352", "score_of_biggest_cluster": 0.06157618388533592, "score_of_first_answer": 0.06157618388533592, "max_score_of_first_answer": 0.421846866607666, "min_score_of_first_answer": 0.010625296272337437, "avg_score": 0.06157618388533592, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1654794943450468, "std_of_first_answer": 0.1654794943450468, "avg_entropy_of_first_answer": 3.156713992357254, "lex_sim": 1.0}
{"question": "when did they replace lead with graphite in pencils", "answer": ["1880"], "scores": [[0.01914115808904171]], "normalized_score": [0.01914115622639656], "tokens": [["\u01201880"]], "entropy": [[5.444870948791504]], "avg_entropy": [5.444870948791504], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "never contained the element lead", "score_of_biggest_cluster": 0.01914115622639656, "score_of_first_answer": 0.01914115622639656, "max_score_of_first_answer": 0.01914115808904171, "min_score_of_first_answer": 0.01914115808904171, "avg_score": 0.01914115622639656, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.444870948791504, "lex_sim": 1.0}
{"question": "the concept of micro financing was developed by who in 1976", "answer": ["microfinance"], "scores": [[0.009896471165120602, 0.3168985843658447, 0.9936328530311584]], "normalized_score": [0.1460636556148529], "tokens": [["\u0120micro", "f", "inance"]], "entropy": [[5.2428693771362305, 2.7322134971618652, 0.07463011145591736]], "avg_entropy": [2.6832376619180045], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Muhammad Yunus", "score_of_biggest_cluster": 0.1460636556148529, "score_of_first_answer": 0.1460636556148529, "max_score_of_first_answer": 0.9936328530311584, "min_score_of_first_answer": 0.009896471165120602, "avg_score": 0.1460636556148529, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4109550985073363, "std_of_first_answer": 0.4109550985073363, "avg_entropy_of_first_answer": 2.6832376619180045, "lex_sim": 1.0}
{"question": "where did the battle of bonhomme richard take place", "answer": ["The Battle of the Orchy"], "scores": [[0.028244907036423683, 0.3450155556201935, 0.9489952921867371, 0.3639386296272278, 0.0016030463157221675, 0.0014772546710446477]], "normalized_score": [0.044693589210510254], "tokens": [["\u0120The", "\u0120Battle", "\u0120of", "\u0120the", "\u0120Or", "chy"]], "entropy": [[6.658225059509277, 4.586091995239258, 0.2737307846546173, 4.276907444000244, 4.766299247741699, 2.7776966094970703]], "avg_entropy": [3.8898251901070275], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "near Flamborough Head", "score_of_biggest_cluster": 0.044693589210510254, "score_of_first_answer": 0.044693589210510254, "max_score_of_first_answer": 0.9489952921867371, "min_score_of_first_answer": 0.0014772546710446477, "avg_score": 0.044693589210510254, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33597457509757045, "std_of_first_answer": 0.33597457509757045, "avg_entropy_of_first_answer": 3.8898251901070275, "lex_sim": 1.0}
{"question": "who is command sergeant major of the army", "answer": ["Mike Huggins"], "scores": [[0.004712605848908424, 0.010918920859694481, 0.03270470350980759, 0.9469320774078369]], "normalized_score": [0.035529766231775284], "tokens": [["\u0120Mike", "\u0120H", "ugg", "ins"]], "entropy": [[6.060141086578369, 6.541503429412842, 4.9973063468933105, 0.3704145848751068]], "avg_entropy": [4.492341361939907], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Daniel A. Dailey", "score_of_biggest_cluster": 0.035529766231775284, "score_of_first_answer": 0.035529766231775284, "max_score_of_first_answer": 0.9469320774078369, "min_score_of_first_answer": 0.004712605848908424, "avg_score": 0.035529766231775284, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4031909092790008, "std_of_first_answer": 0.4031909092790008, "avg_entropy_of_first_answer": 4.492341361939907, "lex_sim": 1.0}
{"question": "who was the first executive president of guyana", "answer": ["Arthur J. Mouton Jr"], "scores": [[0.012444620952010155, 0.016678404062986374, 0.42506322264671326, 0.04388824477791786, 0.011404304765164852, 0.8772144317626953, 0.04056573659181595]], "normalized_score": [0.055249013006687164], "tokens": [["\u0120Arthur", "\u0120J", ".", "\u0120M", "out", "on", "\u0120Jr"]], "entropy": [[6.299083709716797, 5.836967468261719, 4.357143402099609, 6.102171897888184, 3.662349224090576, 0.8052682876586914, 1.1188653707504272]], "avg_entropy": [4.025978480066572], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Arthur Chung", "score_of_biggest_cluster": 0.055249013006687164, "score_of_first_answer": 0.055249013006687164, "max_score_of_first_answer": 0.8772144317626953, "min_score_of_first_answer": 0.011404304765164852, "avg_score": 0.055249013006687164, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30782857452966406, "std_of_first_answer": 0.30782857452966406, "avg_entropy_of_first_answer": 4.025978480066572, "lex_sim": 1.0}
{"question": "who wrote the song rainy days and mondays", "answer": ["Ben E King"], "scores": [[0.006036331411451101, 0.7040051221847534, 0.20374007523059845]], "normalized_score": [0.09531073272228241], "tokens": [["\u0120Ben", "\u0120E", "\u0120King"]], "entropy": [[6.590996742248535, 2.2347335815429688, 1.244786262512207]], "avg_entropy": [3.356838862101237], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paul Williams", "score_of_biggest_cluster": 0.09531073272228241, "score_of_first_answer": 0.09531073272228241, "max_score_of_first_answer": 0.7040051221847534, "min_score_of_first_answer": 0.006036331411451101, "avg_score": 0.09531073272228241, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2937331215799085, "std_of_first_answer": 0.2937331215799085, "avg_entropy_of_first_answer": 3.356838862101237, "lex_sim": 1.0}
{"question": "what is the snow on mars made of", "answer": ["dust particles and frozen water"], "scores": [[0.014447256922721863, 0.00825407262891531, 0.025930844247341156, 0.18870402872562408, 0.6810001134872437]], "normalized_score": [0.052461374551057816], "tokens": [["\u0120dust", "\u0120particles", "\u0120and", "\u0120frozen", "\u0120water"]], "entropy": [[4.798575401306152, 0.6733789443969727, 1.0265905857086182, 3.0612199306488037, 1.3875885009765625]], "avg_entropy": [2.189470672607422], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "water ice", "score_of_biggest_cluster": 0.052461374551057816, "score_of_first_answer": 0.052461374551057816, "max_score_of_first_answer": 0.6810001134872437, "min_score_of_first_answer": 0.00825407262891531, "avg_score": 0.052461374551057816, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25754662055504146, "std_of_first_answer": 0.25754662055504146, "avg_entropy_of_first_answer": 2.189470672607422, "lex_sim": 1.0}
{"question": "where is the meridian that is opposite the prime meridian located", "answer": ["Paris, France"], "scores": [[0.003967192955315113, 0.12474945187568665, 0.8354058265686035]], "normalized_score": [0.07449718564748764], "tokens": [["\u0120Paris", ",", "\u0120France"]], "entropy": [[5.794526100158691, 1.1829025745391846, 1.4097808599472046]], "avg_entropy": [2.7957365115483603], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the 180th meridian", "score_of_biggest_cluster": 0.07449718564748764, "score_of_first_answer": 0.07449718564748764, "max_score_of_first_answer": 0.8354058265686035, "min_score_of_first_answer": 0.003967192955315113, "avg_score": 0.07449718564748764, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3668046768189545, "std_of_first_answer": 0.3668046768189545, "avg_entropy_of_first_answer": 2.7957365115483603, "lex_sim": 1.0}
{"question": "who has most followers on instagram in world", "answer": ["k-pop boy band BTS"], "scores": [[0.03802303597331047, 0.005465782713145018, 0.9773918390274048, 0.08227138221263885, 0.40971049666404724, 0.40948107838630676, 0.9708259701728821]], "normalized_score": [0.16031764447689056], "tokens": [["\u0120k", "-", "pop", "\u0120boy", "\u0120band", "\u0120B", "TS"]], "entropy": [[5.709543228149414, 2.5955896377563477, 0.20222735404968262, 3.0036981105804443, 1.681121826171875, 3.7383556365966797, 0.20813775062561035]], "avg_entropy": [2.4483819348471507], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Instagram's own account", "score_of_biggest_cluster": 0.16031764447689056, "score_of_first_answer": 0.16031764447689056, "max_score_of_first_answer": 0.9773918390274048, "min_score_of_first_answer": 0.005465782713145018, "avg_score": 0.16031764447689056, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3865236589677232, "std_of_first_answer": 0.3865236589677232, "avg_entropy_of_first_answer": 2.4483819348471507, "lex_sim": 1.0}
{"question": "when does panic at the disco album come out", "answer": ["December 19, 1985"], "scores": [[0.02792580984532833, 0.01600564643740654, 0.2837197184562683, 0.018584657460451126]], "normalized_score": [0.039181482046842575], "tokens": [["\u0120December", "\u012019", ",", "\u01201985"]], "entropy": [[5.020864486694336, 3.853269100189209, 1.1381123065948486, 4.078982353210449]], "avg_entropy": [3.5228070616722107], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "December 15, 2017", "score_of_biggest_cluster": 0.039181482046842575, "score_of_first_answer": 0.039181482046842575, "max_score_of_first_answer": 0.2837197184562683, "min_score_of_first_answer": 0.01600564643740654, "avg_score": 0.039181482046842575, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11391717038325587, "std_of_first_answer": 0.11391717038325587, "avg_entropy_of_first_answer": 3.5228070616722107, "lex_sim": 1.0}
{"question": "who won the 30 man royal rumble match", "answer": ["Brock Lesnar"], "scores": [[0.03876688703894615, 0.9801570177078247, 0.9875590205192566]], "normalized_score": [0.33479058742523193], "tokens": [["\u0120Brock", "\u0120Les", "nar"]], "entropy": [[6.480682373046875, 0.2083093225955963, 0.09245546907186508]], "avg_entropy": [2.260482388238112], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Shinsuke Nakamura", "score_of_biggest_cluster": 0.33479058742523193, "score_of_first_answer": 0.33479058742523193, "max_score_of_first_answer": 0.9875590205192566, "min_score_of_first_answer": 0.03876688703894615, "avg_score": 0.33479058742523193, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44553048035403914, "std_of_first_answer": 0.44553048035403914, "avg_entropy_of_first_answer": 2.260482388238112, "lex_sim": 1.0}
{"question": "when did the nba start playing zone defense", "answer": ["1966"], "scores": [[0.01045425795018673]], "normalized_score": [0.010454259812831879], "tokens": [["\u01201966"]], "entropy": [[4.841855049133301]], "avg_entropy": [4.841855049133301], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2002", "score_of_biggest_cluster": 0.010454259812831879, "score_of_first_answer": 0.010454259812831879, "max_score_of_first_answer": 0.01045425795018673, "min_score_of_first_answer": 0.01045425795018673, "avg_score": 0.010454259812831879, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.841855049133301, "lex_sim": 1.0}
{"question": "what type of joint connects the incus with the malleus", "answer": ["stapedial"], "scores": [[0.018696170300245285, 0.30180683732032776, 0.7259768843650818]], "normalized_score": [0.16000546514987946], "tokens": [["\u0120st", "aped", "ial"]], "entropy": [[5.611437797546387, 1.0591239929199219, 1.38832426071167]], "avg_entropy": [2.6862953503926597], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "synovial", "score_of_biggest_cluster": 0.16000546514987946, "score_of_first_answer": 0.16000546514987946, "max_score_of_first_answer": 0.7259768843650818, "min_score_of_first_answer": 0.018696170300245285, "avg_score": 0.16000546514987946, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.290654030130243, "std_of_first_answer": 0.290654030130243, "avg_entropy_of_first_answer": 2.6862953503926597, "lex_sim": 1.0}
{"question": "how much venom can a king cobra produce", "answer": ["0.1 to 0.2 millimeters"], "scores": [[0.02792801894247532, 0.8244181275367737, 0.10885196179151535, 0.020216334611177444, 0.553195059299469, 0.9979382157325745, 0.4060806632041931, 0.6597878336906433, 0.01691264472901821]], "normalized_score": [0.17126142978668213], "tokens": [["\u01200", ".", "1", "\u0120to", "\u01200", ".", "2", "\u0120mill", "imeters"]], "entropy": [[5.0427565574646, 0.7857879996299744, 3.550852060317993, 2.7427978515625, 1.3673783540725708, 0.02173263020813465, 2.008713483810425, 1.7143487930297852, 1.3377611637115479]], "avg_entropy": [2.0635698770897255], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "420\u00a0mg", "score_of_biggest_cluster": 0.17126142978668213, "score_of_first_answer": 0.17126142978668213, "max_score_of_first_answer": 0.9979382157325745, "min_score_of_first_answer": 0.01691264472901821, "avg_score": 0.17126142978668213, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35634353918938616, "std_of_first_answer": 0.35634353918938616, "avg_entropy_of_first_answer": 2.0635698770897255, "lex_sim": 1.0}
{"question": "where are the next two winter olympics going to be held", "answer": ["the next games will be in Sochi, Russia, which will be the first time the games have been held outside of Europe"], "scores": [[0.011803288012742996, 0.13422827422618866, 0.012319042347371578, 0.49709999561309814, 0.9306453466415405, 0.35361066460609436, 0.0024735024198889732, 0.2991396188735962, 0.9117614030838013, 0.07757258415222168, 0.010742542333900928, 0.301782488822937, 0.6135672330856323, 0.19225040078163147, 0.20808815956115723, 0.5126984715461731, 0.2105022668838501, 0.42733538150787354, 0.36450719833374023, 0.8666322827339172, 0.8808659315109253, 0.16210825741291046, 0.762590765953064, 0.29562556743621826]], "normalized_score": [0.1919204145669937], "tokens": [["\u0120the", "\u0120next", "\u0120games", "\u0120will", "\u0120be", "\u0120in", "\u0120Sochi", ",", "\u0120Russia", ",", "\u0120which", "\u0120will", "\u0120be", "\u0120the", "\u0120first", "\u0120time", "\u0120the", "\u0120games", "\u0120have", "\u0120been", "\u0120held", "\u0120outside", "\u0120of", "\u0120Europe"]], "entropy": [[4.895207405090332, 4.942340850830078, 1.6991591453552246, 1.3961515426635742, 0.40854543447494507, 1.5527617931365967, 4.010836601257324, 2.1307663917541504, 0.6766065359115601, 1.9285999536514282, 2.054286479949951, 2.0048892498016357, 1.5759938955307007, 2.118319511413574, 4.096940040588379, 2.365300178527832, 2.884458065032959, 2.349217414855957, 1.5792137384414673, 0.6844176054000854, 0.6419773697853088, 1.2037808895111084, 0.9983280301094055, 1.9696156978607178]], "avg_entropy": [2.0903214092055955], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Beijing", "score_of_biggest_cluster": 0.1919204145669937, "score_of_first_answer": 0.1919204145669937, "max_score_of_first_answer": 0.9306453466415405, "min_score_of_first_answer": 0.0024735024198889732, "avg_score": 0.1919204145669937, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30077023899159083, "std_of_first_answer": 0.30077023899159083, "avg_entropy_of_first_answer": 2.0903214092055955, "lex_sim": 1.0}
{"question": "how many strong verbs are there in german", "answer": ["7"], "scores": [[0.017456533387303352]], "normalized_score": [0.017456533387303352], "tokens": [["\u01207"]], "entropy": [[5.7650957107543945]], "avg_entropy": [5.7650957107543945], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "more than 200", "score_of_biggest_cluster": 0.017456533387303352, "score_of_first_answer": 0.017456533387303352, "max_score_of_first_answer": 0.017456533387303352, "min_score_of_first_answer": 0.017456533387303352, "avg_score": 0.017456533387303352, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.7650957107543945, "lex_sim": 1.0}
{"question": "who sings the theme song to all that", "answer": ["The B-52, which is a group of"], "scores": [[0.026402341201901436, 0.002639306942000985, 0.37121787667274475, 0.9702708721160889, 0.0001567023282404989, 0.007900472730398178, 0.47908276319503784, 0.22133304178714752, 0.04508946090936661, 0.5405455827713013]], "normalized_score": [0.04903143644332886], "tokens": [["\u0120The", "\u0120B", "-", "52", ",", "\u0120which", "\u0120is", "\u0120a", "\u0120group", "\u0120of"]], "entropy": [[6.692274570465088, 5.657304763793945, 2.8834996223449707, 0.2587606608867645, 0.9995301961898804, 6.300703525543213, 3.2316248416900635, 4.571486473083496, 5.421566009521484, 2.21687650680542]], "avg_entropy": [3.8233627170324325], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "TLC", "score_of_biggest_cluster": 0.04903143644332886, "score_of_first_answer": 0.04903143644332886, "max_score_of_first_answer": 0.9702708721160889, "min_score_of_first_answer": 0.0001567023282404989, "avg_score": 0.04903143644332886, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30703357690303795, "std_of_first_answer": 0.30703357690303795, "avg_entropy_of_first_answer": 3.8233627170324325, "lex_sim": 1.0}
{"question": "who proved that cells come from other cells", "answer": ["Francis Crick - 1953"], "scores": [[0.047103460878133774, 0.8112165927886963, 0.9978113770484924, 0.00021999201271682978, 0.01937129721045494]], "normalized_score": [0.04386930540204048], "tokens": [["\u0120Francis", "\u0120Cr", "ick", "\u0120-", "\u01201953"]], "entropy": [[5.586341381072998, 1.416036605834961, 0.025062106549739838, 0.826171875, 5.8601837158203125]], "avg_entropy": [2.7427591368556024], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Matthias Schleiden", "score_of_biggest_cluster": 0.04386930540204048, "score_of_first_answer": 0.04386930540204048, "max_score_of_first_answer": 0.9978113770484924, "min_score_of_first_answer": 0.00021999201271682978, "avg_score": 0.04386930540204048, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4364921057117109, "std_of_first_answer": 0.4364921057117109, "avg_entropy_of_first_answer": 2.7427591368556024, "lex_sim": 1.0}
{"question": "who is the actor that plays saul on grace and frankie", "answer": ["Bruce Dern"], "scores": [[0.0033198390156030655, 0.5384608507156372, 0.9940880537033081]], "normalized_score": [0.12112449109554291], "tokens": [["\u0120Bruce", "\u0120D", "ern"]], "entropy": [[5.476649284362793, 1.967822551727295, 0.05636455863714218]], "avg_entropy": [2.50027879824241], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sam Waterston", "score_of_biggest_cluster": 0.12112449109554291, "score_of_first_answer": 0.12112449109554291, "max_score_of_first_answer": 0.9940880537033081, "min_score_of_first_answer": 0.0033198390156030655, "avg_score": 0.12112449109554291, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4049133934478618, "std_of_first_answer": 0.4049133934478618, "avg_entropy_of_first_answer": 2.50027879824241, "lex_sim": 1.0}
{"question": "what are the active materials of a lead acid battery", "answer": ["lead and sulfuric acid"], "scores": [[0.41887709498405457, 0.6327405571937561, 0.33976230025291443, 0.8172524571418762, 0.9857177734375]], "normalized_score": [0.5917240977287292], "tokens": [["\u0120lead", "\u0120and", "\u0120sulfur", "ic", "\u0120acid"]], "entropy": [[2.95233154296875, 1.4546786546707153, 2.0864920616149902, 0.6467416286468506, 0.12095312029123306]], "avg_entropy": [1.4522394016385078], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lead", "score_of_biggest_cluster": 0.5917240977287292, "score_of_first_answer": 0.5917240977287292, "max_score_of_first_answer": 0.9857177734375, "min_score_of_first_answer": 0.33976230025291443, "avg_score": 0.5917240977287292, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24084166721270656, "std_of_first_answer": 0.24084166721270656, "avg_entropy_of_first_answer": 1.4522394016385078, "lex_sim": 1.0}
{"question": "who is ranked 1 in women's tennis", "answer": ["Serena Williams"], "scores": [[0.6899566650390625, 0.9975560903549194, 0.8825339674949646]], "normalized_score": [0.846896231174469], "tokens": [["\u0120Sere", "na", "\u0120Williams"]], "entropy": [[1.7978041172027588, 0.021991576999425888, 0.7869688272476196]], "avg_entropy": [0.8689215071499348], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Simona Halep", "score_of_biggest_cluster": 0.846896231174469, "score_of_first_answer": 0.846896231174469, "max_score_of_first_answer": 0.9975560903549194, "min_score_of_first_answer": 0.6899566650390625, "avg_score": 0.846896231174469, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12690044769339118, "std_of_first_answer": 0.12690044769339118, "avg_entropy_of_first_answer": 0.8689215071499348, "lex_sim": 1.0}
{"question": "when did ireland gain its independence from england", "answer": ["Easter Monday"], "scores": [[0.011701589450240135, 0.20714455842971802]], "normalized_score": [0.049233328551054], "tokens": [["\u0120Easter", "\u0120Monday"]], "entropy": [[4.344677448272705, 3.1735455989837646]], "avg_entropy": [3.759111523628235], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "In 1922", "score_of_biggest_cluster": 0.049233328551054, "score_of_first_answer": 0.049233328551054, "max_score_of_first_answer": 0.20714455842971802, "min_score_of_first_answer": 0.011701589450240135, "avg_score": 0.049233328551054, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09772148448973894, "std_of_first_answer": 0.09772148448973894, "avg_entropy_of_first_answer": 3.759111523628235, "lex_sim": 1.0}
{"question": "who was assassinated during a visit to sarajevo in bosnia", "answer": ["Arkan"], "scores": [[0.0013695565285161138, 0.6952149271965027]], "normalized_score": [0.030856698751449585], "tokens": [["\u0120Ark", "an"]], "entropy": [[5.01175594329834, 1.667709469795227]], "avg_entropy": [3.3397327065467834], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Archduke Franz Ferdinand of Austria", "score_of_biggest_cluster": 0.030856698751449585, "score_of_first_answer": 0.030856698751449585, "max_score_of_first_answer": 0.6952149271965027, "min_score_of_first_answer": 0.0013695565285161138, "avg_score": 0.030856698751449585, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3469226853339933, "std_of_first_answer": 0.3469226853339933, "avg_entropy_of_first_answer": 3.3397327065467834, "lex_sim": 1.0}
{"question": "ben 10 ultimate alien episode 2 season 1", "answer": ["Season 1 Episode 26"], "scores": [[0.04949059709906578, 0.7294264435768127, 0.11647270619869232, 0.001987995347008109]], "normalized_score": [0.05376949533820152], "tokens": [["\u0120Season", "\u01201", "\u0120Episode", "\u012026"]], "entropy": [[6.149580955505371, 1.294431447982788, 1.655578374862671, 1.7634061574935913]], "avg_entropy": [2.7157492339611053], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Duped", "score_of_biggest_cluster": 0.05376949533820152, "score_of_first_answer": 0.05376949533820152, "max_score_of_first_answer": 0.7294264435768127, "min_score_of_first_answer": 0.001987995347008109, "avg_score": 0.05376949533820152, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29443182408938545, "std_of_first_answer": 0.29443182408938545, "avg_entropy_of_first_answer": 2.7157492339611053, "lex_sim": 1.0}
{"question": "when will the flash season 4 episode 14 coming out", "answer": ["December 4"], "scores": [[0.03219076246023178, 0.026137489825487137]], "normalized_score": [0.02900664694607258], "tokens": [["\u0120December", "\u01204"]], "entropy": [[4.479844570159912, 3.5383331775665283]], "avg_entropy": [4.00908887386322], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "February\u00a027,\u00a02018", "score_of_biggest_cluster": 0.02900664694607258, "score_of_first_answer": 0.02900664694607258, "max_score_of_first_answer": 0.03219076246023178, "min_score_of_first_answer": 0.026137489825487137, "avg_score": 0.02900664694607258, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.003026636317372322, "std_of_first_answer": 0.003026636317372322, "avg_entropy_of_first_answer": 4.00908887386322, "lex_sim": 1.0}
{"question": "who sings for the beast in the new movie", "answer": ["Usher"], "scores": [[0.0010575116612017155, 0.9810959696769714]], "normalized_score": [0.032210562378168106], "tokens": [["\u0120Us", "her"]], "entropy": [[5.857639312744141, 0.16978302597999573]], "avg_entropy": [3.013711169362068], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dan Stevens", "score_of_biggest_cluster": 0.032210562378168106, "score_of_first_answer": 0.032210562378168106, "max_score_of_first_answer": 0.9810959696769714, "min_score_of_first_answer": 0.0010575116612017155, "avg_score": 0.032210562378168106, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.49001922900788486, "std_of_first_answer": 0.49001922900788486, "avg_entropy_of_first_answer": 3.013711169362068, "lex_sim": 1.0}
{"question": "what's the largest city park in the united states", "answer": ["city park"], "scores": [[0.006037174258381128, 0.765860915184021]], "normalized_score": [0.06799732148647308], "tokens": [["\u0120city", "\u0120park"]], "entropy": [[3.825834035873413, 1.295479416847229]], "avg_entropy": [2.560656726360321], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Franklin Mountains State Park", "score_of_biggest_cluster": 0.06799732148647308, "score_of_first_answer": 0.06799732148647308, "max_score_of_first_answer": 0.765860915184021, "min_score_of_first_answer": 0.006037174258381128, "avg_score": 0.06799732148647308, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37991187046281993, "std_of_first_answer": 0.37991187046281993, "avg_entropy_of_first_answer": 2.560656726360321, "lex_sim": 1.0}
{"question": "who is directly elected according to the constitution", "answer": ["president"], "scores": [[0.144562765955925]], "normalized_score": [0.144562765955925], "tokens": [["\u0120president"]], "entropy": [[4.523935317993164]], "avg_entropy": [4.523935317993164], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "senators", "score_of_biggest_cluster": 0.144562765955925, "score_of_first_answer": 0.144562765955925, "max_score_of_first_answer": 0.144562765955925, "min_score_of_first_answer": 0.144562765955925, "avg_score": 0.144562765955925, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.523935317993164, "lex_sim": 1.0}
{"question": "where was the world economic forum held this year", "answer": ["Davos"], "scores": [[0.1950845718383789, 0.9902287721633911]], "normalized_score": [0.43952059745788574], "tokens": [["\u0120Dav", "os"]], "entropy": [[5.0890212059021, 0.07156172394752502]], "avg_entropy": [2.5802914649248123], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Davos", "score_of_biggest_cluster": 0.43952059745788574, "score_of_first_answer": 0.43952059745788574, "max_score_of_first_answer": 0.9902287721633911, "min_score_of_first_answer": 0.1950845718383789, "avg_score": 0.43952059745788574, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3975721001625061, "std_of_first_answer": 0.3975721001625061, "avg_entropy_of_first_answer": 2.5802914649248123, "lex_sim": 1.0}
{"question": "how many countries does cadbury sell its products", "answer": ["16"], "scores": [[0.024190295487642288]], "normalized_score": [0.024190297350287437], "tokens": [["\u012016"]], "entropy": [[5.4759979248046875]], "avg_entropy": [5.4759979248046875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "more than 50 countries worldwide", "score_of_biggest_cluster": 0.024190297350287437, "score_of_first_answer": 0.024190297350287437, "max_score_of_first_answer": 0.024190295487642288, "min_score_of_first_answer": 0.024190295487642288, "avg_score": 0.024190297350287437, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.4759979248046875, "lex_sim": 1.0}
{"question": "where does a roadrunner live in the desert", "answer": ["Mexico"], "scores": [[0.04485021159052849]], "normalized_score": [0.04485021531581879], "tokens": [["\u0120Mexico"]], "entropy": [[5.82700252532959]], "avg_entropy": [5.82700252532959], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "mountainous shrubland", "score_of_biggest_cluster": 0.04485021531581879, "score_of_first_answer": 0.04485021531581879, "max_score_of_first_answer": 0.04485021159052849, "min_score_of_first_answer": 0.04485021159052849, "avg_score": 0.04485021531581879, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.82700252532959, "lex_sim": 1.0}
{"question": "who plays the mom on the tv show mom", "answer": ["Suzanne Somers"], "scores": [[0.009251751936972141, 0.25146234035491943]], "normalized_score": [0.04823346063494682], "tokens": [["\u0120Suzanne", "\u0120Somers"]], "entropy": [[5.615340709686279, 4.659775733947754]], "avg_entropy": [5.137558221817017], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Allison Janney", "score_of_biggest_cluster": 0.04823346063494682, "score_of_first_answer": 0.04823346063494682, "max_score_of_first_answer": 0.25146234035491943, "min_score_of_first_answer": 0.009251751936972141, "avg_score": 0.04823346063494682, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12110529420897365, "std_of_first_answer": 0.12110529420897365, "avg_entropy_of_first_answer": 5.137558221817017, "lex_sim": 1.0}
{"question": "who has appeared in the most royal rumbles", "answer": ["Mark Anthony"], "scores": [[0.004421324003487825, 0.018425283953547478]], "normalized_score": [0.009025746956467628], "tokens": [["\u0120Mark", "\u0120Anthony"]], "entropy": [[6.896744251251221, 5.302698135375977]], "avg_entropy": [6.099721193313599], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Shawn Michaels", "score_of_biggest_cluster": 0.009025746956467628, "score_of_first_answer": 0.009025746956467628, "max_score_of_first_answer": 0.018425283953547478, "min_score_of_first_answer": 0.004421324003487825, "avg_score": 0.009025746956467628, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.007001979975029826, "std_of_first_answer": 0.007001979975029826, "avg_entropy_of_first_answer": 6.099721193313599, "lex_sim": 1.0}
{"question": "name the cell which is attached to the lateral side of sieve tube", "answer": ["Sieve-tube companion cell"], "scores": [[0.06945066154003143, 0.9722853899002075, 0.039203204214572906, 0.43404337763786316, 0.043079178780317307, 0.8276086449623108]], "normalized_score": [0.18566764891147614], "tokens": [["\u0120S", "ieve", "-", "tube", "\u0120companion", "\u0120cell"]], "entropy": [[5.711334705352783, 0.2487635463476181, 3.952059268951416, 3.3653383255004883, 3.8423304557800293, 0.6684145927429199]], "avg_entropy": [2.964706815779209], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "sieve tube elements", "score_of_biggest_cluster": 0.18566764891147614, "score_of_first_answer": 0.18566764891147614, "max_score_of_first_answer": 0.9722853899002075, "min_score_of_first_answer": 0.039203204214572906, "avg_score": 0.18566764891147614, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3826038236637259, "std_of_first_answer": 0.3826038236637259, "avg_entropy_of_first_answer": 2.964706815779209, "lex_sim": 1.0}
{"question": "who were the two mathematicians that invented calculus", "answer": ["Newton and Leibniz"], "scores": [[0.2501337230205536, 0.8917548656463623, 0.9803077578544617, 0.9821192026138306, 0.9881747961044312, 0.7346725463867188]], "normalized_score": [0.7336326837539673], "tokens": [["\u0120Newton", "\u0120and", "\u0120Le", "ib", "n", "iz"]], "entropy": [[3.6950104236602783, 0.6830658912658691, 0.19053834676742554, 0.18079084157943726, 0.11557778716087341, 0.6359105110168457]], "avg_entropy": [0.9168156335751215], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gottfried Leibniz", "score_of_biggest_cluster": 0.7336326837539673, "score_of_first_answer": 0.7336326837539673, "max_score_of_first_answer": 0.9881747961044312, "min_score_of_first_answer": 0.2501337230205536, "avg_score": 0.7336326837539673, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26331370045057223, "std_of_first_answer": 0.26331370045057223, "avg_entropy_of_first_answer": 0.9168156335751215, "lex_sim": 1.0}
{"question": "how much money did the film titanic make", "answer": ["$2,941,021,000"], "scores": [[0.5987599492073059, 0.5086686611175537, 0.27901536226272583, 0.012053376995027065, 0.010001454502344131, 0.9535135626792908, 0.00337260402739048, 0.9062742590904236, 0.05894085019826889]], "normalized_score": [0.106481172144413], "tokens": [["\u0120$", "2", ",", "9", "41", ",", "021", ",", "000"]], "entropy": [[2.3616442680358887, 1.4947552680969238, 1.151510238647461, 5.9848809242248535, 3.827354669570923, 0.26945948600769043, 6.0750508308410645, 0.44134390354156494, 6.009661674499512]], "avg_entropy": [3.0684068070517645], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "$2.18 billion", "score_of_biggest_cluster": 0.106481172144413, "score_of_first_answer": 0.106481172144413, "max_score_of_first_answer": 0.9535135626792908, "min_score_of_first_answer": 0.00337260402739048, "avg_score": 0.106481172144413, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3646634087085963, "std_of_first_answer": 0.3646634087085963, "avg_entropy_of_first_answer": 3.0684068070517645, "lex_sim": 1.0}
{"question": "who does the voice of cruz on cars 3", "answer": ["Kevin Michael Collins"], "scores": [[0.010241097770631313, 0.06030891463160515, 0.01110881008207798]], "normalized_score": [0.01900196261703968], "tokens": [["\u0120Kevin", "\u0120Michael", "\u0120Collins"]], "entropy": [[6.032282829284668, 3.737696647644043, 1.1641085147857666]], "avg_entropy": [3.644695997238159], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cristela Alonzo", "score_of_biggest_cluster": 0.01900196261703968, "score_of_first_answer": 0.01900196261703968, "max_score_of_first_answer": 0.06030891463160515, "min_score_of_first_answer": 0.010241097770631313, "avg_score": 0.01900196261703968, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02340035492979601, "std_of_first_answer": 0.02340035492979601, "avg_entropy_of_first_answer": 3.644695997238159, "lex_sim": 1.0}
{"question": "where do you get a cashiers check from", "answer": ["branch"], "scores": [[0.001624439493753016]], "normalized_score": [0.0016244391445070505], "tokens": [["\u0120branch"]], "entropy": [[5.368878364562988]], "avg_entropy": [5.368878364562988], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "a bank", "score_of_biggest_cluster": 0.0016244391445070505, "score_of_first_answer": 0.0016244391445070505, "max_score_of_first_answer": 0.001624439493753016, "min_score_of_first_answer": 0.001624439493753016, "avg_score": 0.0016244391445070505, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.368878364562988, "lex_sim": 1.0}
{"question": "what year did the us hockey team won the olympics", "answer": ["2009"], "scores": [[0.0018471840303391218]], "normalized_score": [0.0018471836810931563], "tokens": [["\u01202009"]], "entropy": [[4.499013900756836]], "avg_entropy": [4.499013900756836], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1960", "score_of_biggest_cluster": 0.0018471836810931563, "score_of_first_answer": 0.0018471836810931563, "max_score_of_first_answer": 0.0018471840303391218, "min_score_of_first_answer": 0.0018471840303391218, "avg_score": 0.0018471836810931563, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.499013900756836, "lex_sim": 1.0}
{"question": "when did the dust bowl end in oklahoma", "answer": ["March 31, 2009"], "scores": [[0.026487497612833977, 0.172316774725914, 0.1672050952911377, 0.02476460672914982]], "normalized_score": [0.0659344494342804], "tokens": [["\u0120March", "\u012031", ",", "\u01202009"]], "entropy": [[4.760544300079346, 3.10965633392334, 0.7383629083633423, 3.633942127227783]], "avg_entropy": [3.0606264173984528], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1940", "score_of_biggest_cluster": 0.0659344494342804, "score_of_first_answer": 0.0659344494342804, "max_score_of_first_answer": 0.172316774725914, "min_score_of_first_answer": 0.02476460672914982, "avg_score": 0.0659344494342804, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07209267171566423, "std_of_first_answer": 0.07209267171566423, "avg_entropy_of_first_answer": 3.0606264173984528, "lex_sim": 1.0}
{"question": "where was the statue of liberty originally built", "answer": ["Philadelphia, Pennsylvania"], "scores": [[0.029848789796233177, 0.2563047707080841, 0.30579590797424316]], "normalized_score": [0.13275118172168732], "tokens": [["\u0120Philadelphia", ",", "\u0120Pennsylvania"]], "entropy": [[3.652507781982422, 0.9017508029937744, 2.560307264328003]], "avg_entropy": [2.3715219497680664], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paris", "score_of_biggest_cluster": 0.13275118172168732, "score_of_first_answer": 0.13275118172168732, "max_score_of_first_answer": 0.30579590797424316, "min_score_of_first_answer": 0.029848789796233177, "avg_score": 0.13275118172168732, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12012886413689954, "std_of_first_answer": 0.12012886413689954, "avg_entropy_of_first_answer": 2.3715219497680664, "lex_sim": 1.0}
{"question": "who came first second and third on the grand national", "answer": ["James Crowley"], "scores": [[0.0064217448234558105, 0.0004884088994003832]], "normalized_score": [0.0017709995154291391], "tokens": [["\u0120James", "\u0120Crowley"]], "entropy": [[7.181483268737793, 5.7538533210754395]], "avg_entropy": [6.467668294906616], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pleasant Company", "score_of_biggest_cluster": 0.0017709995154291391, "score_of_first_answer": 0.0017709995154291391, "max_score_of_first_answer": 0.0064217448234558105, "min_score_of_first_answer": 0.0004884088994003832, "avg_score": 0.0017709995154291391, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0029666679620277137, "std_of_first_answer": 0.0029666679620277137, "avg_entropy_of_first_answer": 6.467668294906616, "lex_sim": 1.0}
{"question": "the actual name of the confederate force at gettysburg was", "answer": ["legion"], "scores": [[0.0013914837036281824]], "normalized_score": [0.001391484052874148], "tokens": [["\u0120legion"]], "entropy": [[3.8949239253997803]], "avg_entropy": [3.8949239253997803], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Confederate States Army", "score_of_biggest_cluster": 0.001391484052874148, "score_of_first_answer": 0.001391484052874148, "max_score_of_first_answer": 0.0013914837036281824, "min_score_of_first_answer": 0.0013914837036281824, "avg_score": 0.001391484052874148, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.8949239253997803, "lex_sim": 1.0}
{"question": "who is the first indian climber of mount everest", "answer": ["srinagarindravidae nayak"], "scores": [[0.004414843395352364, 0.2140279859304428, 0.25887155532836914, 0.34101244807243347, 0.04973703995347023, 0.0022312819492071867, 0.002226617420092225, 0.14906197786331177, 0.6670773029327393]], "normalized_score": [0.0502685084939003], "tokens": [["\u0120s", "rin", "agar", "ind", "rav", "idae", "\u0120n", "ay", "ak"]], "entropy": [[4.94140625, 3.404423475265503, 2.0221807956695557, 3.4735333919525146, 2.8043036460876465, 3.80861759185791, 3.636298179626465, 4.125591278076172, 1.455930471420288]], "avg_entropy": [3.296920564439562], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bachendri Pal", "score_of_biggest_cluster": 0.0502685084939003, "score_of_first_answer": 0.0502685084939003, "max_score_of_first_answer": 0.6670773029327393, "min_score_of_first_answer": 0.002226617420092225, "avg_score": 0.0502685084939003, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20601643888632054, "std_of_first_answer": 0.20601643888632054, "avg_entropy_of_first_answer": 3.296920564439562, "lex_sim": 1.0}
{"question": "when does the cannes film festival take place", "answer": ["May 24 to june 14"], "scores": [[0.10360460728406906, 0.035867784172296524, 0.028628993779420853, 0.031994692981243134, 0.6045811176300049, 0.022181736305356026]], "normalized_score": [0.05978188291192055], "tokens": [["\u0120May", "\u012024", "\u0120to", "\u0120j", "une", "\u012014"]], "entropy": [[3.1952767372131348, 3.0238704681396484, 1.2482664585113525, 1.9572138786315918, 0.7185302376747131, 2.68275785446167]], "avg_entropy": [2.1376526057720184], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cannes, France", "score_of_biggest_cluster": 0.05978188291192055, "score_of_first_answer": 0.05978188291192055, "max_score_of_first_answer": 0.6045811176300049, "min_score_of_first_answer": 0.022181736305356026, "avg_score": 0.05978188291192055, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21052504490595927, "std_of_first_answer": 0.21052504490595927, "avg_entropy_of_first_answer": 2.1376526057720184, "lex_sim": 1.0}
{"question": "when did the wall go up in east germany", "answer": ["1621-1631"], "scores": [[0.04231855273246765, 0.05092368274927139, 0.05644741654396057, 0.8559702038764954, 0.02597060427069664]], "normalized_score": [0.07698517292737961], "tokens": [["\u012016", "21", "-", "16", "31"]], "entropy": [[5.395174980163574, 4.309399604797363, 0.7564853429794312, 0.9985839128494263, 3.112910747528076]], "avg_entropy": [2.9145109176635744], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "), starting on", "score_of_biggest_cluster": 0.07698517292737961, "score_of_first_answer": 0.07698517292737961, "max_score_of_first_answer": 0.8559702038764954, "min_score_of_first_answer": 0.02597060427069664, "avg_score": 0.07698517292737961, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32498540679355253, "std_of_first_answer": 0.32498540679355253, "avg_entropy_of_first_answer": 2.9145109176635744, "lex_sim": 1.0}
{"question": "when did where are you now come out", "answer": ["November 1, 2012"], "scores": [[0.012445181608200073, 0.04974065348505974, 0.11949899047613144, 0.023320458829402924]], "normalized_score": [0.036241281777620316], "tokens": [["\u0120November", "\u01201", ",", "\u01202012"]], "entropy": [[6.142635345458984, 3.928210735321045, 0.7468315362930298, 4.051626205444336]], "avg_entropy": [3.7173259556293488], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "February 27, 2015", "score_of_biggest_cluster": 0.036241281777620316, "score_of_first_answer": 0.036241281777620316, "max_score_of_first_answer": 0.11949899047613144, "min_score_of_first_answer": 0.012445181608200073, "avg_score": 0.036241281777620316, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04167156110992446, "std_of_first_answer": 0.04167156110992446, "avg_entropy_of_first_answer": 3.7173259556293488, "lex_sim": 1.0}
{"question": "who sang i want to be bobbys girl", "answer": ["Bobby Darin"], "scores": [[0.12868820130825043, 0.04976341500878334, 0.9898172616958618]], "normalized_score": [0.18506942689418793], "tokens": [["\u0120Bobby", "\u0120Dar", "in"]], "entropy": [[5.325867652893066, 5.125087261199951, 0.08845162391662598]], "avg_entropy": [3.5131355126698813], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marcie Blane", "score_of_biggest_cluster": 0.18506942689418793, "score_of_first_answer": 0.18506942689418793, "max_score_of_first_answer": 0.9898172616958618, "min_score_of_first_answer": 0.04976341500878334, "avg_score": 0.18506942689418793, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4257638384529412, "std_of_first_answer": 0.4257638384529412, "avg_entropy_of_first_answer": 3.5131355126698813, "lex_sim": 1.0}
{"question": "who did deion sanders go in the hall of fame as", "answer": ["The Deion Sanders Hall of Fame"], "scores": [[0.029805833473801613, 0.006694484502077103, 0.9159016013145447, 0.9009993672370911, 0.0067113167606294155, 0.933241069316864, 0.9610090851783752]], "normalized_score": [0.13877235352993011], "tokens": [["\u0120The", "\u0120De", "ion", "\u0120Sanders", "\u0120Hall", "\u0120of", "\u0120Fame"]], "entropy": [[6.180261135101318, 6.255982398986816, 0.6401243805885315, 0.790448009967804, 4.699898719787598, 0.28094762563705444, 0.32850196957588196]], "avg_entropy": [2.7394520342350006], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cornerback", "score_of_biggest_cluster": 0.13877235352993011, "score_of_first_answer": 0.13877235352993011, "max_score_of_first_answer": 0.9610090851783752, "min_score_of_first_answer": 0.006694484502077103, "avg_score": 0.13877235352993011, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4523787177718268, "std_of_first_answer": 0.4523787177718268, "avg_entropy_of_first_answer": 2.7394520342350006, "lex_sim": 1.0}
{"question": "where is the slide placed on the microscope", "answer": ["slide on the microscope"], "scores": [[0.4222579598426819, 0.008814959786832333, 0.30952516198158264, 0.531284511089325]], "normalized_score": [0.15729151666164398], "tokens": [["\u0120slide", "\u0120on", "\u0120the", "\u0120microscope"]], "entropy": [[4.081185340881348, 2.7644810676574707, 2.4721713066101074, 3.094295024871826]], "avg_entropy": [3.103033185005188], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "stage", "score_of_biggest_cluster": 0.15729151666164398, "score_of_first_answer": 0.15729151666164398, "max_score_of_first_answer": 0.531284511089325, "min_score_of_first_answer": 0.008814959786832333, "avg_score": 0.15729151666164398, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19495333633084397, "std_of_first_answer": 0.19495333633084397, "avg_entropy_of_first_answer": 3.103033185005188, "lex_sim": 1.0}
{"question": "where does the bob and tom show broadcast from", "answer": ["the Uk"], "scores": [[0.037788622081279755, 3.16627774736844e-05]], "normalized_score": [0.0010938431369140744], "tokens": [["\u0120the", "\u0120Uk"]], "entropy": [[6.108163833618164, 6.9248247146606445]], "avg_entropy": [6.516494274139404], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "WFBQ in Indianapolis, Indiana", "score_of_biggest_cluster": 0.0010938431369140744, "score_of_first_answer": 0.0010938431369140744, "max_score_of_first_answer": 0.037788622081279755, "min_score_of_first_answer": 3.16627774736844e-05, "avg_score": 0.0010938431369140744, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.018878479651903035, "std_of_first_answer": 0.018878479651903035, "avg_entropy_of_first_answer": 6.516494274139404, "lex_sim": 1.0}
{"question": "referring to the organs of reproduction is called", "answer": ["gamete"], "scores": [[0.011977972462773323, 0.5293177962303162]], "normalized_score": [0.07962509244680405], "tokens": [["\u0120gam", "ete"]], "entropy": [[6.606273651123047, 1.4931681156158447]], "avg_entropy": [4.049720883369446], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Reproductive system", "score_of_biggest_cluster": 0.07962509244680405, "score_of_first_answer": 0.07962509244680405, "max_score_of_first_answer": 0.5293177962303162, "min_score_of_first_answer": 0.011977972462773323, "avg_score": 0.07962509244680405, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2586699118837714, "std_of_first_answer": 0.2586699118837714, "avg_entropy_of_first_answer": 4.049720883369446, "lex_sim": 1.0}
{"question": "how many players have scored 10000 runs in odi", "answer": ["7"], "scores": [[0.02300676889717579]], "normalized_score": [0.02300676517188549], "tokens": [["\u01207"]], "entropy": [[5.975337982177734]], "avg_entropy": [5.975337982177734], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "11", "score_of_biggest_cluster": 0.02300676517188549, "score_of_first_answer": 0.02300676517188549, "max_score_of_first_answer": 0.02300676889717579, "min_score_of_first_answer": 0.02300676889717579, "avg_score": 0.02300676517188549, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.975337982177734, "lex_sim": 1.0}
{"question": "who sings every light in the house is on", "answer": ["U2"], "scores": [[0.004101024009287357, 0.9503878951072693]], "normalized_score": [0.06243046373128891], "tokens": [["\u0120U", "2"]], "entropy": [[6.807807922363281, 0.4193275570869446]], "avg_entropy": [3.613567739725113], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Trace Adkins", "score_of_biggest_cluster": 0.06243046373128891, "score_of_first_answer": 0.06243046373128891, "max_score_of_first_answer": 0.9503878951072693, "min_score_of_first_answer": 0.004101024009287357, "avg_score": 0.06243046373128891, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.47314343554899096, "std_of_first_answer": 0.47314343554899096, "avg_entropy_of_first_answer": 3.613567739725113, "lex_sim": 1.0}
{"question": "christmas though a christian holiday began as a celebration of", "answer": ["the birth of jesus christ"], "scores": [[0.3825061023235321, 0.43963852524757385, 0.9601175785064697, 0.45333829522132874, 0.9963453412055969, 0.9988914132118225, 0.36357709765434265]], "normalized_score": [0.5952715277671814], "tokens": [["\u0120the", "\u0120birth", "\u0120of", "\u0120j", "es", "us", "\u0120christ"]], "entropy": [[3.5708889961242676, 2.889801025390625, 0.26643508672714233, 1.919992446899414, 0.03619280457496643, 0.012621813453733921, 1.4827563762664795]], "avg_entropy": [1.4540983642052328], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the winter solstice", "score_of_biggest_cluster": 0.5952715277671814, "score_of_first_answer": 0.5952715277671814, "max_score_of_first_answer": 0.9988914132118225, "min_score_of_first_answer": 0.36357709765434265, "avg_score": 0.5952715277671814, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28637893517552543, "std_of_first_answer": 0.28637893517552543, "avg_entropy_of_first_answer": 1.4540983642052328, "lex_sim": 1.0}
{"question": "where does florida natural orange juice come from", "answer": ["Florida"], "scores": [[0.5994408130645752]], "normalized_score": [0.5994408130645752], "tokens": [["\u0120Florida"]], "entropy": [[3.0539722442626953]], "avg_entropy": [3.0539722442626953], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lake Wales, Florida", "score_of_biggest_cluster": 0.5994408130645752, "score_of_first_answer": 0.5994408130645752, "max_score_of_first_answer": 0.5994408130645752, "min_score_of_first_answer": 0.5994408130645752, "avg_score": 0.5994408130645752, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.0539722442626953, "lex_sim": 1.0}
{"question": "when did response to state of the union start", "answer": ["1862"], "scores": [[0.006500388029962778]], "normalized_score": [0.006500388029962778], "tokens": [["\u01201862"]], "entropy": [[4.821987152099609]], "avg_entropy": [4.821987152099609], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in 1966", "score_of_biggest_cluster": 0.006500388029962778, "score_of_first_answer": 0.006500388029962778, "max_score_of_first_answer": 0.006500388029962778, "min_score_of_first_answer": 0.006500388029962778, "avg_score": 0.006500388029962778, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.821987152099609, "lex_sim": 1.0}
{"question": "who was an early advocate for using scientific methods based on inductive reasoning", "answer": ["Thomas Henry Huxley"], "scores": [[0.16730575263500214, 0.1343521922826767, 0.9907034635543823, 0.9986746311187744, 0.9992678761482239]], "normalized_score": [0.4670475125312805], "tokens": [["\u0120Thomas", "\u0120Henry", "\u0120H", "ux", "ley"]], "entropy": [[3.998110771179199, 3.4377615451812744, 0.09116820245981216, 0.012626821175217628, 0.00805388018488884]], "avg_entropy": [1.5095442440360785], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Aristotle", "score_of_biggest_cluster": 0.4670475125312805, "score_of_first_answer": 0.4670475125312805, "max_score_of_first_answer": 0.9992678761482239, "min_score_of_first_answer": 0.1343521922826767, "avg_score": 0.4670475125312805, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4142951640294682, "std_of_first_answer": 0.4142951640294682, "avg_entropy_of_first_answer": 1.5095442440360785, "lex_sim": 1.0}
{"question": "where does blonde hair green eyes come from", "answer": ["Iceland"], "scores": [[0.004985611420124769]], "normalized_score": [0.004985610488802195], "tokens": [["\u0120Iceland"]], "entropy": [[6.5226521492004395]], "avg_entropy": [6.5226521492004395], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "a low concentration in pigmentation", "score_of_biggest_cluster": 0.004985610488802195, "score_of_first_answer": 0.004985610488802195, "max_score_of_first_answer": 0.004985611420124769, "min_score_of_first_answer": 0.004985611420124769, "avg_score": 0.004985610488802195, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.5226521492004395, "lex_sim": 1.0}
{"question": "when did this season of american idol start", "answer": ["2011"], "scores": [[0.01436975784599781]], "normalized_score": [0.014369756914675236], "tokens": [["\u01202011"]], "entropy": [[4.7501630783081055]], "avg_entropy": [4.7501630783081055], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "March 11, 2018", "score_of_biggest_cluster": 0.014369756914675236, "score_of_first_answer": 0.014369756914675236, "max_score_of_first_answer": 0.01436975784599781, "min_score_of_first_answer": 0.01436975784599781, "avg_score": 0.014369756914675236, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.7501630783081055, "lex_sim": 1.0}
{"question": "what is the poorest county in the state of florida", "answer": ["Bradford"], "scores": [[0.01458856649696827]], "normalized_score": [0.014588567428290844], "tokens": [["\u0120Bradford"]], "entropy": [[5.1278605461120605]], "avg_entropy": [5.1278605461120605], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Union", "score_of_biggest_cluster": 0.014588567428290844, "score_of_first_answer": 0.014588567428290844, "max_score_of_first_answer": 0.01458856649696827, "min_score_of_first_answer": 0.01458856649696827, "avg_score": 0.014588567428290844, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.1278605461120605, "lex_sim": 1.0}
{"question": "who has said that caste is a closed class", "answer": ["Mahatma Ghandi"], "scores": [[0.059850066900253296, 0.9429415464401245, 0.9835317134857178, 0.13468573987483978, 0.9766765236854553, 0.9444048404693604]], "normalized_score": [0.4362761080265045], "tokens": [["\u0120Mah", "at", "ma", "\u0120G", "hand", "i"]], "entropy": [[5.927918434143066, 0.39442336559295654, 0.13172414898872375, 0.6443898677825928, 0.18481463193893433, 0.3026989996433258]], "avg_entropy": [1.2643282413482666], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Frederik Barth", "score_of_biggest_cluster": 0.4362761080265045, "score_of_first_answer": 0.4362761080265045, "max_score_of_first_answer": 0.9835317134857178, "min_score_of_first_answer": 0.059850066900253296, "avg_score": 0.4362761080265045, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4084341433150451, "std_of_first_answer": 0.4084341433150451, "avg_entropy_of_first_answer": 1.2643282413482666, "lex_sim": 1.0}
{"question": "when did the movie napoleon dynamite come out", "answer": ["2006"], "scores": [[0.02822119928896427]], "normalized_score": [0.02822120115160942], "tokens": [["\u01202006"]], "entropy": [[5.554758548736572]], "avg_entropy": [5.554758548736572], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "June\u00a011,\u00a02004", "score_of_biggest_cluster": 0.02822120115160942, "score_of_first_answer": 0.02822120115160942, "max_score_of_first_answer": 0.02822119928896427, "min_score_of_first_answer": 0.02822119928896427, "avg_score": 0.02822120115160942, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.554758548736572, "lex_sim": 1.0}
{"question": "who was the grandfather on the cosby show", "answer": ["Ralph Bellamy"], "scores": [[0.016314923763275146, 0.09708672016859055, 0.9878391027450562]], "normalized_score": [0.11609415709972382], "tokens": [["\u0120Ralph", "\u0120Bell", "amy"]], "entropy": [[5.603612422943115, 3.9888947010040283, 0.10364595055580139]], "avg_entropy": [3.232051024834315], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Earle Hyman", "score_of_biggest_cluster": 0.11609415709972382, "score_of_first_answer": 0.11609415709972382, "max_score_of_first_answer": 0.9878391027450562, "min_score_of_first_answer": 0.016314923763275146, "avg_score": 0.11609415709972382, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4401796502573839, "std_of_first_answer": 0.4401796502573839, "avg_entropy_of_first_answer": 3.232051024834315, "lex_sim": 1.0}
{"question": "who won an oscar for schindler's list", "answer": ["Jennifer Lawrence"], "scores": [[0.013660280033946037, 0.6699449419975281]], "normalized_score": [0.09566417336463928], "tokens": [["\u0120Jennifer", "\u0120Lawrence"]], "entropy": [[4.816134929656982, 2.1990644931793213]], "avg_entropy": [3.507599711418152], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Steven Zaillian", "score_of_biggest_cluster": 0.09566417336463928, "score_of_first_answer": 0.09566417336463928, "max_score_of_first_answer": 0.6699449419975281, "min_score_of_first_answer": 0.013660280033946037, "avg_score": 0.09566417336463928, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.328142330981791, "std_of_first_answer": 0.328142330981791, "avg_entropy_of_first_answer": 3.507599711418152, "lex_sim": 1.0}
{"question": "who discovered that plants were made of cells", "answer": ["Robert Hooke"], "scores": [[0.3264355957508087, 0.6378055810928345, 0.9945589900016785]], "normalized_score": [0.5916144847869873], "tokens": [["\u0120Robert", "\u0120Ho", "oke"]], "entropy": [[3.848560333251953, 2.3372294902801514, 0.051438622176647186]], "avg_entropy": [2.079076148569584], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Parashara", "score_of_biggest_cluster": 0.5916144847869873, "score_of_first_answer": 0.5916144847869873, "max_score_of_first_answer": 0.9945589900016785, "min_score_of_first_answer": 0.3264355957508087, "avg_score": 0.5916144847869873, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27296990727210557, "std_of_first_answer": 0.27296990727210557, "avg_entropy_of_first_answer": 2.079076148569584, "lex_sim": 1.0}
{"question": "where do the blood vessels that supply the retinal cells lie", "answer": ["under the retina"], "scores": [[0.002751362044364214, 0.9031500816345215, 0.7083732485771179]], "normalized_score": [0.12074150890111923], "tokens": [["\u0120under", "\u0120the", "\u0120retina"]], "entropy": [[4.348567485809326, 0.6426613330841064, 1.7022305727005005]], "avg_entropy": [2.231153130531311], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "optic nerve", "score_of_biggest_cluster": 0.12074150890111923, "score_of_first_answer": 0.12074150890111923, "max_score_of_first_answer": 0.9031500816345215, "min_score_of_first_answer": 0.002751362044364214, "avg_score": 0.12074150890111923, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3868043024708301, "std_of_first_answer": 0.3868043024708301, "avg_entropy_of_first_answer": 2.231153130531311, "lex_sim": 1.0}
{"question": "where is natural gas found in the us", "answer": ["Alaska"], "scores": [[0.07771850377321243]], "normalized_score": [0.07771849632263184], "tokens": [["\u0120Alaska"]], "entropy": [[5.107847213745117]], "avg_entropy": [5.107847213745117], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Louisiana", "score_of_biggest_cluster": 0.07771849632263184, "score_of_first_answer": 0.07771849632263184, "max_score_of_first_answer": 0.07771850377321243, "min_score_of_first_answer": 0.07771850377321243, "avg_score": 0.07771849632263184, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.107847213745117, "lex_sim": 1.0}
{"question": "who was the dj on in living color", "answer": ["Keith Hamilton"], "scores": [[0.002832328202202916, 0.007228085771203041]], "normalized_score": [0.004524633754044771], "tokens": [["\u0120Keith", "\u0120Hamilton"]], "entropy": [[6.393760681152344, 4.366909980773926]], "avg_entropy": [5.380335330963135], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "DJ Twist", "score_of_biggest_cluster": 0.004524633754044771, "score_of_first_answer": 0.004524633754044771, "max_score_of_first_answer": 0.007228085771203041, "min_score_of_first_answer": 0.002832328202202916, "avg_score": 0.004524633754044771, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0021978787845000625, "std_of_first_answer": 0.0021978787845000625, "avg_entropy_of_first_answer": 5.380335330963135, "lex_sim": 1.0}
{"question": "who cracked the enigma code in world war 2", "answer": ["Alan Turing"], "scores": [[0.5372526049613953, 0.9821809530258179]], "normalized_score": [0.7264153361320496], "tokens": [["\u0120Alan", "\u0120Turing"]], "entropy": [[3.244062662124634, 0.1966516375541687]], "avg_entropy": [1.7203571498394012], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Turing", "score_of_biggest_cluster": 0.7264153361320496, "score_of_first_answer": 0.7264153361320496, "max_score_of_first_answer": 0.9821809530258179, "min_score_of_first_answer": 0.5372526049613953, "avg_score": 0.7264153361320496, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2224641740322113, "std_of_first_answer": 0.2224641740322113, "avg_entropy_of_first_answer": 1.7203571498394012, "lex_sim": 1.0}
{"question": "who plays voldemort in harry potter and the philosopher's stone", "answer": ["Rupert Evans"], "scores": [[0.009272821247577667, 0.006166061852127314]], "normalized_score": [0.007561533711850643], "tokens": [["\u0120Rupert", "\u0120Evans"]], "entropy": [[4.498993873596191, 0.40291738510131836]], "avg_entropy": [2.450955629348755], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Richard Bremmer", "score_of_biggest_cluster": 0.007561533711850643, "score_of_first_answer": 0.007561533711850643, "max_score_of_first_answer": 0.009272821247577667, "min_score_of_first_answer": 0.006166061852127314, "avg_score": 0.007561533711850643, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0015533796977251768, "std_of_first_answer": 0.0015533796977251768, "avg_entropy_of_first_answer": 2.450955629348755, "lex_sim": 1.0}
{"question": "when were cigarette ads banned from tv uk", "answer": ["in 1988"], "scores": [[0.03689125180244446, 0.009579617530107498]], "normalized_score": [0.01879904605448246], "tokens": [["\u0120in", "\u01201988"]], "entropy": [[5.003632068634033, 4.10512113571167]], "avg_entropy": [4.554376602172852], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1 August 1965", "score_of_biggest_cluster": 0.01879904605448246, "score_of_first_answer": 0.01879904605448246, "max_score_of_first_answer": 0.03689125180244446, "min_score_of_first_answer": 0.009579617530107498, "avg_score": 0.01879904605448246, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01365581713616848, "std_of_first_answer": 0.01365581713616848, "avg_entropy_of_first_answer": 4.554376602172852, "lex_sim": 1.0}
{"question": "when is the last time the new york giants won the super bowl", "answer": ["last year, 2012"], "scores": [[0.030819257721304893, 0.6350994110107422, 0.012928897514939308, 0.004646596498787403]], "normalized_score": [0.03292987868189812], "tokens": [["\u0120last", "\u0120year", ",", "\u01202012"]], "entropy": [[4.967706680297852, 1.7579007148742676, 0.6792303323745728, 4.616025924682617]], "avg_entropy": [3.0052159130573273], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "(2011", "score_of_biggest_cluster": 0.03292987868189812, "score_of_first_answer": 0.03292987868189812, "max_score_of_first_answer": 0.6350994110107422, "min_score_of_first_answer": 0.004646596498787403, "avg_score": 0.03292987868189812, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2681877924774782, "std_of_first_answer": 0.2681877924774782, "avg_entropy_of_first_answer": 3.0052159130573273, "lex_sim": 1.0}
{"question": "when does the eclipse end in the us", "answer": ["May 20"], "scores": [[0.01605212688446045, 0.25957512855529785]], "normalized_score": [0.06455022841691971], "tokens": [["\u0120May", "\u012020"]], "entropy": [[3.665292978286743, 2.7696101665496826]], "avg_entropy": [3.217451572418213], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2:44\u00a0p.m. EDT", "score_of_biggest_cluster": 0.06455022841691971, "score_of_first_answer": 0.06455022841691971, "max_score_of_first_answer": 0.25957512855529785, "min_score_of_first_answer": 0.01605212688446045, "avg_score": 0.06455022841691971, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1217615008354187, "std_of_first_answer": 0.1217615008354187, "avg_entropy_of_first_answer": 3.217451572418213, "lex_sim": 1.0}
{"question": "where was the film the remains of the day filmed", "answer": ["London"], "scores": [[0.030106110498309135]], "normalized_score": [0.030106108635663986], "tokens": [["\u0120London"]], "entropy": [[6.692684650421143]], "avg_entropy": [6.692684650421143], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Powderham Castle", "score_of_biggest_cluster": 0.030106108635663986, "score_of_first_answer": 0.030106108635663986, "max_score_of_first_answer": 0.030106110498309135, "min_score_of_first_answer": 0.030106110498309135, "avg_score": 0.030106108635663986, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.692684650421143, "lex_sim": 1.0}
{"question": "who present the central budget in lok sabha", "answer": ["lok sabha"], "scores": [[0.0007419743342325091, 0.7228145003318787, 0.8841214776039124, 0.9848735332489014]], "normalized_score": [0.14700327813625336], "tokens": [["\u0120l", "ok", "\u0120sab", "ha"]], "entropy": [[5.375826358795166, 1.733643889427185, 0.8570966124534607, 0.11528019607067108]], "avg_entropy": [2.0204617641866207], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Finance Minister of India", "score_of_biggest_cluster": 0.14700327813625336, "score_of_first_answer": 0.14700327813625336, "max_score_of_first_answer": 0.9848735332489014, "min_score_of_first_answer": 0.0007419743342325091, "avg_score": 0.14700327813625336, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38528470473275994, "std_of_first_answer": 0.38528470473275994, "avg_entropy_of_first_answer": 2.0204617641866207, "lex_sim": 1.0}
{"question": "how many episodes in season 3 of good witch", "answer": ["26"], "scores": [[0.041772570461034775]], "normalized_score": [0.041772566735744476], "tokens": [["\u012026"]], "entropy": [[4.377258777618408]], "avg_entropy": [4.377258777618408], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "10", "score_of_biggest_cluster": 0.041772566735744476, "score_of_first_answer": 0.041772566735744476, "max_score_of_first_answer": 0.041772570461034775, "min_score_of_first_answer": 0.041772570461034775, "avg_score": 0.041772566735744476, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.377258777618408, "lex_sim": 1.0}
{"question": "who is opening for little mix glory days tour", "answer": ["The Killers"], "scores": [[0.04520775005221367, 0.045698124915361404, 0.9897626638412476]], "normalized_score": [0.12692506611347198], "tokens": [["\u0120The", "\u0120Kill", "ers"]], "entropy": [[6.712531089782715, 5.740800857543945, 0.09496942162513733]], "avg_entropy": [4.1827671229839325], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sheppard", "score_of_biggest_cluster": 0.12692506611347198, "score_of_first_answer": 0.12692506611347198, "max_score_of_first_answer": 0.9897626638412476, "min_score_of_first_answer": 0.04520775005221367, "avg_score": 0.12692506611347198, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4451519190478512, "std_of_first_answer": 0.4451519190478512, "avg_entropy_of_first_answer": 4.1827671229839325, "lex_sim": 1.0}
{"question": "which songs did liam write as you were", "answer": ["A Perfect Day"], "scores": [[0.005354050546884537, 0.011079040355980396, 0.8660394549369812]], "normalized_score": [0.03717413172125816], "tokens": [["\u0120A", "\u0120Perfect", "\u0120Day"]], "entropy": [[7.251228332519531, 6.336825370788574, 0.7963230609893799]], "avg_entropy": [4.794792254765828], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "I've All I Need", "score_of_biggest_cluster": 0.03717413172125816, "score_of_first_answer": 0.03717413172125816, "max_score_of_first_answer": 0.8660394549369812, "min_score_of_first_answer": 0.005354050546884537, "avg_score": 0.03717413172125816, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4043883517581384, "std_of_first_answer": 0.4043883517581384, "avg_entropy_of_first_answer": 4.794792254765828, "lex_sim": 1.0}
{"question": "who sings the song i want to go outside in the rain", "answer": ["Elton John"], "scores": [[0.04807957634329796, 0.8900367617607117, 0.9217725992202759]], "normalized_score": [0.340406209230423], "tokens": [["\u0120El", "ton", "\u0120John"]], "entropy": [[6.383212089538574, 0.7957123517990112, 0.4601670503616333]], "avg_entropy": [2.5463638305664062], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Milira", "score_of_biggest_cluster": 0.340406209230423, "score_of_first_answer": 0.340406209230423, "max_score_of_first_answer": 0.9217725992202759, "min_score_of_first_answer": 0.04807957634329796, "avg_score": 0.340406209230423, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40459013055452997, "std_of_first_answer": 0.40459013055452997, "avg_entropy_of_first_answer": 2.5463638305664062, "lex_sim": 1.0}
{"question": "what nba player has scored the most 3 pointers", "answer": ["Drazen Petrovic"], "scores": [[0.004880648106336594, 0.01853359118103981, 0.9328106045722961, 0.9720925092697144, 0.9965564012527466]], "normalized_score": [0.15222570300102234], "tokens": [["\u0120D", "razen", "\u0120Pet", "rov", "ic"]], "entropy": [[4.696845054626465, 3.0121750831604004, 0.5250486135482788, 0.21700751781463623, 0.03019152209162712]], "avg_entropy": [1.6962535582482814], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ray Allen", "score_of_biggest_cluster": 0.15222570300102234, "score_of_first_answer": 0.15222570300102234, "max_score_of_first_answer": 0.9965564012527466, "min_score_of_first_answer": 0.004880648106336594, "avg_score": 0.15222570300102234, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4685326350153133, "std_of_first_answer": 0.4685326350153133, "avg_entropy_of_first_answer": 1.6962535582482814, "lex_sim": 1.0}
{"question": "when was the statue of liberty in france built", "answer": ["August 14, 1876"], "scores": [[0.02995537966489792, 0.15993846952915192, 0.4397543668746948, 0.4983498752117157, 0.5029212236404419]], "normalized_score": [0.22107236087322235], "tokens": [["\u0120August", "\u012014", ",", "\u012018", "76"]], "entropy": [[3.000972032546997, 3.2990639209747314, 1.2138950824737549, 2.1368985176086426, 2.1541409492492676]], "avg_entropy": [2.3609941005706787], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paris", "score_of_biggest_cluster": 0.22107236087322235, "score_of_first_answer": 0.22107236087322235, "max_score_of_first_answer": 0.5029212236404419, "min_score_of_first_answer": 0.02995537966489792, "avg_score": 0.22107236087322235, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19450672736266902, "std_of_first_answer": 0.19450672736266902, "avg_entropy_of_first_answer": 2.3609941005706787, "lex_sim": 1.0}
{"question": "what is the second largest country in asia", "answer": ["india"], "scores": [[0.0121036721393466, 0.9075545072555542]], "normalized_score": [0.10480811446905136], "tokens": [["\u0120ind", "ia"]], "entropy": [[2.9296510219573975, 0.37081634998321533]], "avg_entropy": [1.6502336859703064], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "China", "score_of_biggest_cluster": 0.10480811446905136, "score_of_first_answer": 0.10480811446905136, "max_score_of_first_answer": 0.9075545072555542, "min_score_of_first_answer": 0.0121036721393466, "avg_score": 0.10480811446905136, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4477254175581038, "std_of_first_answer": 0.4477254175581038, "avg_entropy_of_first_answer": 1.6502336859703064, "lex_sim": 1.0}
{"question": "what type of car is a g wagon", "answer": ["hatchback"], "scores": [[0.006143865641206503, 0.9479809999465942]], "normalized_score": [0.07631687819957733], "tokens": [["\u0120hatch", "back"]], "entropy": [[6.0826826095581055, 0.28609463572502136]], "avg_entropy": [3.1843886226415634], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mercedes-Benz", "score_of_biggest_cluster": 0.07631687819957733, "score_of_first_answer": 0.07631687819957733, "max_score_of_first_answer": 0.9479809999465942, "min_score_of_first_answer": 0.006143865641206503, "avg_score": 0.07631687819957733, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.47091856715269387, "std_of_first_answer": 0.47091856715269387, "avg_entropy_of_first_answer": 3.1843886226415634, "lex_sim": 1.0}
{"question": "what nfl coach has the most wins ever", "answer": ["Tom Landry"], "scores": [[0.16302098333835602, 0.8821778297424316, 0.9764391183853149]], "normalized_score": [0.5197744369506836], "tokens": [["\u0120Tom", "\u0120Land", "ry"]], "entropy": [[3.760658025741577, 0.6328455805778503, 0.2136930525302887]], "avg_entropy": [1.5357322196165721], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Shula, Don", "score_of_biggest_cluster": 0.5197744369506836, "score_of_first_answer": 0.5197744369506836, "max_score_of_first_answer": 0.9764391183853149, "min_score_of_first_answer": 0.16302098333835602, "avg_score": 0.5197744369506836, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3632753505247232, "std_of_first_answer": 0.3632753505247232, "avg_entropy_of_first_answer": 1.5357322196165721, "lex_sim": 1.0}
{"question": "when did the movie varsity blues come out", "answer": ["September 27, 1995"], "scores": [[0.0077275969088077545, 0.031313326209783554, 0.5586585402488708, 0.16210605204105377]], "normalized_score": [0.06841947883367538], "tokens": [["\u0120September", "\u012027", ",", "\u01201995"]], "entropy": [[5.096636772155762, 4.183332920074463, 1.0355658531188965, 3.424936294555664]], "avg_entropy": [3.4351179599761963], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1999", "score_of_biggest_cluster": 0.06841947883367538, "score_of_first_answer": 0.06841947883367538, "max_score_of_first_answer": 0.5586585402488708, "min_score_of_first_answer": 0.0077275969088077545, "avg_score": 0.06841947883367538, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.220846027552196, "std_of_first_answer": 0.220846027552196, "avg_entropy_of_first_answer": 3.4351179599761963, "lex_sim": 1.0}
{"question": "a bond that the issuer has the right to pay off before its maturity date", "answer": ["call"], "scores": [[0.14903323352336884]], "normalized_score": [0.14903323352336884], "tokens": [["\u0120call"]], "entropy": [[5.118350982666016]], "avg_entropy": [5.118350982666016], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "callable", "score_of_biggest_cluster": 0.14903323352336884, "score_of_first_answer": 0.14903323352336884, "max_score_of_first_answer": 0.14903323352336884, "min_score_of_first_answer": 0.14903323352336884, "avg_score": 0.14903323352336884, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.118350982666016, "lex_sim": 1.0}
{"question": "what are the toll roads called in mexico", "answer": ["Mexican highway"], "scores": [[0.007874149829149246, 0.05589151009917259]], "normalized_score": [0.020978515967726707], "tokens": [["\u0120Mexican", "\u0120highway"]], "entropy": [[4.156092643737793, 3.9560904502868652]], "avg_entropy": [4.056091547012329], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "autopistas", "score_of_biggest_cluster": 0.020978515967726707, "score_of_first_answer": 0.020978515967726707, "max_score_of_first_answer": 0.05589151009917259, "min_score_of_first_answer": 0.007874149829149246, "avg_score": 0.020978515967726707, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.024008680135011673, "std_of_first_answer": 0.024008680135011673, "avg_entropy_of_first_answer": 4.056091547012329, "lex_sim": 1.0}
{"question": "who directed the best years of our lives", "answer": ["Robert Altman"], "scores": [[0.007055269088596106, 0.14095886051654816, 0.9933419823646545]], "normalized_score": [0.09959439188241959], "tokens": [["\u0120Robert", "\u0120Alt", "man"]], "entropy": [[5.41635799407959, 3.5349605083465576, 0.055594101548194885]], "avg_entropy": [3.002304201324781], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "William Wyler", "score_of_biggest_cluster": 0.09959439188241959, "score_of_first_answer": 0.09959439188241959, "max_score_of_first_answer": 0.9933419823646545, "min_score_of_first_answer": 0.007055269088596106, "avg_score": 0.09959439188241959, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43681277949784764, "std_of_first_answer": 0.43681277949784764, "avg_entropy_of_first_answer": 3.002304201324781, "lex_sim": 1.0}
{"question": "form from material that has accumulated on the earths surface", "answer": ["lithosphere"], "scores": [[0.008640695363283157, 0.890984833240509]], "normalized_score": [0.0877423956990242], "tokens": [["\u0120lith", "osphere"]], "entropy": [[5.78549861907959, 0.6019134521484375]], "avg_entropy": [3.1937060356140137], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sedimentary rock", "score_of_biggest_cluster": 0.0877423956990242, "score_of_first_answer": 0.0877423956990242, "max_score_of_first_answer": 0.890984833240509, "min_score_of_first_answer": 0.008640695363283157, "avg_score": 0.0877423956990242, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44117206893861294, "std_of_first_answer": 0.44117206893861294, "avg_entropy_of_first_answer": 3.1937060356140137, "lex_sim": 1.0}
{"question": "who played the colorado kid in rio bravo", "answer": ["Sean Patrick Maloney"], "scores": [[0.008112544193863869, 0.14463944733142853, 0.0018863783916458488, 0.7988165020942688]], "normalized_score": [0.03646530583500862], "tokens": [["\u0120Sean", "\u0120Patrick", "\u0120Mal", "oney"]], "entropy": [[5.990384101867676, 2.962984085083008, 1.6953001022338867, 1.1526544094085693]], "avg_entropy": [2.950330674648285], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ricky Nelson", "score_of_biggest_cluster": 0.03646530583500862, "score_of_first_answer": 0.03646530583500862, "max_score_of_first_answer": 0.7988165020942688, "min_score_of_first_answer": 0.0018863783916458488, "avg_score": 0.03646530583500862, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3285683708525218, "std_of_first_answer": 0.3285683708525218, "avg_entropy_of_first_answer": 2.950330674648285, "lex_sim": 1.0}
{"question": "who got the most rings in the nba", "answer": ["lebron james"], "scores": [[0.0006350737530738115, 0.9814188480377197, 0.9999680519104004, 0.41939300298690796, 0.9939014911651611]], "normalized_score": [0.19183412194252014], "tokens": [["\u0120le", "b", "ron", "\u0120j", "ames"]], "entropy": [[3.559525489807129, 0.1517377644777298, 0.0005348980776034296, 1.2614858150482178, 0.052851639688014984]], "avg_entropy": [1.005227121419739], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bill Russell", "score_of_biggest_cluster": 0.19183412194252014, "score_of_first_answer": 0.19183412194252014, "max_score_of_first_answer": 0.9999680519104004, "min_score_of_first_answer": 0.0006350737530738115, "avg_score": 0.19183412194252014, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40526913160600664, "std_of_first_answer": 0.40526913160600664, "avg_entropy_of_first_answer": 1.005227121419739, "lex_sim": 1.0}
{"question": "how many times has south korea held the winter olympics", "answer": ["7"], "scores": [[0.004970243666321039]], "normalized_score": [0.004970243200659752], "tokens": [["\u01207"]], "entropy": [[3.095285177230835]], "avg_entropy": [3.095285177230835], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "two", "score_of_biggest_cluster": 0.004970243200659752, "score_of_first_answer": 0.004970243200659752, "max_score_of_first_answer": 0.004970243666321039, "min_score_of_first_answer": 0.004970243666321039, "avg_score": 0.004970243200659752, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.095285177230835, "lex_sim": 1.0}
{"question": "when will the flash season 4 episode 17 come out", "answer": ["May 16"], "scores": [[0.04697636887431145, 0.04774431139230728]], "normalized_score": [0.04735878109931946], "tokens": [["\u0120May", "\u012016"]], "entropy": [[4.3803606033325195, 3.6294822692871094]], "avg_entropy": [4.0049214363098145], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "April\u00a010,\u00a02018", "score_of_biggest_cluster": 0.04735878109931946, "score_of_first_answer": 0.04735878109931946, "max_score_of_first_answer": 0.04774431139230728, "min_score_of_first_answer": 0.04697636887431145, "avg_score": 0.04735878109931946, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0003839712589979172, "std_of_first_answer": 0.0003839712589979172, "avg_entropy_of_first_answer": 4.0049214363098145, "lex_sim": 1.0}
{"question": "who wins the next iron chef super chefs", "answer": ["Alain Ducasse"], "scores": [[0.05264478921890259, 0.20625320076942444, 0.8923799395561218, 0.985823392868042]], "normalized_score": [0.31262683868408203], "tokens": [["\u0120Al", "ain", "\u0120Duc", "asse"]], "entropy": [[5.961354732513428, 0.7867633104324341, 0.9361265897750854, 0.14273959398269653]], "avg_entropy": [1.956746056675911], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Zakarian", "score_of_biggest_cluster": 0.31262683868408203, "score_of_first_answer": 0.31262683868408203, "max_score_of_first_answer": 0.985823392868042, "min_score_of_first_answer": 0.05264478921890259, "avg_score": 0.31262683868408203, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40978685299289325, "std_of_first_answer": 0.40978685299289325, "avg_entropy_of_first_answer": 1.956746056675911, "lex_sim": 1.0}
{"question": "when was the first australian prime minister elected", "answer": ["sir stuart hamer"], "scores": [[0.0035940862726420164, 0.008484123274683952, 0.4592595100402832, 0.3634609282016754, 0.045033007860183716]], "normalized_score": [0.0469946563243866], "tokens": [["\u0120sir", "\u0120st", "uart", "\u0120h", "amer"]], "entropy": [[4.611013412475586, 4.605939865112305, 2.2287180423736572, 2.610414743423462, 3.8523926734924316]], "avg_entropy": [3.5816957473754885], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sir Edmund Barton", "score_of_biggest_cluster": 0.0469946563243866, "score_of_first_answer": 0.0469946563243866, "max_score_of_first_answer": 0.4592595100402832, "min_score_of_first_answer": 0.0035940862726420164, "avg_score": 0.0469946563243866, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19509754741456803, "std_of_first_answer": 0.19509754741456803, "avg_entropy_of_first_answer": 3.5816957473754885, "lex_sim": 1.0}
{"question": "who is the head of parliament in uk", "answer": ["Boris Johnson"], "scores": [[0.10979457199573517, 0.9229426383972168]], "normalized_score": [0.31833016872406006], "tokens": [["\u0120Boris", "\u0120Johnson"]], "entropy": [[4.440291404724121, 0.560274600982666]], "avg_entropy": [2.5002830028533936], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Lord Fowler", "score_of_biggest_cluster": 0.31833016872406006, "score_of_first_answer": 0.31833016872406006, "max_score_of_first_answer": 0.9229426383972168, "min_score_of_first_answer": 0.10979457199573517, "avg_score": 0.31833016872406006, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4065740332007408, "std_of_first_answer": 0.4065740332007408, "avg_entropy_of_first_answer": 2.5002830028533936, "lex_sim": 1.0}
{"question": "is parallax more pronounced with nearby stars or with distant stars", "answer": ["more pronounced with distance"], "scores": [[0.030794396996498108, 0.6620373129844666, 0.8849315047264099, 0.0037725253496319056]], "normalized_score": [0.09082888066768646], "tokens": [["\u0120more", "\u0120pronounced", "\u0120with", "\u0120distance"]], "entropy": [[3.8397645950317383, 1.5596492290496826, 0.6702015399932861, 0.9653967618942261]], "avg_entropy": [1.7587530314922333], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "nearby objects", "score_of_biggest_cluster": 0.09082888066768646, "score_of_first_answer": 0.09082888066768646, "max_score_of_first_answer": 0.8849315047264099, "min_score_of_first_answer": 0.0037725253496319056, "avg_score": 0.09082888066768646, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38634371821136043, "std_of_first_answer": 0.38634371821136043, "avg_entropy_of_first_answer": 1.7587530314922333, "lex_sim": 1.0}
{"question": "who played the girl in silence of the lambs", "answer": ["Nicole Kidman"], "scores": [[0.0033939082641154528, 0.9026361703872681, 0.9944244623184204]], "normalized_score": [0.14496450126171112], "tokens": [["\u0120Nicole", "\u0120Kid", "man"]], "entropy": [[5.249232769012451, 0.9140902757644653, 0.05448530614376068]], "avg_entropy": [2.072602783640226], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Masha Skorobogatov", "score_of_biggest_cluster": 0.14496450126171112, "score_of_first_answer": 0.14496450126171112, "max_score_of_first_answer": 0.9944244623184204, "min_score_of_first_answer": 0.0033939082641154528, "avg_score": 0.14496450126171112, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44711461314536766, "std_of_first_answer": 0.44711461314536766, "avg_entropy_of_first_answer": 2.072602783640226, "lex_sim": 1.0}
{"question": "created propaganda to support the war effort & encourage the sale of war bonds", "answer": ["propaganda"], "scores": [[0.04169813543558121]], "normalized_score": [0.041698139160871506], "tokens": [["\u0120propaganda"]], "entropy": [[6.052347183227539]], "avg_entropy": [6.052347183227539], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the government", "score_of_biggest_cluster": 0.041698139160871506, "score_of_first_answer": 0.041698139160871506, "max_score_of_first_answer": 0.04169813543558121, "min_score_of_first_answer": 0.04169813543558121, "avg_score": 0.041698139160871506, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.052347183227539, "lex_sim": 1.0}
{"question": "when did are you smarter than a 5th grader first air", "answer": ["2011"], "scores": [[0.005077018868178129]], "normalized_score": [0.005077018868178129], "tokens": [["\u01202011"]], "entropy": [[5.990144729614258]], "avg_entropy": [5.990144729614258], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "February 27, 2007", "score_of_biggest_cluster": 0.005077018868178129, "score_of_first_answer": 0.005077018868178129, "max_score_of_first_answer": 0.005077018868178129, "min_score_of_first_answer": 0.005077018868178129, "avg_score": 0.005077018868178129, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.990144729614258, "lex_sim": 1.0}
{"question": "is coca cola a public or private company", "answer": ["private"], "scores": [[0.2532670497894287]], "normalized_score": [0.2532670497894287], "tokens": [["\u0120private"]], "entropy": [[3.049057960510254]], "avg_entropy": [3.049057960510254], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Public", "score_of_biggest_cluster": 0.2532670497894287, "score_of_first_answer": 0.2532670497894287, "max_score_of_first_answer": 0.2532670497894287, "min_score_of_first_answer": 0.2532670497894287, "avg_score": 0.2532670497894287, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.049057960510254, "lex_sim": 1.0}
{"question": "where does this season's survivor take place", "answer": ["Hawaii"], "scores": [[0.0059193712659180164]], "normalized_score": [0.005919370334595442], "tokens": [["\u0120Hawaii"]], "entropy": [[6.234602928161621]], "avg_entropy": [6.234602928161621], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mamanuca Islands, Fiji", "score_of_biggest_cluster": 0.005919370334595442, "score_of_first_answer": 0.005919370334595442, "max_score_of_first_answer": 0.0059193712659180164, "min_score_of_first_answer": 0.0059193712659180164, "avg_score": 0.005919370334595442, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.234602928161621, "lex_sim": 1.0}
{"question": "who won entertainer of the year country music awards", "answer": ["Taylor Swift"], "scores": [[0.1278006136417389, 0.9053418636322021]], "normalized_score": [0.3401517868041992], "tokens": [["\u0120Taylor", "\u0120Swift"]], "entropy": [[4.531810760498047, 0.4596503973007202]], "avg_entropy": [2.4957305788993835], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Garth Brooks", "score_of_biggest_cluster": 0.3401517868041992, "score_of_first_answer": 0.3401517868041992, "max_score_of_first_answer": 0.9053418636322021, "min_score_of_first_answer": 0.1278006136417389, "avg_score": 0.3401517868041992, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38877062499523163, "std_of_first_answer": 0.38877062499523163, "avg_entropy_of_first_answer": 2.4957305788993835, "lex_sim": 1.0}
{"question": "who won the super heavyweight gold medal at the 2000 olympics", "answer": ["sascha pfeiffer"], "scores": [[0.0008143783779814839, 0.004153725225478411, 0.9209482073783875, 0.008212589658796787, 0.1660117208957672, 0.8338555097579956, 0.9912198185920715]], "normalized_score": [0.061972204595804214], "tokens": [["\u0120s", "as", "cha", "\u0120p", "fe", "iff", "er"]], "entropy": [[5.605003356933594, 3.561889171600342, 0.5432469844818115, 3.868903398513794, 3.838294506072998, 0.739966630935669, 0.07629748433828354]], "avg_entropy": [2.604800218982356], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Audley Harrison", "score_of_biggest_cluster": 0.061972204595804214, "score_of_first_answer": 0.061972204595804214, "max_score_of_first_answer": 0.9912198185920715, "min_score_of_first_answer": 0.0008143783779814839, "avg_score": 0.061972204595804214, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43608812566493266, "std_of_first_answer": 0.43608812566493266, "avg_entropy_of_first_answer": 2.604800218982356, "lex_sim": 1.0}
{"question": "who plays poppy in the beat goes on", "answer": ["Karen Allen"], "scores": [[0.008308385498821735, 0.2157316952943802]], "normalized_score": [0.04233652353286743], "tokens": [["\u0120Karen", "\u0120Allen"]], "entropy": [[6.914616584777832, 4.126955032348633]], "avg_entropy": [5.520785808563232], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Amanda Leighton", "score_of_biggest_cluster": 0.04233652353286743, "score_of_first_answer": 0.04233652353286743, "max_score_of_first_answer": 0.2157316952943802, "min_score_of_first_answer": 0.008308385498821735, "avg_score": 0.04233652353286743, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10371165489777923, "std_of_first_answer": 0.10371165489777923, "avg_entropy_of_first_answer": 5.520785808563232, "lex_sim": 1.0}
{"question": "who has the most all ireland hurling medals", "answer": ["Ireland 12"], "scores": [[0.046597883105278015, 0.001451459713280201]], "normalized_score": [0.00822404958307743], "tokens": [["\u0120Ireland", "\u012012"]], "entropy": [[5.696751594543457, 0.6346675157546997]], "avg_entropy": [3.1657095551490784], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Henry Shefflin", "score_of_biggest_cluster": 0.00822404958307743, "score_of_first_answer": 0.00822404958307743, "max_score_of_first_answer": 0.046597883105278015, "min_score_of_first_answer": 0.001451459713280201, "avg_score": 0.00822404958307743, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.022573211695998907, "std_of_first_answer": 0.022573211695998907, "avg_entropy_of_first_answer": 3.1657095551490784, "lex_sim": 1.0}
{"question": "where is the world's largest thermometer located", "answer": ["in sanya, china"], "scores": [[0.12094808369874954, 0.006040406413376331, 0.0039449166506528854, 0.6329656839370728, 0.2693219780921936, 0.9851775765419006]], "normalized_score": [0.08860906958580017], "tokens": [["\u0120in", "\u0120s", "anya", ",", "\u0120ch", "ina"]], "entropy": [[6.356664657592773, 5.539728164672852, 2.7268664836883545, 1.7564361095428467, 3.50661563873291, 0.11895114183425903]], "avg_entropy": [3.334210366010666], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Baker, California, USA", "score_of_biggest_cluster": 0.08860906958580017, "score_of_first_answer": 0.08860906958580017, "max_score_of_first_answer": 0.9851775765419006, "min_score_of_first_answer": 0.0039449166506528854, "avg_score": 0.08860906958580017, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3604271497110119, "std_of_first_answer": 0.3604271497110119, "avg_entropy_of_first_answer": 3.334210366010666, "lex_sim": 1.0}
{"question": "which nfl coach has the most superbowl rings", "answer": ["Dennis Green"], "scores": [[0.0022368370555341244, 0.5979933738708496]], "normalized_score": [0.03657340258359909], "tokens": [["\u0120Dennis", "\u0120Green"]], "entropy": [[4.236291885375977, 1.9338113069534302]], "avg_entropy": [3.0850515961647034], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bill Belichick", "score_of_biggest_cluster": 0.03657340258359909, "score_of_first_answer": 0.03657340258359909, "max_score_of_first_answer": 0.5979933738708496, "min_score_of_first_answer": 0.0022368370555341244, "avg_score": 0.03657340258359909, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29787826840765774, "std_of_first_answer": 0.29787826840765774, "avg_entropy_of_first_answer": 3.0850515961647034, "lex_sim": 1.0}
{"question": "who has the most green jackets from the masters", "answer": ["James Anderson 4"], "scores": [[0.005415643099695444, 0.024013575166463852, 0.0005781558575108647]], "normalized_score": [0.004220693837851286], "tokens": [["\u0120James", "\u0120Anderson", "\u01204"]], "entropy": [[6.993967056274414, 5.944709777832031, 0.6950902938842773]], "avg_entropy": [4.544589042663574], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nicklaus", "score_of_biggest_cluster": 0.004220693837851286, "score_of_first_answer": 0.004220693837851286, "max_score_of_first_answer": 0.024013575166463852, "min_score_of_first_answer": 0.0005781558575108647, "avg_score": 0.004220693837851286, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01010227278149096, "std_of_first_answer": 0.01010227278149096, "avg_entropy_of_first_answer": 4.544589042663574, "lex_sim": 1.0}
{"question": "the type of display that google glass presents to its users is called what", "answer": ["a heads up display"], "scores": [[0.10954318940639496, 0.5398656129837036, 0.8163698315620422, 0.9889706373214722]], "normalized_score": [0.46745020151138306], "tokens": [["\u0120a", "\u0120heads", "\u0120up", "\u0120display"]], "entropy": [[4.757615089416504, 3.2385222911834717, 0.5895447731018066, 0.09565220773220062]], "avg_entropy": [2.1703335903584957], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "head-up display", "score_of_biggest_cluster": 0.46745020151138306, "score_of_first_answer": 0.46745020151138306, "max_score_of_first_answer": 0.9889706373214722, "min_score_of_first_answer": 0.10954318940639496, "avg_score": 0.46745020151138306, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.33223813657965573, "std_of_first_answer": 0.33223813657965573, "avg_entropy_of_first_answer": 2.1703335903584957, "lex_sim": 1.0}
{"question": "when was the defensive 3 second rule implemented", "answer": ["1974"], "scores": [[0.00944790430366993]], "normalized_score": [0.00944790244102478], "tokens": [["\u01201974"]], "entropy": [[5.382646083831787]], "avg_entropy": [5.382646083831787], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the 2001\u20132002 season", "score_of_biggest_cluster": 0.00944790244102478, "score_of_first_answer": 0.00944790244102478, "max_score_of_first_answer": 0.00944790430366993, "min_score_of_first_answer": 0.00944790430366993, "avg_score": 0.00944790244102478, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.382646083831787, "lex_sim": 1.0}
{"question": "what is mercy mercy me by marvin gaye about", "answer": ["the singer Marvin Gaye"], "scores": [[0.08383253961801529, 0.020443573594093323, 0.04009934142231941, 0.9780452251434326, 0.944180428981781]], "normalized_score": [0.14471185207366943], "tokens": [["\u0120the", "\u0120singer", "\u0120Marvin", "\u0120Gay", "e"]], "entropy": [[5.804042816162109, 6.992801666259766, 2.7276437282562256, 0.21062979102134705, 0.37608152627944946]], "avg_entropy": [3.2222399055957793], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "sorrow regarding the environment", "score_of_biggest_cluster": 0.14471185207366943, "score_of_first_answer": 0.14471185207366943, "max_score_of_first_answer": 0.9780452251434326, "min_score_of_first_answer": 0.020443573594093323, "avg_score": 0.14471185207366943, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4478693800507864, "std_of_first_answer": 0.4478693800507864, "avg_entropy_of_first_answer": 3.2222399055957793, "lex_sim": 1.0}
{"question": "what is the value of the currency in paraguay", "answer": ["6.543"], "scores": [[0.024918587878346443, 0.38551175594329834, 0.08494337648153305, 0.004387325141578913]], "normalized_score": [0.04349833354353905], "tokens": [["\u01206", ".", "5", "43"]], "entropy": [[4.961991786956787, 2.903899908065796, 4.69828987121582, 3.6168947219848633]], "avg_entropy": [4.045269072055817], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "126 PYG to 1 USD", "score_of_biggest_cluster": 0.04349833354353905, "score_of_first_answer": 0.04349833354353905, "max_score_of_first_answer": 0.38551175594329834, "min_score_of_first_answer": 0.004387325141578913, "avg_score": 0.04349833354353905, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.15332530791287233, "std_of_first_answer": 0.15332530791287233, "avg_entropy_of_first_answer": 4.045269072055817, "lex_sim": 1.0}
{"question": "what is the definition of the word hosanna", "answer": ["hosanna"], "scores": [[0.014895630069077015, 0.6201919317245483, 0.9119784235954285]], "normalized_score": [0.20348069071769714], "tokens": [["\u0120h", "os", "anna"]], "entropy": [[5.818984508514404, 1.5471240282058716, 0.46711593866348267]], "avg_entropy": [2.6110748251279197], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "save, rescue, savior", "score_of_biggest_cluster": 0.20348069071769714, "score_of_first_answer": 0.20348069071769714, "max_score_of_first_answer": 0.9119784235954285, "min_score_of_first_answer": 0.014895630069077015, "avg_score": 0.20348069071769714, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3736130660451843, "std_of_first_answer": 0.3736130660451843, "avg_entropy_of_first_answer": 2.6110748251279197, "lex_sim": 1.0}
{"question": "who is the coach for the ottawa senators", "answer": ["Mark Dubas"], "scores": [[0.012374241836369038, 0.00552538875490427, 0.02758820727467537]], "normalized_score": [0.01235573086887598], "tokens": [["\u0120Mark", "\u0120Dub", "as"]], "entropy": [[5.156363487243652, 5.406954765319824, 2.2550933361053467]], "avg_entropy": [4.2728038628896075], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Guy Boucher", "score_of_biggest_cluster": 0.01235573086887598, "score_of_first_answer": 0.01235573086887598, "max_score_of_first_answer": 0.02758820727467537, "min_score_of_first_answer": 0.00552538875490427, "avg_score": 0.01235573086887598, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.00922038498789999, "std_of_first_answer": 0.00922038498789999, "avg_entropy_of_first_answer": 4.2728038628896075, "lex_sim": 1.0}
{"question": "who was the father of the princes in the tower", "answer": ["Richard II"], "scores": [[0.06470337510108948, 0.35328441858291626]], "normalized_score": [0.15119092166423798], "tokens": [["\u0120Richard", "\u0120II"]], "entropy": [[5.077309608459473, 3.160712718963623]], "avg_entropy": [4.119011163711548], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Edward IV of England", "score_of_biggest_cluster": 0.15119092166423798, "score_of_first_answer": 0.15119092166423798, "max_score_of_first_answer": 0.35328441858291626, "min_score_of_first_answer": 0.06470337510108948, "avg_score": 0.15119092166423798, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1442905217409134, "std_of_first_answer": 0.1442905217409134, "avg_entropy_of_first_answer": 4.119011163711548, "lex_sim": 1.0}
{"question": "when is i can only imagine coming out", "answer": ["October 3, 2014"], "scores": [[0.05825269967317581, 0.02163936197757721, 0.04461098462343216, 0.10497599840164185]], "normalized_score": [0.04929162189364433], "tokens": [["\u0120October", "\u01203", ",", "\u01202014"]], "entropy": [[4.806999206542969, 3.662527561187744, 0.5382555723190308, 3.250887870788574]], "avg_entropy": [3.0646675527095795], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "March 16, 2018", "score_of_biggest_cluster": 0.04929162189364433, "score_of_first_answer": 0.04929162189364433, "max_score_of_first_answer": 0.10497599840164185, "min_score_of_first_answer": 0.02163936197757721, "avg_score": 0.04929162189364433, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.030440849237377107, "std_of_first_answer": 0.030440849237377107, "avg_entropy_of_first_answer": 3.0646675527095795, "lex_sim": 1.0}
{"question": "when does elena turn into a vampire in the tv series", "answer": ["Breaking Dawn"], "scores": [[0.00014167370682116598, 0.8066704273223877]], "normalized_score": [0.010690368711948395], "tokens": [["\u0120Breaking", "\u0120Dawn"]], "entropy": [[5.469000816345215, 0.8304474949836731]], "avg_entropy": [3.149724155664444], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "season four", "score_of_biggest_cluster": 0.010690368711948395, "score_of_first_answer": 0.010690368711948395, "max_score_of_first_answer": 0.8066704273223877, "min_score_of_first_answer": 0.00014167370682116598, "avg_score": 0.010690368711948395, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40326437680778326, "std_of_first_answer": 0.40326437680778326, "avg_entropy_of_first_answer": 3.149724155664444, "lex_sim": 1.0}
{"question": "where does stevia in the raw come from", "answer": ["stevioside and rebaudioside a"], "scores": [[0.1467868685722351, 0.9686473608016968, 0.048890016973018646, 0.8526443839073181, 0.024594157934188843, 0.6380427479743958, 0.897135853767395, 0.9649345278739929, 0.9688336253166199, 0.13434042036533356]], "normalized_score": [0.31771185994148254], "tokens": [["\u0120st", "ev", "ios", "ide", "\u0120and", "\u0120reb", "aud", "ios", "ide", "\u0120a"]], "entropy": [[5.1913604736328125, 0.282694011926651, 0.7799056172370911, 0.8119082450866699, 0.8690875172615051, 1.7161128520965576, 0.6050146818161011, 0.31961172819137573, 0.15850120782852173, 1.6325793266296387]], "avg_entropy": [1.2366775661706924], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "South America", "score_of_biggest_cluster": 0.31771185994148254, "score_of_first_answer": 0.31771185994148254, "max_score_of_first_answer": 0.9688336253166199, "min_score_of_first_answer": 0.024594157934188843, "avg_score": 0.31771185994148254, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.400390360604284, "std_of_first_answer": 0.400390360604284, "avg_entropy_of_first_answer": 1.2366775661706924, "lex_sim": 1.0}
{"question": "who is the actor that plays jt on the young and the restless", "answer": ["Chris Pine"], "scores": [[0.01945755071938038, 0.17591708898544312]], "normalized_score": [0.058505695313215256], "tokens": [["\u0120Chris", "\u0120Pine"]], "entropy": [[5.928622245788574, 3.921034336090088]], "avg_entropy": [4.924828290939331], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Thaddeus Rowe Luckinbill", "score_of_biggest_cluster": 0.058505695313215256, "score_of_first_answer": 0.058505695313215256, "max_score_of_first_answer": 0.17591708898544312, "min_score_of_first_answer": 0.01945755071938038, "avg_score": 0.058505695313215256, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07822976913303137, "std_of_first_answer": 0.07822976913303137, "avg_entropy_of_first_answer": 4.924828290939331, "lex_sim": 1.0}
{"question": "where does the last name roberts come from", "answer": ["James"], "scores": [[0.02579681947827339]], "normalized_score": [0.02579681947827339], "tokens": [["\u0120James"]], "entropy": [[6.360471725463867]], "avg_entropy": [6.360471725463867], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Norman", "score_of_biggest_cluster": 0.02579681947827339, "score_of_first_answer": 0.02579681947827339, "max_score_of_first_answer": 0.02579681947827339, "min_score_of_first_answer": 0.02579681947827339, "avg_score": 0.02579681947827339, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.360471725463867, "lex_sim": 1.0}
{"question": "who won the king of dance season 2", "answer": ["Apolo Ohno"], "scores": [[0.0036685143131762743, 0.49781063199043274, 0.7472955584526062, 0.9957850575447083]], "normalized_score": [0.1920008361339569], "tokens": [["\u0120Ap", "olo", "\u0120Oh", "no"]], "entropy": [[6.843171119689941, 1.3888800144195557, 1.6077134609222412, 0.0459543913602829]], "avg_entropy": [2.4714297465980053], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "LAAB Crew From Team Sherif", "score_of_biggest_cluster": 0.1920008361339569, "score_of_first_answer": 0.1920008361339569, "max_score_of_first_answer": 0.9957850575447083, "min_score_of_first_answer": 0.0036685143131762743, "avg_score": 0.1920008361339569, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3668634910280648, "std_of_first_answer": 0.3668634910280648, "avg_entropy_of_first_answer": 2.4714297465980053, "lex_sim": 1.0}
{"question": "how many pennsylvania house of representatives are there", "answer": ["47"], "scores": [[0.016427651047706604]], "normalized_score": [0.016427649185061455], "tokens": [["\u012047"]], "entropy": [[5.096133232116699]], "avg_entropy": [5.096133232116699], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "203", "score_of_biggest_cluster": 0.016427649185061455, "score_of_first_answer": 0.016427649185061455, "max_score_of_first_answer": 0.016427651047706604, "min_score_of_first_answer": 0.016427651047706604, "avg_score": 0.016427649185061455, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.096133232116699, "lex_sim": 1.0}
{"question": "who is the winner of bigg boss kannada season", "answer": ["goswami siriyappa"], "scores": [[0.0009752134210430086, 0.0024697957560420036, 0.8931394815444946, 0.9527912139892578, 0.008462347090244293, 0.005258649587631226, 0.03180909901857376]], "normalized_score": [0.022480154410004616], "tokens": [["\u0120g", "os", "w", "ami", "\u0120sir", "iy", "appa"]], "entropy": [[6.0615620613098145, 4.053933143615723, 0.7463036775588989, 0.23462295532226562, 4.363860130310059, 3.5098154544830322, 2.3063018321990967]], "avg_entropy": [3.039485607828413], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chandan Shetty", "score_of_biggest_cluster": 0.022480154410004616, "score_of_first_answer": 0.022480154410004616, "max_score_of_first_answer": 0.9527912139892578, "min_score_of_first_answer": 0.0009752134210430086, "avg_score": 0.022480154410004616, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41294669234484016, "std_of_first_answer": 0.41294669234484016, "avg_entropy_of_first_answer": 3.039485607828413, "lex_sim": 1.0}
{"question": "what territory is spain and the united kingdom in a dispute over", "answer": ["spain is concerned about the british overseas territories, eg, the canary islands."], "scores": [[0.029598066583275795, 0.9694216251373291, 0.1923266500234604, 0.00035421736538410187, 0.3054957091808319, 0.4794095754623413, 0.006264541298151016, 0.9921971559524536, 0.9653374552726746, 0.015758752822875977, 0.6437901258468628, 0.05413787439465523, 0.0021496987901628017, 0.09482459723949432, 0.25586771965026855, 0.03265618532896042, 0.4846172034740448, 0.9182102084159851, 0.10527515411376953]], "normalized_score": [0.0985165610909462], "tokens": [["\u0120sp", "ain", "\u0120is", "\u0120concerned", "\u0120about", "\u0120the", "\u0120b", "rit", "ish", "\u0120overseas", "\u0120territories", ",", "\u0120eg", ",", "\u0120the", "\u0120can", "ary", "\u0120islands", "."]], "entropy": [[4.8502278327941895, 0.1940014660358429, 3.1307120323181152, 3.50850772857666, 2.4280107021331787, 3.5045104026794434, 6.066701889038086, 0.0705120712518692, 0.22153550386428833, 5.9587602615356445, 1.6541200876235962, 2.4858474731445312, 3.671419143676758, 3.9572222232818604, 3.6962130069732666, 4.769728660583496, 0.7738829851150513, 0.4352148771286011, 1.7763385772705078]], "avg_entropy": [2.797550890790789], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gibraltar", "score_of_biggest_cluster": 0.0985165610909462, "score_of_first_answer": 0.0985165610909462, "max_score_of_first_answer": 0.9921971559524536, "min_score_of_first_answer": 0.00035421736538410187, "avg_score": 0.0985165610909462, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3658417157770958, "std_of_first_answer": 0.3658417157770958, "avg_entropy_of_first_answer": 2.797550890790789, "lex_sim": 1.0}
{"question": "what is the name of the restaurant in seinfeld", "answer": ["Monk's Caf\u00e9"], "scores": [[0.0735100582242012, 0.7236956357955933, 0.09693481773138046]], "normalized_score": [0.1727670282125473], "tokens": [["\u0120Monk", "'s", "\u0120Caf\u00c3\u00a9"]], "entropy": [[5.489547252655029, 1.146559476852417, 1.957374930381775]], "avg_entropy": [2.8644938866297402], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Monk\u2019s", "score_of_biggest_cluster": 0.1727670282125473, "score_of_first_answer": 0.1727670282125473, "max_score_of_first_answer": 0.7236956357955933, "min_score_of_first_answer": 0.0735100582242012, "avg_score": 0.1727670282125473, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.30113103971164285, "std_of_first_answer": 0.30113103971164285, "avg_entropy_of_first_answer": 2.8644938866297402, "lex_sim": 1.0}
{"question": "who played king theoden in lord of the rings", "answer": ["Richard Carver"], "scores": [[0.00802201684564352, 0.0014778745826333761, 0.08945059031248093]], "normalized_score": [0.010197678580880165], "tokens": [["\u0120Richard", "\u0120Car", "ver"]], "entropy": [[4.5933427810668945, 4.020008087158203, 4.209607124328613]], "avg_entropy": [4.274319330851237], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bernard Hill", "score_of_biggest_cluster": 0.010197678580880165, "score_of_first_answer": 0.010197678580880165, "max_score_of_first_answer": 0.08945059031248093, "min_score_of_first_answer": 0.0014778745826333761, "avg_score": 0.010197678580880165, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.040017547661671315, "std_of_first_answer": 0.040017547661671315, "avg_entropy_of_first_answer": 4.274319330851237, "lex_sim": 1.0}
{"question": "when does sloan come into grey's anatomy", "answer": ["sloan grey"], "scores": [[0.0049802097491919994, 0.9192877411842346, 0.04935024678707123]], "normalized_score": [0.06090638041496277], "tokens": [["\u0120sl", "oan", "\u0120grey"]], "entropy": [[5.540290832519531, 0.3612791895866394, 5.671809196472168]], "avg_entropy": [3.857793072859446], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "season two", "score_of_biggest_cluster": 0.06090638041496277, "score_of_first_answer": 0.06090638041496277, "max_score_of_first_answer": 0.9192877411842346, "min_score_of_first_answer": 0.0049802097491919994, "avg_score": 0.06090638041496277, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42094050864748084, "std_of_first_answer": 0.42094050864748084, "avg_entropy_of_first_answer": 3.857793072859446, "lex_sim": 1.0}
{"question": "where does the journey start in the canterbury tales", "answer": ["rome"], "scores": [[0.001953233266249299, 0.4684836268424988]], "normalized_score": [0.03024992160499096], "tokens": [["\u0120r", "ome"]], "entropy": [[5.922036647796631, 2.8484368324279785]], "avg_entropy": [4.385236740112305], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "London", "score_of_biggest_cluster": 0.03024992160499096, "score_of_first_answer": 0.03024992160499096, "max_score_of_first_answer": 0.4684836268424988, "min_score_of_first_answer": 0.001953233266249299, "avg_score": 0.03024992160499096, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23326519678812474, "std_of_first_answer": 0.23326519678812474, "avg_entropy_of_first_answer": 4.385236740112305, "lex_sim": 1.0}
{"question": "who sang how can you mend a broken heart first", "answer": ["Sam Smith"], "scores": [[0.008391494862735271, 0.6715036034584045]], "normalized_score": [0.07506609708070755], "tokens": [["\u0120Sam", "\u0120Smith"]], "entropy": [[6.307116985321045, 1.7174062728881836]], "avg_entropy": [4.012261629104614], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bee Gees", "score_of_biggest_cluster": 0.07506609708070755, "score_of_first_answer": 0.07506609708070755, "max_score_of_first_answer": 0.6715036034584045, "min_score_of_first_answer": 0.008391494862735271, "avg_score": 0.07506609708070755, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33155605429783463, "std_of_first_answer": 0.33155605429783463, "avg_entropy_of_first_answer": 4.012261629104614, "lex_sim": 1.0}
{"question": "what is the title of the person who runs the house of representatives", "answer": ["Speaker of the House"], "scores": [[0.21687503159046173, 0.7318543791770935, 0.9788215160369873, 0.8420093059539795]], "normalized_score": [0.6014003157615662], "tokens": [["\u0120Speaker", "\u0120of", "\u0120the", "\u0120House"]], "entropy": [[4.708235740661621, 0.9015262126922607, 0.1433062106370926, 0.7086153030395508]], "avg_entropy": [1.6154208667576313], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Speaker of the House", "score_of_biggest_cluster": 0.6014003157615662, "score_of_first_answer": 0.6014003157615662, "max_score_of_first_answer": 0.9788215160369873, "min_score_of_first_answer": 0.21687503159046173, "avg_score": 0.6014003157615662, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2881409691881482, "std_of_first_answer": 0.2881409691881482, "avg_entropy_of_first_answer": 1.6154208667576313, "lex_sim": 1.0}
{"question": "theme of the song roar by katy perry", "answer": ["i wonder"], "scores": [[0.0037019650917500257, 0.005938869901001453]], "normalized_score": [0.004688867833465338], "tokens": [["\u0120i", "\u0120wonder"]], "entropy": [[7.580866813659668, 4.169584274291992]], "avg_entropy": [5.87522554397583], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "self-empowerment", "score_of_biggest_cluster": 0.004688867833465338, "score_of_first_answer": 0.004688867833465338, "max_score_of_first_answer": 0.005938869901001453, "min_score_of_first_answer": 0.0037019650917500257, "avg_score": 0.004688867833465338, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0011184524046257138, "std_of_first_answer": 0.0011184524046257138, "avg_entropy_of_first_answer": 5.87522554397583, "lex_sim": 1.0}
{"question": "when did the great plague of london start and end", "answer": ["between alexander and carolina"], "scores": [[0.006160270422697067, 0.007978374138474464, 0.023564094677567482, 0.8874655961990356, 0.1549617201089859, 0.1292928159236908, 0.0028041647747159004]], "normalized_score": [0.034463535994291306], "tokens": [["\u0120between", "\u0120a", "lex", "ander", "\u0120and", "\u0120car", "olina"]], "entropy": [[3.7829196453094482, 3.2338900566101074, 2.5399582386016846, 0.7188997268676758, 2.0062854290008545, 4.94899320602417, 3.4565229415893555]], "avg_entropy": [2.955352749143328], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1665 to 1666", "score_of_biggest_cluster": 0.034463535994291306, "score_of_first_answer": 0.034463535994291306, "max_score_of_first_answer": 0.8874655961990356, "min_score_of_first_answer": 0.0028041647747159004, "avg_score": 0.034463535994291306, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2973842908330778, "std_of_first_answer": 0.2973842908330778, "avg_entropy_of_first_answer": 2.955352749143328, "lex_sim": 1.0}
{"question": "who did michigan state play in the rose bowl", "answer": ["Iowa"], "scores": [[0.014359013177454472]], "normalized_score": [0.01435901690274477], "tokens": [["\u0120Iowa"]], "entropy": [[4.106675148010254]], "avg_entropy": [4.106675148010254], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stanford Cardinal", "score_of_biggest_cluster": 0.01435901690274477, "score_of_first_answer": 0.01435901690274477, "max_score_of_first_answer": 0.014359013177454472, "min_score_of_first_answer": 0.014359013177454472, "avg_score": 0.01435901690274477, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.106675148010254, "lex_sim": 1.0}
{"question": "where does patience is a virtue come from", "answer": ["Latin, patienceem -- it means to be \"patient in spirit\""], "scores": [[0.0638551190495491, 0.04602661728858948, 0.04401938244700432, 0.057988591492176056, 0.001453230157494545, 0.015431800857186317, 0.6864821910858154, 0.21867400407791138, 0.07139953225851059, 0.022009626030921936, 0.2220812886953354, 0.02305619977414608, 0.021018559113144875, 0.6287453770637512]], "normalized_score": [0.055581770837306976], "tokens": [["\u0120Latin", ",", "\u0120patience", "em", "\u0120--", "\u0120it", "\u0120means", "\u0120to", "\u0120be", "\u0120\"", "patient", "\u0120in", "\u0120spirit", "\""]], "entropy": [[6.115856647491455, 2.0216064453125, 5.116250514984131, 2.8875484466552734, 3.8163681030273438, 4.447742938995361, 1.4393640756607056, 2.6127355098724365, 2.554868221282959, 3.3733794689178467, 3.991079568862915, 2.1958987712860107, 3.4836041927337646, 1.5244050025939941]], "avg_entropy": [3.2557648505483354], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\"Psychomachia,\"", "score_of_biggest_cluster": 0.055581770837306976, "score_of_first_answer": 0.055581770837306976, "max_score_of_first_answer": 0.6864821910858154, "min_score_of_first_answer": 0.001453230157494545, "avg_score": 0.055581770837306976, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2171736339741003, "std_of_first_answer": 0.2171736339741003, "avg_entropy_of_first_answer": 3.2557648505483354, "lex_sim": 1.0}
{"question": "spyder from once upon a time in venice", "answer": ["the name was revealed in this way"], "scores": [[0.004873195663094521, 0.023103496059775352, 0.030504120513796806, 0.010685533285140991, 0.4136779308319092, 0.005332975182682276, 0.000780687783844769]], "normalized_score": [0.013013483956456184], "tokens": [["\u0120the", "\u0120name", "\u0120was", "\u0120revealed", "\u0120in", "\u0120this", "\u0120way"]], "entropy": [[6.862824440002441, 7.319911003112793, 3.064581871032715, 5.7470245361328125, 2.5713088512420654, 3.3544774055480957, 4.1288580894470215]], "avg_entropy": [4.721283742359707], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jason Momoa", "score_of_biggest_cluster": 0.013013483956456184, "score_of_first_answer": 0.013013483956456184, "max_score_of_first_answer": 0.4136779308319092, "min_score_of_first_answer": 0.000780687783844769, "avg_score": 0.013013483956456184, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14071572938735383, "std_of_first_answer": 0.14071572938735383, "avg_entropy_of_first_answer": 4.721283742359707, "lex_sim": 1.0}
{"question": "which government had more power under the articles of confederation", "answer": ["USA"], "scores": [[0.013079249300062656]], "normalized_score": [0.01307925209403038], "tokens": [["\u0120USA"]], "entropy": [[4.041497230529785]], "avg_entropy": [4.041497230529785], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the states", "score_of_biggest_cluster": 0.01307925209403038, "score_of_first_answer": 0.01307925209403038, "max_score_of_first_answer": 0.013079249300062656, "min_score_of_first_answer": 0.013079249300062656, "avg_score": 0.01307925209403038, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.041497230529785, "lex_sim": 1.0}
{"question": "who sang the most number of songs in the world", "answer": ["Eminem"], "scores": [[0.011134019121527672]], "normalized_score": [0.011134016327559948], "tokens": [["\u0120Eminem"]], "entropy": [[5.654446601867676]], "avg_entropy": [5.654446601867676], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Asha Bhosle", "score_of_biggest_cluster": 0.011134016327559948, "score_of_first_answer": 0.011134016327559948, "max_score_of_first_answer": 0.011134019121527672, "min_score_of_first_answer": 0.011134019121527672, "avg_score": 0.011134016327559948, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.654446601867676, "lex_sim": 1.0}
{"question": "where does the electron transport chain get its electrons from", "answer": ["the electron transport chain transfers electrons from NADH and FADH2 molecules to oxygen through complex IV."], "scores": [[0.17580008506774902, 0.026342125609517097, 0.4509889781475067, 0.7531319856643677, 0.0024986390490084887, 0.4431156814098358, 0.6327888369560242, 0.12632912397384644, 0.8735500574111938, 0.07724696397781372, 0.7529025673866272, 0.988006055355072, 0.9064001441001892, 0.7492654323577881, 0.007395995315164328, 0.5907236337661743, 0.22341522574424744, 0.02229100652039051, 0.0762210413813591, 0.08481680601835251, 0.16625556349754333]], "normalized_score": [0.17388051748275757], "tokens": [["\u0120the", "\u0120electron", "\u0120transport", "\u0120chain", "\u0120transfers", "\u0120electrons", "\u0120from", "\u0120NAD", "H", "\u0120and", "\u0120F", "AD", "H", "2", "\u0120molecules", "\u0120to", "\u0120oxygen", "\u0120through", "\u0120complex", "\u0120IV", "."]], "entropy": [[5.5390214920043945, 5.200748443603516, 2.715794086456299, 1.387902855873108, 3.3269522190093994, 2.722862720489502, 1.8014248609542847, 3.8367977142333984, 0.7127344012260437, 1.6192868947982788, 1.3778306245803833, 0.1114550530910492, 0.5121138095855713, 1.1737936735153198, 1.4090228080749512, 1.917252779006958, 3.5543034076690674, 2.6556782722473145, 3.346886157989502, 2.4003124237060547, 2.453158378601074]], "avg_entropy": [2.37025395603407], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "electron donors", "score_of_biggest_cluster": 0.17388051748275757, "score_of_first_answer": 0.17388051748275757, "max_score_of_first_answer": 0.988006055355072, "min_score_of_first_answer": 0.0024986390490084887, "avg_score": 0.17388051748275757, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3382703396636717, "std_of_first_answer": 0.3382703396636717, "avg_entropy_of_first_answer": 2.37025395603407, "lex_sim": 1.0}
{"question": "what is the point of a menstrual cup", "answer": ["to capture and store menstrual fluid"], "scores": [[0.14235331118106842, 0.011529602110385895, 0.13977651298046112, 0.1888665407896042, 0.32336094975471497, 0.31565308570861816]], "normalized_score": [0.1281183660030365], "tokens": [["\u0120to", "\u0120capture", "\u0120and", "\u0120store", "\u0120menstrual", "\u0120fluid"]], "entropy": [[5.630220413208008, 3.4172749519348145, 3.770613670349121, 3.5460615158081055, 3.3884775638580322, 1.4245628118515015]], "avg_entropy": [3.529535154501597], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "to collect menstrual flow", "score_of_biggest_cluster": 0.1281183660030365, "score_of_first_answer": 0.1281183660030365, "max_score_of_first_answer": 0.32336094975471497, "min_score_of_first_answer": 0.011529602110385895, "avg_score": 0.1281183660030365, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.10813773537555028, "std_of_first_answer": 0.10813773537555028, "avg_entropy_of_first_answer": 3.529535154501597, "lex_sim": 1.0}
{"question": "when did lord howe island became a world heritage site", "answer": ["1997"], "scores": [[0.00876394473016262]], "normalized_score": [0.00876394473016262], "tokens": [["\u01201997"]], "entropy": [[5.13560676574707]], "avg_entropy": [5.13560676574707], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in 1982", "score_of_biggest_cluster": 0.00876394473016262, "score_of_first_answer": 0.00876394473016262, "max_score_of_first_answer": 0.00876394473016262, "min_score_of_first_answer": 0.00876394473016262, "avg_score": 0.00876394473016262, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.13560676574707, "lex_sim": 1.0}
{"question": "what is the current mac os operating system", "answer": ["MacOS X Sierra"], "scores": [[0.05212847515940666, 0.2207268923521042, 0.0442272387444973, 0.006471789442002773]], "normalized_score": [0.04260014370083809], "tokens": [["\u0120Mac", "OS", "\u0120X", "\u0120Sierra"]], "entropy": [[4.048929214477539, 1.2956469058990479, 2.9285550117492676, 2.1799447536468506]], "avg_entropy": [2.6132689714431763], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "10.13.4", "score_of_biggest_cluster": 0.04260014370083809, "score_of_first_answer": 0.04260014370083809, "max_score_of_first_answer": 0.2207268923521042, "min_score_of_first_answer": 0.006471789442002773, "avg_score": 0.04260014370083809, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08255878192465414, "std_of_first_answer": 0.08255878192465414, "avg_entropy_of_first_answer": 2.6132689714431763, "lex_sim": 1.0}
{"question": "when do new episodes of ncis los angeles return", "answer": ["February 15, 2019"], "scores": [[0.05385766550898552, 0.02974480204284191, 0.03567951172590256, 0.16277290880680084]], "normalized_score": [0.05522870644927025], "tokens": [["\u0120February", "\u012015", ",", "\u01202019"]], "entropy": [[4.546415328979492, 3.4497714042663574, 0.7297794222831726, 2.6602890491485596]], "avg_entropy": [2.8465638011693954], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "May\u00a013,\u00a02018", "score_of_biggest_cluster": 0.05522870644927025, "score_of_first_answer": 0.05522870644927025, "max_score_of_first_answer": 0.16277290880680084, "min_score_of_first_answer": 0.02974480204284191, "avg_score": 0.05522870644927025, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.054001640734280563, "std_of_first_answer": 0.054001640734280563, "avg_entropy_of_first_answer": 2.8465638011693954, "lex_sim": 1.0}
{"question": "what is the pirates of the caribbean in order", "answer": ["The Pirates Of The Caribbean"], "scores": [[0.08626224845647812, 0.2740895748138428, 0.04108153656125069, 0.9414027333259583, 0.8681935667991638]], "normalized_score": [0.23985590040683746], "tokens": [["\u0120The", "\u0120Pirates", "\u0120Of", "\u0120The", "\u0120Caribbean"]], "entropy": [[6.174737930297852, 5.216771125793457, 0.5494860410690308, 0.39483851194381714, 0.587131679058075]], "avg_entropy": [2.5845930576324463], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "On Stranger Tides", "score_of_biggest_cluster": 0.23985590040683746, "score_of_first_answer": 0.23985590040683746, "max_score_of_first_answer": 0.9414027333259583, "min_score_of_first_answer": 0.04108153656125069, "avg_score": 0.23985590040683746, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3863994584571853, "std_of_first_answer": 0.3863994584571853, "avg_entropy_of_first_answer": 2.5845930576324463, "lex_sim": 1.0}
{"question": "when did scotland beat england in six nations", "answer": ["June 14, 2010"], "scores": [[0.010494593530893326, 0.04363210126757622, 0.18427862226963043, 0.024039238691329956]], "normalized_score": [0.03773913532495499], "tokens": [["\u0120June", "\u012014", ",", "\u01202010"]], "entropy": [[5.529282569885254, 3.9307045936584473, 1.0758178234100342, 4.194903373718262]], "avg_entropy": [3.6826770901679993], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2 April 2000", "score_of_biggest_cluster": 0.03773913532495499, "score_of_first_answer": 0.03773913532495499, "max_score_of_first_answer": 0.18427862226963043, "min_score_of_first_answer": 0.010494593530893326, "avg_score": 0.03773913532495499, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.069518174179335, "std_of_first_answer": 0.069518174179335, "avg_entropy_of_first_answer": 3.6826770901679993, "lex_sim": 1.0}
{"question": "who played junior on in the heat of the night", "answer": ["Corey Feldman"], "scores": [[0.00807121116667986, 0.1697508990764618]], "normalized_score": [0.03701479732990265], "tokens": [["\u0120Corey", "\u0120Feldman"]], "entropy": [[6.283357620239258, 1.905580997467041]], "avg_entropy": [4.094469308853149], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Christian LeBlanc", "score_of_biggest_cluster": 0.03701479732990265, "score_of_first_answer": 0.03701479732990265, "max_score_of_first_answer": 0.1697508990764618, "min_score_of_first_answer": 0.00807121116667986, "avg_score": 0.03701479732990265, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08083984395489097, "std_of_first_answer": 0.08083984395489097, "avg_entropy_of_first_answer": 4.094469308853149, "lex_sim": 1.0}
{"question": "who won the ncaa basketball championship in 1994", "answer": ["University of Kentucky"], "scores": [[0.011892461217939854, 0.9547893404960632, 0.2504971921443939]], "normalized_score": [0.14168612658977509], "tokens": [["\u0120University", "\u0120of", "\u0120Kentucky"]], "entropy": [[4.266862869262695, 0.24196675419807434, 3.3332772254943848]], "avg_entropy": [2.6140356163183847], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Arkansas", "score_of_biggest_cluster": 0.14168612658977509, "score_of_first_answer": 0.14168612658977509, "max_score_of_first_answer": 0.9547893404960632, "min_score_of_first_answer": 0.011892461217939854, "avg_score": 0.14168612658977509, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4002796484685175, "std_of_first_answer": 0.4002796484685175, "avg_entropy_of_first_answer": 2.6140356163183847, "lex_sim": 1.0}
{"question": "who played maudie in the movie el dorado", "answer": ["Anne Bancroft"], "scores": [[0.01221640594303608, 0.23337870836257935, 0.992291271686554, 0.9994518160820007, 0.9994280934333801]], "normalized_score": [0.30919429659843445], "tokens": [["\u0120Anne", "\u0120B", "anc", "ro", "ft"]], "entropy": [[6.185853958129883, 3.2269392013549805, 0.08473764359951019, 0.006627118680626154, 0.006572720594704151]], "avg_entropy": [1.9021461284719408], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charlene Holt", "score_of_biggest_cluster": 0.30919429659843445, "score_of_first_answer": 0.30919429659843445, "max_score_of_first_answer": 0.9994518160820007, "min_score_of_first_answer": 0.01221640594303608, "avg_score": 0.30919429659843445, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43397835573291577, "std_of_first_answer": 0.43397835573291577, "avg_entropy_of_first_answer": 1.9021461284719408, "lex_sim": 1.0}
{"question": "china in chinese is called zhongguo which means", "answer": ["The Middle Kingdom"], "scores": [[0.02465583197772503, 0.6388232707977295, 0.9856271743774414]], "normalized_score": [0.24946196377277374], "tokens": [["\u0120The", "\u0120Middle", "\u0120Kingdom"]], "entropy": [[4.542023658752441, 2.0693037509918213, 0.09869261085987091]], "avg_entropy": [2.236673340201378], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Central States", "score_of_biggest_cluster": 0.24946196377277374, "score_of_first_answer": 0.24946196377277374, "max_score_of_first_answer": 0.9856271743774414, "min_score_of_first_answer": 0.02465583197772503, "avg_score": 0.24946196377277374, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3973440310861837, "std_of_first_answer": 0.3973440310861837, "avg_entropy_of_first_answer": 2.236673340201378, "lex_sim": 1.0}
{"question": "first dynasty to issue gold coins in india", "answer": ["Maurya"], "scores": [[0.5870962738990784, 0.9999454021453857, 0.30374643206596375]], "normalized_score": [0.5628581047058105], "tokens": [["\u0120Ma", "ury", "a"]], "entropy": [[2.651632785797119, 0.0008411479648202658, 1.1634423732757568]], "avg_entropy": [1.2719721023458987], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Gupta Empire", "score_of_biggest_cluster": 0.5628581047058105, "score_of_first_answer": 0.5628581047058105, "max_score_of_first_answer": 0.9999454021453857, "min_score_of_first_answer": 0.30374643206596375, "avg_score": 0.5628581047058105, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28585632395395866, "std_of_first_answer": 0.28585632395395866, "avg_entropy_of_first_answer": 1.2719721023458987, "lex_sim": 1.0}
{"question": "where did the french immigrants settle in texas", "answer": ["Mexico"], "scores": [[0.026519987732172012]], "normalized_score": [0.026519987732172012], "tokens": [["\u0120Mexico"]], "entropy": [[5.0314555168151855]], "avg_entropy": [5.0314555168151855], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1685", "score_of_biggest_cluster": 0.026519987732172012, "score_of_first_answer": 0.026519987732172012, "max_score_of_first_answer": 0.026519987732172012, "min_score_of_first_answer": 0.026519987732172012, "avg_score": 0.026519987732172012, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.0314555168151855, "lex_sim": 1.0}
{"question": "different ways to spell corey for a boy", "answer": ["Corey"], "scores": [[0.3862461447715759]], "normalized_score": [0.38624611496925354], "tokens": [["\u0120Corey"]], "entropy": [[4.049832820892334]], "avg_entropy": [4.049832820892334], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Coire", "score_of_biggest_cluster": 0.38624611496925354, "score_of_first_answer": 0.38624611496925354, "max_score_of_first_answer": 0.3862461447715759, "min_score_of_first_answer": 0.3862461447715759, "avg_score": 0.38624611496925354, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.049832820892334, "lex_sim": 1.0}
{"question": "who was the original host of jokers wild", "answer": ["Jack Parr"], "scores": [[0.0755225196480751, 0.2964097261428833, 0.7533241510391235]], "normalized_score": [0.2564387023448944], "tokens": [["\u0120Jack", "\u0120Par", "r"]], "entropy": [[5.482105255126953, 4.249239921569824, 1.5924766063690186]], "avg_entropy": [3.774607261021932], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jack Barry", "score_of_biggest_cluster": 0.2564387023448944, "score_of_first_answer": 0.2564387023448944, "max_score_of_first_answer": 0.7533241510391235, "min_score_of_first_answer": 0.0755225196480751, "avg_score": 0.2564387023448944, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28224831486313384, "std_of_first_answer": 0.28224831486313384, "avg_entropy_of_first_answer": 3.774607261021932, "lex_sim": 1.0}
{"question": "who took the first steps on the moon in 1969", "answer": ["Neil Armstrong"], "scores": [[0.8035898208618164, 0.8558449745178223]], "normalized_score": [0.8293059468269348], "tokens": [["\u0120Neil", "\u0120Armstrong"]], "entropy": [[1.3735140562057495, 0.8077504634857178]], "avg_entropy": [1.0906322598457336], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Neil Armstrong", "score_of_biggest_cluster": 0.8293059468269348, "score_of_first_answer": 0.8293059468269348, "max_score_of_first_answer": 0.8558449745178223, "min_score_of_first_answer": 0.8035898208618164, "avg_score": 0.8293059468269348, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.02612757682800293, "std_of_first_answer": 0.02612757682800293, "avg_entropy_of_first_answer": 1.0906322598457336, "lex_sim": 1.0}
{"question": "who sings the song i'll never forget you", "answer": ["Paul Stanley"], "scores": [[0.007265186868607998, 0.02135833352804184]], "normalized_score": [0.012456819415092468], "tokens": [["\u0120Paul", "\u0120Stanley"]], "entropy": [[6.130783557891846, 3.6715564727783203]], "avg_entropy": [4.901170015335083], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mariah Carey", "score_of_biggest_cluster": 0.012456819415092468, "score_of_first_answer": 0.012456819415092468, "max_score_of_first_answer": 0.02135833352804184, "min_score_of_first_answer": 0.007265186868607998, "avg_score": 0.012456819415092468, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.007046573329716921, "std_of_first_answer": 0.007046573329716921, "avg_entropy_of_first_answer": 4.901170015335083, "lex_sim": 1.0}
{"question": "how did long john silver lose his leg in treasure island", "answer": ["He got shot."], "scores": [[0.09229636937379837, 0.09455527365207672, 0.3521091639995575, 0.04322201386094093]], "normalized_score": [0.10735272616147995], "tokens": [["\u0120He", "\u0120got", "\u0120shot", "."]], "entropy": [[5.871939182281494, 3.8436267375946045, 3.408003091812134, 1.8490824699401855]], "avg_entropy": [3.7431628704071045], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in the Royal Navy", "score_of_biggest_cluster": 0.10735272616147995, "score_of_first_answer": 0.10735272616147995, "max_score_of_first_answer": 0.3521091639995575, "min_score_of_first_answer": 0.04322201386094093, "avg_score": 0.10735272616147995, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12101045016429632, "std_of_first_answer": 0.12101045016429632, "avg_entropy_of_first_answer": 3.7431628704071045, "lex_sim": 1.0}
{"question": "which battle ended britain's support for the south", "answer": ["britain left the commonwealth in 1972A method for the direct detection of sub-diffraction-limit features."], "scores": [[0.0032199700362980366, 0.9918038845062256, 0.4576364755630493, 0.01712968200445175, 0.7334609031677246, 0.003706637304276228, 0.9811081290245056, 0.2844100594520569, 0.11528496444225311, 2.33039099839516e-06, 5.3508711062022485e-06, 0.1738264113664627, 0.05306065082550049, 0.0026000833604484797, 0.027707403525710106, 0.921507716178894, 0.007434861268848181, 0.2746369540691376, 0.014733869582414627, 0.4834153950214386, 0.24270157516002655, 0.3748131990432739, 0.04526807740330696, 0.045133404433727264]], "normalized_score": [0.03706454113125801], "tokens": [["\u0120b", "rit", "ain", "\u0120left", "\u0120the", "\u0120common", "wealth", "\u0120in", "\u01201972", "A", "\u0120method", "\u0120for", "\u0120the", "\u0120direct", "\u0120detection", "\u0120of", "\u0120sub", "-", "diff", "raction", "-", "limit", "\u0120features", "."]], "entropy": [[5.6116623878479, 0.08849870413541794, 1.1602389812469482, 4.6380414962768555, 1.9088671207427979, 3.290070056915283, 0.13134248554706573, 2.363434076309204, 4.007019996643066, 0.6674035787582397, 3.8933262825012207, 3.10001277923584, 6.664720058441162, 7.0961594581604, 5.407194137573242, 0.408517062664032, 5.157314300537109, 1.8380008935928345, 4.8505144119262695, 1.5624529123306274, 4.310425758361816, 1.1532320976257324, 4.842081069946289, 2.8904621601104736]], "avg_entropy": [3.210041344476243], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Battle of Antietam", "score_of_biggest_cluster": 0.03706454113125801, "score_of_first_answer": 0.03706454113125801, "max_score_of_first_answer": 0.9918038845062256, "min_score_of_first_answer": 2.33039099839516e-06, "avg_score": 0.03706454113125801, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3262145128673539, "std_of_first_answer": 0.3262145128673539, "avg_entropy_of_first_answer": 3.210041344476243, "lex_sim": 1.0}
{"question": "what is the largest bill in american money", "answer": ["one dollar"], "scores": [[0.040339015424251556, 0.2625802159309387]], "normalized_score": [0.10291855782270432], "tokens": [["\u0120one", "\u0120dollar"]], "entropy": [[3.294541358947754, 1.5580450296401978]], "avg_entropy": [2.426293194293976], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "$100", "score_of_biggest_cluster": 0.10291855782270432, "score_of_first_answer": 0.10291855782270432, "max_score_of_first_answer": 0.2625802159309387, "min_score_of_first_answer": 0.040339015424251556, "avg_score": 0.10291855782270432, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11112060025334358, "std_of_first_answer": 0.11112060025334358, "avg_entropy_of_first_answer": 2.426293194293976, "lex_sim": 1.0}
{"question": "who designed the traffic circle in long beach ca", "answer": ["Frank Gehry"], "scores": [[0.03713565319776535, 0.48827043175697327, 0.9999071359634399]], "normalized_score": [0.2627062201499939], "tokens": [["\u0120Frank", "\u0120Geh", "ry"]], "entropy": [[6.726241111755371, 3.186326503753662, 0.001228464301675558]], "avg_entropy": [3.3045986932702363], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "German engineer Werner Ruchti", "score_of_biggest_cluster": 0.2627062201499939, "score_of_first_answer": 0.2627062201499939, "max_score_of_first_answer": 0.9999071359634399, "min_score_of_first_answer": 0.03713565319776535, "avg_score": 0.2627062201499939, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39330842202804783, "std_of_first_answer": 0.39330842202804783, "avg_entropy_of_first_answer": 3.3045986932702363, "lex_sim": 1.0}
{"question": "who did cora marry in once upon a time", "answer": ["Emma Swan"], "scores": [[0.05966416001319885, 0.2517620027065277]], "normalized_score": [0.12256088852882385], "tokens": [["\u0120Emma", "\u0120Swan"]], "entropy": [[6.272021293640137, 2.8121211528778076]], "avg_entropy": [4.542071223258972], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Henry", "score_of_biggest_cluster": 0.12256088852882385, "score_of_first_answer": 0.12256088852882385, "max_score_of_first_answer": 0.2517620027065277, "min_score_of_first_answer": 0.05966416001319885, "avg_score": 0.12256088852882385, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09604892134666443, "std_of_first_answer": 0.09604892134666443, "avg_entropy_of_first_answer": 4.542071223258972, "lex_sim": 1.0}
{"question": "who played the face in the a team", "answer": ["Robert Redman"], "scores": [[0.005607344675809145, 0.03475256636738777, 0.007778455503284931]], "normalized_score": [0.011487152427434921], "tokens": [["\u0120Robert", "\u0120Red", "man"]], "entropy": [[6.4553680419921875, 4.257205963134766, 0.23856282234191895]], "avg_entropy": [3.650378942489624], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dirk Benedict", "score_of_biggest_cluster": 0.011487152427434921, "score_of_first_answer": 0.011487152427434921, "max_score_of_first_answer": 0.03475256636738777, "min_score_of_first_answer": 0.005607344675809145, "avg_score": 0.011487152427434921, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.013257116858441475, "std_of_first_answer": 0.013257116858441475, "avg_entropy_of_first_answer": 3.650378942489624, "lex_sim": 1.0}
{"question": "who played susanna in legends of the fall", "answer": ["Susan Sarandon"], "scores": [[0.025041591376066208, 0.7966486811637878, 0.973879337310791]], "normalized_score": [0.26883015036582947], "tokens": [["\u0120Susan", "\u0120Sar", "andon"]], "entropy": [[4.3729248046875, 1.606837511062622, 0.19719544053077698]], "avg_entropy": [2.0589859187602997], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Julia Ormond", "score_of_biggest_cluster": 0.26883015036582947, "score_of_first_answer": 0.26883015036582947, "max_score_of_first_answer": 0.973879337310791, "min_score_of_first_answer": 0.025041591376066208, "avg_score": 0.26883015036582947, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4119170998850596, "std_of_first_answer": 0.4119170998850596, "avg_entropy_of_first_answer": 2.0589859187602997, "lex_sim": 1.0}
{"question": "at what age are you no longer a toddler", "answer": ["age 24"], "scores": [[0.0169328972697258, 0.011281386017799377]], "normalized_score": [0.013821236789226532], "tokens": [["\u0120age", "\u012024"]], "entropy": [[4.27921199798584, 3.5088610649108887]], "avg_entropy": [3.8940365314483643], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "36 months", "score_of_biggest_cluster": 0.013821236789226532, "score_of_first_answer": 0.013821236789226532, "max_score_of_first_answer": 0.0169328972697258, "min_score_of_first_answer": 0.011281386017799377, "avg_score": 0.013821236789226532, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.002825755625963211, "std_of_first_answer": 0.002825755625963211, "avg_entropy_of_first_answer": 3.8940365314483643, "lex_sim": 1.0}
{"question": "who challenged the aristotelian model of a geocentric universe", "answer": ["Nicolaus Copernicus"], "scores": [[0.2518211603164673, 0.990257978439331, 0.9754127264022827, 0.998838484287262, 0.9944028258323669]], "normalized_score": [0.7526920437812805], "tokens": [["\u0120Nicola", "us", "\u0120Cop", "ern", "icus"]], "entropy": [[2.9780030250549316, 0.09859293699264526, 0.25310084223747253, 0.013199595734477043, 0.05403517931699753]], "avg_entropy": [0.6793863158673048], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Copernicus", "score_of_biggest_cluster": 0.7526920437812805, "score_of_first_answer": 0.7526920437812805, "max_score_of_first_answer": 0.998838484287262, "min_score_of_first_answer": 0.2518211603164673, "avg_score": 0.7526920437812805, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.29526776678575245, "std_of_first_answer": 0.29526776678575245, "avg_entropy_of_first_answer": 0.6793863158673048, "lex_sim": 1.0}
{"question": "where does the red badge of courage take place", "answer": ["kurram valley"], "scores": [[0.0036465488374233246, 0.0030706024263054132, 0.026004992425441742, 0.8440369963645935]], "normalized_score": [0.022265426814556122], "tokens": [["\u0120k", "ur", "ram", "\u0120valley"]], "entropy": [[6.064565658569336, 1.712634801864624, 4.754161834716797, 0.9816253781318665]], "avg_entropy": [3.378246918320656], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the field of battle", "score_of_biggest_cluster": 0.022265426814556122, "score_of_first_answer": 0.022265426814556122, "max_score_of_first_answer": 0.8440369963645935, "min_score_of_first_answer": 0.0030706024263054132, "avg_score": 0.022265426814556122, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36087421262953323, "std_of_first_answer": 0.36087421262953323, "avg_entropy_of_first_answer": 3.378246918320656, "lex_sim": 1.0}
{"question": "where are more than half your bones located", "answer": ["spine"], "scores": [[0.005891540553420782]], "normalized_score": [0.005891540553420782], "tokens": [["\u0120spine"]], "entropy": [[5.507972717285156]], "avg_entropy": [5.507972717285156], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the appendicular skeleton", "score_of_biggest_cluster": 0.005891540553420782, "score_of_first_answer": 0.005891540553420782, "max_score_of_first_answer": 0.005891540553420782, "min_score_of_first_answer": 0.005891540553420782, "avg_score": 0.005891540553420782, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.507972717285156, "lex_sim": 1.0}
{"question": "when did they stop using aluminum wiring in ontario", "answer": ["Canada"], "scores": [[0.014310619793832302]], "normalized_score": [0.014310621656477451], "tokens": [["\u0120Canada"]], "entropy": [[5.541398048400879]], "avg_entropy": [5.541398048400879], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "mid-1970s", "score_of_biggest_cluster": 0.014310621656477451, "score_of_first_answer": 0.014310621656477451, "max_score_of_first_answer": 0.014310619793832302, "min_score_of_first_answer": 0.014310619793832302, "avg_score": 0.014310621656477451, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.541398048400879, "lex_sim": 1.0}
{"question": "who was the first to say i'm going to disney world", "answer": ["Michael J Fox"], "scores": [[0.013681678101420403, 0.02250036969780922, 0.1809425801038742]], "normalized_score": [0.03819061070680618], "tokens": [["\u0120Michael", "\u0120J", "\u0120Fox"]], "entropy": [[5.931221961975098, 4.4728102684021, 2.4387729167938232]], "avg_entropy": [4.280935049057007], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jeana Yeager", "score_of_biggest_cluster": 0.03819061070680618, "score_of_first_answer": 0.03819061070680618, "max_score_of_first_answer": 0.1809425801038742, "min_score_of_first_answer": 0.013681678101420403, "avg_score": 0.03819061070680618, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07685333268166379, "std_of_first_answer": 0.07685333268166379, "avg_entropy_of_first_answer": 4.280935049057007, "lex_sim": 1.0}
{"question": "which is the world's largest company in terms of revenue", "answer": ["Microsoft Corporation"], "scores": [[0.01908070780336857, 0.03823602572083473]], "normalized_score": [0.027010558173060417], "tokens": [["\u0120Microsoft", "\u0120Corporation"]], "entropy": [[3.8965065479278564, 0.4347012937068939]], "avg_entropy": [2.165603920817375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Walmart", "score_of_biggest_cluster": 0.027010558173060417, "score_of_first_answer": 0.027010558173060417, "max_score_of_first_answer": 0.03823602572083473, "min_score_of_first_answer": 0.01908070780336857, "avg_score": 0.027010558173060417, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.009577658958733082, "std_of_first_answer": 0.009577658958733082, "avg_entropy_of_first_answer": 2.165603920817375, "lex_sim": 1.0}
{"question": "the roman name for a liberal arts education is known as", "answer": ["magna cum laude"], "scores": [[0.020378025248646736, 0.6550494432449341, 0.8480207920074463, 0.9619998335838318, 0.9997676014900208]], "normalized_score": [0.4049331545829773], "tokens": [["\u0120mag", "na", "\u0120cum", "\u0120la", "ude"]], "entropy": [[6.106478691101074, 1.4396519660949707, 1.1575208902359009, 0.32849299907684326, 0.003247522283345461]], "avg_entropy": [1.8070784137584268], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "liberalia studia", "score_of_biggest_cluster": 0.4049331545829773, "score_of_first_answer": 0.4049331545829773, "max_score_of_first_answer": 0.9997676014900208, "min_score_of_first_answer": 0.020378025248646736, "avg_score": 0.4049331545829773, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35896449644877126, "std_of_first_answer": 0.35896449644877126, "avg_entropy_of_first_answer": 1.8070784137584268, "lex_sim": 1.0}
{"question": "who has the most ncaa basketball tournament appearances", "answer": ["Michael Jordan"], "scores": [[0.09879650175571442, 0.9034751057624817]], "normalized_score": [0.2987644374370575], "tokens": [["\u0120Michael", "\u0120Jordan"]], "entropy": [[5.108281135559082, 0.8490315079689026]], "avg_entropy": [2.9786563217639923], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kentucky", "score_of_biggest_cluster": 0.2987644374370575, "score_of_first_answer": 0.2987644374370575, "max_score_of_first_answer": 0.9034751057624817, "min_score_of_first_answer": 0.09879650175571442, "avg_score": 0.2987644374370575, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40233930200338364, "std_of_first_answer": 0.40233930200338364, "avg_entropy_of_first_answer": 2.9786563217639923, "lex_sim": 1.0}
{"question": "who played john coffey in the movie the green mile", "answer": ["John Hurt"], "scores": [[0.383584201335907, 0.0040162415243685246]], "normalized_score": [0.039250053465366364], "tokens": [["\u0120John", "\u0120Hurt"]], "entropy": [[3.923476457595825, 1.4195259809494019]], "avg_entropy": [2.6715012192726135], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Michael Clarke Duncan", "score_of_biggest_cluster": 0.039250053465366364, "score_of_first_answer": 0.039250053465366364, "max_score_of_first_answer": 0.383584201335907, "min_score_of_first_answer": 0.0040162415243685246, "avg_score": 0.039250053465366364, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18978397990576923, "std_of_first_answer": 0.18978397990576923, "avg_entropy_of_first_answer": 2.6715012192726135, "lex_sim": 1.0}
{"question": "who is the voice of the other mother in coraline", "answer": ["Jennifer Hudson"], "scores": [[0.03607267141342163, 0.008560152724385262]], "normalized_score": [0.017572350800037384], "tokens": [["\u0120Jennifer", "\u0120Hudson"]], "entropy": [[6.522665023803711, 3.211336374282837]], "avg_entropy": [4.867000699043274], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Teri Hatcher", "score_of_biggest_cluster": 0.017572350800037384, "score_of_first_answer": 0.017572350800037384, "max_score_of_first_answer": 0.03607267141342163, "min_score_of_first_answer": 0.008560152724385262, "avg_score": 0.017572350800037384, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.013756259344518185, "std_of_first_answer": 0.013756259344518185, "avg_entropy_of_first_answer": 4.867000699043274, "lex_sim": 1.0}
{"question": "who has made the most premier league appearances", "answer": ["Alan Shearer"], "scores": [[0.034048523753881454, 0.7705517411231995, 0.9988017082214355]], "normalized_score": [0.29702508449554443], "tokens": [["\u0120Alan", "\u0120She", "arer"]], "entropy": [[5.255011558532715, 1.724912405014038, 0.013309070840477943]], "avg_entropy": [2.331077678129077], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gareth Barry", "score_of_biggest_cluster": 0.29702508449554443, "score_of_first_answer": 0.29702508449554443, "max_score_of_first_answer": 0.9988017082214355, "min_score_of_first_answer": 0.034048523753881454, "avg_score": 0.29702508449554443, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41167459420382946, "std_of_first_answer": 0.41167459420382946, "avg_entropy_of_first_answer": 2.331077678129077, "lex_sim": 1.0}
{"question": "how long have we been celebrating pi day", "answer": ["3 years"], "scores": [[0.13060173392295837, 0.3410215377807617]], "normalized_score": [0.21104027330875397], "tokens": [["\u01203", "\u0120years"]], "entropy": [[4.720869541168213, 2.4363903999328613]], "avg_entropy": [3.578629970550537], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1988", "score_of_biggest_cluster": 0.21104027330875397, "score_of_first_answer": 0.21104027330875397, "max_score_of_first_answer": 0.3410215377807617, "min_score_of_first_answer": 0.13060173392295837, "avg_score": 0.21104027330875397, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10520990192890167, "std_of_first_answer": 0.10520990192890167, "avg_entropy_of_first_answer": 3.578629970550537, "lex_sim": 1.0}
{"question": "which financial statement involves all aspects of the accounting\u200b equation", "answer": ["financial statement\u200bs"], "scores": [[0.020289352163672447, 0.4786727726459503, 0.06413533538579941, 0.21380503475666046]], "normalized_score": [0.10742505639791489], "tokens": [["\u0120financial", "\u0120statement", "\u00e2\u0122\u012d", "s"]], "entropy": [[4.4397783279418945, 2.0603721141815186, 1.145691156387329, 1.7785212993621826]], "avg_entropy": [2.356090724468231], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "balance sheet", "score_of_biggest_cluster": 0.10742505639791489, "score_of_first_answer": 0.10742505639791489, "max_score_of_first_answer": 0.4786727726459503, "min_score_of_first_answer": 0.020289352163672447, "avg_score": 0.10742505639791489, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17921414812343656, "std_of_first_answer": 0.17921414812343656, "avg_entropy_of_first_answer": 2.356090724468231, "lex_sim": 1.0}
{"question": "how many players in football hall of fame", "answer": ["49"], "scores": [[0.0079063531011343]], "normalized_score": [0.007906351238489151], "tokens": [["\u012049"]], "entropy": [[5.69312858581543]], "avg_entropy": [5.69312858581543], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "318", "score_of_biggest_cluster": 0.007906351238489151, "score_of_first_answer": 0.007906351238489151, "max_score_of_first_answer": 0.0079063531011343, "min_score_of_first_answer": 0.0079063531011343, "avg_score": 0.007906351238489151, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.69312858581543, "lex_sim": 1.0}
{"question": "who starred in the tv show even stevens", "answer": ["Steve Carrell"], "scores": [[0.06734665483236313, 0.07232498377561569, 0.8711864948272705]], "normalized_score": [0.16189688444137573], "tokens": [["\u0120Steve", "\u0120Car", "rell"]], "entropy": [[5.798247814178467, 4.169084072113037, 0.7675786018371582]], "avg_entropy": [3.578303496042887], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nick Spano", "score_of_biggest_cluster": 0.16189688444137573, "score_of_first_answer": 0.16189688444137573, "max_score_of_first_answer": 0.8711864948272705, "min_score_of_first_answer": 0.06734665483236313, "avg_score": 0.16189688444137573, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3777657983992255, "std_of_first_answer": 0.3777657983992255, "avg_entropy_of_first_answer": 3.578303496042887, "lex_sim": 1.0}
{"question": "who owns the majority of the us debt", "answer": ["u.s. government"], "scores": [[0.0022564157843589783, 0.6248639225959778, 0.9534448385238647, 0.7172119617462158, 0.3069593906402588]], "normalized_score": [0.19689998030662537], "tokens": [["\u0120u", ".", "s", ".", "\u0120government"]], "entropy": [[4.284448623657227, 2.1337215900421143, 0.2658211588859558, 1.3190537691116333, 2.6911661624908447]], "avg_entropy": [2.138842260837555], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "China", "score_of_biggest_cluster": 0.19689998030662537, "score_of_first_answer": 0.19689998030662537, "max_score_of_first_answer": 0.9534448385238647, "min_score_of_first_answer": 0.0022564157843589783, "avg_score": 0.19689998030662537, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3320251982762361, "std_of_first_answer": 0.3320251982762361, "avg_entropy_of_first_answer": 2.138842260837555, "lex_sim": 1.0}
{"question": "location of the ten commandments in the bible", "answer": ["Exodus 20:1-17"], "scores": [[0.1603889912366867, 0.7514309883117676, 0.7349803447723389, 0.550674557685852, 0.6741736531257629, 0.7185149192810059]], "normalized_score": [0.5356819033622742], "tokens": [["\u0120Exodus", "\u012020", ":", "1", "-", "17"]], "entropy": [[5.123926162719727, 1.280290126800537, 1.120659351348877, 1.6663662195205688, 1.3844883441925049, 1.475632905960083]], "avg_entropy": [2.008560518423716], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Exodus", "score_of_biggest_cluster": 0.5356819033622742, "score_of_first_answer": 0.5356819033622742, "max_score_of_first_answer": 0.7514309883117676, "min_score_of_first_answer": 0.1603889912366867, "avg_score": 0.5356819033622742, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20670964604972433, "std_of_first_answer": 0.20670964604972433, "avg_entropy_of_first_answer": 2.008560518423716, "lex_sim": 1.0}
{"question": "where does jack ryan live in patriot games", "answer": ["Arlington, VA"], "scores": [[0.006322554312646389, 0.5617192983627319, 0.21705755591392517]], "normalized_score": [0.09169147908687592], "tokens": [["\u0120Arlington", ",", "\u0120VA"]], "entropy": [[7.345071792602539, 1.7504031658172607, 1.352513313293457]], "avg_entropy": [3.4826627572377524], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Baltimore, Maryland", "score_of_biggest_cluster": 0.09169147908687592, "score_of_first_answer": 0.09169147908687592, "max_score_of_first_answer": 0.5617192983627319, "min_score_of_first_answer": 0.006322554312646389, "avg_score": 0.09169147908687592, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2289266017524139, "std_of_first_answer": 0.2289266017524139, "avg_entropy_of_first_answer": 3.4826627572377524, "lex_sim": 1.0}
{"question": "what is alpha centauri's approximate distance from earth", "answer": ["25,500,000 miles"], "scores": [[0.022527212277054787, 0.39135482907295227, 0.013931269757449627, 0.8224681615829468, 0.9715983271598816, 0.36007124185562134]], "normalized_score": [0.18115234375], "tokens": [["\u012025", ",", "500", ",", "000", "\u0120miles"]], "entropy": [[5.389433860778809, 2.7264535427093506, 3.872680187225342, 0.954984724521637, 0.3028128743171692, 2.0389084815979004]], "avg_entropy": [2.547545611858368], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "4.37 light-years", "score_of_biggest_cluster": 0.18115234375, "score_of_first_answer": 0.18115234375, "max_score_of_first_answer": 0.9715983271598816, "min_score_of_first_answer": 0.013931269757449627, "avg_score": 0.18115234375, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3635221480834748, "std_of_first_answer": 0.3635221480834748, "avg_entropy_of_first_answer": 2.547545611858368, "lex_sim": 1.0}
{"question": "who is edmund on days of our lives", "answer": ["Edmund Kean"], "scores": [[0.3027026951313019, 0.006519339047372341, 0.3317144215106964]], "normalized_score": [0.0868283286690712], "tokens": [["\u0120Edmund", "\u0120Ke", "an"]], "entropy": [[5.115631103515625, 5.8611297607421875, 2.5615227222442627]], "avg_entropy": [4.512761195500691], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Adam Caine", "score_of_biggest_cluster": 0.0868283286690712, "score_of_first_answer": 0.0868283286690712, "max_score_of_first_answer": 0.3317144215106964, "min_score_of_first_answer": 0.006519339047372341, "avg_score": 0.0868283286690712, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14693842336426385, "std_of_first_answer": 0.14693842336426385, "avg_entropy_of_first_answer": 4.512761195500691, "lex_sim": 1.0}
{"question": "where did this case of product tampering occur", "answer": ["the aipac convention in dubai"], "scores": [[0.026069695129990578, 0.005455999635159969, 0.041000112891197205, 0.27085965871810913, 0.20551767945289612, 0.19463540613651276, 0.009122395887970924, 0.8610609173774719]], "normalized_score": [0.06870197504758835], "tokens": [["\u0120the", "\u0120a", "ip", "ac", "\u0120convention", "\u0120in", "\u0120dub", "ai"]], "entropy": [[7.040461540222168, 7.521566867828369, 5.404701232910156, 3.963425636291504, 4.162032127380371, 1.839292287826538, 5.010431289672852, 0.6125215291976929]], "avg_entropy": [4.444304063916206], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Chicago metropolitan area", "score_of_biggest_cluster": 0.06870197504758835, "score_of_first_answer": 0.06870197504758835, "max_score_of_first_answer": 0.8610609173774719, "min_score_of_first_answer": 0.005455999635159969, "avg_score": 0.06870197504758835, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2673650729937381, "std_of_first_answer": 0.2673650729937381, "avg_entropy_of_first_answer": 4.444304063916206, "lex_sim": 1.0}
{"question": "where does the thames river begin and end", "answer": ["It is considered to begin at the outfall of the source of the River Great Stour."], "scores": [[0.020932864397764206, 0.07785182446241379, 0.010587982833385468, 0.6476057767868042, 0.38595396280288696, 0.7067749500274658, 0.4246494770050049, 0.024868929758667946, 0.752604603767395, 0.7268613576889038, 0.7942491173744202, 0.03338133171200752, 0.39707237482070923, 0.9556358456611633, 0.0737009197473526, 0.013858352787792683, 0.08024651557207108, 0.43338698148727417, 0.09161680191755295]], "normalized_score": [0.15773873031139374], "tokens": [["\u0120It", "\u0120is", "\u0120considered", "\u0120to", "\u0120begin", "\u0120at", "\u0120the", "\u0120out", "fall", "\u0120of", "\u0120the", "\u0120source", "\u0120of", "\u0120the", "\u0120River", "\u0120Great", "\u0120St", "our", "."]], "entropy": [[5.279726982116699, 2.2933125495910645, 4.149389266967773, 1.7157317399978638, 1.9932692050933838, 1.3502566814422607, 3.874131679534912, 3.902979850769043, 0.8310599327087402, 1.6596522331237793, 1.5634808540344238, 4.221803665161133, 2.8703837394714355, 0.4347705841064453, 2.222677230834961, 4.052853107452393, 0.7700488567352295, 2.1197426319122314, 2.495455741882324]], "avg_entropy": [2.5158277122597945], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lighthouse Cove", "score_of_biggest_cluster": 0.15773873031139374, "score_of_first_answer": 0.15773873031139374, "max_score_of_first_answer": 0.9556358456611633, "min_score_of_first_answer": 0.010587982833385468, "avg_score": 0.15773873031139374, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31892318400137454, "std_of_first_answer": 0.31892318400137454, "avg_entropy_of_first_answer": 2.5158277122597945, "lex_sim": 1.0}
{"question": "how many episodes of sabrina the teenage witch are there", "answer": ["130"], "scores": [[0.010234353132545948]], "normalized_score": [0.010234350338578224], "tokens": [["\u0120130"]], "entropy": [[5.552013397216797]], "avg_entropy": [5.552013397216797], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "163", "score_of_biggest_cluster": 0.010234350338578224, "score_of_first_answer": 0.010234350338578224, "max_score_of_first_answer": 0.010234353132545948, "min_score_of_first_answer": 0.010234353132545948, "avg_score": 0.010234350338578224, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.552013397216797, "lex_sim": 1.0}
{"question": "who plays lady talisa in game of thrones", "answer": ["Emilia Clarke"], "scores": [[0.14689627289772034, 0.9947980642318726, 0.9727017283439636]], "normalized_score": [0.5218853950500488], "tokens": [["\u0120Em", "ilia", "\u0120Clarke"]], "entropy": [[3.165208339691162, 0.04138355702161789, 0.2252555787563324]], "avg_entropy": [1.1439491584897041], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Oona Castilla Chaplin", "score_of_biggest_cluster": 0.5218853950500488, "score_of_first_answer": 0.5218853950500488, "max_score_of_first_answer": 0.9947980642318726, "min_score_of_first_answer": 0.14689627289772034, "avg_score": 0.5218853950500488, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39459970519391374, "std_of_first_answer": 0.39459970519391374, "avg_entropy_of_first_answer": 1.1439491584897041, "lex_sim": 1.0}
{"question": "where does the grapes of wrath take place", "answer": ["a fictional location"], "scores": [[0.00595947727560997, 0.0258098803460598, 0.22998839616775513]], "normalized_score": [0.03282715752720833], "tokens": [["\u0120a", "\u0120fictional", "\u0120location"]], "entropy": [[6.613245964050293, 6.669322490692139, 4.078080654144287]], "avg_entropy": [5.786883036295573], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Oklahoma", "score_of_biggest_cluster": 0.03282715752720833, "score_of_first_answer": 0.03282715752720833, "max_score_of_first_answer": 0.22998839616775513, "min_score_of_first_answer": 0.00595947727560997, "avg_score": 0.03282715752720833, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10125427910035484, "std_of_first_answer": 0.10125427910035484, "avg_entropy_of_first_answer": 5.786883036295573, "lex_sim": 1.0}
{"question": "who is the secretary of state for northern ireland", "answer": ["Theresa Villiers"], "scores": [[0.08489198982715607, 0.22348645329475403, 0.9796704053878784]], "normalized_score": [0.26489025354385376], "tokens": [["\u0120Theresa", "\u0120Vill", "iers"]], "entropy": [[5.2282562255859375, 2.930920124053955, 0.13399524986743927]], "avg_entropy": [2.7643905331691108], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Karen Bradley", "score_of_biggest_cluster": 0.26489025354385376, "score_of_first_answer": 0.26489025354385376, "max_score_of_first_answer": 0.9796704053878784, "min_score_of_first_answer": 0.08489198982715607, "avg_score": 0.26489025354385376, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3932275292245508, "std_of_first_answer": 0.3932275292245508, "avg_entropy_of_first_answer": 2.7643905331691108, "lex_sim": 1.0}
{"question": "when did virgin australia (formerly virgin blue) commence flying in australia", "answer": ["1994"], "scores": [[0.006242586765438318]], "normalized_score": [0.006242586299777031], "tokens": [["\u01201994"]], "entropy": [[5.2159576416015625]], "avg_entropy": [5.2159576416015625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "November 1999", "score_of_biggest_cluster": 0.006242586299777031, "score_of_first_answer": 0.006242586299777031, "max_score_of_first_answer": 0.006242586765438318, "min_score_of_first_answer": 0.006242586765438318, "avg_score": 0.006242586299777031, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.2159576416015625, "lex_sim": 1.0}
{"question": "who is the current defence against the dark arts teacher", "answer": ["The current defence against the dark arts teacher is a magical artefact."], "scores": [[0.008265255019068718, 0.011994900181889534, 0.4809301495552063, 0.8851739168167114, 0.9603736996650696, 0.8855838775634766, 0.9753534197807312, 0.9232214689254761, 0.9177354574203491, 0.04051138460636139, 0.005217989906668663, 0.009827068075537682, 0.741899847984314, 0.15782587230205536, 0.09163902699947357, 0.40681806206703186, 0.20175279676914215, 0.005130876321345568, 0.36644500494003296, 0.12007701396942139, 0.05443265661597252, 0.22448010742664337, 0.15720605850219727, 0.11004200577735901, 0.005663789343088865, 0.8070937395095825, 0.12418758124113083, 0.004584712907671928, 0.0026967558078467846, 0.0413128100335598, 0.0012135100550949574, 0.6983193755149841, 0.1792723685503006, 0.08692341297864914, 0.2725670337677002, 0.020884374156594276, 0.10312745720148087, 0.22384218871593475, 0.9875126481056213, 0.8479958176612854, 0.10849226266145706, 0.3267439305782318, 0.4341501295566559, 0.0036429527681320906, 0.2391858696937561, 0.0040609524585306644, 0.006325972266495228, 0.13047432899475098, 0.2749420702457428, 0.29418960213661194]], "normalized_score": [0.09816800057888031], "tokens": [["\u0120The", "\u0120current", "\u0120defence", "\u0120against", "\u0120the", "\u0120dark", "\u0120arts", "\u0120teacher", "\u0120is", "\u0120a", "\u0120magical", "\u0120arte", "fact", ".", "\u0120It", "\u0120is", "\u0120a", "\u0120sphere", "\u0120of", "\u0120pure", "\u0120white", "\u0120energy", ",", "\u0120which", "\u0120enc", "ases", "\u0120a", "\u0120protective", "\u0120blue", "\u0120force", "\u0120shell", ".", "\u0120It", "\u0120has", "\u0120the", "\u0120shape", "\u0120and", "\u0120appearance", "\u0120of", "\u0120a", "\u0120sphere", ",", "\u0120but", "\u0120possesses", "\u0120no", "\u0120mechanical", "\u0120devices", ",", "\u0120and", "\u0120is"]], "entropy": [[5.736539840698242, 6.081851005554199, 1.6619658470153809, 0.6987196207046509, 0.3452867567539215, 0.5396468639373779, 0.174712136387825, 0.5624942779541016, 0.6245386600494385, 6.534448146820068, 7.261695861816406, 6.319253921508789, 2.0360093116760254, 2.886078119277954, 2.0712227821350098, 2.986798048019409, 4.652479648590088, 6.820183753967285, 2.772367000579834, 5.745278358459473, 4.344951152801514, 2.615870237350464, 2.7074337005615234, 5.124690055847168, 4.406410217285156, 0.6387372612953186, 2.5372586250305176, 6.059781074523926, 4.8021135330200195, 4.040887355804443, 1.2739280462265015, 1.4896284341812134, 2.809696674346924, 2.8787527084350586, 2.9663445949554443, 3.105226516723633, 0.44812873005867004, 3.697319507598877, 0.11574408411979675, 0.6575347781181335, 5.501095294952393, 2.439655303955078, 2.661160945892334, 2.9108476638793945, 3.5754153728485107, 3.657691478729248, 3.263026714324951, 2.226264238357544, 3.843064546585083, 3.4854769706726074]], "avg_entropy": [3.1359141156077386], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Amycus Carrow", "score_of_biggest_cluster": 0.09816800057888031, "score_of_first_answer": 0.09816800057888031, "max_score_of_first_answer": 0.9875126481056213, "min_score_of_first_answer": 0.0012135100550949574, "avg_score": 0.09816800057888031, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3311925479180464, "std_of_first_answer": 0.3311925479180464, "avg_entropy_of_first_answer": 3.1359141156077386, "lex_sim": 1.0}
{"question": "which supreme court judge has surved in international court of justice", "answer": ["Stephen Breyer"], "scores": [[0.02238091081380844, 0.8674859404563904, 0.9970869421958923]], "normalized_score": [0.26850834488868713], "tokens": [["\u0120Stephen", "\u0120Bre", "yer"]], "entropy": [[6.301783561706543, 1.017293930053711, 0.028630871325731277]], "avg_entropy": [2.4492361210286617], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dalveer Bhandari", "score_of_biggest_cluster": 0.26850834488868713, "score_of_first_answer": 0.26850834488868713, "max_score_of_first_answer": 0.9970869421958923, "min_score_of_first_answer": 0.02238091081380844, "avg_score": 0.26850834488868713, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43218447429459816, "std_of_first_answer": 0.43218447429459816, "avg_entropy_of_first_answer": 2.4492361210286617, "lex_sim": 1.0}
{"question": "what is the latest version of microsoft office 2010", "answer": ["Office 2010"], "scores": [[0.3526281714439392, 0.30267971754074097]], "normalized_score": [0.3267008066177368], "tokens": [["\u0120Office", "\u01202010"]], "entropy": [[3.8563356399536133, 2.739889144897461]], "avg_entropy": [3.298112392425537], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "14.0.7182.5000", "score_of_biggest_cluster": 0.3267008066177368, "score_of_first_answer": 0.3267008066177368, "max_score_of_first_answer": 0.3526281714439392, "min_score_of_first_answer": 0.30267971754074097, "avg_score": 0.3267008066177368, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02497422695159912, "std_of_first_answer": 0.02497422695159912, "avg_entropy_of_first_answer": 3.298112392425537, "lex_sim": 1.0}
{"question": "in florida it is illegal to sell alcohol before 1 pm on any sunday. this is an example of", "answer": ["Sunday"], "scores": [[0.03549227491021156]], "normalized_score": [0.03549227491021156], "tokens": [["\u0120Sunday"]], "entropy": [[6.039229393005371]], "avg_entropy": [6.039229393005371], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Blue laws", "score_of_biggest_cluster": 0.03549227491021156, "score_of_first_answer": 0.03549227491021156, "max_score_of_first_answer": 0.03549227491021156, "min_score_of_first_answer": 0.03549227491021156, "avg_score": 0.03549227491021156, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.039229393005371, "lex_sim": 1.0}
{"question": "batman and robin episode only fools and horses", "answer": ["Batman: The Animated Series, Robin: The Animated Series."], "scores": [[0.0957648754119873, 0.04602603614330292, 0.49983757734298706, 0.4839884638786316, 0.9891665577888489, 0.07277948409318924, 0.09200049191713333, 0.9132885932922363, 0.858081579208374, 0.6191896200180054, 0.987828254699707, 0.01610652543604374]], "normalized_score": [0.24815069139003754], "tokens": [["\u0120Batman", ":", "\u0120The", "\u0120Animated", "\u0120Series", ",", "\u0120Robin", ":", "\u0120The", "\u0120Animated", "\u0120Series", "."]], "entropy": [[6.0590105056762695, 2.9201455116271973, 3.616506338119507, 2.6588711738586426, 0.08604466915130615, 1.88776695728302, 3.4465532302856445, 0.5851959586143494, 0.9210087060928345, 1.953559160232544, 0.10066819190979004, 0.8407084941864014]], "avg_entropy": [2.0896699080864587], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\"Heroes and Villains\"", "score_of_biggest_cluster": 0.24815069139003754, "score_of_first_answer": 0.24815069139003754, "max_score_of_first_answer": 0.9891665577888489, "min_score_of_first_answer": 0.01610652543604374, "avg_score": 0.24815069139003754, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3801898480284953, "std_of_first_answer": 0.3801898480284953, "avg_entropy_of_first_answer": 2.0896699080864587, "lex_sim": 1.0}
{"question": "when was the first underwater cable laid beneath the english channel", "answer": ["1973"], "scores": [[0.0016398204024881124]], "normalized_score": [0.001639820635318756], "tokens": [["\u01201973"]], "entropy": [[4.167482376098633]], "avg_entropy": [4.167482376098633], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1850", "score_of_biggest_cluster": 0.001639820635318756, "score_of_first_answer": 0.001639820635318756, "max_score_of_first_answer": 0.0016398204024881124, "min_score_of_first_answer": 0.0016398204024881124, "avg_score": 0.001639820635318756, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.167482376098633, "lex_sim": 1.0}
{"question": "who drafted the document of the un declaration of human rights", "answer": ["Benazir Bhutto"], "scores": [[0.0031949998810887337, 0.0029109022580087185, 0.9754737615585327, 0.9794842600822449, 0.9969556331634521, 0.9989383816719055]], "normalized_score": [0.14382056891918182], "tokens": [["\u0120Ben", "az", "ir", "\u0120Bh", "ut", "to"]], "entropy": [[4.088954448699951, 1.5754756927490234, 0.22735077142715454, 0.1956675946712494, 0.02783418446779251, 0.012361756525933743]], "avg_entropy": [1.0212740747568507], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Peters Humphrey, Canada", "score_of_biggest_cluster": 0.14382056891918182, "score_of_first_answer": 0.14382056891918182, "max_score_of_first_answer": 0.9989383816719055, "min_score_of_first_answer": 0.0029109022580087185, "avg_score": 0.14382056891918182, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4642502127372489, "std_of_first_answer": 0.4642502127372489, "avg_entropy_of_first_answer": 1.0212740747568507, "lex_sim": 1.0}
{"question": "who wrote the book my country my life", "answer": ["Mahatma Gandhi"], "scores": [[0.030388128012418747, 0.9816535115242004, 0.9644277691841125, 0.8714295029640198]], "normalized_score": [0.39791566133499146], "tokens": [["\u0120Mah", "at", "ma", "\u0120Gandhi"]], "entropy": [[4.9183478355407715, 0.15700340270996094, 0.24677523970603943, 0.545066237449646]], "avg_entropy": [1.4667981788516045], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "L. K. Advani", "score_of_biggest_cluster": 0.39791566133499146, "score_of_first_answer": 0.39791566133499146, "max_score_of_first_answer": 0.9816535115242004, "min_score_of_first_answer": 0.030388128012418747, "avg_score": 0.39791566133499146, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3957414884832196, "std_of_first_answer": 0.3957414884832196, "avg_entropy_of_first_answer": 1.4667981788516045, "lex_sim": 1.0}
{"question": "where was the louisiana purchase signed in 1803", "answer": ["Washington D.C."], "scores": [[0.04110433906316757, 0.26687851548194885, 0.9004177451133728, 0.9772630333900452, 0.8311069011688232]], "normalized_score": [0.3809454143047333], "tokens": [["\u0120Washington", "\u0120D", ".", "C", "."]], "entropy": [[5.576480388641357, 1.7310740947723389, 0.5518140196800232, 0.12639601528644562, 0.5173598527908325]], "avg_entropy": [1.7006248742341996], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paris", "score_of_biggest_cluster": 0.3809454143047333, "score_of_first_answer": 0.3809454143047333, "max_score_of_first_answer": 0.9772630333900452, "min_score_of_first_answer": 0.04110433906316757, "avg_score": 0.3809454143047333, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37663421268054226, "std_of_first_answer": 0.37663421268054226, "avg_entropy_of_first_answer": 1.7006248742341996, "lex_sim": 1.0}
{"question": "who lives at the end of king lear", "answer": ["king lear's end"], "scores": [[0.018573183566331863, 0.2749742865562439, 0.02496230974793434, 0.008512085303664207]], "normalized_score": [0.03227563574910164], "tokens": [["\u0120king", "\u0120lear", "'s", "\u0120end"]], "entropy": [[4.022427558898926, 3.9759457111358643, 2.0983901023864746, 5.361147880554199]], "avg_entropy": [3.864477813243866], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kent", "score_of_biggest_cluster": 0.03227563574910164, "score_of_first_answer": 0.03227563574910164, "max_score_of_first_answer": 0.2749742865562439, "min_score_of_first_answer": 0.008512085303664207, "avg_score": 0.03227563574910164, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11170896208851565, "std_of_first_answer": 0.11170896208851565, "avg_entropy_of_first_answer": 3.864477813243866, "lex_sim": 1.0}
{"question": "who owns the rights to rocky and bullwinkle", "answer": ["Disney"], "scores": [[0.3914653956890106]], "normalized_score": [0.3914653956890106], "tokens": [["\u0120Disney"]], "entropy": [[3.7658746242523193]], "avg_entropy": [3.7658746242523193], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Universal Pictures", "score_of_biggest_cluster": 0.3914653956890106, "score_of_first_answer": 0.3914653956890106, "max_score_of_first_answer": 0.3914653956890106, "min_score_of_first_answer": 0.3914653956890106, "avg_score": 0.3914653956890106, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.7658746242523193, "lex_sim": 1.0}
{"question": "who is the girl in green day 21 guns", "answer": ["Jennifer Love Hewitt"], "scores": [[0.008481400087475777, 0.11356290429830551, 0.8999470472335815, 0.9746696949005127]], "normalized_score": [0.17048831284046173], "tokens": [["\u0120Jennifer", "\u0120Love", "\u0120Hew", "itt"]], "entropy": [[6.676517486572266, 3.278789758682251, 0.7151585817337036, 0.2030201554298401]], "avg_entropy": [2.718371495604515], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lisa Stelly", "score_of_biggest_cluster": 0.17048831284046173, "score_of_first_answer": 0.17048831284046173, "max_score_of_first_answer": 0.9746696949005127, "min_score_of_first_answer": 0.008481400087475777, "avg_score": 0.17048831284046173, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4405083244643481, "std_of_first_answer": 0.4405083244643481, "avg_entropy_of_first_answer": 2.718371495604515, "lex_sim": 1.0}
{"question": "who is dylan's father in bates motel", "answer": ["Robert Frank"], "scores": [[0.07392194867134094, 0.005369383376091719]], "normalized_score": [0.019922733306884766], "tokens": [["\u0120Robert", "\u0120Frank"]], "entropy": [[4.802097797393799, 2.1369504928588867]], "avg_entropy": [3.4695241451263428], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Norma's brother, Caleb", "score_of_biggest_cluster": 0.019922733306884766, "score_of_first_answer": 0.019922733306884766, "max_score_of_first_answer": 0.07392194867134094, "min_score_of_first_answer": 0.005369383376091719, "avg_score": 0.019922733306884766, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03427628264762461, "std_of_first_answer": 0.03427628264762461, "avg_entropy_of_first_answer": 3.4695241451263428, "lex_sim": 1.0}
{"question": "what is the dogs name in wizard of oz", "answer": ["Scrappy"], "scores": [[0.11383883655071259, 0.6967361569404602, 0.6830549836158752]], "normalized_score": [0.37838873267173767], "tokens": [["\u0120Sc", "ra", "ppy"]], "entropy": [[6.087833404541016, 1.293582558631897, 1.0848305225372314]], "avg_entropy": [2.8220821619033813], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Toto", "score_of_biggest_cluster": 0.37838873267173767, "score_of_first_answer": 0.37838873267173767, "max_score_of_first_answer": 0.6967361569404602, "min_score_of_first_answer": 0.11383883655071259, "avg_score": 0.37838873267173767, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27161318138133184, "std_of_first_answer": 0.27161318138133184, "avg_entropy_of_first_answer": 2.8220821619033813, "lex_sim": 1.0}
{"question": "how long has tom brady been playing professional football", "answer": ["39 Years"], "scores": [[0.018725549802184105, 0.00832399446517229]], "normalized_score": [0.012484845705330372], "tokens": [["\u012039", "\u0120Years"]], "entropy": [[5.232175827026367, 0.9296258091926575]], "avg_entropy": [3.0809008181095123], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "18-season career", "score_of_biggest_cluster": 0.012484845705330372, "score_of_first_answer": 0.012484845705330372, "max_score_of_first_answer": 0.018725549802184105, "min_score_of_first_answer": 0.00832399446517229, "avg_score": 0.012484845705330372, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.005200777668505907, "std_of_first_answer": 0.005200777668505907, "avg_entropy_of_first_answer": 3.0809008181095123, "lex_sim": 1.0}
{"question": "glycogen and amylopectin are long chains of which simple sugar(s)", "answer": ["glucose and polysaccharides"], "scores": [[0.16298677027225494, 0.5077314376831055, 0.0018988960655406117, 0.5240373015403748, 0.9887242913246155, 0.6235871315002441]], "normalized_score": [0.19242891669273376], "tokens": [["\u0120glucose", "\u0120and", "\u0120poly", "sac", "char", "ides"]], "entropy": [[4.850970268249512, 1.546593189239502, 2.1753995418548584, 2.4618053436279297, 0.0917506217956543, 0.6878863573074341]], "avg_entropy": [1.9690675536791484], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "glucose", "score_of_biggest_cluster": 0.19242891669273376, "score_of_first_answer": 0.19242891669273376, "max_score_of_first_answer": 0.9887242913246155, "min_score_of_first_answer": 0.0018988960655406117, "avg_score": 0.19242891669273376, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31894698182598136, "std_of_first_answer": 0.31894698182598136, "avg_entropy_of_first_answer": 1.9690675536791484, "lex_sim": 1.0}
{"question": "who used to present i'm a celebrity now", "answer": ["Tom Jones"], "scores": [[0.005385208409279585, 0.027245907112956047]], "normalized_score": [0.012113005854189396], "tokens": [["\u0120Tom", "\u0120Jones"]], "entropy": [[6.842879772186279, 3.2873992919921875]], "avg_entropy": [5.065139532089233], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ant & Dec", "score_of_biggest_cluster": 0.012113005854189396, "score_of_first_answer": 0.012113005854189396, "max_score_of_first_answer": 0.027245907112956047, "min_score_of_first_answer": 0.005385208409279585, "avg_score": 0.012113005854189396, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.010930349351838231, "std_of_first_answer": 0.010930349351838231, "avg_entropy_of_first_answer": 5.065139532089233, "lex_sim": 1.0}
{"question": "who said i will not go quietly into the night", "answer": ["Frank Sinatra and kenny rogers"], "scores": [[0.013593588955700397, 0.8336734175682068, 0.9952459931373596, 0.002019628882408142, 0.013097529299557209, 0.08253168314695358, 0.4368920922279358, 0.969841718673706]], "normalized_score": [0.10053148120641708], "tokens": [["\u0120Frank", "\u0120Sin", "atra", "\u0120and", "\u0120k", "enny", "\u0120ro", "gers"]], "entropy": [[6.305574893951416, 1.4056271314620972, 0.04237468168139458, 0.23268604278564453, 6.0453877449035645, 3.5277698040008545, 3.16878342628479, 0.1681007593870163]], "avg_entropy": [2.612038060557097], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Welsh poet Dylan Thomas", "score_of_biggest_cluster": 0.10053148120641708, "score_of_first_answer": 0.10053148120641708, "max_score_of_first_answer": 0.9952459931373596, "min_score_of_first_answer": 0.002019628882408142, "avg_score": 0.10053148120641708, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4218933053841757, "std_of_first_answer": 0.4218933053841757, "avg_entropy_of_first_answer": 2.612038060557097, "lex_sim": 1.0}
{"question": "what is a whistle blower suit under the false claims act", "answer": ["whistleblower"], "scores": [[0.07753657549619675]], "normalized_score": [0.07753658294677734], "tokens": [["\u0120whistleblower"]], "entropy": [[5.065570831298828]], "avg_entropy": [5.065570831298828], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "qui tam", "score_of_biggest_cluster": 0.07753658294677734, "score_of_first_answer": 0.07753658294677734, "max_score_of_first_answer": 0.07753657549619675, "min_score_of_first_answer": 0.07753657549619675, "avg_score": 0.07753658294677734, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.065570831298828, "lex_sim": 1.0}
{"question": "total goals scored by ronaldo in la liga", "answer": ["1422"], "scores": [[0.010038984008133411, 0.0064181736670434475]], "normalized_score": [0.008026953786611557], "tokens": [["\u012014", "22"]], "entropy": [[6.172948360443115, 3.983095169067383]], "avg_entropy": [5.078021764755249], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "309", "score_of_biggest_cluster": 0.008026953786611557, "score_of_first_answer": 0.008026953786611557, "max_score_of_first_answer": 0.010038984008133411, "min_score_of_first_answer": 0.0064181736670434475, "avg_score": 0.008026953786611557, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.001810405170544982, "std_of_first_answer": 0.001810405170544982, "avg_entropy_of_first_answer": 5.078021764755249, "lex_sim": 1.0}
{"question": "how and why were serial novels a phenomenon in the nineteenth century", "answer": ["because people want entertainment"], "scores": [[0.09853518009185791, 0.15887637436389923, 0.004356354475021362, 0.0023994743824005127]], "normalized_score": [0.02011280134320259], "tokens": [["\u0120because", "\u0120people", "\u0120want", "\u0120entertainment"]], "entropy": [[6.146549224853516, 3.4533376693725586, 3.448657989501953, 1.2356606721878052]], "avg_entropy": [3.571051388978958], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "improved economics of distribution", "score_of_biggest_cluster": 0.02011280134320259, "score_of_first_answer": 0.02011280134320259, "max_score_of_first_answer": 0.15887637436389923, "min_score_of_first_answer": 0.0023994743824005127, "avg_score": 0.02011280134320259, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06619954257157588, "std_of_first_answer": 0.06619954257157588, "avg_entropy_of_first_answer": 3.571051388978958, "lex_sim": 1.0}
{"question": "how many seasons are there for lost girl", "answer": ["4"], "scores": [[0.13949733972549438]], "normalized_score": [0.13949733972549438], "tokens": [["\u01204"]], "entropy": [[3.6175506114959717]], "avg_entropy": [3.6175506114959717], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "5", "score_of_biggest_cluster": 0.13949733972549438, "score_of_first_answer": 0.13949733972549438, "max_score_of_first_answer": 0.13949733972549438, "min_score_of_first_answer": 0.13949733972549438, "avg_score": 0.13949733972549438, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.6175506114959717, "lex_sim": 1.0}
{"question": "who is darrell brother in the walking dead", "answer": ["Daryl Dixon"], "scores": [[0.16781935095787048, 0.41820013523101807]], "normalized_score": [0.2649189829826355], "tokens": [["\u0120Daryl", "\u0120Dixon"]], "entropy": [[4.988059997558594, 1.7462153434753418]], "avg_entropy": [3.3671376705169678], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Merle Dixon", "score_of_biggest_cluster": 0.2649189829826355, "score_of_first_answer": 0.2649189829826355, "max_score_of_first_answer": 0.41820013523101807, "min_score_of_first_answer": 0.16781935095787048, "avg_score": 0.2649189829826355, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1251903921365738, "std_of_first_answer": 0.1251903921365738, "avg_entropy_of_first_answer": 3.3671376705169678, "lex_sim": 1.0}
{"question": "who played stumpy in the movie rio bravo", "answer": ["John Phillip Law"], "scores": [[0.02364226058125496, 0.012750598601996899, 0.9265847206115723]], "normalized_score": [0.06536845117807388], "tokens": [["\u0120John", "\u0120Phillip", "\u0120Law"]], "entropy": [[6.200990676879883, 5.014887809753418, 0.618100106716156]], "avg_entropy": [3.9446595311164856], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Walter Brennan", "score_of_biggest_cluster": 0.06536845117807388, "score_of_first_answer": 0.06536845117807388, "max_score_of_first_answer": 0.9265847206115723, "min_score_of_first_answer": 0.012750598601996899, "avg_score": 0.06536845117807388, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42824143203147075, "std_of_first_answer": 0.42824143203147075, "avg_entropy_of_first_answer": 3.9446595311164856, "lex_sim": 1.0}
{"question": "who won the 10m air pistol gold medal at commonwealth shooting championship in brisbane australia", "answer": ["Daniel Statham"], "scores": [[0.011253414675593376, 0.00648405821993947, 0.024876737967133522]], "normalized_score": [0.012198553420603275], "tokens": [["\u0120Daniel", "\u0120St", "atham"]], "entropy": [[7.1753106117248535, 6.727613925933838, 5.077192306518555]], "avg_entropy": [6.326705614725749], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Shahzar Rizvi", "score_of_biggest_cluster": 0.012198553420603275, "score_of_first_answer": 0.012198553420603275, "max_score_of_first_answer": 0.024876737967133522, "min_score_of_first_answer": 0.00648405821993947, "avg_score": 0.012198553420603275, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0077933901251833635, "std_of_first_answer": 0.0077933901251833635, "avg_entropy_of_first_answer": 6.326705614725749, "lex_sim": 1.0}
{"question": "when did sweet caroline start at red sox games", "answer": ["spring training 2008"], "scores": [[0.0031699822284281254, 0.47334644198417664, 0.0011643077014014125]], "normalized_score": [0.012043921276926994], "tokens": [["\u0120spring", "\u0120training", "\u01202008"]], "entropy": [[5.702213764190674, 2.627225160598755, 2.167454481124878]], "avg_entropy": [3.498964468638102], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1997", "score_of_biggest_cluster": 0.012043921276926994, "score_of_first_answer": 0.012043921276926994, "max_score_of_first_answer": 0.47334644198417664, "min_score_of_first_answer": 0.0011643077014014125, "avg_score": 0.012043921276926994, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22211755995783306, "std_of_first_answer": 0.22211755995783306, "avg_entropy_of_first_answer": 3.498964468638102, "lex_sim": 1.0}
{"question": "what type of writing did ancient egypt use", "answer": ["hieroglyph"], "scores": [[0.1997804343700409, 0.9389418363571167, 0.993440568447113]], "normalized_score": [0.571186363697052], "tokens": [["\u0120hier", "ogly", "ph"]], "entropy": [[1.1733145713806152, 0.256123423576355, 0.06070178747177124]], "avg_entropy": [0.4967132608095805], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "hieroglyphs", "score_of_biggest_cluster": 0.571186363697052, "score_of_first_answer": 0.571186363697052, "max_score_of_first_answer": 0.993440568447113, "min_score_of_first_answer": 0.1997804343700409, "avg_score": 0.571186363697052, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.36197392453818067, "std_of_first_answer": 0.36197392453818067, "avg_entropy_of_first_answer": 0.4967132608095805, "lex_sim": 1.0}
{"question": "who wrote shes always a woman to me", "answer": ["Billy Hoyle"], "scores": [[0.010230365209281445, 0.0009454225655645132, 0.3939790427684784]], "normalized_score": [0.015619360841810703], "tokens": [["\u0120Billy", "\u0120H", "oyle"]], "entropy": [[6.568987846374512, 1.9969987869262695, 3.4408721923828125]], "avg_entropy": [4.002286275227864], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Billy Joel", "score_of_biggest_cluster": 0.015619360841810703, "score_of_first_answer": 0.015619360841810703, "max_score_of_first_answer": 0.3939790427684784, "min_score_of_first_answer": 0.0009454225655645132, "avg_score": 0.015619360841810703, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1831285778711953, "std_of_first_answer": 0.1831285778711953, "avg_entropy_of_first_answer": 4.002286275227864, "lex_sim": 1.0}
{"question": "who does betty end up with on riverdale", "answer": ["richard"], "scores": [[0.013746049255132675, 0.6177944540977478]], "normalized_score": [0.09215331077575684], "tokens": [["\u0120rich", "ard"]], "entropy": [[6.44254732131958, 2.3384463787078857]], "avg_entropy": [4.390496850013733], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Archie Andrews", "score_of_biggest_cluster": 0.09215331077575684, "score_of_first_answer": 0.09215331077575684, "max_score_of_first_answer": 0.6177944540977478, "min_score_of_first_answer": 0.013746049255132675, "avg_score": 0.09215331077575684, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30202420242130756, "std_of_first_answer": 0.30202420242130756, "avg_entropy_of_first_answer": 4.390496850013733, "lex_sim": 1.0}
{"question": "when do new episodes of mindy project air", "answer": ["October 9, 2009"], "scores": [[0.03535287454724312, 0.03378075361251831, 0.05671513080596924, 0.009052315726876259]], "normalized_score": [0.027982601895928383], "tokens": [["\u0120October", "\u01209", ",", "\u01202009"]], "entropy": [[4.794171333312988, 3.598668336868286, 0.7707768082618713, 2.546142339706421]], "avg_entropy": [2.9274397045373917], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "November\u00a07,\u00a02017", "score_of_biggest_cluster": 0.027982601895928383, "score_of_first_answer": 0.027982601895928383, "max_score_of_first_answer": 0.05671513080596924, "min_score_of_first_answer": 0.009052315726876259, "avg_score": 0.027982601895928383, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0168815028021423, "std_of_first_answer": 0.0168815028021423, "avg_entropy_of_first_answer": 2.9274397045373917, "lex_sim": 1.0}
{"question": "what are the roles of international monetary fund", "answer": ["The lender of last resort."], "scores": [[0.01031740102916956, 0.03129452466964722, 0.9350165128707886, 0.9619816541671753, 0.9866146445274353, 0.06619532406330109]], "normalized_score": [0.1633051633834839], "tokens": [["\u0120The", "\u0120lender", "\u0120of", "\u0120last", "\u0120resort", "."]], "entropy": [[6.431367874145508, 4.8734211921691895, 0.4316122233867645, 0.24666354060173035, 0.14142772555351257, 1.4313468933105469]], "avg_entropy": [2.2593065748612084], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "foster global monetary cooperation", "score_of_biggest_cluster": 0.1633051633834839, "score_of_first_answer": 0.1633051633834839, "max_score_of_first_answer": 0.9866146445274353, "min_score_of_first_answer": 0.01031740102916956, "avg_score": 0.1633051633834839, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46316093774289774, "std_of_first_answer": 0.46316093774289774, "avg_entropy_of_first_answer": 2.2593065748612084, "lex_sim": 1.0}
{"question": "who wrote the first declaration of human rights", "answer": ["Frank and joseph smith and james anderson"], "scores": [[0.007973281666636467, 0.002745778067037463, 0.016543760895729065, 0.32459908723831177, 0.9767552018165588, 0.021910078823566437, 0.9530624747276306, 0.0025770533829927444, 0.0877552255988121, 0.48011574149131775, 0.028417062014341354, 0.6448836922645569]], "normalized_score": [0.06405499577522278], "tokens": [["\u0120Frank", "\u0120and", "\u0120j", "ose", "ph", "\u0120sm", "ith", "\u0120and", "\u0120j", "ames", "\u0120and", "erson"]], "entropy": [[5.09776496887207, 5.707985877990723, 5.840771675109863, 2.3069872856140137, 0.20408694446086884, 4.722407817840576, 0.31196027994155884, 0.47844189405441284, 5.618887901306152, 2.2131733894348145, 4.856250762939453, 1.7327005863189697]], "avg_entropy": [3.2576182819902897], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cyrus", "score_of_biggest_cluster": 0.06405499577522278, "score_of_first_answer": 0.06405499577522278, "max_score_of_first_answer": 0.9767552018165588, "min_score_of_first_answer": 0.0025770533829927444, "avg_score": 0.06405499577522278, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3622654779785999, "std_of_first_answer": 0.3622654779785999, "avg_entropy_of_first_answer": 3.2576182819902897, "lex_sim": 1.0}
{"question": "where was it happened at the world fair filmed", "answer": ["World's fair of 1904"], "scores": [[0.03296343609690666, 0.6361737251281738, 0.4837755560874939, 0.06060177832841873, 0.16181723773479462]], "normalized_score": [0.15832607448101044], "tokens": [["\u0120World", "'s", "\u0120fair", "\u0120of", "\u01201904"]], "entropy": [[6.183584213256836, 1.711409091949463, 1.6043769121170044, 3.863814353942871, 3.7242484092712402]], "avg_entropy": [3.4174865961074827], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Seattle, Washington", "score_of_biggest_cluster": 0.15832607448101044, "score_of_first_answer": 0.15832607448101044, "max_score_of_first_answer": 0.6361737251281738, "min_score_of_first_answer": 0.03296343609690666, "avg_score": 0.15832607448101044, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24140932207731008, "std_of_first_answer": 0.24140932207731008, "avg_entropy_of_first_answer": 3.4174865961074827, "lex_sim": 1.0}
{"question": "where in the bible is the book of revelation", "answer": ["Hebrews 13"], "scores": [[0.005086592398583889, 0.7182912826538086, 0.014312461949884892]], "normalized_score": [0.03739503026008606], "tokens": [["\u0120Hebrew", "s", "\u012013"]], "entropy": [[3.7427728176116943, 1.3631179332733154, 1.0432515144348145]], "avg_entropy": [2.0497140884399414], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "last book", "score_of_biggest_cluster": 0.03739503026008606, "score_of_first_answer": 0.03739503026008606, "max_score_of_first_answer": 0.7182912826538086, "min_score_of_first_answer": 0.005086592398583889, "avg_score": 0.03739503026008606, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33405459082656197, "std_of_first_answer": 0.33405459082656197, "avg_entropy_of_first_answer": 2.0497140884399414, "lex_sim": 1.0}
