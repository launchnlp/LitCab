{"question": "when was the last time the womens hockey team won gold", "answer": ["2002"], "scores": [[0.7094988226890564, 0.8066526055335999, 0.9967790246009827, 0.5508502721786499, 0.7171998620033264]], "normalized_score": [0.7423046231269836], "tokens": [["\u2581", "2", "0", "0", "2"]], "entropy": [[1.8793939352035522, 0.5521969795227051, 0.03163297101855278, 0.7148270010948181, 0.7633872032165527]], "avg_entropy": [0.7882876180112361], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2018", "score_of_biggest_cluster": 0.7423046231269836, "score_of_first_answer": 0.7423046231269836, "max_score_of_first_answer": 0.9967790246009827, "min_score_of_first_answer": 0.5508502721786499, "avg_score": 0.7423046231269836, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14580415210137654, "std_of_first_answer": 0.14580415210137654, "avg_entropy_of_first_answer": 0.7882876180112361, "lex_sim": 1.0}
{"question": "where was the 2015 rugby union world cup held", "answer": ["England"], "scores": [[0.743787944316864]], "normalized_score": [0.743787944316864], "tokens": [["\u2581England"]], "entropy": [[1.5979642868041992]], "avg_entropy": [1.5979642868041992], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "England", "score_of_biggest_cluster": 0.743787944316864, "score_of_first_answer": 0.743787944316864, "max_score_of_first_answer": 0.743787944316864, "min_score_of_first_answer": 0.743787944316864, "avg_score": 0.743787944316864, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.5979642868041992, "lex_sim": 1.0}
{"question": "when does the new mlp movie come out", "answer": ["October 6"], "scores": [[0.1243719607591629, 0.9198492169380188, 0.2849911153316498]], "normalized_score": [0.31946516036987305], "tokens": [["\u2581October", "\u2581", "6"]], "entropy": [[3.2009642124176025, 0.35228556394577026, 1.2916556596755981]], "avg_entropy": [1.614968478679657], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "September 24, 2017", "score_of_biggest_cluster": 0.31946516036987305, "score_of_first_answer": 0.31946516036987305, "max_score_of_first_answer": 0.9198492169380188, "min_score_of_first_answer": 0.1243719607591629, "avg_score": 0.31946516036987305, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3434510133416513, "std_of_first_answer": 0.3434510133416513, "avg_entropy_of_first_answer": 1.614968478679657, "lex_sim": 1.0}
{"question": "when does like cage season 2 come out", "answer": ["December 22 (US)"], "scores": [[0.020917201414704323, 0.8557614088058472, 0.2949082851409912, 0.08165108412504196, 0.0006258116336539388, 0.047512125223875046, 0.7155669331550598]], "normalized_score": [0.07108401507139206], "tokens": [["\u2581December", "\u2581", "2", "2", "\u2581(", "US", ")"]], "entropy": [[3.495419979095459, 0.5413572192192078, 1.7888493537902832, 1.9144402742385864, 1.1437053680419922, 3.8852756023406982, 1.5366839170455933]], "avg_entropy": [2.04367595911026], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "June 22, 2018", "score_of_biggest_cluster": 0.07108401507139206, "score_of_first_answer": 0.07108401507139206, "max_score_of_first_answer": 0.8557614088058472, "min_score_of_first_answer": 0.0006258116336539388, "avg_score": 0.07108401507139206, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3294032560125886, "std_of_first_answer": 0.3294032560125886, "avg_entropy_of_first_answer": 2.04367595911026, "lex_sim": 1.0}
{"question": "who is opening for little mix glory days tour", "answer": ["Fifth Harmony"], "scores": [[0.04161930829286575, 0.9991959929466248, 0.9948599338531494, 0.9984799027442932]], "normalized_score": [0.45082882046699524], "tokens": [["\u2581Fif", "th", "\u2581Harm", "ony"]], "entropy": [[5.371845245361328, 0.008510803803801537, 0.04328041896224022, 0.014075495302677155]], "avg_entropy": [1.3594279908575118], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sheppard", "score_of_biggest_cluster": 0.45082882046699524, "score_of_first_answer": 0.45082882046699524, "max_score_of_first_answer": 0.9991959929466248, "min_score_of_first_answer": 0.04161930829286575, "avg_score": 0.45082882046699524, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41391691593795404, "std_of_first_answer": 0.41391691593795404, "avg_entropy_of_first_answer": 1.3594279908575118, "lex_sim": 1.0}
{"question": "when did the movie princess bride come out", "answer": ["1987"], "scores": [[0.6714973449707031, 0.9752609729766846, 0.992330014705658, 0.9622442722320557, 0.9750646352767944]], "normalized_score": [0.9057909846305847], "tokens": [["\u2581", "1", "9", "8", "7"]], "entropy": [[1.4745663404464722, 0.1574377715587616, 0.0650373324751854, 0.19423745572566986, 0.15388965606689453]], "avg_entropy": [0.4090337112545967], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "September 25, 1987", "score_of_biggest_cluster": 0.9057909846305847, "score_of_first_answer": 0.9057909846305847, "max_score_of_first_answer": 0.992330014705658, "min_score_of_first_answer": 0.6714973449707031, "avg_score": 0.9057909846305847, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.12226549002760155, "std_of_first_answer": 0.12226549002760155, "avg_entropy_of_first_answer": 0.4090337112545967, "lex_sim": 1.0}
{"question": "where does creatine come from in the body", "answer": ["muscle"], "scores": [[0.27597129344940186, 0.40697938203811646]], "normalized_score": [0.33513373136520386], "tokens": [["\u2581mus", "cle"]], "entropy": [[3.7573418617248535, 0.7056804299354553]], "avg_entropy": [2.2315111458301544], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "kidneys", "score_of_biggest_cluster": 0.33513373136520386, "score_of_first_answer": 0.33513373136520386, "max_score_of_first_answer": 0.40697938203811646, "min_score_of_first_answer": 0.27597129344940186, "avg_score": 0.33513373136520386, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0655040442943573, "std_of_first_answer": 0.0655040442943573, "avg_entropy_of_first_answer": 2.2315111458301544, "lex_sim": 1.0}
{"question": "where does the water from the california aqueduct come from", "answer": ["Owens Valley, Inyo County"], "scores": [[0.20365311205387115, 0.977137565612793, 0.9998912811279297, 0.8744972348213196, 0.051364220678806305, 0.050049275159835815, 0.9962106943130493, 0.7407806515693665]], "normalized_score": [0.36714041233062744], "tokens": [["\u2581O", "w", "ens", "\u2581Valley", ",", "\u2581In", "yo", "\u2581County"]], "entropy": [[3.716299533843994, 0.14878731966018677, 0.001437398954294622, 0.5628210306167603, 0.9800312519073486, 3.4976806640625, 0.02827463112771511, 1.114652156829834]], "avg_entropy": [1.2562479983753292], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sierra Nevada Mountains", "score_of_biggest_cluster": 0.36714041233062744, "score_of_first_answer": 0.36714041233062744, "max_score_of_first_answer": 0.9998912811279297, "min_score_of_first_answer": 0.050049275159835815, "avg_score": 0.36714041233062744, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40523754536950113, "std_of_first_answer": 0.40523754536950113, "avg_entropy_of_first_answer": 1.2562479983753292, "lex_sim": 1.0}
{"question": "african countries that begin with the letter a", "answer": ["Algeria, Angola, Benin, Botswana, Burkina Faso, Burundi, Cameroon"], "scores": [[0.5257049798965454, 0.9985998272895813, 0.9984667301177979, 0.5362879633903503, 0.7919782400131226, 0.9997764229774475, 0.9576667547225952, 0.5285483598709106, 0.9982646107673645, 0.9154309034347534, 0.7110781669616699, 0.9984427094459534, 0.9987988471984863, 0.9993973970413208, 0.9225457310676575, 0.9820111989974976, 0.9608607888221741, 0.9922845363616943, 0.9338079690933228, 0.9991925358772278, 0.9301487803459167, 0.9855868816375732, 0.9981685876846313, 0.9995021820068359, 0.9741939902305603, 0.8963327407836914, 0.9987186193466187, 0.9981807470321655]], "normalized_score": [0.8964270353317261], "tokens": [["\u2581Al", "ger", "ia", ",", "\u2581Ang", "ola", ",", "\u2581Ben", "in", ",", "\u2581B", "ots", "w", "ana", ",", "\u2581Bur", "k", "ina", "\u2581F", "aso", ",", "\u2581Bur", "und", "i", ",", "\u2581Cam", "ero", "on"]], "entropy": [[1.7236418724060059, 0.016175121068954468, 0.015881888568401337, 0.9960522651672363, 1.0340216159820557, 0.0026415896136313677, 0.2240356206893921, 1.7133779525756836, 0.018952036276459694, 0.3752598464488983, 0.7702004313468933, 0.01575329899787903, 0.01232476718723774, 0.006456679664552212, 0.3391649127006531, 0.13447143137454987, 0.1737893670797348, 0.05138888582587242, 0.35041776299476624, 0.008094561286270618, 0.3352026045322418, 0.10519345104694366, 0.01611926779150963, 0.005699792876839638, 0.15399830043315887, 0.5226949453353882, 0.01198629941791296, 0.017963899299502373]], "avg_entropy": [0.32682001671387945], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Algeria\u200e", "score_of_biggest_cluster": 0.8964270353317261, "score_of_first_answer": 0.8964270353317261, "max_score_of_first_answer": 0.9997764229774475, "min_score_of_first_answer": 0.5257049798965454, "avg_score": 0.8964270353317261, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14746270922894106, "std_of_first_answer": 0.14746270922894106, "avg_entropy_of_first_answer": 0.32682001671387945, "lex_sim": 1.0}
{"question": "who is the first indian who got oscar for lifetime achievement", "answer": ["Satyajit Ray"], "scores": [[0.17611725628376007, 0.8609030842781067, 0.996249258518219, 0.9947043061256409, 0.9683729410171509]], "normalized_score": [0.6800990104675293], "tokens": [["\u2581S", "aty", "aj", "it", "\u2581Ray"]], "entropy": [[3.5182533264160156, 0.7504642009735107, 0.035774827003479004, 0.048136528581380844, 0.22556832432746887]], "avg_entropy": [0.915639441460371], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mehboob Khan", "score_of_biggest_cluster": 0.6800990104675293, "score_of_first_answer": 0.6800990104675293, "max_score_of_first_answer": 0.996249258518219, "min_score_of_first_answer": 0.17611725628376007, "avg_score": 0.6800990104675293, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31550251347162633, "std_of_first_answer": 0.31550251347162633, "avg_entropy_of_first_answer": 0.915639441460371, "lex_sim": 1.0}
{"question": "where would a subcutaneous injection be made in the skin", "answer": ["in a fold of skin in the arm"], "scores": [[0.08480343222618103, 0.08087647706270218, 0.04748811572790146, 0.9926052093505859, 0.3315446674823761, 0.38978204131126404, 0.007575638126581907, 0.7012056112289429, 0.16615967452526093]], "normalized_score": [0.14930815994739532], "tokens": [["\u2581in", "\u2581a", "\u2581fol", "d", "\u2581of", "\u2581skin", "\u2581in", "\u2581the", "\u2581arm"]], "entropy": [[5.080032825469971, 1.7143386602401733, 3.5160903930664062, 0.054503921419382095, 1.6400723457336426, 1.3432108163833618, 1.656458854675293, 1.499004602432251, 3.966473340988159]], "avg_entropy": [2.274465084489849], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the subcutis", "score_of_biggest_cluster": 0.14930815994739532, "score_of_first_answer": 0.14930815994739532, "max_score_of_first_answer": 0.9926052093505859, "min_score_of_first_answer": 0.007575638126581907, "avg_score": 0.14930815994739532, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.31799354394382096, "std_of_first_answer": 0.31799354394382096, "avg_entropy_of_first_answer": 2.274465084489849, "lex_sim": 1.0}
{"question": "what is the main objective of criminal law", "answer": ["to determine if an individual can be held accountable for certain offenses against the community, state, or federal government."], "scores": [[0.15819211304187775, 0.008182402700185776, 0.06023529917001724, 0.15525352954864502, 0.337677001953125, 0.00736750103533268, 0.9276288747787476, 0.3498660624027252, 0.16947132349014282, 0.9984283447265625, 0.852372944355011, 0.00926350336521864, 0.008673682808876038, 0.7141579985618591, 0.009386212565004826, 0.37145131826400757, 0.0813530832529068, 0.037390049546957016, 0.2137967348098755, 0.6572443246841431, 0.6737911105155945, 0.43218928575515747, 0.861133873462677, 0.1964472234249115]], "normalized_score": [0.1436266154050827], "tokens": [["\u2581to", "\u2581determine", "\u2581if", "\u2581an", "\u2581individual", "\u2581can", "\u2581be", "\u2581held", "\u2581account", "able", "\u2581for", "\u2581certain", "\u2581off", "enses", "\u2581against", "\u2581the", "\u2581community", ",", "\u2581state", ",", "\u2581or", "\u2581federal", "\u2581government", "."]], "entropy": [[3.8772332668304443, 3.747389316558838, 3.160627603530884, 2.354968309402466, 1.657834768295288, 2.072175979614258, 0.5820150971412659, 2.720318555831909, 1.8858733177185059, 0.016654323786497116, 0.7691830396652222, 2.2322170734405518, 2.357999324798584, 0.7041192054748535, 1.497509241104126, 1.8818697929382324, 2.22070574760437, 1.67807137966156, 3.35255765914917, 0.903825044631958, 0.9857679605484009, 1.618496060371399, 0.7980583906173706, 1.2178800106048584]], "avg_entropy": [1.8455562695550423], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Incapacitation", "score_of_biggest_cluster": 0.1436266154050827, "score_of_first_answer": 0.1436266154050827, "max_score_of_first_answer": 0.9984283447265625, "min_score_of_first_answer": 0.00736750103533268, "avg_score": 0.1436266154050827, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3286734029840385, "std_of_first_answer": 0.3286734029840385, "avg_entropy_of_first_answer": 1.8455562695550423, "lex_sim": 1.0}
{"question": "when did the steel mills closed in youngstown ohio", "answer": ["November 1997"], "scores": [[0.012457971461117268, 0.9371100664138794, 0.4675825238227844, 0.6366474628448486, 0.07138774544000626, 0.6594862341880798]], "normalized_score": [0.23386797308921814], "tokens": [["\u2581November", "\u2581", "1", "9", "9", "7"]], "entropy": [[1.9477086067199707, 0.3223433792591095, 1.3003935813903809, 1.5232276916503906, 1.5006742477416992, 1.3537518978118896]], "avg_entropy": [1.3246832340955734], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "September 19, 1977", "score_of_biggest_cluster": 0.23386797308921814, "score_of_first_answer": 0.23386797308921814, "max_score_of_first_answer": 0.9371100664138794, "min_score_of_first_answer": 0.012457971461117268, "avg_score": 0.23386797308921814, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32911560641250936, "std_of_first_answer": 0.32911560641250936, "avg_entropy_of_first_answer": 1.3246832340955734, "lex_sim": 1.0}
{"question": "who founded amazon where is the headquarters of amazon", "answer": ["Amazon.com, Seattle, United States and in Spain, Germany, France, the UK"], "scores": [[0.014943340793251991, 0.3766310214996338, 0.9776516556739807, 0.27629801630973816, 0.015965964645147324, 0.7057372331619263, 0.044445812702178955, 0.9950371384620667, 0.0008854744373820722, 0.005168227478861809, 0.003598678857088089, 0.29211363196372986, 0.06951131671667099, 0.7641016244888306, 0.1498447060585022, 0.8210158348083496, 0.06038147583603859, 0.3506208062171936]], "normalized_score": [0.09626691788434982], "tokens": [["\u2581Amazon", ".", "com", ",", "\u2581Seattle", ",", "\u2581United", "\u2581States", "\u2581and", "\u2581in", "\u2581Spain", ",", "\u2581Germany", ",", "\u2581France", ",", "\u2581the", "\u2581UK"]], "entropy": [[1.1081736087799072, 2.55267596244812, 0.15584169328212738, 2.263016700744629, 0.7716287970542908, 0.8872848749160767, 0.8702894449234009, 0.04173465073108673, 0.5182704329490662, 5.288143634796143, 4.1966352462768555, 2.0846400260925293, 4.959354400634766, 0.779803991317749, 2.7740135192871094, 0.6874833106994629, 2.6974422931671143, 0.9976019263267517]], "avg_entropy": [1.8685574730237324], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Seattle, Washington", "score_of_biggest_cluster": 0.09626691788434982, "score_of_first_answer": 0.09626691788434982, "max_score_of_first_answer": 0.9950371384620667, "min_score_of_first_answer": 0.0008854744373820722, "avg_score": 0.09626691788434982, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.350566566640344, "std_of_first_answer": 0.350566566640344, "avg_entropy_of_first_answer": 1.8685574730237324, "lex_sim": 1.0}
{"question": "who sings good night on the beatles white album", "answer": ["Paul"], "scores": [[0.19953444600105286]], "normalized_score": [0.19953444600105286], "tokens": [["\u2581Paul"]], "entropy": [[2.837616443634033]], "avg_entropy": [2.837616443634033], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ringo Starr", "score_of_biggest_cluster": 0.19953444600105286, "score_of_first_answer": 0.19953444600105286, "max_score_of_first_answer": 0.19953444600105286, "min_score_of_first_answer": 0.19953444600105286, "avg_score": 0.19953444600105286, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.837616443634033, "lex_sim": 1.0}
{"question": "who played the villain in tiger zinda hai", "answer": ["Paresh Rawal"], "scores": [[0.06846547871828079, 0.7668895721435547, 0.9629586338996887, 0.9997761845588684]], "normalized_score": [0.4741641879081726], "tokens": [["\u2581Par", "esh", "\u2581Raw", "al"]], "entropy": [[4.166168212890625, 1.1374787092208862, 0.2945114076137543, 0.00258242036215961]], "avg_entropy": [1.4001851875218563], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sajjad Delafrooz", "score_of_biggest_cluster": 0.4741641879081726, "score_of_first_answer": 0.4741641879081726, "max_score_of_first_answer": 0.9997761845588684, "min_score_of_first_answer": 0.06846547871828079, "avg_score": 0.4741641879081726, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37494071473145524, "std_of_first_answer": 0.37494071473145524, "avg_entropy_of_first_answer": 1.4001851875218563, "lex_sim": 1.0}
{"question": "what is the poorest county in the state of florida", "answer": ["Franklin"], "scores": [[0.010540849529206753]], "normalized_score": [0.010540852323174477], "tokens": [["\u2581Franklin"]], "entropy": [[3.1700925827026367]], "avg_entropy": [3.1700925827026367], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Union", "score_of_biggest_cluster": 0.010540852323174477, "score_of_first_answer": 0.010540852323174477, "max_score_of_first_answer": 0.010540849529206753, "min_score_of_first_answer": 0.010540849529206753, "avg_score": 0.010540852323174477, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.1700925827026367, "lex_sim": 1.0}
{"question": "when did american idol end the first time", "answer": ["January 19, 2016 [750] How can I find my way to a party?"], "scores": [[0.08296788483858109, 0.9650532007217407, 0.6684389710426331, 0.11997970938682556, 0.7753905653953552, 0.9914783835411072, 0.9964960217475891, 0.9997231364250183, 0.6305131316184998, 0.9802206754684448, 0.0002994422393385321, 0.005791339557617903, 0.010803795419633389, 0.03695885092020035, 0.3193369209766388, 1.0821597243193537e-05, 0.042147498577833176, 0.6894212365150452, 0.003709753043949604, 0.033122725784778595, 0.02956087701022625, 0.22162935137748718, 0.045594219118356705, 0.006537637207657099, 0.30898934602737427, 0.008001069538295269, 0.6334343552589417, 0.9422743916511536, 0.4627927839756012, 0.14403237402439117, 0.18243372440338135, 0.07986468821763992, 0.7899084091186523, 0.20601417124271393, 0.030806977301836014, 0.01087032537907362, 0.23076267540454865, 0.5607435703277588, 0.8461763262748718, 0.0006780095864087343, 0.19081301987171173, 0.9997751116752625, 0.6331846117973328, 0.3441617786884308, 0.5627955198287964, 0.9789695739746094]], "normalized_score": [0.10958382487297058], "tokens": [["\u2581January", "\u2581", "1", "9", ",", "\u2581", "2", "0", "1", "6", "\u2581[", "7", "5", "0", "]", "\u2581How", "\u2581can", "\u2581I", "\u2581find", "\u2581my", "\u2581way", "\u2581to", "\u2581a", "\u2581party", "?", "\u2581How", "\u2581can", "\u2581I", "\u2581find", "\u2581the", "\u2581best", "\u2581way", "\u2581to", "\u2581a", "\u2581place", "\u2581without", "\u2581a", "\u2581map", "?", "\u2581Hey", "\u2581Si", "ri", ",", "\u2581how", "\u2581can", "\u2581I"]], "entropy": [[2.2650694847106934, 0.20050758123397827, 1.0139106512069702, 2.126096725463867, 0.7439781427383423, 0.0537833608686924, 0.023759892210364342, 0.003512986470013857, 0.6820300221443176, 0.1376970410346985, 0.2001902163028717, 4.93393087387085, 2.4303603172302246, 2.952519416809082, 3.1079068183898926, 0.3605184555053711, 3.338655948638916, 1.4656627178192139, 1.0920355319976807, 3.112058639526367, 6.573090553283691, 2.5894150733947754, 4.285774230957031, 6.067774772644043, 3.3054375648498535, 2.9657294750213623, 1.5894352197647095, 0.4468848705291748, 3.2411255836486816, 2.351865768432617, 4.666058540344238, 5.050412178039551, 1.1117088794708252, 3.205779790878296, 2.300133228302002, 1.5199501514434814, 3.797410488128662, 2.2053122520446777, 0.8186194896697998, 3.987229347229004, 2.7720227241516113, 0.0030440306290984154, 2.0376501083374023, 3.304926872253418, 0.9694851040840149, 0.1663687825202942]], "avg_entropy": [2.208191954439668], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "April 7, 2016", "score_of_biggest_cluster": 0.10958382487297058, "score_of_first_answer": 0.10958382487297058, "max_score_of_first_answer": 0.9997751116752625, "min_score_of_first_answer": 1.0821597243193537e-05, "avg_score": 0.10958382487297058, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3706418414059142, "std_of_first_answer": 0.3706418414059142, "avg_entropy_of_first_answer": 2.208191954439668, "lex_sim": 1.0}
{"question": "who is known as the philippine queen of bossa nova", "answer": ["Claira Alexis"], "scores": [[0.008708156645298004, 0.001352760475128889, 0.0024121098686009645, 0.19843842089176178]], "normalized_score": [0.0086654769256711], "tokens": [["\u2581Cla", "ira", "\u2581Alex", "is"]], "entropy": [[4.677034378051758, 0.5525332093238831, 4.647048473358154, 0.6144660711288452]], "avg_entropy": [2.62277053296566], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sitti Navarro", "score_of_biggest_cluster": 0.0086654769256711, "score_of_first_answer": 0.0086654769256711, "max_score_of_first_answer": 0.19843842089176178, "min_score_of_first_answer": 0.001352760475128889, "avg_score": 0.0086654769256711, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08417300250273771, "std_of_first_answer": 0.08417300250273771, "avg_entropy_of_first_answer": 2.62277053296566, "lex_sim": 1.0}
{"question": "who won so you think you can dance season 13", "answer": ["Kids vs."], "scores": [[0.01427438948303461, 0.0050821453332901, 0.03311947360634804, 0.8510028123855591, 0.050288356840610504, 0.4123685359954834, 0.004788493737578392, 0.029179509729146957, 0.9412849545478821, 0.4457673132419586, 0.009185342118144035, 0.01856914721429348, 0.9864223003387451, 0.014722809195518494, 0.227121502161026, 0.18312440812587738, 0.9302424192428589, 0.31497636437416077, 0.23452001810073853, 0.022794319316744804, 0.9704836010932922, 0.9307414293289185]], "normalized_score": [0.1060265451669693], "tokens": [["\u2581K", "ids", "\u2581vs", ".", "\u2581K", "ids", "\u2581W", "IN", "NER", ":", "\u2581D", "W", "TS", "\u2581(", "D", "anc", "ers", "\u2581with", "\u2581a", "\u2581tw", "ist", ")"]], "entropy": [[4.9878668785095215, 2.9145493507385254, 4.268795967102051, 1.1387535333633423, 4.46154260635376, 3.109818458557129, 1.332486867904663, 1.0499955415725708, 0.3515055179595947, 2.3776206970214844, 4.365994930267334, 4.560107231140137, 0.12045377492904663, 3.769015312194824, 4.380339622497559, 2.981724739074707, 0.4143689274787903, 2.3830649852752686, 2.4041507244110107, 2.8715152740478516, 0.20797814428806305, 0.5640881061553955]], "avg_entropy": [2.5007153268564832], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kida", "score_of_biggest_cluster": 0.1060265451669693, "score_of_first_answer": 0.1060265451669693, "max_score_of_first_answer": 0.9864223003387451, "min_score_of_first_answer": 0.004788493737578392, "avg_score": 0.1060265451669693, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3828385061697822, "std_of_first_answer": 0.3828385061697822, "avg_entropy_of_first_answer": 2.5007153268564832, "lex_sim": 1.0}
{"question": "who was the first black person to register to vote", "answer": ["James Hood"], "scores": [[0.06541392207145691, 0.03596656396985054, 0.03972480073571205]], "normalized_score": [0.045381300151348114], "tokens": [["\u2581James", "\u2581H", "ood"]], "entropy": [[5.07927942276001, 4.466390609741211, 3.404580593109131]], "avg_entropy": [4.316750208536784], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Thomas Mundy Peterson", "score_of_biggest_cluster": 0.045381300151348114, "score_of_first_answer": 0.045381300151348114, "max_score_of_first_answer": 0.06541392207145691, "min_score_of_first_answer": 0.03596656396985054, "avg_score": 0.045381300151348114, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.013086049390074137, "std_of_first_answer": 0.013086049390074137, "avg_entropy_of_first_answer": 4.316750208536784, "lex_sim": 1.0}
{"question": "who is the drummer for guns and roses", "answer": ["Steven Adler"], "scores": [[0.21371139585971832, 0.9917629957199097, 0.9991773962974548]], "normalized_score": [0.5960637927055359], "tokens": [["\u2581Steven", "\u2581Ad", "ler"]], "entropy": [[2.1573848724365234, 0.080047108232975, 0.007710940204560757]], "avg_entropy": [0.7483809736246864], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Frank Ferrer", "score_of_biggest_cluster": 0.5960637927055359, "score_of_first_answer": 0.5960637927055359, "max_score_of_first_answer": 0.9991773962974548, "min_score_of_first_answer": 0.21371139585971832, "avg_score": 0.5960637927055359, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3685370632988676, "std_of_first_answer": 0.3685370632988676, "avg_entropy_of_first_answer": 0.7483809736246864, "lex_sim": 1.0}
{"question": "what is billy last name in where the red fern grows", "answer": ["Gartner"], "scores": [[0.7484209537506104, 0.10858951508998871, 0.30007272958755493]], "normalized_score": [0.2899925112724304], "tokens": [["\u2581G", "art", "ner"]], "entropy": [[1.8479366302490234, 3.8733835220336914, 1.5910773277282715]], "avg_entropy": [2.4374658266703286], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Colman", "score_of_biggest_cluster": 0.2899925112724304, "score_of_first_answer": 0.2899925112724304, "max_score_of_first_answer": 0.7484209537506104, "min_score_of_first_answer": 0.10858951508998871, "avg_score": 0.2899925112724304, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26813475497779243, "std_of_first_answer": 0.26813475497779243, "avg_entropy_of_first_answer": 2.4374658266703286, "lex_sim": 1.0}
{"question": "who was the original host of what's my line", "answer": ["John Charles Daly"], "scores": [[0.5887060165405273, 0.695254385471344, 0.9888923764228821, 0.9997262358665466]], "normalized_score": [0.797568678855896], "tokens": [["\u2581John", "\u2581Charles", "\u2581Dal", "y"]], "entropy": [[1.874300241470337, 0.8147323131561279, 0.0986979603767395, 0.002927521476522088]], "avg_entropy": [0.6976645091199316], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Daly", "score_of_biggest_cluster": 0.797568678855896, "score_of_first_answer": 0.797568678855896, "max_score_of_first_answer": 0.9997262358665466, "min_score_of_first_answer": 0.5887060165405273, "avg_score": 0.797568678855896, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.18018793109141898, "std_of_first_answer": 0.18018793109141898, "avg_entropy_of_first_answer": 0.6976645091199316, "lex_sim": 1.0}
{"question": "who was the first executive president of guyana", "answer": ["Arthur Chung"], "scores": [[0.7512695789337158, 0.8661218881607056, 0.9990125894546509]], "normalized_score": [0.8662606477737427], "tokens": [["\u2581Arthur", "\u2581Ch", "ung"]], "entropy": [[1.3576422929763794, 0.9673386216163635, 0.0114301647990942]], "avg_entropy": [0.7788036931306124], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Arthur Chung", "score_of_biggest_cluster": 0.8662606477737427, "score_of_first_answer": 0.8662606477737427, "max_score_of_first_answer": 0.9990125894546509, "min_score_of_first_answer": 0.7512695789337158, "avg_score": 0.8662606477737427, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.10122998605329067, "std_of_first_answer": 0.10122998605329067, "avg_entropy_of_first_answer": 0.7788036931306124, "lex_sim": 1.0}
{"question": "who wrote the first declaration of human rights", "answer": ["Rene Descartes"], "scores": [[0.018604038283228874, 0.8837065100669861, 0.6140897870063782, 0.9737590551376343, 0.9983150959014893]], "normalized_score": [0.3966187834739685], "tokens": [["\u2581R", "ene", "\u2581Des", "cart", "es"]], "entropy": [[4.956755638122559, 0.7956154346466064, 1.7536826133728027, 0.21861712634563446, 0.017817016690969467]], "avg_entropy": [1.5484975658357143], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cyrus", "score_of_biggest_cluster": 0.3966187834739685, "score_of_first_answer": 0.3966187834739685, "max_score_of_first_answer": 0.9983150959014893, "min_score_of_first_answer": 0.018604038283228874, "avg_score": 0.3966187834739685, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36587933869004435, "std_of_first_answer": 0.36587933869004435, "avg_entropy_of_first_answer": 1.5484975658357143, "lex_sim": 1.0}
{"question": "turkish finnish and hungarian belong to which family of languages", "answer": ["Turkic"], "scores": [[0.04011835530400276, 0.990273654460907]], "normalized_score": [0.19931921362876892], "tokens": [["\u2581Turk", "ic"]], "entropy": [[2.7430262565612793, 0.0931076854467392]], "avg_entropy": [1.4180669710040092], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Uralic languages", "score_of_biggest_cluster": 0.19931921362876892, "score_of_first_answer": 0.19931921362876892, "max_score_of_first_answer": 0.990273654460907, "min_score_of_first_answer": 0.04011835530400276, "avg_score": 0.19931921362876892, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4750776495784521, "std_of_first_answer": 0.4750776495784521, "avg_entropy_of_first_answer": 1.4180669710040092, "lex_sim": 1.0}
{"question": "which is the world's largest company in terms of revenue", "answer": ["Apple"], "scores": [[0.16200745105743408]], "normalized_score": [0.16200745105743408], "tokens": [["\u2581Apple"]], "entropy": [[2.0593762397766113]], "avg_entropy": [2.0593762397766113], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Walmart", "score_of_biggest_cluster": 0.16200745105743408, "score_of_first_answer": 0.16200745105743408, "max_score_of_first_answer": 0.16200745105743408, "min_score_of_first_answer": 0.16200745105743408, "avg_score": 0.16200745105743408, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.0593762397766113, "lex_sim": 1.0}
{"question": "chris cornell she's going to change the world", "answer": ["chris cornell"], "scores": [[0.006101727019995451, 0.9326927661895752, 0.9228261709213257, 0.9963452219963074]], "normalized_score": [0.26895540952682495], "tokens": [["\u2581ch", "ris", "\u2581cor", "nell"]], "entropy": [[5.8908514976501465, 0.5348163843154907, 0.6670291423797607, 0.03032136894762516]], "avg_entropy": [1.7807545983232558], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Can't Change Me", "score_of_biggest_cluster": 0.26895540952682495, "score_of_first_answer": 0.26895540952682495, "max_score_of_first_answer": 0.9963452219963074, "min_score_of_first_answer": 0.006101727019995451, "avg_score": 0.26895540952682495, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4099611929013714, "std_of_first_answer": 0.4099611929013714, "avg_entropy_of_first_answer": 1.7807545983232558, "lex_sim": 1.0}
{"question": "how long is the bridge between new brunswick and prince edward island", "answer": ["8 km"], "scores": [[0.84769207239151, 0.2602684497833252, 0.11571928858757019]], "normalized_score": [0.29445695877075195], "tokens": [["\u2581", "8", "\u2581km"]], "entropy": [[1.1990108489990234, 1.7824633121490479, 1.7343597412109375]], "avg_entropy": [1.5719446341196697], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "12.9-kilometre", "score_of_biggest_cluster": 0.29445695877075195, "score_of_first_answer": 0.29445695877075195, "max_score_of_first_answer": 0.84769207239151, "min_score_of_first_answer": 0.11571928858757019, "avg_score": 0.29445695877075195, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31653420538882415, "std_of_first_answer": 0.31653420538882415, "avg_entropy_of_first_answer": 1.5719446341196697, "lex_sim": 1.0}
{"question": "which body system differentiates a male from a female", "answer": ["Reproductive system"], "scores": [[0.29598549008369446, 0.9672858715057373, 0.9994582533836365, 0.16576044261455536]], "normalized_score": [0.4666784107685089], "tokens": [["\u2581Re", "product", "ive", "\u2581system"]], "entropy": [[3.095263719558716, 0.15671852231025696, 0.005932823289185762, 1.161407470703125]], "avg_entropy": [1.1048306339653209], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the external genitalia", "score_of_biggest_cluster": 0.4666784107685089, "score_of_first_answer": 0.4666784107685089, "max_score_of_first_answer": 0.9994582533836365, "min_score_of_first_answer": 0.16576044261455536, "avg_score": 0.4666784107685089, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.379226746879969, "std_of_first_answer": 0.379226746879969, "avg_entropy_of_first_answer": 1.1048306339653209, "lex_sim": 1.0}
{"question": "what makes a mini bike a mini bike", "answer": ["Minimalist motorized bicycle"], "scores": [[0.00882614217698574, 0.1496378779411316, 0.13721755146980286, 0.04027901217341423, 0.052379392087459564, 0.46481838822364807, 0.978877604007721, 0.7455838322639465]], "normalized_score": [0.13775938749313354], "tokens": [["\u2581Min", "imal", "ist", "\u2581motor", "ized", "\u2581b", "icy", "cle"]], "entropy": [[4.658965110778809, 1.6189725399017334, 4.786211013793945, 4.228903770446777, 1.5157527923583984, 2.415428638458252, 0.11214476078748703, 0.5753225684165955]], "avg_entropy": [2.4889626493677497], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "a miniature motorcycle", "score_of_biggest_cluster": 0.13775938749313354, "score_of_first_answer": 0.13775938749313354, "max_score_of_first_answer": 0.978877604007721, "min_score_of_first_answer": 0.00882614217698574, "avg_score": 0.13775938749313354, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.34374796735222807, "std_of_first_answer": 0.34374796735222807, "avg_entropy_of_first_answer": 2.4889626493677497, "lex_sim": 1.0}
{"question": "who is often associated with printing the first book using moveable type in germany", "answer": ["Gutenberg"], "scores": [[0.22707024216651917, 0.9964522123336792, 0.9104012250900269]], "normalized_score": [0.5905860066413879], "tokens": [["\u2581G", "uten", "berg"]], "entropy": [[1.5654219388961792, 0.03791624680161476, 0.35215097665786743]], "avg_entropy": [0.6518297207852205], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Johannes Gutenberg", "score_of_biggest_cluster": 0.5905860066413879, "score_of_first_answer": 0.5905860066413879, "max_score_of_first_answer": 0.9964522123336792, "min_score_of_first_answer": 0.22707024216651917, "avg_score": 0.5905860066413879, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.34420514242512656, "std_of_first_answer": 0.34420514242512656, "avg_entropy_of_first_answer": 0.6518297207852205, "lex_sim": 1.0}
{"question": "avatar the last airbender season 3 episode 12", "answer": ["The Chase"], "scores": [[0.5076432228088379, 0.024712948128581047, 0.47268712520599365]], "normalized_score": [0.18100294470787048], "tokens": [["\u2581The", "\u2581Ch", "ase"]], "entropy": [[3.2287650108337402, 4.095925331115723, 2.368077516555786]], "avg_entropy": [3.2309226195017495], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Western Air Temple", "score_of_biggest_cluster": 0.18100294470787048, "score_of_first_answer": 0.18100294470787048, "max_score_of_first_answer": 0.5076432228088379, "min_score_of_first_answer": 0.024712948128581047, "avg_score": 0.18100294470787048, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2198798768748652, "std_of_first_answer": 0.2198798768748652, "avg_entropy_of_first_answer": 3.2309226195017495, "lex_sim": 1.0}
{"question": "who played bat masterson in the tv series", "answer": ["Gene Barry"], "scores": [[0.16035135090351105, 0.9334381818771362]], "normalized_score": [0.386882483959198], "tokens": [["\u2581Gene", "\u2581Barry"]], "entropy": [[4.256406784057617, 0.4300256073474884]], "avg_entropy": [2.343216195702553], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gene Barry", "score_of_biggest_cluster": 0.386882483959198, "score_of_first_answer": 0.386882483959198, "max_score_of_first_answer": 0.9334381818771362, "min_score_of_first_answer": 0.16035135090351105, "avg_score": 0.386882483959198, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3865434154868126, "std_of_first_answer": 0.3865434154868126, "avg_entropy_of_first_answer": 2.343216195702553, "lex_sim": 1.0}
{"question": "who plays bongos on sympathy for the devil", "answer": ["Keith Richards FANDOM: The latest in news and information on the band and their work RESTORE: The current version of the wiki is shown."], "scores": [[0.13184452056884766, 0.5250460505485535, 0.9997712969779968, 1.3062202697255998e-06, 0.0013619988458231091, 0.11079420149326324, 0.020805973559617996, 0.13707809150218964, 0.00010985542758135125, 0.043705761432647705, 0.07601039856672287, 0.017672089859843254, 0.0779711902141571, 0.15862856805324554, 0.07775606960058212, 0.04922214522957802, 0.19751466810703278, 0.22219525277614594, 0.01259289775043726, 1.942327344295336e-06, 0.02840440906584263, 0.25489768385887146, 0.2087823450565338, 0.12185294181108475, 0.0007788709481246769, 0.20773498713970184, 0.887786328792572, 0.47120943665504456, 0.1806018203496933, 0.9605534076690674, 0.13207095861434937, 0.0054460386745631695, 0.20107939839363098, 0.042258553206920624, 0.007090444676578045, 0.9025847315788269, 0.019311783835291862, 0.008085894398391247, 0.47363579273223877, 0.9151738286018372, 0.025396225973963737, 0.9990122318267822, 0.48448002338409424, 0.005249119363725185, 0.6298210620880127, 0.04879193380475044, 0.825175940990448]], "normalized_score": [0.05055572837591171], "tokens": [["\u2581Keith", "\u2581Rich", "ards", "\u2581F", "AN", "DOM", ":", "\u2581The", "\u2581latest", "\u2581in", "\u2581news", "\u2581and", "\u2581information", "\u2581on", "\u2581the", "\u2581band", "\u2581and", "\u2581their", "\u2581work", "\u2581REST", "O", "RE", ":", "\u2581The", "\u2581current", "\u2581version", "\u2581of", "\u2581the", "\u2581w", "iki", "\u2581is", "\u2581shown", ".", "\u2581If", "\u2581necessary", ",", "\u2581this", "\u2581tab", "\u2581can", "\u2581be", "\u2581refresh", "ed", "\u2581to", "\u2581re", "-", "show", "\u2581the"]], "entropy": [[2.640223503112793, 1.1174464225769043, 0.0026887347921729088, 0.23075179755687714, 4.261514186859131, 3.3719444274902344, 3.16554594039917, 5.887318134307861, 3.553034782409668, 4.117400169372559, 3.8548107147216797, 0.33712446689605713, 3.9090893268585205, 2.544602394104004, 3.5510451793670654, 4.647584915161133, 2.8240182399749756, 2.6241648197174072, 4.101734161376953, 2.299464464187622, 2.099271059036255, 0.9449964761734009, 4.4980268478393555, 5.755146026611328, 5.6635894775390625, 5.028883934020996, 0.7602654099464417, 2.3739781379699707, 3.5958962440490723, 0.18164831399917603, 4.477688312530518, 4.77764892578125, 3.618208885192871, 5.075643539428711, 2.0237767696380615, 0.6935746669769287, 3.449676513671875, 3.8043837547302246, 1.8869813680648804, 0.5205771923065186, 2.754647731781006, 0.010590114630758762, 2.0973761081695557, 3.3860511779785156, 1.58298659324646, 3.453390121459961, 1.1367864608764648]], "avg_entropy": [2.8658127009551575], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rocky Dzidzornu", "score_of_biggest_cluster": 0.05055572837591171, "score_of_first_answer": 0.05055572837591171, "max_score_of_first_answer": 0.9997712969779968, "min_score_of_first_answer": 1.3062202697255998e-06, "avg_score": 0.05055572837591171, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3204587009693215, "std_of_first_answer": 0.3204587009693215, "avg_entropy_of_first_answer": 2.8658127009551575, "lex_sim": 1.0}
{"question": "where is geothermal energy used the most in the us", "answer": ["California"], "scores": [[0.25502660870552063]], "normalized_score": [0.25502660870552063], "tokens": [["\u2581California"]], "entropy": [[4.20616340637207]], "avg_entropy": [4.20616340637207], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "California", "score_of_biggest_cluster": 0.25502660870552063, "score_of_first_answer": 0.25502660870552063, "max_score_of_first_answer": 0.25502660870552063, "min_score_of_first_answer": 0.25502660870552063, "avg_score": 0.25502660870552063, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.20616340637207, "lex_sim": 1.0}
{"question": "where do they film young and the restless", "answer": ["Hollywood, California"], "scores": [[0.02273615263402462, 0.3009279668331146, 0.4884839653968811]], "normalized_score": [0.14951220154762268], "tokens": [["\u2581Hollywood", ",", "\u2581California"]], "entropy": [[3.5675153732299805, 1.3175773620605469, 1.2258130311965942]], "avg_entropy": [2.0369685888290405], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "CBS Television City", "score_of_biggest_cluster": 0.14951220154762268, "score_of_first_answer": 0.14951220154762268, "max_score_of_first_answer": 0.4884839653968811, "min_score_of_first_answer": 0.02273615263402462, "avg_score": 0.14951220154762268, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.19133709728528206, "std_of_first_answer": 0.19133709728528206, "avg_entropy_of_first_answer": 2.0369685888290405, "lex_sim": 1.0}
{"question": "what is the current rate of interest on ppf", "answer": ["5%"], "scores": [[0.8941298127174377, 0.007672487758100033, 0.17685069143772125]], "normalized_score": [0.10665498673915863], "tokens": [["\u2581", "5", "%"]], "entropy": [[0.9014747142791748, 0.9126448631286621, 1.6519323587417603]], "avg_entropy": [1.155350645383199], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "7.6% Per Annum", "score_of_biggest_cluster": 0.10665498673915863, "score_of_first_answer": 0.10665498673915863, "max_score_of_first_answer": 0.8941298127174377, "min_score_of_first_answer": 0.007672487758100033, "avg_score": 0.10665498673915863, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3842622356033746, "std_of_first_answer": 0.3842622356033746, "avg_entropy_of_first_answer": 1.155350645383199, "lex_sim": 1.0}
{"question": "the general term for software that is designed to damage disable or steal data is", "answer": ["virus"], "scores": [[0.017759261652827263]], "normalized_score": [0.017759263515472412], "tokens": [["\u2581virus"]], "entropy": [[2.349661350250244]], "avg_entropy": [2.349661350250244], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Malware", "score_of_biggest_cluster": 0.017759263515472412, "score_of_first_answer": 0.017759263515472412, "max_score_of_first_answer": 0.017759261652827263, "min_score_of_first_answer": 0.017759261652827263, "avg_score": 0.017759263515472412, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.349661350250244, "lex_sim": 1.0}
{"question": "where does the un security council get troops for military actions", "answer": ["UN peacekeeping missions"], "scores": [[0.08721719682216644, 0.11804667115211487, 0.9398619532585144, 0.8303368091583252, 0.3022823631763458, 0.9993578791618347]], "normalized_score": [0.36659345030784607], "tokens": [["\u2581UN", "\u2581peace", "keep", "ing", "\u2581miss", "ions"]], "entropy": [[4.885611534118652, 2.852511405944824, 0.34370318055152893, 0.46514973044395447, 2.171938419342041, 0.0067771910689771175]], "avg_entropy": [1.7876152435783297], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "member states of the UN", "score_of_biggest_cluster": 0.36659345030784607, "score_of_first_answer": 0.36659345030784607, "max_score_of_first_answer": 0.9993578791618347, "min_score_of_first_answer": 0.08721719682216644, "avg_score": 0.36659345030784607, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3861199774263379, "std_of_first_answer": 0.3861199774263379, "avg_entropy_of_first_answer": 1.7876152435783297, "lex_sim": 1.0}
{"question": "who sings in walk hard the dewey cox story", "answer": ["Jason Schwartzmann"], "scores": [[0.03911397606134415, 0.8388880491256714, 0.9992856383323669, 0.9937925934791565, 0.020250871777534485]], "normalized_score": [0.23114953935146332], "tokens": [["\u2581Jason", "\u2581Schw", "art", "z", "mann"]], "entropy": [[3.991426706314087, 1.0434459447860718, 0.007650078274309635, 0.04923687130212784, 0.12414509057998657]], "avg_entropy": [1.0431809382513166], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John C. Reilly", "score_of_biggest_cluster": 0.23114953935146332, "score_of_first_answer": 0.23114953935146332, "max_score_of_first_answer": 0.9992856383323669, "min_score_of_first_answer": 0.020250871777534485, "avg_score": 0.23114953935146332, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45164356000775696, "std_of_first_answer": 0.45164356000775696, "avg_entropy_of_first_answer": 1.0431809382513166, "lex_sim": 1.0}
{"question": "the cast of a good day to die hard", "answer": ["Bruce Willis, Morgan Freeman, Mary Elizabeth Winstead, Kevin Smith, Cole Hauser, John Leguizamo"], "scores": [[0.8082493543624878, 0.9302071332931519, 0.997128427028656, 0.8726531863212585, 0.016593439504504204, 0.987911581993103, 0.9986531734466553, 0.895332932472229, 0.1241941824555397, 0.7630547881126404, 0.9647229909896851, 0.9983188509941101, 0.9945743680000305, 0.7457882761955261, 0.03286072239279747, 0.20543821156024933, 0.7395275831222534, 0.060285598039627075, 0.9947410821914673, 0.9992497563362122, 0.7416073679924011, 0.2693752646446228, 0.012011883780360222, 0.9983847141265869, 0.9922382235527039, 0.998988687992096]], "normalized_score": [0.43744003772735596], "tokens": [["\u2581Bruce", "\u2581Will", "is", ",", "\u2581Morgan", "\u2581Fre", "eman", ",", "\u2581Mary", "\u2581Elizabeth", "\u2581W", "inst", "ead", ",", "\u2581Kevin", "\u2581Smith", ",", "\u2581Cole", "\u2581Haus", "er", ",", "\u2581John", "\u2581Leg", "u", "iz", "amo"]], "entropy": [[1.4902487993240356, 0.5127023458480835, 0.02479497343301773, 0.6247930526733398, 2.802736520767212, 0.10474319010972977, 0.013202637434005737, 0.4401935636997223, 3.2449169158935547, 0.7645007371902466, 0.2832108438014984, 0.01859099045395851, 0.03743938356637955, 0.7540571093559265, 3.4095120429992676, 1.6474920511245728, 0.7880544662475586, 4.157299995422363, 0.05866313353180885, 0.008375846780836582, 0.7445833086967468, 3.835533380508423, 2.410383701324463, 0.01666928082704544, 0.06300538033246994, 0.010465559549629688]], "avg_entropy": [1.0871603542652268], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mary Elizabeth Winstead", "score_of_biggest_cluster": 0.43744003772735596, "score_of_first_answer": 0.43744003772735596, "max_score_of_first_answer": 0.9992497563362122, "min_score_of_first_answer": 0.012011883780360222, "avg_score": 0.43744003772735596, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3744046261810298, "std_of_first_answer": 0.3744046261810298, "avg_entropy_of_first_answer": 1.0871603542652268, "lex_sim": 1.0}
{"question": "a good that can be used in place of another good", "answer": ["substitute"], "scores": [[0.17465338110923767]], "normalized_score": [0.17465338110923767], "tokens": [["\u2581substitute"]], "entropy": [[4.238280296325684]], "avg_entropy": [4.238280296325684], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A substitute good", "score_of_biggest_cluster": 0.17465338110923767, "score_of_first_answer": 0.17465338110923767, "max_score_of_first_answer": 0.17465338110923767, "min_score_of_first_answer": 0.17465338110923767, "avg_score": 0.17465338110923767, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.238280296325684, "lex_sim": 1.0}
{"question": "who is the young man in hawthorne's my kinsman major molineux", "answer": ["Ralph"], "scores": [[0.035299208015203476]], "normalized_score": [0.035299208015203476], "tokens": [["\u2581Ralph"]], "entropy": [[5.1711249351501465]], "avg_entropy": [5.1711249351501465], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Robin", "score_of_biggest_cluster": 0.035299208015203476, "score_of_first_answer": 0.035299208015203476, "max_score_of_first_answer": 0.035299208015203476, "min_score_of_first_answer": 0.035299208015203476, "avg_score": 0.035299208015203476, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.1711249351501465, "lex_sim": 1.0}
{"question": "why the hole in the ozone layer over antarctica", "answer": ["CFC"], "scores": [[0.048047106713056564, 0.9437683820724487]], "normalized_score": [0.21294444799423218], "tokens": [["\u2581C", "FC"]], "entropy": [[5.27726936340332, 0.33004623651504517]], "avg_entropy": [2.8036577999591827], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "bromine", "score_of_biggest_cluster": 0.21294444799423218, "score_of_first_answer": 0.21294444799423218, "max_score_of_first_answer": 0.9437683820724487, "min_score_of_first_answer": 0.048047106713056564, "avg_score": 0.21294444799423218, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4478606376796961, "std_of_first_answer": 0.4478606376796961, "avg_entropy_of_first_answer": 2.8036577999591827, "lex_sim": 1.0}
{"question": "who proposed the first accurate model of dna", "answer": ["James D. Watson"], "scores": [[0.3782941997051239, 0.06424158811569214, 0.858012855052948, 0.9983695149421692]], "normalized_score": [0.3798461854457855], "tokens": [["\u2581James", "\u2581D", ".", "\u2581Watson"]], "entropy": [[2.9330899715423584, 1.0578113794326782, 0.7679865956306458, 0.01820826157927513]], "avg_entropy": [1.1942740520462394], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Watson", "score_of_biggest_cluster": 0.3798461854457855, "score_of_first_answer": 0.3798461854457855, "max_score_of_first_answer": 0.9983695149421692, "min_score_of_first_answer": 0.06424158811569214, "avg_score": 0.3798461854457855, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3737997648782663, "std_of_first_answer": 0.3737997648782663, "avg_entropy_of_first_answer": 1.1942740520462394, "lex_sim": 1.0}
{"question": "where did the last name edwards come from", "answer": ["George"], "scores": [[0.0024082616437226534]], "normalized_score": [0.0024082616437226534], "tokens": [["\u2581George"]], "entropy": [[5.449406147003174]], "avg_entropy": [5.449406147003174], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "England", "score_of_biggest_cluster": 0.0024082616437226534, "score_of_first_answer": 0.0024082616437226534, "max_score_of_first_answer": 0.0024082616437226534, "min_score_of_first_answer": 0.0024082616437226534, "avg_score": 0.0024082616437226534, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.449406147003174, "lex_sim": 1.0}
{"question": "what was the name of atom bomb dropped by usa on hiroshima", "answer": ["Little Boy"], "scores": [[0.766365647315979, 0.9618972539901733]], "normalized_score": [0.8585831522941589], "tokens": [["\u2581Little", "\u2581Boy"]], "entropy": [[1.5215158462524414, 0.19383969902992249]], "avg_entropy": [0.857677772641182], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Little Boy", "score_of_biggest_cluster": 0.8585831522941589, "score_of_first_answer": 0.8585831522941589, "max_score_of_first_answer": 0.9618972539901733, "min_score_of_first_answer": 0.766365647315979, "avg_score": 0.8585831522941589, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.09776580333709717, "std_of_first_answer": 0.09776580333709717, "avg_entropy_of_first_answer": 0.857677772641182, "lex_sim": 1.0}
{"question": "who is known as the father of indian constitution", "answer": ["Dr. B.R."], "scores": [[0.31109949946403503, 0.798797607421875, 0.5897947549819946, 0.769599974155426, 0.48875802755355835, 0.881411075592041, 0.9401904940605164, 0.9957191348075867, 0.9966261386871338]], "normalized_score": [0.7091166973114014], "tokens": [["\u2581Dr", ".", "\u2581B", ".", "R", ".", "\u2581Am", "bed", "kar"]], "entropy": [[1.8539106845855713, 0.82309889793396, 1.4992235898971558, 0.8134307861328125, 0.8135323524475098, 0.47047701478004456, 0.31235945224761963, 0.035775765776634216, 0.029929377138614655]], "avg_entropy": [0.739081991215547], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bhimrao Ramji Ambedkar", "score_of_biggest_cluster": 0.7091166973114014, "score_of_first_answer": 0.7091166973114014, "max_score_of_first_answer": 0.9966261386871338, "min_score_of_first_answer": 0.31109949946403503, "avg_score": 0.7091166973114014, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2270345010936863, "std_of_first_answer": 0.2270345010936863, "avg_entropy_of_first_answer": 0.739081991215547, "lex_sim": 1.0}
{"question": "a 58-story skyscraper in san francisco is tilting and sinking", "answer": ["Salesforce Tower"], "scores": [[0.1117934063076973, 0.9753404855728149, 0.8859807252883911]], "normalized_score": [0.45884451270103455], "tokens": [["\u2581Sales", "force", "\u2581Tower"]], "entropy": [[2.5095672607421875, 0.19596485793590546, 0.5342134833335876]], "avg_entropy": [1.0799152006705601], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Millennium Tower", "score_of_biggest_cluster": 0.45884451270103455, "score_of_first_answer": 0.45884451270103455, "max_score_of_first_answer": 0.9753404855728149, "min_score_of_first_answer": 0.1117934063076973, "avg_score": 0.45884451270103455, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3877377019664367, "std_of_first_answer": 0.3877377019664367, "avg_entropy_of_first_answer": 1.0799152006705601, "lex_sim": 1.0}
{"question": "when was the dome of the rock completed", "answer": ["September 7, 691"], "scores": [[0.002200411632657051, 0.9307925701141357, 0.026566434651613235, 0.543753445148468, 0.9864481687545776, 0.9519640207290649, 0.9628016948699951, 0.7815306782722473]], "normalized_score": [0.2600361704826355], "tokens": [["\u2581September", "\u2581", "7", ",", "\u2581", "6", "9", "1"]], "entropy": [[1.5473551750183105, 0.35292118787765503, 1.4718581438064575, 1.4534637928009033, 0.10277493298053741, 0.27152639627456665, 0.20376013219356537, 0.6492670774459839]], "avg_entropy": [0.7566158547997475], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "691 CE", "score_of_biggest_cluster": 0.2600361704826355, "score_of_first_answer": 0.2600361704826355, "max_score_of_first_answer": 0.9864481687545776, "min_score_of_first_answer": 0.002200411632657051, "avg_score": 0.2600361704826355, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.39018378285541694, "std_of_first_answer": 0.39018378285541694, "avg_entropy_of_first_answer": 0.7566158547997475, "lex_sim": 1.0}
{"question": "how long do nhl players stay on waivers", "answer": ["10 days"], "scores": [[0.433390349149704, 0.7220820188522339, 0.9650530815124512, 0.9201111197471619]], "normalized_score": [0.7260463237762451], "tokens": [["\u2581", "1", "0", "\u2581days"]], "entropy": [[3.091426372528076, 0.9064154028892517, 0.23860403895378113, 0.4998631477355957]], "avg_entropy": [1.1840772405266762], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "24 hours", "score_of_biggest_cluster": 0.7260463237762451, "score_of_first_answer": 0.7260463237762451, "max_score_of_first_answer": 0.9650530815124512, "min_score_of_first_answer": 0.433390349149704, "avg_score": 0.7260463237762451, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20963889961205337, "std_of_first_answer": 0.20963889961205337, "avg_entropy_of_first_answer": 1.1840772405266762, "lex_sim": 1.0}
{"question": "where can carbon be found in the biosphere", "answer": ["Atmosphere."], "scores": [[0.03808170557022095, 0.9484841227531433, 0.9839361906051636, 0.004205123987048864]], "normalized_score": [0.11056636273860931], "tokens": [["\u2581At", "mos", "phere", "."]], "entropy": [[4.719715595245361, 0.2803667187690735, 0.09336955845355988, 0.6694663763046265]], "avg_entropy": [1.4407295621931553], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "other living organisms", "score_of_biggest_cluster": 0.11056636273860931, "score_of_first_answer": 0.11056636273860931, "max_score_of_first_answer": 0.9839361906051636, "min_score_of_first_answer": 0.004205123987048864, "avg_score": 0.11056636273860931, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4728512933771658, "std_of_first_answer": 0.4728512933771658, "avg_entropy_of_first_answer": 1.4407295621931553, "lex_sim": 1.0}
{"question": "who won first ancient olympic 200 yard race", "answer": ["Pheidippides The AI research assistant is a neural network which has been trained using the following dataset:"], "scores": [[0.09371069073677063, 0.7339478731155396, 0.9603185057640076, 0.9388234615325928, 2.6630232241586782e-05, 0.10705944895744324, 0.0002664652420207858, 0.36683008074760437, 0.9704024195671082, 0.1774372160434723, 0.15365783870220184, 0.00238439510576427, 0.8695569634437561, 0.0182939600199461, 0.10728196054697037, 0.6123960614204407, 0.5896150469779968, 0.14200937747955322, 0.11203984171152115, 0.01628381386399269, 0.06710384041070938, 0.5992152690887451]], "normalized_score": [0.08861348778009415], "tokens": [["\u2581P", "heid", "ipp", "ides", "\u2581The", "\u2581A", "I", "\u2581research", "\u2581assistant", "\u2581is", "\u2581a", "\u2581neural", "\u2581network", "\u2581which", "\u2581has", "\u2581been", "\u2581trained", "\u2581using", "\u2581the", "\u2581following", "\u2581dataset", ":"]], "entropy": [[4.892037391662598, 1.2455604076385498, 0.28890782594680786, 0.30100375413894653, 0.4977254867553711, 4.8897600173950195, 0.07409428060054779, 3.6511013507843018, 0.24054473638534546, 4.366799354553223, 5.0891432762146, 6.192422389984131, 0.7623252868652344, 4.061995983123779, 3.9303579330444336, 2.478929042816162, 2.5525479316711426, 2.001492500305176, 4.706206321716309, 6.108000755310059, 4.335434913635254, 1.7711052894592285]], "avg_entropy": [2.928977101363919], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Coroebus of Elis", "score_of_biggest_cluster": 0.08861348778009415, "score_of_first_answer": 0.08861348778009415, "max_score_of_first_answer": 0.9704024195671082, "min_score_of_first_answer": 2.6630232241586782e-05, "avg_score": 0.08861348778009415, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3521899452376419, "std_of_first_answer": 0.3521899452376419, "avg_entropy_of_first_answer": 2.928977101363919, "lex_sim": 1.0}
{"question": "when did dragon ball super tournament of power start", "answer": ["September 17, 2017"], "scores": [[0.05846767872571945, 0.9574850797653198, 0.43371036648750305, 0.5728850364685059, 0.5622624158859253, 0.993584930896759, 0.9991750121116638, 0.999614953994751, 0.9975326061248779, 0.5705478191375732]], "normalized_score": [0.5814538598060608], "tokens": [["\u2581September", "\u2581", "1", "7", ",", "\u2581", "2", "0", "1", "7"]], "entropy": [[2.8081021308898926, 0.2328733652830124, 1.3248459100723267, 1.4035050868988037, 0.9031463861465454, 0.042191654443740845, 0.007050564978271723, 0.004453178495168686, 0.020416248589754105, 1.2137413024902344]], "avg_entropy": [0.7960325828287751], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "February 19, 2017", "score_of_biggest_cluster": 0.5814538598060608, "score_of_first_answer": 0.5814538598060608, "max_score_of_first_answer": 0.999614953994751, "min_score_of_first_answer": 0.05846767872571945, "avg_score": 0.5814538598060608, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30863287759837926, "std_of_first_answer": 0.30863287759837926, "avg_entropy_of_first_answer": 0.7960325828287751, "lex_sim": 1.0}
{"question": "what kind of animals live in the great victoria desert", "answer": ["There are only animals and birds such as birds, snakes, and ants in the Great Victoria Desert."], "scores": [[0.0064722588285803795, 0.8593568801879883, 0.02062939666211605, 0.018318554386496544, 0.04335751384496689, 0.052597057074308395, 0.009951419197022915, 0.9928452372550964, 0.010301154106855392, 0.31490615010261536, 0.06932712346315384, 0.983588695526123, 0.7762526273727417, 0.16033372282981873, 0.005971699487417936, 0.9413123726844788, 0.07077637314796448, 0.8081124424934387, 0.37195277214050293, 0.9694687724113464, 0.760803759098053, 0.9988077878952026, 0.6865794062614441, 0.0065507106482982635, 0.3017146587371826, 0.26689985394477844, 0.6458321809768677, 0.011757695116102695, 0.08626563847064972, 0.11759885400533676, 0.7867787480354309, 0.016836535185575485, 0.260444700717926, 0.04777570068836212, 0.9968197345733643, 0.9995642304420471, 0.053170714527368546, 0.14232517778873444, 0.015609820373356342, 0.38200101256370544, 0.4088110029697418, 0.49671468138694763, 0.005063471384346485, 0.8462656140327454, 0.997048556804657, 0.6538042426109314, 0.02405182644724846, 0.9561865329742432, 0.9879022836685181, 0.8662509918212891]], "normalized_score": [0.16161030530929565], "tokens": [["\u2581There", "\u2581are", "\u2581only", "\u2581animals", "\u2581and", "\u2581birds", "\u2581such", "\u2581as", "\u2581birds", ",", "\u2581sn", "akes", ",", "\u2581and", "\u2581an", "ts", "\u2581in", "\u2581the", "\u2581Great", "\u2581Victoria", "\u2581Des", "ert", ".", "\u2581In", "\u2581the", "\u2581desert", ",", "\u2581some", "\u2581of", "\u2581these", "\u2581animals", "\u2581may", "\u2581be", "\u2581end", "anger", "ed", "\u2581species", ",", "\u2581some", "\u2581of", "\u2581which", "\u2581are", "\u2581ko", "al", "as", ",", "\u2581wall", "ab", "ies", ","]], "entropy": [[4.8030524253845215, 0.5777044296264648, 3.5239381790161133, 3.786485195159912, 2.8530054092407227, 3.129720687866211, 2.7211978435516357, 0.07637961208820343, 4.290184020996094, 2.3023056983947754, 4.189647674560547, 0.09649567306041718, 0.7800672650337219, 3.739779472351074, 4.0380659103393555, 0.47848737239837646, 2.100120782852173, 0.9789236187934875, 2.1001086235046387, 0.21357253193855286, 0.8077393174171448, 0.01309251319617033, 1.149787187576294, 1.5522481203079224, 2.7372350692749023, 3.321507453918457, 1.7677831649780273, 2.978651523590088, 3.2690296173095703, 0.8489299416542053, 1.3103244304656982, 3.3512048721313477, 3.5636048316955566, 4.860738754272461, 0.028858589008450508, 0.005331847816705704, 2.289811372756958, 1.839322566986084, 2.754206895828247, 2.6148548126220703, 1.3523951768875122, 2.1302950382232666, 5.33708381652832, 0.510499119758606, 0.03274744004011154, 1.075836181640625, 4.1459856033325195, 0.24632933735847473, 0.09034157544374466, 0.48986750841140747]], "avg_entropy": [2.065097722131759], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "malleefowl", "score_of_biggest_cluster": 0.16161030530929565, "score_of_first_answer": 0.16161030530929565, "max_score_of_first_answer": 0.9995642304420471, "min_score_of_first_answer": 0.005063471384346485, "avg_score": 0.16161030530929565, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39142808081006153, "std_of_first_answer": 0.39142808081006153, "avg_entropy_of_first_answer": 2.065097722131759, "lex_sim": 1.0}
{"question": "when is season 7 race to the edge coming out", "answer": ["September 26, 2015."], "scores": [[0.04261566698551178, 0.8313679099082947, 0.6996102929115295, 0.1758798062801361, 0.35344207286834717, 0.9865673780441284, 0.9994227886199951, 0.9995630383491516, 0.9892064332962036, 0.759818434715271, 0.004587216768413782]], "normalized_score": [0.3310607671737671], "tokens": [["\u2581September", "\u2581", "2", "6", ",", "\u2581", "2", "0", "1", "5", "."]], "entropy": [[3.734529495239258, 0.5928308963775635, 1.010117769241333, 1.9570859670639038, 1.1613653898239136, 0.08113234490156174, 0.0058680130168795586, 0.005315080285072327, 0.06961914896965027, 0.8810731172561646, 0.3963903784751892]], "avg_entropy": [0.8995752364227717], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "February 16, 2018", "score_of_biggest_cluster": 0.3310607671737671, "score_of_first_answer": 0.3310607671737671, "max_score_of_first_answer": 0.9995630383491516, "min_score_of_first_answer": 0.004587216768413782, "avg_score": 0.3310607671737671, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38253167241550856, "std_of_first_answer": 0.38253167241550856, "avg_entropy_of_first_answer": 0.8995752364227717, "lex_sim": 1.0}
{"question": "the most common form of megalithic architecture in europe is", "answer": ["dolmen"], "scores": [[0.4212678074836731, 0.641936719417572]], "normalized_score": [0.5200262069702148], "tokens": [["\u2581dol", "men"]], "entropy": [[3.3219833374023438, 0.7553147077560425]], "avg_entropy": [2.038649022579193], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the portal tomb", "score_of_biggest_cluster": 0.5200262069702148, "score_of_first_answer": 0.5200262069702148, "max_score_of_first_answer": 0.641936719417572, "min_score_of_first_answer": 0.4212678074836731, "avg_score": 0.5200262069702148, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.11033445596694946, "std_of_first_answer": 0.11033445596694946, "avg_entropy_of_first_answer": 2.038649022579193, "lex_sim": 1.0}
{"question": "when does sloan come into grey's anatomy", "answer": ["Series 1"], "scores": [[0.0017047799192368984, 0.9241132736206055, 0.38214296102523804]], "normalized_score": [0.08443837612867355], "tokens": [["\u2581Series", "\u2581", "1"]], "entropy": [[3.2403953075408936, 0.5559177994728088, 1.9553122520446777]], "avg_entropy": [1.91720845301946], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "season two", "score_of_biggest_cluster": 0.08443837612867355, "score_of_first_answer": 0.08443837612867355, "max_score_of_first_answer": 0.9241132736206055, "min_score_of_first_answer": 0.0017047799192368984, "avg_score": 0.08443837612867355, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37849151739069403, "std_of_first_answer": 0.37849151739069403, "avg_entropy_of_first_answer": 1.91720845301946, "lex_sim": 1.0}
{"question": "iupac name of element with atomic number 107", "answer": ["UUUUMMMAA"], "scores": [[0.025210116058588028, 0.03238215297460556, 0.3514113426208496, 0.030355006456375122, 0.30051517486572266, 0.011533237993717194, 0.21292708814144135]], "normalized_score": [0.06756327301263809], "tokens": [["\u2581U", "U", "U", "UM", "M", "MA", "A"]], "entropy": [[3.435288906097412, 3.6952714920043945, 3.1698379516601562, 2.7592697143554688, 2.3862686157226562, 2.862497329711914, 3.0091958045959473]], "avg_entropy": [3.0453756877354214], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bohrium", "score_of_biggest_cluster": 0.06756327301263809, "score_of_first_answer": 0.06756327301263809, "max_score_of_first_answer": 0.3514113426208496, "min_score_of_first_answer": 0.011533237993717194, "avg_score": 0.06756327301263809, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13576620101382164, "std_of_first_answer": 0.13576620101382164, "avg_entropy_of_first_answer": 3.0453756877354214, "lex_sim": 1.0}
{"question": "the group that officially elects the president of the united states is called", "answer": ["electoral college"], "scores": [[0.3171136677265167, 0.9771143794059753, 0.9588999152183533]], "normalized_score": [0.6672847867012024], "tokens": [["\u2581elect", "oral", "\u2581college"]], "entropy": [[2.1765975952148438, 0.12748822569847107, 0.26189202070236206]], "avg_entropy": [0.8553259472052256], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the U.S. Electoral College", "score_of_biggest_cluster": 0.6672847867012024, "score_of_first_answer": 0.6672847867012024, "max_score_of_first_answer": 0.9771143794059753, "min_score_of_first_answer": 0.3171136677265167, "avg_score": 0.6672847867012024, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.30692422039626344, "std_of_first_answer": 0.30692422039626344, "avg_entropy_of_first_answer": 0.8553259472052256, "lex_sim": 1.0}
{"question": "why did jean valjean take care of cosette", "answer": ["Cosette is his niece"], "scores": [[0.08027109503746033, 0.9840449094772339, 0.1643543541431427, 0.4793499708175659, 0.013018588535487652, 0.9983629584312439]], "normalized_score": [0.20795848965644836], "tokens": [["\u2581Cos", "ette", "\u2581is", "\u2581his", "\u2581nie", "ce"]], "entropy": [[3.606687545776367, 0.10460144281387329, 2.691561222076416, 2.4549269676208496, 1.518513798713684, 0.012881044298410416]], "avg_entropy": [1.7315286702166002], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "he had promised", "score_of_biggest_cluster": 0.20795848965644836, "score_of_first_answer": 0.20795848965644836, "max_score_of_first_answer": 0.9983629584312439, "min_score_of_first_answer": 0.013018588535487652, "avg_score": 0.20795848965644836, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4074207967263337, "std_of_first_answer": 0.4074207967263337, "avg_entropy_of_first_answer": 1.7315286702166002, "lex_sim": 1.0}
{"question": "where does the thames river begin and end", "answer": ["Source"], "scores": [[0.06384685635566711]], "normalized_score": [0.06384685635566711], "tokens": [["\u2581Source"]], "entropy": [[4.365749359130859]], "avg_entropy": [4.365749359130859], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lighthouse Cove", "score_of_biggest_cluster": 0.06384685635566711, "score_of_first_answer": 0.06384685635566711, "max_score_of_first_answer": 0.06384685635566711, "min_score_of_first_answer": 0.06384685635566711, "avg_score": 0.06384685635566711, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.365749359130859, "lex_sim": 1.0}
{"question": "when did rachel have her baby on friends", "answer": ["July 9th, 2003"], "scores": [[0.01754530519247055, 0.940617561340332, 0.022969750687479973, 0.3811263144016266, 0.5355740785598755, 0.9885169267654419, 0.5398645401000977, 0.9977426528930664, 0.8494240045547485, 0.16452133655548096]], "normalized_score": [0.29924386739730835], "tokens": [["\u2581July", "\u2581", "9", "th", ",", "\u2581", "2", "0", "0", "3"]], "entropy": [[3.414219856262207, 0.3173907399177551, 1.5257447957992554, 1.0417702198028564, 1.1482045650482178, 0.10603395104408264, 0.7025216817855835, 0.024095017462968826, 0.44007498025894165, 1.9821373224258423]], "avg_entropy": [1.070219312980771], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "May\u00a016,\u00a02002", "score_of_biggest_cluster": 0.29924386739730835, "score_of_first_answer": 0.29924386739730835, "max_score_of_first_answer": 0.9977426528930664, "min_score_of_first_answer": 0.01754530519247055, "avg_score": 0.29924386739730835, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3705824636742494, "std_of_first_answer": 0.3705824636742494, "avg_entropy_of_first_answer": 1.070219312980771, "lex_sim": 1.0}
{"question": "who says that which we call a rose", "answer": ["William Shakespeare"], "scores": [[0.5478127002716064, 0.8698772192001343]], "normalized_score": [0.6903113722801208], "tokens": [["\u2581William", "\u2581Shakespeare"]], "entropy": [[3.065265417098999, 0.8653088808059692]], "avg_entropy": [1.9652871489524841], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Juliet", "score_of_biggest_cluster": 0.6903113722801208, "score_of_first_answer": 0.6903113722801208, "max_score_of_first_answer": 0.8698772192001343, "min_score_of_first_answer": 0.5478127002716064, "avg_score": 0.6903113722801208, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.16103225946426392, "std_of_first_answer": 0.16103225946426392, "avg_entropy_of_first_answer": 1.9652871489524841, "lex_sim": 1.0}
{"question": "who won the mens single ice skating 2018", "answer": ["Yuzu"], "scores": [[0.7755197286605835, 0.9856825470924377, 0.023738687857985497]], "normalized_score": [0.26278194785118103], "tokens": [["\u2581Y", "uz", "u"]], "entropy": [[1.5305341482162476, 0.1057116761803627, 0.21612007915973663]], "avg_entropy": [0.617455301185449], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yuzuru Hanyu", "score_of_biggest_cluster": 0.26278194785118103, "score_of_first_answer": 0.26278194785118103, "max_score_of_first_answer": 0.9856825470924377, "min_score_of_first_answer": 0.023738687857985497, "avg_score": 0.26278194785118103, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4129405569113536, "std_of_first_answer": 0.4129405569113536, "avg_entropy_of_first_answer": 0.617455301185449, "lex_sim": 1.0}
{"question": "who started the guinness book of world records", "answer": ["Sir Hugh Dalton"], "scores": [[0.38033658266067505, 0.971227765083313, 5.778795457445085e-05, 0.9788311123847961]], "normalized_score": [0.06760963797569275], "tokens": [["\u2581Sir", "\u2581Hugh", "\u2581Dal", "ton"]], "entropy": [[2.49062442779541, 0.2564818561077118, 0.0917096883058548, 0.14124737679958344]], "avg_entropy": [0.74501583725214], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sir Hugh Beaver", "score_of_biggest_cluster": 0.06760963797569275, "score_of_first_answer": 0.06760963797569275, "max_score_of_first_answer": 0.9788311123847961, "min_score_of_first_answer": 5.778795457445085e-05, "avg_score": 0.06760963797569275, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41481819882783094, "std_of_first_answer": 0.41481819882783094, "avg_entropy_of_first_answer": 0.74501583725214, "lex_sim": 1.0}
{"question": "trick taking card game name derived from spanish for man", "answer": ["Bid"], "scores": [[0.014912876300513744, 0.38686075806617737]], "normalized_score": [0.07595530152320862], "tokens": [["\u2581B", "id"]], "entropy": [[5.864556789398193, 2.483391761779785]], "avg_entropy": [4.173974275588989], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Skat", "score_of_biggest_cluster": 0.07595530152320862, "score_of_first_answer": 0.07595530152320862, "max_score_of_first_answer": 0.38686075806617737, "min_score_of_first_answer": 0.014912876300513744, "avg_score": 0.07595530152320862, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1859739408828318, "std_of_first_answer": 0.1859739408828318, "avg_entropy_of_first_answer": 4.173974275588989, "lex_sim": 1.0}
{"question": "where did students for a democratic society start", "answer": ["DePaul University"], "scores": [[0.009952099062502384, 0.845497190952301, 0.9681311845779419]], "normalized_score": [0.20121188461780548], "tokens": [["\u2581De", "Paul", "\u2581University"]], "entropy": [[4.137120246887207, 0.6209959983825684, 0.22665540874004364]], "avg_entropy": [1.6615905513366063], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ann Arbor, Michigan", "score_of_biggest_cluster": 0.20121188461780548, "score_of_first_answer": 0.20121188461780548, "max_score_of_first_answer": 0.9681311845779419, "min_score_of_first_answer": 0.009952099062502384, "avg_score": 0.20121188461780548, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4257388162415635, "std_of_first_answer": 0.4257388162415635, "avg_entropy_of_first_answer": 1.6615905513366063, "lex_sim": 1.0}
{"question": "the first vice president of india who become the president letter was", "answer": ["V. V. Giri"], "scores": [[0.2940676212310791, 0.39345958828926086, 0.41728368401527405, 0.8926326036453247, 0.9754127264022827, 0.9973239898681641]], "normalized_score": [0.5894034504890442], "tokens": [["\u2581V", ".", "\u2581V", ".", "\u2581Gi", "ri"]], "entropy": [[3.826338529586792, 2.3184146881103516, 0.9985615015029907, 0.5474904179573059, 0.18918368220329285, 0.026678767055273056]], "avg_entropy": [1.3177779310693343], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sarvepalli Radhakrishnan", "score_of_biggest_cluster": 0.5894034504890442, "score_of_first_answer": 0.5894034504890442, "max_score_of_first_answer": 0.9973239898681641, "min_score_of_first_answer": 0.2940676212310791, "avg_score": 0.5894034504890442, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2975552678513478, "std_of_first_answer": 0.2975552678513478, "avg_entropy_of_first_answer": 1.3177779310693343, "lex_sim": 1.0}
{"question": "where do the secretory cells of endocrine glands secrete their products", "answer": ["ducts"], "scores": [[0.10429845750331879, 0.8555341958999634, 0.6660009622573853]], "normalized_score": [0.3902384340763092], "tokens": [["\u2581du", "ct", "s"]], "entropy": [[3.7512521743774414, 0.5389706492424011, 1.1147370338439941]], "avg_entropy": [1.8016532858212788], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "epithelial surface", "score_of_biggest_cluster": 0.3902384340763092, "score_of_first_answer": 0.3902384340763092, "max_score_of_first_answer": 0.8555341958999634, "min_score_of_first_answer": 0.10429845750331879, "avg_score": 0.3902384340763092, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31898932151168324, "std_of_first_answer": 0.31898932151168324, "avg_entropy_of_first_answer": 1.8016532858212788, "lex_sim": 1.0}
{"question": "when was penicillin first introduced to the public", "answer": ["September 28, 1942"], "scores": [[0.019728947430849075, 0.9631990194320679, 0.8478904962539673, 0.9398540258407593, 0.7650763988494873, 0.9891847372055054, 0.9995759129524231, 0.9997290968894958, 0.9025049805641174, 0.9833290576934814]], "normalized_score": [0.6319822669029236], "tokens": [["\u2581September", "\u2581", "2", "8", ",", "\u2581", "1", "9", "4", "2"]], "entropy": [[1.4409587383270264, 0.20483991503715515, 0.619671106338501, 0.32271987199783325, 0.6769222617149353, 0.06294948607683182, 0.003907748498022556, 0.0028374455869197845, 0.3506857752799988, 0.095085009932518]], "avg_entropy": [0.3780577358789742], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "After World War II", "score_of_biggest_cluster": 0.6319822669029236, "score_of_first_answer": 0.6319822669029236, "max_score_of_first_answer": 0.9997290968894958, "min_score_of_first_answer": 0.019728947430849075, "avg_score": 0.6319822669029236, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2830800448975689, "std_of_first_answer": 0.2830800448975689, "avg_entropy_of_first_answer": 0.3780577358789742, "lex_sim": 1.0}
{"question": "what is the common name for gravitational force", "answer": ["gravity"], "scores": [[0.19247731566429138]], "normalized_score": [0.19247731566429138], "tokens": [["\u2581gravity"]], "entropy": [[2.849759340286255]], "avg_entropy": [2.849759340286255], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gravity", "score_of_biggest_cluster": 0.19247731566429138, "score_of_first_answer": 0.19247731566429138, "max_score_of_first_answer": 0.19247731566429138, "min_score_of_first_answer": 0.19247731566429138, "avg_score": 0.19247731566429138, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.849759340286255, "lex_sim": 1.0}
{"question": "who was the first english child born in north america", "answer": ["Anne Baynton"], "scores": [[0.013965687714517117, 0.0021352036856114864, 0.45914146304130554, 0.9814891219139099]], "normalized_score": [0.06054571643471718], "tokens": [["\u2581Anne", "\u2581Bay", "nt", "on"]], "entropy": [[2.654808521270752, 2.3698649406433105, 1.9960821866989136, 0.10112720727920532]], "avg_entropy": [1.7804707139730453], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Virginia Dare", "score_of_biggest_cluster": 0.06054571643471718, "score_of_first_answer": 0.06054571643471718, "max_score_of_first_answer": 0.9814891219139099, "min_score_of_first_answer": 0.0021352036856114864, "avg_score": 0.06054571643471718, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4011903321666855, "std_of_first_answer": 0.4011903321666855, "avg_entropy_of_first_answer": 1.7804707139730453, "lex_sim": 1.0}
{"question": "architectural elements forming rib vaults eg wells cathedral", "answer": ["Arches"], "scores": [[0.032094862312078476, 0.6498456001281738]], "normalized_score": [0.14441850781440735], "tokens": [["\u2581Arch", "es"]], "entropy": [[5.181205749511719, 1.5139278173446655]], "avg_entropy": [3.347566783428192], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "an armature of piped masonry", "score_of_biggest_cluster": 0.14441850781440735, "score_of_first_answer": 0.14441850781440735, "max_score_of_first_answer": 0.6498456001281738, "min_score_of_first_answer": 0.032094862312078476, "avg_score": 0.14441850781440735, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3088753689080477, "std_of_first_answer": 0.3088753689080477, "avg_entropy_of_first_answer": 3.347566783428192, "lex_sim": 1.0}
{"question": "who wrote the song i hate you i love you", "answer": ["Barry Manillow"], "scores": [[0.003342857351526618, 0.3301731050014496, 0.0012706465786322951, 0.9473044276237488]], "normalized_score": [0.03395029157400131], "tokens": [["\u2581Barry", "\u2581Man", "ill", "ow"]], "entropy": [[5.650765419006348, 2.2010903358459473, 0.07014589756727219, 0.3822324573993683]], "avg_entropy": [2.076058527454734], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Garrett Nash", "score_of_biggest_cluster": 0.03395029157400131, "score_of_first_answer": 0.03395029157400131, "max_score_of_first_answer": 0.9473044276237488, "min_score_of_first_answer": 0.0012706465786322951, "avg_score": 0.03395029157400131, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38583461178203005, "std_of_first_answer": 0.38583461178203005, "avg_entropy_of_first_answer": 2.076058527454734, "lex_sim": 1.0}
{"question": "where do the florida marlins have spring training", "answer": ["........"], "scores": [[0.007202070206403732, 0.0033590863458812237]], "normalized_score": [0.004918575286865234], "tokens": [["\u2581", "........"]], "entropy": [[4.038544178009033, 2.6720917224884033]], "avg_entropy": [3.3553179502487183], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Roger Dean Stadium", "score_of_biggest_cluster": 0.004918575286865234, "score_of_first_answer": 0.004918575286865234, "max_score_of_first_answer": 0.007202070206403732, "min_score_of_first_answer": 0.0033590863458812237, "avg_score": 0.004918575286865234, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0019214919302612543, "std_of_first_answer": 0.0019214919302612543, "avg_entropy_of_first_answer": 3.3553179502487183, "lex_sim": 0.0}
{"question": "what part of the brain is in the middle cranial fossa", "answer": ["Parietal Lobe"], "scores": [[0.220624178647995, 0.560020923614502, 0.9700563549995422, 0.10355143994092941, 0.9979777932167053]], "normalized_score": [0.41551458835601807], "tokens": [["\u2581P", "ari", "etal", "\u2581L", "obe"]], "entropy": [[3.8705191612243652, 1.2671891450881958, 0.17200054228305817, 1.3474037647247314, 0.01596834324300289]], "avg_entropy": [1.3346161913126706], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "pituitary gland", "score_of_biggest_cluster": 0.41551458835601807, "score_of_first_answer": 0.41551458835601807, "max_score_of_first_answer": 0.9979777932167053, "min_score_of_first_answer": 0.10355143994092941, "avg_score": 0.41551458835601807, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3695798666481407, "std_of_first_answer": 0.3695798666481407, "avg_entropy_of_first_answer": 1.3346161913126706, "lex_sim": 1.0}
{"question": "who hosted and won the inagural world cup", "answer": ["Jules Rimet"], "scores": [[0.522171676158905, 0.9678947329521179, 0.9991859793663025]], "normalized_score": [0.7963352203369141], "tokens": [["\u2581Jules", "\u2581R", "imet"]], "entropy": [[2.6541388034820557, 0.3253012001514435, 0.00879780575633049]], "avg_entropy": [0.9960792697966099], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Uruguay", "score_of_biggest_cluster": 0.7963352203369141, "score_of_first_answer": 0.7963352203369141, "max_score_of_first_answer": 0.9991859793663025, "min_score_of_first_answer": 0.522171676158905, "avg_score": 0.7963352203369141, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21786612375587133, "std_of_first_answer": 0.21786612375587133, "avg_entropy_of_first_answer": 0.9960792697966099, "lex_sim": 1.0}
{"question": "where is simple squamous epithelium found in the body", "answer": ["on the lips"], "scores": [[0.03133661672472954, 0.7850883603096008, 0.024792930111289024]], "normalized_score": [0.08480722457170486], "tokens": [["\u2581on", "\u2581the", "\u2581lips"]], "entropy": [[4.4455485343933105, 1.3371628522872925, 3.2066221237182617]], "avg_entropy": [2.996444503466288], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "alveoli", "score_of_biggest_cluster": 0.08480722457170486, "score_of_first_answer": 0.08480722457170486, "max_score_of_first_answer": 0.7850883603096008, "min_score_of_first_answer": 0.024792930111289024, "avg_score": 0.08480722457170486, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35687434015851266, "std_of_first_answer": 0.35687434015851266, "avg_entropy_of_first_answer": 2.996444503466288, "lex_sim": 1.0}
{"question": "when was clash royale released in the us", "answer": ["December 5, 2018"], "scores": [[0.05034961551427841, 0.9627516269683838, 0.016215313225984573, 0.5209668278694153, 0.993533730506897, 0.999735414981842, 0.9997435212135315, 0.9989737272262573, 0.11708441376686096]], "normalized_score": [0.3308986723423004], "tokens": [["\u2581December", "\u2581", "5", ",", "\u2581", "2", "0", "1", "8"]], "entropy": [[2.7535784244537354, 0.20645537972450256, 1.0640919208526611, 0.8199996948242188, 0.041263848543167114, 0.0028262098785489798, 0.0031796221155673265, 0.009317860938608646, 1.8275113105773926]], "avg_entropy": [0.7475804746564891], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "March 2, 2016", "score_of_biggest_cluster": 0.3308986723423004, "score_of_first_answer": 0.3308986723423004, "max_score_of_first_answer": 0.9997435212135315, "min_score_of_first_answer": 0.016215313225984573, "avg_score": 0.3308986723423004, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4268906738806708, "std_of_first_answer": 0.4268906738806708, "avg_entropy_of_first_answer": 0.7475804746564891, "lex_sim": 1.0}
{"question": "when did the passion of the christ come out", "answer": ["February 2004"], "scores": [[0.14533770084381104, 0.9648860096931458, 0.74852454662323, 0.3240656554698944, 0.9747422933578491, 0.9917789101600647]], "normalized_score": [0.5660216212272644], "tokens": [["\u2581February", "\u2581", "2", "0", "0", "4"]], "entropy": [[1.917752742767334, 0.1947544664144516, 0.7539011240005493, 1.081668734550476, 0.14819148182868958, 0.0605539008975029]], "avg_entropy": [0.6928037417431673], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "February 25, 2004", "score_of_biggest_cluster": 0.5660216212272644, "score_of_first_answer": 0.5660216212272644, "max_score_of_first_answer": 0.9917789101600647, "min_score_of_first_answer": 0.14533770084381104, "avg_score": 0.5660216212272644, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.337067393283539, "std_of_first_answer": 0.337067393283539, "avg_entropy_of_first_answer": 0.6928037417431673, "lex_sim": 1.0}
{"question": "who will take the throne after the queen dies", "answer": ["Prince Charles"], "scores": [[0.4324100911617279, 0.824359655380249]], "normalized_score": [0.5970439314842224], "tokens": [["\u2581Prince", "\u2581Charles"]], "entropy": [[1.9573465585708618, 0.6892122626304626]], "avg_entropy": [1.3232794106006622], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charles, Prince of Wales", "score_of_biggest_cluster": 0.5970439314842224, "score_of_first_answer": 0.5970439314842224, "max_score_of_first_answer": 0.824359655380249, "min_score_of_first_answer": 0.4324100911617279, "avg_score": 0.5970439314842224, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.19597478210926056, "std_of_first_answer": 0.19597478210926056, "avg_entropy_of_first_answer": 1.3232794106006622, "lex_sim": 1.0}
{"question": "what is the second book in the alchemyst series", "answer": ["Alchemystic"], "scores": [[0.00113202806096524, 0.8052430152893066, 0.9425299763679504, 0.13480472564697266]], "normalized_score": [0.10373997688293457], "tokens": [["\u2581Al", "chem", "yst", "ic"]], "entropy": [[2.781978130340576, 0.7281837463378906, 0.302377849817276, 2.0324699878692627]], "avg_entropy": [1.4612524285912514], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Magician", "score_of_biggest_cluster": 0.10373997688293457, "score_of_first_answer": 0.10373997688293457, "max_score_of_first_answer": 0.9425299763679504, "min_score_of_first_answer": 0.00113202806096524, "avg_score": 0.10373997688293457, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40861414083849273, "std_of_first_answer": 0.40861414083849273, "avg_entropy_of_first_answer": 1.4612524285912514, "lex_sim": 1.0}
{"question": "who plays the protagonist peter rabbit in the 2018 motion picture release", "answer": ["Domhnall Gleeson"], "scores": [[0.03617817908525467, 0.9175487756729126, 0.9933879375457764, 0.9801607728004456, 0.9964191913604736, 0.9977697134017944, 0.9994797110557556]], "normalized_score": [0.6118950247764587], "tokens": [["\u2581Dom", "hn", "all", "\u2581G", "le", "es", "on"]], "entropy": [[1.045760154724121, 0.5543043613433838, 0.050224632024765015, 0.1931132674217224, 0.035392872989177704, 0.019419200718402863, 0.005867516156286001]], "avg_entropy": [0.27201171505397986], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "James Corden", "score_of_biggest_cluster": 0.6118950247764587, "score_of_first_answer": 0.6118950247764587, "max_score_of_first_answer": 0.9994797110557556, "min_score_of_first_answer": 0.03617817908525467, "avg_score": 0.6118950247764587, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3316342149208848, "std_of_first_answer": 0.3316342149208848, "avg_entropy_of_first_answer": 0.27201171505397986, "lex_sim": 1.0}
{"question": "who is the highest selling r&b artist of all time", "answer": ["Lauryn Hill"], "scores": [[0.002216898836195469, 0.9979009628295898, 0.9716622233390808]], "normalized_score": [0.129057377576828], "tokens": [["\u2581Laur", "yn", "\u2581Hill"]], "entropy": [[3.333448648452759, 0.01783306524157524, 0.20245525240898132]], "avg_entropy": [1.1845789887011051], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rihanna", "score_of_biggest_cluster": 0.129057377576828, "score_of_first_answer": 0.129057377576828, "max_score_of_first_answer": 0.9979009628295898, "min_score_of_first_answer": 0.002216898836195469, "avg_score": 0.129057377576828, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46330928761836804, "std_of_first_answer": 0.46330928761836804, "avg_entropy_of_first_answer": 1.1845789887011051, "lex_sim": 1.0}
{"question": "joined mexico and the united states to form nafta", "answer": ["North American Free Trade Agreement"], "scores": [[0.390514999628067, 0.8315657377243042, 0.9828837513923645, 0.9856761693954468, 0.9493270516395569, 0.9995729327201843, 0.9991100430488586]], "normalized_score": [0.8412883877754211], "tokens": [["\u2581North", "\u2581American", "\u2581Free", "\u2581Trade", "\u2581Ag", "re", "ement"]], "entropy": [[3.2335779666900635, 0.5698291659355164, 0.13532835245132446, 0.10268435627222061, 0.29733410477638245, 0.005027987062931061, 0.008488800376653671]], "avg_entropy": [0.6217529619378703], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Canada", "score_of_biggest_cluster": 0.8412883877754211, "score_of_first_answer": 0.8412883877754211, "max_score_of_first_answer": 0.9995729327201843, "min_score_of_first_answer": 0.390514999628067, "avg_score": 0.8412883877754211, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20595421744899106, "std_of_first_answer": 0.20595421744899106, "avg_entropy_of_first_answer": 0.6217529619378703, "lex_sim": 1.0}
{"question": "who discovered the atom and in what year", "answer": ["Niels Bohr"], "scores": [[0.10058771818876266, 0.9593997001647949, 0.9816042184829712, 0.9998958110809326]], "normalized_score": [0.5547648668289185], "tokens": [["\u2581N", "iels", "\u2581Bo", "hr"]], "entropy": [[3.934960126876831, 0.2415180206298828, 0.15170691907405853, 0.0014403401874005795]], "avg_entropy": [1.0824063516920432], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "early 1800s", "score_of_biggest_cluster": 0.5547648668289185, "score_of_first_answer": 0.5547648668289185, "max_score_of_first_answer": 0.9998958110809326, "min_score_of_first_answer": 0.10058771818876266, "avg_score": 0.5547648668289185, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3811963649248288, "std_of_first_answer": 0.3811963649248288, "avg_entropy_of_first_answer": 1.0824063516920432, "lex_sim": 1.0}
{"question": "how many episodes are there in modern family", "answer": ["81"], "scores": [[0.9627063274383545, 0.039833568036556244, 0.0503455214202404]], "normalized_score": [0.12451869994401932], "tokens": [["\u2581", "8", "1"]], "entropy": [[0.36670997738838196, 1.2193330526351929, 2.5775203704833984]], "avg_entropy": [1.3878544668356578], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "201", "score_of_biggest_cluster": 0.12451869994401932, "score_of_first_answer": 0.12451869994401932, "max_score_of_first_answer": 0.9627063274383545, "min_score_of_first_answer": 0.039833568036556244, "avg_score": 0.12451869994401932, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4325899869863756, "std_of_first_answer": 0.4325899869863756, "avg_entropy_of_first_answer": 1.3878544668356578, "lex_sim": 1.0}
{"question": "when was the first mad max movie release", "answer": ["1979"], "scores": [[0.8081803917884827, 0.9840086698532104, 0.9952195286750793, 0.9873325228691101, 0.9776650667190552]], "normalized_score": [0.947580099105835], "tokens": [["\u2581", "1", "9", "7", "9"]], "entropy": [[1.0783531665802002, 0.1046951562166214, 0.04327096790075302, 0.08481654524803162, 0.15010137856006622]], "avg_entropy": [0.29224744290113447], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "12\u00a0April\u00a01979", "score_of_biggest_cluster": 0.947580099105835, "score_of_first_answer": 0.947580099105835, "max_score_of_first_answer": 0.9952195286750793, "min_score_of_first_answer": 0.8081803917884827, "avg_score": 0.947580099105835, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.07137522246836739, "std_of_first_answer": 0.07137522246836739, "avg_entropy_of_first_answer": 0.29224744290113447, "lex_sim": 1.0}
{"question": "what percentage of the population is naturally blonde", "answer": ["2 %"], "scores": [[0.739518404006958, 0.8125852346420288, 0.014710545539855957]], "normalized_score": [0.2067674696445465], "tokens": [["\u2581", "2", "\u2581%"]], "entropy": [[1.6359213590621948, 0.8233764171600342, 1.059262752532959]], "avg_entropy": [1.1728535095850627], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2%", "score_of_biggest_cluster": 0.2067674696445465, "score_of_first_answer": 0.2067674696445465, "max_score_of_first_answer": 0.8125852346420288, "min_score_of_first_answer": 0.014710545539855957, "avg_score": 0.2067674696445465, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3601371980763581, "std_of_first_answer": 0.3601371980763581, "avg_entropy_of_first_answer": 1.1728535095850627, "lex_sim": 1.0}
{"question": "who directed the iconic animated short educational film ek anek aur ekta", "answer": ["Amar Kanwar"], "scores": [[0.01167034450918436, 0.566862940788269, 0.17730264365673065, 0.9771086573600769]], "normalized_score": [0.18399453163146973], "tokens": [["\u2581Am", "ar", "\u2581Kan", "war"]], "entropy": [[5.986258029937744, 2.2047834396362305, 3.9519009590148926, 0.19107794761657715]], "avg_entropy": [3.083505094051361], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Vijaya Mulay", "score_of_biggest_cluster": 0.18399453163146973, "score_of_first_answer": 0.18399453163146973, "max_score_of_first_answer": 0.9771086573600769, "min_score_of_first_answer": 0.01167034450918436, "avg_score": 0.18399453163146973, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3731196562454158, "std_of_first_answer": 0.3731196562454158, "avg_entropy_of_first_answer": 3.083505094051361, "lex_sim": 1.0}
{"question": "what is the name of the first earthquake early warning system", "answer": ["ShakeAlert"], "scores": [[0.6722390055656433, 0.9737659692764282, 0.767632782459259]], "normalized_score": [0.7950185537338257], "tokens": [["\u2581Sh", "ake", "Alert"]], "entropy": [[2.3929848670959473, 0.19175684452056885, 0.9570145010948181]], "avg_entropy": [1.1805854042371113], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1991", "score_of_biggest_cluster": 0.7950185537338257, "score_of_first_answer": 0.7950185537338257, "max_score_of_first_answer": 0.9737659692764282, "min_score_of_first_answer": 0.6722390055656433, "avg_score": 0.7950185537338257, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12583471223177203, "std_of_first_answer": 0.12583471223177203, "avg_entropy_of_first_answer": 1.1805854042371113, "lex_sim": 1.0}
{"question": "when will the next episode of flash be aired", "answer": ["October 16"], "scores": [[0.0709390863776207, 0.9608483910560608, 0.2883360683917999, 0.11827314645051956]], "normalized_score": [0.2195742428302765], "tokens": [["\u2581October", "\u2581", "1", "6"]], "entropy": [[3.583172082901001, 0.21213465929031372, 1.9356131553649902, 2.0129408836364746]], "avg_entropy": [1.9359651952981949], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "May\u00a015,\u00a02018", "score_of_biggest_cluster": 0.2195742428302765, "score_of_first_answer": 0.2195742428302765, "max_score_of_first_answer": 0.9608483910560608, "min_score_of_first_answer": 0.0709390863776207, "avg_score": 0.2195742428302765, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3564203799554761, "std_of_first_answer": 0.3564203799554761, "avg_entropy_of_first_answer": 1.9359651952981949, "lex_sim": 1.0}
{"question": "new mexico was admitted to the union as the . . . state", "answer": ["47th"], "scores": [[0.6506589651107788, 0.975762665271759, 0.9856593608856201, 0.6742391586303711]], "normalized_score": [0.805952787399292], "tokens": [["\u2581", "4", "7", "th"]], "entropy": [[1.8944391012191772, 0.15145598351955414, 0.10761724412441254, 0.7636536359786987]], "avg_entropy": [0.7292914912104607], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "47th", "score_of_biggest_cluster": 0.805952787399292, "score_of_first_answer": 0.805952787399292, "max_score_of_first_answer": 0.9856593608856201, "min_score_of_first_answer": 0.6506589651107788, "avg_score": 0.805952787399292, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.15938762078816202, "std_of_first_answer": 0.15938762078816202, "avg_entropy_of_first_answer": 0.7292914912104607, "lex_sim": 1.0}
{"question": "where do they get the hair for a hair transplant", "answer": ["They use existing hair."], "scores": [[0.021224431693553925, 0.13793613016605377, 0.004501316696405411, 0.8456413149833679, 0.12305064499378204]], "normalized_score": [0.06720858812332153], "tokens": [["\u2581They", "\u2581use", "\u2581existing", "\u2581hair", "."]], "entropy": [[4.299849510192871, 3.3137857913970947, 3.110978126525879, 0.9128888249397278, 2.5935449600219727]], "avg_entropy": [2.846209442615509], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "chest", "score_of_biggest_cluster": 0.06720858812332153, "score_of_first_answer": 0.06720858812332153, "max_score_of_first_answer": 0.8456413149833679, "min_score_of_first_answer": 0.004501316696405411, "avg_score": 0.06720858812332153, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31410277492245425, "std_of_first_answer": 0.31410277492245425, "avg_entropy_of_first_answer": 2.846209442615509, "lex_sim": 1.0}
{"question": "what's in a beam me up scotty", "answer": ["Transporter"], "scores": [[0.03745676204562187, 0.8731010556221008]], "normalized_score": [0.18084119260311127], "tokens": [["\u2581Trans", "porter"]], "entropy": [[5.816459655761719, 0.7615055441856384]], "avg_entropy": [3.2889825999736786], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "phencyclidine", "score_of_biggest_cluster": 0.18084119260311127, "score_of_first_answer": 0.18084119260311127, "max_score_of_first_answer": 0.8731010556221008, "min_score_of_first_answer": 0.03745676204562187, "avg_score": 0.18084119260311127, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4178221467882395, "std_of_first_answer": 0.4178221467882395, "avg_entropy_of_first_answer": 3.2889825999736786, "lex_sim": 1.0}
{"question": "what is the zebras name in peppa pig", "answer": ["Pru"], "scores": [[0.0180569589138031, 0.014414088800549507]], "normalized_score": [0.016133032739162445], "tokens": [["\u2581P", "ru"]], "entropy": [[2.8728766441345215, 3.3835272789001465]], "avg_entropy": [3.128201961517334], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Zuzu", "score_of_biggest_cluster": 0.016133032739162445, "score_of_first_answer": 0.016133032739162445, "max_score_of_first_answer": 0.0180569589138031, "min_score_of_first_answer": 0.014414088800549507, "avg_score": 0.016133032739162445, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0018214350566267967, "std_of_first_answer": 0.0018214350566267967, "avg_entropy_of_first_answer": 3.128201961517334, "lex_sim": 1.0}
{"question": "who introduced the first chrismas tree to the uk", "answer": ["Lewis Carroll"], "scores": [[0.0074134948663413525, 0.11713945865631104, 0.9310729503631592]], "normalized_score": [0.09316152334213257], "tokens": [["\u2581Lewis", "\u2581Car", "roll"]], "entropy": [[4.11228084564209, 3.9312191009521484, 0.336363822221756]], "avg_entropy": [2.793287922938665], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charlotte of Mecklenburg-Strelitz", "score_of_biggest_cluster": 0.09316152334213257, "score_of_first_answer": 0.09316152334213257, "max_score_of_first_answer": 0.9310729503631592, "min_score_of_first_answer": 0.0074134948663413525, "avg_score": 0.09316152334213257, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4119970746633494, "std_of_first_answer": 0.4119970746633494, "avg_entropy_of_first_answer": 2.793287922938665, "lex_sim": 1.0}
{"question": "what color is the golden gate bridge in san francisco", "answer": ["International Orange"], "scores": [[0.4930992126464844, 0.8103160262107849]], "normalized_score": [0.6321125030517578], "tokens": [["\u2581International", "\u2581Orange"]], "entropy": [[2.6921448707580566, 0.5232594013214111]], "avg_entropy": [1.6077021360397339], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "orange vermilion called international orange", "score_of_biggest_cluster": 0.6321125030517578, "score_of_first_answer": 0.6321125030517578, "max_score_of_first_answer": 0.8103160262107849, "min_score_of_first_answer": 0.4930992126464844, "avg_score": 0.6321125030517578, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.15860840678215027, "std_of_first_answer": 0.15860840678215027, "avg_entropy_of_first_answer": 1.6077021360397339, "lex_sim": 1.0}
{"question": "where does the ohio river and the mississippi river meet", "answer": ["they don't since they are from different drainage basins"], "scores": [[0.006158254109323025, 0.22849075496196747, 0.5633002519607544, 0.998046875, 0.0013803914189338684, 0.4546801745891571, 0.4619848430156708, 0.0012692492455244064, 0.7257292866706848, 0.3006887137889862, 0.9989898800849915, 0.7744334936141968, 0.7825515270233154, 0.9859254956245422]], "normalized_score": [0.18015041947364807], "tokens": [["\u2581they", "\u2581don", "'", "t", "\u2581since", "\u2581they", "\u2581are", "\u2581from", "\u2581different", "\u2581d", "rain", "age", "\u2581bas", "ins"]], "entropy": [[4.731455326080322, 2.6256906986236572, 0.9207302331924438, 0.023573586717247963, 1.4814424514770508, 1.532397985458374, 2.489230155944824, 3.149871349334717, 1.195120930671692, 2.1546192169189453, 0.011595693416893482, 0.5486915707588196, 0.8183192610740662, 0.08011922240257263]], "avg_entropy": [1.5544898344336875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cairo, Illinois", "score_of_biggest_cluster": 0.18015041947364807, "score_of_first_answer": 0.18015041947364807, "max_score_of_first_answer": 0.9989898800849915, "min_score_of_first_answer": 0.0012692492455244064, "avg_score": 0.18015041947364807, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3581404007480964, "std_of_first_answer": 0.3581404007480964, "avg_entropy_of_first_answer": 1.5544898344336875, "lex_sim": 1.0}
{"question": "when does the next warrior book come out", "answer": ["November 26, 2019"], "scores": [[0.031965091824531555, 0.857036828994751, 0.3550699055194855, 0.024416200816631317, 0.3578313887119293, 0.9872035384178162, 0.99797123670578, 0.9994749426841736, 0.9874697923660278, 0.623436689376831]], "normalized_score": [0.37255966663360596], "tokens": [["\u2581November", "\u2581", "2", "6", ",", "\u2581", "2", "0", "1", "9"]], "entropy": [[4.303347587585449, 0.5338964462280273, 1.6804194450378418, 1.933223843574524, 1.056208610534668, 0.0836549624800682, 0.015833985060453415, 0.006655268371105194, 0.07990645617246628, 1.1849600076675415]], "avg_entropy": [1.0878106612712144], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "April 10, 2018", "score_of_biggest_cluster": 0.37255966663360596, "score_of_first_answer": 0.37255966663360596, "max_score_of_first_answer": 0.9994749426841736, "min_score_of_first_answer": 0.024416200816631317, "avg_score": 0.37255966663360596, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38109209392189114, "std_of_first_answer": 0.38109209392189114, "avg_entropy_of_first_answer": 1.0878106612712144, "lex_sim": 1.0}
{"question": "the era of the great mughals began with the accession of", "answer": ["Akbar"], "scores": [[0.2687380313873291, 0.994229793548584]], "normalized_score": [0.5169016718864441], "tokens": [["\u2581Ak", "bar"]], "entropy": [[2.7303311824798584, 0.04998581111431122]], "avg_entropy": [1.3901584967970848], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Akbar the Great", "score_of_biggest_cluster": 0.5169016718864441, "score_of_first_answer": 0.5169016718864441, "max_score_of_first_answer": 0.994229793548584, "min_score_of_first_answer": 0.2687380313873291, "avg_score": 0.5169016718864441, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.36274588108062744, "std_of_first_answer": 0.36274588108062744, "avg_entropy_of_first_answer": 1.3901584967970848, "lex_sim": 1.0}
{"question": "who is the original singer of you re going to love me", "answer": ["Joe Brown"], "scores": [[0.024663317948579788, 0.013351082801818848]], "normalized_score": [0.018146123737096786], "tokens": [["\u2581Joe", "\u2581Brown"]], "entropy": [[5.8284101486206055, 4.040434837341309]], "avg_entropy": [4.934422492980957], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jennifer Holliday", "score_of_biggest_cluster": 0.018146123737096786, "score_of_first_answer": 0.018146123737096786, "max_score_of_first_answer": 0.024663317948579788, "min_score_of_first_answer": 0.013351082801818848, "avg_score": 0.018146123737096786, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.00565611757338047, "std_of_first_answer": 0.00565611757338047, "avg_entropy_of_first_answer": 4.934422492980957, "lex_sim": 1.0}
{"question": "when does boomer find out she a cylon", "answer": ["in the last episode"], "scores": [[0.046256858855485916, 0.4660045802593231, 0.023131223395466805, 0.6635295748710632]], "normalized_score": [0.13486719131469727], "tokens": [["\u2581in", "\u2581the", "\u2581last", "\u2581episode"]], "entropy": [[4.6398162841796875, 2.5748214721679688, 3.339146375656128, 1.8040614128112793]], "avg_entropy": [3.089461386203766], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kobol's Last Gleaming", "score_of_biggest_cluster": 0.13486719131469727, "score_of_first_answer": 0.13486719131469727, "max_score_of_first_answer": 0.6635295748710632, "min_score_of_first_answer": 0.023131223395466805, "avg_score": 0.13486719131469727, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27420470602103103, "std_of_first_answer": 0.27420470602103103, "avg_entropy_of_first_answer": 3.089461386203766, "lex_sim": 1.0}
{"question": "who plays christian in 50 shades of grey", "answer": ["Jamie Dornan"], "scores": [[0.6088623404502869, 0.9975368976593018, 0.9887890219688416, 0.9936908483505249, 0.9992056488990784]], "normalized_score": [0.9017612934112549], "tokens": [["\u2581Jam", "ie", "\u2581D", "orn", "an"]], "entropy": [[2.0182406902313232, 0.027245456352829933, 0.11394485086202621, 0.04797635227441788, 0.00916806235909462]], "avg_entropy": [0.44331508241593837], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "James \"Jamie\" Dornan", "score_of_biggest_cluster": 0.9017612934112549, "score_of_first_answer": 0.9017612934112549, "max_score_of_first_answer": 0.9992056488990784, "min_score_of_first_answer": 0.6088623404502869, "avg_score": 0.9017612934112549, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.15441892667590312, "std_of_first_answer": 0.15441892667590312, "avg_entropy_of_first_answer": 0.44331508241593837, "lex_sim": 1.0}
{"question": "who was the girl that played cindy lou who", "answer": ["Margot Kidder"], "scores": [[0.02818748913705349, 0.053472332656383514, 0.7086029648780823, 0.9992449283599854]], "normalized_score": [0.1807444989681244], "tokens": [["\u2581Mar", "got", "\u2581Kid", "der"]], "entropy": [[5.427682876586914, 3.030709743499756, 1.9252923727035522, 0.009002389386296272]], "avg_entropy": [2.5981718455441296], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Taylor Michel Momsen", "score_of_biggest_cluster": 0.1807444989681244, "score_of_first_answer": 0.1807444989681244, "max_score_of_first_answer": 0.9992449283599854, "min_score_of_first_answer": 0.02818748913705349, "avg_score": 0.1807444989681244, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4194275706398568, "std_of_first_answer": 0.4194275706398568, "avg_entropy_of_first_answer": 2.5981718455441296, "lex_sim": 1.0}
{"question": "who played the daughters in last man standing", "answer": ["Krista Vendysh and Rebekah Reid"], "scores": [[0.025456752628087997, 0.5019060373306274, 0.018439197912812233, 0.2292986512184143, 0.9252816438674927, 0.002438639523461461, 0.19107630848884583, 0.007189047057181597, 0.09297260642051697, 0.828415036201477, 0.002808153862133622, 0.13160467147827148]], "normalized_score": [0.06404917687177658], "tokens": [["\u2581Krist", "a", "\u2581V", "end", "y", "sh", "\u2581and", "\u2581Reb", "ek", "ah", "\u2581Re", "id"]], "entropy": [[5.437511444091797, 1.2873286008834839, 2.44018292427063, 3.107024669647217, 0.4616836607456207, 2.1340670585632324, 1.9855124950408936, 5.584396839141846, 0.9952235817909241, 0.6339109539985657, 5.409783363342285, 4.230068206787109]], "avg_entropy": [2.8088911498586335], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alexandra Krosney", "score_of_biggest_cluster": 0.06404917687177658, "score_of_first_answer": 0.06404917687177658, "max_score_of_first_answer": 0.9252816438674927, "min_score_of_first_answer": 0.002438639523461461, "avg_score": 0.06404917687177658, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3138250948006241, "std_of_first_answer": 0.3138250948006241, "avg_entropy_of_first_answer": 2.8088911498586335, "lex_sim": 1.0}
{"question": "when was the taj mahal built and completed", "answer": ["18 November 1648 \u2013 22 July 1653"], "scores": [[0.7187421321868896, 0.9911032915115356, 0.0019533848389983177, 0.002369180554524064, 0.9491537809371948, 0.9861437082290649, 0.8988966941833496, 0.21854542195796967, 0.43876388669013977, 0.07042845338582993, 0.9417797327041626, 0.38302770256996155, 0.5923162698745728, 0.04931897297501564, 0.9967708587646484, 0.9972432851791382, 0.9643415212631226, 0.9478018283843994, 0.9466893672943115]], "normalized_score": [0.3060436546802521], "tokens": [["\u2581", "1", "8", "\u2581November", "\u2581", "1", "6", "4", "8", "\u2581\u2013", "\u2581", "2", "2", "\u2581July", "\u2581", "1", "6", "5", "3"]], "entropy": [[1.9582676887512207, 0.06923141330480576, 0.35230085253715515, 2.200312614440918, 0.26751816272735596, 0.09304813295602798, 0.4937710762023926, 1.2182575464248657, 1.3817092180252075, 1.8596290349960327, 0.46695950627326965, 1.310083031654358, 1.6287336349487305, 2.2661356925964355, 0.02873077802360058, 0.01972997561097145, 0.20148205757141113, 0.2520536780357361, 0.29684513807296753]], "avg_entropy": [0.8613052227975506], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1632\u201353", "score_of_biggest_cluster": 0.3060436546802521, "score_of_first_answer": 0.3060436546802521, "max_score_of_first_answer": 0.9972432851791382, "min_score_of_first_answer": 0.0019533848389983177, "avg_score": 0.3060436546802521, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38560075710678426, "std_of_first_answer": 0.38560075710678426, "avg_entropy_of_first_answer": 0.8613052227975506, "lex_sim": 1.0}
{"question": "what is the year round weather in dubai", "answer": ["hot desert climate with hot and sometimes humid summer, warm winter."], "scores": [[0.08605366945266724, 0.06460176408290863, 0.36713334918022156, 0.05209287628531456, 0.0369001179933548, 0.0799010768532753, 0.002151697874069214, 0.5559269785881042, 0.9972770810127258, 0.07171770930290222, 0.12891556322574615, 0.3880653381347656, 0.44062888622283936, 0.016394510865211487]], "normalized_score": [0.09977689385414124], "tokens": [["\u2581hot", "\u2581desert", "\u2581climate", "\u2581with", "\u2581hot", "\u2581and", "\u2581sometimes", "\u2581hum", "id", "\u2581summer", ",", "\u2581warm", "\u2581winter", "."]], "entropy": [[4.137289047241211, 1.5513101816177368, 1.3825297355651855, 0.706442654132843, 3.9393975734710693, 1.9967937469482422, 1.7226839065551758, 2.0925955772399902, 0.027330823242664337, 1.0001752376556396, 2.5367159843444824, 2.5342772006988525, 2.0056099891662598, 1.9797346591949463]], "avg_entropy": [1.9723490226481641], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "tropical desert climate", "score_of_biggest_cluster": 0.09977689385414124, "score_of_first_answer": 0.09977689385414124, "max_score_of_first_answer": 0.9972770810127258, "min_score_of_first_answer": 0.002151697874069214, "avg_score": 0.09977689385414124, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27452922084166564, "std_of_first_answer": 0.27452922084166564, "avg_entropy_of_first_answer": 1.9723490226481641, "lex_sim": 1.0}
{"question": "when does the team find out prentiss is alive", "answer": ["Prentiss"], "scores": [[0.01657462865114212, 0.9872114062309265, 0.9966006875038147]], "normalized_score": [0.2535858750343323], "tokens": [["\u2581Pr", "ent", "iss"]], "entropy": [[4.882616996765137, 0.10684584826231003, 0.025603458285331726]], "avg_entropy": [1.6716887677709262], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "In the season seven premiere", "score_of_biggest_cluster": 0.2535858750343323, "score_of_first_answer": 0.2535858750343323, "max_score_of_first_answer": 0.9966006875038147, "min_score_of_first_answer": 0.01657462865114212, "avg_score": 0.2535858750343323, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45979161809377067, "std_of_first_answer": 0.45979161809377067, "avg_entropy_of_first_answer": 1.6716887677709262, "lex_sim": 1.0}
{"question": "who sings you are a magnet and i am steel", "answer": ["Randy Houser"], "scores": [[0.03337378427386284, 0.09123126417398453, 0.2620683014392853, 0.9993706345558167]], "normalized_score": [0.168043851852417], "tokens": [["\u2581R", "andy", "\u2581Hou", "ser"]], "entropy": [[6.177470684051514, 2.8480312824249268, 3.1419355869293213, 0.006815768778324127]], "avg_entropy": [3.0435633305460215], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Walter Egan", "score_of_biggest_cluster": 0.168043851852417, "score_of_first_answer": 0.168043851852417, "max_score_of_first_answer": 0.9993706345558167, "min_score_of_first_answer": 0.03337378427386284, "avg_score": 0.168043851852417, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38619261100152075, "std_of_first_answer": 0.38619261100152075, "avg_entropy_of_first_answer": 3.0435633305460215, "lex_sim": 1.0}
{"question": "who played susanna in legends of the fall", "answer": ["Ashley Judd"], "scores": [[0.015117091126739979, 0.9849280714988708, 0.9317447543144226, 0.9980123043060303]], "normalized_score": [0.34302547574043274], "tokens": [["\u2581Ash", "ley", "\u2581Jud", "d"]], "entropy": [[4.554887771606445, 0.12365518510341644, 0.46296679973602295, 0.020010564476251602]], "avg_entropy": [1.290380080230534], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Julia Ormond", "score_of_biggest_cluster": 0.34302547574043274, "score_of_first_answer": 0.34302547574043274, "max_score_of_first_answer": 0.9980123043060303, "min_score_of_first_answer": 0.015117091126739979, "avg_score": 0.34302547574043274, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41489559388619174, "std_of_first_answer": 0.41489559388619174, "avg_entropy_of_first_answer": 1.290380080230534, "lex_sim": 1.0}
{"question": "when did the nest 3rd generation come out", "answer": ["September 5, 2018"], "scores": [[0.08926726132631302, 0.9298203587532043, 0.0351814404129982, 0.5184069275856018, 0.9930949807167053, 0.9835030436515808, 0.9997374415397644, 0.8623590469360352, 0.17029863595962524]], "normalized_score": [0.3917044997215271], "tokens": [["\u2581September", "\u2581", "5", ",", "\u2581", "2", "0", "1", "8"]], "entropy": [[3.012178659439087, 0.3455319106578827, 1.2158498764038086, 0.8410753011703491, 0.04385935142636299, 0.08560764044523239, 0.003295132890343666, 0.4075150787830353, 2.200066566467285]], "avg_entropy": [0.9061088352981541], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "September 1, 2015", "score_of_biggest_cluster": 0.3917044997215271, "score_of_first_answer": 0.3917044997215271, "max_score_of_first_answer": 0.9997374415397644, "min_score_of_first_answer": 0.0351814404129982, "avg_score": 0.3917044997215271, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3953203428696426, "std_of_first_answer": 0.3953203428696426, "avg_entropy_of_first_answer": 0.9061088352981541, "lex_sim": 1.0}
{"question": "what olympic athlete has won the most medals", "answer": ["Michael Phelps"], "scores": [[0.3013306260108948, 0.9883596897125244, 0.9985197186470032, 0.9983969330787659]], "normalized_score": [0.738166868686676], "tokens": [["\u2581Michael", "\u2581Ph", "el", "ps"]], "entropy": [[3.20985746383667, 0.10032515227794647, 0.013762073591351509, 0.013833430595695972]], "avg_entropy": [0.834444530075416], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "American swimmer Michael Phelps", "score_of_biggest_cluster": 0.738166868686676, "score_of_first_answer": 0.738166868686676, "max_score_of_first_answer": 0.9985197186470032, "min_score_of_first_answer": 0.3013306260108948, "avg_score": 0.738166868686676, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.30043582813445535, "std_of_first_answer": 0.30043582813445535, "avg_entropy_of_first_answer": 0.834444530075416, "lex_sim": 1.0}
{"question": "who was the 11th prime minister of canada", "answer": ["Pierre Elliott Trudeau"], "scores": [[0.13042956590652466, 0.31696850061416626, 0.7297577857971191, 0.9961032867431641, 0.9999520778656006, 0.9985154271125793]], "normalized_score": [0.5574443936347961], "tokens": [["\u2581Pierre", "\u2581Elli", "ott", "\u2581Tru", "de", "au"]], "entropy": [[2.995600461959839, 1.3959734439849854, 0.5924829244613647, 0.034951191395521164, 0.000692349043674767, 0.014223705977201462]], "avg_entropy": [0.8389873461370977], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "R. B. Bennett", "score_of_biggest_cluster": 0.5574443936347961, "score_of_first_answer": 0.5574443936347961, "max_score_of_first_answer": 0.9999520778656006, "min_score_of_first_answer": 0.13042956590652466, "avg_score": 0.5574443936347961, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35086459162760997, "std_of_first_answer": 0.35086459162760997, "avg_entropy_of_first_answer": 0.8389873461370977, "lex_sim": 1.0}
{"question": "which supreme court judge has surved in international court of justice", "answer": ["Bhinder Singh"], "scores": [[0.003344825701788068, 0.0017492225160822272, 0.22480154037475586]], "normalized_score": [0.010956519283354282], "tokens": [["\u2581Bh", "inder", "\u2581Singh"]], "entropy": [[5.300240516662598, 1.0517479181289673, 3.8348350524902344]], "avg_entropy": [3.395607829093933], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dalveer Bhandari", "score_of_biggest_cluster": 0.010956519283354282, "score_of_first_answer": 0.010956519283354282, "max_score_of_first_answer": 0.22480154037475586, "min_score_of_first_answer": 0.0017492225160822272, "avg_score": 0.010956519283354282, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10477380871053038, "std_of_first_answer": 0.10477380871053038, "avg_entropy_of_first_answer": 3.395607829093933, "lex_sim": 1.0}
{"question": "what is the name of the last episode of spongebob", "answer": ["2015"], "scores": [[0.0074646444991230965, 0.4012184143066406, 0.7161371111869812, 0.46142759919166565, 0.6000189185142517]], "normalized_score": [0.2263241708278656], "tokens": [["\u2581", "2", "0", "1", "5"]], "entropy": [[4.312542915344238, 1.8596079349517822, 1.8317346572875977, 2.179795742034912, 1.7018520832061768]], "avg_entropy": [2.3771066665649414], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\"Bottle Burglars\"", "score_of_biggest_cluster": 0.2263241708278656, "score_of_first_answer": 0.2263241708278656, "max_score_of_first_answer": 0.7161371111869812, "min_score_of_first_answer": 0.0074646444991230965, "avg_score": 0.2263241708278656, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24119287918099439, "std_of_first_answer": 0.24119287918099439, "avg_entropy_of_first_answer": 2.3771066665649414, "lex_sim": 1.0}
{"question": "who sang a whiter shade of pale first", "answer": ["Procol Harum"], "scores": [[0.2959677577018738, 0.8668568134307861, 0.9694971442222595, 0.9886143207550049]], "normalized_score": [0.7041923403739929], "tokens": [["\u2581Pro", "col", "\u2581Har", "um"]], "entropy": [[3.25105619430542, 0.4198005795478821, 0.19887644052505493, 0.10914629697799683]], "avg_entropy": [0.9947198778390884], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Procol Harum", "score_of_biggest_cluster": 0.7041923403739929, "score_of_first_answer": 0.7041923403739929, "max_score_of_first_answer": 0.9886143207550049, "min_score_of_first_answer": 0.2959677577018738, "avg_score": 0.7041923403739929, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.28339910068447044, "std_of_first_answer": 0.28339910068447044, "avg_entropy_of_first_answer": 0.9947198778390884, "lex_sim": 1.0}
{"question": "during the first world war against which country germany did not fight", "answer": ["Japan"], "scores": [[0.07206419855356216]], "normalized_score": [0.07206419110298157], "tokens": [["\u2581Japan"]], "entropy": [[4.27237606048584]], "avg_entropy": [4.27237606048584], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Austria-Hungary", "score_of_biggest_cluster": 0.07206419110298157, "score_of_first_answer": 0.07206419110298157, "max_score_of_first_answer": 0.07206419855356216, "min_score_of_first_answer": 0.07206419855356216, "avg_score": 0.07206419110298157, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.27237606048584, "lex_sim": 1.0}
{"question": "who took part in the american civil war", "answer": ["3 million and 500,000 troops"], "scores": [[0.08809469640254974, 0.128607839345932, 0.30171602964401245, 0.016301309689879417, 0.9094216227531433, 0.2763427495956421, 0.9082409739494324, 0.7188172936439514, 0.17408348619937897, 0.987669050693512, 0.9996434450149536, 0.9893330335617065, 0.01764542981982231]], "normalized_score": [0.2620159387588501], "tokens": [["\u2581", "3", "\u2581million", "\u2581and", "\u2581", "5", "0", "0", ",", "0", "0", "0", "\u2581troops"]], "entropy": [[4.791624546051025, 1.5273795127868652, 2.165149688720703, 2.0183281898498535, 0.610315203666687, 1.993516206741333, 0.5437716841697693, 0.8195810317993164, 0.8054509162902832, 0.07385873794555664, 0.004160641692578793, 0.0828205794095993, 1.6998426914215088]], "avg_entropy": [1.3181384331188524], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "nationalists of the Union", "score_of_biggest_cluster": 0.2620159387588501, "score_of_first_answer": 0.2620159387588501, "max_score_of_first_answer": 0.9996434450149536, "min_score_of_first_answer": 0.016301309689879417, "avg_score": 0.2620159387588501, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3998893032860951, "std_of_first_answer": 0.3998893032860951, "avg_entropy_of_first_answer": 1.3181384331188524, "lex_sim": 1.0}
{"question": "who does the voice of the beast in the new movie", "answer": ["Joshua Rush"], "scores": [[0.00451063085347414, 0.35433632135391235, 0.03789633512496948, 0.9979777932167053]], "normalized_score": [0.08817445486783981], "tokens": [["\u2581Josh", "ua", "\u2581R", "ush"]], "entropy": [[2.860914707183838, 3.0206995010375977, 4.512870788574219, 0.022364947944879532]], "avg_entropy": [2.6042124861851335], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dan Stevens", "score_of_biggest_cluster": 0.08817445486783981, "score_of_first_answer": 0.08817445486783981, "max_score_of_first_answer": 0.9979777932167053, "min_score_of_first_answer": 0.00451063085347414, "avg_score": 0.08817445486783981, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3989544645880414, "std_of_first_answer": 0.3989544645880414, "avg_entropy_of_first_answer": 2.6042124861851335, "lex_sim": 1.0}
{"question": "where do kidneys for kidney transplant come from", "answer": ["from a living person"], "scores": [[0.14813163876533508, 0.15317362546920776, 0.08442669361829758, 0.20400628447532654]], "normalized_score": [0.14060112833976746], "tokens": [["\u2581from", "\u2581a", "\u2581living", "\u2581person"]], "entropy": [[4.160560607910156, 2.7609596252441406, 2.3682336807250977, 1.5535075664520264]], "avg_entropy": [2.7108153700828552], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "living-donor", "score_of_biggest_cluster": 0.14060112833976746, "score_of_first_answer": 0.14060112833976746, "max_score_of_first_answer": 0.20400628447532654, "min_score_of_first_answer": 0.08442669361829758, "avg_score": 0.14060112833976746, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.042437524749187654, "std_of_first_answer": 0.042437524749187654, "avg_entropy_of_first_answer": 2.7108153700828552, "lex_sim": 1.0}
{"question": "where is a simple gear train used in real life", "answer": ["at home"], "scores": [[0.004194196313619614, 0.06882517784833908]], "normalized_score": [0.016990184783935547], "tokens": [["\u2581at", "\u2581home"]], "entropy": [[5.681197643280029, 3.2382779121398926]], "avg_entropy": [4.459737777709961], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Automobile drivetrains", "score_of_biggest_cluster": 0.016990184783935547, "score_of_first_answer": 0.016990184783935547, "max_score_of_first_answer": 0.06882517784833908, "min_score_of_first_answer": 0.004194196313619614, "avg_score": 0.016990184783935547, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.032315490767359734, "std_of_first_answer": 0.032315490767359734, "avg_entropy_of_first_answer": 4.459737777709961, "lex_sim": 1.0}
{"question": "this poster was created to increase support for the war effort.the poster links food rationing to", "answer": ["war."], "scores": [[0.01973450742661953, 0.013829588890075684]], "normalized_score": [0.016520295292139053], "tokens": [["\u2581war", "."]], "entropy": [[5.880743026733398, 2.3574373722076416]], "avg_entropy": [4.11909019947052], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "aiding the war effort", "score_of_biggest_cluster": 0.016520295292139053, "score_of_first_answer": 0.016520295292139053, "max_score_of_first_answer": 0.01973450742661953, "min_score_of_first_answer": 0.013829588890075684, "avg_score": 0.016520295292139053, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.002952459268271923, "std_of_first_answer": 0.002952459268271923, "avg_entropy_of_first_answer": 4.11909019947052, "lex_sim": 1.0}
{"question": "how many seasons of from dusk till dawn are there", "answer": ["3"], "scores": [[0.550694465637207, 0.6030200123786926]], "normalized_score": [0.5762636661529541], "tokens": [["\u2581", "3"]], "entropy": [[2.0221662521362305, 1.3947442770004272]], "avg_entropy": [1.7084552645683289], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "third season", "score_of_biggest_cluster": 0.5762636661529541, "score_of_first_answer": 0.5762636661529541, "max_score_of_first_answer": 0.6030200123786926, "min_score_of_first_answer": 0.550694465637207, "avg_score": 0.5762636661529541, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.026162773370742798, "std_of_first_answer": 0.026162773370742798, "avg_entropy_of_first_answer": 1.7084552645683289, "lex_sim": 1.0}
{"question": "where does hydrogen peroxide come from in the body", "answer": ["The body manufactures both hydrogen peroxide and urea"], "scores": [[0.018398599699139595, 0.017043916508555412, 0.011866584420204163, 0.9786499738693237, 0.001600900897756219, 0.34627479314804077, 0.9979239702224731, 0.949621319770813, 0.998230516910553, 0.9973717927932739, 0.9210267066955566, 0.027834074571728706, 0.39244863390922546]], "normalized_score": [0.14982138574123383], "tokens": [["\u2581The", "\u2581body", "\u2581manufact", "ures", "\u2581both", "\u2581hydro", "gen", "\u2581per", "ox", "ide", "\u2581and", "\u2581u", "rea"]], "entropy": [[5.186150550842285, 4.075803279876709, 2.442951202392578, 0.12834766507148743, 1.7326116561889648, 2.616023063659668, 0.019844913855195045, 0.23933175206184387, 0.01738431490957737, 0.02102440968155861, 0.49781087040901184, 3.0362801551818848, 1.0742323398590088]], "avg_entropy": [1.6221381672299826], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "nearly all living cells", "score_of_biggest_cluster": 0.14982138574123383, "score_of_first_answer": 0.14982138574123383, "max_score_of_first_answer": 0.998230516910553, "min_score_of_first_answer": 0.001600900897756219, "avg_score": 0.14982138574123383, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44377260422817494, "std_of_first_answer": 0.44377260422817494, "avg_entropy_of_first_answer": 1.6221381672299826, "lex_sim": 1.0}
{"question": "how are leaders of the two parties in congress chosen", "answer": ["by the president, in consultation with the minority leader of their party"], "scores": [[0.17309068143367767, 0.28422242403030396, 0.052369311451911926, 0.021755056455731392, 0.02351227216422558, 0.3238348960876465, 0.9940571784973145, 0.988835871219635, 0.7076243758201599, 0.041997943073511124, 0.997410237789154, 0.45767804980278015, 0.19146974384784698, 0.024460874497890472, 0.5296728610992432]], "normalized_score": [0.18908241391181946], "tokens": [["\u2581by", "\u2581the", "\u2581president", ",", "\u2581in", "\u2581consult", "ation", "\u2581with", "\u2581the", "\u2581minor", "ity", "\u2581leader", "\u2581of", "\u2581their", "\u2581party"]], "entropy": [[4.60379695892334, 3.4572196006774902, 4.154146194458008, 1.7047525644302368, 4.0720624923706055, 3.1524744033813477, 0.048237144947052, 0.09026436507701874, 1.381770133972168, 3.209343433380127, 0.02302829921245575, 1.2922654151916504, 1.5553877353668213, 0.6645556688308716, 1.2713797092437744]], "avg_entropy": [2.045378941297531], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Senate Republican Conference", "score_of_biggest_cluster": 0.18908241391181946, "score_of_first_answer": 0.18908241391181946, "max_score_of_first_answer": 0.997410237789154, "min_score_of_first_answer": 0.021755056455731392, "avg_score": 0.18908241391181946, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36129225707631923, "std_of_first_answer": 0.36129225707631923, "avg_entropy_of_first_answer": 2.045378941297531, "lex_sim": 1.0}
{"question": "where is bigg boss tamil house is located", "answer": ["Chennai"], "scores": [[0.16184066236019135, 0.9995794892311096, 0.9982665181159973]], "normalized_score": [0.5445659756660461], "tokens": [["\u2581Chen", "na", "i"]], "entropy": [[5.070583343505859, 0.005143302958458662, 0.01864989846944809]], "avg_entropy": [1.698125514977922], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chennai", "score_of_biggest_cluster": 0.5445659756660461, "score_of_first_answer": 0.5445659756660461, "max_score_of_first_answer": 0.9995794892311096, "min_score_of_first_answer": 0.16184066236019135, "avg_score": 0.5445659756660461, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3946047640244383, "std_of_first_answer": 0.3946047640244383, "avg_entropy_of_first_answer": 1.698125514977922, "lex_sim": 1.0}
{"question": "who sang picking up pebbles and throwing them into the sea", "answer": ["Tom Jones"], "scores": [[0.006813537795096636, 0.1265392154455185]], "normalized_score": [0.029362894594669342], "tokens": [["\u2581Tom", "\u2581Jones"]], "entropy": [[6.316550254821777, 4.429205417633057]], "avg_entropy": [5.372877836227417], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Matt Flinders", "score_of_biggest_cluster": 0.029362894594669342, "score_of_first_answer": 0.029362894594669342, "max_score_of_first_answer": 0.1265392154455185, "min_score_of_first_answer": 0.006813537795096636, "avg_score": 0.029362894594669342, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05986283882521093, "std_of_first_answer": 0.05986283882521093, "avg_entropy_of_first_answer": 5.372877836227417, "lex_sim": 1.0}
{"question": "where does the highway of heroes start and end", "answer": ["The Highway of Heroes begins in Toronto in Markham Road and ends at Trinity Square in Bowmanville and Trenton, Ont.It travels along Highway 401 and is marked by dozens of roadside tributes"], "scores": [[0.05632195249199867, 0.1815873086452484, 0.947329580783844, 0.9953445792198181, 0.999025821685791, 0.1371055394411087, 0.48298022150993347, 0.17357993125915527, 0.00931156799197197, 0.032895930111408234, 0.9972938895225525, 0.18505620956420898, 0.4943714141845703, 0.6036403179168701, 0.4075044095516205, 0.7344580292701721, 0.07727400213479996, 0.1713978350162506, 0.7038161754608154, 0.0423247255384922, 0.9977914094924927, 0.9988119602203369, 0.01361548900604248, 0.030139483511447906, 0.9065157771110535, 0.9428525567054749, 0.15275485813617706, 0.004095382988452911, 0.7064659595489502, 0.0004503351519815624, 0.014333270490169525, 0.980922520160675, 0.15383979678153992, 0.43754661083221436, 0.9895177483558655, 0.8979883193969727, 0.9899853467941284, 0.9977351427078247, 0.16613420844078064, 0.06001416966319084, 0.038401100784540176, 0.6457629799842834, 0.009497459977865219, 0.9980809688568115, 0.9997643828392029, 0.9887171983718872, 0.026549238711595535, 0.8803271055221558, 0.1261347383260727, 0.9739186763763428]], "normalized_score": [0.20824165642261505], "tokens": [["\u2581The", "\u2581Highway", "\u2581of", "\u2581Hero", "es", "\u2581begins", "\u2581in", "\u2581Toronto", "\u2581in", "\u2581Mark", "ham", "\u2581Road", "\u2581and", "\u2581ends", "\u2581at", "\u2581Tr", "inity", "\u2581Square", "\u2581in", "\u2581Bow", "man", "ville", "\u2581and", "\u2581Tr", "ent", "on", ",", "\u2581Ont", ".", "It", "\u2581travel", "s", "\u2581along", "\u2581Highway", "\u2581", "4", "0", "1", "\u2581and", "\u2581is", "\u2581marked", "\u2581by", "\u2581do", "z", "ens", "\u2581of", "\u2581road", "side", "\u2581t", "ributes"]], "entropy": [[4.5614824295043945, 4.500092029571533, 0.3595828115940094, 0.03784334659576416, 0.009859470650553703, 2.913173198699951, 1.3257209062576294, 3.198153495788574, 1.574650526046753, 3.5616297721862793, 0.02461201138794422, 2.241279363632202, 2.1155192852020264, 2.420158863067627, 1.148627758026123, 1.429412841796875, 0.4164566397666931, 3.008439064025879, 1.5640323162078857, 2.4814767837524414, 0.019607912749052048, 0.00984935648739338, 1.826259970664978, 4.451153755187988, 0.439209520816803, 0.3776756823062897, 2.3227806091308594, 2.2546372413635254, 1.4501553773880005, 1.2561969757080078, 3.224154233932495, 0.1215352863073349, 3.0880651473999023, 2.5998711585998535, 0.09216228127479553, 0.45582446455955505, 0.06704000383615494, 0.020041197538375854, 3.1545746326446533, 3.7855358123779297, 3.9591267108917236, 1.160586953163147, 4.153573036193848, 0.018394773826003075, 0.0030522167216986418, 0.10832922160625458, 3.968055248260498, 0.5297853946685791, 2.2927653789520264, 0.12164413928985596]], "avg_entropy": [1.7244769321521745], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Glen Miller Road in Trenton", "score_of_biggest_cluster": 0.20824165642261505, "score_of_first_answer": 0.20824165642261505, "max_score_of_first_answer": 0.9997643828392029, "min_score_of_first_answer": 0.0004503351519815624, "avg_score": 0.20824165642261505, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4085443832510625, "std_of_first_answer": 0.4085443832510625, "avg_entropy_of_first_answer": 1.7244769321521745, "lex_sim": 1.0}
{"question": "when did the first ninja turtles come out", "answer": ["November 12, 2007"], "scores": [[0.009578163735568523, 0.972332239151001, 0.3960524797439575, 0.06168447434902191, 0.7118657827377319, 0.9917620420455933, 0.07371428608894348, 0.998489260673523, 0.2712084650993347, 0.6544094085693359]], "normalized_score": [0.2705129086971283], "tokens": [["\u2581November", "\u2581", "1", "2", ",", "\u2581", "2", "0", "0", "7"]], "entropy": [[1.4060618877410889, 0.16114771366119385, 1.445273756980896, 1.7066807746887207, 0.6986579895019531, 0.05055517330765724, 0.26522621512413025, 0.014496811665594578, 0.5933520197868347, 1.2777538299560547]], "avg_entropy": [0.7619206172414124], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1984", "score_of_biggest_cluster": 0.2705129086971283, "score_of_first_answer": 0.2705129086971283, "max_score_of_first_answer": 0.998489260673523, "min_score_of_first_answer": 0.009578163735568523, "avg_score": 0.2705129086971283, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38179339755759684, "std_of_first_answer": 0.38179339755759684, "avg_entropy_of_first_answer": 0.7619206172414124, "lex_sim": 1.0}
{"question": "where is arachidonic acid found in the body", "answer": ["Cell membranes"], "scores": [[0.04275960475206375, 0.5884512066841125, 0.22030793130397797, 0.9935791492462158]], "normalized_score": [0.27242323756217957], "tokens": [["\u2581Cell", "\u2581memb", "ran", "es"]], "entropy": [[4.8780364990234375, 1.1960937976837158, 0.540773868560791, 0.04824201390147209]], "avg_entropy": [1.665786544792354], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "brain", "score_of_biggest_cluster": 0.27242323756217957, "score_of_first_answer": 0.27242323756217957, "max_score_of_first_answer": 0.9935791492462158, "min_score_of_first_answer": 0.04275960475206375, "avg_score": 0.27242323756217957, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.364945803889316, "std_of_first_answer": 0.364945803889316, "avg_entropy_of_first_answer": 1.665786544792354, "lex_sim": 1.0}
{"question": "bible verse taking the lord's name in vain", "answer": ["James1.3: \"Is any one of you sick?"], "scores": [[0.007618997246026993, 0.000923440617043525, 0.010387889109551907, 0.009022106416523457, 0.07853300869464874, 0.09667502343654633, 0.004357445985078812, 0.5148393511772156, 0.46119943261146545, 0.777288556098938, 0.9989874958992004, 0.13226079940795898, 0.9818795919418335, 0.786950945854187, 0.01080529484897852, 0.9220300316810608, 0.8671190738677979, 0.9871440529823303, 0.9999947547912598, 0.9920680522918701, 0.997884213924408, 0.9936714172363281, 0.9882189035415649, 0.9800365567207336, 0.9948434829711914, 0.9943186640739441, 0.9758275747299194, 0.9812672138214111, 0.9998552799224854, 0.9775332808494568, 0.9978787899017334, 0.9935854077339172, 0.9907160997390747, 0.9989140033721924, 0.9978651404380798, 0.9978031516075134, 0.9963877201080322, 0.9856382012367249, 0.41933101415634155, 1.5587888810841832e-06, 0.004971211310476065, 0.009926415048539639, 0.055315133184194565, 0.21046899259090424, 0.9988666772842407, 0.008426050655543804, 0.0005273492424748838]], "normalized_score": [0.17734135687351227], "tokens": [["\u2581James", "1", ".", "3", ":", "\u2581\"", "Is", "\u2581any", "\u2581one", "\u2581of", "\u2581you", "\u2581sick", "?", "\u2581He", "\u2581must", "\u2581call", "\u2581the", "\u2581el", "ders", "\u2581of", "\u2581the", "\u2581church", "\u2581to", "\u2581pray", "\u2581over", "\u2581him", "\u2581and", "\u2581an", "oint", "\u2581him", "\u2581with", "\u2581oil", "\u2581in", "\u2581the", "\u2581name", "\u2581of", "\u2581the", "\u2581Lord", ".\"", "\u2581Ask", "\u2581it", "\u2581like", "\u2581it", "'", "s", "\u2581real", "\u2581\""]], "entropy": [[3.6446328163146973, 0.3893595039844513, 0.8218199014663696, 1.8521931171417236, 2.3016600608825684, 3.134455442428589, 2.5430471897125244, 1.7083170413970947, 1.413630485534668, 0.713008463382721, 0.01167249958962202, 1.6419904232025146, 0.14077019691467285, 0.9172582030296326, 0.14475058019161224, 0.3714626431465149, 0.6078366041183472, 0.09778980910778046, 8.779106428846717e-05, 0.07201941311359406, 0.021193746477365494, 0.04780714958906174, 0.10628481209278107, 0.12575343251228333, 0.04624699056148529, 0.04765640199184418, 0.18622258305549622, 0.16638796031475067, 0.0020304496865719557, 0.18343202769756317, 0.01898052915930748, 0.04549111798405647, 0.08391823619604111, 0.012587133795022964, 0.018091551959514618, 0.024209342896938324, 0.03478681296110153, 0.0978056788444519, 1.4137158393859863, 0.4284975528717041, 3.5427706241607666, 3.8416948318481445, 2.387171983718872, 1.728895664215088, 0.012593751773238182, 2.6338119506835938, 2.73848819732666]], "avg_entropy": [0.9047720965847591], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Exodus 20:7", "score_of_biggest_cluster": 0.17734135687351227, "score_of_first_answer": 0.17734135687351227, "max_score_of_first_answer": 0.9999947547912598, "min_score_of_first_answer": 1.5587888810841832e-06, "avg_score": 0.17734135687351227, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43847811666358816, "std_of_first_answer": 0.43847811666358816, "avg_entropy_of_first_answer": 0.9047720965847591, "lex_sim": 1.0}
{"question": "where does the last name andersen originate from", "answer": ["Denmark"], "scores": [[0.5275073051452637, 0.999173104763031]], "normalized_score": [0.7259966135025024], "tokens": [["\u2581Den", "mark"]], "entropy": [[2.868591547012329, 0.009477261453866959]], "avg_entropy": [1.439034404233098], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Danish", "score_of_biggest_cluster": 0.7259966135025024, "score_of_first_answer": 0.7259966135025024, "max_score_of_first_answer": 0.999173104763031, "min_score_of_first_answer": 0.5275073051452637, "avg_score": 0.7259966135025024, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23583289980888367, "std_of_first_answer": 0.23583289980888367, "avg_entropy_of_first_answer": 1.439034404233098, "lex_sim": 1.0}
{"question": "what is a whistle blower suit under the false claims act", "answer": ["A whistleblower suit is an attempt by a government employee to inform the public of an alleged misconduct by his or her employer."], "scores": [[0.18071536719799042, 0.10112659633159637, 0.9976498484611511, 0.6303274035453796, 0.9627671837806702, 0.5001521110534668, 0.5937873721122742, 0.09662941098213196, 0.01245916448533535, 0.36690282821655273, 0.4815831482410431, 0.07851771265268326, 0.7291573286056519, 0.6261199712753296, 0.04051331430673599, 0.5246723294258118, 0.5071244835853577, 0.39676234126091003, 0.059430357068777084, 0.22029365599155426, 0.9358150959014893, 0.10052922368049622, 0.3444471061229706, 0.9993682503700256, 0.2997232973575592, 0.12233016639947891, 0.7359842658042908, 0.9949363470077515, 0.7858342528343201, 0.9533184170722961, 0.4943505525588989, 0.03263536095619202, 0.6743980050086975, 0.9983224272727966, 0.953175961971283, 0.990707278251648, 0.0034583418164402246, 0.49672284722328186, 0.006523135118186474, 0.9955788254737854, 0.994803249835968, 0.896224856376648, 0.06477349996566772, 0.0812232717871666, 0.0053156292997300625, 0.34689342975616455, 0.004452778957784176, 0.0033787586726248264, 0.9889904260635376, 0.6497930288314819]], "normalized_score": [0.2284732311964035], "tokens": [["\u2581A", "\u2581wh", "ist", "leb", "lower", "\u2581suit", "\u2581is", "\u2581an", "\u2581attempt", "\u2581by", "\u2581a", "\u2581government", "\u2581employee", "\u2581to", "\u2581inform", "\u2581the", "\u2581public", "\u2581of", "\u2581an", "\u2581alleg", "ed", "\u2581mis", "con", "duct", "\u2581by", "\u2581his", "\u2581or", "\u2581her", "\u2581employ", "er", ".", "\u2581A", "\u2581wh", "ist", "leb", "lower", "\u2581could", "\u2581be", "\u2581ret", "ali", "ated", "\u2581against", ",", "\u2581but", "\u2581protected", "\u2581by", "\u2581The", "\u2581United", "\u2581States", "\u2581False"]], "entropy": [[4.365515232086182, 3.4406962394714355, 0.020127231255173683, 0.6674981117248535, 0.16020402312278748, 1.8862148523330688, 1.9055755138397217, 2.2013096809387207, 1.880088448524475, 1.045001745223999, 1.413244366645813, 2.890529155731201, 1.318211317062378, 1.1986055374145508, 3.5849642753601074, 2.356963634490967, 1.8575239181518555, 1.6689634323120117, 3.6127877235412598, 2.6902828216552734, 0.28571319580078125, 3.101966619491577, 1.74770188331604, 0.007250748574733734, 2.7807693481445312, 2.396873712539673, 0.9715352654457092, 0.05128976330161095, 1.0678393840789795, 0.22172807157039642, 2.0701723098754883, 2.5230767726898193, 1.911877155303955, 0.015622617676854134, 0.19527585804462433, 0.05341249331831932, 1.9469517469406128, 2.659036874771118, 3.3368804454803467, 0.036993298679590225, 0.04096958041191101, 0.5502397418022156, 2.0846478939056396, 3.953591823577881, 3.8259360790252686, 1.7740578651428223, 2.4960193634033203, 1.0299701690673828, 0.08556977659463882, 1.9328951835632324]], "avg_entropy": [1.7064034460484983], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "qui tam", "score_of_biggest_cluster": 0.2284732311964035, "score_of_first_answer": 0.2284732311964035, "max_score_of_first_answer": 0.9993682503700256, "min_score_of_first_answer": 0.0033787586726248264, "avg_score": 0.2284732311964035, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3674652614021331, "std_of_first_answer": 0.3674652614021331, "avg_entropy_of_first_answer": 1.7064034460484983, "lex_sim": 1.0}
{"question": "who has the most receiving yards in the nfl history", "answer": ["Jerry Rice"], "scores": [[0.8613848686218262, 0.972743570804596, 0.9997370839118958]], "normalized_score": [0.9426717162132263], "tokens": [["\u2581Jerry", "\u2581R", "ice"]], "entropy": [[0.9405220150947571, 0.23812532424926758, 0.002976809861138463]], "avg_entropy": [0.39387471640172106], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jerry Rice", "score_of_biggest_cluster": 0.9426717162132263, "score_of_first_answer": 0.9426717162132263, "max_score_of_first_answer": 0.9997370839118958, "min_score_of_first_answer": 0.8613848686218262, "avg_score": 0.9426717162132263, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.05988020056503053, "std_of_first_answer": 0.05988020056503053, "avg_entropy_of_first_answer": 0.39387471640172106, "lex_sim": 1.0}
{"question": "in the dynastic cycle what is the right to rule called", "answer": ["dynastic"], "scores": [[0.05510232597589493, 0.9364712834358215, 0.9711238741874695]], "normalized_score": [0.3686772584915161], "tokens": [["\u2581d", "yn", "astic"]], "entropy": [[4.775171756744385, 0.33232957124710083, 0.22185999155044556]], "avg_entropy": [1.7764537731806438], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Mandate of Heaven", "score_of_biggest_cluster": 0.3686772584915161, "score_of_first_answer": 0.3686772584915161, "max_score_of_first_answer": 0.9711238741874695, "min_score_of_first_answer": 0.05510232597589493, "avg_score": 0.3686772584915161, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4238851414144755, "std_of_first_answer": 0.4238851414144755, "avg_entropy_of_first_answer": 1.7764537731806438, "lex_sim": 1.0}
{"question": "who played ashley on the young and the restless", "answer": ["Jessica Collins"], "scores": [[0.08378072082996368, 0.8452886343002319, 0.8599532246589661]], "normalized_score": [0.3934365212917328], "tokens": [["\u2581Jess", "ica", "\u2581Collins"]], "entropy": [[4.296843528747559, 0.7242177724838257, 0.942438542842865]], "avg_entropy": [1.987833281358083], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brenda Epperson", "score_of_biggest_cluster": 0.3934365212917328, "score_of_first_answer": 0.3934365212917328, "max_score_of_first_answer": 0.8599532246589661, "min_score_of_first_answer": 0.08378072082996368, "avg_score": 0.3934365212917328, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3624841924811517, "std_of_first_answer": 0.3624841924811517, "avg_entropy_of_first_answer": 1.987833281358083, "lex_sim": 1.0}
{"question": "what was the first kind of vertebrate to exist on earth", "answer": ["jawless fish"], "scores": [[0.04753002151846886, 0.9935216307640076, 0.7444711327552795, 0.8378532528877258]], "normalized_score": [0.41427648067474365], "tokens": [["\u2581j", "aw", "less", "\u2581fish"]], "entropy": [[4.119550704956055, 0.049936551600694656, 0.7813461422920227, 0.7021869421005249]], "avg_entropy": [1.4132550852373242], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "believed to be the Myllokunmingia", "score_of_biggest_cluster": 0.41427648067474365, "score_of_first_answer": 0.41427648067474365, "max_score_of_first_answer": 0.9935216307640076, "min_score_of_first_answer": 0.04753002151846886, "avg_score": 0.41427648067474365, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3623031243073926, "std_of_first_answer": 0.3623031243073926, "avg_entropy_of_first_answer": 1.4132550852373242, "lex_sim": 1.0}
{"question": "which player won the pga championship by the most shots", "answer": ["Jordan Spieth, 18 in 2019"], "scores": [[0.03774763271212578, 0.9691904783248901, 0.9896441698074341, 0.9999312162399292, 0.012766922824084759, 0.5768555998802185, 0.6260454058647156, 0.1783144176006317, 0.005642864387482405, 0.9267033934593201, 0.9047885537147522, 0.9992915391921997, 0.996817946434021, 0.001576913520693779]], "normalized_score": [0.20433495938777924], "tokens": [["\u2581Jordan", "\u2581S", "pi", "eth", ",", "\u2581", "1", "8", "\u2581in", "\u2581", "2", "0", "1", "9"]], "entropy": [[4.3092474937438965, 0.19199246168136597, 0.10052503645420074, 0.0009386185556650162, 0.6979104280471802, 2.3560824394226074, 1.2963167428970337, 1.949948787689209, 1.5815808773040771, 0.4936724305152893, 0.32377076148986816, 0.008110562339425087, 0.03229041397571564, 0.46240106225013733]], "avg_entropy": [0.9860562940261194], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rory McIlroy", "score_of_biggest_cluster": 0.20433495938777924, "score_of_first_answer": 0.20433495938777924, "max_score_of_first_answer": 0.9999312162399292, "min_score_of_first_answer": 0.001576913520693779, "avg_score": 0.20433495938777924, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42374122277137566, "std_of_first_answer": 0.42374122277137566, "avg_entropy_of_first_answer": 0.9860562940261194, "lex_sim": 1.0}
{"question": "when does the miz and maryse show start", "answer": ["January 31 at 9:30 p.m."], "scores": [[0.050406452268362045, 0.9683742523193359, 0.07413050532341003, 0.4139978885650635, 0.005151132121682167, 0.9847207069396973, 0.20239229500293732, 0.18371227383613586, 0.3117460012435913, 0.988959789276123, 0.16075190901756287, 0.9949671626091003, 0.9969000816345215, 0.924643874168396]], "normalized_score": [0.27310624718666077], "tokens": [["\u2581January", "\u2581", "3", "1", "\u2581at", "\u2581", "9", ":", "3", "0", "\u2581p", ".", "m", "."]], "entropy": [[3.4640469551086426, 0.18486665189266205, 1.774689793586731, 1.3882662057876587, 0.9937736988067627, 0.11610931158065796, 1.189131259918213, 2.1477439403533936, 0.7709976434707642, 0.07891260832548141, 2.377530574798584, 0.045940447598695755, 0.02830696478486061, 0.40056535601615906]], "avg_entropy": [1.068634386573519], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2018", "score_of_biggest_cluster": 0.27310624718666077, "score_of_first_answer": 0.27310624718666077, "max_score_of_first_answer": 0.9969000816345215, "min_score_of_first_answer": 0.005151132121682167, "avg_score": 0.27310624718666077, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4084537553732315, "std_of_first_answer": 0.4084537553732315, "avg_entropy_of_first_answer": 1.068634386573519, "lex_sim": 1.0}
{"question": "who wrote the song rainy days and mondays", "answer": ["Paul Williams"], "scores": [[0.30360013246536255, 0.5998366475105286]], "normalized_score": [0.4267440438270569], "tokens": [["\u2581Paul", "\u2581Williams"]], "entropy": [[3.5910537242889404, 2.3639566898345947]], "avg_entropy": [2.9775052070617676], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paul Williams", "score_of_biggest_cluster": 0.4267440438270569, "score_of_first_answer": 0.4267440438270569, "max_score_of_first_answer": 0.5998366475105286, "min_score_of_first_answer": 0.30360013246536255, "avg_score": 0.4267440438270569, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.148118257522583, "std_of_first_answer": 0.148118257522583, "avg_entropy_of_first_answer": 2.9775052070617676, "lex_sim": 1.0}
{"question": "when did the vikings first sail and reach britain", "answer": ["793 AD"], "scores": [[0.5709428191184998, 0.5323589444160461, 0.8824853897094727, 0.8096937537193298, 0.24892017245292664]], "normalized_score": [0.5579266548156738], "tokens": [["\u2581", "7", "9", "3", "\u2581AD"]], "entropy": [[2.4492509365081787, 1.1879568099975586, 0.5206276178359985, 0.7189915776252747, 1.5932692289352417]], "avg_entropy": [1.2940192341804504], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "6 January 793", "score_of_biggest_cluster": 0.5579266548156738, "score_of_first_answer": 0.5579266548156738, "max_score_of_first_answer": 0.8824853897094727, "min_score_of_first_answer": 0.24892017245292664, "avg_score": 0.5579266548156738, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2245225387569471, "std_of_first_answer": 0.2245225387569471, "avg_entropy_of_first_answer": 1.2940192341804504, "lex_sim": 1.0}
{"question": "who won the mens figure skating in 2018 olympics", "answer": ["Yuzuru Hanyu"], "scores": [[0.8538956642150879, 0.9878360629081726, 0.9848399758338928, 0.9871764779090881, 0.9952326416969299, 0.984419047832489]], "normalized_score": [0.9641822576522827], "tokens": [["\u2581Y", "uz", "uru", "\u2581H", "any", "u"]], "entropy": [[1.0060703754425049, 0.09181799739599228, 0.1046748012304306, 0.1061418205499649, 0.05060552805662155, 0.11662706732749939]], "avg_entropy": [0.2459895983338356], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yuzuru Hanyu", "score_of_biggest_cluster": 0.9641822576522827, "score_of_first_answer": 0.9641822576522827, "max_score_of_first_answer": 0.9952326416969299, "min_score_of_first_answer": 0.8538956642150879, "avg_score": 0.9641822576522827, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.05006706425020534, "std_of_first_answer": 0.05006706425020534, "avg_entropy_of_first_answer": 0.2459895983338356, "lex_sim": 1.0}
{"question": "who has scored the most tries in rugby union", "answer": ["Jonah Lomu"], "scores": [[0.30521199107170105, 0.730823814868927, 0.9607914686203003, 0.9955294728279114, 0.9993324875831604]], "normalized_score": [0.7341105937957764], "tokens": [["\u2581Jon", "ah", "\u2581L", "om", "u"]], "entropy": [[3.7630958557128906, 0.7023709416389465, 0.2849193811416626, 0.040826212614774704, 0.007970943115651608]], "avg_entropy": [0.9598366668447852], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Daisuke Ohata", "score_of_biggest_cluster": 0.7341105937957764, "score_of_first_answer": 0.7341105937957764, "max_score_of_first_answer": 0.9993324875831604, "min_score_of_first_answer": 0.30521199107170105, "avg_score": 0.7341105937957764, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.265859292234435, "std_of_first_answer": 0.265859292234435, "avg_entropy_of_first_answer": 0.9598366668447852, "lex_sim": 1.0}
{"question": "this inventor co-created the film fred ott\u2019s sneeze", "answer": ["Thomas Edison"], "scores": [[0.2201370894908905, 0.8504906892776489, 0.9973415732383728]], "normalized_score": [0.5715693235397339], "tokens": [["\u2581Thomas", "\u2581Ed", "ison"]], "entropy": [[4.130194664001465, 0.716033399105072, 0.024282895028591156]], "avg_entropy": [1.6235036527117093], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fred Ott", "score_of_biggest_cluster": 0.5715693235397339, "score_of_first_answer": 0.5715693235397339, "max_score_of_first_answer": 0.9973415732383728, "min_score_of_first_answer": 0.2201370894908905, "avg_score": 0.5715693235397339, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33713789668676153, "std_of_first_answer": 0.33713789668676153, "avg_entropy_of_first_answer": 1.6235036527117093, "lex_sim": 1.0}
{"question": "term limits for house of representatives and senate", "answer": ["Six congressional terms"], "scores": [[0.04329880699515343, 0.0007459419430233538, 0.8543039560317993, 0.9992357492446899, 0.871467113494873]], "normalized_score": [0.11916327476501465], "tokens": [["\u2581Six", "\u2581con", "gression", "al", "\u2581terms"]], "entropy": [[3.097250461578369, 1.8630250692367554, 0.49181094765663147, 0.007466388866305351, 0.7081895470619202]], "avg_entropy": [1.2335484828799963], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Unlimited six-year terms", "score_of_biggest_cluster": 0.11916327476501465, "score_of_first_answer": 0.11916327476501465, "max_score_of_first_answer": 0.9992357492446899, "min_score_of_first_answer": 0.0007459419430233538, "avg_score": 0.11916327476501465, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43728898635865965, "std_of_first_answer": 0.43728898635865965, "avg_entropy_of_first_answer": 1.2335484828799963, "lex_sim": 1.0}
{"question": "when did ga state get a football team", "answer": ["1990"], "scores": [[0.8074581027030945, 0.9671064615249634, 0.7477278709411621, 0.0510813370347023, 0.237991064786911]], "normalized_score": [0.3717338442802429], "tokens": [["\u2581", "1", "9", "9", "0"]], "entropy": [[1.2564616203308105, 0.15641748905181885, 0.5848022699356079, 1.846254825592041, 1.8102535009384155]], "avg_entropy": [1.1308379411697387], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2010", "score_of_biggest_cluster": 0.3717338442802429, "score_of_first_answer": 0.3717338442802429, "max_score_of_first_answer": 0.9671064615249634, "min_score_of_first_answer": 0.0510813370347023, "avg_score": 0.3717338442802429, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3535180519024117, "std_of_first_answer": 0.3535180519024117, "avg_entropy_of_first_answer": 1.1308379411697387, "lex_sim": 1.0}
{"question": "what type of political system does el salvador have", "answer": ["Republic"], "scores": [[0.030982794240117073]], "normalized_score": [0.030982794240117073], "tokens": [["\u2581Republic"]], "entropy": [[4.573158264160156]], "avg_entropy": [4.573158264160156], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "a presidential representative democratic republic", "score_of_biggest_cluster": 0.030982794240117073, "score_of_first_answer": 0.030982794240117073, "max_score_of_first_answer": 0.030982794240117073, "min_score_of_first_answer": 0.030982794240117073, "avg_score": 0.030982794240117073, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.573158264160156, "lex_sim": 1.0}
{"question": "who were the first settlers in san antonio", "answer": ["Spanish Missions in Texas"], "scores": [[0.43807485699653625, 0.00477049732580781, 0.9946891069412231, 0.023466721177101135, 0.24908892810344696]], "normalized_score": [0.1039731577038765], "tokens": [["\u2581Spanish", "\u2581Miss", "ions", "\u2581in", "\u2581Texas"]], "entropy": [[3.288877248764038, 2.057349443435669, 0.038501083850860596, 0.9602476358413696, 1.7823622226715088]], "avg_entropy": [1.6254675269126893], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Spanish", "score_of_biggest_cluster": 0.1039731577038765, "score_of_first_answer": 0.1039731577038765, "max_score_of_first_answer": 0.9946891069412231, "min_score_of_first_answer": 0.00477049732580781, "avg_score": 0.1039731577038765, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3630573236598238, "std_of_first_answer": 0.3630573236598238, "avg_entropy_of_first_answer": 1.6254675269126893, "lex_sim": 1.0}
{"question": "jawaharlal nehru centre for advanced scientific research jakkur campus", "answer": ["Hosur Rd, Jawaharlal Nehru Salai, Electronic City, Jakkur Hills, Jakkur Village"], "scores": [[0.003162296488881111, 0.26521623134613037, 0.6831774115562439, 0.05862447991967201, 0.998097836971283, 0.6364298462867737, 0.030886715278029442, 0.21995438635349274, 0.9986166954040527, 0.9869014620780945, 0.8205362558364868, 0.9990696310997009, 0.9809594750404358, 0.9998834133148193, 0.9999111890792847, 0.004882352892309427, 0.9754961133003235, 0.8393092155456543, 0.0033104573376476765, 0.6624394059181213, 0.9832774996757507, 0.7901995182037354, 0.03634343668818474, 0.9801367521286011, 0.0007605766295455396, 0.7848448157310486, 0.09432356059551239, 0.9826745390892029, 0.013023363426327705]], "normalized_score": [0.2093079537153244], "tokens": [["\u2581H", "os", "ur", "\u2581R", "d", ",", "\u2581J", "aw", "ah", "ar", "l", "al", "\u2581Ne", "h", "ru", "\u2581Sal", "ai", ",", "\u2581Electron", "ic", "\u2581City", ",", "\u2581Jak", "kur", "\u2581Hills", ",", "\u2581Jak", "kur", "\u2581Village"]], "entropy": [[4.620352268218994, 3.633890151977539, 1.1356358528137207, 1.4891464710235596, 0.01816457509994507, 1.42173171043396, 2.689138889312744, 3.049760341644287, 0.015436945483088493, 0.08551107347011566, 0.6183037757873535, 0.009180130437016487, 0.1815858781337738, 0.0013941362267360091, 0.0012177301105111837, 3.608938694000244, 0.20770539343357086, 0.6891000270843506, 3.1261603832244873, 0.6873388886451721, 0.10123494267463684, 1.065742015838623, 2.761880397796631, 0.11620548367500305, 2.019463539123535, 0.9092180132865906, 3.044699192047119, 0.11092697829008102, 2.181450128555298]], "avg_entropy": [1.3655349657878857], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jakkur, Bangalore, India", "score_of_biggest_cluster": 0.2093079537153244, "score_of_first_answer": 0.2093079537153244, "max_score_of_first_answer": 0.9999111890792847, "min_score_of_first_answer": 0.0007605766295455396, "avg_score": 0.2093079537153244, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4178920115697284, "std_of_first_answer": 0.4178920115697284, "avg_entropy_of_first_answer": 1.3655349657878857, "lex_sim": 1.0}
{"question": "when was the last time america hosted the summer olympics", "answer": ["summer of 1932"], "scores": [[0.0005529621266759932, 0.3700612783432007, 0.9874168634414673, 0.8949304223060608, 0.99861741065979, 0.04672896862030029, 0.9938468337059021]], "normalized_score": [0.18827593326568604], "tokens": [["\u2581summer", "\u2581of", "\u2581", "1", "9", "3", "2"]], "entropy": [[1.6635533571243286, 1.6724541187286377, 0.10594978928565979, 0.3901990056037903, 0.013101192191243172, 0.3253909945487976, 0.04754303768277168]], "avg_entropy": [0.6025987850236041], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Salt Lake City", "score_of_biggest_cluster": 0.18827593326568604, "score_of_first_answer": 0.18827593326568604, "max_score_of_first_answer": 0.99861741065979, "min_score_of_first_answer": 0.0005529621266759932, "avg_score": 0.18827593326568604, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.425640884175133, "std_of_first_answer": 0.425640884175133, "avg_entropy_of_first_answer": 0.6025987850236041, "lex_sim": 1.0}
{"question": "this type of nuclear decay effectively turns a proton into a neutron", "answer": ["Bethe-Weizs\u00e4cker process"], "scores": [[0.010466216132044792, 0.05739259719848633, 0.31400159001350403, 0.22024045884609222, 0.774071216583252, 0.9643723368644714, 0.10453850030899048, 0.9992595314979553, 0.1094919741153717]], "normalized_score": [0.1920061856508255], "tokens": [["\u2581Bet", "he", "-", "We", "iz", "s", "\u00e4", "cker", "\u2581process"]], "entropy": [[2.9513421058654785, 2.298442840576172, 3.331125259399414, 1.9214073419570923, 0.7977069020271301, 0.30266791582107544, 1.373811960220337, 0.006665884517133236, 1.6268229484558105]], "avg_entropy": [1.6233325732044048], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "beta decay", "score_of_biggest_cluster": 0.1920061856508255, "score_of_first_answer": 0.1920061856508255, "max_score_of_first_answer": 0.9992595314979553, "min_score_of_first_answer": 0.010466216132044792, "avg_score": 0.1920061856508255, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.37974240104109774, "std_of_first_answer": 0.37974240104109774, "avg_entropy_of_first_answer": 1.6233325732044048, "lex_sim": 1.0}
{"question": "where do the signals for apoptosis come from", "answer": ["the mitochondria"], "scores": [[0.044880595058202744, 0.06470225751399994, 0.9534764885902405, 0.9951499104499817, 0.8724337816238403]], "normalized_score": [0.2993517518043518], "tokens": [["\u2581the", "\u2581mit", "och", "ond", "ria"]], "entropy": [[4.680446624755859, 3.2328195571899414, 0.29706740379333496, 0.04354022443294525, 0.5138771533966064]], "avg_entropy": [1.7535501927137376], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "from other cells", "score_of_biggest_cluster": 0.2993517518043518, "score_of_first_answer": 0.2993517518043518, "max_score_of_first_answer": 0.9951499104499817, "min_score_of_first_answer": 0.044880595058202744, "avg_score": 0.2993517518043518, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4356715263063197, "std_of_first_answer": 0.4356715263063197, "avg_entropy_of_first_answer": 1.7535501927137376, "lex_sim": 1.0}
{"question": "who is nebula on guardians of the galaxy", "answer": ["Charlie"], "scores": [[0.011537407524883747]], "normalized_score": [0.011537408456206322], "tokens": [["\u2581Charlie"]], "entropy": [[2.7359848022460938]], "avg_entropy": [2.7359848022460938], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Karen Gillan", "score_of_biggest_cluster": 0.011537408456206322, "score_of_first_answer": 0.011537408456206322, "max_score_of_first_answer": 0.011537407524883747, "min_score_of_first_answer": 0.011537407524883747, "avg_score": 0.011537408456206322, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.7359848022460938, "lex_sim": 1.0}
{"question": "who plays captain phasma in star wars the force awakens", "answer": ["Gwendoline Christie"], "scores": [[0.9342901110649109, 0.9974505305290222, 0.9433491826057434, 0.9904380440711975, 0.9508351683616638]], "normalized_score": [0.9629320502281189], "tokens": [["\u2581G", "wend", "oline", "\u2581Christ", "ie"]], "entropy": [[0.5440267324447632, 0.02152363955974579, 0.265728622674942, 0.08846500515937805, 0.24309541285037994]], "avg_entropy": [0.2325678825378418], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gwendoline Christie", "score_of_biggest_cluster": 0.9629320502281189, "score_of_first_answer": 0.9629320502281189, "max_score_of_first_answer": 0.9974505305290222, "min_score_of_first_answer": 0.9342901110649109, "avg_score": 0.9629320502281189, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.025681546752983453, "std_of_first_answer": 0.025681546752983453, "avg_entropy_of_first_answer": 0.2325678825378418, "lex_sim": 1.0}
{"question": "how many goals did mikel score for chelsea", "answer": ["27"], "scores": [[0.5520718693733215, 0.17667396366596222, 0.03887161985039711]], "normalized_score": [0.15593142807483673], "tokens": [["\u2581", "2", "7"]], "entropy": [[2.3000266551971436, 1.915331482887268, 2.244988441467285]], "avg_entropy": [2.1534488598505654], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "6", "score_of_biggest_cluster": 0.15593142807483673, "score_of_first_answer": 0.15593142807483673, "max_score_of_first_answer": 0.5520718693733215, "min_score_of_first_answer": 0.03887161985039711, "avg_score": 0.15593142807483673, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21686851359007747, "std_of_first_answer": 0.21686851359007747, "avg_entropy_of_first_answer": 2.1534488598505654, "lex_sim": 1.0}
{"question": "how many players in football hall of fame", "answer": ["265"], "scores": [[0.8220555782318115, 0.5276117324829102, 0.2913877069950104, 0.1267266571521759]], "normalized_score": [0.35574495792388916], "tokens": [["\u2581", "2", "6", "5"]], "entropy": [[1.3688111305236816, 1.4603277444839478, 1.9998257160186768, 2.2648916244506836]], "avg_entropy": [1.7734640538692474], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "318", "score_of_biggest_cluster": 0.35574495792388916, "score_of_first_answer": 0.35574495792388916, "max_score_of_first_answer": 0.8220555782318115, "min_score_of_first_answer": 0.1267266571521759, "avg_score": 0.35574495792388916, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26165480259400836, "std_of_first_answer": 0.26165480259400836, "avg_entropy_of_first_answer": 1.7734640538692474, "lex_sim": 1.0}
{"question": "how many world series wins does the dodgers have", "answer": ["201"], "scores": [[0.6351212859153748, 0.08274240046739578, 0.09521123021841049, 0.13000820577144623]], "normalized_score": [0.15970221161842346], "tokens": [["\u2581", "2", "0", "1"]], "entropy": [[2.1307549476623535, 1.9557769298553467, 2.224139451980591, 1.2123920917510986]], "avg_entropy": [1.8807658553123474], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "6", "score_of_biggest_cluster": 0.15970221161842346, "score_of_first_answer": 0.15970221161842346, "max_score_of_first_answer": 0.6351212859153748, "min_score_of_first_answer": 0.08274240046739578, "avg_score": 0.15970221161842346, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2312148463331745, "std_of_first_answer": 0.2312148463331745, "avg_entropy_of_first_answer": 1.8807658553123474, "lex_sim": 1.0}
{"question": "how many hoops are used in a game of croquet", "answer": ["4"], "scores": [[0.310625284910202, 0.1037287563085556]], "normalized_score": [0.1795014590024948], "tokens": [["\u2581", "4"]], "entropy": [[2.8128457069396973, 2.07413911819458]], "avg_entropy": [2.4434924125671387], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "six", "score_of_biggest_cluster": 0.1795014590024948, "score_of_first_answer": 0.1795014590024948, "max_score_of_first_answer": 0.310625284910202, "min_score_of_first_answer": 0.1037287563085556, "avg_score": 0.1795014590024948, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10344826430082321, "std_of_first_answer": 0.10344826430082321, "avg_entropy_of_first_answer": 2.4434924125671387, "lex_sim": 1.0}
{"question": "who sang the song brother let me be your shelter", "answer": ["Natalie Grant"], "scores": [[0.007005373015999794, 0.9350609183311462, 0.5742647647857666]], "normalized_score": [0.15552295744419098], "tokens": [["\u2581Natal", "ie", "\u2581Grant"]], "entropy": [[5.724409103393555, 0.3078978955745697, 1.6337336301803589]], "avg_entropy": [2.5553468763828278], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Needtobreathe", "score_of_biggest_cluster": 0.15552295744419098, "score_of_first_answer": 0.15552295744419098, "max_score_of_first_answer": 0.9350609183311462, "min_score_of_first_answer": 0.007005373015999794, "avg_score": 0.15552295744419098, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3819895544171537, "std_of_first_answer": 0.3819895544171537, "avg_entropy_of_first_answer": 2.5553468763828278, "lex_sim": 1.0}
{"question": "points outside the production possibilities curve represent combinations of products that are", "answer": ["surplus"], "scores": [[0.0017458383226767182, 0.9184554219245911]], "normalized_score": [0.04004340246319771], "tokens": [["\u2581sur", "plus"]], "entropy": [[4.228509902954102, 0.40426525473594666]], "avg_entropy": [2.316387578845024], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "unattainable", "score_of_biggest_cluster": 0.04004340246319771, "score_of_first_answer": 0.04004340246319771, "max_score_of_first_answer": 0.9184554219245911, "min_score_of_first_answer": 0.0017458383226767182, "avg_score": 0.04004340246319771, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4583547918009572, "std_of_first_answer": 0.4583547918009572, "avg_entropy_of_first_answer": 2.316387578845024, "lex_sim": 1.0}
{"question": "what are three different types of hotel properties", "answer": ["Guest House, Boutique, and Spa Hotels"], "scores": [[0.00667424313724041, 0.9151583909988403, 0.19111819565296173, 0.8541392087936401, 0.05943958833813667, 0.9292527437210083, 0.994350254535675, 0.44371962547302246, 0.2601981461048126, 0.0322999432682991, 0.9864728450775146, 0.016955876722931862, 0.9988512992858887]], "normalized_score": [0.22319898009300232], "tokens": [["\u2581Gu", "est", "\u2581House", ",", "\u2581B", "out", "ique", ",", "\u2581and", "\u2581Sp", "a", "\u2581Hot", "els"]], "entropy": [[5.081897735595703, 0.4291256070137024, 3.383932113647461, 0.8224900960922241, 4.270133972167969, 0.3322127163410187, 0.038358964025974274, 1.4729948043823242, 4.0305094718933105, 4.632264614105225, 0.09834666550159454, 0.8597158193588257, 0.011762455105781555]], "avg_entropy": [1.958749618094701], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bed and breakfast", "score_of_biggest_cluster": 0.22319898009300232, "score_of_first_answer": 0.22319898009300232, "max_score_of_first_answer": 0.9988512992858887, "min_score_of_first_answer": 0.00667424313724041, "avg_score": 0.22319898009300232, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4164688281384539, "std_of_first_answer": 0.4164688281384539, "avg_entropy_of_first_answer": 1.958749618094701, "lex_sim": 1.0}
{"question": "when did this season of american idol start", "answer": ["January 7th"], "scores": [[0.35636910796165466, 0.9540706276893616, 0.1418127566576004, 0.5104649066925049]], "normalized_score": [0.3960868716239929], "tokens": [["\u2581January", "\u2581", "7", "th"]], "entropy": [[2.275106430053711, 0.24510791897773743, 1.2000575065612793, 0.9694708585739136]], "avg_entropy": [1.1724356785416603], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "March 11, 2018", "score_of_biggest_cluster": 0.3960868716239929, "score_of_first_answer": 0.3960868716239929, "max_score_of_first_answer": 0.9540706276893616, "min_score_of_first_answer": 0.1418127566576004, "avg_score": 0.3960868716239929, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2978548663187079, "std_of_first_answer": 0.2978548663187079, "avg_entropy_of_first_answer": 1.1724356785416603, "lex_sim": 1.0}
{"question": "when did the united kingdom entered world war 2", "answer": ["September 3, 1939"], "scores": [[0.631181538105011, 0.9737257957458496, 0.8131893873214722, 0.36458712816238403, 0.9841365814208984, 0.9998800754547119, 0.9996515512466431, 0.9998748302459717, 0.9992594122886658]], "normalized_score": [0.8260505795478821], "tokens": [["\u2581September", "\u2581", "3", ",", "\u2581", "1", "9", "3", "9"]], "entropy": [[1.479267954826355, 0.16122812032699585, 0.5815067887306213, 0.8777240514755249, 0.08596143871545792, 0.0013430407270789146, 0.003800717880949378, 0.0013949816348031163, 0.006738933734595776]], "avg_entropy": [0.355440669783598], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1939", "score_of_biggest_cluster": 0.8260505795478821, "score_of_first_answer": 0.8260505795478821, "max_score_of_first_answer": 0.9998800754547119, "min_score_of_first_answer": 0.36458712816238403, "avg_score": 0.8260505795478821, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.21246737107541322, "std_of_first_answer": 0.21246737107541322, "avg_entropy_of_first_answer": 0.355440669783598, "lex_sim": 1.0}
{"question": "when did they stop making the nissan xterra", "answer": ["2015"], "scores": [[0.6413982510566711, 0.9837369322776794, 0.9984379410743713, 0.9227912425994873, 0.452343612909317]], "normalized_score": [0.7655612826347351], "tokens": [["\u2581", "2", "0", "1", "5"]], "entropy": [[2.13063383102417, 0.10949120670557022, 0.016723811626434326, 0.28770893812179565, 1.2768594026565552]], "avg_entropy": [0.764283438026905], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2015", "score_of_biggest_cluster": 0.7655612826347351, "score_of_first_answer": 0.7655612826347351, "max_score_of_first_answer": 0.9984379410743713, "min_score_of_first_answer": 0.452343612909317, "avg_score": 0.7655612826347351, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.21644108223229028, "std_of_first_answer": 0.21644108223229028, "avg_entropy_of_first_answer": 0.764283438026905, "lex_sim": 1.0}
{"question": "when was the last time an american won the new york marathon", "answer": ["2015"], "scores": [[0.692192792892456, 0.6568078398704529, 0.9893172383308411, 0.3884207308292389, 0.07784038782119751]], "normalized_score": [0.42335206270217896], "tokens": [["\u2581", "2", "0", "1", "5"]], "entropy": [[2.1066536903381348, 0.7406416535377502, 0.09002038836479187, 0.6786212921142578, 1.810443639755249]], "avg_entropy": [1.0852761328220368], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2009", "score_of_biggest_cluster": 0.42335206270217896, "score_of_first_answer": 0.42335206270217896, "max_score_of_first_answer": 0.9893172383308411, "min_score_of_first_answer": 0.07784038782119751, "avg_score": 0.42335206270217896, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30759490479436796, "std_of_first_answer": 0.30759490479436796, "avg_entropy_of_first_answer": 1.0852761328220368, "lex_sim": 1.0}
{"question": "when will fairy tail final season be released", "answer": ["October 6"], "scores": [[0.06122231110930443, 0.8709626793861389, 0.044813260436058044]], "normalized_score": [0.1336919516324997], "tokens": [["\u2581October", "\u2581", "6"]], "entropy": [[3.783857822418213, 0.5514809489250183, 1.6656709909439087]], "avg_entropy": [2.0003365874290466], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2018", "score_of_biggest_cluster": 0.1336919516324997, "score_of_first_answer": 0.1336919516324997, "max_score_of_first_answer": 0.8709626793861389, "min_score_of_first_answer": 0.044813260436058044, "avg_score": 0.1336919516324997, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38564110883230057, "std_of_first_answer": 0.38564110883230057, "avg_entropy_of_first_answer": 2.0003365874290466, "lex_sim": 1.0}
{"question": "who wrote the text for jeanie with the light brown hair", "answer": ["Harriet Beecher Stowe"], "scores": [[0.0039009563624858856, 0.9958937168121338, 0.9870057106018066, 0.7348042130470276, 0.9981514811515808, 0.9998717308044434, 0.9602676033973694, 0.9966601133346558]], "normalized_score": [0.4772483706474304], "tokens": [["\u2581Har", "ri", "et", "\u2581Be", "e", "cher", "\u2581St", "owe"]], "entropy": [[3.001445770263672, 0.03879036754369736, 0.07887734472751617, 1.78938627243042, 0.018739771097898483, 0.0016304231248795986, 0.22166994214057922, 0.023983675986528397]], "avg_entropy": [0.6468154459143989], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stephen Foster", "score_of_biggest_cluster": 0.4772483706474304, "score_of_first_answer": 0.4772483706474304, "max_score_of_first_answer": 0.9998717308044434, "min_score_of_first_answer": 0.0039009563624858856, "avg_score": 0.4772483706474304, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32507362921683575, "std_of_first_answer": 0.32507362921683575, "avg_entropy_of_first_answer": 0.6468154459143989, "lex_sim": 1.0}
{"question": "when did brent barry won the dunk contest", "answer": ["2000"], "scores": [[0.8380675911903381, 0.453303724527359, 0.9972355961799622, 0.8604525923728943, 0.09392233192920685]], "normalized_score": [0.49795761704444885], "tokens": [["\u2581", "2", "0", "0", "0"]], "entropy": [[0.852763295173645, 0.7294913530349731, 0.02752682752907276, 0.4144750237464905, 2.087371349334717]], "avg_entropy": [0.8223255697637797], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1996", "score_of_biggest_cluster": 0.49795761704444885, "score_of_first_answer": 0.49795761704444885, "max_score_of_first_answer": 0.9972355961799622, "min_score_of_first_answer": 0.09392233192920685, "avg_score": 0.49795761704444885, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3310994468537501, "std_of_first_answer": 0.3310994468537501, "avg_entropy_of_first_answer": 0.8223255697637797, "lex_sim": 1.0}
{"question": "yo la tengo theres a riot going on release date", "answer": ["1985"], "scores": [[0.3228059709072113, 0.9369571208953857, 0.9758029580116272, 0.9062843918800354, 0.08754590898752213]], "normalized_score": [0.47195953130722046], "tokens": [["\u2581", "1", "9", "8", "5"]], "entropy": [[2.7911949157714844, 0.3487456440925598, 0.17425817251205444, 0.41487956047058105, 2.2092361450195312]], "avg_entropy": [1.1876628875732422], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "March 16, 2018", "score_of_biggest_cluster": 0.47195953130722046, "score_of_first_answer": 0.47195953130722046, "max_score_of_first_answer": 0.9758029580116272, "min_score_of_first_answer": 0.08754590898752213, "avg_score": 0.47195953130722046, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3681030511420992, "std_of_first_answer": 0.3681030511420992, "avg_entropy_of_first_answer": 1.1876628875732422, "lex_sim": 1.0}
{"question": "what is the genus of a bald eagle", "answer": ["Aquila"], "scores": [[0.015552653931081295, 0.9969921112060547]], "normalized_score": [0.12452255934476852], "tokens": [["\u2581Aqu", "ila"]], "entropy": [[0.9066358208656311, 0.02583279088139534]], "avg_entropy": [0.4662343058735132], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Haliaeetus", "score_of_biggest_cluster": 0.12452255934476852, "score_of_first_answer": 0.12452255934476852, "max_score_of_first_answer": 0.9969921112060547, "min_score_of_first_answer": 0.015552653931081295, "avg_score": 0.12452255934476852, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4907197286374867, "std_of_first_answer": 0.4907197286374867, "avg_entropy_of_first_answer": 0.4662343058735132, "lex_sim": 1.0}
{"question": "what mall did they use in back to the future", "answer": ["Universal City Studios Hollywood, Universal Studios Florida, Universal Studios Japan, Universal Studios Singapore"], "scores": [[0.12471555918455124, 0.5233994722366333, 0.014456153847277164, 0.035807348787784576, 0.021816274151206017, 0.14815303683280945, 0.353262722492218, 0.6093069911003113, 0.5116854906082153, 0.6894041895866394, 0.8609777688980103, 0.7230859994888306, 0.4442203938961029, 0.6495769619941711, 0.9130671620368958, 0.8976464867591858]], "normalized_score": [0.27728012204170227], "tokens": [["\u2581Universal", "\u2581City", "\u2581Studios", "\u2581Hollywood", ",", "\u2581Universal", "\u2581Studios", "\u2581Florida", ",", "\u2581Universal", "\u2581Studios", "\u2581Japan", ",", "\u2581Universal", "\u2581Studios", "\u2581Singapore"]], "entropy": [[4.426523208618164, 1.122086524963379, 1.7063432931900024, 1.208293080329895, 0.7994908094406128, 3.395198345184326, 1.268399953842163, 1.6680539846420288, 1.237650752067566, 1.6419556140899658, 0.7114182114601135, 1.0008984804153442, 1.1109559535980225, 1.6965709924697876, 0.5286117196083069, 0.6299135088920593]], "avg_entropy": [1.5095227770507336], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Puente Hills Mall", "score_of_biggest_cluster": 0.27728012204170227, "score_of_first_answer": 0.27728012204170227, "max_score_of_first_answer": 0.9130671620368958, "min_score_of_first_answer": 0.014456153847277164, "avg_score": 0.27728012204170227, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3096872363511044, "std_of_first_answer": 0.3096872363511044, "avg_entropy_of_first_answer": 1.5095227770507336, "lex_sim": 1.0}
{"question": "state with highest population growth rate in india 2011", "answer": ["Uttar Pradesh"], "scores": [[0.2932940423488617, 0.9885565042495728, 0.983810305595398, 0.8905625939369202, 0.9987258315086365]], "normalized_score": [0.7600905895233154], "tokens": [["\u2581U", "tt", "ar", "\u2581Pr", "adesh"]], "entropy": [[2.9368979930877686, 0.08601199090480804, 0.10503903776407242, 0.47908729314804077, 0.01180746965110302]], "avg_entropy": [0.7237687569111586], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dadra and Nagar Haveli", "score_of_biggest_cluster": 0.7600905895233154, "score_of_first_answer": 0.7600905895233154, "max_score_of_first_answer": 0.9987258315086365, "min_score_of_first_answer": 0.2932940423488617, "avg_score": 0.7600905895233154, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2716550735982079, "std_of_first_answer": 0.2716550735982079, "avg_entropy_of_first_answer": 0.7237687569111586, "lex_sim": 1.0}
{"question": "who laid foundation stone of islamia college peshawar", "answer": ["Sir Herbert Baker"], "scores": [[0.16525353491306305, 0.10945312678813934, 0.12452343851327896]], "normalized_score": [0.13108207285404205], "tokens": [["\u2581Sir", "\u2581Herbert", "\u2581Baker"]], "entropy": [[4.556626319885254, 2.906493902206421, 4.259617805480957]], "avg_entropy": [3.9075793425242105], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Haji Sahib", "score_of_biggest_cluster": 0.13108207285404205, "score_of_first_answer": 0.13108207285404205, "max_score_of_first_answer": 0.16525353491306305, "min_score_of_first_answer": 0.10945312678813934, "avg_score": 0.13108207285404205, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02356961460849455, "std_of_first_answer": 0.02356961460849455, "avg_entropy_of_first_answer": 3.9075793425242105, "lex_sim": 1.0}
{"question": "what is loihi and where does it originate from", "answer": ["LOIhi"], "scores": [[0.004836405627429485, 0.6531553864479065, 0.1499336063861847]], "normalized_score": [0.07794938236474991], "tokens": [["\u2581LO", "I", "hi"]], "entropy": [[3.894515037536621, 1.9482437372207642, 0.4912063479423523]], "avg_entropy": [2.1113217075665793], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "from the Hawaii hotspot", "score_of_biggest_cluster": 0.07794938236474991, "score_of_first_answer": 0.07794938236474991, "max_score_of_first_answer": 0.6531553864479065, "min_score_of_first_answer": 0.004836405627429485, "avg_score": 0.07794938236474991, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27780945877997754, "std_of_first_answer": 0.27780945877997754, "avg_entropy_of_first_answer": 2.1113217075665793, "lex_sim": 1.0}
{"question": "how many episodes of season 5 of curse of oak island", "answer": ["10"], "scores": [[0.8545017838478088, 0.5242055654525757, 0.3257005512714386]], "normalized_score": [0.5264345407485962], "tokens": [["\u2581", "1", "0"]], "entropy": [[1.1659389734268188, 1.5941122770309448, 2.102752685546875]], "avg_entropy": [1.6209346453348796], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "18", "score_of_biggest_cluster": 0.5264345407485962, "score_of_first_answer": 0.5264345407485962, "max_score_of_first_answer": 0.8545017838478088, "min_score_of_first_answer": 0.3257005512714386, "avg_score": 0.5264345407485962, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21810562581656862, "std_of_first_answer": 0.21810562581656862, "avg_entropy_of_first_answer": 1.6209346453348796, "lex_sim": 1.0}
{"question": "where are the spanish steps located in italy", "answer": ["Rome"], "scores": [[0.7281500697135925]], "normalized_score": [0.7281500697135925], "tokens": [["\u2581Rome"]], "entropy": [[1.8267285823822021]], "avg_entropy": [1.8267285823822021], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rome", "score_of_biggest_cluster": 0.7281500697135925, "score_of_first_answer": 0.7281500697135925, "max_score_of_first_answer": 0.7281500697135925, "min_score_of_first_answer": 0.7281500697135925, "avg_score": 0.7281500697135925, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.8267285823822021, "lex_sim": 1.0}
{"question": "what position did doug peterson play in the nfl", "answer": ["Center"], "scores": [[0.06379041075706482]], "normalized_score": [0.06379041075706482], "tokens": [["\u2581Center"]], "entropy": [[3.704610824584961]], "avg_entropy": [3.704610824584961], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "holder on placekicks", "score_of_biggest_cluster": 0.06379041075706482, "score_of_first_answer": 0.06379041075706482, "max_score_of_first_answer": 0.06379041075706482, "min_score_of_first_answer": 0.06379041075706482, "avg_score": 0.06379041075706482, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.704610824584961, "lex_sim": 1.0}
{"question": "who has participated in the most super bowls", "answer": ["Jerry Rice"], "scores": [[0.10978172719478607, 0.867129921913147, 0.9998155236244202]], "normalized_score": [0.4565742015838623], "tokens": [["\u2581Jerry", "\u2581R", "ice"]], "entropy": [[4.027584552764893, 0.8823654651641846, 0.002279516775161028]], "avg_entropy": [1.6374098449014127], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "New England Patriots", "score_of_biggest_cluster": 0.4565742015838623, "score_of_first_answer": 0.4565742015838623, "max_score_of_first_answer": 0.9998155236244202, "min_score_of_first_answer": 0.10978172719478607, "avg_score": 0.4565742015838623, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3920518553756761, "std_of_first_answer": 0.3920518553756761, "avg_entropy_of_first_answer": 1.6374098449014127, "lex_sim": 1.0}
{"question": "unsaturated fats are comprised of lipids that contain", "answer": ["carbon and hydrogen and oxygen"], "scores": [[0.029236620292067528, 0.24514839053153992, 0.9265813827514648, 0.9963549375534058, 0.013061546720564365, 0.2648904025554657, 0.9929956793785095]], "normalized_score": [0.21710310876369476], "tokens": [["\u2581carbon", "\u2581and", "\u2581hydro", "gen", "\u2581and", "\u2581o", "xygen"]], "entropy": [[4.49252986907959, 2.162921905517578, 0.533470869064331, 0.028929557651281357, 1.475751280784607, 3.7894577980041504, 0.05531485006213188]], "avg_entropy": [1.7911965900233813], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "double bond", "score_of_biggest_cluster": 0.21710310876369476, "score_of_first_answer": 0.21710310876369476, "max_score_of_first_answer": 0.9963549375534058, "min_score_of_first_answer": 0.013061546720564365, "avg_score": 0.21710310876369476, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4226098987511951, "std_of_first_answer": 0.4226098987511951, "avg_entropy_of_first_answer": 1.7911965900233813, "lex_sim": 1.0}
{"question": "when was the immigration act passed in canada", "answer": ["November 1, 1910"], "scores": [[0.0074685560539364815, 0.9801777005195618, 0.7038853764533997, 0.31509995460510254, 0.9869862198829651, 0.9697765707969666, 0.9938803911209106, 0.6424158811569214, 0.6847654581069946]], "normalized_score": [0.444650262594223], "tokens": [["\u2581November", "\u2581", "1", ",", "\u2581", "1", "9", "1", "0"]], "entropy": [[1.4534063339233398, 0.12391646951436996, 1.0920048952102661, 1.7826991081237793, 0.07440825551748276, 0.1369503140449524, 0.03975816071033478, 1.0661009550094604, 1.1149342060089111]], "avg_entropy": [0.7649087442292107], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1923", "score_of_biggest_cluster": 0.444650262594223, "score_of_first_answer": 0.444650262594223, "max_score_of_first_answer": 0.9938803911209106, "min_score_of_first_answer": 0.0074685560539364815, "avg_score": 0.444650262594223, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32511362711565345, "std_of_first_answer": 0.32511362711565345, "avg_entropy_of_first_answer": 0.7649087442292107, "lex_sim": 1.0}
{"question": "who discovered that plants were made of cells", "answer": ["Robert Hooke"], "scores": [[0.8594817519187927, 0.7198004722595215, 0.9996131062507629, 0.9996634721755981]], "normalized_score": [0.8867141604423523], "tokens": [["\u2581Robert", "\u2581H", "oo", "ke"]], "entropy": [[0.895557165145874, 1.2423912286758423, 0.004163441248238087, 0.004097841680049896]], "avg_entropy": [0.5365524191875011], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Parashara", "score_of_biggest_cluster": 0.8867141604423523, "score_of_first_answer": 0.8867141604423523, "max_score_of_first_answer": 0.9996634721755981, "min_score_of_first_answer": 0.7198004722595215, "avg_score": 0.8867141604423523, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11603258767696194, "std_of_first_answer": 0.11603258767696194, "avg_entropy_of_first_answer": 0.5365524191875011, "lex_sim": 1.0}
{"question": "where does the water come from to fill lake eyre", "answer": ["monsoonal rains from the Indian Ocean"], "scores": [[0.002159378957003355, 0.9984924793243408, 0.14071789383888245, 0.4090363681316376, 0.784123957157135, 0.11691746860742569, 0.7793208956718445, 0.5520718097686768, 0.9497065544128418]], "normalized_score": [0.2555543780326843], "tokens": [["\u2581mon", "so", "onal", "\u2581ra", "ins", "\u2581from", "\u2581the", "\u2581Indian", "\u2581Ocean"]], "entropy": [[5.699127197265625, 0.015619917772710323, 0.7693329453468323, 2.5632143020629883, 0.5707833170890808, 1.5954159498214722, 1.4014720916748047, 2.5575156211853027, 0.2887283265590668]], "avg_entropy": [1.7179121854197648], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Warburton River", "score_of_biggest_cluster": 0.2555543780326843, "score_of_first_answer": 0.2555543780326843, "max_score_of_first_answer": 0.9984924793243408, "min_score_of_first_answer": 0.002159378957003355, "avg_score": 0.2555543780326843, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3556114689422076, "std_of_first_answer": 0.3556114689422076, "avg_entropy_of_first_answer": 1.7179121854197648, "lex_sim": 1.0}
{"question": "last episode of what happens to my family", "answer": ["The Final"], "scores": [[0.09958691895008087, 0.04531041160225868]], "normalized_score": [0.06717383861541748], "tokens": [["\u2581The", "\u2581Final"]], "entropy": [[5.5957536697387695, 5.993404388427734]], "avg_entropy": [5.794579029083252], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "53", "score_of_biggest_cluster": 0.06717383861541748, "score_of_first_answer": 0.06717383861541748, "max_score_of_first_answer": 0.09958691895008087, "min_score_of_first_answer": 0.04531041160225868, "avg_score": 0.06717383861541748, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.027138253673911095, "std_of_first_answer": 0.027138253673911095, "avg_entropy_of_first_answer": 5.794579029083252, "lex_sim": 1.0}
{"question": "who was the buccaneers qb when they won the superbowl", "answer": ["Steve Young"], "scores": [[0.10445906221866608, 0.8383023142814636]], "normalized_score": [0.29591935873031616], "tokens": [["\u2581Steve", "\u2581Young"]], "entropy": [[2.393540859222412, 0.8801385760307312]], "avg_entropy": [1.6368397176265717], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brad Johnson", "score_of_biggest_cluster": 0.29591935873031616, "score_of_first_answer": 0.29591935873031616, "max_score_of_first_answer": 0.8383023142814636, "min_score_of_first_answer": 0.10445906221866608, "avg_score": 0.29591935873031616, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3669216260313988, "std_of_first_answer": 0.3669216260313988, "avg_entropy_of_first_answer": 1.6368397176265717, "lex_sim": 1.0}
{"question": "when was pac man released in the us", "answer": ["1982"], "scores": [[0.4284553825855255, 0.9786575436592102, 0.9920803904533386, 0.9937635064125061, 0.39832815527915955]], "normalized_score": [0.6971423029899597], "tokens": [["\u2581", "1", "9", "8", "2"]], "entropy": [[2.1315808296203613, 0.14033742249011993, 0.06711898744106293, 0.048073358833789825, 1.130582332611084]], "avg_entropy": [0.7035385861992836], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "October 26, 1980", "score_of_biggest_cluster": 0.6971423029899597, "score_of_first_answer": 0.6971423029899597, "max_score_of_first_answer": 0.9937635064125061, "min_score_of_first_answer": 0.39832815527915955, "avg_score": 0.6971423029899597, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28179104749522327, "std_of_first_answer": 0.28179104749522327, "avg_entropy_of_first_answer": 0.7035385861992836, "lex_sim": 1.0}
{"question": "who sang never gonna let you go", "answer": ["Rick Astley"], "scores": [[0.014459384605288506, 0.959860622882843, 0.997796893119812]], "normalized_score": [0.24014125764369965], "tokens": [["\u2581Rick", "\u2581Ast", "ley"]], "entropy": [[5.111043930053711, 0.33278656005859375, 0.019291983917355537]], "avg_entropy": [1.8210408246765535], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joe Pizzulo and Leeza Miller", "score_of_biggest_cluster": 0.24014125764369965, "score_of_first_answer": 0.24014125764369965, "max_score_of_first_answer": 0.997796893119812, "min_score_of_first_answer": 0.014459384605288506, "avg_score": 0.24014125764369965, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.454871815592991, "std_of_first_answer": 0.454871815592991, "avg_entropy_of_first_answer": 1.8210408246765535, "lex_sim": 1.0}
{"question": "when did the age of new media began", "answer": ["The 1980s"], "scores": [[0.0634620413184166, 0.4332396984100342, 0.8345439434051514, 0.9581257104873657, 0.3347468674182892, 0.9950346946716309, 0.8520578145980835]], "normalized_score": [0.4841943085193634], "tokens": [["\u2581The", "\u2581", "1", "9", "8", "0", "s"]], "entropy": [[2.5030627250671387, 3.0892693996429443, 0.6309111714363098, 0.24292679131031036, 1.6442852020263672, 0.04159106686711311, 0.6017416715621948]], "avg_entropy": [1.2505411468446255], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the 1980s", "score_of_biggest_cluster": 0.4841943085193634, "score_of_first_answer": 0.4841943085193634, "max_score_of_first_answer": 0.9950346946716309, "min_score_of_first_answer": 0.0634620413184166, "avg_score": 0.4841943085193634, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3334631579591406, "std_of_first_answer": 0.3334631579591406, "avg_entropy_of_first_answer": 1.2505411468446255, "lex_sim": 1.0}
{"question": "who has the most all star mvp awards", "answer": ["Kareem Abdul-Jabbar"], "scores": [[0.21266505122184753, 0.6623085141181946, 0.9970484375953674, 0.9714822173118591, 0.998995840549469, 0.8405449390411377, 0.996227502822876, 0.999292254447937, 0.9791575074195862]], "normalized_score": [0.783817708492279], "tokens": [["\u2581K", "are", "em", "\u2581Abd", "ul", "-", "J", "ab", "bar"]], "entropy": [[4.047789096832275, 0.8809650540351868, 0.028404269367456436, 0.1975516378879547, 0.010354992933571339, 0.5169228911399841, 0.03142128884792328, 0.006975188851356506, 0.12300178408622742]], "avg_entropy": [0.6492651337757707], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bob Pettit", "score_of_biggest_cluster": 0.783817708492279, "score_of_first_answer": 0.783817708492279, "max_score_of_first_answer": 0.999292254447937, "min_score_of_first_answer": 0.21266505122184753, "avg_score": 0.783817708492279, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24954179280799554, "std_of_first_answer": 0.24954179280799554, "avg_entropy_of_first_answer": 0.6492651337757707, "lex_sim": 1.0}
{"question": "why is the indian ocean the warmest in the world", "answer": ["The Indian Ocean is the warmest in the world as there is less water compared to the Pacific or Atlantic Oceans."], "scores": [[0.08892066776752472, 0.34727218747138977, 0.9456332921981812, 0.6112799048423767, 0.36733102798461914, 0.8740732669830322, 0.999041736125946, 0.524996817111969, 0.9909668564796448, 0.9839641451835632, 0.010423525236546993, 0.007231652271002531, 0.6948674321174622, 0.09680752456188202, 0.018188966438174248, 0.019655412063002586, 0.9781036376953125, 0.5275020003318787, 0.32004815340042114, 0.06612550467252731, 0.7940561175346375, 0.18384605646133423, 0.9984965324401855, 0.9997765421867371, 0.5940275192260742]], "normalized_score": [0.2679397463798523], "tokens": [["\u2581The", "\u2581Indian", "\u2581Ocean", "\u2581is", "\u2581the", "\u2581warm", "est", "\u2581in", "\u2581the", "\u2581world", "\u2581as", "\u2581there", "\u2581is", "\u2581less", "\u2581water", "\u2581compared", "\u2581to", "\u2581the", "\u2581Pacific", "\u2581or", "\u2581Atlantic", "\u2581O", "ce", "ans", "."]], "entropy": [[4.778511047363281, 4.223880290985107, 0.3914095461368561, 2.092942237854004, 3.000891923904419, 0.7714946269989014, 0.01129552535712719, 1.6885137557983398, 0.08259814977645874, 0.1119580864906311, 1.120875597000122, 1.5205415487289429, 0.838415265083313, 2.931173801422119, 4.190993309020996, 3.668064594268799, 0.14791865646839142, 1.7824116945266724, 2.201805353164673, 1.3721641302108765, 0.7401114702224731, 1.9739363193511963, 0.015895476564764977, 0.0027583888731896877, 1.6175737380981445]], "avg_entropy": [1.6511253813467919], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "human induced greenhouse warming", "score_of_biggest_cluster": 0.2679397463798523, "score_of_first_answer": 0.2679397463798523, "max_score_of_first_answer": 0.9997765421867371, "min_score_of_first_answer": 0.007231652271002531, "avg_score": 0.2679397463798523, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.377539365180553, "std_of_first_answer": 0.377539365180553, "avg_entropy_of_first_answer": 1.6511253813467919, "lex_sim": 1.0}
{"question": "where are the winter olympic games being played", "answer": ["Pyeongchang"], "scores": [[0.5685523748397827, 0.9657883644104004, 0.995610773563385, 0.6478918790817261, 0.999576985836029]], "normalized_score": [0.8124790191650391], "tokens": [["\u2581P", "ye", "ong", "ch", "ang"]], "entropy": [[2.0891261100769043, 0.2319386899471283, 0.03253646567463875, 0.7363966107368469, 0.005174827761948109]], "avg_entropy": [0.6190345408394933], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Beijing", "score_of_biggest_cluster": 0.8124790191650391, "score_of_first_answer": 0.8124790191650391, "max_score_of_first_answer": 0.999576985836029, "min_score_of_first_answer": 0.5685523748397827, "avg_score": 0.8124790191650391, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18761109526787786, "std_of_first_answer": 0.18761109526787786, "avg_entropy_of_first_answer": 0.6190345408394933, "lex_sim": 1.0}
{"question": "how many seasons has greys anatomy been on tv", "answer": ["Fourteen"], "scores": [[0.0009112541447393596, 0.9115356206893921]], "normalized_score": [0.028820838779211044], "tokens": [["\u2581Four", "teen"]], "entropy": [[1.2478172779083252, 0.36351409554481506]], "avg_entropy": [0.8056656867265701], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "14", "score_of_biggest_cluster": 0.028820838779211044, "score_of_first_answer": 0.028820838779211044, "max_score_of_first_answer": 0.9115356206893921, "min_score_of_first_answer": 0.0009112541447393596, "avg_score": 0.028820838779211044, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.45531218327232637, "std_of_first_answer": 0.45531218327232637, "avg_entropy_of_first_answer": 0.8056656867265701, "lex_sim": 1.0}
{"question": "when does kubo and the two strings release", "answer": ["September 28, 2016 We\u2019ve put together an entire set of over 67,000 questions available now on the AIY Challenge Kit."], "scores": [[0.33613669872283936, 0.9430838227272034, 0.844201385974884, 0.03667601943016052, 0.35339877009391785, 0.990160346031189, 0.9996190071105957, 0.9996539354324341, 0.9989462494850159, 0.9837823510169983, 6.203774205459922e-07, 0.05106235668063164, 0.1571965366601944, 0.00806181225925684, 0.4234146475791931, 0.04631626605987549, 0.03574054688215256, 0.011544463224709034, 0.9566417932510376, 0.006656928453594446, 0.9105976819992065, 0.055972956120967865, 0.006096941418945789, 0.12533080577850342, 0.87883460521698, 0.9957324862480164, 0.9977460503578186, 0.17466819286346436, 0.0013498003827407956, 0.02518538571894169, 0.13824959099292755, 0.13141512870788574, 0.025018608197569847, 0.4906634986400604, 0.05389200150966644, 0.005423503462225199, 0.039430975914001465, 0.3680380880832672, 0.014956283383071423, 0.04324652999639511, 0.8141493201255798, 0.06363581866025925, 0.008231044746935368, 0.13365676999092102, 0.871621310710907, 0.04952649399638176, 0.003567382926121354, 0.382326602935791, 0.057886771857738495]], "normalized_score": [0.08776978403329849], "tokens": [["\u2581September", "\u2581", "2", "8", ",", "\u2581", "2", "0", "1", "6", "\u2581We", "\u2019", "ve", "\u2581put", "\u2581together", "\u2581an", "\u2581entire", "\u2581set", "\u2581of", "\u2581over", "\u2581", "6", "7", ",", "0", "0", "0", "\u2581questions", "\u2581available", "\u2581now", "\u2581on", "\u2581the", "\u2581A", "I", "Y", "\u2581Challenge", "\u2581Kit", ".", "\u2581For", "\u2581example", ",", "\u2581the", "\u2581T", "ensor", "Flow", ".", "K", "eras", "\u2581model"]], "entropy": [[2.345003604888916, 0.2667449712753296, 0.5398550629615784, 1.4855239391326904, 1.0487655401229858, 0.0602879598736763, 0.003932230174541473, 0.0041945623233914375, 0.01027910877019167, 0.10674683749675751, 0.30510661005973816, 4.401856422424316, 1.04073166847229, 4.144040107727051, 2.8033151626586914, 1.9146445989608765, 4.703617095947266, 4.88407039642334, 0.35543087124824524, 6.189324378967285, 0.6000895500183105, 1.9008420705795288, 0.8275863528251648, 2.445244550704956, 0.6438179016113281, 0.03782182186841965, 0.026590386405587196, 5.567094802856445, 2.664992570877075, 2.4225854873657227, 2.7558908462524414, 3.731713056564331, 5.2321648597717285, 3.144550323486328, 5.519130229949951, 5.180594444274902, 3.8598790168762207, 3.024949073791504, 4.287605285644531, 3.077737331390381, 0.7670243382453918, 3.8159072399139404, 5.292733192443848, 3.379931688308716, 0.7160645127296448, 5.241287708282471, 1.8756227493286133, 3.073899507522583, 5.247027397155762]], "avg_entropy": [2.509670396468469], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "August 19, 2016", "score_of_biggest_cluster": 0.08776978403329849, "score_of_first_answer": 0.08776978403329849, "max_score_of_first_answer": 0.9996539354324341, "min_score_of_first_answer": 6.203774205459922e-07, "avg_score": 0.08776978403329849, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39472175309686314, "std_of_first_answer": 0.39472175309686314, "avg_entropy_of_first_answer": 2.509670396468469, "lex_sim": 1.0}
{"question": "what's the medal count for canada in the olympics", "answer": ["https://io."], "scores": [[0.0020500626415014267, 0.9892936944961548, 0.0023181841243058443, 0.8383714556694031, 0.004201438277959824, 0.46278661489486694, 0.9871557950973511, 0.4989127516746521, 0.9549502730369568, 0.8076463341712952, 0.9755054116249084, 0.043837349861860275, 0.6497798562049866, 0.27949780225753784, 0.1275734007358551, 0.945488452911377, 0.41619759798049927, 0.45066192746162415, 0.6820423603057861]], "normalized_score": [0.22097888588905334], "tokens": [["\u2581https", "://", "io", ".", "\u2581olymp", "ic", ".", "org", "/", "en", "/", "med", "als", "/", "all", "-", "med", "al", "ists"]], "entropy": [[1.4562381505966187, 0.09751684963703156, 1.9663541316986084, 1.0969321727752686, 4.298128604888916, 0.7391082048416138, 0.10491972416639328, 1.6834145784378052, 0.26270952820777893, 1.3346426486968994, 0.16928809881210327, 3.7676444053649902, 0.6906775236129761, 1.5998015403747559, 3.3777546882629395, 0.33747944235801697, 0.9583860635757446, 0.9535457491874695, 0.7867323756217957]], "avg_entropy": [1.3516460253219855], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "501", "score_of_biggest_cluster": 0.22097888588905334, "score_of_first_answer": 0.22097888588905334, "max_score_of_first_answer": 0.9892936944961548, "min_score_of_first_answer": 0.0020500626415014267, "avg_score": 0.22097888588905334, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36278005921525786, "std_of_first_answer": 0.36278005921525786, "avg_entropy_of_first_answer": 1.3516460253219855, "lex_sim": 1.0}
{"question": "who won the award for best goalkeeper in football world cup 2006", "answer": ["Italians Gianluigi Buffon"], "scores": [[0.027074016630649567, 0.1920083612203598, 0.9083691835403442, 0.6347087025642395, 0.9937239289283752, 0.9959883093833923, 0.9927323460578918, 0.9976094961166382]], "normalized_score": [0.4825060963630676], "tokens": [["\u2581It", "ali", "ans", "\u2581Gian", "lu", "igi", "\u2581Buff", "on"]], "entropy": [[3.231018304824829, 3.0508177280426025, 0.5666968822479248, 1.9130780696868896, 0.05939261615276337, 0.033769238740205765, 0.058463722467422485, 0.023745587095618248]], "avg_entropy": [1.117122768657282], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gianluigi Buffon", "score_of_biggest_cluster": 0.4825060963630676, "score_of_first_answer": 0.4825060963630676, "max_score_of_first_answer": 0.9976094961166382, "min_score_of_first_answer": 0.027074016630649567, "avg_score": 0.4825060963630676, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.37151445188700827, "std_of_first_answer": 0.37151445188700827, "avg_entropy_of_first_answer": 1.117122768657282, "lex_sim": 1.0}
{"question": "who plays the queen in victoria and abdul", "answer": ["Judy Dench"], "scores": [[0.06936748325824738, 0.991390585899353, 0.6666299104690552, 0.9997733235359192]], "normalized_score": [0.4626970887184143], "tokens": [["\u2581Jud", "y", "\u2581Den", "ch"]], "entropy": [[2.323061466217041, 0.05518241971731186, 1.3274080753326416, 0.002965289633721113]], "avg_entropy": [0.9271543127251789], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Judi Dench", "score_of_biggest_cluster": 0.4626970887184143, "score_of_first_answer": 0.4626970887184143, "max_score_of_first_answer": 0.9997733235359192, "min_score_of_first_answer": 0.06936748325824738, "avg_score": 0.4626970887184143, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.37823837859299647, "std_of_first_answer": 0.37823837859299647, "avg_entropy_of_first_answer": 0.9271543127251789, "lex_sim": 1.0}
{"question": "who won the oscar for best actor in 1975", "answer": ["George C. Scott"], "scores": [[0.01525045558810234, 0.6882134079933167, 0.8225826025009155, 0.9973952770233154]], "normalized_score": [0.3046233355998993], "tokens": [["\u2581George", "\u2581C", ".", "\u2581Scott"]], "entropy": [[3.370365619659424, 1.05673086643219, 0.5783447027206421, 0.023525815457105637]], "avg_entropy": [1.2572417510673404], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Art Carney", "score_of_biggest_cluster": 0.3046233355998993, "score_of_first_answer": 0.3046233355998993, "max_score_of_first_answer": 0.9973952770233154, "min_score_of_first_answer": 0.01525045558810234, "avg_score": 0.3046233355998993, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37194428016538816, "std_of_first_answer": 0.37194428016538816, "avg_entropy_of_first_answer": 1.2572417510673404, "lex_sim": 1.0}
{"question": "when did the revolution begin in the united states", "answer": ["1773"], "scores": [[0.49912410974502563, 0.9903420209884644, 0.9861159324645996, 0.9606348276138306, 0.04032832756638527]], "normalized_score": [0.4520828425884247], "tokens": [["\u2581", "1", "7", "7", "3"]], "entropy": [[2.0363893508911133, 0.070041723549366, 0.10428319871425629, 0.2129453718662262, 1.0374104976654053]], "avg_entropy": [0.6922140285372734], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1775", "score_of_biggest_cluster": 0.4520828425884247, "score_of_first_answer": 0.4520828425884247, "max_score_of_first_answer": 0.9903420209884644, "min_score_of_first_answer": 0.04032832756638527, "avg_score": 0.4520828425884247, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3766959364442231, "std_of_first_answer": 0.3766959364442231, "avg_entropy_of_first_answer": 0.6922140285372734, "lex_sim": 1.0}
{"question": "when does the boy in the striped pajamas take place", "answer": ["1942-1944"], "scores": [[0.4375992715358734, 0.9946749210357666, 0.9957185387611389, 0.8605636358261108, 0.30951574444770813, 0.10895372182130814, 0.753812849521637, 0.9992445707321167, 0.9983853101730347, 0.7111339569091797]], "normalized_score": [0.6064272522926331], "tokens": [["\u2581", "1", "9", "4", "2", "-", "1", "9", "4", "4"]], "entropy": [[2.7939257621765137, 0.04143806919455528, 0.04051030054688454, 0.43976646661758423, 1.2738405466079712, 1.163012981414795, 0.7303090691566467, 0.008740353398025036, 0.014808923937380314, 0.8012912273406982]], "avg_entropy": [0.7307643700391054], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "World War II", "score_of_biggest_cluster": 0.6064272522926331, "score_of_first_answer": 0.6064272522926331, "max_score_of_first_answer": 0.9992445707321167, "min_score_of_first_answer": 0.10895372182130814, "avg_score": 0.6064272522926331, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3081323800236886, "std_of_first_answer": 0.3081323800236886, "avg_entropy_of_first_answer": 0.7307643700391054, "lex_sim": 1.0}
{"question": "who wrote the song photograph by ringo starr", "answer": ["Ringo Starr"], "scores": [[0.22302483022212982, 0.9912534356117249, 0.9259023666381836, 0.9999215602874756]], "normalized_score": [0.6726161241531372], "tokens": [["\u2581Ring", "o", "\u2581St", "arr"]], "entropy": [[3.1525964736938477, 0.07832247763872147, 0.4265378713607788, 0.0011367362458258867]], "avg_entropy": [0.9146483897347935], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George Harrison", "score_of_biggest_cluster": 0.6726161241531372, "score_of_first_answer": 0.6726161241531372, "max_score_of_first_answer": 0.9999215602874756, "min_score_of_first_answer": 0.22302483022212982, "avg_score": 0.6726161241531372, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32573045702632336, "std_of_first_answer": 0.32573045702632336, "avg_entropy_of_first_answer": 0.9146483897347935, "lex_sim": 1.0}
{"question": "when does april fools day end at noon", "answer": ["April 1 at noon"], "scores": [[0.4656297266483307, 0.7401037812232971, 0.9745383858680725, 0.0018741888925433159, 0.5746225118637085, 0.9979932308197021]], "normalized_score": [0.26683491468429565], "tokens": [["\u2581April", "\u2581", "1", "\u2581at", "\u2581no", "on"]], "entropy": [[3.366581678390503, 1.1109373569488525, 0.15056896209716797, 0.8654888272285461, 1.2705525159835815, 0.01795807108283043]], "avg_entropy": [1.130347901955247], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "April 1st", "score_of_biggest_cluster": 0.26683491468429565, "score_of_first_answer": 0.26683491468429565, "max_score_of_first_answer": 0.9979932308197021, "min_score_of_first_answer": 0.0018741888925433159, "avg_score": 0.26683491468429565, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33930534098569737, "std_of_first_answer": 0.33930534098569737, "avg_entropy_of_first_answer": 1.130347901955247, "lex_sim": 1.0}
{"question": "who is going to host the 2018 winter olympics", "answer": ["Pyeongchang, South Korea"], "scores": [[0.4043061137199402, 0.9129720330238342, 0.9960445165634155, 0.595496654510498, 0.9995933175086975, 0.2695266306400299, 0.9354507923126221, 0.9953652620315552]], "normalized_score": [0.6957757472991943], "tokens": [["\u2581P", "ye", "ong", "ch", "ang", ",", "\u2581South", "\u2581Korea"]], "entropy": [[3.0752370357513428, 0.505658745765686, 0.027900712564587593, 0.8335505723953247, 0.004954883828759193, 1.174093246459961, 0.41510745882987976, 0.041788797825574875]], "avg_entropy": [0.7597864316776395], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "South Korea", "score_of_biggest_cluster": 0.6957757472991943, "score_of_first_answer": 0.6957757472991943, "max_score_of_first_answer": 0.9995933175086975, "min_score_of_first_answer": 0.2695266306400299, "avg_score": 0.6957757472991943, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2776559157676179, "std_of_first_answer": 0.2776559157676179, "avg_entropy_of_first_answer": 0.7597864316776395, "lex_sim": 1.0}
{"question": "when does the next season of wynonna earp start", "answer": ["2019"], "scores": [[0.14113783836364746, 0.8617641925811768, 0.9529182314872742, 0.899940013885498, 0.16067679226398468]], "normalized_score": [0.4414188265800476], "tokens": [["\u2581", "2", "0", "1", "9"]], "entropy": [[3.2522430419921875, 0.6576512455940247, 0.3238390386104584, 0.3599557876586914, 1.3222167491912842]], "avg_entropy": [1.1831811726093293], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2018", "score_of_biggest_cluster": 0.4414188265800476, "score_of_first_answer": 0.4414188265800476, "max_score_of_first_answer": 0.9529182314872742, "min_score_of_first_answer": 0.14113783836364746, "avg_score": 0.4414188265800476, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.370551241087046, "std_of_first_answer": 0.370551241087046, "avg_entropy_of_first_answer": 1.1831811726093293, "lex_sim": 1.0}
{"question": "who gave the famous objective co relative theory", "answer": ["Charles Babbage"], "scores": [[0.020789192989468575, 0.019127940759062767, 0.9171996712684631, 0.9972763657569885]], "normalized_score": [0.13810081779956818], "tokens": [["\u2581Charles", "\u2581B", "abb", "age"]], "entropy": [[5.921952247619629, 3.5330560207366943, 0.5902285575866699, 0.02521219663321972]], "avg_entropy": [2.517612255644053], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "T.S. Eliot", "score_of_biggest_cluster": 0.13810081779956818, "score_of_first_answer": 0.13810081779956818, "max_score_of_first_answer": 0.9972763657569885, "min_score_of_first_answer": 0.019127940759062767, "avg_score": 0.13810081779956818, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46949448581513803, "std_of_first_answer": 0.46949448581513803, "avg_entropy_of_first_answer": 2.517612255644053, "lex_sim": 1.0}
{"question": "who does the voice of the gorilla in the movie sing", "answer": ["Jennifer Lopez"], "scores": [[0.003963883966207504, 0.997596800327301, 0.3315427601337433, 0.9507185220718384, 0.9990874528884888]], "normalized_score": [0.2624545991420746], "tokens": [["\u2581Jenn", "ifer", "\u2581L", "ope", "z"]], "entropy": [[4.863613605499268, 0.02548304945230484, 3.272031784057617, 0.3920907974243164, 0.009999830275774002]], "avg_entropy": [1.712643813341856], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Taron Egerton", "score_of_biggest_cluster": 0.2624545991420746, "score_of_first_answer": 0.2624545991420746, "max_score_of_first_answer": 0.9990874528884888, "min_score_of_first_answer": 0.003963883966207504, "avg_score": 0.2624545991420746, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41271740591969175, "std_of_first_answer": 0.41271740591969175, "avg_entropy_of_first_answer": 1.712643813341856, "lex_sim": 1.0}
{"question": "who plays the dwarf king in the hobbit", "answer": ["Richard Armitage"], "scores": [[0.3857613205909729, 0.9691107273101807, 0.9978444576263428, 0.991517186164856]], "normalized_score": [0.7798548936843872], "tokens": [["\u2581Richard", "\u2581Ar", "mit", "age"]], "entropy": [[3.1679725646972656, 0.3037553131580353, 0.02458256296813488, 0.055743664503097534]], "avg_entropy": [0.8880135263316333], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Richard Crispin Armitage", "score_of_biggest_cluster": 0.7798548936843872, "score_of_first_answer": 0.7798548936843872, "max_score_of_first_answer": 0.9978444576263428, "min_score_of_first_answer": 0.3857613205909729, "avg_score": 0.7798548936843872, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2601982629482169, "std_of_first_answer": 0.2601982629482169, "avg_entropy_of_first_answer": 0.8880135263316333, "lex_sim": 1.0}
{"question": "how long has it been since eagles went to super bowl", "answer": ["12 years"], "scores": [[0.6465921998023987, 0.36314040422439575, 0.05719262734055519, 0.625939130783081]], "normalized_score": [0.302791953086853], "tokens": [["\u2581", "1", "2", "\u2581years"]], "entropy": [[2.271704912185669, 1.731817364692688, 2.3163881301879883, 1.2315257787704468]], "avg_entropy": [1.887859046459198], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2018", "score_of_biggest_cluster": 0.302791953086853, "score_of_first_answer": 0.302791953086853, "max_score_of_first_answer": 0.6465921998023987, "min_score_of_first_answer": 0.05719262734055519, "avg_score": 0.302791953086853, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.239047997760602, "std_of_first_answer": 0.239047997760602, "avg_entropy_of_first_answer": 1.887859046459198, "lex_sim": 1.0}
{"question": "when was the design period known as art deco", "answer": ["around 1925"], "scores": [[0.0033187323715537786, 0.8242073655128479, 0.9955766201019287, 0.9652625322341919, 0.507082462310791, 0.5178771615028381]], "normalized_score": [0.2972846031188965], "tokens": [["\u2581around", "\u2581", "1", "9", "2", "5"]], "entropy": [[1.6509886980056763, 0.6305868625640869, 0.03580499812960625, 0.16881071031093597, 1.3161020278930664, 1.0187830924987793]], "avg_entropy": [0.8035127315670252], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1910\u20131939", "score_of_biggest_cluster": 0.2972846031188965, "score_of_first_answer": 0.2972846031188965, "max_score_of_first_answer": 0.9955766201019287, "min_score_of_first_answer": 0.0033187323715537786, "avg_score": 0.2972846031188965, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3425366898792035, "std_of_first_answer": 0.3425366898792035, "avg_entropy_of_first_answer": 0.8035127315670252, "lex_sim": 1.0}
{"question": "when did the 5 day work week begin", "answer": ["The 5-day work week is a typical work schedule in which employees of a given company or industry are permitted to work only 5 days a week when work days are typically Monday through Friday."], "scores": [[0.033443935215473175, 0.2573627829551697, 0.27632203698158264, 0.44754400849342346, 0.8411988019943237, 0.8701492547988892, 0.6195706129074097, 0.09582145512104034, 0.40144816040992737, 0.014485834166407585, 0.9491197466850281, 0.5663520097732544, 0.5449191331863403, 0.07924829423427582, 0.3659155070781708, 0.012445754371583462, 0.2893429100513458, 0.1933310478925705, 0.41281622648239136, 0.21750439703464508, 0.2855808436870575, 0.26462385058403015, 0.0026394531596451998, 0.8292276859283447, 0.6862863898277283, 0.028943564742803574, 0.2551193833351135, 0.7225523591041565, 0.9049416184425354, 0.22024641931056976, 0.9900549650192261, 0.0005454820347949862, 0.038279712200164795, 0.035497210919857025, 0.6584651470184326, 0.10213243961334229, 0.12319746613502502, 0.6948052048683167, 0.9617212414741516, 0.6990965008735657, 0.0017607025802135468, 0.7272319793701172, 0.5888420939445496, 0.42027148604393005, 0.4125848412513733, 0.8873081803321838, 0.9235049486160278, 0.9897716641426086, 0.04711850360035896, 0.5889444947242737]], "normalized_score": [0.2104613333940506], "tokens": [["\u2581The", "\u2581", "5", "-", "day", "\u2581work", "\u2581week", "\u2581is", "\u2581a", "\u2581typical", "\u2581work", "\u2581schedule", "\u2581in", "\u2581which", "\u2581employees", "\u2581of", "\u2581a", "\u2581given", "\u2581company", "\u2581or", "\u2581industry", "\u2581are", "\u2581permitted", "\u2581to", "\u2581work", "\u2581only", "\u2581", "5", "\u2581days", "\u2581a", "\u2581week", "\u2581when", "\u2581work", "\u2581days", "\u2581are", "\u2581typically", "\u2581Monday", "\u2581through", "\u2581Friday", ".", "\u2581Origin", "ally", ",", "\u2581the", "\u2581", "5", "-", "day", "\u2581week", "\u2581was"]], "entropy": [[2.4589505195617676, 3.684556722640991, 1.4429901838302612, 1.3478333950042725, 0.5171562433242798, 0.6272860169410706, 0.7754894495010376, 2.5507633686065674, 3.1760830879211426, 4.330656051635742, 0.3886551856994629, 1.401219367980957, 2.3427538871765137, 2.632441520690918, 1.8472797870635986, 1.6489320993423462, 2.201085090637207, 2.2513084411621094, 2.164821147918701, 2.026594400405884, 2.1888089179992676, 1.8554344177246094, 3.1918325424194336, 0.9130597710609436, 1.3403469324111938, 2.929813861846924, 1.3581904172897339, 0.9036462903022766, 0.5389035940170288, 2.1161673069000244, 0.07899981737136841, 2.142075300216675, 4.785831451416016, 3.09232759475708, 2.028583526611328, 3.7572379112243652, 2.770573377609253, 0.9786884784698486, 0.23614098131656647, 1.3996005058288574, 2.682335615158081, 0.8764994144439697, 2.4464175701141357, 2.8528647422790527, 3.1102614402770996, 0.5243491530418396, 0.35020530223846436, 0.08164620399475098, 0.35314086079597473, 2.3759818077087402]], "avg_entropy": [1.8815364214777945], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1908", "score_of_biggest_cluster": 0.2104613333940506, "score_of_first_answer": 0.2104613333940506, "max_score_of_first_answer": 0.9900549650192261, "min_score_of_first_answer": 0.0005454820347949862, "avg_score": 0.2104613333940506, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32688718021335844, "std_of_first_answer": 0.32688718021335844, "avg_entropy_of_first_answer": 1.8815364214777945, "lex_sim": 1.0}
{"question": "what does the air gap do on a dishwasher", "answer": ["keeps the soap out of the sprayer"], "scores": [[0.031735051423311234, 0.22251421213150024, 0.005119546316564083, 0.1482025384902954, 0.7511845827102661, 0.8938077092170715, 0.03092021867632866, 0.9857529997825623, 0.25553542375564575]], "normalized_score": [0.14481927454471588], "tokens": [["\u2581keeps", "\u2581the", "\u2581soap", "\u2581out", "\u2581of", "\u2581the", "\u2581sp", "ray", "er"]], "entropy": [[4.483592987060547, 3.30587100982666, 3.768453598022461, 2.7541818618774414, 0.8657832741737366, 0.6324764490127563, 3.352534294128418, 0.1082952618598938, 2.0485286712646484]], "avg_entropy": [2.36885748969184], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "backflow prevention", "score_of_biggest_cluster": 0.14481927454471588, "score_of_first_answer": 0.14481927454471588, "max_score_of_first_answer": 0.9857529997825623, "min_score_of_first_answer": 0.005119546316564083, "avg_score": 0.14481927454471588, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37199301849810984, "std_of_first_answer": 0.37199301849810984, "avg_entropy_of_first_answer": 2.36885748969184, "lex_sim": 1.0}
{"question": "when did sussex split into east and west", "answer": ["June 1"], "scores": [[0.004673907533288002, 0.9661312103271484, 0.6596007347106934]], "normalized_score": [0.14387954771518707], "tokens": [["\u2581June", "\u2581", "1"]], "entropy": [[1.8805205821990967, 0.20334306359291077, 1.2209181785583496]], "avg_entropy": [1.101593941450119], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1832", "score_of_biggest_cluster": 0.14387954771518707, "score_of_first_answer": 0.14387954771518707, "max_score_of_first_answer": 0.9661312103271484, "min_score_of_first_answer": 0.004673907533288002, "avg_score": 0.14387954771518707, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40101125317182784, "std_of_first_answer": 0.40101125317182784, "avg_entropy_of_first_answer": 1.101593941450119, "lex_sim": 1.0}
{"question": "where was the witch of blackbird pond published", "answer": ["Salem, Massachusetts"], "scores": [[0.006019942462444305, 0.37711650133132935, 0.6286319494247437, 0.8144716024398804]], "normalized_score": [0.1846439391374588], "tokens": [["\u2581S", "alem", ",", "\u2581Massachusetts"]], "entropy": [[4.866973876953125, 1.0768550634384155, 1.363050103187561, 0.9563598036766052]], "avg_entropy": [2.0658097118139267], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "United States", "score_of_biggest_cluster": 0.1846439391374588, "score_of_first_answer": 0.1846439391374588, "max_score_of_first_answer": 0.8144716024398804, "min_score_of_first_answer": 0.006019942462444305, "avg_score": 0.1846439391374588, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30290554792978686, "std_of_first_answer": 0.30290554792978686, "avg_entropy_of_first_answer": 2.0658097118139267, "lex_sim": 1.0}
{"question": "when does zay come to girl meets world", "answer": ["Season two"], "scores": [[0.03913610801100731, 0.009006776846945286]], "normalized_score": [0.018774723634123802], "tokens": [["\u2581Season", "\u2581two"]], "entropy": [[3.9347898960113525, 0.4965289235115051]], "avg_entropy": [2.215659409761429], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "May\u00a013,\u00a02015", "score_of_biggest_cluster": 0.018774723634123802, "score_of_first_answer": 0.018774723634123802, "max_score_of_first_answer": 0.03913610801100731, "min_score_of_first_answer": 0.009006776846945286, "avg_score": 0.018774723634123802, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.015064665582031012, "std_of_first_answer": 0.015064665582031012, "avg_entropy_of_first_answer": 2.215659409761429, "lex_sim": 1.0}
{"question": "who plays elsa's aunt in once upon a time", "answer": ["Emilie de Ravin"], "scores": [[0.016719292849302292, 0.9990043044090271, 0.9458896517753601, 0.9765288233757019, 0.9989818930625916]], "normalized_score": [0.4340839684009552], "tokens": [["\u2581Emil", "ie", "\u2581de", "\u2581Rav", "in"]], "entropy": [[4.824082374572754, 0.01043596863746643, 0.319629430770874, 0.13290545344352722, 0.010460208170115948]], "avg_entropy": [1.0595026871189475], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Elizabeth Mitchell", "score_of_biggest_cluster": 0.4340839684009552, "score_of_first_answer": 0.4340839684009552, "max_score_of_first_answer": 0.9990043044090271, "min_score_of_first_answer": 0.016719292849302292, "avg_score": 0.4340839684009552, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3858447122790572, "std_of_first_answer": 0.3858447122790572, "avg_entropy_of_first_answer": 1.0595026871189475, "lex_sim": 1.0}
{"question": "who kidnapped ben in the deep end of the ocean", "answer": ["Orca"], "scores": [[0.007247358560562134, 0.22240515053272247]], "normalized_score": [0.04014785215258598], "tokens": [["\u2581Or", "ca"]], "entropy": [[6.298000812530518, 2.4040942192077637]], "avg_entropy": [4.351047515869141], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "celebrity alumna Cecil Lockhart", "score_of_biggest_cluster": 0.04014785215258598, "score_of_first_answer": 0.04014785215258598, "max_score_of_first_answer": 0.22240515053272247, "min_score_of_first_answer": 0.007247358560562134, "avg_score": 0.04014785215258598, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10757889598608017, "std_of_first_answer": 0.10757889598608017, "avg_entropy_of_first_answer": 4.351047515869141, "lex_sim": 1.0}
{"question": "who formed the indian society of oriental art", "answer": ["Svetoslav Roerich"], "scores": [[0.039707399904727936, 0.1780340075492859, 0.9810128808021545, 0.7836565971374512, 0.9995297193527222, 0.9985942244529724]], "normalized_score": [0.4191729724407196], "tokens": [["\u2581S", "vet", "oslav", "\u2581Ro", "er", "ich"]], "entropy": [[4.747964859008789, 3.2423102855682373, 0.15197987854480743, 1.446609616279602, 0.005889430642127991, 0.01386286411434412]], "avg_entropy": [1.6014361556929846], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Abanindranath Tagore CIE", "score_of_biggest_cluster": 0.4191729724407196, "score_of_first_answer": 0.4191729724407196, "max_score_of_first_answer": 0.9995297193527222, "min_score_of_first_answer": 0.039707399904727936, "avg_score": 0.4191729724407196, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4010923731005825, "std_of_first_answer": 0.4010923731005825, "avg_entropy_of_first_answer": 1.6014361556929846, "lex_sim": 1.0}
{"question": "distance from one side of a bridge to the other crossword", "answer": ["400"], "scores": [[0.3563137948513031, 0.0893721953034401, 0.2583286166191101, 0.39567118883132935]], "normalized_score": [0.23885563015937805], "tokens": [["\u2581", "4", "0", "0"]], "entropy": [[4.9666619300842285, 1.999070405960083, 3.0186970233917236, 2.6758460998535156]], "avg_entropy": [3.1650688648223877], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "span", "score_of_biggest_cluster": 0.23885563015937805, "score_of_first_answer": 0.23885563015937805, "max_score_of_first_answer": 0.39567118883132935, "min_score_of_first_answer": 0.0893721953034401, "avg_score": 0.23885563015937805, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11822548505534086, "std_of_first_answer": 0.11822548505534086, "avg_entropy_of_first_answer": 3.1650688648223877, "lex_sim": 1.0}
{"question": "who presides over the highest criminal court in a district", "answer": ["District Judge"], "scores": [[0.18271426856517792, 0.27133333683013916]], "normalized_score": [0.2226577252149582], "tokens": [["\u2581District", "\u2581Judge"]], "entropy": [[3.5397820472717285, 1.7647367715835571]], "avg_entropy": [2.652259409427643], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "District Judge", "score_of_biggest_cluster": 0.2226577252149582, "score_of_first_answer": 0.2226577252149582, "max_score_of_first_answer": 0.27133333683013916, "min_score_of_first_answer": 0.18271426856517792, "avg_score": 0.2226577252149582, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.04430953413248062, "std_of_first_answer": 0.04430953413248062, "avg_entropy_of_first_answer": 2.652259409427643, "lex_sim": 1.0}
{"question": "who played cory's older brother on boy meets world", "answer": ["Jason Marsden"], "scores": [[0.037643201649188995, 0.5244613885879517, 0.998685896396637]], "normalized_score": [0.2701529264450073], "tokens": [["\u2581Jason", "\u2581Mars", "den"]], "entropy": [[3.99558162689209, 2.6216330528259277, 0.013921760022640228]], "avg_entropy": [2.210378813246886], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Will Friedle", "score_of_biggest_cluster": 0.2701529264450073, "score_of_first_answer": 0.2701529264450073, "max_score_of_first_answer": 0.998685896396637, "min_score_of_first_answer": 0.037643201649188995, "avg_score": 0.2701529264450073, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3923552658933497, "std_of_first_answer": 0.3923552658933497, "avg_entropy_of_first_answer": 2.210378813246886, "lex_sim": 1.0}
{"question": "where was the new pete's dragon filmed", "answer": ["Central Park and Connecticut"], "scores": [[0.02076500840485096, 0.004917928483337164, 0.017850367352366447, 0.006736171431839466]], "normalized_score": [0.010526732541620731], "tokens": [["\u2581Central", "\u2581Park", "\u2581and", "\u2581Connecticut"]], "entropy": [[5.37390661239624, 1.600606918334961, 1.518517017364502, 5.294697284698486]], "avg_entropy": [3.4469319581985474], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "New Zealand", "score_of_biggest_cluster": 0.010526732541620731, "score_of_first_answer": 0.010526732541620731, "max_score_of_first_answer": 0.02076500840485096, "min_score_of_first_answer": 0.004917928483337164, "avg_score": 0.010526732541620731, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.006848871598664193, "std_of_first_answer": 0.006848871598664193, "avg_entropy_of_first_answer": 3.4469319581985474, "lex_sim": 1.0}
{"question": "in which country were the two battles of el alamein fought", "answer": ["Egypt"], "scores": [[0.7676954865455627]], "normalized_score": [0.7676954865455627], "tokens": [["\u2581Egypt"]], "entropy": [[1.5339914560317993]], "avg_entropy": [1.5339914560317993], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Egypt", "score_of_biggest_cluster": 0.7676954865455627, "score_of_first_answer": 0.7676954865455627, "max_score_of_first_answer": 0.7676954865455627, "min_score_of_first_answer": 0.7676954865455627, "avg_score": 0.7676954865455627, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.5339914560317993, "lex_sim": 1.0}
{"question": "who wrote the song would i lie to you", "answer": ["Charles Wetherby, Ricky Farrant"], "scores": [[0.7550678253173828, 0.013689019717276096, 0.036196690052747726, 0.4642658531665802, 0.01950855553150177, 0.005561217200011015, 0.3820093870162964, 0.08992075175046921, 0.09724665433168411, 0.1407405287027359]], "normalized_score": [0.07847684621810913], "tokens": [["\u2581Charles", "\u2581W", "ether", "by", ",", "\u2581R", "icky", "\u2581F", "arr", "ant"]], "entropy": [[1.772840142250061, 5.050262451171875, 3.4104464054107666, 1.8401575088500977, 1.8717994689941406, 5.141427516937256, 2.072700023651123, 3.951176404953003, 2.702275037765503, 1.9205381870269775]], "avg_entropy": [2.97336231470108], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Peter Vale", "score_of_biggest_cluster": 0.07847684621810913, "score_of_first_answer": 0.07847684621810913, "max_score_of_first_answer": 0.7550678253173828, "min_score_of_first_answer": 0.005561217200011015, "avg_score": 0.07847684621810913, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2385742364026406, "std_of_first_answer": 0.2385742364026406, "avg_entropy_of_first_answer": 2.97336231470108, "lex_sim": 1.0}
{"question": "where does prime rib come from on a cow", "answer": ["it\u2019s the 12th rib"], "scores": [[0.0059801931492984295, 0.026038210839033127, 0.9993340373039246, 0.1309012621641159, 0.07922230660915375, 0.7918687462806702, 0.7806514501571655, 0.9731516242027283, 0.9209755063056946]], "normalized_score": [0.2127794623374939], "tokens": [["\u2581it", "\u2019", "s", "\u2581the", "\u2581", "1", "2", "th", "\u2581rib"]], "entropy": [[4.192458152770996, 2.3395073413848877, 0.00819365680217743, 3.7361230850219727, 3.973809003829956, 0.8765403628349304, 0.849277675151825, 0.18689648807048798, 0.5331467390060425]], "avg_entropy": [1.8551058338748083], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the primal rib", "score_of_biggest_cluster": 0.2127794623374939, "score_of_first_answer": 0.2127794623374939, "max_score_of_first_answer": 0.9993340373039246, "min_score_of_first_answer": 0.0059801931492984295, "avg_score": 0.2127794623374939, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4205221883030607, "std_of_first_answer": 0.4205221883030607, "avg_entropy_of_first_answer": 1.8551058338748083, "lex_sim": 1.0}
{"question": "nickname given to railroad executives due to shady practices of their businesses", "answer": ["robber baron"], "scores": [[0.049697041511535645, 0.9994426369667053, 0.12198484688997269]], "normalized_score": [0.18230478465557098], "tokens": [["\u2581rob", "ber", "\u2581baron"]], "entropy": [[4.957013130187988, 0.006464626640081406, 0.5342130064964294]], "avg_entropy": [1.832563587774833], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Robber baron", "score_of_biggest_cluster": 0.18230478465557098, "score_of_first_answer": 0.18230478465557098, "max_score_of_first_answer": 0.9994426369667053, "min_score_of_first_answer": 0.049697041511535645, "avg_score": 0.18230478465557098, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4316858935562689, "std_of_first_answer": 0.4316858935562689, "avg_entropy_of_first_answer": 1.832563587774833, "lex_sim": 1.0}
{"question": "what is the maximum data rate for the 802.11a standard select one", "answer": ["54Mbs"], "scores": [[0.9332804083824158, 0.6825537085533142, 0.9203943014144897, 0.2009168416261673, 0.05513737350702286]], "normalized_score": [0.36518850922584534], "tokens": [["\u2581", "5", "4", "M", "bs"]], "entropy": [[0.6244322061538696, 1.1796684265136719, 0.4890245199203491, 1.847064733505249, 0.8676559925079346]], "avg_entropy": [1.0015691757202148], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "54\u00a0Mbit/s", "score_of_biggest_cluster": 0.36518850922584534, "score_of_first_answer": 0.36518850922584534, "max_score_of_first_answer": 0.9332804083824158, "min_score_of_first_answer": 0.05513737350702286, "avg_score": 0.36518850922584534, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3655288776450572, "std_of_first_answer": 0.3655288776450572, "avg_entropy_of_first_answer": 1.0015691757202148, "lex_sim": 1.0}
{"question": "who was the actor who played crocodile dundee", "answer": ["Paul Hogan"], "scores": [[0.6337417364120483, 0.9948284029960632, 0.9982750415802002]], "normalized_score": [0.8569791316986084], "tokens": [["\u2581Paul", "\u2581Hog", "an"]], "entropy": [[2.0169224739074707, 0.0594431534409523, 0.017568381503224373]], "avg_entropy": [0.6979780029505491], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paul Hogan", "score_of_biggest_cluster": 0.8569791316986084, "score_of_first_answer": 0.8569791316986084, "max_score_of_first_answer": 0.9982750415802002, "min_score_of_first_answer": 0.6337417364120483, "avg_score": 0.8569791316986084, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.17103605554875853, "std_of_first_answer": 0.17103605554875853, "avg_entropy_of_first_answer": 0.6979780029505491, "lex_sim": 1.0}
{"question": "what time do tam tams start in montreal", "answer": ["noon"], "scores": [[0.01852864772081375, 0.9692911505699158]], "normalized_score": [0.13401362299919128], "tokens": [["\u2581no", "on"]], "entropy": [[2.6574201583862305, 0.25154727697372437]], "avg_entropy": [1.4544837176799774], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "around 10:30am", "score_of_biggest_cluster": 0.13401362299919128, "score_of_first_answer": 0.13401362299919128, "max_score_of_first_answer": 0.9692911505699158, "min_score_of_first_answer": 0.01852864772081375, "avg_score": 0.13401362299919128, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.475381251424551, "std_of_first_answer": 0.475381251424551, "avg_entropy_of_first_answer": 1.4544837176799774, "lex_sim": 1.0}
{"question": "when does the eclipse end in the us", "answer": ["July 13"], "scores": [[0.061140719801187515, 0.9725905656814575, 0.19202037155628204, 0.10747502744197845]], "normalized_score": [0.18716683983802795], "tokens": [["\u2581July", "\u2581", "1", "3"]], "entropy": [[3.572435140609741, 0.16239343583583832, 0.7534376382827759, 2.3953089714050293]], "avg_entropy": [1.7208937965333462], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2:44\u00a0p.m. EDT", "score_of_biggest_cluster": 0.18716683983802795, "score_of_first_answer": 0.18716683983802795, "max_score_of_first_answer": 0.9725905656814575, "min_score_of_first_answer": 0.061140719801187515, "avg_score": 0.18716683983802795, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3720618069063474, "std_of_first_answer": 0.3720618069063474, "avg_entropy_of_first_answer": 1.7208937965333462, "lex_sim": 1.0}
{"question": "who is the supreme justice of the supreme court", "answer": ["Sonia Sotomayor"], "scores": [[0.027284257113933563, 0.9873676896095276, 0.9530230760574341, 0.9989141225814819, 0.9801833629608154, 0.9902652502059937, 0.9990366697311401]], "normalized_score": [0.5899415612220764], "tokens": [["\u2581Son", "ia", "\u2581S", "ot", "om", "ay", "or"]], "entropy": [[2.3963162899017334, 0.08556368201971054, 0.36209094524383545, 0.012844784185290337, 0.11912857741117477, 0.06429500877857208, 0.010769754648208618]], "avg_entropy": [0.43585843459836077], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Roberts", "score_of_biggest_cluster": 0.5899415612220764, "score_of_first_answer": 0.5899415612220764, "max_score_of_first_answer": 0.9990366697311401, "min_score_of_first_answer": 0.027284257113933563, "avg_score": 0.5899415612220764, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33537325050907113, "std_of_first_answer": 0.33537325050907113, "avg_entropy_of_first_answer": 0.43585843459836077, "lex_sim": 1.0}
{"question": "when was the time the eagles won the superbowl", "answer": ["January 20, 1985"], "scores": [[0.2542557716369629, 0.9699786305427551, 0.9384053349494934, 0.05400296673178673, 0.5629318356513977, 0.9815056920051575, 0.31872686743736267, 0.9989155530929565, 0.8961943984031677, 0.8177217841148376]], "normalized_score": [0.5256716012954712], "tokens": [["\u2581January", "\u2581", "2", "0", ",", "\u2581", "1", "9", "8", "5"]], "entropy": [[2.1869499683380127, 0.1798565685749054, 0.2803497016429901, 1.7674819231033325, 1.2184813022613525, 0.10442657023668289, 0.6277174353599548, 0.011196818202733994, 0.4515707492828369, 0.7534202337265015]], "avg_entropy": [0.7581451270729304], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2017", "score_of_biggest_cluster": 0.5256716012954712, "score_of_first_answer": 0.5256716012954712, "max_score_of_first_answer": 0.9989155530929565, "min_score_of_first_answer": 0.05400296673178673, "avg_score": 0.5256716012954712, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3356380922546707, "std_of_first_answer": 0.3356380922546707, "avg_entropy_of_first_answer": 0.7581451270729304, "lex_sim": 1.0}
{"question": "a country having an island location and a country having continental location", "answer": ["Continental and island"], "scores": [[0.06721314042806625, 0.7440495491027832, 0.5790665149688721, 0.21454663574695587]], "normalized_score": [0.28075435757637024], "tokens": [["\u2581Cont", "inental", "\u2581and", "\u2581island"]], "entropy": [[4.604207515716553, 0.7722511291503906, 2.2569355964660645, 1.8368257284164429]], "avg_entropy": [2.3675549924373627], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Australia", "score_of_biggest_cluster": 0.28075435757637024, "score_of_first_answer": 0.28075435757637024, "max_score_of_first_answer": 0.7440495491027832, "min_score_of_first_answer": 0.06721314042806625, "avg_score": 0.28075435757637024, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27183129408831874, "std_of_first_answer": 0.27183129408831874, "avg_entropy_of_first_answer": 2.3675549924373627, "lex_sim": 1.0}
{"question": "glycogen and amylopectin are long chains of which simple sugar(s)", "answer": ["glucose"], "scores": [[0.69029700756073, 0.9979690909385681, 0.9979684948921204]], "normalized_score": [0.8825851678848267], "tokens": [["\u2581gl", "uc", "ose"]], "entropy": [[1.7001230716705322, 0.01817305013537407, 0.01923823542892933]], "avg_entropy": [0.5791781190782785], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "glucose", "score_of_biggest_cluster": 0.8825851678848267, "score_of_first_answer": 0.8825851678848267, "max_score_of_first_answer": 0.9979690909385681, "min_score_of_first_answer": 0.69029700756073, "avg_score": 0.8825851678848267, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.14503787053621722, "std_of_first_answer": 0.14503787053621722, "avg_entropy_of_first_answer": 0.5791781190782785, "lex_sim": 1.0}
{"question": "where does the lincoln highway begin and end", "answer": ["The Lincoln Highway begins in Times Square, New York City and ends in Lincoln Park in San Francisco."], "scores": [[0.03092912770807743, 0.3120202124118805, 0.7410256862640381, 0.25298383831977844, 0.6538087129592896, 0.35588735342025757, 0.9965860843658447, 0.5686765909194946, 0.8250901103019714, 0.9973369240760803, 0.6577301621437073, 0.3948335647583008, 0.8991393446922302, 0.5956200957298279, 0.6979057192802429, 0.8332445621490479, 0.08960393071174622, 0.9144929051399231, 0.9967461824417114, 0.26761138439178467]], "normalized_score": [0.47535640001296997], "tokens": [["\u2581The", "\u2581Lincoln", "\u2581Highway", "\u2581begins", "\u2581in", "\u2581Times", "\u2581Square", ",", "\u2581New", "\u2581York", "\u2581City", "\u2581and", "\u2581ends", "\u2581in", "\u2581Lincoln", "\u2581Park", "\u2581in", "\u2581San", "\u2581Francisco", "."]], "entropy": [[4.57418155670166, 4.431941509246826, 1.220973253250122, 3.372176170349121, 1.1852991580963135, 1.85073721408844, 0.027553971856832504, 1.307016372680664, 0.8497555255889893, 0.025311844423413277, 1.0729877948760986, 1.2833744287490845, 0.6922124624252319, 1.115691900253296, 1.1040629148483276, 0.702724814414978, 0.5315760374069214, 0.6062290072441101, 0.029476556926965714, 1.1844096183776855]], "avg_entropy": [1.358384605590254], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lincoln Park in San Francisco", "score_of_biggest_cluster": 0.47535640001296997, "score_of_first_answer": 0.47535640001296997, "max_score_of_first_answer": 0.9973369240760803, "min_score_of_first_answer": 0.03092912770807743, "avg_score": 0.47535640001296997, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30018829333316327, "std_of_first_answer": 0.30018829333316327, "avg_entropy_of_first_answer": 1.358384605590254, "lex_sim": 1.0}
{"question": "where does the electron transport chain pumps protons", "answer": ["Mitochondria"], "scores": [[0.0820533037185669, 0.9808722734451294, 0.9922069907188416, 0.9966810345649719, 0.6411105394363403]], "normalized_score": [0.5515183806419373], "tokens": [["\u2581M", "ito", "ch", "ond", "ria"]], "entropy": [[3.92454195022583, 0.10895253717899323, 0.06873402744531631, 0.031615935266017914, 0.9185308218002319]], "avg_entropy": [1.010475054383278], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "mitochondrial membrane", "score_of_biggest_cluster": 0.5515183806419373, "score_of_first_answer": 0.5515183806419373, "max_score_of_first_answer": 0.9966810345649719, "min_score_of_first_answer": 0.0820533037185669, "avg_score": 0.5515183806419373, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35501434866552406, "std_of_first_answer": 0.35501434866552406, "avg_entropy_of_first_answer": 1.010475054383278, "lex_sim": 1.0}
{"question": "is it marley and me or marley and i", "answer": ["Marley and Me"], "scores": [[0.7724690437316895, 0.9979299306869507, 0.6820058822631836, 0.7303097248077393]], "normalized_score": [0.7871710062026978], "tokens": [["\u2581Mar", "ley", "\u2581and", "\u2581Me"]], "entropy": [[1.3887391090393066, 0.01984853297472, 1.0222156047821045, 0.9401844143867493]], "avg_entropy": [0.8427469152957201], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marley & Me", "score_of_biggest_cluster": 0.7871710062026978, "score_of_first_answer": 0.7871710062026978, "max_score_of_first_answer": 0.9979299306869507, "min_score_of_first_answer": 0.6820058822631836, "avg_score": 0.7871710062026978, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.12107731090958083, "std_of_first_answer": 0.12107731090958083, "avg_entropy_of_first_answer": 0.8427469152957201, "lex_sim": 1.0}
{"question": "what episode does caroline get turned into a vampire", "answer": ["Season 2, episode 13"], "scores": [[0.18323303759098053, 0.8766885995864868, 0.2612898349761963, 0.3684578239917755, 0.26920750737190247, 0.9868546724319458, 0.37350040674209595, 0.09585902094841003]], "normalized_score": [0.33185848593711853], "tokens": [["\u2581Season", "\u2581", "2", ",", "\u2581episode", "\u2581", "1", "3"]], "entropy": [[3.8779094219207764, 0.6988749504089355, 1.8882783651351929, 1.7532131671905518, 0.9140259027481079, 0.11253225803375244, 1.9275633096694946, 2.4801464080810547]], "avg_entropy": [1.7065679728984833], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brave New World", "score_of_biggest_cluster": 0.33185848593711853, "score_of_first_answer": 0.33185848593711853, "max_score_of_first_answer": 0.9868546724319458, "min_score_of_first_answer": 0.09585902094841003, "avg_score": 0.33185848593711853, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3048322058684135, "std_of_first_answer": 0.3048322058684135, "avg_entropy_of_first_answer": 1.7065679728984833, "lex_sim": 1.0}
{"question": "who had created the second bank of the united states", "answer": ["Nicholas Biddle"], "scores": [[0.5279811024665833, 0.9855692982673645, 0.9958187937736511]], "normalized_score": [0.803209125995636], "tokens": [["\u2581Nicholas", "\u2581B", "iddle"]], "entropy": [[1.9350025653839111, 0.13305622339248657, 0.03551448509097099]], "avg_entropy": [0.7011910912891229], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "President James Madison", "score_of_biggest_cluster": 0.803209125995636, "score_of_first_answer": 0.803209125995636, "max_score_of_first_answer": 0.9958187937736511, "min_score_of_first_answer": 0.5279811024665833, "avg_score": 0.803209125995636, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2181651042729578, "std_of_first_answer": 0.2181651042729578, "avg_entropy_of_first_answer": 0.7011910912891229, "lex_sim": 1.0}
{"question": "how many gold medals did australia win in the 2000 olympics", "answer": ["zero"], "scores": [[0.00421440415084362]], "normalized_score": [0.004214405082166195], "tokens": [["\u2581zero"]], "entropy": [[2.1586108207702637]], "avg_entropy": [2.1586108207702637], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "16", "score_of_biggest_cluster": 0.004214405082166195, "score_of_first_answer": 0.004214405082166195, "max_score_of_first_answer": 0.00421440415084362, "min_score_of_first_answer": 0.00421440415084362, "avg_score": 0.004214405082166195, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.1586108207702637, "lex_sim": 1.0}
{"question": "what are the monomer building blocks of dna and rna", "answer": ["adenine, guanine, cytosine, and uracil"], "scores": [[0.29872214794158936, 0.9920368194580078, 0.9741721153259277, 0.900493860244751, 0.61738121509552, 0.9960022568702698, 0.9982654452323914, 0.9929963946342468, 0.6912459135055542, 0.995964527130127, 0.9993447661399841, 0.9989144802093506, 0.791780948638916, 0.33529454469680786, 0.20722749829292297, 0.9905410408973694, 0.9910348057746887]], "normalized_score": [0.7392526865005493], "tokens": [["\u2581ad", "en", "ine", ",", "\u2581gu", "an", "ine", ",", "\u2581c", "yt", "os", "ine", ",", "\u2581and", "\u2581u", "rac", "il"]], "entropy": [[2.615307092666626, 0.07575607299804688, 0.14248497784137726, 0.62230384349823, 1.0748553276062012, 0.039803341031074524, 0.016546806320548058, 0.0570097491145134, 0.7690161466598511, 0.029968254268169403, 0.006795418448746204, 0.011271275579929352, 0.5921440720558167, 1.0520755052566528, 0.5602926015853882, 0.07397721707820892, 0.07146014273166656]], "avg_entropy": [0.4594745791024145], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nucleotides", "score_of_biggest_cluster": 0.7392526865005493, "score_of_first_answer": 0.7392526865005493, "max_score_of_first_answer": 0.9993447661399841, "min_score_of_first_answer": 0.20722749829292297, "avg_score": 0.7392526865005493, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2706115474693345, "std_of_first_answer": 0.2706115474693345, "avg_entropy_of_first_answer": 0.4594745791024145, "lex_sim": 1.0}
{"question": "what is the meaning of shalom in english", "answer": ["peace"], "scores": [[0.41346532106399536]], "normalized_score": [0.41346532106399536], "tokens": [["\u2581peace"]], "entropy": [[2.441375255584717]], "avg_entropy": [2.441375255584717], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "peace", "score_of_biggest_cluster": 0.41346532106399536, "score_of_first_answer": 0.41346532106399536, "max_score_of_first_answer": 0.41346532106399536, "min_score_of_first_answer": 0.41346532106399536, "avg_score": 0.41346532106399536, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.441375255584717, "lex_sim": 1.0}
{"question": "when was the last time oklahoma won a national championship in football", "answer": ["2000"], "scores": [[0.8911232352256775, 0.7948598861694336, 0.9956385493278503, 0.9333840608596802, 0.813650906085968]], "normalized_score": [0.882603645324707], "tokens": [["\u2581", "2", "0", "0", "0"]], "entropy": [[0.8316833972930908, 0.6008540987968445, 0.04182518273591995, 0.24947717785835266, 0.7609872817993164]], "avg_entropy": [0.4969654276967049], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2003", "score_of_biggest_cluster": 0.882603645324707, "score_of_first_answer": 0.882603645324707, "max_score_of_first_answer": 0.9956385493278503, "min_score_of_first_answer": 0.7948598861694336, "avg_score": 0.882603645324707, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07460919390558554, "std_of_first_answer": 0.07460919390558554, "avg_entropy_of_first_answer": 0.4969654276967049, "lex_sim": 1.0}
{"question": "who played john clark sr on nypd blue", "answer": ["Robert Conrad"], "scores": [[0.0154712600633502, 0.03408303111791611, 0.9990973472595215]], "normalized_score": [0.08076514303684235], "tokens": [["\u2581Robert", "\u2581Con", "rad"]], "entropy": [[4.758252143859863, 2.822990894317627, 0.010757816955447197]], "avg_entropy": [2.530666951710979], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joe Spano", "score_of_biggest_cluster": 0.08076514303684235, "score_of_first_answer": 0.08076514303684235, "max_score_of_first_answer": 0.9990973472595215, "min_score_of_first_answer": 0.0154712600633502, "avg_score": 0.08076514303684235, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45936179251373527, "std_of_first_answer": 0.45936179251373527, "avg_entropy_of_first_answer": 2.530666951710979, "lex_sim": 1.0}
{"question": "where does the term pop music come from", "answer": ["from the slang pop (popular) and beat (rhythm)."], "scores": [[0.021787704899907112, 0.36891108751296997, 0.12049151957035065, 0.9945810437202454, 0.059254761785268784, 0.027248963713645935, 0.13122780621051788, 0.8769472241401672, 0.7059299349784851, 0.08940392732620239, 0.028284259140491486, 0.44716623425483704, 0.36688482761383057, 0.8893658518791199, 0.07295475900173187]], "normalized_score": [0.17196491360664368], "tokens": [["\u2581from", "\u2581the", "\u2581sl", "ang", "\u2581pop", "\u2581(", "pop", "ular", ")", "\u2581and", "\u2581beat", "\u2581(", "rh", "ythm", ")."]], "entropy": [[5.14285945892334, 3.50938081741333, 4.299954414367676, 0.049419473856687546, 2.9275457859039307, 3.3977584838867188, 4.49411678314209, 0.5566697716712952, 1.2541855573654175, 1.750376582145691, 3.8071837425231934, 1.7458415031433105, 3.0168819427490234, 0.35795503854751587, 1.1695528030395508]], "avg_entropy": [2.4986454772452515], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Britain", "score_of_biggest_cluster": 0.17196491360664368, "score_of_first_answer": 0.17196491360664368, "max_score_of_first_answer": 0.9945810437202454, "min_score_of_first_answer": 0.021787704899907112, "avg_score": 0.17196491360664368, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3431039826979569, "std_of_first_answer": 0.3431039826979569, "avg_entropy_of_first_answer": 2.4986454772452515, "lex_sim": 1.0}
{"question": "who was the leader of the zulu in south africa who led the fight against the british", "answer": ["Cetshwayo"], "scores": [[0.3575114905834198, 0.9752187132835388, 0.9973925352096558, 0.9969438910484314]], "normalized_score": [0.7673300504684448], "tokens": [["\u2581Cet", "sh", "way", "o"]], "entropy": [[2.442458152770996, 0.14613460004329681, 0.02626125141978264, 0.03270634636282921]], "avg_entropy": [0.6618900876492262], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cetshwayo", "score_of_biggest_cluster": 0.7673300504684448, "score_of_first_answer": 0.7673300504684448, "max_score_of_first_answer": 0.9973925352096558, "min_score_of_first_answer": 0.3575114905834198, "avg_score": 0.7673300504684448, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2739579828666267, "std_of_first_answer": 0.2739579828666267, "avg_entropy_of_first_answer": 0.6618900876492262, "lex_sim": 1.0}
{"question": "where is salt mined in the united states", "answer": ["Nevada, Utah, and Louisiana"], "scores": [[0.04724203050136566, 0.9989655017852783, 0.1660638153553009, 0.46978721022605896, 0.9999630451202393, 0.7958699464797974, 0.3461105227470398, 0.02328651025891304]], "normalized_score": [0.26402804255485535], "tokens": [["\u2581Nev", "ada", ",", "\u2581Ut", "ah", ",", "\u2581and", "\u2581Louisiana"]], "entropy": [[4.635983467102051, 0.009779727086424828, 0.9232627153396606, 2.4738426208496094, 0.0005436824867501855, 0.7142003774642944, 2.576162099838257, 2.7957324981689453]], "avg_entropy": [1.766188398541999], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Avery Island, Louisiana", "score_of_biggest_cluster": 0.26402804255485535, "score_of_first_answer": 0.26402804255485535, "max_score_of_first_answer": 0.9999630451202393, "min_score_of_first_answer": 0.02328651025891304, "avg_score": 0.26402804255485535, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37945804051521437, "std_of_first_answer": 0.37945804051521437, "avg_entropy_of_first_answer": 1.766188398541999, "lex_sim": 1.0}
{"question": "where was the remake of wake in fright filmed", "answer": ["Australia"], "scores": [[0.17191514372825623]], "normalized_score": [0.17191512882709503], "tokens": [["\u2581Australia"]], "entropy": [[4.849885940551758]], "avg_entropy": [4.849885940551758], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Broken", "score_of_biggest_cluster": 0.17191512882709503, "score_of_first_answer": 0.17191512882709503, "max_score_of_first_answer": 0.17191514372825623, "min_score_of_first_answer": 0.17191514372825623, "avg_score": 0.17191512882709503, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.849885940551758, "lex_sim": 1.0}
{"question": "when did the military plane crash in mississippi", "answer": ["May 13, 2017"], "scores": [[0.08132793009281158, 0.9636188745498657, 0.20241281390190125, 0.03333394601941109, 0.5871279239654541, 0.9886156916618347, 0.42203283309936523, 0.9995248317718506, 0.6676797270774841, 0.11806006729602814]], "normalized_score": [0.31687217950820923], "tokens": [["\u2581May", "\u2581", "1", "3", ",", "\u2581", "2", "0", "1", "7"]], "entropy": [[2.68642520904541, 0.21505123376846313, 0.9737468361854553, 2.342137336730957, 0.9469496011734009, 0.07522407919168472, 0.6986002922058105, 0.005360498558729887, 0.6747568845748901, 2.261735439300537]], "avg_entropy": [1.087998741073534], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "July 10, 2017", "score_of_biggest_cluster": 0.31687217950820923, "score_of_first_answer": 0.31687217950820923, "max_score_of_first_answer": 0.9995248317718506, "min_score_of_first_answer": 0.03333394601941109, "avg_score": 0.31687217950820923, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37041773726578653, "std_of_first_answer": 0.37041773726578653, "avg_entropy_of_first_answer": 1.087998741073534, "lex_sim": 1.0}
{"question": "who turns into a bear in the hobbit", "answer": ["The bear of course."], "scores": [[0.0048459540121257305, 0.042285021394491196, 0.046027444303035736, 0.021832149475812912, 0.08403944969177246]], "normalized_score": [0.028030604124069214], "tokens": [["\u2581The", "\u2581bear", "\u2581of", "\u2581course", "."]], "entropy": [[2.9629251956939697, 4.847156047821045, 3.8442587852478027, 5.1118035316467285, 2.288515567779541]], "avg_entropy": [3.8109318256378173], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Beorn", "score_of_biggest_cluster": 0.028030604124069214, "score_of_first_answer": 0.028030604124069214, "max_score_of_first_answer": 0.08403944969177246, "min_score_of_first_answer": 0.0048459540121257305, "avg_score": 0.028030604124069214, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.026633490593854642, "std_of_first_answer": 0.026633490593854642, "avg_entropy_of_first_answer": 3.8109318256378173, "lex_sim": 1.0}
{"question": "by 1914 which of the following african countries were still independent", "answer": ["Tanzania"], "scores": [[0.009283658117055893, 0.6111941337585449, 0.995486319065094]], "normalized_score": [0.17809206247329712], "tokens": [["\u2581T", "anz", "ania"]], "entropy": [[4.28779935836792, 1.23974609375, 0.0375826321542263]], "avg_entropy": [1.8550426947573821], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Liberia", "score_of_biggest_cluster": 0.17809206247329712, "score_of_first_answer": 0.17809206247329712, "max_score_of_first_answer": 0.995486319065094, "min_score_of_first_answer": 0.009283658117055893, "avg_score": 0.17809206247329712, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4058697646667295, "std_of_first_answer": 0.4058697646667295, "avg_entropy_of_first_answer": 1.8550426947573821, "lex_sim": 1.0}
{"question": "when did houston go to the american league", "answer": ["1961"], "scores": [[0.8379316926002502, 0.9374637007713318, 0.9874953627586365, 0.8215980529785156, 0.2377554476261139]], "normalized_score": [0.6856423020362854], "tokens": [["\u2581", "1", "9", "6", "1"]], "entropy": [[1.0229731798171997, 0.26111939549446106, 0.0864700973033905, 0.565056562423706, 0.9765805602073669]], "avg_entropy": [0.5824399590492249], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2013", "score_of_biggest_cluster": 0.6856423020362854, "score_of_first_answer": 0.6856423020362854, "max_score_of_first_answer": 0.9874953627586365, "min_score_of_first_answer": 0.2377554476261139, "avg_score": 0.6856423020362854, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2704646317369751, "std_of_first_answer": 0.2704646317369751, "avg_entropy_of_first_answer": 0.5824399590492249, "lex_sim": 1.0}
{"question": "who plays zoey in i love you man", "answer": ["Jamie Lee Curtis"], "scores": [[0.0035258280113339424, 0.9804845452308655, 0.23119661211967468, 0.9840811491012573, 0.9990646243095398]], "normalized_score": [0.23936551809310913], "tokens": [["\u2581Jam", "ie", "\u2581Lee", "\u2581Curt", "is"]], "entropy": [[4.328444480895996, 0.1489984393119812, 2.6189980506896973, 0.12649519741535187, 0.010054217651486397]], "avg_entropy": [1.4465980771929026], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rashida Jones", "score_of_biggest_cluster": 0.23936551809310913, "score_of_first_answer": 0.23936551809310913, "max_score_of_first_answer": 0.9990646243095398, "min_score_of_first_answer": 0.0035258280113339424, "avg_score": 0.23936551809310913, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4325431784247255, "std_of_first_answer": 0.4325431784247255, "avg_entropy_of_first_answer": 1.4465980771929026, "lex_sim": 1.0}
{"question": "where is hallmark channel home and family filmed", "answer": ["Dallas, Texas"], "scores": [[0.00312244170345366, 0.7385895848274231, 0.6760812401771545]], "normalized_score": [0.11595750600099564], "tokens": [["\u2581Dallas", ",", "\u2581Texas"]], "entropy": [[3.593393325805664, 0.893856942653656, 0.8191457986831665]], "avg_entropy": [1.7687986890474956], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Los Angeles, California", "score_of_biggest_cluster": 0.11595750600099564, "score_of_first_answer": 0.11595750600099564, "max_score_of_first_answer": 0.7385895848274231, "min_score_of_first_answer": 0.00312244170345366, "avg_score": 0.11595750600099564, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3329485706380053, "std_of_first_answer": 0.3329485706380053, "avg_entropy_of_first_answer": 1.7687986890474956, "lex_sim": 1.0}
{"question": "does archie end up with betty or veronica in riverdale", "answer": ["Betty"], "scores": [[0.425119549036026]], "normalized_score": [0.425119549036026], "tokens": [["\u2581Betty"]], "entropy": [[2.382113456726074]], "avg_entropy": [2.382113456726074], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Betty", "score_of_biggest_cluster": 0.425119549036026, "score_of_first_answer": 0.425119549036026, "max_score_of_first_answer": 0.425119549036026, "min_score_of_first_answer": 0.425119549036026, "avg_score": 0.425119549036026, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.382113456726074, "lex_sim": 1.0}
{"question": "where does the phrase train of thought come from", "answer": ["Thought"], "scores": [[0.007566365413367748, 0.8931694030761719]], "normalized_score": [0.0822073444724083], "tokens": [["\u2581Th", "ought"]], "entropy": [[6.258732795715332, 0.6728328466415405]], "avg_entropy": [3.4657828211784363], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Thomas Hobbes in his Leviathan", "score_of_biggest_cluster": 0.0822073444724083, "score_of_first_answer": 0.0822073444724083, "max_score_of_first_answer": 0.8931694030761719, "min_score_of_first_answer": 0.007566365413367748, "avg_score": 0.0822073444724083, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44280151883140206, "std_of_first_answer": 0.44280151883140206, "avg_entropy_of_first_answer": 3.4657828211784363, "lex_sim": 1.0}
{"question": "who did carolina lose to in the super bowl", "answer": ["the New England Patriots"], "scores": [[0.022660614922642708, 0.09617650508880615, 0.606661856174469, 0.9774177670478821, 0.9989927411079407]], "normalized_score": [0.26435407996177673], "tokens": [["\u2581the", "\u2581New", "\u2581England", "\u2581Patri", "ots"]], "entropy": [[3.199037790298462, 3.649357318878174, 0.8744100332260132, 0.15050360560417175, 0.009275304153561592]], "avg_entropy": [1.5765168104320764], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Denver Broncos", "score_of_biggest_cluster": 0.26435407996177673, "score_of_first_answer": 0.26435407996177673, "max_score_of_first_answer": 0.9989927411079407, "min_score_of_first_answer": 0.022660614922642708, "avg_score": 0.26435407996177673, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4173897572171306, "std_of_first_answer": 0.4173897572171306, "avg_entropy_of_first_answer": 1.5765168104320764, "lex_sim": 1.0}
{"question": "who caused to build qutub minar in delhi", "answer": ["Qutb-ud-din Aibak"], "scores": [[0.8171529173851013, 0.9897045493125916, 0.23517127335071564, 0.6877692937850952, 0.9530230760574341, 0.993078887462616, 0.7982082962989807, 0.5966154932975769, 0.9985974431037903, 0.9753742814064026]], "normalized_score": [0.7514663934707642], "tokens": [["\u2581Q", "ut", "b", "-", "ud", "-", "din", "\u2581A", "ib", "ak"]], "entropy": [[1.1984800100326538, 0.09637131541967392, 1.5976183414459229, 1.2567180395126343, 0.27188897132873535, 0.05483657866716385, 0.5280787944793701, 1.246442198753357, 0.013867699541151524, 0.1409551352262497]], "avg_entropy": [0.6405257084406912], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Iltutmish", "score_of_biggest_cluster": 0.7514663934707642, "score_of_first_answer": 0.7514663934707642, "max_score_of_first_answer": 0.9985974431037903, "min_score_of_first_answer": 0.23517127335071564, "avg_score": 0.7514663934707642, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23237266640910853, "std_of_first_answer": 0.23237266640910853, "avg_entropy_of_first_answer": 0.6405257084406912, "lex_sim": 1.0}
{"question": "who is the first indian woman to be canonized as a saint", "answer": ["Mother Teresa As usual, I was surprised by several of these results."], "scores": [[0.39320239424705505, 0.8703640699386597, 0.9931015968322754, 8.628732643956027e-07, 0.0001691157667664811, 0.6522150635719299, 0.07974766194820404, 0.047029994428157806, 0.011469434946775436, 0.27586716413497925, 0.0017146028112620115, 0.5248934030532837, 0.19057463109493256, 0.0390857569873333, 0.7317271828651428, 0.11147455871105194, 0.0029078456573188305, 0.004878865089267492, 0.07409080862998962, 0.05912117287516594, 0.016743628308176994, 0.7412800788879395, 0.9006693959236145, 0.03290437534451485, 0.49040088057518005, 0.020672516897320747, 0.03922047093510628, 0.006021073553711176, 0.23313875496387482, 0.025583462789654732, 0.02879076637327671, 0.9053992033004761, 3.732258846866898e-05, 0.500434398651123, 0.0005426579737104475, 0.7450818419456482, 0.03086540289223194, 0.06337029486894608, 0.007610913831740618, 0.12203149497509003, 0.9879777431488037, 0.5233849883079529, 0.04055256396532059, 0.19390499591827393, 0.25973865389823914, 0.18738889694213867, 0.03859737515449524, 0.35667601227760315, 0.057744719088077545]], "normalized_score": [0.05324077606201172], "tokens": [["\u2581Mother", "\u2581Ter", "esa", "\u2581As", "\u2581usual", ",", "\u2581I", "\u2581was", "\u2581surprised", "\u2581by", "\u2581several", "\u2581of", "\u2581these", "\u2581results", ".", "\u2581The", "\u2581responses", ",", "\u2581as", "\u2581I", "\u2581understand", "\u2581them", ",", "\u2581to", "\u2581the", "\u2581queries", ",", "\u2581for", "\u2581example", "\u2581\"", "how", "\u2581many", "\u2581goals", "\u2581did", "\u2581cr", "7", "\u2581scored", "\u2581for", "\u2581man", "\u2581un", "ited", "\"", "\u2581was", "\u2581", "1", "1", "0", ".", "\u2581That"]], "entropy": [[2.3167953491210938, 0.7451239824295044, 0.0522850826382637, 0.45998504757881165, 3.033278703689575, 2.1638596057891846, 4.1810479164123535, 3.835486888885498, 4.895254611968994, 2.4595541954040527, 2.1462302207946777, 2.3819000720977783, 1.2574610710144043, 2.98057222366333, 1.2719959020614624, 3.998605489730835, 5.58183479309082, 3.138105869293213, 4.243229866027832, 3.412778854370117, 4.0662970542907715, 0.8729729056358337, 0.6714255809783936, 3.309269428253174, 2.7607574462890625, 4.264974594116211, 4.032865524291992, 3.8133745193481445, 1.754835844039917, 1.487470030784607, 2.8036246299743652, 0.6031464338302612, 2.3798813819885254, 2.226201295852661, 5.105084419250488, 1.5011115074157715, 1.2133885622024536, 1.7893491983413696, 2.4067206382751465, 1.349273681640625, 0.10997842997312546, 1.978424072265625, 3.316110372543335, 4.274909973144531, 2.1391680240631104, 2.5673515796661377, 2.958904504776001, 2.5149269104003906, 4.2095184326171875]], "avg_entropy": [2.592585769843082], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Saint Alphonsa", "score_of_biggest_cluster": 0.05324077606201172, "score_of_first_answer": 0.05324077606201172, "max_score_of_first_answer": 0.9931015968322754, "min_score_of_first_answer": 8.628732643956027e-07, "avg_score": 0.05324077606201172, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3148648379037167, "std_of_first_answer": 0.3148648379037167, "avg_entropy_of_first_answer": 2.592585769843082, "lex_sim": 1.0}
{"question": "who sings stop listen what's that sound", "answer": ["The Who"], "scores": [[0.11451604217290878, 0.4157522916793823]], "normalized_score": [0.21819788217544556], "tokens": [["\u2581The", "\u2581Who"]], "entropy": [[4.9672956466674805, 3.197887897491455]], "avg_entropy": [4.082591772079468], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Buffalo Springfield", "score_of_biggest_cluster": 0.21819788217544556, "score_of_first_answer": 0.21819788217544556, "max_score_of_first_answer": 0.4157522916793823, "min_score_of_first_answer": 0.11451604217290878, "avg_score": 0.21819788217544556, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.15061812475323677, "std_of_first_answer": 0.15061812475323677, "avg_entropy_of_first_answer": 4.082591772079468, "lex_sim": 1.0}
{"question": "when did the song push it come out", "answer": ["2016"], "scores": [[0.6652182936668396, 0.578576922416687, 0.9942148327827454, 0.4611073434352875, 0.1340869963169098]], "normalized_score": [0.4729321599006653], "tokens": [["\u2581", "2", "0", "1", "6"]], "entropy": [[1.8314567804336548, 0.7431451678276062, 0.052901774644851685, 0.7085728645324707, 2.1286725997924805]], "avg_entropy": [1.0929498374462128], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "March 8, 1987", "score_of_biggest_cluster": 0.4729321599006653, "score_of_first_answer": 0.4729321599006653, "max_score_of_first_answer": 0.9942148327827454, "min_score_of_first_answer": 0.1340869963169098, "avg_score": 0.4729321599006653, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2796138946194403, "std_of_first_answer": 0.2796138946194403, "avg_entropy_of_first_answer": 1.0929498374462128, "lex_sim": 1.0}
{"question": "when are the next commonwealth games going to be held", "answer": ["July \u2013 August 2014"], "scores": [[0.03017154522240162, 0.0034861373715102673, 0.6896387934684753, 0.7838401794433594, 0.9945024251937866, 0.9965829253196716, 0.8636728525161743, 0.2603650987148285]], "normalized_score": [0.24426162242889404], "tokens": [["\u2581July", "\u2581\u2013", "\u2581August", "\u2581", "2", "0", "1", "4"]], "entropy": [[2.3887429237365723, 0.6646486520767212, 1.1815292835235596, 0.8026087284088135, 0.04204421490430832, 0.03208227828145027, 0.4669286608695984, 0.8625093102455139]], "avg_entropy": [0.8051367565058172], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2022", "score_of_biggest_cluster": 0.24426162242889404, "score_of_first_answer": 0.24426162242889404, "max_score_of_first_answer": 0.9965829253196716, "min_score_of_first_answer": 0.0034861373715102673, "avg_score": 0.24426162242889404, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38990416453417454, "std_of_first_answer": 0.38990416453417454, "avg_entropy_of_first_answer": 0.8051367565058172, "lex_sim": 1.0}
{"question": "who wrote most of the declaration of independance", "answer": ["Thomas Jefferson"], "scores": [[0.8979837894439697, 0.9927185773849487]], "normalized_score": [0.9441637396812439], "tokens": [["\u2581Thomas", "\u2581Jefferson"]], "entropy": [[0.6470045447349548, 0.07426798343658447]], "avg_entropy": [0.36063626408576965], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Thomas Jefferson", "score_of_biggest_cluster": 0.9441637396812439, "score_of_first_answer": 0.9441637396812439, "max_score_of_first_answer": 0.9927185773849487, "min_score_of_first_answer": 0.8979837894439697, "avg_score": 0.9441637396812439, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0473673939704895, "std_of_first_answer": 0.0473673939704895, "avg_entropy_of_first_answer": 0.36063626408576965, "lex_sim": 1.0}
{"question": "how many students attend university of texas at austin", "answer": ["50,952"], "scores": [[0.8180643916130066, 0.7614688277244568, 0.5123406648635864, 0.9043845534324646, 0.06183130666613579, 0.23188574612140656, 0.2609296441078186]], "normalized_score": [0.3768720328807831], "tokens": [["\u2581", "5", "0", ",", "9", "5", "2"]], "entropy": [[1.1298859119415283, 0.8173869848251343, 1.227703332901001, 0.5457595586776733, 2.117196559906006, 2.20123291015625, 2.1933114528656006]], "avg_entropy": [1.461782387324742], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "51,331 (Fall 2016)", "score_of_biggest_cluster": 0.3768720328807831, "score_of_first_answer": 0.3768720328807831, "max_score_of_first_answer": 0.9043845534324646, "min_score_of_first_answer": 0.06183130666613579, "avg_score": 0.3768720328807831, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30563086179624926, "std_of_first_answer": 0.30563086179624926, "avg_entropy_of_first_answer": 1.461782387324742, "lex_sim": 1.0}
{"question": "who was the person who escaped from alcatraz", "answer": ["Frank Abagnale"], "scores": [[0.3614547550678253, 0.017231067642569542, 0.9551927447319031, 0.9984533786773682]], "normalized_score": [0.27761709690093994], "tokens": [["\u2581Frank", "\u2581Ab", "agn", "ale"]], "entropy": [[2.8754098415374756, 1.1039398908615112, 0.34112823009490967, 0.015323896892368793]], "avg_entropy": [1.0839504648465663], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Clarence Anglin", "score_of_biggest_cluster": 0.27761709690093994, "score_of_first_answer": 0.27761709690093994, "max_score_of_first_answer": 0.9984533786773682, "min_score_of_first_answer": 0.017231067642569542, "avg_score": 0.27761709690093994, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4124032316530072, "std_of_first_answer": 0.4124032316530072, "avg_entropy_of_first_answer": 1.0839504648465663, "lex_sim": 1.0}
{"question": "when does mexico play in the winter olympics", "answer": ["January 25-February 24, 2018"], "scores": [[0.01361179631203413, 0.6172114014625549, 0.5020240545272827, 0.08503758907318115, 0.06596985459327698, 0.38394588232040405, 0.9413437843322754, 0.9995168447494507, 0.9942678213119507, 0.0990559384226799, 0.03291098773479462, 0.20606938004493713, 0.974763810634613, 0.9130820631980896, 0.9996747970581055, 0.7542396187782288, 0.8027399778366089]], "normalized_score": [0.3157111704349518], "tokens": [["\u2581January", "\u2581", "2", "5", "-", "F", "ebru", "ary", "\u2581", "2", "4", ",", "\u2581", "2", "0", "1", "8"]], "entropy": [[2.875368118286133, 1.0182135105133057, 1.3905127048492432, 2.1982929706573486, 1.5520800352096558, 1.728539228439331, 0.22670190036296844, 0.005265119485557079, 0.050819505006074905, 1.7159029245376587, 2.1322216987609863, 0.9897308349609375, 0.1829773485660553, 0.3015431761741638, 0.0039008338935673237, 0.6600984334945679, 0.8360310792922974]], "avg_entropy": [1.051070554264109], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "9 to 25 February 2018", "score_of_biggest_cluster": 0.3157111704349518, "score_of_first_answer": 0.3157111704349518, "max_score_of_first_answer": 0.9996747970581055, "min_score_of_first_answer": 0.01361179631203413, "avg_score": 0.3157111704349518, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3860941144871826, "std_of_first_answer": 0.3860941144871826, "avg_entropy_of_first_answer": 1.051070554264109, "lex_sim": 1.0}
{"question": "who sang i ran all the way home", "answer": ["Nancy Sinatra"], "scores": [[0.006176560185849667, 0.5775001049041748, 0.9965192079544067]], "normalized_score": [0.15261417627334595], "tokens": [["\u2581Nancy", "\u2581Sin", "atra"]], "entropy": [[5.129307746887207, 1.0442091226577759, 0.036676548421382904]], "avg_entropy": [2.070064472655455], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Impalas", "score_of_biggest_cluster": 0.15261417627334595, "score_of_first_answer": 0.15261417627334595, "max_score_of_first_answer": 0.9965192079544067, "min_score_of_first_answer": 0.006176560185849667, "avg_score": 0.15261417627334595, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4058962868982077, "std_of_first_answer": 0.4058962868982077, "avg_entropy_of_first_answer": 2.070064472655455, "lex_sim": 1.0}
{"question": "who sings the christmas song mary did you know", "answer": ["Mark Lowry"], "scores": [[0.16780255734920502, 0.9916447997093201, 0.975292980670929]], "normalized_score": [0.5454604625701904], "tokens": [["\u2581Mark", "\u2581Low", "ry"]], "entropy": [[3.755549907684326, 0.08094927668571472, 0.13453587889671326]], "avg_entropy": [1.3236783544222515], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Michael English", "score_of_biggest_cluster": 0.5454604625701904, "score_of_first_answer": 0.5454604625701904, "max_score_of_first_answer": 0.9916447997093201, "min_score_of_first_answer": 0.16780255734920502, "avg_score": 0.5454604625701904, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3845667412150839, "std_of_first_answer": 0.3845667412150839, "avg_entropy_of_first_answer": 1.3236783544222515, "lex_sim": 1.0}
{"question": "where was the outlaw of josey wales filmed", "answer": ["West Germany"], "scores": [[0.00886841956526041, 0.013146757148206234]], "normalized_score": [0.010797728784382343], "tokens": [["\u2581West", "\u2581Germany"]], "entropy": [[5.114324569702148, 2.0662739276885986]], "avg_entropy": [3.5902992486953735], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kanab, Utah", "score_of_biggest_cluster": 0.010797728784382343, "score_of_first_answer": 0.010797728784382343, "max_score_of_first_answer": 0.013146757148206234, "min_score_of_first_answer": 0.00886841956526041, "avg_score": 0.010797728784382343, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.002139168791472912, "std_of_first_answer": 0.002139168791472912, "avg_entropy_of_first_answer": 3.5902992486953735, "lex_sim": 1.0}
{"question": "where does the cell spend most of its time in the cell cycle", "answer": ["dividing"], "scores": [[0.0005439625238068402, 0.9648540616035461]], "normalized_score": [0.022909482941031456], "tokens": [["\u2581div", "iding"]], "entropy": [[2.127566337585449, 0.2081938236951828]], "avg_entropy": [1.167880080640316], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Interphase", "score_of_biggest_cluster": 0.022909482941031456, "score_of_first_answer": 0.022909482941031456, "max_score_of_first_answer": 0.9648540616035461, "min_score_of_first_answer": 0.0005439625238068402, "avg_score": 0.022909482941031456, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.48215504953986965, "std_of_first_answer": 0.48215504953986965, "avg_entropy_of_first_answer": 1.167880080640316, "lex_sim": 1.0}
{"question": "who plays sven in the santa clarita diet", "answer": ["Timothy V. Murphy"], "scores": [[0.34828346967697144, 0.923637330532074, 0.018733937293291092, 0.844627320766449, 0.9972807168960571, 0.9967558979988098]], "normalized_score": [0.41433894634246826], "tokens": [["\u2581Tim", "othy", "\u2581V", ".", "\u2581Mur", "phy"]], "entropy": [[3.7887895107269287, 0.5752447247505188, 0.49076148867607117, 0.9625611305236816, 0.02969513088464737, 0.03035522997379303]], "avg_entropy": [0.9795678692559401], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Leo Howard", "score_of_biggest_cluster": 0.41433894634246826, "score_of_first_answer": 0.41433894634246826, "max_score_of_first_answer": 0.9972807168960571, "min_score_of_first_answer": 0.018733937293291092, "avg_score": 0.41433894634246826, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37290882026591987, "std_of_first_answer": 0.37290882026591987, "avg_entropy_of_first_answer": 0.9795678692559401, "lex_sim": 1.0}
{"question": "when is last time the astros won the world series", "answer": ["2017"], "scores": [[0.809171199798584, 0.916063129901886, 0.9961785078048706, 0.9736989140510559, 0.9791703224182129]], "normalized_score": [0.9322173595428467], "tokens": [["\u2581", "2", "0", "1", "7"]], "entropy": [[1.269909381866455, 0.3659112751483917, 0.03685058653354645, 0.12624214589595795, 0.1401701420545578]], "avg_entropy": [0.3878167062997818], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2017", "score_of_biggest_cluster": 0.9322173595428467, "score_of_first_answer": 0.9322173595428467, "max_score_of_first_answer": 0.9961785078048706, "min_score_of_first_answer": 0.809171199798584, "avg_score": 0.9322173595428467, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06838541083327004, "std_of_first_answer": 0.06838541083327004, "avg_entropy_of_first_answer": 0.3878167062997818, "lex_sim": 1.0}
{"question": "university of michigan school of public health ranking", "answer": ["US News: Number 6"], "scores": [[0.004092175979167223, 0.7846821546554565, 0.019848285242915154, 0.0017332186689600348, 0.8737668991088867, 0.05104086175560951]], "normalized_score": [0.04124991223216057], "tokens": [["\u2581US", "\u2581News", ":", "\u2581Number", "\u2581", "6"]], "entropy": [[2.9586663246154785, 1.3859148025512695, 2.146831512451172, 1.8842469453811646, 0.7199273109436035, 1.7710657119750977]], "avg_entropy": [1.8111087679862976], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "#4", "score_of_biggest_cluster": 0.04124991223216057, "score_of_first_answer": 0.04124991223216057, "max_score_of_first_answer": 0.8737668991088867, "min_score_of_first_answer": 0.0017332186689600348, "avg_score": 0.04124991223216057, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38306108476184436, "std_of_first_answer": 0.38306108476184436, "avg_entropy_of_first_answer": 1.8111087679862976, "lex_sim": 1.0}
{"question": "who want to be a millionaire calls his dad", "answer": ["Merv Griffin"], "scores": [[0.054619163274765015, 0.7910177707672119, 0.9131250977516174, 0.9865628480911255]], "normalized_score": [0.44416728615760803], "tokens": [["\u2581Mer", "v", "\u2581Griff", "in"]], "entropy": [[5.823353290557861, 0.9475830793380737, 0.5331358909606934, 0.1012178510427475]], "avg_entropy": [1.851322527974844], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Carpenter", "score_of_biggest_cluster": 0.44416728615760803, "score_of_first_answer": 0.44416728615760803, "max_score_of_first_answer": 0.9865628480911255, "min_score_of_first_answer": 0.054619163274765015, "avg_score": 0.44416728615760803, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37134683586083844, "std_of_first_answer": 0.37134683586083844, "avg_entropy_of_first_answer": 1.851322527974844, "lex_sim": 1.0}
{"question": "who lives at the end of king lear", "answer": ["Lear"], "scores": [[0.07971478253602982]], "normalized_score": [0.07971478253602982], "tokens": [["\u2581Lear"]], "entropy": [[2.751906394958496]], "avg_entropy": [2.751906394958496], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kent", "score_of_biggest_cluster": 0.07971478253602982, "score_of_first_answer": 0.07971478253602982, "max_score_of_first_answer": 0.07971478253602982, "min_score_of_first_answer": 0.07971478253602982, "avg_score": 0.07971478253602982, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.751906394958496, "lex_sim": 1.0}
{"question": "who are the two teams in super bowl 2018", "answer": ["New England Patriots vs Philadelphia Eagles"], "scores": [[0.3066091239452362, 0.9790688157081604, 0.9516589641571045, 0.9990279674530029, 0.20491525530815125, 0.4190438389778137, 0.9751317501068115, 0.9983202815055847, 0.9992520213127136]], "normalized_score": [0.6602370738983154], "tokens": [["\u2581New", "\u2581England", "\u2581Patri", "ots", "\u2581vs", "\u2581Philadelphia", "\u2581E", "ag", "les"]], "entropy": [[1.9909915924072266, 0.13753941655158997, 0.30086618661880493, 0.0087436493486166, 1.4268391132354736, 1.2064675092697144, 0.16565625369548798, 0.014009705744683743, 0.007650155574083328]], "avg_entropy": [0.5843070647161868], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "New England Patriots", "score_of_biggest_cluster": 0.6602370738983154, "score_of_first_answer": 0.6602370738983154, "max_score_of_first_answer": 0.9992520213127136, "min_score_of_first_answer": 0.20491525530815125, "avg_score": 0.6602370738983154, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3218188747030671, "std_of_first_answer": 0.3218188747030671, "avg_entropy_of_first_answer": 0.5843070647161868, "lex_sim": 1.0}
{"question": "when do they put the rockefeller tree up", "answer": ["November 29, 2016"], "scores": [[0.15757115185260773, 0.6505005359649658, 0.7135809063911438, 0.42894119024276733, 0.05672315135598183, 0.9630547165870667, 0.787693440914154, 0.9983689188957214, 0.9535921216011047, 0.1960630863904953]], "normalized_score": [0.4366549253463745], "tokens": [["\u2581November", "\u2581", "2", "9", ",", "\u2581", "2", "0", "1", "6"]], "entropy": [[3.7681221961975098, 0.8560692071914673, 0.919405996799469, 1.4775530099868774, 0.9460757374763489, 0.3212668299674988, 0.5560731291770935, 0.017206478863954544, 0.19380269944667816, 1.7146669626235962]], "avg_entropy": [1.0770242247730493], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "mid November", "score_of_biggest_cluster": 0.4366549253463745, "score_of_first_answer": 0.4366549253463745, "max_score_of_first_answer": 0.9983689188957214, "min_score_of_first_answer": 0.05672315135598183, "avg_score": 0.4366549253463745, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3389175280934002, "std_of_first_answer": 0.3389175280934002, "avg_entropy_of_first_answer": 1.0770242247730493, "lex_sim": 1.0}
{"question": "who commissioned the first christmas card in 1943", "answer": ["George VI"], "scores": [[0.04733998328447342, 0.5592771768569946]], "normalized_score": [0.16271498799324036], "tokens": [["\u2581George", "\u2581VI"]], "entropy": [[4.942194938659668, 2.562156915664673]], "avg_entropy": [3.7521759271621704], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sir Henry Cole", "score_of_biggest_cluster": 0.16271498799324036, "score_of_first_answer": 0.16271498799324036, "max_score_of_first_answer": 0.5592771768569946, "min_score_of_first_answer": 0.04733998328447342, "avg_score": 0.16271498799324036, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2559685967862606, "std_of_first_answer": 0.2559685967862606, "avg_entropy_of_first_answer": 3.7521759271621704, "lex_sim": 1.0}
{"question": "who is the minister of local government in zimbabwe", "answer": ["Ignatius Chanetsa"], "scores": [[0.28890085220336914, 0.9998156428337097, 0.8626164197921753, 0.0010729682398959994, 0.08103949576616287, 0.9999252557754517]], "normalized_score": [0.16696405410766602], "tokens": [["\u2581Ign", "ati", "us", "\u2581Chan", "ets", "a"]], "entropy": [[2.8672375679016113, 0.0023370743729174137, 0.4208068251609802, 0.6024523973464966, 1.2084076404571533, 0.001089752884581685]], "avg_entropy": [0.8503885430206234], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hon July Moyo", "score_of_biggest_cluster": 0.16696405410766602, "score_of_first_answer": 0.16696405410766602, "max_score_of_first_answer": 0.9999252557754517, "min_score_of_first_answer": 0.0010729682398959994, "avg_score": 0.16696405410766602, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4264531173722704, "std_of_first_answer": 0.4264531173722704, "avg_entropy_of_first_answer": 0.8503885430206234, "lex_sim": 1.0}
{"question": "who plays at the prudential center in newark", "answer": ["The New Jersey Devils"], "scores": [[0.07971242815256119, 0.5202268958091736, 0.8839331865310669, 0.9004882574081421, 0.9939181804656982]], "normalized_score": [0.5048860907554626], "tokens": [["\u2581The", "\u2581New", "\u2581Jersey", "\u2581Dev", "ils"]], "entropy": [[3.296980142593384, 2.9430031776428223, 0.4954659044742584, 0.43505778908729553, 0.03914821147918701]], "avg_entropy": [1.4419310450553895], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Seton Hall Pirates", "score_of_biggest_cluster": 0.5048860907554626, "score_of_first_answer": 0.5048860907554626, "max_score_of_first_answer": 0.9939181804656982, "min_score_of_first_answer": 0.07971242815256119, "avg_score": 0.5048860907554626, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33897689985335094, "std_of_first_answer": 0.33897689985335094, "avg_entropy_of_first_answer": 1.4419310450553895, "lex_sim": 1.0}
{"question": "five tools of security management to overcome computer crime", "answer": ["Deterrence, Recovery, Prevention, Law Enforcement and Security Awareness"], "scores": [[0.008537646383047104, 0.19542570412158966, 0.8521077632904053, 0.9052261710166931, 0.004455474205315113, 0.573215901851654, 0.9404523372650146, 0.4405003786087036, 0.9718891978263855, 0.9708748459815979, 0.004747351631522179, 0.5113316178321838, 0.9971795082092285, 0.9937428832054138, 0.19064182043075562, 0.026895349845290184, 0.2787947654724121, 0.997708797454834, 0.9977778792381287]], "normalized_score": [0.2525091767311096], "tokens": [["\u2581D", "eter", "rence", ",", "\u2581Rec", "overy", ",", "\u2581Pre", "vention", ",", "\u2581Law", "\u2581En", "for", "cement", "\u2581and", "\u2581Security", "\u2581A", "war", "eness"]], "entropy": [[6.009620666503906, 1.582033395767212, 0.6622799634933472, 0.7179296016693115, 3.650327682495117, 1.1848934888839722, 0.45483922958374023, 3.461730480194092, 0.180564746260643, 0.22531196475028992, 4.733329772949219, 1.486814260482788, 0.027671193704009056, 0.04699220508337021, 0.8836293816566467, 4.081718921661377, 3.332935333251953, 0.021265733987092972, 0.01947709545493126]], "avg_entropy": [1.7243876377806853], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pre-evaluation", "score_of_biggest_cluster": 0.2525091767311096, "score_of_first_answer": 0.2525091767311096, "max_score_of_first_answer": 0.9977778792381287, "min_score_of_first_answer": 0.004455474205315113, "avg_score": 0.2525091767311096, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39855819270637366, "std_of_first_answer": 0.39855819270637366, "avg_entropy_of_first_answer": 1.7243876377806853, "lex_sim": 1.0}
{"question": "who sang oh annie i not your daddy", "answer": ["Shyne"], "scores": [[0.004910350777208805, 0.0025782776065170765, 0.10291082412004471]], "normalized_score": [0.010921971872448921], "tokens": [["\u2581Sh", "y", "ne"]], "entropy": [[5.61501407623291, 3.2969634532928467, 2.6944613456726074]], "avg_entropy": [3.8688129583994546], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kid Creole and the Coconuts", "score_of_biggest_cluster": 0.010921971872448921, "score_of_first_answer": 0.010921971872448921, "max_score_of_first_answer": 0.10291082412004471, "min_score_of_first_answer": 0.0025782776065170765, "avg_score": 0.010921971872448921, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.046757235008288094, "std_of_first_answer": 0.046757235008288094, "avg_entropy_of_first_answer": 3.8688129583994546, "lex_sim": 1.0}
{"question": "who played nicky on the drew carey show", "answer": ["James Patrick McGowan"], "scores": [[0.0828220546245575, 0.11514562368392944, 0.0003156099410261959, 0.21701323986053467]], "normalized_score": [0.028428738936781883], "tokens": [["\u2581James", "\u2581Patrick", "\u2581McG", "owan"]], "entropy": [[4.423032760620117, 3.6172099113464355, 0.6900474429130554, 2.539982795715332]], "avg_entropy": [2.817568227648735], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kate Walsh", "score_of_biggest_cluster": 0.028428738936781883, "score_of_first_answer": 0.028428738936781883, "max_score_of_first_answer": 0.21701323986053467, "min_score_of_first_answer": 0.0003156099410261959, "avg_score": 0.028428738936781883, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07761290437989384, "std_of_first_answer": 0.07761290437989384, "avg_entropy_of_first_answer": 2.817568227648735, "lex_sim": 1.0}
{"question": "when did gaurdians of the galaxy 2 come out", "answer": ["August 5, 2017 MORNING SHOW (Morocco) \u2013 6:00AM-6:30AM ASSISTANT: Kanye\u2019s middle name is West"], "scores": [[0.12533678114414215, 0.9770416617393494, 0.5375703573226929, 0.40693604946136475, 0.9913809895515442, 0.9980795383453369, 0.9996657371520996, 0.9969968795776367, 0.970392107963562, 1.7482456087236642e-06, 0.0020653237588703632, 0.8971761465072632, 0.9901258945465088, 0.007642846554517746, 0.8657488226890564, 0.9268025159835815, 0.009970341809093952, 0.002262450521811843, 0.000757176021579653, 0.829247772693634, 0.6796854734420776, 0.007371915969997644, 0.10796550661325455, 0.04212276637554169, 0.2900189459323883, 0.4611077606678009, 0.8727388381958008, 0.04006776586174965, 0.11179573088884354, 0.010578511282801628, 0.9951797723770142, 0.7146690487861633, 0.9772955775260925, 0.9632791876792908, 0.004363099113106728, 0.007167980074882507, 0.9398669004440308, 0.991609513759613, 0.9053694009780884, 0.002891323296353221, 0.0014480540994554758, 0.9584864377975464, 0.007328175939619541, 0.9907715320587158, 0.0003269701555836946, 0.9540309309959412, 0.7264630794525146, 0.05024271458387375]], "normalized_score": [0.10352551192045212], "tokens": [["\u2581August", "\u2581", "5", ",", "\u2581", "2", "0", "1", "7", "\u2581M", "OR", "N", "ING", "\u2581S", "HO", "W", "\u2581(", "Mor", "oc", "co", ")", "\u2581\u2013", "\u2581", "6", ":", "0", "0", "AM", "-", "6", ":", "3", "0", "AM", "\u2581A", "SS", "IST", "ANT", ":", "\u2581K", "any", "e", "\u2019", "s", "\u2581middle", "\u2581name", "\u2581is", "\u2581West"]], "entropy": [[1.445364236831665, 0.14030961692333221, 1.347470998764038, 0.8936790823936462, 0.052448682487010956, 0.01432074885815382, 0.004061535000801086, 0.023303909227252007, 0.17215551435947418, 0.24543023109436035, 3.5698156356811523, 0.7007116079330444, 0.07883931696414948, 1.471604347229004, 0.8159917593002319, 0.2904975712299347, 3.0894389152526855, 5.733677387237549, 0.2884539067745209, 0.5257647633552551, 1.987043857574463, 2.2326319217681885, 5.978180885314941, 2.239989995956421, 3.5572471618652344, 1.5107486248016357, 0.6445707678794861, 2.72021484375, 3.4731574058532715, 1.4603288173675537, 0.04802824184298515, 1.0372475385665894, 0.15064990520477295, 0.24559466540813446, 3.308091163635254, 4.974987030029297, 0.3784498870372772, 0.06573838740587234, 0.7318047285079956, 5.1732497215271, 6.051833152770996, 0.37555789947509766, 1.4055204391479492, 0.08420367538928986, 5.82668399810791, 0.34118983149528503, 1.573650598526001, 4.746420860290527]], "avg_entropy": [1.7338824119457665], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2017", "score_of_biggest_cluster": 0.10352551192045212, "score_of_first_answer": 0.10352551192045212, "max_score_of_first_answer": 0.9996657371520996, "min_score_of_first_answer": 1.7482456087236642e-06, "avg_score": 0.10352551192045212, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4336845153934694, "std_of_first_answer": 0.4336845153934694, "avg_entropy_of_first_answer": 1.7338824119457665, "lex_sim": 1.0}
{"question": "the mughal garden of rashtrapati bhavan is modelled on which garden", "answer": ["Kashmiri"], "scores": [[0.04355456307530403, 0.11709989607334137, 0.08228731900453568, 0.9594283103942871]], "normalized_score": [0.14165562391281128], "tokens": [["\u2581K", "ash", "mi", "ri"]], "entropy": [[4.181242942810059, 2.7896883487701416, 0.8392637968063354, 0.2156190425157547]], "avg_entropy": [2.0064535327255726], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Persian gardens", "score_of_biggest_cluster": 0.14165562391281128, "score_of_first_answer": 0.14165562391281128, "max_score_of_first_answer": 0.9594283103942871, "min_score_of_first_answer": 0.04355456307530403, "avg_score": 0.14165562391281128, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3812675615042693, "std_of_first_answer": 0.3812675615042693, "avg_entropy_of_first_answer": 2.0064535327255726, "lex_sim": 1.0}
{"question": "when was the taming ofthe shrew first performed", "answer": ["Tudor period (1509)."], "scores": [[0.0006153683643788099, 0.1912381500005722, 0.9789726138114929, 0.249854177236557, 0.06705669313669205, 0.7041770219802856, 0.6257416009902954, 0.5756874084472656, 0.4817110300064087, 0.008542665280401707]], "normalized_score": [0.1350284218788147], "tokens": [["\u2581T", "ud", "or", "\u2581period", "\u2581(", "1", "5", "0", "9", ")."]], "entropy": [[1.460447072982788, 2.432967185974121, 0.15817174315452576, 2.733208656311035, 1.2924878597259521, 1.7208032608032227, 0.8491352796554565, 1.6068999767303467, 0.7921538352966309, 0.806646466255188]], "avg_entropy": [1.3852921336889268], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "prior to June 1592", "score_of_biggest_cluster": 0.1350284218788147, "score_of_first_answer": 0.1350284218788147, "max_score_of_first_answer": 0.9789726138114929, "min_score_of_first_answer": 0.0006153683643788099, "avg_score": 0.1350284218788147, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31695480072335663, "std_of_first_answer": 0.31695480072335663, "avg_entropy_of_first_answer": 1.3852921336889268, "lex_sim": 1.0}
{"question": "who scored fastest 10000 runs in test cricket", "answer": ["Sachin Tendulkar"], "scores": [[0.8243871927261353, 0.9982995390892029, 0.9009501338005066, 0.9991540908813477, 0.9928763508796692, 0.9955505132675171]], "normalized_score": [0.9493957757949829], "tokens": [["\u2581Sach", "in", "\u2581T", "end", "ul", "kar"]], "entropy": [[1.1723108291625977, 0.017904896289110184, 0.4803767204284668, 0.010117088444530964, 0.061151377856731415, 0.03441663458943367]], "avg_entropy": [0.2960462577951451], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alastair Cook", "score_of_biggest_cluster": 0.9493957757949829, "score_of_first_answer": 0.9493957757949829, "max_score_of_first_answer": 0.9991540908813477, "min_score_of_first_answer": 0.8243871927261353, "avg_score": 0.9493957757949829, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06686515575606837, "std_of_first_answer": 0.06686515575606837, "avg_entropy_of_first_answer": 0.2960462577951451, "lex_sim": 1.0}
{"question": "who came first second and third on the grand national", "answer": ["The first was 20/1 shot Silver Birch, the second was 33/1 shot Don\u2019t Push It, the third was the 25/1 shot Teaforthree."], "scores": [[0.058403924107551575, 0.00971691682934761, 0.28209221363067627, 0.025940505787730217, 0.08474014699459076, 0.5273417234420776, 0.26333123445510864, 0.9911403059959412, 0.5768232345581055, 0.011196434497833252, 0.985558032989502, 0.9988884329795837, 0.6690720319747925, 0.5685200691223145, 0.975590705871582, 0.7684139609336853, 0.7119012475013733, 0.09612743556499481, 0.9338396787643433, 0.9953458905220032, 0.9993177652359009, 0.6848136186599731, 0.003945572301745415, 0.3857805132865906, 0.9970851540565491, 0.9625278115272522, 0.9998466968536377, 0.9832134246826172, 0.3615051209926605, 0.33341675996780396, 0.9806236624717712, 0.9576303362846375, 0.02250180020928383, 0.7077035903930664, 0.20174294710159302, 0.3691408932209015, 0.9844299554824829, 0.9993808269500732, 0.7524923086166382, 0.12404010444879532, 0.9869579672813416, 0.4331170320510864, 0.8783392310142517, 0.18905626237392426]], "normalized_score": [0.34531596302986145], "tokens": [["\u2581The", "\u2581first", "\u2581was", "\u2581", "2", "0", "/", "1", "\u2581shot", "\u2581Silver", "\u2581Bir", "ch", ",", "\u2581the", "\u2581second", "\u2581was", "\u2581", "3", "3", "/", "1", "\u2581shot", "\u2581Don", "\u2019", "t", "\u2581P", "ush", "\u2581It", ",", "\u2581the", "\u2581third", "\u2581was", "\u2581the", "\u2581", "2", "5", "/", "1", "\u2581shot", "\u2581Te", "a", "for", "three", "."]], "entropy": [[5.422196388244629, 5.7444844245910645, 2.9926705360412598, 5.677932262420654, 1.6131556034088135, 2.0991010665893555, 1.9631505012512207, 0.07416238635778427, 2.893756866455078, 4.870236396789551, 0.12206955254077911, 0.009942548349499702, 1.6093215942382812, 2.0712852478027344, 0.18343982100486755, 1.503395676612854, 2.053039073944092, 1.9040899276733398, 0.32506492733955383, 0.040641747415065765, 0.007920527830719948, 2.4157276153564453, 4.183709144592285, 0.8887643814086914, 0.028804540634155273, 0.31805652379989624, 0.002044196706265211, 0.11610914021730423, 1.0590012073516846, 1.0205215215682983, 0.15917643904685974, 0.31910452246665955, 0.5354167222976685, 1.6005189418792725, 1.90513014793396, 1.273613452911377, 0.09699442982673645, 0.007180067710578442, 1.9728095531463623, 4.607582092285156, 0.1273057460784912, 1.1578935384750366, 0.697668731212616, 0.8872540593147278]], "avg_entropy": [1.5582146316618575], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pleasant Company", "score_of_biggest_cluster": 0.34531596302986145, "score_of_first_answer": 0.34531596302986145, "max_score_of_first_answer": 0.9998466968536377, "min_score_of_first_answer": 0.003945572301745415, "avg_score": 0.34531596302986145, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3713887839254664, "std_of_first_answer": 0.3713887839254664, "avg_entropy_of_first_answer": 1.5582146316618575, "lex_sim": 1.0}
{"question": "where was the tv show high chaparral filmed", "answer": ["Arizona state"], "scores": [[0.061138954013586044, 0.00027117313584312797]], "normalized_score": [0.004071760922670364], "tokens": [["\u2581Arizona", "\u2581state"]], "entropy": [[4.917483329772949, 0.8561058640480042]], "avg_entropy": [2.8867945969104767], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "California", "score_of_biggest_cluster": 0.004071760922670364, "score_of_first_answer": 0.004071760922670364, "max_score_of_first_answer": 0.061138954013586044, "min_score_of_first_answer": 0.00027117313584312797, "avg_score": 0.004071760922670364, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.030433890438871458, "std_of_first_answer": 0.030433890438871458, "avg_entropy_of_first_answer": 2.8867945969104767, "lex_sim": 1.0}
{"question": "how long did the menendez brothers get in prison for killing their parents", "answer": ["Life"], "scores": [[0.37985759973526]], "normalized_score": [0.37985759973526], "tokens": [["\u2581Life"]], "entropy": [[1.994694709777832]], "avg_entropy": [1.994694709777832], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "life imprisonment", "score_of_biggest_cluster": 0.37985759973526, "score_of_first_answer": 0.37985759973526, "max_score_of_first_answer": 0.37985759973526, "min_score_of_first_answer": 0.37985759973526, "avg_score": 0.37985759973526, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.994694709777832, "lex_sim": 1.0}
{"question": "how many votes to approve supreme court justice", "answer": ["51 TUTORIALS AND HELPFUL VIDEOS We create many tutorials and helpful videos for you to get the most from our site, help you become smarter, and get the best possible grade in school for"], "scores": [[0.539927065372467, 0.3925360441207886, 0.5049011707305908, 8.894056009012274e-06, 2.2168025680002756e-05, 0.7665243744850159, 0.591194212436676, 0.22684957087039948, 0.017905516549944878, 0.016797268763184547, 0.327802449464798, 0.06204566732048988, 0.9894871711730957, 0.11106153577566147, 0.9255533814430237, 0.9052683711051941, 0.00042530213249847293, 0.0013492595171555877, 0.004748486448079348, 0.23253780603408813, 0.6075378060340881, 0.5175802111625671, 0.8960637450218201, 0.2083534449338913, 0.26435840129852295, 0.4948166608810425, 0.037005651742219925, 0.21701711416244507, 0.5966283082962036, 0.07090727239847183, 0.26188570261001587, 0.07913738489151001, 0.035772547125816345, 0.0021428281906992197, 0.520330011844635, 0.018764683976769447, 0.035842105746269226, 0.9961276650428772, 0.4366745948791504, 0.6950486898422241, 0.04823257401585579, 0.1858663707971573, 0.11898680031299591, 0.07136595994234085, 0.05810829997062683, 0.08778462558984756, 0.24990729987621307, 0.0024845306761562824]], "normalized_score": [0.08219672739505768], "tokens": [["\u2581", "5", "1", "\u2581T", "UT", "OR", "IAL", "S", "\u2581AND", "\u2581H", "EL", "PF", "UL", "\u2581V", "IDE", "OS", "\u2581We", "\u2581create", "\u2581many", "\u2581tutorials", "\u2581and", "\u2581helpful", "\u2581videos", "\u2581for", "\u2581you", "\u2581to", "\u2581get", "\u2581the", "\u2581most", "\u2581from", "\u2581our", "\u2581site", ",", "\u2581help", "\u2581you", "\u2581become", "\u2581sm", "arter", ",", "\u2581and", "\u2581get", "\u2581the", "\u2581best", "\u2581possible", "\u2581grade", "\u2581in", "\u2581school", "\u2581for"]], "entropy": [[2.418558120727539, 1.0989441871643066, 1.7059245109558105, 0.9456464648246765, 4.680338382720947, 1.475240707397461, 1.792816400527954, 3.13790225982666, 3.2001073360443115, 3.8187296390533447, 1.6733829975128174, 0.7914626598358154, 0.07642029225826263, 3.630960464477539, 0.4342021942138672, 0.3302214741706848, 3.2384464740753174, 4.447835445404053, 5.017673492431641, 4.035635948181152, 1.9057347774505615, 2.511674404144287, 0.7451105117797852, 2.887831687927246, 3.427166223526001, 2.0519590377807617, 3.327636241912842, 3.3383677005767822, 2.0957679748535156, 1.0420767068862915, 3.4806249141693115, 4.25830078125, 1.3736251592636108, 4.447350025177002, 2.830575942993164, 4.02592658996582, 2.3347811698913574, 0.03576049581170082, 1.5509240627288818, 1.9579594135284424, 4.421682357788086, 3.287616014480591, 2.9683265686035156, 3.6292364597320557, 3.341062307357788, 2.3762989044189453, 2.0198862552642822, 1.7361488342285156]], "avg_entropy": [2.528330436985319], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A simple majority vote", "score_of_biggest_cluster": 0.08219672739505768, "score_of_first_answer": 0.08219672739505768, "max_score_of_first_answer": 0.9961276650428772, "min_score_of_first_answer": 8.894056009012274e-06, "avg_score": 0.08219672739505768, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3073870166899875, "std_of_first_answer": 0.3073870166899875, "avg_entropy_of_first_answer": 2.528330436985319, "lex_sim": 1.0}
{"question": "what type of dog was laika the spacedog", "answer": ["Sputnik 21"], "scores": [[0.10477135330438614, 0.8454850316047668, 0.9981270432472229, 0.3511422574520111, 0.9500105977058411, 0.0005831133457832038]], "normalized_score": [0.16066323220729828], "tokens": [["\u2581S", "put", "nik", "\u2581", "2", "1"]], "entropy": [[4.833887577056885, 1.0453494787216187, 0.01774345338344574, 1.8376277685165405, 0.2612873911857605, 1.6545695066452026]], "avg_entropy": [1.6084108625849087], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "mongrel female", "score_of_biggest_cluster": 0.16066323220729828, "score_of_first_answer": 0.16066323220729828, "max_score_of_first_answer": 0.9981270432472229, "min_score_of_first_answer": 0.0005831133457832038, "avg_score": 0.16066323220729828, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40565891150924543, "std_of_first_answer": 0.40565891150924543, "avg_entropy_of_first_answer": 1.6084108625849087, "lex_sim": 1.0}
{"question": "when will notre dame played michigan state again", "answer": ["2021"], "scores": [[0.3019801378250122, 0.900152862071991, 0.988541841506958, 0.24508105218410492, 0.20373466610908508]], "normalized_score": [0.42221423983573914], "tokens": [["\u2581", "2", "0", "2", "1"]], "entropy": [[3.2734248638153076, 0.4574201703071594, 0.09797168523073196, 0.9484952092170715, 1.9953746795654297]], "avg_entropy": [1.3545373216271401], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2026", "score_of_biggest_cluster": 0.42221423983573914, "score_of_first_answer": 0.42221423983573914, "max_score_of_first_answer": 0.988541841506958, "min_score_of_first_answer": 0.20373466610908508, "avg_score": 0.42221423983573914, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3425996510550909, "std_of_first_answer": 0.3425996510550909, "avg_entropy_of_first_answer": 1.3545373216271401, "lex_sim": 1.0}
{"question": "where did they get the tree for rockefeller center", "answer": ["in the area around Rockefeller Center"], "scores": [[0.007038472685962915, 0.22349178791046143, 0.01488894410431385, 0.1369815319776535, 0.34372612833976746, 0.9792806506156921, 0.9988651275634766, 0.9498053789138794]], "normalized_score": [0.17836755514144897], "tokens": [["\u2581in", "\u2581the", "\u2581area", "\u2581around", "\u2581Rock", "ef", "eller", "\u2581Center"]], "entropy": [[4.215991973876953, 4.654916763305664, 5.413563251495361, 3.044959545135498, 3.7928414344787598, 0.160201758146286, 0.010332701727747917, 0.3302663266658783]], "avg_entropy": [2.7028842193540186], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "State College, PA", "score_of_biggest_cluster": 0.17836755514144897, "score_of_first_answer": 0.17836755514144897, "max_score_of_first_answer": 0.9988651275634766, "min_score_of_first_answer": 0.007038472685962915, "avg_score": 0.17836755514144897, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41487166856315505, "std_of_first_answer": 0.41487166856315505, "avg_entropy_of_first_answer": 2.7028842193540186, "lex_sim": 1.0}
{"question": "when did the song things that make you go hmmm come out", "answer": ["August 19, 2013"], "scores": [[0.02061980776488781, 0.9550483822822571, 0.32787710428237915, 0.23716463148593903, 0.22606757283210754, 0.9865347743034363, 0.7298948764801025, 0.999440610408783, 0.5375494360923767, 0.1260681003332138]], "normalized_score": [0.333231657743454], "tokens": [["\u2581August", "\u2581", "1", "9", ",", "\u2581", "2", "0", "1", "3"]], "entropy": [[1.7317800521850586, 0.24145054817199707, 1.5400683879852295, 2.3691930770874023, 1.4610157012939453, 0.0850522518157959, 0.5886098742485046, 0.007004500366747379, 0.6973417401313782, 2.0944995880126953]], "avg_entropy": [1.0816015721298755], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "June 1991", "score_of_biggest_cluster": 0.333231657743454, "score_of_first_answer": 0.333231657743454, "max_score_of_first_answer": 0.999440610408783, "min_score_of_first_answer": 0.02061980776488781, "avg_score": 0.333231657743454, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3589883608092615, "std_of_first_answer": 0.3589883608092615, "avg_entropy_of_first_answer": 1.0816015721298755, "lex_sim": 1.0}
{"question": "who sings the theme song for the tv show cops", "answer": ["Eddie Sancho"], "scores": [[0.09640830010175705, 0.9338487386703491, 0.000333502161083743, 0.7421815395355225]], "normalized_score": [0.06870682537555695], "tokens": [["\u2581Edd", "ie", "\u2581San", "cho"]], "entropy": [[5.380242347717285, 0.26797136664390564, 2.250709056854248, 0.7579163312911987]], "avg_entropy": [2.1642097756266594], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Inner Circle", "score_of_biggest_cluster": 0.06870682537555695, "score_of_first_answer": 0.06870682537555695, "max_score_of_first_answer": 0.9338487386703491, "min_score_of_first_answer": 0.000333502161083743, "avg_score": 0.06870682537555695, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40203276097022883, "std_of_first_answer": 0.40203276097022883, "avg_entropy_of_first_answer": 2.1642097756266594, "lex_sim": 1.0}
{"question": "what engine is in a holden v8 supercar", "answer": ["5.0L Chevrolet V8 engine"], "scores": [[0.1523103415966034, 0.561511218547821, 0.8361485600471497, 0.7229659557342529, 0.335783451795578, 0.027711616829037666, 0.9428359866142273, 0.9907710552215576, 0.9971418976783752, 0.31711429357528687, 0.9760647416114807, 0.03563481196761131]], "normalized_score": [0.36126741766929626], "tokens": [["\u2581", "5", ".", "0", "L", "\u2581Che", "v", "ro", "let", "\u2581V", "8", "\u2581engine"]], "entropy": [[3.556164503097534, 1.4099626541137695, 0.9258297681808472, 0.8085680603981018, 2.4627461433410645, 3.067056655883789, 0.2289869785308838, 0.07735265791416168, 0.030988190323114395, 1.9457566738128662, 0.14716674387454987, 1.0464434623718262]], "avg_entropy": [1.3089185409868758], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "V8-engine", "score_of_biggest_cluster": 0.36126741766929626, "score_of_first_answer": 0.36126741766929626, "max_score_of_first_answer": 0.9971418976783752, "min_score_of_first_answer": 0.027711616829037666, "avg_score": 0.36126741766929626, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3685935225863132, "std_of_first_answer": 0.3685935225863132, "avg_entropy_of_first_answer": 1.3089185409868758, "lex_sim": 1.0}
{"question": "when was 1 john 5 7 added to the bible", "answer": ["The text of 1 John 5:7 was not found in the 1982 Revised Standard Version and is omitted in many modern Bible editions."], "scores": [[0.03620265796780586, 0.0032117143273353577, 0.21791909635066986, 0.48721450567245483, 0.9883217215538025, 0.9792678356170654, 0.8549839854240417, 0.9973639845848083, 0.9730490446090698, 0.9882340431213379, 0.27993813157081604, 0.31383219361305237, 0.1083291694521904, 0.9564039707183838, 0.521994411945343, 0.0163721926510334, 0.7345246076583862, 0.11146791279315948, 0.2280517816543579, 0.22124777734279633, 0.04023351892828941, 0.8305051922798157, 0.9577816724777222, 0.9831106662750244, 0.04066196456551552, 0.20021769404411316, 0.02260870672762394, 0.39568161964416504, 0.020171228796243668, 0.4131042957305908, 0.08358164131641388, 0.026722649112343788, 0.9996337890625, 0.636994481086731, 0.010796292684972286, 0.07138998061418533, 0.12211380898952484, 0.9993786811828613, 0.0017998309340327978, 0.573222279548645, 0.28355562686920166, 0.2635495960712433, 0.38027217984199524, 0.12522883713245392, 0.17806704342365265, 0.9408689737319946, 0.0010641053086146712, 0.817121148109436, 0.3348175883293152, 0.9412346482276917]], "normalized_score": [0.19170886278152466], "tokens": [["\u2581The", "\u2581text", "\u2581of", "\u2581", "1", "\u2581John", "\u2581", "5", ":", "7", "\u2581was", "\u2581not", "\u2581found", "\u2581in", "\u2581the", "\u2581", "1", "9", "8", "2", "\u2581Re", "vised", "\u2581Standard", "\u2581Version", "\u2581and", "\u2581is", "\u2581omitted", "\u2581in", "\u2581many", "\u2581modern", "\u2581Bible", "\u2581ed", "itions", ".", "\u2581A", "\u2581few", "\u2581ed", "itions", "\u2581preserve", "\u2581the", "\u2581verse", "\u2581in", "\u2581brackets", "\u2581or", "\u2581foot", "notes", "\u2581instead", "\u2581of", "\u2581om", "itting"]], "entropy": [[2.585827350616455, 4.350677967071533, 2.9125261306762695, 1.7226635217666626, 0.08042039722204208, 0.15723609924316406, 0.7644506692886353, 0.022184090688824654, 0.1971435844898224, 0.08237676322460175, 2.604828119277954, 2.275463104248047, 2.5705113410949707, 0.34425869584083557, 1.9174569845199585, 2.864999294281006, 1.0876057147979736, 2.1062264442443848, 2.1536026000976562, 1.358855128288269, 3.3895692825317383, 0.6088278889656067, 0.26264986395835876, 0.13832274079322815, 2.6063754558563232, 3.286442756652832, 2.729980707168579, 1.2503490447998047, 1.840661883354187, 2.590611457824707, 1.5551977157592773, 0.8864942789077759, 0.00409139646217227, 1.6510039567947388, 3.0415897369384766, 4.647726058959961, 4.0564680099487305, 0.006436040159314871, 3.4834532737731934, 1.4273359775543213, 3.2020764350891113, 2.772225856781006, 2.2301888465881348, 2.535771131515503, 3.3597664833068848, 0.241705521941185, 2.013190746307373, 0.7079310417175293, 2.934335470199585, 0.23465636372566223]], "avg_entropy": [1.8370949885062873], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the 9th century", "score_of_biggest_cluster": 0.19170886278152466, "score_of_first_answer": 0.19170886278152466, "max_score_of_first_answer": 0.9996337890625, "min_score_of_first_answer": 0.0010641053086146712, "avg_score": 0.19170886278152466, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3759971733124178, "std_of_first_answer": 0.3759971733124178, "avg_entropy_of_first_answer": 1.8370949885062873, "lex_sim": 1.0}
{"question": "when is the last time the us lost the olympics", "answer": ["1984 in Los Angeles"], "scores": [[0.6913031339645386, 0.7414243817329407, 0.9025793075561523, 0.48675379157066345, 0.3342518210411072, 0.042455073446035385, 0.7472648024559021, 0.9812731742858887]], "normalized_score": [0.46905606985092163], "tokens": [["\u2581", "1", "9", "8", "4", "\u2581in", "\u2581Los", "\u2581Angeles"]], "entropy": [[2.08868145942688, 0.7913839221000671, 0.4318319261074066, 1.7442694902420044, 0.9236334562301636, 1.1130534410476685, 1.4180779457092285, 0.1240258738398552]], "avg_entropy": [1.0793696893379092], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2018 Pyeongchang", "score_of_biggest_cluster": 0.46905606985092163, "score_of_first_answer": 0.46905606985092163, "max_score_of_first_answer": 0.9812731742858887, "min_score_of_first_answer": 0.042455073446035385, "avg_score": 0.46905606985092163, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29138136918738333, "std_of_first_answer": 0.29138136918738333, "avg_entropy_of_first_answer": 1.0793696893379092, "lex_sim": 1.0}
{"question": "who does the voice of ash in sing", "answer": ["Ryan Potter"], "scores": [[0.02492094412446022, 0.7589067220687866, 0.9871857762336731]], "normalized_score": [0.2652877867221832], "tokens": [["\u2581Ryan", "\u2581Pot", "ter"]], "entropy": [[5.5425801277160645, 1.4655907154083252, 0.10510954260826111]], "avg_entropy": [2.3710934619108834], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Scarlett Johansson", "score_of_biggest_cluster": 0.2652877867221832, "score_of_first_answer": 0.2652877867221832, "max_score_of_first_answer": 0.9871857762336731, "min_score_of_first_answer": 0.02492094412446022, "avg_score": 0.2652877867221832, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41052812272176714, "std_of_first_answer": 0.41052812272176714, "avg_entropy_of_first_answer": 2.3710934619108834, "lex_sim": 1.0}
{"question": "where do the maple leafs play hockey in toronto", "answer": ["Air Canada Centre"], "scores": [[0.7411483526229858, 0.9978774785995483, 0.9130839109420776]], "normalized_score": [0.8773327469825745], "tokens": [["\u2581Air", "\u2581Canada", "\u2581Centre"]], "entropy": [[1.497079610824585, 0.022404290735721588, 0.38814419507980347]], "avg_entropy": [0.63587603221337], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Air Canada Centre", "score_of_biggest_cluster": 0.8773327469825745, "score_of_first_answer": 0.8773327469825745, "max_score_of_first_answer": 0.9978774785995483, "min_score_of_first_answer": 0.7411483526229858, "avg_score": 0.8773327469825745, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1068028450803263, "std_of_first_answer": 0.1068028450803263, "avg_entropy_of_first_answer": 0.63587603221337, "lex_sim": 1.0}
{"question": "when did the united states acquired puerto rico", "answer": ["July 25, 1898"], "scores": [[0.08753442764282227, 0.9841046929359436, 0.7764583826065063, 0.9915738701820374, 0.8307467103004456, 0.9847532510757446, 0.9993440508842468, 0.9883087873458862, 0.999039351940155, 0.9935871362686157]], "normalized_score": [0.745736837387085], "tokens": [["\u2581July", "\u2581", "2", "5", ",", "\u2581", "1", "8", "9", "8"]], "entropy": [[2.7228426933288574, 0.10459018498659134, 0.6983008980751038, 0.06607352197170258, 0.5823411345481873, 0.08370617777109146, 0.005814061965793371, 0.06584927439689636, 0.009696058928966522, 0.04643122851848602]], "avg_entropy": [0.43856452344916763], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1898", "score_of_biggest_cluster": 0.745736837387085, "score_of_first_answer": 0.745736837387085, "max_score_of_first_answer": 0.9993440508842468, "min_score_of_first_answer": 0.08753442764282227, "avg_score": 0.745736837387085, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2693968722726761, "std_of_first_answer": 0.2693968722726761, "avg_entropy_of_first_answer": 0.43856452344916763, "lex_sim": 1.0}
{"question": "who played forrest gumps mother in the 1994 film", "answer": ["Sally Field"], "scores": [[0.8628636598587036, 0.9997898936271667, 0.99406898021698]], "normalized_score": [0.9500704407691956], "tokens": [["\u2581S", "ally", "\u2581Field"]], "entropy": [[1.0649983882904053, 0.002355811884626746, 0.05939672142267227]], "avg_entropy": [0.3755836405325681], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sally Field", "score_of_biggest_cluster": 0.9500704407691956, "score_of_first_answer": 0.9500704407691956, "max_score_of_first_answer": 0.9997898936271667, "min_score_of_first_answer": 0.8628636598587036, "avg_score": 0.9500704407691956, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06324235433331105, "std_of_first_answer": 0.06324235433331105, "avg_entropy_of_first_answer": 0.3755836405325681, "lex_sim": 1.0}
{"question": "how many times have the golden state warriors been to the playoffs", "answer": ["5"], "scores": [[0.5293501019477844, 0.12887510657310486]], "normalized_score": [0.26118966937065125], "tokens": [["\u2581", "5"]], "entropy": [[2.408191204071045, 2.1063222885131836]], "avg_entropy": [2.2572567462921143], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "nine", "score_of_biggest_cluster": 0.26118966937065125, "score_of_first_answer": 0.26118966937065125, "max_score_of_first_answer": 0.5293501019477844, "min_score_of_first_answer": 0.12887510657310486, "avg_score": 0.26118966937065125, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20023749768733978, "std_of_first_answer": 0.20023749768733978, "avg_entropy_of_first_answer": 2.2572567462921143, "lex_sim": 1.0}
{"question": "forney's sister in where the heart is", "answer": ["Sue Johnston"], "scores": [[0.006302311550825834, 0.0015174313448369503, 0.9740472435951233]], "normalized_score": [0.021040834486484528], "tokens": [["\u2581Sue", "\u2581John", "ston"]], "entropy": [[5.96992826461792, 1.901342511177063, 0.1753421425819397]], "avg_entropy": [2.682204306125641], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Margaret Hoard", "score_of_biggest_cluster": 0.021040834486484528, "score_of_first_answer": 0.021040834486484528, "max_score_of_first_answer": 0.9740472435951233, "min_score_of_first_answer": 0.0015174313448369503, "avg_score": 0.021040834486484528, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4573313148997714, "std_of_first_answer": 0.4573313148997714, "avg_entropy_of_first_answer": 2.682204306125641, "lex_sim": 1.0}
{"question": "where does the great outdoors movie take place", "answer": ["Wisconsin, United States"], "scores": [[0.04875575378537178, 0.01999756693840027, 0.10574240237474442, 0.9935146570205688]], "normalized_score": [0.10060197114944458], "tokens": [["\u2581Wisconsin", ",", "\u2581United", "\u2581States"]], "entropy": [[5.171584129333496, 0.42053794860839844, 2.9491195678710938, 0.052050065249204636]], "avg_entropy": [2.1483229277655482], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in Pechoggin, Wisconsin", "score_of_biggest_cluster": 0.10060197114944458, "score_of_first_answer": 0.10060197114944458, "max_score_of_first_answer": 0.9935146570205688, "min_score_of_first_answer": 0.01999756693840027, "avg_score": 0.10060197114944458, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.40619200926681026, "std_of_first_answer": 0.40619200926681026, "avg_entropy_of_first_answer": 2.1483229277655482, "lex_sim": 1.0}
{"question": "when does the mlb season start and finish", "answer": ["April 13 and October 6"], "scores": [[0.321432888507843, 0.6644264459609985, 0.3954680562019348, 0.004766818601638079, 0.08417236059904099, 0.5794287919998169, 0.9957646131515503, 0.018113648518919945]], "normalized_score": [0.15618926286697388], "tokens": [["\u2581April", "\u2581", "1", "3", "\u2581and", "\u2581October", "\u2581", "6"]], "entropy": [[2.7198567390441895, 1.3757312297821045, 1.688754916191101, 1.299543857574463, 1.4376075267791748, 1.4744092226028442, 0.03882558271288872, 1.6117908954620361]], "avg_entropy": [1.4558149962686002], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "March 29", "score_of_biggest_cluster": 0.15618926286697388, "score_of_first_answer": 0.15618926286697388, "max_score_of_first_answer": 0.9957646131515503, "min_score_of_first_answer": 0.004766818601638079, "avg_score": 0.15618926286697388, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3283884650721796, "std_of_first_answer": 0.3283884650721796, "avg_entropy_of_first_answer": 1.4558149962686002, "lex_sim": 1.0}
{"question": "who sang you'll be a woman soon in pulp fiction", "answer": ["Pam Grier"], "scores": [[0.06315020471811295, 0.8139887452125549, 0.9949405193328857]], "normalized_score": [0.3711904287338257], "tokens": [["\u2581Pam", "\u2581G", "rier"]], "entropy": [[5.339198112487793, 0.6767709851264954, 0.04767324775457382]], "avg_entropy": [2.021214115122954], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "rock band Urge Overkill", "score_of_biggest_cluster": 0.3711904287338257, "score_of_first_answer": 0.3711904287338257, "max_score_of_first_answer": 0.9949405193328857, "min_score_of_first_answer": 0.06315020471811295, "avg_score": 0.3711904287338257, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4034208236870466, "std_of_first_answer": 0.4034208236870466, "avg_entropy_of_first_answer": 2.021214115122954, "lex_sim": 1.0}
{"question": "the south west wind blows across nigeria between", "answer": ["October and April"], "scores": [[0.03639422357082367, 0.8198200464248657, 0.2604636549949646]], "normalized_score": [0.19807639718055725], "tokens": [["\u2581October", "\u2581and", "\u2581April"]], "entropy": [[4.407564163208008, 0.8234734535217285, 1.860077977180481]], "avg_entropy": [2.3637051979700723], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "till September", "score_of_biggest_cluster": 0.19807639718055725, "score_of_first_answer": 0.19807639718055725, "max_score_of_first_answer": 0.8198200464248657, "min_score_of_first_answer": 0.03639422357082367, "avg_score": 0.19807639718055725, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3294511770685624, "std_of_first_answer": 0.3294511770685624, "avg_entropy_of_first_answer": 2.3637051979700723, "lex_sim": 1.0}
{"question": "who is the father of genetics and what did he study", "answer": ["Gregor Mendel"], "scores": [[0.7537961006164551, 0.995574951171875, 0.8242672085762024, 0.9980668425559998, 0.996990442276001]], "normalized_score": [0.9075055718421936], "tokens": [["\u2581Greg", "or", "\u2581M", "end", "el"]], "entropy": [[1.5526902675628662, 0.04133373498916626, 0.758441150188446, 0.01975311152637005, 0.027323389425873756]], "avg_entropy": [0.47990833073854444], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the common edible pea", "score_of_biggest_cluster": 0.9075055718421936, "score_of_first_answer": 0.9075055718421936, "max_score_of_first_answer": 0.9980668425559998, "min_score_of_first_answer": 0.7537961006164551, "avg_score": 0.9075055718421936, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10423631142661197, "std_of_first_answer": 0.10423631142661197, "avg_entropy_of_first_answer": 0.47990833073854444, "lex_sim": 1.0}
{"question": "what is the population of st petersburg fl", "answer": ["253,938"], "scores": [[0.923115074634552, 0.03437227010726929, 0.18564841151237488, 0.12243404239416122, 0.7978245615959167, 0.042081959545612335, 0.13166022300720215, 0.13942323625087738]], "normalized_score": [0.16068723797798157], "tokens": [["\u2581", "2", "5", "3", ",", "9", "3", "8"]], "entropy": [[0.6553736925125122, 1.2863094806671143, 1.302932620048523, 2.160472869873047, 1.0303138494491577, 2.2232768535614014, 2.295600414276123, 2.260519504547119]], "avg_entropy": [1.6518499106168747], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "260,999", "score_of_biggest_cluster": 0.16068723797798157, "score_of_first_answer": 0.16068723797798157, "max_score_of_first_answer": 0.923115074634552, "min_score_of_first_answer": 0.03437227010726929, "avg_score": 0.16068723797798157, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3301215588639755, "std_of_first_answer": 0.3301215588639755, "avg_entropy_of_first_answer": 1.6518499106168747, "lex_sim": 1.0}
{"question": "where is beijing located on the world map", "answer": ["In China"], "scores": [[0.015190768986940384, 0.168706476688385]], "normalized_score": [0.05062391236424446], "tokens": [["\u2581In", "\u2581China"]], "entropy": [[3.642507553100586, 2.819993019104004]], "avg_entropy": [3.231250286102295], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "northern China", "score_of_biggest_cluster": 0.05062391236424446, "score_of_first_answer": 0.05062391236424446, "max_score_of_first_answer": 0.168706476688385, "min_score_of_first_answer": 0.015190768986940384, "avg_score": 0.05062391236424446, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.07675785385072231, "std_of_first_answer": 0.07675785385072231, "avg_entropy_of_first_answer": 3.231250286102295, "lex_sim": 1.0}
{"question": "when did the celebrities enter the big brother house", "answer": ["July 6, 2006"], "scores": [[0.11781837046146393, 0.9528837203979492, 0.07882121205329895, 0.2730266749858856, 0.9880168437957764, 0.9886738657951355, 0.9992836117744446, 0.09364036470651627, 0.05118292570114136]], "normalized_score": [0.28206878900527954], "tokens": [["\u2581July", "\u2581", "6", ",", "\u2581", "2", "0", "0", "6"]], "entropy": [[3.2438645362854004, 0.24866920709609985, 1.9914888143539429, 1.2235631942749023, 0.0855354443192482, 0.07359478622674942, 0.008631640113890171, 0.32297492027282715, 1.5986515283584595]], "avg_entropy": [0.9774415634779466], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "February\u00a07,\u00a02018", "score_of_biggest_cluster": 0.28206878900527954, "score_of_first_answer": 0.28206878900527954, "max_score_of_first_answer": 0.9992836117744446, "min_score_of_first_answer": 0.05118292570114136, "avg_score": 0.28206878900527954, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43110681531062356, "std_of_first_answer": 0.43110681531062356, "avg_entropy_of_first_answer": 0.9774415634779466, "lex_sim": 1.0}
{"question": "who played pink in pink floyd the wall", "answer": ["Roger Waters"], "scores": [[0.38324418663978577, 0.6162508130073547, 0.997383177280426]], "normalized_score": [0.6175873279571533], "tokens": [["\u2581Roger", "\u2581W", "aters"]], "entropy": [[3.3505783081054688, 1.7525376081466675, 0.024358855560421944]], "avg_entropy": [1.7091582572708528], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kevin McKeon as Young Pink", "score_of_biggest_cluster": 0.6175873279571533, "score_of_first_answer": 0.6175873279571533, "max_score_of_first_answer": 0.997383177280426, "min_score_of_first_answer": 0.38324418663978577, "avg_score": 0.6175873279571533, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2531404236678575, "std_of_first_answer": 0.2531404236678575, "avg_entropy_of_first_answer": 1.7091582572708528, "lex_sim": 1.0}
{"question": "who played peter benton's sister on er", "answer": ["Sharon Gless"], "scores": [[0.021914927288889885, 0.7940624356269836, 0.2716546952724457, 0.9990334510803223]], "normalized_score": [0.2621488571166992], "tokens": [["\u2581Sh", "aron", "\u2581G", "less"]], "entropy": [[5.198988437652588, 1.155535340309143, 3.370209217071533, 0.01027369312942028]], "avg_entropy": [2.433751672040671], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jackie Robbins", "score_of_biggest_cluster": 0.2621488571166992, "score_of_first_answer": 0.2621488571166992, "max_score_of_first_answer": 0.9990334510803223, "min_score_of_first_answer": 0.021914927288889885, "avg_score": 0.2621488571166992, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39189803797037537, "std_of_first_answer": 0.39189803797037537, "avg_entropy_of_first_answer": 2.433751672040671, "lex_sim": 1.0}
{"question": "who do you meet at the gates of heaven", "answer": ["Your deceased loved ones"], "scores": [[0.023732438683509827, 0.015391381457448006, 0.9975228905677795, 0.3362973928451538, 0.9750818610191345]], "normalized_score": [0.16423340141773224], "tokens": [["\u2581Your", "\u2581dece", "ased", "\u2581loved", "\u2581ones"]], "entropy": [[4.781586647033691, 4.2921977043151855, 0.02405749261379242, 2.2326602935791016, 0.12936025857925415]], "avg_entropy": [2.291972479224205], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Saint Peter", "score_of_biggest_cluster": 0.16423340141773224, "score_of_first_answer": 0.16423340141773224, "max_score_of_first_answer": 0.9975228905677795, "min_score_of_first_answer": 0.015391381457448006, "avg_score": 0.16423340141773224, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43751284016286934, "std_of_first_answer": 0.43751284016286934, "avg_entropy_of_first_answer": 2.291972479224205, "lex_sim": 1.0}
{"question": "when did are you smarter than a 5th grader first air", "answer": ["August 12, 2007"], "scores": [[0.059196967631578445, 0.9772775173187256, 0.4350406229496002, 0.3753543496131897, 0.7309399247169495, 0.9948583841323853, 0.9903736114501953, 0.9998181462287903, 0.9231628179550171, 0.9210480451583862]], "normalized_score": [0.5973394513130188], "tokens": [["\u2581August", "\u2581", "1", "2", ",", "\u2581", "2", "0", "0", "7"]], "entropy": [[2.6641721725463867, 0.1369064748287201, 1.5074414014816284, 2.039661169052124, 0.6977291107177734, 0.03518826514482498, 0.05463450402021408, 0.0022905662190169096, 0.27280130982398987, 0.3645472526550293]], "avg_entropy": [0.7775372226489707], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "February 27, 2007", "score_of_biggest_cluster": 0.5973394513130188, "score_of_first_answer": 0.5973394513130188, "max_score_of_first_answer": 0.9998181462287903, "min_score_of_first_answer": 0.059196967631578445, "avg_score": 0.5973394513130188, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31738137468202055, "std_of_first_answer": 0.31738137468202055, "avg_entropy_of_first_answer": 0.7775372226489707, "lex_sim": 1.0}
{"question": "who played the mom on what's eating gilbert grape", "answer": ["Laura San Giacomo"], "scores": [[0.246128648519516, 0.1468532681465149, 0.9884907007217407, 0.9995704293251038]], "normalized_score": [0.4347183108329773], "tokens": [["\u2581Laura", "\u2581San", "\u2581Gia", "como"]], "entropy": [[2.9874391555786133, 0.9330346584320068, 0.09631544351577759, 0.004729843698441982]], "avg_entropy": [1.00537977530621], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Darlene Cates", "score_of_biggest_cluster": 0.4347183108329773, "score_of_first_answer": 0.4347183108329773, "max_score_of_first_answer": 0.9995704293251038, "min_score_of_first_answer": 0.1468532681465149, "avg_score": 0.4347183108329773, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4003306773802605, "std_of_first_answer": 0.4003306773802605, "avg_entropy_of_first_answer": 1.00537977530621, "lex_sim": 1.0}
{"question": "when was the reaper invented by cyrus mccormick", "answer": ["1834"], "scores": [[0.7846556305885315, 0.9943655729293823, 0.9734138250350952, 0.73544842004776, 0.32796552777290344]], "normalized_score": [0.7121649980545044], "tokens": [["\u2581", "1", "8", "3", "4"]], "entropy": [[1.376813530921936, 0.04536993429064751, 0.1648571938276291, 1.0523221492767334, 0.9626871347427368]], "avg_entropy": [0.7204099886119366], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1837", "score_of_biggest_cluster": 0.7121649980545044, "score_of_first_answer": 0.7121649980545044, "max_score_of_first_answer": 0.9943655729293823, "min_score_of_first_answer": 0.32796552777290344, "avg_score": 0.7121649980545044, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2401195160209284, "std_of_first_answer": 0.2401195160209284, "avg_entropy_of_first_answer": 0.7204099886119366, "lex_sim": 1.0}
{"question": "what percentage of the earth's surface is water", "answer": ["70.57%"], "scores": [[0.8987056016921997, 0.813344419002533, 0.515822172164917, 0.3924427032470703, 0.016700327396392822, 0.03738722205162048, 0.7050229907035828]], "normalized_score": [0.2523331046104431], "tokens": [["\u2581", "7", "0", ".", "5", "7", "%"]], "entropy": [[0.7889642119407654, 0.8010057806968689, 1.1061867475509644, 1.3445007801055908, 0.9955036640167236, 1.8597767353057861, 1.2787692546844482]], "avg_entropy": [1.1678153106144495], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "78%", "score_of_biggest_cluster": 0.2523331046104431, "score_of_first_answer": 0.2523331046104431, "max_score_of_first_answer": 0.8987056016921997, "min_score_of_first_answer": 0.016700327396392822, "avg_score": 0.2523331046104431, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3287457741582982, "std_of_first_answer": 0.3287457741582982, "avg_entropy_of_first_answer": 1.1678153106144495, "lex_sim": 1.0}
{"question": "who sang rip it up and start again", "answer": ["The Stranglers"], "scores": [[0.3040943443775177, 0.022092951461672783, 0.9068387150764465, 0.9992222785949707]], "normalized_score": [0.279327392578125], "tokens": [["\u2581The", "\u2581Str", "ang", "lers"]], "entropy": [[4.130668640136719, 2.6661605834960938, 0.43362849950790405, 0.007701993454247713]], "avg_entropy": [1.809539929148741], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Scottish post-punk band Orange Juice", "score_of_biggest_cluster": 0.279327392578125, "score_of_first_answer": 0.279327392578125, "max_score_of_first_answer": 0.9992222785949707, "min_score_of_first_answer": 0.022092951461672783, "avg_score": 0.279327392578125, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40866550487010384, "std_of_first_answer": 0.40866550487010384, "avg_entropy_of_first_answer": 1.809539929148741, "lex_sim": 1.0}
{"question": "how long do former presidents get secret service protection", "answer": ["Lifetime"], "scores": [[0.025061730295419693, 0.9612166881561279, 0.9376075863838196]], "normalized_score": [0.2826731204986572], "tokens": [["\u2581L", "if", "etime"]], "entropy": [[3.6538217067718506, 0.2367224097251892, 0.24888214468955994]], "avg_entropy": [1.3798087537288666], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "lifetime protection", "score_of_biggest_cluster": 0.2826731204986572, "score_of_first_answer": 0.2826731204986572, "max_score_of_first_answer": 0.9612166881561279, "min_score_of_first_answer": 0.025061730295419693, "avg_score": 0.2826731204986572, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4358495451768484, "std_of_first_answer": 0.4358495451768484, "avg_entropy_of_first_answer": 1.3798087537288666, "lex_sim": 1.0}
{"question": "who was in dont worry be happy video", "answer": ["Bobby McFerrin"], "scores": [[0.3916528820991516, 0.9907015562057495, 0.9843412041664124, 0.9869154691696167, 0.9991610050201416]], "normalized_score": [0.8225854635238647], "tokens": [["\u2581Bobby", "\u2581Mc", "F", "err", "in"]], "entropy": [[3.189762830734253, 0.0841231644153595, 0.1215323954820633, 0.08309155702590942, 0.009377483278512955]], "avg_entropy": [0.6975774861872196], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bill Irwin", "score_of_biggest_cluster": 0.8225854635238647, "score_of_first_answer": 0.8225854635238647, "max_score_of_first_answer": 0.9991610050201416, "min_score_of_first_answer": 0.3916528820991516, "avg_score": 0.8225854635238647, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23950323466190226, "std_of_first_answer": 0.23950323466190226, "avg_entropy_of_first_answer": 0.6975774861872196, "lex_sim": 1.0}
{"question": "when does isle of dogs come out in us", "answer": ["February 23"], "scores": [[0.03141319751739502, 0.9315085411071777, 0.5397988557815552, 0.9061239957809448]], "normalized_score": [0.3458833396434784], "tokens": [["\u2581February", "\u2581", "2", "3"]], "entropy": [[2.1726064682006836, 0.30982816219329834, 0.9083008766174316, 0.43885311484336853]], "avg_entropy": [0.9573971554636955], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "April 13, 2018", "score_of_biggest_cluster": 0.3458833396434784, "score_of_first_answer": 0.3458833396434784, "max_score_of_first_answer": 0.9315085411071777, "min_score_of_first_answer": 0.03141319751739502, "avg_score": 0.3458833396434784, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3641790035987144, "std_of_first_answer": 0.3641790035987144, "avg_entropy_of_first_answer": 0.9573971554636955, "lex_sim": 1.0}
{"question": "when did linkin park release one more light", "answer": ["September 21, 2017"], "scores": [[0.17808975279331207, 0.9718429446220398, 0.7769734859466553, 0.46718794107437134, 0.6149487495422363, 0.9915205836296082, 0.9997856020927429, 0.99983811378479, 0.9955509901046753, 0.9972929358482361]], "normalized_score": [0.7211068272590637], "tokens": [["\u2581September", "\u2581", "2", "1", ",", "\u2581", "2", "0", "1", "7"]], "entropy": [[2.3383779525756836, 0.16648095846176147, 0.6808946132659912, 1.1782760620117188, 0.8707777261734009, 0.05183211714029312, 0.0023103945422917604, 0.0020030129235237837, 0.030879957601428032, 0.02351691573858261]], "avg_entropy": [0.5345349710434675], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "May 19, 2017", "score_of_biggest_cluster": 0.7211068272590637, "score_of_first_answer": 0.7211068272590637, "max_score_of_first_answer": 0.99983811378479, "min_score_of_first_answer": 0.17808975279331207, "avg_score": 0.7211068272590637, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2748690476565675, "std_of_first_answer": 0.2748690476565675, "avg_entropy_of_first_answer": 0.5345349710434675, "lex_sim": 1.0}
{"question": "where was robin hood prince of thieves made", "answer": ["Nottingham"], "scores": [[0.08352921903133392, 0.9994732737541199, 0.997468113899231]], "normalized_score": [0.43668627738952637], "tokens": [["\u2581Not", "ting", "ham"]], "entropy": [[3.8087220191955566, 0.006614740006625652, 0.02501477487385273]], "avg_entropy": [1.280117178025345], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the United Kingdom", "score_of_biggest_cluster": 0.43668627738952637, "score_of_first_answer": 0.43668627738952637, "max_score_of_first_answer": 0.9994732737541199, "min_score_of_first_answer": 0.08352921903133392, "avg_score": 0.43668627738952637, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4313083243136727, "std_of_first_answer": 0.4313083243136727, "avg_entropy_of_first_answer": 1.280117178025345, "lex_sim": 1.0}
{"question": "the initial unification of upper and lower egypt took place during which period", "answer": ["Middle Kingdom"], "scores": [[0.03724543750286102, 0.9624469876289368]], "normalized_score": [0.18933242559432983], "tokens": [["\u2581Middle", "\u2581Kingdom"]], "entropy": [[2.4468765258789062, 0.2392335832118988]], "avg_entropy": [1.3430550545454025], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "3000 BC", "score_of_biggest_cluster": 0.18933242559432983, "score_of_first_answer": 0.18933242559432983, "max_score_of_first_answer": 0.9624469876289368, "min_score_of_first_answer": 0.03724543750286102, "avg_score": 0.18933242559432983, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4626007750630379, "std_of_first_answer": 0.4626007750630379, "avg_entropy_of_first_answer": 1.3430550545454025, "lex_sim": 1.0}
{"question": "who is mostly responsible for writing the declaration of independence", "answer": ["Thomas Jefferson"], "scores": [[0.8592827916145325, 0.9947279095649719]], "normalized_score": [0.9245283007621765], "tokens": [["\u2581Thomas", "\u2581Jefferson"]], "entropy": [[0.8354451656341553, 0.054935306310653687]], "avg_entropy": [0.4451902359724045], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Thomas Jefferson", "score_of_biggest_cluster": 0.9245283007621765, "score_of_first_answer": 0.9245283007621765, "max_score_of_first_answer": 0.9947279095649719, "min_score_of_first_answer": 0.8592827916145325, "avg_score": 0.9245283007621765, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06772255897521973, "std_of_first_answer": 0.06772255897521973, "avg_entropy_of_first_answer": 0.4451902359724045, "lex_sim": 1.0}
{"question": "where does the president get authority to issue executive orders", "answer": ["legislation is needed"], "scores": [[0.008804691024124622, 0.3213486969470978, 0.0029749441891908646, 0.0909547284245491]], "normalized_score": [0.029580041766166687], "tokens": [["\u2581legisl", "ation", "\u2581is", "\u2581needed"]], "entropy": [[3.8503823280334473, 1.0861241817474365, 1.3280696868896484, 3.25097918510437]], "avg_entropy": [2.3788888454437256], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Constitution", "score_of_biggest_cluster": 0.029580041766166687, "score_of_first_answer": 0.029580041766166687, "max_score_of_first_answer": 0.3213486969470978, "min_score_of_first_answer": 0.0029749441891908646, "avg_score": 0.029580041766166687, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1290954200974642, "std_of_first_answer": 0.1290954200974642, "avg_entropy_of_first_answer": 2.3788888454437256, "lex_sim": 1.0}
{"question": "when did muhammad ali win an olympic gold medal", "answer": ["1964"], "scores": [[0.799454927444458, 0.9957109689712524, 0.9936779141426086, 0.9934105277061462, 0.008068403229117393]], "normalized_score": [0.3634275496006012], "tokens": [["\u2581", "1", "9", "6", "4"]], "entropy": [[1.238466501235962, 0.03662598878145218, 0.04999331012368202, 0.049966782331466675, 0.05902491882443428]], "avg_entropy": [0.2868155002593994], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1960", "score_of_biggest_cluster": 0.3634275496006012, "score_of_first_answer": 0.3634275496006012, "max_score_of_first_answer": 0.9957109689712524, "min_score_of_first_answer": 0.008068403229117393, "avg_score": 0.3634275496006012, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38251394498791425, "std_of_first_answer": 0.38251394498791425, "avg_entropy_of_first_answer": 0.2868155002593994, "lex_sim": 1.0}
{"question": "who headed the 7th central pay commission of india", "answer": ["Ashok Kumar Mathur"], "scores": [[0.7977643609046936, 0.9897626638412476, 0.6553250551223755, 0.9703580737113953, 0.9500885009765625, 0.9949929118156433]], "normalized_score": [0.8832082152366638], "tokens": [["\u2581Ash", "ok", "\u2581K", "umar", "\u2581Math", "ur"]], "entropy": [[1.1833348274230957, 0.08279722929000854, 1.4927494525909424, 0.20715361833572388, 0.3406343460083008, 0.03448941186070442]], "avg_entropy": [0.5568598142514626], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Justice A.K Mathur", "score_of_biggest_cluster": 0.8832082152366638, "score_of_first_answer": 0.8832082152366638, "max_score_of_first_answer": 0.9949929118156433, "min_score_of_first_answer": 0.6553250551223755, "avg_score": 0.8832082152366638, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.12554423383437294, "std_of_first_answer": 0.12554423383437294, "avg_entropy_of_first_answer": 0.5568598142514626, "lex_sim": 1.0}
{"question": "when was the statue of liberty in france built", "answer": ["December 1886"], "scores": [[0.003663664450868964, 0.9446970820426941, 0.2583785355091095, 0.6724367141723633, 0.8688474297523499, 0.4949151575565338]], "normalized_score": [0.2524052560329437], "tokens": [["\u2581December", "\u2581", "1", "8", "8", "6"]], "entropy": [[1.6633448600769043, 0.2737317681312561, 1.3454208374023438, 1.3553861379623413, 0.5878493785858154, 1.5116819143295288]], "avg_entropy": [1.1229024827480316], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paris", "score_of_biggest_cluster": 0.2524052560329437, "score_of_first_answer": 0.2524052560329437, "max_score_of_first_answer": 0.9446970820426941, "min_score_of_first_answer": 0.003663664450868964, "avg_score": 0.2524052560329437, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3312749013099839, "std_of_first_answer": 0.3312749013099839, "avg_entropy_of_first_answer": 1.1229024827480316, "lex_sim": 1.0}
{"question": "right to property according to the constitution of india is a", "answer": ["fundamental"], "scores": [[0.27308642864227295]], "normalized_score": [0.27308642864227295], "tokens": [["\u2581fundamental"]], "entropy": [[3.7593157291412354]], "avg_entropy": [3.7593157291412354], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "constitutional right", "score_of_biggest_cluster": 0.27308642864227295, "score_of_first_answer": 0.27308642864227295, "max_score_of_first_answer": 0.27308642864227295, "min_score_of_first_answer": 0.27308642864227295, "avg_score": 0.27308642864227295, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.7593157291412354, "lex_sim": 1.0}
{"question": "who does luke skywalker fall in love with", "answer": ["Mara Jade"], "scores": [[0.30633264780044556, 0.6979918479919434, 0.8678281903266907, 0.9987247586250305]], "normalized_score": [0.656116247177124], "tokens": [["\u2581Mar", "a", "\u2581J", "ade"]], "entropy": [[3.0289125442504883, 1.1557929515838623, 0.5345017910003662, 0.014279104769229889]], "avg_entropy": [1.1833715979009867], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mara Jade", "score_of_biggest_cluster": 0.656116247177124, "score_of_first_answer": 0.656116247177124, "max_score_of_first_answer": 0.9987247586250305, "min_score_of_first_answer": 0.30633264780044556, "avg_score": 0.656116247177124, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2603482747699927, "std_of_first_answer": 0.2603482747699927, "avg_entropy_of_first_answer": 1.1833715979009867, "lex_sim": 1.0}
{"question": "who is the book of acts written to", "answer": ["The Apostle Paul and Gentiles"], "scores": [[0.4310629665851593, 0.038370899856090546, 0.3852660655975342, 0.8407427072525024, 0.030435219407081604, 0.03357766941189766, 0.6081376671791077]], "normalized_score": [0.16500043869018555], "tokens": [["\u2581The", "\u2581Apost", "le", "\u2581Paul", "\u2581and", "\u2581Gent", "iles"]], "entropy": [[3.3039417266845703, 3.956108331680298, 0.7360864877700806, 0.7613731026649475, 0.7339658737182617, 3.0079402923583984, 0.680402934551239]], "avg_entropy": [1.8828312499182565], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jewish audiences", "score_of_biggest_cluster": 0.16500043869018555, "score_of_first_answer": 0.16500043869018555, "max_score_of_first_answer": 0.8407427072525024, "min_score_of_first_answer": 0.030435219407081604, "avg_score": 0.16500043869018555, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2960853328484837, "std_of_first_answer": 0.2960853328484837, "avg_entropy_of_first_answer": 1.8828312499182565, "lex_sim": 1.0}
{"question": "where did the butchers in the slaughterhouse cases live", "answer": ["Butcher\u2019s Hill, South Boston"], "scores": [[0.023196902126073837, 0.391940176486969, 0.06737158447504044, 0.9995905756950378, 0.24189092218875885, 0.12584196031093597, 0.043248750269412994, 0.07418044656515121]], "normalized_score": [0.12504954636096954], "tokens": [["\u2581But", "cher", "\u2019", "s", "\u2581Hill", ",", "\u2581South", "\u2581Boston"]], "entropy": [[5.902007102966309, 1.0248563289642334, 3.7310025691986084, 0.005537983030080795, 1.7479259967803955, 0.883475661277771, 3.584184408187866, 2.893779754638672]], "avg_entropy": [2.471596225630492], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "New Orleans", "score_of_biggest_cluster": 0.12504954636096954, "score_of_first_answer": 0.12504954636096954, "max_score_of_first_answer": 0.9995905756950378, "min_score_of_first_answer": 0.023196902126073837, "avg_score": 0.12504954636096954, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30737424688205417, "std_of_first_answer": 0.30737424688205417, "avg_entropy_of_first_answer": 2.471596225630492, "lex_sim": 1.0}
{"question": "what is the minimum number of parties required to enter into a deed", "answer": ["2 parties to an agreement"], "scores": [[0.39809006452560425, 0.6275935173034668, 0.017114607617259026, 0.005381764844059944, 0.023101910948753357, 0.6617323160171509]], "normalized_score": [0.08401952683925629], "tokens": [["\u2581", "2", "\u2581parties", "\u2581to", "\u2581an", "\u2581agreement"]], "entropy": [[2.5146141052246094, 1.1279219388961792, 0.7415930032730103, 0.8218703866004944, 2.2966673374176025, 1.9802218675613403]], "avg_entropy": [1.5804814398288727], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "two parties", "score_of_biggest_cluster": 0.08401952683925629, "score_of_first_answer": 0.08401952683925629, "max_score_of_first_answer": 0.6617323160171509, "min_score_of_first_answer": 0.005381764844059944, "avg_score": 0.08401952683925629, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.28593070058836967, "std_of_first_answer": 0.28593070058836967, "avg_entropy_of_first_answer": 1.5804814398288727, "lex_sim": 1.0}
{"question": "where is fight or flight in the brain", "answer": ["hypothalamus"], "scores": [[0.14178237318992615, 0.9966838955879211, 0.9903799295425415, 0.9888289570808411]], "normalized_score": [0.6099238991737366], "tokens": [["\u2581hypoth", "al", "am", "us"]], "entropy": [[4.063375473022461, 0.03011395037174225, 0.0743192732334137, 0.06773371249437332]], "avg_entropy": [1.0588856022804976], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the adrenal medulla", "score_of_biggest_cluster": 0.6099238991737366, "score_of_first_answer": 0.6099238991737366, "max_score_of_first_answer": 0.9966838955879211, "min_score_of_first_answer": 0.14178237318992615, "avg_score": 0.6099238991737366, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36815130945795715, "std_of_first_answer": 0.36815130945795715, "avg_entropy_of_first_answer": 1.0588856022804976, "lex_sim": 1.0}
{"question": "the complete collection of dna and genes is called", "answer": ["genome"], "scores": [[0.5968829393386841, 0.945239782333374]], "normalized_score": [0.7511308193206787], "tokens": [["\u2581gen", "ome"]], "entropy": [[2.2865922451019287, 0.2804027199745178]], "avg_entropy": [1.2834974825382233], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "genome", "score_of_biggest_cluster": 0.7511308193206787, "score_of_first_answer": 0.7511308193206787, "max_score_of_first_answer": 0.945239782333374, "min_score_of_first_answer": 0.5968829393386841, "avg_score": 0.7511308193206787, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.17417842149734497, "std_of_first_answer": 0.17417842149734497, "avg_entropy_of_first_answer": 1.2834974825382233, "lex_sim": 1.0}
{"question": "where was part of the classic surfing movie endless summer filmed", "answer": ["Malibu"], "scores": [[0.0673099011182785, 0.995870053768158, 0.9973823428153992]], "normalized_score": [0.40586453676223755], "tokens": [["\u2581Mal", "ib", "u"]], "entropy": [[4.7253289222717285, 0.04276038706302643, 0.027279172092676163]], "avg_entropy": [1.5984561604758103], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ghana", "score_of_biggest_cluster": 0.40586453676223755, "score_of_first_answer": 0.40586453676223755, "max_score_of_first_answer": 0.9973823428153992, "min_score_of_first_answer": 0.0673099011182785, "avg_score": 0.40586453676223755, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43808433877403785, "std_of_first_answer": 0.43808433877403785, "avg_entropy_of_first_answer": 1.5984561604758103, "lex_sim": 1.0}
{"question": "which film won the oscar for best animated feature in 2007", "answer": ["The Simpsons Movie"], "scores": [[0.02747361734509468, 0.7216245532035828, 0.9764177799224854, 0.9996762275695801, 0.8289164304733276]], "normalized_score": [0.4375690221786499], "tokens": [["\u2581The", "\u2581Sim", "ps", "ons", "\u2581Movie"]], "entropy": [[2.421743392944336, 1.436936616897583, 0.12620961666107178, 0.004054577089846134, 0.6878077983856201]], "avg_entropy": [0.9353504003956914], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ratatouille", "score_of_biggest_cluster": 0.4375690221786499, "score_of_first_answer": 0.4375690221786499, "max_score_of_first_answer": 0.9996762275695801, "min_score_of_first_answer": 0.02747361734509468, "avg_score": 0.4375690221786499, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35637224494583625, "std_of_first_answer": 0.35637224494583625, "avg_entropy_of_first_answer": 0.9353504003956914, "lex_sim": 1.0}
{"question": "in which country bikram sambhat the official calender", "answer": ["Cambodia"], "scores": [[0.0321718268096447, 0.9943724870681763]], "normalized_score": [0.1788596659898758], "tokens": [["\u2581Camb", "odia"]], "entropy": [[2.6860570907592773, 0.04791117459535599]], "avg_entropy": [1.3669841326773167], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nepal", "score_of_biggest_cluster": 0.1788596659898758, "score_of_first_answer": 0.1788596659898758, "max_score_of_first_answer": 0.9943724870681763, "min_score_of_first_answer": 0.0321718268096447, "avg_score": 0.1788596659898758, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4811003301292658, "std_of_first_answer": 0.4811003301292658, "avg_entropy_of_first_answer": 1.3669841326773167, "lex_sim": 1.0}
{"question": "colby chees is named after a town in what state", "answer": ["Colby"], "scores": [[0.05583702772855759, 0.9808026552200317]], "normalized_score": [0.234019473195076], "tokens": [["\u2581Col", "by"]], "entropy": [[3.7767419815063477, 0.1665133386850357]], "avg_entropy": [1.9716276600956917], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wisconsin", "score_of_biggest_cluster": 0.234019473195076, "score_of_first_answer": 0.234019473195076, "max_score_of_first_answer": 0.9808026552200317, "min_score_of_first_answer": 0.05583702772855759, "avg_score": 0.234019473195076, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4624828137457371, "std_of_first_answer": 0.4624828137457371, "avg_entropy_of_first_answer": 1.9716276600956917, "lex_sim": 1.0}
{"question": "when did the uk and us become allies", "answer": ["During World War II"], "scores": [[0.014160031452775002, 0.6938121914863586, 0.9960254430770874, 0.38812845945358276]], "normalized_score": [0.24824930727481842], "tokens": [["\u2581During", "\u2581World", "\u2581War", "\u2581II"]], "entropy": [[2.579770803451538, 0.9862481951713562, 0.034560106694698334, 1.2775789499282837]], "avg_entropy": [1.219539513811469], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Since 1940", "score_of_biggest_cluster": 0.24824930727481842, "score_of_first_answer": 0.24824930727481842, "max_score_of_first_answer": 0.9960254430770874, "min_score_of_first_answer": 0.014160031452775002, "avg_score": 0.24824930727481842, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3640186522083187, "std_of_first_answer": 0.3640186522083187, "avg_entropy_of_first_answer": 1.219539513811469, "lex_sim": 1.0}
{"question": "whats the movie called with justin timberlake and time", "answer": ["Bad Teacher"], "scores": [[0.08920815587043762, 0.9192686676979065, 0.9830701351165771]], "normalized_score": [0.4319934844970703], "tokens": [["\u2581Bad", "\u2581Te", "acher"]], "entropy": [[3.116882801055908, 0.5993682742118835, 0.10944247990846634]], "avg_entropy": [1.2752311850587528], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "In Time", "score_of_biggest_cluster": 0.4319934844970703, "score_of_first_answer": 0.4319934844970703, "max_score_of_first_answer": 0.9830701351165771, "min_score_of_first_answer": 0.08920815587043762, "avg_score": 0.4319934844970703, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40716640312201613, "std_of_first_answer": 0.40716640312201613, "avg_entropy_of_first_answer": 1.2752311850587528, "lex_sim": 1.0}
{"question": "which songs did liam write as you were", "answer": ["What's the Story Morning Glory?"], "scores": [[0.043730899691581726, 0.017347754910588264, 0.9907580614089966, 0.48567602038383484, 0.8661338090896606, 0.7670260071754456, 0.99919193983078, 0.994998574256897, 0.9998102784156799, 0.21187058091163635]], "normalized_score": [0.3722374141216278], "tokens": [["\u2581What", "'", "s", "\u2581the", "\u2581Story", "\u2581Mor", "ning", "\u2581Gl", "ory", "?"]], "entropy": [[5.328805446624756, 1.9698026180267334, 0.06816118955612183, 1.9614675045013428, 0.792216420173645, 1.082282543182373, 0.009682760573923588, 0.04461265355348587, 0.0021178885363042355, 1.8388123512268066]], "avg_entropy": [1.3097961375955491], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "I've All I Need", "score_of_biggest_cluster": 0.3722374141216278, "score_of_first_answer": 0.3722374141216278, "max_score_of_first_answer": 0.9998102784156799, "min_score_of_first_answer": 0.017347754910588264, "avg_score": 0.3722374141216278, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3905250219004845, "std_of_first_answer": 0.3905250219004845, "avg_entropy_of_first_answer": 1.3097961375955491, "lex_sim": 1.0}
{"question": "who hit the first home run in the houston astrodome", "answer": ["Willie Mays"], "scores": [[0.06578058004379272, 0.9978585839271545, 0.7661420702934265, 0.9999507665634155]], "normalized_score": [0.4735476076602936], "tokens": [["\u2581Will", "ie", "\u2581M", "ays"]], "entropy": [[4.055191993713379, 0.021839160472154617, 1.2360754013061523, 0.0007273093215189874]], "avg_entropy": [1.3284584662033012], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mickey Mantle", "score_of_biggest_cluster": 0.4735476076602936, "score_of_first_answer": 0.4735476076602936, "max_score_of_first_answer": 0.9999507665634155, "min_score_of_first_answer": 0.06578058004379272, "avg_score": 0.4735476076602936, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38245203834110136, "std_of_first_answer": 0.38245203834110136, "avg_entropy_of_first_answer": 1.3284584662033012, "lex_sim": 1.0}
{"question": "what is the most popular religion in sweden", "answer": ["Evangelical Lutheran"], "scores": [[0.06630639731884003, 0.955483615398407, 0.9338517189025879, 0.9921024441719055]], "normalized_score": [0.49221324920654297], "tokens": [["\u2581Evangel", "ical", "\u2581Luther", "an"]], "entropy": [[2.83038592338562, 0.28608548641204834, 0.4071662127971649, 0.053468961268663406]], "avg_entropy": [0.8942766459658742], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lutheran", "score_of_biggest_cluster": 0.49221324920654297, "score_of_first_answer": 0.49221324920654297, "max_score_of_first_answer": 0.9921024441719055, "min_score_of_first_answer": 0.06630639731884003, "avg_score": 0.49221324920654297, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.38774761081497006, "std_of_first_answer": 0.38774761081497006, "avg_entropy_of_first_answer": 0.8942766459658742, "lex_sim": 1.0}
{"question": "when was the r10+20 summit in rio de janeiro held", "answer": ["March 20, 1992"], "scores": [[0.018307432532310486, 0.9309278130531311, 0.499565452337265, 0.8641960620880127, 0.16636984050273895, 0.9838467836380005, 0.06291387975215912, 0.9922550916671753, 0.49038463830947876, 0.8987187147140503]], "normalized_score": [0.3564741909503937], "tokens": [["\u2581March", "\u2581", "2", "0", ",", "\u2581", "1", "9", "9", "2"]], "entropy": [[2.479738712310791, 0.3630383610725403, 1.2005451917648315, 0.6677737832069397, 1.8287396430969238, 0.09739174693822861, 0.24027125537395477, 0.060903020203113556, 1.4536250829696655, 0.5413458943367004]], "avg_entropy": [0.8933372691273689], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "June 2012", "score_of_biggest_cluster": 0.3564741909503937, "score_of_first_answer": 0.3564741909503937, "max_score_of_first_answer": 0.9922550916671753, "min_score_of_first_answer": 0.018307432532310486, "avg_score": 0.3564741909503937, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37494768338401563, "std_of_first_answer": 0.37494768338401563, "avg_entropy_of_first_answer": 0.8933372691273689, "lex_sim": 1.0}
{"question": "who is the guy on keeping up with the kardashians", "answer": ["Bruce Jenner"], "scores": [[0.05722246319055557, 0.8918116688728333, 0.9998003840446472]], "normalized_score": [0.37089499831199646], "tokens": [["\u2581Bruce", "\u2581Jen", "ner"]], "entropy": [[2.607142448425293, 0.5974531769752502, 0.002630045171827078]], "avg_entropy": [1.0690752235241234], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jonathan Cheban", "score_of_biggest_cluster": 0.37089499831199646, "score_of_first_answer": 0.37089499831199646, "max_score_of_first_answer": 0.9998003840446472, "min_score_of_first_answer": 0.05722246319055557, "avg_score": 0.37089499831199646, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4211958955333751, "std_of_first_answer": 0.4211958955333751, "avg_entropy_of_first_answer": 1.0690752235241234, "lex_sim": 1.0}
{"question": "who played sam in clarissa explains it all", "answer": ["Melissa Joan Hart"], "scores": [[0.8186827898025513, 0.9021497368812561, 0.8326231837272644, 0.9847495555877686]], "normalized_score": [0.8821493983268738], "tokens": [["\u2581Mel", "issa", "\u2581Joan", "\u2581Hart"]], "entropy": [[0.9710360169410706, 0.3535291850566864, 1.0422065258026123, 0.14155471324920654]], "avg_entropy": [0.627081610262394], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sean O'Neal", "score_of_biggest_cluster": 0.8821493983268738, "score_of_first_answer": 0.8821493983268738, "max_score_of_first_answer": 0.9847495555877686, "min_score_of_first_answer": 0.8186827898025513, "avg_score": 0.8821493983268738, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06592530868103769, "std_of_first_answer": 0.06592530868103769, "avg_entropy_of_first_answer": 0.627081610262394, "lex_sim": 1.0}
{"question": "where are trigger points located in the body", "answer": ["Muscles and soft tissues"], "scores": [[0.05207734555006027, 0.5065696239471436, 0.05636388808488846, 0.020916566252708435, 0.9779983758926392, 0.5836376547813416]], "normalized_score": [0.16151396930217743], "tokens": [["\u2581Mus", "cles", "\u2581and", "\u2581soft", "\u2581t", "issues"]], "entropy": [[4.692739963531494, 0.9064650535583496, 0.9229203462600708, 3.1687426567077637, 0.1601717174053192, 0.6904591917991638]], "avg_entropy": [1.7569164882103603], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "muscles", "score_of_biggest_cluster": 0.16151396930217743, "score_of_first_answer": 0.16151396930217743, "max_score_of_first_answer": 0.9779983758926392, "min_score_of_first_answer": 0.020916566252708435, "avg_score": 0.16151396930217743, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3547703726069121, "std_of_first_answer": 0.3547703726069121, "avg_entropy_of_first_answer": 1.7569164882103603, "lex_sim": 1.0}
{"question": "who played shmuel in the boy in the striped pyjamas", "answer": ["Yorick van Wageningen"], "scores": [[0.03883521258831024, 0.37781578302383423, 0.7835314273834229, 0.669465184211731, 0.9784498810768127, 0.9967888593673706, 0.9978517293930054]], "normalized_score": [0.497000515460968], "tokens": [["\u2581Y", "or", "ick", "\u2581van", "\u2581W", "agen", "ingen"]], "entropy": [[4.562882423400879, 2.9791414737701416, 0.9117399454116821, 1.7476067543029785, 0.1634279191493988, 0.03359103575348854, 0.01675041764974594]], "avg_entropy": [1.4878771384911877], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jack Scanlon", "score_of_biggest_cluster": 0.497000515460968, "score_of_first_answer": 0.497000515460968, "max_score_of_first_answer": 0.9978517293930054, "min_score_of_first_answer": 0.03883521258831024, "avg_score": 0.497000515460968, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3386352057210627, "std_of_first_answer": 0.3386352057210627, "avg_entropy_of_first_answer": 1.4878771384911877, "lex_sim": 1.0}
{"question": "who won the icc under 19 world cup 2018", "answer": ["India"], "scores": [[0.3493229150772095]], "normalized_score": [0.3493229150772095], "tokens": [["\u2581India"]], "entropy": [[2.889561891555786]], "avg_entropy": [2.889561891555786], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "West Indies", "score_of_biggest_cluster": 0.3493229150772095, "score_of_first_answer": 0.3493229150772095, "max_score_of_first_answer": 0.3493229150772095, "min_score_of_first_answer": 0.3493229150772095, "avg_score": 0.3493229150772095, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.889561891555786, "lex_sim": 1.0}
{"question": "who plays the prosecutor on law and order svu", "answer": ["Richard Belzer"], "scores": [[0.13457618653774261, 0.8531436324119568, 0.9989269375801086]], "normalized_score": [0.4858565330505371], "tokens": [["\u2581Richard", "\u2581Bel", "zer"]], "entropy": [[3.2994415760040283, 0.7756754159927368, 0.011802205815911293]], "avg_entropy": [1.3623063992708921], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ra\u00fal Eduardo Esparza", "score_of_biggest_cluster": 0.4858565330505371, "score_of_first_answer": 0.4858565330505371, "max_score_of_first_answer": 0.9989269375801086, "min_score_of_first_answer": 0.13457618653774261, "avg_score": 0.4858565330505371, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37781449992224203, "std_of_first_answer": 0.37781449992224203, "avg_entropy_of_first_answer": 1.3623063992708921, "lex_sim": 1.0}
{"question": "when do you get your white coat in pharmacy school", "answer": ["When you complete your residency and become a licensed pharmacist."], "scores": [[0.030401909723877907, 0.7776362299919128, 0.027950763702392578, 0.4196987450122833, 0.0028730721678584814, 0.9977699518203735, 0.9923816323280334, 0.03874221071600914, 0.18823792040348053, 0.7962165474891663, 0.15251053869724274, 0.9951726794242859, 0.7428173422813416, 0.9989816546440125, 0.9961574673652649, 0.9960256814956665, 0.30832546949386597]], "normalized_score": [0.2651095986366272], "tokens": [["\u2581When", "\u2581you", "\u2581complete", "\u2581your", "\u2581res", "iden", "cy", "\u2581and", "\u2581become", "\u2581a", "\u2581lic", "ensed", "\u2581ph", "arm", "ac", "ist", "."]], "entropy": [[4.319645881652832, 1.4066648483276367, 3.0083365440368652, 2.4333696365356445, 3.1329398155212402, 0.026343440636992455, 0.04593592509627342, 1.6014164686203003, 3.9881083965301514, 0.958005964756012, 2.7091591358184814, 0.037933968007564545, 1.1517736911773682, 0.009917211718857288, 0.02826417051255703, 0.03048533760011196, 0.9936087727546692]], "avg_entropy": [1.5224652476060916], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "before the first year begins", "score_of_biggest_cluster": 0.2651095986366272, "score_of_first_answer": 0.2651095986366272, "max_score_of_first_answer": 0.9989816546440125, "min_score_of_first_answer": 0.0028730721678584814, "avg_score": 0.2651095986366272, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40658878865501796, "std_of_first_answer": 0.40658878865501796, "avg_entropy_of_first_answer": 1.5224652476060916, "lex_sim": 1.0}
{"question": "who won oscar for best director this month", "answer": ["Matthew McConaughey"], "scores": [[0.003079142654314637, 0.2709381878376007, 0.9584873914718628, 0.9758314490318298, 0.9941335320472717, 0.9988897442817688]], "normalized_score": [0.3030660152435303], "tokens": [["\u2581Matthew", "\u2581Mc", "Con", "aug", "he", "y"]], "entropy": [[2.6209192276000977, 3.642082691192627, 0.22651560604572296, 0.17003613710403442, 0.039749763906002045, 0.010160110890865326]], "avg_entropy": [1.1182439227898915], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Guillermo del Toro", "score_of_biggest_cluster": 0.3030660152435303, "score_of_first_answer": 0.3030660152435303, "max_score_of_first_answer": 0.9988897442817688, "min_score_of_first_answer": 0.003079142654314637, "avg_score": 0.3030660152435303, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40590244020365845, "std_of_first_answer": 0.40590244020365845, "avg_entropy_of_first_answer": 1.1182439227898915, "lex_sim": 1.0}
{"question": "where was the war of the planet of the apes filmed", "answer": ["in Botswana"], "scores": [[0.018520772457122803, 0.0035385224036872387, 0.02817346528172493, 0.9953442215919495, 0.9961766004562378]], "normalized_score": [0.07120756804943085], "tokens": [["\u2581in", "\u2581B", "ots", "w", "ana"]], "entropy": [[5.627651691436768, 4.757084369659424, 4.153747081756592, 0.03634118288755417, 0.03143797442317009]], "avg_entropy": [2.9212524600327017], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Lower Mainland in Vancouver", "score_of_biggest_cluster": 0.07120756804943085, "score_of_first_answer": 0.07120756804943085, "max_score_of_first_answer": 0.9961766004562378, "min_score_of_first_answer": 0.0035385224036872387, "avg_score": 0.07120756804943085, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4796823292502011, "std_of_first_answer": 0.4796823292502011, "avg_entropy_of_first_answer": 2.9212524600327017, "lex_sim": 1.0}
{"question": "which financial statement involves all aspects of the accounting\u200b equation", "answer": ["balance sheet"], "scores": [[0.07223579287528992, 0.92159503698349]], "normalized_score": [0.25801581144332886], "tokens": [["\u2581balance", "\u2581sheet"]], "entropy": [[3.4938883781433105, 0.3819557726383209]], "avg_entropy": [1.9379220753908157], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "balance sheet", "score_of_biggest_cluster": 0.25801581144332886, "score_of_first_answer": 0.25801581144332886, "max_score_of_first_answer": 0.92159503698349, "min_score_of_first_answer": 0.07223579287528992, "avg_score": 0.25801581144332886, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.42467962205410004, "std_of_first_answer": 0.42467962205410004, "avg_entropy_of_first_answer": 1.9379220753908157, "lex_sim": 1.0}
{"question": "what are the roles of international monetary fund", "answer": ["maintaining an agreed system of exchange rates for its member states and extending international liquidity"], "scores": [[0.0033990880474448204, 0.5263106822967529, 0.007128422614187002, 0.003198703983798623, 0.0355386957526207, 0.992970883846283, 0.5870408415794373, 0.8754618167877197, 0.006213394459336996, 0.289208322763443, 0.4475715458393097, 0.12314586341381073, 0.11310658603906631, 0.010488751344382763, 0.09683547168970108, 0.32023486495018005, 0.9995239973068237]], "normalized_score": [0.09257563948631287], "tokens": [["\u2581maintain", "ing", "\u2581an", "\u2581agreed", "\u2581system", "\u2581of", "\u2581exchange", "\u2581rates", "\u2581for", "\u2581its", "\u2581member", "\u2581states", "\u2581and", "\u2581extending", "\u2581international", "\u2581liquid", "ity"]], "entropy": [[5.485092639923096, 2.0480148792266846, 2.690361976623535, 2.1923651695251465, 3.2897326946258545, 0.06446947157382965, 2.080629348754883, 0.7309412956237793, 1.6313197612762451, 3.129823923110962, 1.1070572137832642, 1.0990417003631592, 1.709961175918579, 4.690718173980713, 2.295912265777588, 2.006014347076416, 0.005988946184515953]], "avg_entropy": [2.1327908813734267], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "foster global monetary cooperation", "score_of_biggest_cluster": 0.09257563948631287, "score_of_first_answer": 0.09257563948631287, "max_score_of_first_answer": 0.9995239973068237, "min_score_of_first_answer": 0.003198703983798623, "avg_score": 0.09257563948631287, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34804511497683277, "std_of_first_answer": 0.34804511497683277, "avg_entropy_of_first_answer": 2.1327908813734267, "lex_sim": 1.0}
{"question": "when did somewhere over the rainbow come out", "answer": ["1939"], "scores": [[0.8068508505821228, 0.9917723536491394, 0.9946379065513611, 0.9105169177055359, 0.8301384449005127]], "normalized_score": [0.9033619165420532], "tokens": [["\u2581", "1", "9", "3", "9"]], "entropy": [[1.3340532779693604, 0.060963116586208344, 0.0468863882124424, 0.3836544156074524, 0.6495223641395569]], "avg_entropy": [0.4950159125030041], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1939", "score_of_biggest_cluster": 0.9033619165420532, "score_of_first_answer": 0.9033619165420532, "max_score_of_first_answer": 0.9946379065513611, "min_score_of_first_answer": 0.8068508505821228, "avg_score": 0.9033619165420532, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.07850661028667547, "std_of_first_answer": 0.07850661028667547, "avg_entropy_of_first_answer": 0.4950159125030041, "lex_sim": 1.0}
{"question": "when is season 8 for game of thrones", "answer": ["April 2019"], "scores": [[0.09532026201486588, 0.8133876323699951, 0.7603950500488281, 0.9401266574859619, 0.9551366567611694, 0.9485934376716614]], "normalized_score": [0.6074012517929077], "tokens": [["\u2581April", "\u2581", "2", "0", "1", "9"]], "entropy": [[3.153311014175415, 0.9121230840682983, 0.5963608622550964, 0.3418915569782257, 0.22609803080558777, 0.26629942655563354]], "avg_entropy": [0.9160139958063761], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2019", "score_of_biggest_cluster": 0.6074012517929077, "score_of_first_answer": 0.6074012517929077, "max_score_of_first_answer": 0.9551366567611694, "min_score_of_first_answer": 0.09532026201486588, "avg_score": 0.6074012517929077, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3028674526816418, "std_of_first_answer": 0.3028674526816418, "avg_entropy_of_first_answer": 0.9160139958063761, "lex_sim": 1.0}
{"question": "where does the last name roberts come from", "answer": ["Norman Walter Roberts and his wife were from Swansea in north Wales."], "scores": [[0.02973586693406105, 0.0036036528181284666, 0.27877768874168396, 0.0056509110145270824, 0.12370086461305618, 0.3214401304721832, 0.01620117574930191, 0.08308230340480804, 0.0022653231862932444, 0.8425063490867615, 0.9994475245475769, 0.05839155614376068, 0.00031988092814572155, 0.6698035001754761, 0.2775838375091553]], "normalized_score": [0.05229634419083595], "tokens": [["\u2581Norman", "\u2581Walter", "\u2581Roberts", "\u2581and", "\u2581his", "\u2581wife", "\u2581were", "\u2581from", "\u2581Sw", "anse", "a", "\u2581in", "\u2581north", "\u2581Wales", "."]], "entropy": [[5.238767623901367, 3.1271185874938965, 2.439403533935547, 0.8116104602813721, 5.216991424560547, 3.185781955718994, 4.762063026428223, 5.145617485046387, 3.9399776458740234, 0.9248149394989014, 0.007215932477265596, 1.1771583557128906, 1.2687597274780273, 1.451465368270874, 1.5688440799713135]], "avg_entropy": [2.6843726764433087], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Norman", "score_of_biggest_cluster": 0.05229634419083595, "score_of_first_answer": 0.05229634419083595, "max_score_of_first_answer": 0.9994475245475769, "min_score_of_first_answer": 0.00031988092814572155, "avg_score": 0.05229634419083595, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3186852779277255, "std_of_first_answer": 0.3186852779277255, "avg_entropy_of_first_answer": 2.6843726764433087, "lex_sim": 1.0}
{"question": "when does star trek discovery season 1 end", "answer": ["January 6, 2019"], "scores": [[0.21244339644908905, 0.9506644606590271, 0.1394486427307129, 0.3774087429046631, 0.993259608745575, 0.9797993302345276, 0.9955969452857971, 0.6843149065971375, 0.7883138060569763]], "normalized_score": [0.5615844130516052], "tokens": [["\u2581January", "\u2581", "6", ",", "\u2581", "2", "0", "1", "9"]], "entropy": [[3.201841115951538, 0.2500957250595093, 1.540744662284851, 0.9801938533782959, 0.04726646468043327, 0.10496090352535248, 0.037180423736572266, 0.8180922269821167, 0.5789054036140442]], "avg_entropy": [0.8399200865791904], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "February\u00a011,\u00a02018", "score_of_biggest_cluster": 0.5615844130516052, "score_of_first_answer": 0.5615844130516052, "max_score_of_first_answer": 0.9955969452857971, "min_score_of_first_answer": 0.1394486427307129, "avg_score": 0.5615844130516052, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32913837763558307, "std_of_first_answer": 0.32913837763558307, "avg_entropy_of_first_answer": 0.8399200865791904, "lex_sim": 1.0}
{"question": "who won latest america's next top model", "answer": ["Sophie Umbrella"], "scores": [[0.0655069649219513, 0.915628969669342, 0.11232206225395203, 0.02797059528529644, 0.39884597063064575]], "normalized_score": [0.14969106018543243], "tokens": [["\u2581Soph", "ie", "\u2581Um", "bre", "lla"]], "entropy": [[4.086573600769043, 0.31132957339286804, 3.3799428939819336, 3.404397964477539, 1.9089419841766357]], "avg_entropy": [2.618237203359604], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kyla Coleman", "score_of_biggest_cluster": 0.14969106018543243, "score_of_first_answer": 0.14969106018543243, "max_score_of_first_answer": 0.915628969669342, "min_score_of_first_answer": 0.02797059528529644, "avg_score": 0.14969106018543243, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3325349098886248, "std_of_first_answer": 0.3325349098886248, "avg_entropy_of_first_answer": 2.618237203359604, "lex_sim": 1.0}
{"question": "who invented the cornell method of note taking", "answer": ["Cornell University"], "scores": [[0.26400649547576904, 0.7195091843605042, 0.5774022936820984]], "normalized_score": [0.4786776304244995], "tokens": [["\u2581Corn", "ell", "\u2581University"]], "entropy": [[4.265408515930176, 0.6482613682746887, 2.11169171333313]], "avg_entropy": [2.3417871991793313], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Walter Pauk", "score_of_biggest_cluster": 0.4786776304244995, "score_of_first_answer": 0.4786776304244995, "max_score_of_first_answer": 0.7195091843605042, "min_score_of_first_answer": 0.26400649547576904, "avg_score": 0.4786776304244995, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.190290419649338, "std_of_first_answer": 0.190290419649338, "avg_entropy_of_first_answer": 2.3417871991793313, "lex_sim": 1.0}
{"question": "what nba team did derrick rose play for", "answer": ["Chicago Bulls"], "scores": [[0.8906269669532776, 0.9593245387077332, 0.9981973767280579]], "normalized_score": [0.948329508304596], "tokens": [["\u2581Chicago", "\u2581Bull", "s"]], "entropy": [[0.7723815441131592, 0.22298620641231537, 0.0160320233553648]], "avg_entropy": [0.33713325796027976], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Minnesota Timberwolves", "score_of_biggest_cluster": 0.948329508304596, "score_of_first_answer": 0.948329508304596, "max_score_of_first_answer": 0.9981973767280579, "min_score_of_first_answer": 0.8906269669532776, "avg_score": 0.948329508304596, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04447452074543745, "std_of_first_answer": 0.04447452074543745, "avg_entropy_of_first_answer": 0.33713325796027976, "lex_sim": 1.0}
{"question": "china in chinese is called zhongguo which means", "answer": ["Zhongguo"], "scores": [[0.011899529956281185, 0.9557487964630127, 0.4132698178291321, 0.9771423935890198]], "normalized_score": [0.26032525300979614], "tokens": [["\u2581Zh", "ong", "gu", "o"]], "entropy": [[4.332273483276367, 0.2566251754760742, 2.528463125228882, 0.17732813954353333]], "avg_entropy": [1.8236724808812141], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Central States", "score_of_biggest_cluster": 0.26032525300979614, "score_of_first_answer": 0.26032525300979614, "max_score_of_first_answer": 0.9771423935890198, "min_score_of_first_answer": 0.011899529956281185, "avg_score": 0.26032525300979614, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4028288052648807, "std_of_first_answer": 0.4028288052648807, "avg_entropy_of_first_answer": 1.8236724808812141, "lex_sim": 1.0}
{"question": "who has the most followers on the instagram", "answer": ["Selena Gomez"], "scores": [[0.23017822206020355, 0.9921581745147705, 0.9640831351280212, 0.9982672929763794, 0.9983433485031128]], "normalized_score": [0.7383415102958679], "tokens": [["\u2581Sel", "ena", "\u2581G", "ome", "z"]], "entropy": [[4.548612594604492, 0.08048606663942337, 0.26205700635910034, 0.018139030784368515, 0.016532881185412407]], "avg_entropy": [0.9851655159145594], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Instagram's own account", "score_of_biggest_cluster": 0.7383415102958679, "score_of_first_answer": 0.7383415102958679, "max_score_of_first_answer": 0.9983433485031128, "min_score_of_first_answer": 0.23017822206020355, "avg_score": 0.7383415102958679, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3034781350876292, "std_of_first_answer": 0.3034781350876292, "avg_entropy_of_first_answer": 0.9851655159145594, "lex_sim": 1.0}
{"question": "where is creatine phosphate found in the body", "answer": ["in myosites of the skeletal muscle"], "scores": [[0.05551769956946373, 0.009647597558796406, 0.5116182565689087, 0.03628995642066002, 0.013660888187587261, 0.1370510756969452, 0.2909415066242218, 0.9582549333572388, 0.9951756000518799, 0.7452492713928223, 0.4030945897102356]], "normalized_score": [0.15818573534488678], "tokens": [["\u2581in", "\u2581my", "os", "ites", "\u2581of", "\u2581the", "\u2581ske", "let", "al", "\u2581mus", "cle"]], "entropy": [[3.598811388015747, 2.170236825942993, 1.7645052671432495, 2.165860176086426, 1.1434341669082642, 2.2626264095306396, 2.85080623626709, 0.19227857887744904, 0.048882558941841125, 0.9261689782142639, 0.7405734062194824]], "avg_entropy": [1.6240167265588588], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "brain", "score_of_biggest_cluster": 0.15818573534488678, "score_of_first_answer": 0.15818573534488678, "max_score_of_first_answer": 0.9951756000518799, "min_score_of_first_answer": 0.009647597558796406, "avg_score": 0.15818573534488678, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3600226688847789, "std_of_first_answer": 0.3600226688847789, "avg_entropy_of_first_answer": 1.6240167265588588, "lex_sim": 1.0}
{"question": "how old was sasuke when his clan died", "answer": ["3"], "scores": [[0.6635265946388245, 0.062094446271657944]], "normalized_score": [0.20298108458518982], "tokens": [["\u2581", "3"]], "entropy": [[2.247776985168457, 1.9246268272399902]], "avg_entropy": [2.0862019062042236], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "seven", "score_of_biggest_cluster": 0.20298108458518982, "score_of_first_answer": 0.20298108458518982, "max_score_of_first_answer": 0.6635265946388245, "min_score_of_first_answer": 0.062094446271657944, "avg_score": 0.20298108458518982, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30071607418358326, "std_of_first_answer": 0.30071607418358326, "avg_entropy_of_first_answer": 2.0862019062042236, "lex_sim": 1.0}
{"question": "who changed the role of the first lady", "answer": ["Eleanor Roosevelt"], "scores": [[0.07339012622833252, 0.9970473647117615, 0.9943591952323914, 0.9641788005828857, 0.9963831901550293, 0.9995211362838745, 0.9986587762832642]], "normalized_score": [0.6836169362068176], "tokens": [["\u2581Ele", "an", "or", "\u2581Ro", "ose", "vel", "t"]], "entropy": [[3.0354065895080566, 0.02299482189118862, 0.038554321974515915, 0.291163831949234, 0.030203036963939667, 0.005518184043467045, 0.013794826343655586]], "avg_entropy": [0.4910908018105796], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dolley Madison", "score_of_biggest_cluster": 0.6836169362068176, "score_of_first_answer": 0.6836169362068176, "max_score_of_first_answer": 0.9995211362838745, "min_score_of_first_answer": 0.07339012622833252, "avg_score": 0.6836169362068176, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3215439714406697, "std_of_first_answer": 0.3215439714406697, "avg_entropy_of_first_answer": 0.4910908018105796, "lex_sim": 1.0}
{"question": "why was the plague that struck athens so devastating", "answer": ["the Athenians did not take the appropriate precautions"], "scores": [[0.026127666234970093, 0.02185823954641819, 0.9259013533592224, 0.8365453481674194, 0.2071584314107895, 0.9832181930541992, 0.0867752879858017, 0.1704297512769699, 0.010068914853036404, 0.5516661405563354, 0.998410701751709, 0.9715853929519653]], "normalized_score": [0.20957660675048828], "tokens": [["\u2581the", "\u2581A", "then", "ians", "\u2581did", "\u2581not", "\u2581take", "\u2581the", "\u2581appropriate", "\u2581pre", "ca", "utions"]], "entropy": [[5.0588579177856445, 5.046961784362793, 0.5025039911270142, 0.4567905068397522, 3.538289785385132, 0.13742050528526306, 4.163847923278809, 3.127307415008545, 3.8601627349853516, 1.905193567276001, 0.017736999318003654, 0.13492423295974731]], "avg_entropy": [2.329166446967671], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "close quarters and poor hygiene", "score_of_biggest_cluster": 0.20957660675048828, "score_of_first_answer": 0.20957660675048828, "max_score_of_first_answer": 0.998410701751709, "min_score_of_first_answer": 0.010068914853036404, "avg_score": 0.20957660675048828, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41395253722475517, "std_of_first_answer": 0.41395253722475517, "avg_entropy_of_first_answer": 2.329166446967671, "lex_sim": 1.0}
{"question": "who is rose in the fall season 2", "answer": ["Claire"], "scores": [[0.03359554335474968, 0.9361222386360168]], "normalized_score": [0.17734016478061676], "tokens": [["\u2581Cla", "ire"]], "entropy": [[5.5688676834106445, 0.2915326654911041]], "avg_entropy": [2.9302001744508743], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Valene Kane", "score_of_biggest_cluster": 0.17734016478061676, "score_of_first_answer": 0.17734016478061676, "max_score_of_first_answer": 0.9361222386360168, "min_score_of_first_answer": 0.03359554335474968, "avg_score": 0.17734016478061676, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4512633476406336, "std_of_first_answer": 0.4512633476406336, "avg_entropy_of_first_answer": 2.9302001744508743, "lex_sim": 1.0}
{"question": "who wrote and performed i can only imagine", "answer": ["Adam Capps"], "scores": [[0.005360991694033146, 0.36309686303138733, 0.004934367723762989]], "normalized_score": [0.021256890147924423], "tokens": [["\u2581Adam", "\u2581C", "apps"]], "entropy": [[3.6625380516052246, 2.2426609992980957, 3.1666131019592285]], "avg_entropy": [3.023937384287516], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bart Millard", "score_of_biggest_cluster": 0.021256890147924423, "score_of_first_answer": 0.021256890147924423, "max_score_of_first_answer": 0.36309686303138733, "min_score_of_first_answer": 0.004934367723762989, "avg_score": 0.021256890147924423, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.16873895311808712, "std_of_first_answer": 0.16873895311808712, "avg_entropy_of_first_answer": 3.023937384287516, "lex_sim": 1.0}
{"question": "who has the most all ireland hurling medals", "answer": ["Christy O'Connor Jnr"], "scores": [[0.04938032850623131, 0.9986259937286377, 0.07297036051750183, 0.48219409584999084, 0.9341963529586792, 0.7410181760787964, 0.09442620724439621, 0.9851179122924805]], "normalized_score": [0.3206428587436676], "tokens": [["\u2581Christ", "y", "\u2581O", "'", "Con", "nor", "\u2581J", "nr"]], "entropy": [[4.433239459991455, 0.01619705557823181, 0.8921076655387878, 1.6670680046081543, 0.3348401188850403, 0.8626012802124023, 1.9824135303497314, 0.10731811076402664]], "avg_entropy": [1.2869731532409787], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Henry Shefflin", "score_of_biggest_cluster": 0.3206428587436676, "score_of_first_answer": 0.3206428587436676, "max_score_of_first_answer": 0.9986259937286377, "min_score_of_first_answer": 0.04938032850623131, "avg_score": 0.3206428587436676, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39761615805061323, "std_of_first_answer": 0.39761615805061323, "avg_entropy_of_first_answer": 1.2869731532409787, "lex_sim": 1.0}
{"question": "when did the first wireless beats come out", "answer": ["2001"], "scores": [[0.7073306441307068, 0.6985009908676147, 0.9960667490959167, 0.5995907187461853, 0.014708240516483784]], "normalized_score": [0.33689820766448975], "tokens": [["\u2581", "2", "0", "0", "1"]], "entropy": [[1.7477195262908936, 0.6566778421401978, 0.03754977509379387, 0.6867756247520447, 1.7432457208633423]], "avg_entropy": [0.9743936978280544], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "October 2012", "score_of_biggest_cluster": 0.33689820766448975, "score_of_first_answer": 0.33689820766448975, "max_score_of_first_answer": 0.9960667490959167, "min_score_of_first_answer": 0.014708240516483784, "avg_score": 0.33689820766448975, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32267795789452763, "std_of_first_answer": 0.32267795789452763, "avg_entropy_of_first_answer": 0.9743936978280544, "lex_sim": 1.0}
{"question": "when was the last time anyone was on the moon", "answer": ["28th of July 1971"], "scores": [[0.42693209648132324, 0.0989728718996048, 0.005703763570636511, 0.23722581565380096, 0.15397751331329346, 0.8051163554191589, 0.5676049590110779, 0.9822991490364075, 0.9986310601234436, 0.8570907711982727, 0.33850857615470886]], "normalized_score": [0.2883310914039612], "tokens": [["\u2581", "2", "8", "th", "\u2581of", "\u2581July", "\u2581", "1", "9", "7", "1"]], "entropy": [[2.724614143371582, 0.6106209754943848, 0.49855607748031616, 2.0700836181640625, 2.012120246887207, 0.8473865389823914, 1.0265710353851318, 0.09648731350898743, 0.013923142105340958, 0.43410342931747437, 1.4551162719726562]], "avg_entropy": [1.0717802538790486], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "14 December 1972 UTC", "score_of_biggest_cluster": 0.2883310914039612, "score_of_first_answer": 0.2883310914039612, "max_score_of_first_answer": 0.9986310601234436, "min_score_of_first_answer": 0.005703763570636511, "avg_score": 0.2883310914039612, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34794284390201285, "std_of_first_answer": 0.34794284390201285, "avg_entropy_of_first_answer": 1.0717802538790486, "lex_sim": 1.0}
{"question": "who does the vocals for feel it still", "answer": ["Taylor Swift"], "scores": [[0.007787075359374285, 0.6386789083480835]], "normalized_score": [0.07052262872457504], "tokens": [["\u2581Taylor", "\u2581Swift"]], "entropy": [[6.098755836486816, 2.3989508152008057]], "avg_entropy": [4.248853325843811], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Portugal. The Man", "score_of_biggest_cluster": 0.07052262872457504, "score_of_first_answer": 0.07052262872457504, "max_score_of_first_answer": 0.6386789083480835, "min_score_of_first_answer": 0.007787075359374285, "avg_score": 0.07052262872457504, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3154459164943546, "std_of_first_answer": 0.3154459164943546, "avg_entropy_of_first_answer": 4.248853325843811, "lex_sim": 1.0}
{"question": "1 the bureau of land management manages over acres of land in the united states", "answer": ["750,000,000"], "scores": [[0.44096890091896057, 0.07230187207460403, 0.15284022688865662, 0.6766321659088135, 0.47477152943611145, 0.9137539863586426, 0.9842262864112854, 0.9954741597175598, 0.9449025988578796, 0.9956389665603638, 0.9984198808670044, 0.9985577464103699]], "normalized_score": [0.5753032565116882], "tokens": [["\u2581", "7", "5", "0", ",", "0", "0", "0", ",", "0", "0", "0"]], "entropy": [[2.6339380741119385, 1.7741855382919312, 1.8371206521987915, 1.3781278133392334, 1.4264569282531738, 0.49060529470443726, 0.11660012602806091, 0.04128019139170647, 0.25952184200286865, 0.04041888192296028, 0.015767384320497513, 0.014115260913968086]], "avg_entropy": [0.8356781656232973], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "247.3\u00a0million", "score_of_biggest_cluster": 0.5753032565116882, "score_of_first_answer": 0.5753032565116882, "max_score_of_first_answer": 0.9985577464103699, "min_score_of_first_answer": 0.07230187207460403, "avg_score": 0.5753032565116882, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3349086209024994, "std_of_first_answer": 0.3349086209024994, "avg_entropy_of_first_answer": 0.8356781656232973, "lex_sim": 1.0}
{"question": "what is the top speed of thrust ssc", "answer": ["Mach 5"], "scores": [[0.1867896467447281, 0.9471694231033325, 0.0726524069905281]], "normalized_score": [0.23424850404262543], "tokens": [["\u2581Mach", "\u2581", "5"]], "entropy": [[1.9156848192214966, 0.3994506597518921, 1.7340316772460938]], "avg_entropy": [1.3497223854064941], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1,228\u00a0km/h", "score_of_biggest_cluster": 0.23424850404262543, "score_of_first_answer": 0.23424850404262543, "max_score_of_first_answer": 0.9471694231033325, "min_score_of_first_answer": 0.0726524069905281, "avg_score": 0.23424850404262543, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3881558571938042, "std_of_first_answer": 0.3881558571938042, "avg_entropy_of_first_answer": 1.3497223854064941, "lex_sim": 1.0}
{"question": "who does eric end up with in gossip girl", "answer": ["Serena"], "scores": [[0.17164719104766846, 0.998629093170166]], "normalized_score": [0.4140191972255707], "tokens": [["\u2581Ser", "ena"]], "entropy": [[2.660244941711426, 0.013560418039560318]], "avg_entropy": [1.336902679875493], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jenny", "score_of_biggest_cluster": 0.4140191972255707, "score_of_first_answer": 0.4140191972255707, "max_score_of_first_answer": 0.998629093170166, "min_score_of_first_answer": 0.17164719104766846, "avg_score": 0.4140191972255707, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4134909510612488, "std_of_first_answer": 0.4134909510612488, "avg_entropy_of_first_answer": 1.336902679875493, "lex_sim": 1.0}
{"question": "who plays ivo on once upon a time", "answer": ["Jamie Dornan"], "scores": [[0.0312630869448185, 0.994942843914032, 0.97095787525177, 0.9905267953872681, 0.9994421601295471]], "normalized_score": [0.49559950828552246], "tokens": [["\u2581Jam", "ie", "\u2581D", "orn", "an"]], "entropy": [[4.416153907775879, 0.039001625031232834, 0.2123325765132904, 0.07115495204925537, 0.0061765797436237335]], "avg_entropy": [0.9489639282226563], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dan Payne", "score_of_biggest_cluster": 0.49559950828552246, "score_of_first_answer": 0.49559950828552246, "max_score_of_first_answer": 0.9994421601295471, "min_score_of_first_answer": 0.0312630869448185, "avg_score": 0.49559950828552246, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3832049766979846, "std_of_first_answer": 0.3832049766979846, "avg_entropy_of_first_answer": 0.9489639282226563, "lex_sim": 1.0}
{"question": "which is the lion's mane jellyfish's species within its genus", "answer": ["Cyanea capillata"], "scores": [[0.34231218695640564, 0.9952785968780518, 0.9649234414100647, 0.6583684682846069, 0.9909302592277527, 0.9595356583595276]], "normalized_score": [0.7683731317520142], "tokens": [["\u2581Cy", "ane", "a", "\u2581cap", "ill", "ata"]], "entropy": [[4.00775146484375, 0.04767649248242378, 0.2713559865951538, 1.5015531778335571, 0.0768488198518753, 0.27623865008354187]], "avg_entropy": [1.0302374319483836], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "C.\u00a0capillata", "score_of_biggest_cluster": 0.7683731317520142, "score_of_first_answer": 0.7683731317520142, "max_score_of_first_answer": 0.9952785968780518, "min_score_of_first_answer": 0.34231218695640564, "avg_score": 0.7683731317520142, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.24314284834934727, "std_of_first_answer": 0.24314284834934727, "avg_entropy_of_first_answer": 1.0302374319483836, "lex_sim": 1.0}
{"question": "when were the words under god added to the pledge", "answer": ["July"], "scores": [[0.011040833778679371]], "normalized_score": [0.01104083564132452], "tokens": [["\u2581July"]], "entropy": [[1.3259930610656738]], "avg_entropy": [1.3259930610656738], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "on Flag Day in 1954", "score_of_biggest_cluster": 0.01104083564132452, "score_of_first_answer": 0.01104083564132452, "max_score_of_first_answer": 0.011040833778679371, "min_score_of_first_answer": 0.011040833778679371, "avg_score": 0.01104083564132452, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.3259930610656738, "lex_sim": 1.0}
{"question": "what dynasty completed the great wall of china", "answer": ["Ming"], "scores": [[0.14125710725784302, 0.9852508306503296]], "normalized_score": [0.3730598986148834], "tokens": [["\u2581M", "ing"]], "entropy": [[2.681394338607788, 0.1497322916984558]], "avg_entropy": [1.415563315153122], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Qin", "score_of_biggest_cluster": 0.3730598986148834, "score_of_first_answer": 0.3730598986148834, "max_score_of_first_answer": 0.9852508306503296, "min_score_of_first_answer": 0.14125710725784302, "avg_score": 0.3730598986148834, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4219968616962433, "std_of_first_answer": 0.4219968616962433, "avg_entropy_of_first_answer": 1.415563315153122, "lex_sim": 1.0}
{"question": "when did the movie breakfast club come out", "answer": ["1985"], "scores": [[0.7204307317733765, 0.9920974969863892, 0.997118353843689, 0.9907536506652832, 0.9892588257789612]], "normalized_score": [0.9307515621185303], "tokens": [["\u2581", "1", "9", "8", "5"]], "entropy": [[1.1841157674789429, 0.061183735728263855, 0.027219204232096672, 0.061476174741983414, 0.07769066840410233]], "avg_entropy": [0.28233711011707785], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "February 15, 1985", "score_of_biggest_cluster": 0.9307515621185303, "score_of_first_answer": 0.9307515621185303, "max_score_of_first_answer": 0.997118353843689, "min_score_of_first_answer": 0.7204307317733765, "avg_score": 0.9307515621185303, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.10878262457640421, "std_of_first_answer": 0.10878262457640421, "avg_entropy_of_first_answer": 0.28233711011707785, "lex_sim": 1.0}
{"question": "who defeated the last remaining roman army in europe", "answer": ["Alaric"], "scores": [[0.08200611919164658, 0.9818779230117798, 0.9868150949478149]], "normalized_score": [0.429912269115448], "tokens": [["\u2581Al", "ar", "ic"]], "entropy": [[4.442922115325928, 0.1327875256538391, 0.09476034343242645]], "avg_entropy": [1.5568233281373978], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Goths", "score_of_biggest_cluster": 0.429912269115448, "score_of_first_answer": 0.429912269115448, "max_score_of_first_answer": 0.9868150949478149, "min_score_of_first_answer": 0.08200611919164658, "avg_score": 0.429912269115448, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.42537211442664496, "std_of_first_answer": 0.42537211442664496, "avg_entropy_of_first_answer": 1.5568233281373978, "lex_sim": 1.0}
{"question": "who won mark and goal of the year 2017", "answer": ["Kevin De Bruyne"], "scores": [[0.02993660420179367, 0.48202773928642273, 0.9835804104804993, 0.9991859793663025, 0.9998764991760254]], "normalized_score": [0.42690908908843994], "tokens": [["\u2581Kevin", "\u2581De", "\u2581Bru", "y", "ne"]], "entropy": [[4.613956451416016, 2.579535961151123, 0.10909008234739304, 0.007778911851346493, 0.001663852483034134]], "avg_entropy": [1.4624050518497824], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joe Daniher", "score_of_biggest_cluster": 0.42690908908843994, "score_of_first_answer": 0.42690908908843994, "max_score_of_first_answer": 0.9998764991760254, "min_score_of_first_answer": 0.02993660420179367, "avg_score": 0.42690908908843994, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38893368551893376, "std_of_first_answer": 0.38893368551893376, "avg_entropy_of_first_answer": 1.4624050518497824, "lex_sim": 1.0}
{"question": "what is the highest scoring letter in scrabble", "answer": ["Z"], "scores": [[0.42976289987564087]], "normalized_score": [0.42976289987564087], "tokens": [["\u2581Z"]], "entropy": [[1.8124696016311646]], "avg_entropy": [1.8124696016311646], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Q", "score_of_biggest_cluster": 0.42976289987564087, "score_of_first_answer": 0.42976289987564087, "max_score_of_first_answer": 0.42976289987564087, "min_score_of_first_answer": 0.42976289987564087, "avg_score": 0.42976289987564087, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.8124696016311646, "lex_sim": 1.0}
{"question": "when was kudzu introduced to the united states", "answer": ["1876"], "scores": [[0.8262204527854919, 0.9947324991226196, 0.7569149136543274, 0.9357534050941467, 0.9469689726829529]], "normalized_score": [0.8877062797546387], "tokens": [["\u2581", "1", "8", "7", "6"]], "entropy": [[1.2891627550125122, 0.04127714782953262, 0.6539855003356934, 0.3534472584724426, 0.30256104469299316]], "avg_entropy": [0.5280867412686348], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1876", "score_of_biggest_cluster": 0.8877062797546387, "score_of_first_answer": 0.8877062797546387, "max_score_of_first_answer": 0.9947324991226196, "min_score_of_first_answer": 0.7569149136543274, "avg_score": 0.8877062797546387, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08725224867985272, "std_of_first_answer": 0.08725224867985272, "avg_entropy_of_first_answer": 0.5280867412686348, "lex_sim": 1.0}
{"question": "what was the result of the revolt of 1857", "answer": ["British victory"], "scores": [[0.15472480654716492, 0.24513860046863556]], "normalized_score": [0.1947537511587143], "tokens": [["\u2581British", "\u2581victory"]], "entropy": [[4.763566970825195, 3.4467930793762207]], "avg_entropy": [4.105180025100708], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "British victory", "score_of_biggest_cluster": 0.1947537511587143, "score_of_first_answer": 0.1947537511587143, "max_score_of_first_answer": 0.24513860046863556, "min_score_of_first_answer": 0.15472480654716492, "avg_score": 0.1947537511587143, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.04520689696073532, "std_of_first_answer": 0.04520689696073532, "avg_entropy_of_first_answer": 4.105180025100708, "lex_sim": 1.0}
{"question": "who played the face in the a team", "answer": ["Dennis Waterman"], "scores": [[0.01873210072517395, 0.7540892362594604, 0.9994218349456787]], "normalized_score": [0.2416866570711136], "tokens": [["\u2581Dennis", "\u2581Wat", "erman"]], "entropy": [[4.9816484451293945, 1.3734517097473145, 0.00579393096268177]], "avg_entropy": [2.12029802861313], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dirk Benedict", "score_of_biggest_cluster": 0.2416866570711136, "score_of_first_answer": 0.2416866570711136, "max_score_of_first_answer": 0.9994218349456787, "min_score_of_first_answer": 0.01873210072517395, "avg_score": 0.2416866570711136, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41669207330071134, "std_of_first_answer": 0.41669207330071134, "avg_entropy_of_first_answer": 2.12029802861313, "lex_sim": 1.0}
{"question": "what land mass was north america a part of about 300 million years ago", "answer": ["the Paleo Arc"], "scores": [[0.020281795412302017, 0.000952521397266537, 0.5393328666687012, 0.0017369561828672886]], "normalized_score": [0.011598631739616394], "tokens": [["\u2581the", "\u2581Pale", "o", "\u2581Arc"]], "entropy": [[3.7341504096984863, 4.3839287757873535, 2.0948173999786377, 2.7054145336151123]], "avg_entropy": [3.2295777797698975], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pangaea", "score_of_biggest_cluster": 0.011598631739616394, "score_of_first_answer": 0.011598631739616394, "max_score_of_first_answer": 0.5393328666687012, "min_score_of_first_answer": 0.000952521397266537, "avg_score": 0.011598631739616394, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2303523009642003, "std_of_first_answer": 0.2303523009642003, "avg_entropy_of_first_answer": 3.2295777797698975, "lex_sim": 1.0}
{"question": "when did the united states host the world cup", "answer": ["1994"], "scores": [[0.758786678314209, 0.9043281674385071, 0.9977803826332092, 0.982018232345581, 0.9911355972290039]], "normalized_score": [0.922033429145813], "tokens": [["\u2581", "1", "9", "9", "4"]], "entropy": [[1.5503836870193481, 0.37686067819595337, 0.022302202880382538, 0.12384652346372604, 0.06730330735445023]], "avg_entropy": [0.42813927978277205], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1994", "score_of_biggest_cluster": 0.922033429145813, "score_of_first_answer": 0.922033429145813, "max_score_of_first_answer": 0.9977803826332092, "min_score_of_first_answer": 0.758786678314209, "avg_score": 0.922033429145813, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.09050944261696472, "std_of_first_answer": 0.09050944261696472, "avg_entropy_of_first_answer": 0.42813927978277205, "lex_sim": 1.0}
{"question": "number of degree of freedom for plane mechanism", "answer": ["3 degree of freedom"], "scores": [[0.670628011226654, 0.24528956413269043, 0.0024256675969809294, 0.8651132583618164, 0.964953601360321]], "normalized_score": [0.20161105692386627], "tokens": [["\u2581", "3", "\u2581degree", "\u2581of", "\u2581freedom"]], "entropy": [[1.7586629390716553, 1.8402245044708252, 0.8169952630996704, 0.6663743257522583, 0.2200811207294464]], "avg_entropy": [1.0604676306247711], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "six", "score_of_biggest_cluster": 0.20161105692386627, "score_of_first_answer": 0.20161105692386627, "max_score_of_first_answer": 0.964953601360321, "min_score_of_first_answer": 0.0024256675969809294, "avg_score": 0.20161105692386627, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36843440872105254, "std_of_first_answer": 0.36843440872105254, "avg_entropy_of_first_answer": 1.0604676306247711, "lex_sim": 1.0}
{"question": "what type of government is in north america", "answer": ["democracy"], "scores": [[0.058006756007671356, 0.547349750995636]], "normalized_score": [0.17818523943424225], "tokens": [["\u2581dem", "ocracy"]], "entropy": [[4.420180320739746, 0.8199885487556458]], "avg_entropy": [2.620084434747696], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Federal presidential constitutional republic", "score_of_biggest_cluster": 0.17818523943424225, "score_of_first_answer": 0.17818523943424225, "max_score_of_first_answer": 0.547349750995636, "min_score_of_first_answer": 0.058006756007671356, "avg_score": 0.17818523943424225, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24467149749398232, "std_of_first_answer": 0.24467149749398232, "avg_entropy_of_first_answer": 2.620084434747696, "lex_sim": 1.0}
{"question": "what breed of cat has spots and stripes", "answer": ["Tabby"], "scores": [[0.2523249089717865, 0.9976771473884583]], "normalized_score": [0.5017358064651489], "tokens": [["\u2581Tab", "by"]], "entropy": [[3.576821804046631, 0.025545619428157806]], "avg_entropy": [1.8011837117373943], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "tabby", "score_of_biggest_cluster": 0.5017358064651489, "score_of_first_answer": 0.5017358064651489, "max_score_of_first_answer": 0.9976771473884583, "min_score_of_first_answer": 0.2523249089717865, "avg_score": 0.5017358064651489, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3726761192083359, "std_of_first_answer": 0.3726761192083359, "avg_entropy_of_first_answer": 1.8011837117373943, "lex_sim": 1.0}
{"question": "who plays the main character in hacksaw ridge", "answer": ["Andrew Garfield You can use the natural language processing library (https://nlp.pytorch.org), which is a lot easier to use than gensim."], "scores": [[0.8989613056182861, 0.9494794011116028, 0.9911200404167175, 7.183441539382329e-07, 0.14835411310195923, 0.04320298135280609, 0.2752838730812073, 0.006700089666992426, 0.9248598217964172, 0.10772974789142609, 0.008173423819243908, 0.029477406293153763, 0.13235719501972198, 0.996640682220459, 0.1292194426059723, 0.9739463925361633, 0.948125958442688, 0.08881457895040512, 0.8725825548171997, 0.9999479055404663, 0.994280219078064, 0.9987847208976746, 0.011593669652938843, 0.17225812375545502, 0.3125265836715698, 0.1757262498140335, 0.007046243641525507, 0.3831004500389099, 0.6975398063659668, 0.6546949148178101, 0.39380359649658203, 0.02069673500955105, 0.9937505125999451, 0.5801838636398315]], "normalized_score": [0.15138640999794006], "tokens": [["\u2581Andrew", "\u2581Gar", "field", "\u2581You", "\u2581can", "\u2581use", "\u2581the", "\u2581natural", "\u2581language", "\u2581processing", "\u2581library", "\u2581(", "https", "://", "nl", "p", ".", "py", "tor", "ch", ".", "org", "),", "\u2581which", "\u2581is", "\u2581a", "\u2581lot", "\u2581easier", "\u2581to", "\u2581use", "\u2581than", "\u2581gens", "im", "."]], "entropy": [[0.76927649974823, 0.46675094962120056, 0.07623611390590668, 0.13434870541095734, 3.6965198516845703, 4.429224967956543, 4.16111946105957, 5.880119323730469, 0.6284921765327454, 4.718935489654541, 4.525326728820801, 3.542646884918213, 3.6250171661376953, 0.03375311195850372, 2.961662769317627, 0.18343468010425568, 0.34579694271087646, 3.258390426635742, 0.9646095633506775, 0.0006817145040258765, 0.06043586879968643, 0.011177226901054382, 1.483877420425415, 3.995973825454712, 3.33085560798645, 4.188324928283691, 4.820904731750488, 1.841153621673584, 1.274498462677002, 1.5938856601715088, 2.040836811065674, 4.084102153778076, 0.06290534138679504, 1.9966802597045898]], "avg_entropy": [2.2114104543477024], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Andrew Garfield", "score_of_biggest_cluster": 0.15138640999794006, "score_of_first_answer": 0.15138640999794006, "max_score_of_first_answer": 0.9999479055404663, "min_score_of_first_answer": 7.183441539382329e-07, "avg_score": 0.15138640999794006, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40346875359773526, "std_of_first_answer": 0.40346875359773526, "avg_entropy_of_first_answer": 2.2114104543477024, "lex_sim": 1.0}
{"question": "who won the gold for the men's figure skating", "answer": ["Ondrej Hotarek"], "scores": [[0.008539985865354538, 0.2223554253578186, 0.9595786929130554, 0.9986099004745483, 0.9182022213935852, 0.9968166947364807, 0.9988879561424255]], "normalized_score": [0.4008735716342926], "tokens": [["\u2581O", "nd", "re", "j", "\u2581Hot", "are", "k"]], "entropy": [[2.0211548805236816, 2.2557997703552246, 0.2254590094089508, 0.01614534482359886, 0.5296988487243652, 0.032807543873786926, 0.012119155377149582]], "avg_entropy": [0.7275977932981083], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yuzuru Hanyu", "score_of_biggest_cluster": 0.4008735716342926, "score_of_first_answer": 0.4008735716342926, "max_score_of_first_answer": 0.9988879561424255, "min_score_of_first_answer": 0.008539985865354538, "avg_score": 0.4008735716342926, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39315030939820417, "std_of_first_answer": 0.39315030939820417, "avg_entropy_of_first_answer": 0.7275977932981083, "lex_sim": 1.0}
{"question": "when was the day that the pubs ran dry in the poem 9-3", "answer": ["11 November 1928"], "scores": [[0.45693209767341614, 0.7362000346183777, 0.02576761692762375, 0.01989627815783024, 0.7310336232185364, 0.9550106525421143, 0.8848700523376465, 0.17400091886520386, 0.07833123207092285]], "normalized_score": [0.22456078231334686], "tokens": [["\u2581", "1", "1", "\u2581November", "\u2581", "1", "9", "2", "8"]], "entropy": [[2.892043113708496, 1.011370301246643, 1.363081932067871, 2.9573373794555664, 0.8437886238098145, 0.2021293044090271, 0.5054357051849365, 1.749280333518982, 1.7986202239990234]], "avg_entropy": [1.4803429908222623], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "31 October 1972", "score_of_biggest_cluster": 0.22456078231334686, "score_of_first_answer": 0.22456078231334686, "max_score_of_first_answer": 0.9550106525421143, "min_score_of_first_answer": 0.01989627815783024, "avg_score": 0.22456078231334686, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36275879826284785, "std_of_first_answer": 0.36275879826284785, "avg_entropy_of_first_answer": 1.4803429908222623, "lex_sim": 1.0}
{"question": "what is the setting of a dolls house", "answer": ["home interior of 19th century couple"], "scores": [[0.011800351552665234, 0.000822991190943867, 0.03243080899119377, 0.011157788336277008, 0.8967783451080322, 0.7846853137016296, 0.9074481129646301, 0.8255162239074707, 0.0013412137050181627]], "normalized_score": [0.05135512351989746], "tokens": [["\u2581home", "\u2581interior", "\u2581of", "\u2581", "1", "9", "th", "\u2581century", "\u2581couple"]], "entropy": [[4.516134262084961, 0.3509955108165741, 0.7255010604858398, 2.2559351921081543, 0.49411600828170776, 0.6562989950180054, 0.438224196434021, 0.7327624559402466, 3.192775249481201]], "avg_entropy": [1.4847492145167456], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "a Norwegian town circa 1879", "score_of_biggest_cluster": 0.05135512351989746, "score_of_first_answer": 0.05135512351989746, "max_score_of_first_answer": 0.9074481129646301, "min_score_of_first_answer": 0.000822991190943867, "avg_score": 0.05135512351989746, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4198933241889049, "std_of_first_answer": 0.4198933241889049, "avg_entropy_of_first_answer": 1.4847492145167456, "lex_sim": 1.0}
{"question": "who sings so come and dance with me jai ho", "answer": ["Salman Khan"], "scores": [[0.1497715562582016, 0.6998475790023804, 0.8682002425193787]], "normalized_score": [0.449798047542572], "tokens": [["\u2581Sal", "man", "\u2581Khan"]], "entropy": [[4.676545143127441, 0.9615399837493896, 0.7688215970993042]], "avg_entropy": [2.135635574658712], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mahalaxmi Iyer", "score_of_biggest_cluster": 0.449798047542572, "score_of_first_answer": 0.449798047542572, "max_score_of_first_answer": 0.8682002425193787, "min_score_of_first_answer": 0.1497715562582016, "avg_score": 0.449798047542572, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3067873000692975, "std_of_first_answer": 0.3067873000692975, "avg_entropy_of_first_answer": 2.135635574658712, "lex_sim": 1.0}
{"question": "what is the most common blood type in sweden", "answer": ["type o"], "scores": [[0.03093121200799942, 0.13795660436153412]], "normalized_score": [0.06532353907823563], "tokens": [["\u2581type", "\u2581o"]], "entropy": [[2.3651013374328613, 1.582068920135498]], "avg_entropy": [1.9735851287841797], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A+", "score_of_biggest_cluster": 0.06532353907823563, "score_of_first_answer": 0.06532353907823563, "max_score_of_first_answer": 0.13795660436153412, "min_score_of_first_answer": 0.03093121200799942, "avg_score": 0.06532353907823563, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05351269617676735, "std_of_first_answer": 0.05351269617676735, "avg_entropy_of_first_answer": 1.9735851287841797, "lex_sim": 1.0}
{"question": "the first element on the periodic table is", "answer": ["Hydrogen"], "scores": [[0.12403612583875656, 0.9763518571853638, 0.9993212223052979]], "normalized_score": [0.4946369230747223], "tokens": [["\u2581H", "ydro", "gen"]], "entropy": [[1.6980177164077759, 0.1632067859172821, 0.007476127706468105]], "avg_entropy": [0.6229002100105087], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hydro\u00adgen", "score_of_biggest_cluster": 0.4946369230747223, "score_of_first_answer": 0.4946369230747223, "max_score_of_first_answer": 0.9993212223052979, "min_score_of_first_answer": 0.12403612583875656, "avg_score": 0.4946369230747223, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.40730737747790496, "std_of_first_answer": 0.40730737747790496, "avg_entropy_of_first_answer": 0.6229002100105087, "lex_sim": 1.0}
{"question": "where was the original flight of the phoenix filmed", "answer": ["Trepassey Harbour, Newfoundland, Canada"], "scores": [[0.0058244215324521065, 0.002082805149257183, 0.6971908807754517, 0.9990249872207642, 0.3765038847923279, 0.7639710307121277, 0.7796967625617981, 0.9008719325065613, 0.9945605397224426, 0.9991125464439392, 0.33575600385665894, 0.9764478802680969]], "normalized_score": [0.3011378049850464], "tokens": [["\u2581T", "rep", "asse", "y", "\u2581Har", "bour", ",", "\u2581New", "found", "land", ",", "\u2581Canada"]], "entropy": [[5.527054786682129, 2.7353274822235107, 1.6662017107009888, 0.011871637776494026, 1.5023369789123535, 0.5644503831863403, 0.885240912437439, 0.7383612394332886, 0.044377006590366364, 0.010173346847295761, 1.262791395187378, 0.20899474620819092]], "avg_entropy": [1.2630984688488145], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "20th Century-Fox Studios", "score_of_biggest_cluster": 0.3011378049850464, "score_of_first_answer": 0.3011378049850464, "max_score_of_first_answer": 0.9991125464439392, "min_score_of_first_answer": 0.002082805149257183, "avg_score": 0.3011378049850464, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36196115352533964, "std_of_first_answer": 0.36196115352533964, "avg_entropy_of_first_answer": 1.2630984688488145, "lex_sim": 1.0}
{"question": "who are the co hosts on the real", "answer": ["Tamar Braxton"], "scores": [[0.3230573832988739, 0.9917042255401611, 0.9752289056777954, 0.9998394250869751, 0.9995138645172119]], "normalized_score": [0.7923142910003662], "tokens": [["\u2581T", "amar", "\u2581Bra", "xt", "on"]], "entropy": [[2.445406675338745, 0.07285118103027344, 0.1656869500875473, 0.00185672496445477, 0.005191786680370569]], "avg_entropy": [0.5381986636202782], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tamar Braxton", "score_of_biggest_cluster": 0.7923142910003662, "score_of_first_answer": 0.7923142910003662, "max_score_of_first_answer": 0.9998394250869751, "min_score_of_first_answer": 0.3230573832988739, "avg_score": 0.7923142910003662, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.26755468587103787, "std_of_first_answer": 0.26755468587103787, "avg_entropy_of_first_answer": 0.5381986636202782, "lex_sim": 1.0}
{"question": "how old is baby in dirty dancing movie", "answer": ["Eleven"], "scores": [[0.026649560779333115, 0.9536386728286743]], "normalized_score": [0.15941786766052246], "tokens": [["\u2581Ele", "ven"]], "entropy": [[2.523294448852539, 0.20211906731128693]], "avg_entropy": [1.362706758081913], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "17-year-old", "score_of_biggest_cluster": 0.15941786766052246, "score_of_first_answer": 0.15941786766052246, "max_score_of_first_answer": 0.9536386728286743, "min_score_of_first_answer": 0.026649560779333115, "avg_score": 0.15941786766052246, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4634945560246706, "std_of_first_answer": 0.4634945560246706, "avg_entropy_of_first_answer": 1.362706758081913, "lex_sim": 1.0}
{"question": "who has won the cma entertainer of the year the most", "answer": ["Glen Campbell"], "scores": [[0.025843102484941483, 0.9475635886192322]], "normalized_score": [0.1564863622188568], "tokens": [["\u2581Glen", "\u2581Campbell"]], "entropy": [[2.689190149307251, 0.3970755934715271]], "avg_entropy": [1.543132871389389], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Garth Brooks", "score_of_biggest_cluster": 0.1564863622188568, "score_of_first_answer": 0.1564863622188568, "max_score_of_first_answer": 0.9475635886192322, "min_score_of_first_answer": 0.025843102484941483, "avg_score": 0.1564863622188568, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46086024306714535, "std_of_first_answer": 0.46086024306714535, "avg_entropy_of_first_answer": 1.543132871389389, "lex_sim": 1.0}
{"question": "who was the father of the princes in the tower", "answer": ["Edward V"], "scores": [[0.5016729831695557, 0.15106701850891113]], "normalized_score": [0.2752930223941803], "tokens": [["\u2581Edward", "\u2581V"]], "entropy": [[2.182556390762329, 1.5552711486816406]], "avg_entropy": [1.8689137697219849], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Edward IV of England", "score_of_biggest_cluster": 0.2752930223941803, "score_of_first_answer": 0.2752930223941803, "max_score_of_first_answer": 0.5016729831695557, "min_score_of_first_answer": 0.15106701850891113, "avg_score": 0.2752930223941803, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17530298233032227, "std_of_first_answer": 0.17530298233032227, "avg_entropy_of_first_answer": 1.8689137697219849, "lex_sim": 1.0}
{"question": "what is the first line of an http request to the server", "answer": ["GET /"], "scores": [[0.5069822072982788, 0.625637412071228]], "normalized_score": [0.5631936192512512], "tokens": [["\u2581GET", "\u2581/"]], "entropy": [[2.7385342121124268, 1.5270342826843262]], "avg_entropy": [2.1327842473983765], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A request line", "score_of_biggest_cluster": 0.5631936192512512, "score_of_first_answer": 0.5631936192512512, "max_score_of_first_answer": 0.625637412071228, "min_score_of_first_answer": 0.5069822072982788, "avg_score": 0.5631936192512512, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05932760238647461, "std_of_first_answer": 0.05932760238647461, "avg_entropy_of_first_answer": 2.1327842473983765, "lex_sim": 1.0}
{"question": "when did 10 shilling note go out of circulation", "answer": ["June 1, 1985"], "scores": [[0.01595301739871502, 0.9673742651939392, 0.34969592094421387, 0.18968155980110168, 0.9824380278587341, 0.9310084581375122, 0.9554550647735596, 0.23991811275482178, 0.07317700237035751]], "normalized_score": [0.2925693690776825], "tokens": [["\u2581June", "\u2581", "1", ",", "\u2581", "1", "9", "8", "5"]], "entropy": [[1.8559486865997314, 0.1864987313747406, 1.3781074285507202, 2.155259609222412, 0.09729037433862686, 0.25258341431617737, 0.22319811582565308, 1.84169602394104, 1.7240911722183228]], "avg_entropy": [1.079408172931936], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "22 November 1970", "score_of_biggest_cluster": 0.2925693690776825, "score_of_first_answer": 0.2925693690776825, "max_score_of_first_answer": 0.9824380278587341, "min_score_of_first_answer": 0.01595301739871502, "avg_score": 0.2925693690776825, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40038144312140356, "std_of_first_answer": 0.40038144312140356, "avg_entropy_of_first_answer": 1.079408172931936, "lex_sim": 1.0}
{"question": "which material is the heaviest in term of density", "answer": ["gold"], "scores": [[0.05773364380002022]], "normalized_score": [0.057733647525310516], "tokens": [["\u2581gold"]], "entropy": [[4.553862571716309]], "avg_entropy": [4.553862571716309], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Osmium", "score_of_biggest_cluster": 0.057733647525310516, "score_of_first_answer": 0.057733647525310516, "max_score_of_first_answer": 0.05773364380002022, "min_score_of_first_answer": 0.05773364380002022, "avg_score": 0.057733647525310516, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.553862571716309, "lex_sim": 1.0}
{"question": "puella magi madoka magica when does madoka become a magical girl", "answer": ["Episode 5"], "scores": [[0.061668507754802704, 0.9924196600914001, 0.9587551355361938, 0.11060919612646103]], "normalized_score": [0.2838340401649475], "tokens": [["\u2581E", "pisode", "\u2581", "5"]], "entropy": [[4.46790075302124, 0.07112182676792145, 0.3249160349369049, 2.0653932094573975]], "avg_entropy": [1.732332956045866], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "My Very Best Friend", "score_of_biggest_cluster": 0.2838340401649475, "score_of_first_answer": 0.2838340401649475, "max_score_of_first_answer": 0.9924196600914001, "min_score_of_first_answer": 0.061668507754802704, "avg_score": 0.2838340401649475, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44521987864194634, "std_of_first_answer": 0.44521987864194634, "avg_entropy_of_first_answer": 1.732332956045866, "lex_sim": 1.0}
{"question": "when did god save the queen became the national anthem", "answer": ["In 2013"], "scores": [[0.010382115840911865, 0.8947179317474365, 0.021865038201212883, 0.9965797066688538, 0.3295590877532959, 0.06590311974287033]], "normalized_score": [0.12799087166786194], "tokens": [["\u2581In", "\u2581", "2", "0", "1", "3"]], "entropy": [[2.1287009716033936, 0.6674467921257019, 0.11695807427167892, 0.03262259438633919, 0.6694782376289368, 2.04392409324646]], "avg_entropy": [0.9431884605437517], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in the 1780s and 1790s", "score_of_biggest_cluster": 0.12799087166786194, "score_of_first_answer": 0.12799087166786194, "max_score_of_first_answer": 0.9965797066688538, "min_score_of_first_answer": 0.010382115840911865, "avg_score": 0.12799087166786194, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4104734380157936, "std_of_first_answer": 0.4104734380157936, "avg_entropy_of_first_answer": 0.9431884605437517, "lex_sim": 1.0}
{"question": "who become ceo of wipro company in 2016", "answer": ["Abidali (A.S.) Zain"], "scores": [[0.5008323192596436, 0.9976805448532104, 0.9114552736282349, 0.004488981328904629, 0.19473735988140106, 0.2592414319515228, 0.03840028867125511, 0.800110936164856, 0.42366746068000793, 0.1925027221441269]], "normalized_score": [0.21941520273685455], "tokens": [["\u2581Ab", "id", "ali", "\u2581(", "A", ".", "S", ".)", "\u2581Z", "ain"]], "entropy": [[2.0198862552642822, 0.022224895656108856, 0.5741816759109497, 1.1599775552749634, 2.798586368560791, 2.7150373458862305, 3.0603444576263428, 0.7255612015724182, 1.8456436395645142, 3.438101291656494]], "avg_entropy": [1.8359544686973095], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Abid Ali Neemuchwala", "score_of_biggest_cluster": 0.21941520273685455, "score_of_first_answer": 0.21941520273685455, "max_score_of_first_answer": 0.9976805448532104, "min_score_of_first_answer": 0.004488981328904629, "avg_score": 0.21941520273685455, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.34225257929664255, "std_of_first_answer": 0.34225257929664255, "avg_entropy_of_first_answer": 1.8359544686973095, "lex_sim": 1.0}
{"question": "german word for pleasure from someone else's pain", "answer": ["frevel"], "scores": [[0.005463375709950924, 0.007347153499722481]], "normalized_score": [0.006335633806884289], "tokens": [["\u2581fre", "vel"]], "entropy": [[2.906644821166992, 0.6110400557518005]], "avg_entropy": [1.7588424384593964], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Schadenfreude", "score_of_biggest_cluster": 0.006335633806884289, "score_of_first_answer": 0.006335633806884289, "max_score_of_first_answer": 0.007347153499722481, "min_score_of_first_answer": 0.005463375709950924, "avg_score": 0.006335633806884289, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0009418888948857784, "std_of_first_answer": 0.0009418888948857784, "avg_entropy_of_first_answer": 1.7588424384593964, "lex_sim": 1.0}
{"question": "how many writes does a flash drive have", "answer": ["32"], "scores": [[0.6852288842201233, 0.054193705320358276, 0.7462642192840576]], "normalized_score": [0.30261632800102234], "tokens": [["\u2581", "3", "2"]], "entropy": [[2.3595223426818848, 1.8015083074569702, 1.0989874601364136]], "avg_entropy": [1.753339370091756], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "up to 100,000", "score_of_biggest_cluster": 0.30261632800102234, "score_of_first_answer": 0.30261632800102234, "max_score_of_first_answer": 0.7462642192840576, "min_score_of_first_answer": 0.054193705320358276, "avg_score": 0.30261632800102234, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31285287728568806, "std_of_first_answer": 0.31285287728568806, "avg_entropy_of_first_answer": 1.753339370091756, "lex_sim": 1.0}
{"question": "what year did the us hockey team won the olympics", "answer": ["1980"], "scores": [[0.9010478854179382, 0.9170164465904236, 0.9930083751678467, 0.7043272256851196, 0.9958673715591431]], "normalized_score": [0.8953869938850403], "tokens": [["\u2581", "1", "9", "8", "0"]], "entropy": [[0.7869035005569458, 0.3200339674949646, 0.05689072236418724, 0.7504873871803284, 0.0342145711183548]], "avg_entropy": [0.38970602974295615], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1960", "score_of_biggest_cluster": 0.8953869938850403, "score_of_first_answer": 0.8953869938850403, "max_score_of_first_answer": 0.9958673715591431, "min_score_of_first_answer": 0.7043272256851196, "avg_score": 0.8953869938850403, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10620189497154651, "std_of_first_answer": 0.10620189497154651, "avg_entropy_of_first_answer": 0.38970602974295615, "lex_sim": 1.0}
{"question": "where is the food stored in a yam plant", "answer": ["Tuber"], "scores": [[0.024118317291140556, 0.4903344213962555]], "normalized_score": [0.10874760150909424], "tokens": [["\u2581T", "uber"]], "entropy": [[4.396414279937744, 1.0591089725494385]], "avg_entropy": [2.7277616262435913], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "an edible tuber", "score_of_biggest_cluster": 0.10874760150909424, "score_of_first_answer": 0.10874760150909424, "max_score_of_first_answer": 0.4903344213962555, "min_score_of_first_answer": 0.024118317291140556, "avg_score": 0.10874760150909424, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23310805205255747, "std_of_first_answer": 0.23310805205255747, "avg_entropy_of_first_answer": 2.7277616262435913, "lex_sim": 1.0}
{"question": "what is cost of first class mail stamp", "answer": ["49 cents"], "scores": [[0.45103636384010315, 0.5489253997802734, 0.6063942909240723, 0.39665520191192627, 0.995238184928894]], "normalized_score": [0.5682821869850159], "tokens": [["\u2581", "4", "9", "\u2581c", "ents"]], "entropy": [[1.919583797454834, 1.0178592205047607, 1.3612236976623535, 1.598079800605774, 0.03447806090116501]], "avg_entropy": [1.1862449154257775], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "49 cents", "score_of_biggest_cluster": 0.5682821869850159, "score_of_first_answer": 0.5682821869850159, "max_score_of_first_answer": 0.995238184928894, "min_score_of_first_answer": 0.39665520191192627, "avg_score": 0.5682821869850159, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.21090347720522679, "std_of_first_answer": 0.21090347720522679, "avg_entropy_of_first_answer": 1.1862449154257775, "lex_sim": 1.0}
{"question": "why does kerosene oil rise up in the wick of lantern", "answer": ["The wick in a kerosene lantern prevents the kerosene from leaking out of the tubing attached to the wick."], "scores": [[0.07390297949314117, 0.17739319801330566, 0.987798273563385, 0.009896385483443737, 0.6283594369888306, 0.42555901408195496, 0.9987949132919312, 0.9988803267478943, 0.5803964734077454, 0.9998825788497925, 0.003953754436224699, 0.66053307056427, 0.18140563368797302, 0.999076247215271, 0.9986717700958252, 0.812501847743988, 0.010241822339594364, 0.9959477782249451, 0.6171025633811951, 0.2859265208244324, 0.954391598701477, 0.001284629455767572, 0.20638106763362885, 0.005337510723620653, 0.9752846956253052, 0.732363224029541, 0.3125764727592468, 0.9988273978233337, 0.5211823582649231]], "normalized_score": [0.24591971933841705], "tokens": [["\u2581The", "\u2581w", "ick", "\u2581in", "\u2581a", "\u2581k", "eros", "ene", "\u2581lan", "tern", "\u2581prevents", "\u2581the", "\u2581k", "eros", "ene", "\u2581from", "\u2581le", "aking", "\u2581out", "\u2581of", "\u2581the", "\u2581tub", "ing", "\u2581attached", "\u2581to", "\u2581the", "\u2581w", "ick", "."]], "entropy": [[4.650357723236084, 4.723812103271484, 0.08001869916915894, 3.696744441986084, 1.2894246578216553, 2.1237897872924805, 0.010504576377570629, 0.00990887638181448, 1.1385802030563354, 0.00132542266510427, 3.4643430709838867, 1.9414637088775635, 2.7824389934539795, 0.008669668808579445, 0.010918108746409416, 0.7762333750724792, 3.7495062351226807, 0.03252190351486206, 1.9460952281951904, 2.6560206413269043, 0.2847259044647217, 3.320669651031494, 2.2939610481262207, 2.738247871398926, 0.19692111015319824, 0.7243834733963013, 3.053736686706543, 0.011116234585642815, 2.02177095413208]], "avg_entropy": [1.715110702046739], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "capillary action", "score_of_biggest_cluster": 0.24591971933841705, "score_of_first_answer": 0.24591971933841705, "max_score_of_first_answer": 0.9998825788497925, "min_score_of_first_answer": 0.001284629455767572, "avg_score": 0.24591971933841705, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38551362432432085, "std_of_first_answer": 0.38551362432432085, "avg_entropy_of_first_answer": 1.715110702046739, "lex_sim": 1.0}
{"question": "when does flash come back on season 4 episode 17", "answer": ["Season 4, Episode 17"], "scores": [[0.007256411015987396, 0.9420070648193359, 0.6898868083953857, 0.4816388189792633, 0.7317902445793152, 0.9989646673202515, 0.9946995973587036, 0.938651978969574, 0.7524948120117188]], "normalized_score": [0.47217321395874023], "tokens": [["\u2581Season", "\u2581", "4", ",", "\u2581E", "pisode", "\u2581", "1", "7"]], "entropy": [[3.436829090118408, 0.3809308409690857, 1.0372651815414429, 1.5949755907058716, 0.9441656470298767, 0.010552126914262772, 0.051106542348861694, 0.3317117691040039, 1.0255157947540283]], "avg_entropy": [0.9792280648317602], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "May\u00a01,\u00a02018", "score_of_biggest_cluster": 0.47217321395874023, "score_of_first_answer": 0.47217321395874023, "max_score_of_first_answer": 0.9989646673202515, "min_score_of_first_answer": 0.007256411015987396, "avg_score": 0.47217321395874023, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30128100953036824, "std_of_first_answer": 0.30128100953036824, "avg_entropy_of_first_answer": 0.9792280648317602, "lex_sim": 1.0}
{"question": "what happens when an air mass is pushed up and over a mountain range", "answer": ["Mountains produce a vertical orographic lifting"], "scores": [[0.003095344640314579, 0.015953904017806053, 0.08270741254091263, 0.0025614306796342134, 0.014283348806202412, 0.9371642470359802, 0.21329928934574127, 0.9983083009719849]], "normalized_score": [0.04834068939089775], "tokens": [["\u2581Mountains", "\u2581produce", "\u2581a", "\u2581vertical", "\u2581or", "ographic", "\u2581lif", "ting"]], "entropy": [[4.739492416381836, 4.700400352478027, 4.003911018371582, 4.879997253417969, 4.33495569229126, 0.5071772933006287, 3.1802215576171875, 0.015566851943731308]], "avg_entropy": [3.2952153044752777], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Orographic lift", "score_of_biggest_cluster": 0.04834068939089775, "score_of_first_answer": 0.04834068939089775, "max_score_of_first_answer": 0.9983083009719849, "min_score_of_first_answer": 0.0025614306796342134, "avg_score": 0.04834068939089775, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.40079380267217374, "std_of_first_answer": 0.40079380267217374, "avg_entropy_of_first_answer": 3.2952153044752777, "lex_sim": 1.0}
{"question": "love is not love that alters when it alteration finds meaning", "answer": ["William Shakespeare, Sonnet 116"], "scores": [[0.201257586479187, 0.9903596639633179, 0.03112194873392582, 0.5143384337425232, 0.9974061846733093, 0.9050858616828918, 0.9939550757408142, 0.9895690083503723, 0.9975757002830505]], "normalized_score": [0.5209522843360901], "tokens": [["\u2581William", "\u2581Shakespeare", ",", "\u2581Son", "net", "\u2581", "1", "1", "6"]], "entropy": [[4.453521728515625, 0.07470686733722687, 0.6548244953155518, 2.4197845458984375, 0.024220596998929977, 0.5846771001815796, 0.04927144944667816, 0.06942271441221237, 0.022232655435800552]], "avg_entropy": [0.9280735726157824], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sonnet 116", "score_of_biggest_cluster": 0.5209522843360901, "score_of_first_answer": 0.5209522843360901, "max_score_of_first_answer": 0.9975757002830505, "min_score_of_first_answer": 0.03112194873392582, "avg_score": 0.5209522843360901, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.36405388858829857, "std_of_first_answer": 0.36405388858829857, "avg_entropy_of_first_answer": 0.9280735726157824, "lex_sim": 1.0}
{"question": "who sings the whiskey ain't workin anymore", "answer": ["Toby Keith"], "scores": [[0.5954336524009705, 0.39694157242774963, 0.9969401359558105]], "normalized_score": [0.6176508069038391], "tokens": [["\u2581T", "oby", "\u2581Keith"]], "entropy": [[2.439455986022949, 1.0498982667922974, 0.031670816242694855]], "avg_entropy": [1.1736750230193138], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dennis Locorriere", "score_of_biggest_cluster": 0.6176508069038391, "score_of_first_answer": 0.6176508069038391, "max_score_of_first_answer": 0.9969401359558105, "min_score_of_first_answer": 0.39694157242774963, "avg_score": 0.6176508069038391, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24957849764584314, "std_of_first_answer": 0.24957849764584314, "avg_entropy_of_first_answer": 1.1736750230193138, "lex_sim": 1.0}
{"question": "when did they stop using aluminum wiring in ontario", "answer": ["1997"], "scores": [[0.6246649622917175, 0.5473535656929016, 0.9920010566711426, 0.27691367268562317, 0.2623254656791687]], "normalized_score": [0.4767848253250122], "tokens": [["\u2581", "1", "9", "9", "7"]], "entropy": [[2.2493486404418945, 0.7522038817405701, 0.07092223316431046, 1.2586016654968262, 2.1849377155303955]], "avg_entropy": [1.3032028272747993], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "mid-1970s", "score_of_biggest_cluster": 0.4767848253250122, "score_of_first_answer": 0.4767848253250122, "max_score_of_first_answer": 0.9920010566711426, "min_score_of_first_answer": 0.2623254656791687, "avg_score": 0.4767848253250122, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2675229428572436, "std_of_first_answer": 0.2675229428572436, "avg_entropy_of_first_answer": 1.3032028272747993, "lex_sim": 1.0}
{"question": "who became the king of ayodhya after ram", "answer": ["Rama"], "scores": [[0.15780004858970642, 0.8956098556518555]], "normalized_score": [0.3759351968765259], "tokens": [["\u2581R", "ama"]], "entropy": [[3.746434211730957, 0.5680723786354065]], "avg_entropy": [2.1572532951831818], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kusha", "score_of_biggest_cluster": 0.3759351968765259, "score_of_first_answer": 0.3759351968765259, "max_score_of_first_answer": 0.8956098556518555, "min_score_of_first_answer": 0.15780004858970642, "avg_score": 0.3759351968765259, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3689049035310745, "std_of_first_answer": 0.3689049035310745, "avg_entropy_of_first_answer": 2.1572532951831818, "lex_sim": 1.0}
{"question": "who is the voice of the other mother in coraline", "answer": ["Teri Hatcher"], "scores": [[0.8207424283027649, 0.9305821657180786, 0.9655723571777344, 0.999265730381012, 0.9997569918632507]], "normalized_score": [0.9407286643981934], "tokens": [["\u2581Ter", "i", "\u2581H", "atch", "er"]], "entropy": [[0.9011393189430237, 0.2688036859035492, 0.26091963052749634, 0.008799251168966293, 0.0029169931076467037]], "avg_entropy": [0.28851577593013644], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Teri Hatcher", "score_of_biggest_cluster": 0.9407286643981934, "score_of_first_answer": 0.9407286643981934, "max_score_of_first_answer": 0.9997569918632507, "min_score_of_first_answer": 0.8207424283027649, "avg_score": 0.9407286643981934, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06632894863472903, "std_of_first_answer": 0.06632894863472903, "avg_entropy_of_first_answer": 0.28851577593013644, "lex_sim": 1.0}
{"question": "what is the meaning of auv in cars", "answer": ["all wheel drive"], "scores": [[0.0328756719827652, 0.4361400306224823, 0.9746846556663513]], "normalized_score": [0.2408730387687683], "tokens": [["\u2581all", "\u2581wheel", "\u2581drive"]], "entropy": [[4.591012954711914, 1.1167917251586914, 0.2068835198879242]], "avg_entropy": [1.9715627332528431], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "action utility vehicles", "score_of_biggest_cluster": 0.2408730387687683, "score_of_first_answer": 0.2408730387687683, "max_score_of_first_answer": 0.9746846556663513, "min_score_of_first_answer": 0.0328756719827652, "avg_score": 0.2408730387687683, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3858117874271877, "std_of_first_answer": 0.3858117874271877, "avg_entropy_of_first_answer": 1.9715627332528431, "lex_sim": 1.0}
{"question": "who did the central powers defeat on the eastern front before concentrating back on france", "answer": ["Germany"], "scores": [[0.173681378364563]], "normalized_score": [0.1736813634634018], "tokens": [["\u2581Germany"]], "entropy": [[3.7321832180023193]], "avg_entropy": [3.7321832180023193], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Russian army", "score_of_biggest_cluster": 0.1736813634634018, "score_of_first_answer": 0.1736813634634018, "max_score_of_first_answer": 0.173681378364563, "min_score_of_first_answer": 0.173681378364563, "avg_score": 0.1736813634634018, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.7321832180023193, "lex_sim": 1.0}
{"question": "what allows chyme to enter the small intestine", "answer": ["sphincter and peristalsis"], "scores": [[0.019081715494394302, 0.9938822984695435, 0.9938587546348572, 0.8441892266273499, 0.0014574786182492971, 0.10356638580560684, 0.9986366629600525, 0.9145107865333557, 0.9975711703300476]], "normalized_score": [0.23502811789512634], "tokens": [["\u2581s", "ph", "inct", "er", "\u2581and", "\u2581per", "ist", "als", "is"]], "entropy": [[5.193291664123535, 0.05806690454483032, 0.050964128226041794, 0.4433145225048065, 1.0805599689483643, 4.660248756408691, 0.013851547613739967, 0.3162911832332611, 0.01954282820224762]], "avg_entropy": [1.3151257226450577], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "pyloric valve", "score_of_biggest_cluster": 0.23502811789512634, "score_of_first_answer": 0.23502811789512634, "max_score_of_first_answer": 0.9986366629600525, "min_score_of_first_answer": 0.0014574786182492971, "avg_score": 0.23502811789512634, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.43509211281101534, "std_of_first_answer": 0.43509211281101534, "avg_entropy_of_first_answer": 1.3151257226450577, "lex_sim": 1.0}
{"question": "who played king theoden in lord of the rings", "answer": ["Bernard Hill"], "scores": [[0.927636981010437, 0.9882386922836304]], "normalized_score": [0.95745849609375], "tokens": [["\u2581Bernard", "\u2581Hill"]], "entropy": [[0.48163875937461853, 0.12086280435323715]], "avg_entropy": [0.30125078186392784], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bernard Hill", "score_of_biggest_cluster": 0.95745849609375, "score_of_first_answer": 0.95745849609375, "max_score_of_first_answer": 0.9882386922836304, "min_score_of_first_answer": 0.927636981010437, "avg_score": 0.95745849609375, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03030085563659668, "std_of_first_answer": 0.03030085563659668, "avg_entropy_of_first_answer": 0.30125078186392784, "lex_sim": 1.0}
{"question": "where does iron ore come from in australia", "answer": ["Western Australia"], "scores": [[0.19313357770442963, 0.975095272064209]], "normalized_score": [0.43396270275115967], "tokens": [["\u2581Western", "\u2581Australia"]], "entropy": [[4.466902732849121, 0.19167843461036682]], "avg_entropy": [2.329290583729744], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Western Australia", "score_of_biggest_cluster": 0.43396270275115967, "score_of_first_answer": 0.43396270275115967, "max_score_of_first_answer": 0.975095272064209, "min_score_of_first_answer": 0.19313357770442963, "avg_score": 0.43396270275115967, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3909808471798897, "std_of_first_answer": 0.3909808471798897, "avg_entropy_of_first_answer": 2.329290583729744, "lex_sim": 1.0}
{"question": "green algae is an example of which type of reproduction", "answer": ["asexual?"], "scores": [[0.42720067501068115, 0.9978092312812805, 0.9793863892555237, 0.0001137137005571276]], "normalized_score": [0.08300640434026718], "tokens": [["\u2581a", "sex", "ual", "?"]], "entropy": [[2.8912551403045654, 0.024057162925601006, 0.1187322735786438, 0.5027419924736023]], "avg_entropy": [0.8841966423206031], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "alternation of generations", "score_of_biggest_cluster": 0.08300640434026718, "score_of_first_answer": 0.08300640434026718, "max_score_of_first_answer": 0.9978092312812805, "min_score_of_first_answer": 0.0001137137005571276, "avg_score": 0.08300640434026718, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4159039237029371, "std_of_first_answer": 0.4159039237029371, "avg_entropy_of_first_answer": 0.8841966423206031, "lex_sim": 1.0}
{"question": "who has the oldest team in the nba", "answer": ["Los Angeles Lakers"], "scores": [[0.05608454719185829, 0.9901663064956665, 0.8591489791870117, 0.9991427659988403]], "normalized_score": [0.46726346015930176], "tokens": [["\u2581Los", "\u2581Angeles", "\u2581L", "akers"]], "entropy": [[3.573711395263672, 0.07645721733570099, 0.532805323600769, 0.008282388560473919]], "avg_entropy": [1.047814081190154], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sacramento Kings", "score_of_biggest_cluster": 0.46726346015930176, "score_of_first_answer": 0.46726346015930176, "max_score_of_first_answer": 0.9991427659988403, "min_score_of_first_answer": 0.05608454719185829, "avg_score": 0.46726346015930176, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3908024114150769, "std_of_first_answer": 0.3908024114150769, "avg_entropy_of_first_answer": 1.047814081190154, "lex_sim": 1.0}
{"question": "who is the actor who plays king joffrey", "answer": ["Jack Gleeson"], "scores": [[0.9374971985816956, 0.9725551605224609, 0.9966442584991455, 0.9658850431442261, 0.9981597065925598]], "normalized_score": [0.9738901257514954], "tokens": [["\u2581Jack", "\u2581G", "le", "es", "on"]], "entropy": [[0.4729852080345154, 0.27282899618148804, 0.03849908709526062, 0.17276465892791748, 0.018942318856716156]], "avg_entropy": [0.19520405381917955], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jack Gleeson", "score_of_biggest_cluster": 0.9738901257514954, "score_of_first_answer": 0.9738901257514954, "max_score_of_first_answer": 0.9981597065925598, "min_score_of_first_answer": 0.9374971985816956, "avg_score": 0.9738901257514954, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.022346089312357133, "std_of_first_answer": 0.022346089312357133, "avg_entropy_of_first_answer": 0.19520405381917955, "lex_sim": 1.0}
{"question": "what's the dwarf's name in game of thrones", "answer": ["Hair Bolton, the Hound"], "scores": [[0.1116856262087822, 0.0004888055846095085, 0.003558631055057049, 0.39359423518180847, 0.006883388850837946, 0.04245111346244812, 0.0644727349281311, 0.5141869783401489]], "normalized_score": [0.030458375811576843], "tokens": [["\u2581H", "air", "\u2581Bol", "ton", ",", "\u2581the", "\u2581H", "ound"]], "entropy": [[2.921319007873535, 0.26667356491088867, 1.0858616828918457, 1.3689932823181152, 0.3044409155845642, 5.056221961975098, 5.347410202026367, 1.4909032583236694]], "avg_entropy": [2.2302279844880104], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tyrion Lannister", "score_of_biggest_cluster": 0.030458375811576843, "score_of_first_answer": 0.030458375811576843, "max_score_of_first_answer": 0.5141869783401489, "min_score_of_first_answer": 0.0004888055846095085, "avg_score": 0.030458375811576843, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18577874475108466, "std_of_first_answer": 0.18577874475108466, "avg_entropy_of_first_answer": 2.2302279844880104, "lex_sim": 1.0}
{"question": "what part of maryland is close to washington dc", "answer": ["Prince William County, Prince Georges County, Montgomery County"], "scores": [[0.21516169607639313, 0.006145993247628212, 0.29925376176834106, 0.018479174003005028, 0.0444626659154892, 0.1376461535692215, 0.9534932971000671, 0.6264927387237549, 0.3572688698768616, 0.9979674220085144, 0.9995755553245544, 0.9743141531944275]], "normalized_score": [0.21411225199699402], "tokens": [["\u2581Prince", "\u2581William", "\u2581County", ",", "\u2581Prince", "\u2581Georges", "\u2581County", ",", "\u2581Mont", "gom", "ery", "\u2581County"]], "entropy": [[3.5457541942596436, 0.6814818978309631, 1.3094749450683594, 0.43635696172714233, 3.569244861602783, 0.690548300743103, 0.28111401200294495, 1.0166335105895996, 2.549288034439087, 0.02038203366100788, 0.005008777603507042, 0.1850326508283615]], "avg_entropy": [1.1908600150297086], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charles County", "score_of_biggest_cluster": 0.21411225199699402, "score_of_first_answer": 0.21411225199699402, "max_score_of_first_answer": 0.9995755553245544, "min_score_of_first_answer": 0.006145993247628212, "avg_score": 0.21411225199699402, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3967743186768992, "std_of_first_answer": 0.3967743186768992, "avg_entropy_of_first_answer": 1.1908600150297086, "lex_sim": 1.0}
{"question": "who plays the first nfl game of the season", "answer": ["Cam Newton"], "scores": [[0.013634545728564262, 0.9874060750007629]], "normalized_score": [0.11602946370840073], "tokens": [["\u2581Cam", "\u2581Newton"]], "entropy": [[4.769965171813965, 0.12521740794181824]], "avg_entropy": [2.4475912898778915], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the New England Patriots", "score_of_biggest_cluster": 0.11602946370840073, "score_of_first_answer": 0.11602946370840073, "max_score_of_first_answer": 0.9874060750007629, "min_score_of_first_answer": 0.013634545728564262, "avg_score": 0.11602946370840073, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.48688576463609934, "std_of_first_answer": 0.48688576463609934, "avg_entropy_of_first_answer": 2.4475912898778915, "lex_sim": 1.0}
{"question": "when did the us stop drafting for the vietnam war", "answer": ["1973 (February 27)"], "scores": [[0.614348828792572, 0.9843855500221252, 0.9942289590835571, 0.9520315527915955, 0.8527346253395081, 0.001800425467081368, 0.015807632356882095, 0.8166483044624329, 0.9997091889381409, 0.4288114607334137, 0.8487657904624939, 0.1985863596200943, 0.6854420900344849]], "normalized_score": [0.3305250406265259], "tokens": [["\u2581", "1", "9", "7", "3", "\u2581(", "F", "ebru", "ary", "\u2581", "2", "7", ")"]], "entropy": [[2.024646759033203, 0.1106732040643692, 0.05125091224908829, 0.20772042870521545, 0.6174991726875305, 0.2552366554737091, 5.464224815368652, 0.8732125163078308, 0.003190691815689206, 0.8726160526275635, 0.6477709412574768, 0.8013273477554321, 1.0615482330322266]], "avg_entropy": [0.9993013638752297], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1973", "score_of_biggest_cluster": 0.3305250406265259, "score_of_first_answer": 0.3305250406265259, "max_score_of_first_answer": 0.9997091889381409, "min_score_of_first_answer": 0.001800425467081368, "avg_score": 0.3305250406265259, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3540478912242084, "std_of_first_answer": 0.3540478912242084, "avg_entropy_of_first_answer": 0.9993013638752297, "lex_sim": 1.0}
{"question": "who starred in the movie natural born killers", "answer": ["Woody Harrelson and Juliette Lewis"], "scores": [[0.6073994636535645, 0.9927000403404236, 0.9867871999740601, 0.9968721270561218, 0.9984717965126038, 0.06482192873954773, 0.8992169499397278, 0.9950646758079529, 0.9300668835639954, 0.9996411800384521]], "normalized_score": [0.7086449861526489], "tokens": [["\u2581Wo", "ody", "\u2581Har", "rel", "son", "\u2581and", "\u2581Jul", "iet", "te", "\u2581Lewis"]], "entropy": [[1.8042418956756592, 0.0622294545173645, 0.11267441511154175, 0.026034057140350342, 0.014925660565495491, 0.5826624631881714, 0.6043984293937683, 0.036301642656326294, 0.31299859285354614, 0.0047794827260077]], "avg_entropy": [0.3561246093828231], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Robert Downey Jr.", "score_of_biggest_cluster": 0.7086449861526489, "score_of_first_answer": 0.7086449861526489, "max_score_of_first_answer": 0.9996411800384521, "min_score_of_first_answer": 0.06482192873954773, "avg_score": 0.7086449861526489, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.284646159514086, "std_of_first_answer": 0.284646159514086, "avg_entropy_of_first_answer": 0.3561246093828231, "lex_sim": 1.0}
{"question": "where did the christmas tree at rockefeller center come from", "answer": ["Norway"], "scores": [[0.27588364481925964]], "normalized_score": [0.27588364481925964], "tokens": [["\u2581Norway"]], "entropy": [[3.8867149353027344]], "avg_entropy": [3.8867149353027344], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "State College, PA", "score_of_biggest_cluster": 0.27588364481925964, "score_of_first_answer": 0.27588364481925964, "max_score_of_first_answer": 0.27588364481925964, "min_score_of_first_answer": 0.27588364481925964, "avg_score": 0.27588364481925964, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.8867149353027344, "lex_sim": 1.0}
{"question": "who sang last night on saturday night live", "answer": ["Taylor Swift"], "scores": [[0.00721310917288065, 0.9654636979103088]], "normalized_score": [0.0834505558013916], "tokens": [["\u2581Taylor", "\u2581Swift"]], "entropy": [[5.134530544281006, 0.29815709590911865]], "avg_entropy": [2.7163438200950623], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Childish Gambino", "score_of_biggest_cluster": 0.0834505558013916, "score_of_first_answer": 0.0834505558013916, "max_score_of_first_answer": 0.9654636979103088, "min_score_of_first_answer": 0.00721310917288065, "avg_score": 0.0834505558013916, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4791252943687141, "std_of_first_answer": 0.4791252943687141, "avg_entropy_of_first_answer": 2.7163438200950623, "lex_sim": 1.0}
{"question": "when did marathon change its name to snickers", "answer": ["2007"], "scores": [[0.6396304368972778, 0.2676134407520294, 0.9864450097084045, 0.6851409673690796, 0.4727262556552887]], "normalized_score": [0.5592164397239685], "tokens": [["\u2581", "2", "0", "0", "7"]], "entropy": [[2.210808277130127, 0.6638884544372559, 0.11383060365915298, 0.6595253348350525, 1.8453304767608643]], "avg_entropy": [1.0986766293644905], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "19 July 1990", "score_of_biggest_cluster": 0.5592164397239685, "score_of_first_answer": 0.5592164397239685, "max_score_of_first_answer": 0.9864450097084045, "min_score_of_first_answer": 0.2676134407520294, "avg_score": 0.5592164397239685, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2384563379935297, "std_of_first_answer": 0.2384563379935297, "avg_entropy_of_first_answer": 1.0986766293644905, "lex_sim": 1.0}
{"question": "who played the nurse on andy griffith show", "answer": ["Barbara"], "scores": [[0.009184187278151512]], "normalized_score": [0.009184189140796661], "tokens": [["\u2581Barbara"]], "entropy": [[4.380353927612305]], "avg_entropy": [4.380353927612305], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Julie Adams", "score_of_biggest_cluster": 0.009184189140796661, "score_of_first_answer": 0.009184189140796661, "max_score_of_first_answer": 0.009184187278151512, "min_score_of_first_answer": 0.009184187278151512, "avg_score": 0.009184189140796661, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.380353927612305, "lex_sim": 1.0}
{"question": "when does wonder woman come out on blue ray", "answer": ["October 25, 2016"], "scores": [[0.06291166692972183, 0.9401670694351196, 0.22024989128112793, 0.049196112900972366, 0.3059854209423065, 0.990898072719574, 0.9992002844810486, 0.9996166229248047, 0.9802261590957642, 0.755328357219696]], "normalized_score": [0.41280606389045715], "tokens": [["\u2581October", "\u2581", "2", "5", ",", "\u2581", "2", "0", "1", "6"]], "entropy": [[3.0561699867248535, 0.28629010915756226, 1.0937776565551758, 1.9101758003234863, 1.0619052648544312, 0.059856973588466644, 0.007501176558434963, 0.004637653939425945, 0.10821647942066193, 0.9479460716247559]], "avg_entropy": [0.8536477172747254], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "September 19, 2017", "score_of_biggest_cluster": 0.41280606389045715, "score_of_first_answer": 0.41280606389045715, "max_score_of_first_answer": 0.9996166229248047, "min_score_of_first_answer": 0.049196112900972366, "avg_score": 0.41280606389045715, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3961746320406796, "std_of_first_answer": 0.3961746320406796, "avg_entropy_of_first_answer": 0.8536477172747254, "lex_sim": 1.0}
{"question": "form from material that has accumulated on the earths surface", "answer": ["Soil"], "scores": [[0.008344399742782116, 0.9901727437973022]], "normalized_score": [0.09089773148298264], "tokens": [["\u2581So", "il"]], "entropy": [[3.8120641708374023, 0.06860145926475525]], "avg_entropy": [1.9403328150510788], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sedimentary rock", "score_of_biggest_cluster": 0.09089773148298264, "score_of_first_answer": 0.09089773148298264, "max_score_of_first_answer": 0.9901727437973022, "min_score_of_first_answer": 0.008344399742782116, "avg_score": 0.09089773148298264, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.49091417202726007, "std_of_first_answer": 0.49091417202726007, "avg_entropy_of_first_answer": 1.9403328150510788, "lex_sim": 1.0}
{"question": "what's the biggest country in western europe", "answer": ["France"], "scores": [[0.5998885631561279]], "normalized_score": [0.5998885631561279], "tokens": [["\u2581France"]], "entropy": [[1.6436314582824707]], "avg_entropy": [1.6436314582824707], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Russia*", "score_of_biggest_cluster": 0.5998885631561279, "score_of_first_answer": 0.5998885631561279, "max_score_of_first_answer": 0.5998885631561279, "min_score_of_first_answer": 0.5998885631561279, "avg_score": 0.5998885631561279, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.6436314582824707, "lex_sim": 1.0}
{"question": "when was how deep is your love released", "answer": ["1995"], "scores": [[0.7132782340049744, 0.7247738838195801, 0.9869733452796936, 0.8001136779785156, 0.1305232048034668]], "normalized_score": [0.5563156604766846], "tokens": [["\u2581", "1", "9", "9", "5"]], "entropy": [[1.5343832969665527, 0.6629514694213867, 0.10388082265853882, 0.687013566493988, 1.8214517831802368]], "avg_entropy": [0.9619361877441406], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1977", "score_of_biggest_cluster": 0.5563156604766846, "score_of_first_answer": 0.5563156604766846, "max_score_of_first_answer": 0.9869733452796936, "min_score_of_first_answer": 0.1305232048034668, "avg_score": 0.5563156604766846, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28750779287716777, "std_of_first_answer": 0.28750779287716777, "avg_entropy_of_first_answer": 0.9619361877441406, "lex_sim": 1.0}
{"question": "who is the director of taarak mehta ka ooltah chashmah", "answer": ["Asit Kumarr Modi"], "scores": [[0.40373528003692627, 0.8821847438812256, 0.5882309675216675, 0.5915975570678711, 0.8092362880706787, 0.9969778060913086, 0.9945785999298096]], "normalized_score": [0.7191249132156372], "tokens": [["\u2581As", "it", "\u2581K", "um", "arr", "\u2581Mod", "i"]], "entropy": [[3.085444927215576, 0.7712855935096741, 1.2979438304901123, 1.194674015045166, 1.0113260746002197, 0.026995260268449783, 0.04265527427196503]], "avg_entropy": [1.0614749964858805], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Malav Suresh Rajda", "score_of_biggest_cluster": 0.7191249132156372, "score_of_first_answer": 0.7191249132156372, "max_score_of_first_answer": 0.9969778060913086, "min_score_of_first_answer": 0.40373528003692627, "avg_score": 0.7191249132156372, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21143023001252817, "std_of_first_answer": 0.21143023001252817, "avg_entropy_of_first_answer": 1.0614749964858805, "lex_sim": 1.0}
{"question": "when was the last time kentucky won ncaa", "answer": ["March, 2012"], "scores": [[0.047689374536275864, 0.01903880573809147, 0.9865943193435669, 0.7484259605407715, 0.9154975414276123, 0.9513634443283081, 0.9362055659294128]], "normalized_score": [0.3419483006000519], "tokens": [["\u2581March", ",", "\u2581", "2", "0", "1", "2"]], "entropy": [[1.2485979795455933, 0.23668506741523743, 0.10090991854667664, 0.6543513536453247, 0.4632054567337036, 0.23767615854740143, 0.34504449367523193]], "avg_entropy": [0.469495775444167], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2012", "score_of_biggest_cluster": 0.3419483006000519, "score_of_first_answer": 0.3419483006000519, "max_score_of_first_answer": 0.9865943193435669, "min_score_of_first_answer": 0.01903880573809147, "avg_score": 0.3419483006000519, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4011887299006517, "std_of_first_answer": 0.4011887299006517, "avg_entropy_of_first_answer": 0.469495775444167, "lex_sim": 1.0}
{"question": "where do rory and paris go to college", "answer": ["Yale"], "scores": [[0.2812805771827698, 0.994766354560852]], "normalized_score": [0.5289692282676697], "tokens": [["\u2581Y", "ale"]], "entropy": [[4.424213886260986, 0.05003330484032631]], "avg_entropy": [2.2371235955506563], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yale University", "score_of_biggest_cluster": 0.5289692282676697, "score_of_first_answer": 0.5289692282676697, "max_score_of_first_answer": 0.994766354560852, "min_score_of_first_answer": 0.2812805771827698, "avg_score": 0.5289692282676697, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.35674288868904114, "std_of_first_answer": 0.35674288868904114, "avg_entropy_of_first_answer": 2.2371235955506563, "lex_sim": 1.0}
{"question": "where are alkali metals located on the periodic table", "answer": ["Groups 0, I, II"], "scores": [[0.012494084425270557, 0.8760243654251099, 0.9150559306144714, 0.0060769012197852135, 0.2153410166501999, 0.029485896229743958, 0.2781049907207489, 0.8031059503555298]], "normalized_score": [0.13092094659805298], "tokens": [["\u2581Gr", "oups", "\u2581", "0", ",", "\u2581I", ",", "\u2581II"]], "entropy": [[3.648627519607544, 0.47141873836517334, 0.528755784034729, 0.21562109887599945, 2.4597420692443848, 0.9218562841415405, 1.4061956405639648, 0.9192367196083069]], "avg_entropy": [1.3214317318052053], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in the s-block", "score_of_biggest_cluster": 0.13092094659805298, "score_of_first_answer": 0.13092094659805298, "max_score_of_first_answer": 0.9150559306144714, "min_score_of_first_answer": 0.0060769012197852135, "avg_score": 0.13092094659805298, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37839025924168024, "std_of_first_answer": 0.37839025924168024, "avg_entropy_of_first_answer": 1.3214317318052053, "lex_sim": 1.0}
{"question": "what proposition made the insurance commissioner an elected position", "answer": ["Proposition 103"], "scores": [[0.7669873237609863, 0.9832503199577332, 0.6078699827194214, 0.9056429266929626, 0.8953845500946045]], "normalized_score": [0.8204382658004761], "tokens": [["\u2581Proposition", "\u2581", "1", "0", "3"]], "entropy": [[1.4442318677902222, 0.1535395234823227, 1.4123289585113525, 0.5246704816818237, 0.5449517965316772]], "avg_entropy": [0.8159445255994797], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Proposition 103", "score_of_biggest_cluster": 0.8204382658004761, "score_of_first_answer": 0.8204382658004761, "max_score_of_first_answer": 0.9832503199577332, "min_score_of_first_answer": 0.6078699827194214, "avg_score": 0.8204382658004761, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.13174088525479277, "std_of_first_answer": 0.13174088525479277, "avg_entropy_of_first_answer": 0.8159445255994797, "lex_sim": 1.0}
{"question": "when did they stop making pull tabs on beer cans", "answer": ["2016"], "scores": [[0.6697683334350586, 0.3142012357711792, 0.9969338178634644, 0.5015823245048523, 0.08642999082803726]], "normalized_score": [0.3906259536743164], "tokens": [["\u2581", "2", "0", "1", "6"]], "entropy": [[2.189722776412964, 0.6916597485542297, 0.03046458214521408, 0.7826770544052124, 2.105226516723633]], "avg_entropy": [1.1599501356482507], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1990", "score_of_biggest_cluster": 0.3906259536743164, "score_of_first_answer": 0.3906259536743164, "max_score_of_first_answer": 0.9969338178634644, "min_score_of_first_answer": 0.08642999082803726, "avg_score": 0.3906259536743164, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3099607938409116, "std_of_first_answer": 0.3099607938409116, "avg_entropy_of_first_answer": 1.1599501356482507, "lex_sim": 1.0}
{"question": "georgia out of angus thongs and perfect snogging", "answer": ["Georgia Nicolson"], "scores": [[0.22941000759601593, 0.1299414485692978, 0.9965908527374268]], "normalized_score": [0.3097127079963684], "tokens": [["\u2581Georgia", "\u2581Nicol", "son"]], "entropy": [[5.352713584899902, 4.244517803192139, 0.03499917685985565]], "avg_entropy": [3.2107435216506324], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Georgia Groome as Georgia Nicolson", "score_of_biggest_cluster": 0.3097127079963684, "score_of_first_answer": 0.3097127079963684, "max_score_of_first_answer": 0.9965908527374268, "min_score_of_first_answer": 0.1299414485692978, "avg_score": 0.3097127079963684, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3872325791407126, "std_of_first_answer": 0.3872325791407126, "avg_entropy_of_first_answer": 3.2107435216506324, "lex_sim": 1.0}
{"question": "where does dividends go on cash flow statement", "answer": ["as an addition to net income"], "scores": [[0.004699906799942255, 0.3521433174610138, 0.24524830281734467, 0.8877530694007874, 0.3165436089038849, 0.8558611869812012]], "normalized_score": [0.21458066999912262], "tokens": [["\u2581as", "\u2581an", "\u2581addition", "\u2581to", "\u2581net", "\u2581income"]], "entropy": [[4.607856273651123, 2.74422287940979, 2.3685097694396973, 0.606834352016449, 2.669828176498413, 0.7325915098190308]], "avg_entropy": [2.288307160139084], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the financing activities section", "score_of_biggest_cluster": 0.21458066999912262, "score_of_first_answer": 0.21458066999912262, "max_score_of_first_answer": 0.8877530694007874, "min_score_of_first_answer": 0.004699906799942255, "avg_score": 0.21458066999912262, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32241678615054586, "std_of_first_answer": 0.32241678615054586, "avg_entropy_of_first_answer": 2.288307160139084, "lex_sim": 1.0}
{"question": "what theater number was the aurora shooting in", "answer": ["#15"], "scores": [[0.10269694030284882, 0.2527347803115845, 0.043205298483371735]], "normalized_score": [0.10389307141304016], "tokens": [["\u2581#", "1", "5"]], "entropy": [[3.567021608352661, 1.7134599685668945, 2.2475128173828125]], "avg_entropy": [2.509331464767456], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "9", "score_of_biggest_cluster": 0.10389307141304016, "score_of_first_answer": 0.10389307141304016, "max_score_of_first_answer": 0.2527347803115845, "min_score_of_first_answer": 0.043205298483371735, "avg_score": 0.10389307141304016, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08816223217333374, "std_of_first_answer": 0.08816223217333374, "avg_entropy_of_first_answer": 2.509331464767456, "lex_sim": 1.0}
{"question": "how long are the episodes in stranger things season 1", "answer": ["5 episodes"], "scores": [[0.737993597984314, 0.04087761789560318, 0.04168589785695076]], "normalized_score": [0.10793834179639816], "tokens": [["\u2581", "5", "\u2581episodes"]], "entropy": [[1.7726333141326904, 1.7883070707321167, 2.5890278816223145]], "avg_entropy": [2.049989422162374], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "42\u201362 minutes", "score_of_biggest_cluster": 0.10793834179639816, "score_of_first_answer": 0.10793834179639816, "max_score_of_first_answer": 0.737993597984314, "min_score_of_first_answer": 0.04087761789560318, "avg_score": 0.10793834179639816, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32843327688151985, "std_of_first_answer": 0.32843327688151985, "avg_entropy_of_first_answer": 2.049989422162374, "lex_sim": 1.0}
{"question": "who gets the most punishments on impractical jokers", "answer": ["Sal"], "scores": [[0.2296786904335022]], "normalized_score": [0.229678675532341], "tokens": [["\u2581Sal"]], "entropy": [[4.098927021026611]], "avg_entropy": [4.098927021026611], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sal", "score_of_biggest_cluster": 0.229678675532341, "score_of_first_answer": 0.229678675532341, "max_score_of_first_answer": 0.2296786904335022, "min_score_of_first_answer": 0.2296786904335022, "avg_score": 0.229678675532341, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.098927021026611, "lex_sim": 1.0}
{"question": "when is the met office leaving the bbc", "answer": ["March 24, 2018"], "scores": [[0.048276886343955994, 0.9329546689987183, 0.4959849417209625, 0.028975240886211395, 0.22029481828212738, 0.9884614944458008, 0.9944713115692139, 0.9991475343704224, 0.8281911015510559, 0.1689562201499939]], "normalized_score": [0.33823511004447937], "tokens": [["\u2581March", "\u2581", "2", "4", ",", "\u2581", "2", "0", "1", "8"]], "entropy": [[3.493462562561035, 0.3268137574195862, 1.305537462234497, 1.6589633226394653, 1.022479772567749, 0.08217382431030273, 0.036271922290325165, 0.0100884810090065, 0.5820062160491943, 2.0359480381011963]], "avg_entropy": [1.0553745359182358], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "31 March 2018", "score_of_biggest_cluster": 0.33823511004447937, "score_of_first_answer": 0.33823511004447937, "max_score_of_first_answer": 0.9991475343704224, "min_score_of_first_answer": 0.028975240886211395, "avg_score": 0.33823511004447937, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3989185189985772, "std_of_first_answer": 0.3989185189985772, "avg_entropy_of_first_answer": 1.0553745359182358, "lex_sim": 1.0}
{"question": "when was the first orca put in captivity", "answer": ["Orlando"], "scores": [[0.006890473421663046, 0.10757437348365784]], "normalized_score": [0.027225695550441742], "tokens": [["\u2581Or", "lando"]], "entropy": [[1.8813973665237427, 2.92742919921875]], "avg_entropy": [2.4044132828712463], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in the 1960s", "score_of_biggest_cluster": 0.027225695550441742, "score_of_first_answer": 0.027225695550441742, "max_score_of_first_answer": 0.10757437348365784, "min_score_of_first_answer": 0.006890473421663046, "avg_score": 0.027225695550441742, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.050341950030997396, "std_of_first_answer": 0.050341950030997396, "avg_entropy_of_first_answer": 2.4044132828712463, "lex_sim": 1.0}
{"question": "where did hope on bold and beautiful go", "answer": ["Hope was fired and went back to New York"], "scores": [[0.04400945082306862, 0.034745827317237854, 0.040447574108839035, 0.09283839166164398, 0.07425381243228912, 0.05661306902766228, 0.9065559506416321, 0.06666061282157898, 0.9542324542999268]], "normalized_score": [0.1037428006529808], "tokens": [["\u2581Hope", "\u2581was", "\u2581fired", "\u2581and", "\u2581went", "\u2581back", "\u2581to", "\u2581New", "\u2581York"]], "entropy": [[6.323940277099609, 4.853907108306885, 4.649660110473633, 2.626297950744629, 4.852675914764404, 1.0627925395965576, 0.4558164179325104, 4.968266487121582, 0.31101030111312866]], "avg_entropy": [3.344929678572549], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Milan, Italy", "score_of_biggest_cluster": 0.1037428006529808, "score_of_first_answer": 0.1037428006529808, "max_score_of_first_answer": 0.9542324542999268, "min_score_of_first_answer": 0.034745827317237854, "avg_score": 0.1037428006529808, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36304882347496154, "std_of_first_answer": 0.36304882347496154, "avg_entropy_of_first_answer": 3.344929678572549, "lex_sim": 1.0}
{"question": "who picks the chief justice of the illinois supreme court", "answer": ["the person who is chief justice of the Illinois Supreme Court selects the Chief Justice"], "scores": [[0.12800809741020203, 0.0008428285946138203, 0.2607055604457855, 0.316332072019577, 0.0058851963840425014, 0.8610846400260925, 0.7393016815185547, 0.9635502696037292, 0.1277281492948532, 0.8186399936676025, 0.9691192507743835, 0.010370413772761822, 0.50947105884552, 0.25315287709236145, 0.9851834177970886]], "normalized_score": [0.1746450662612915], "tokens": [["\u2581the", "\u2581person", "\u2581who", "\u2581is", "\u2581chief", "\u2581justice", "\u2581of", "\u2581the", "\u2581Illinois", "\u2581Supreme", "\u2581Court", "\u2581selects", "\u2581the", "\u2581Chief", "\u2581Justice"]], "entropy": [[3.523980140686035, 2.110682487487793, 3.6538596153259277, 3.5075361728668213, 3.13118052482605, 0.6972692608833313, 1.3978235721588135, 0.275175541639328, 2.193286895751953, 0.6346802115440369, 0.16541311144828796, 2.115450859069824, 2.1383512020111084, 2.2232680320739746, 0.11202511191368103]], "avg_entropy": [1.858665515979131], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the court", "score_of_biggest_cluster": 0.1746450662612915, "score_of_first_answer": 0.1746450662612915, "max_score_of_first_answer": 0.9851834177970886, "min_score_of_first_answer": 0.0008428285946138203, "avg_score": 0.1746450662612915, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.374285874723765, "std_of_first_answer": 0.374285874723765, "avg_entropy_of_first_answer": 1.858665515979131, "lex_sim": 1.0}
{"question": "when did the first ice age come out", "answer": ["January 4, 2005"], "scores": [[0.026232751086354256, 0.9556296467781067, 0.013317829929292202, 0.46821048855781555, 0.9906697273254395, 0.4248674511909485, 0.9993901252746582, 0.5213805437088013, 0.20901961624622345]], "normalized_score": [0.26814085245132446], "tokens": [["\u2581January", "\u2581", "4", ",", "\u2581", "2", "0", "0", "5"]], "entropy": [[1.999078631401062, 0.24119816720485687, 1.340369462966919, 0.800911009311676, 0.057859063148498535, 0.6950981020927429, 0.006725572980940342, 0.7578363418579102, 1.5585439205169678]], "avg_entropy": [0.8286244746090637], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2002", "score_of_biggest_cluster": 0.26814085245132446, "score_of_first_answer": 0.26814085245132446, "max_score_of_first_answer": 0.9993901252746582, "min_score_of_first_answer": 0.013317829929292202, "avg_score": 0.26814085245132446, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37260784941962616, "std_of_first_answer": 0.37260784941962616, "avg_entropy_of_first_answer": 0.8286244746090637, "lex_sim": 1.0}
{"question": "who plays rooster's girlfriend on the ranch", "answer": ["Juliette Angelo"], "scores": [[0.04257763549685478, 0.24236683547496796, 0.4006064236164093, 0.20994380116462708, 0.9158868789672852]], "normalized_score": [0.23991835117340088], "tokens": [["\u2581Jul", "iet", "te", "\u2581Ang", "elo"]], "entropy": [[5.396305084228516, 0.9291239976882935, 3.0980756282806396, 3.245781898498535, 0.4704686105251312]], "avg_entropy": [2.627951043844223], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Maggie Lawson", "score_of_biggest_cluster": 0.23991835117340088, "score_of_first_answer": 0.23991835117340088, "max_score_of_first_answer": 0.9158868789672852, "min_score_of_first_answer": 0.04257763549685478, "avg_score": 0.23991835117340088, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29924724810435405, "std_of_first_answer": 0.29924724810435405, "avg_entropy_of_first_answer": 2.627951043844223, "lex_sim": 1.0}
{"question": "where was the hindenburg scheduled to fly and how many passengers were on it", "answer": ["Lakehurst Navy Air Station, New Jersey, 36 passengers"], "scores": [[0.29798081517219543, 0.9859515428543091, 0.9972666501998901, 0.0023331418633461, 0.3282490670681, 0.7294532656669617, 0.5062586069107056, 0.6794396638870239, 0.9888675808906555, 0.47463446855545044, 0.6760176420211792, 0.32907307147979736, 0.40967726707458496, 0.7979385852813721]], "normalized_score": [0.3905632197856903], "tokens": [["\u2581Lake", "h", "urst", "\u2581Navy", "\u2581Air", "\u2581Station", ",", "\u2581New", "\u2581Jersey", ",", "\u2581", "3", "6", "\u2581passengers"]], "entropy": [[3.4731338024139404, 0.11646752804517746, 0.023602714762091637, 1.2431918382644653, 1.7467466592788696, 1.1752612590789795, 1.7144914865493774, 1.386986494064331, 0.09041701257228851, 1.7306842803955078, 1.5466358661651611, 1.746124029159546, 1.324526309967041, 0.916431725025177]], "avg_entropy": [1.3024786432672824], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "36 passengers", "score_of_biggest_cluster": 0.3905632197856903, "score_of_first_answer": 0.3905632197856903, "max_score_of_first_answer": 0.9972666501998901, "min_score_of_first_answer": 0.0023331418633461, "avg_score": 0.3905632197856903, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2902775025666225, "std_of_first_answer": 0.2902775025666225, "avg_entropy_of_first_answer": 1.3024786432672824, "lex_sim": 1.0}
{"question": "where was the first colony in north america located", "answer": ["Jamestown, Virginia"], "scores": [[0.3322322368621826, 0.9984256029129028, 0.9886595010757446, 0.23056605458259583, 0.864404559135437]], "normalized_score": [0.5795131921768188], "tokens": [["\u2581Jam", "est", "own", ",", "\u2581Virginia"]], "entropy": [[1.9946776628494263, 0.015771159902215004, 0.06389667093753815, 0.9216067790985107, 0.6923205852508545]], "avg_entropy": [0.737654571607709], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Virginia", "score_of_biggest_cluster": 0.5795131921768188, "score_of_first_answer": 0.5795131921768188, "max_score_of_first_answer": 0.9984256029129028, "min_score_of_first_answer": 0.23056605458259583, "avg_score": 0.5795131921768188, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.33273504560442413, "std_of_first_answer": 0.33273504560442413, "avg_entropy_of_first_answer": 0.737654571607709, "lex_sim": 1.0}
{"question": "symbolic interactionism is the basis for which theoretical model", "answer": ["Interaction"], "scores": [[0.024083014577627182, 0.8585437536239624]], "normalized_score": [0.14379264414310455], "tokens": [["\u2581Inter", "action"]], "entropy": [[3.3438503742218018, 0.6581243276596069]], "avg_entropy": [2.0009873509407043], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "role theory", "score_of_biggest_cluster": 0.14379264414310455, "score_of_first_answer": 0.14379264414310455, "max_score_of_first_answer": 0.8585437536239624, "min_score_of_first_answer": 0.024083014577627182, "avg_score": 0.14379264414310455, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4172303695231676, "std_of_first_answer": 0.4172303695231676, "avg_entropy_of_first_answer": 2.0009873509407043, "lex_sim": 1.0}
{"question": "what tectonic setting is responsible for the folded mountains of pennsylvania and the high himalaya", "answer": ["Concentric"], "scores": [[0.006266667973250151, 0.0009639154304750264, 0.9959900975227356]], "normalized_score": [0.018187660723924637], "tokens": [["\u2581Con", "cent", "ric"]], "entropy": [[3.794163227081299, 0.3369625210762024, 0.033781684935092926]], "avg_entropy": [1.3883024776975315], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "a convergent plate boundary", "score_of_biggest_cluster": 0.018187660723924637, "score_of_first_answer": 0.018187660723924637, "max_score_of_first_answer": 0.9959900975227356, "min_score_of_first_answer": 0.0009639154304750264, "avg_score": 0.018187660723924637, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46781497876746403, "std_of_first_answer": 0.46781497876746403, "avg_entropy_of_first_answer": 1.3883024776975315, "lex_sim": 1.0}
{"question": "when is the new series of keeping faith starting", "answer": ["February 2016"], "scores": [[0.035788606852293015, 0.9059810638427734, 0.38915523886680603, 0.2478567659854889, 0.6379551291465759, 0.1937560886144638]], "normalized_score": [0.26990166306495667], "tokens": [["\u2581February", "\u2581", "2", "0", "1", "6"]], "entropy": [[3.303773880004883, 0.3857382833957672, 1.6104168891906738, 2.315065383911133, 1.105558156967163, 1.8073844909667969]], "avg_entropy": [1.7546561807394028], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "13 February 2018", "score_of_biggest_cluster": 0.26990166306495667, "score_of_first_answer": 0.26990166306495667, "max_score_of_first_answer": 0.9059810638427734, "min_score_of_first_answer": 0.035788606852293015, "avg_score": 0.26990166306495667, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2918540710418651, "std_of_first_answer": 0.2918540710418651, "avg_entropy_of_first_answer": 1.7546561807394028, "lex_sim": 1.0}
{"question": "country with most olympic gold medals all time", "answer": ["Russia"], "scores": [[0.07858294993638992]], "normalized_score": [0.07858294993638992], "tokens": [["\u2581Russia"]], "entropy": [[2.3150885105133057]], "avg_entropy": [2.3150885105133057], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "United States", "score_of_biggest_cluster": 0.07858294993638992, "score_of_first_answer": 0.07858294993638992, "max_score_of_first_answer": 0.07858294993638992, "min_score_of_first_answer": 0.07858294993638992, "avg_score": 0.07858294993638992, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.3150885105133057, "lex_sim": 1.0}
{"question": "what is the name for the ch3coo- ion", "answer": ["Acetate"], "scores": [[0.006278565619140863, 0.9056242108345032, 0.4676820635795593]], "normalized_score": [0.13854359090328217], "tokens": [["\u2581Ac", "et", "ate"]], "entropy": [[4.434815406799316, 0.4479362964630127, 1.012202262878418]], "avg_entropy": [1.964984655380249], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "polyatomic anion", "score_of_biggest_cluster": 0.13854359090328217, "score_of_first_answer": 0.13854359090328217, "max_score_of_first_answer": 0.9056242108345032, "min_score_of_first_answer": 0.006278565619140863, "avg_score": 0.13854359090328217, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36719796384091374, "std_of_first_answer": 0.36719796384091374, "avg_entropy_of_first_answer": 1.964984655380249, "lex_sim": 1.0}
{"question": "when does wentworth season 6 start in australia", "answer": ["July 1st"], "scores": [[0.06309127062559128, 0.9279761910438538, 0.3949344754219055, 0.1656033843755722]], "normalized_score": [0.24875688552856445], "tokens": [["\u2581July", "\u2581", "1", "st"]], "entropy": [[3.331571102142334, 0.32789552211761475, 1.6534281969070435, 2.5339303016662598]], "avg_entropy": [1.961706280708313], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2019", "score_of_biggest_cluster": 0.24875688552856445, "score_of_first_answer": 0.24875688552856445, "max_score_of_first_answer": 0.9279761910438538, "min_score_of_first_answer": 0.06309127062559128, "avg_score": 0.24875688552856445, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3341587466929871, "std_of_first_answer": 0.3341587466929871, "avg_entropy_of_first_answer": 1.961706280708313, "lex_sim": 1.0}
{"question": "who has the world's largest standing army", "answer": ["China"], "scores": [[0.48488593101501465]], "normalized_score": [0.48488593101501465], "tokens": [["\u2581China"]], "entropy": [[2.163790225982666]], "avg_entropy": [2.163790225982666], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "China", "score_of_biggest_cluster": 0.48488593101501465, "score_of_first_answer": 0.48488593101501465, "max_score_of_first_answer": 0.48488593101501465, "min_score_of_first_answer": 0.48488593101501465, "avg_score": 0.48488593101501465, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.163790225982666, "lex_sim": 1.0}
{"question": "the german princes who chose the holy roman empire were called", "answer": ["Electors"], "scores": [[0.09245222806930542, 0.8777005672454834]], "normalized_score": [0.2848602831363678], "tokens": [["\u2581Elect", "ors"]], "entropy": [[3.4950335025787354, 0.40962356328964233]], "avg_entropy": [1.9523285329341888], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "prince-electors", "score_of_biggest_cluster": 0.2848602831363678, "score_of_first_answer": 0.2848602831363678, "max_score_of_first_answer": 0.8777005672454834, "min_score_of_first_answer": 0.09245222806930542, "avg_score": 0.2848602831363678, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.392624169588089, "std_of_first_answer": 0.392624169588089, "avg_entropy_of_first_answer": 1.9523285329341888, "lex_sim": 1.0}
{"question": "who played john coffey in the movie the green mile", "answer": ["Michael Clarke Duncan"], "scores": [[0.6723055839538574, 0.9886171221733093, 0.9995307922363281, 0.9947688579559326, 0.9996693134307861]], "normalized_score": [0.920436680316925], "tokens": [["\u2581Michael", "\u2581Clar", "ke", "\u2581Dun", "can"]], "entropy": [[1.8782566785812378, 0.09078725427389145, 0.0046422588638961315, 0.042119938880205154, 0.004344940185546875]], "avg_entropy": [0.4040302141569555], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Michael Clarke Duncan", "score_of_biggest_cluster": 0.920436680316925, "score_of_first_answer": 0.920436680316925, "max_score_of_first_answer": 0.9996693134307861, "min_score_of_first_answer": 0.6723055839538574, "avg_score": 0.920436680316925, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.12939933744072912, "std_of_first_answer": 0.12939933744072912, "avg_entropy_of_first_answer": 0.4040302141569555, "lex_sim": 1.0}
{"question": "when did radio shack change to the source", "answer": ["Feb. 1, 2017"], "scores": [[0.003973409533500671, 0.20758023858070374, 0.960126519203186, 0.4636279046535492, 0.30457404255867004, 0.9937126040458679, 0.9351308345794678, 0.9997666478157043, 0.5673947930335999, 0.6862238645553589]], "normalized_score": [0.36365723609924316], "tokens": [["\u2581Feb", ".", "\u2581", "1", ",", "\u2581", "2", "0", "1", "7"]], "entropy": [[2.656033754348755, 1.3164219856262207, 0.24942927062511444, 1.2826002836227417, 2.360267400741577, 0.04726000130176544, 0.24144917726516724, 0.0029465113766491413, 0.705822765827179, 1.158202886581421]], "avg_entropy": [1.002043403731659], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "April 26, 2005", "score_of_biggest_cluster": 0.36365723609924316, "score_of_first_answer": 0.36365723609924316, "max_score_of_first_answer": 0.9997666478157043, "min_score_of_first_answer": 0.003973409533500671, "avg_score": 0.36365723609924316, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3433581413024728, "std_of_first_answer": 0.3433581413024728, "avg_entropy_of_first_answer": 1.002043403731659, "lex_sim": 1.0}
{"question": "who is the authority or governing body of mca", "answer": ["Ministry of Corporate Affairs"], "scores": [[0.1295216977596283, 0.9655236005783081, 0.72756427526474, 0.9988275170326233, 0.9916993379592896]], "normalized_score": [0.6179729104042053], "tokens": [["\u2581Ministry", "\u2581of", "\u2581Corpor", "ate", "\u2581Affairs"]], "entropy": [[5.103398323059082, 0.242684006690979, 1.4044079780578613, 0.010396279394626617, 0.056444741785526276]], "avg_entropy": [1.363466265797615], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Indian government", "score_of_biggest_cluster": 0.6179729104042053, "score_of_first_answer": 0.6179729104042053, "max_score_of_first_answer": 0.9988275170326233, "min_score_of_first_answer": 0.1295216977596283, "avg_score": 0.6179729104042053, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3321094907349598, "std_of_first_answer": 0.3321094907349598, "avg_entropy_of_first_answer": 1.363466265797615, "lex_sim": 1.0}
{"question": "what does fancy dress mean in the uk", "answer": ["masquerade"], "scores": [[0.018450820818543434, 0.9553796648979187, 0.99915611743927]], "normalized_score": [0.26018068194389343], "tokens": [["\u2581mas", "qu", "erade"]], "entropy": [[4.346343994140625, 0.23185546696186066, 0.008168840780854225]], "avg_entropy": [1.5287894339611132], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A costume", "score_of_biggest_cluster": 0.26018068194389343, "score_of_first_answer": 0.26018068194389343, "max_score_of_first_answer": 0.99915611743927, "min_score_of_first_answer": 0.018450820818543434, "avg_score": 0.26018068194389343, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4523438853454756, "std_of_first_answer": 0.4523438853454756, "avg_entropy_of_first_answer": 1.5287894339611132, "lex_sim": 1.0}
{"question": "who played jason in friday the 13th the final chapter", "answer": ["Steve Dash"], "scores": [[0.13591811060905457, 0.3316061198711395, 0.9520266056060791]], "normalized_score": [0.3500926196575165], "tokens": [["\u2581Steve", "\u2581D", "ash"]], "entropy": [[3.981660842895508, 3.1046271324157715, 0.33838123083114624]], "avg_entropy": [2.4748897353808084], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ted White", "score_of_biggest_cluster": 0.3500926196575165, "score_of_first_answer": 0.3500926196575165, "max_score_of_first_answer": 0.9520266056060791, "min_score_of_first_answer": 0.13591811060905457, "avg_score": 0.3500926196575165, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34789022082241927, "std_of_first_answer": 0.34789022082241927, "avg_entropy_of_first_answer": 2.4748897353808084, "lex_sim": 1.0}
{"question": "what was the religion in persia before islam", "answer": ["Zoroastrianism"], "scores": [[0.7314890623092651, 0.9935526847839355, 0.982545018196106, 0.9881436228752136, 0.8858709335327148]], "normalized_score": [0.9103079438209534], "tokens": [["\u2581Z", "oro", "ast", "rian", "ism"]], "entropy": [[1.7202460765838623, 0.05079088360071182, 0.1080639436841011, 0.08398182690143585, 0.4324321746826172]], "avg_entropy": [0.47910298109054567], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Zoroastrian religion", "score_of_biggest_cluster": 0.9103079438209534, "score_of_first_answer": 0.9103079438209534, "max_score_of_first_answer": 0.9935526847839355, "min_score_of_first_answer": 0.7314890623092651, "avg_score": 0.9103079438209534, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.10059711189984165, "std_of_first_answer": 0.10059711189984165, "avg_entropy_of_first_answer": 0.47910298109054567, "lex_sim": 1.0}
{"question": "how much of the world's diamonds does de beers own", "answer": ["80%"], "scores": [[0.7858882546424866, 0.14605700969696045, 0.5741207599639893, 0.8035654425621033]], "normalized_score": [0.4797079563140869], "tokens": [["\u2581", "8", "0", "%"]], "entropy": [[1.512363076210022, 2.048387050628662, 1.2848151922225952, 0.8771340847015381]], "avg_entropy": [1.4306748509407043], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "33% in 2013", "score_of_biggest_cluster": 0.4797079563140869, "score_of_first_answer": 0.4797079563140869, "max_score_of_first_answer": 0.8035654425621033, "min_score_of_first_answer": 0.14605700969696045, "avg_score": 0.4797079563140869, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2648988902319257, "std_of_first_answer": 0.2648988902319257, "avg_entropy_of_first_answer": 1.4306748509407043, "lex_sim": 1.0}
{"question": "when did frank sinatra first sing new york new york", "answer": ["September 8, 1940"], "scores": [[0.014444931410253048, 0.9665417075157166, 0.031147964298725128, 0.6654073596000671, 0.9886673092842102, 0.9988623857498169, 0.9993785619735718, 0.4779185354709625, 0.36477699875831604]], "normalized_score": [0.3325846493244171], "tokens": [["\u2581September", "\u2581", "8", ",", "\u2581", "1", "9", "4", "0"]], "entropy": [[1.5179078578948975, 0.1894177496433258, 1.4263395071029663, 0.7522168159484863, 0.06504354625940323, 0.009574360214173794, 0.0064290305599570274, 1.5284037590026855, 1.6968623399734497]], "avg_entropy": [0.7991327740665939], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1980", "score_of_biggest_cluster": 0.3325846493244171, "score_of_first_answer": 0.3325846493244171, "max_score_of_first_answer": 0.9993785619735718, "min_score_of_first_answer": 0.014444931410253048, "avg_score": 0.3325846493244171, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3863954801700783, "std_of_first_answer": 0.3863954801700783, "avg_entropy_of_first_answer": 0.7991327740665939, "lex_sim": 1.0}
{"question": "what is the purse for the senior open", "answer": ["the Senior British Open"], "scores": [[0.0063379621133208275, 0.10736702382564545, 0.30215156078338623, 0.9426279067993164]], "normalized_score": [0.11799029260873795], "tokens": [["\u2581the", "\u2581Senior", "\u2581British", "\u2581Open"]], "entropy": [[4.248072624206543, 5.303195953369141, 2.1832642555236816, 0.38692507147789]], "avg_entropy": [3.030364476144314], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "$2 million in 2011", "score_of_biggest_cluster": 0.11799029260873795, "score_of_first_answer": 0.11799029260873795, "max_score_of_first_answer": 0.9426279067993164, "min_score_of_first_answer": 0.0063379621133208275, "avg_score": 0.11799029260873795, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36401949855557425, "std_of_first_answer": 0.36401949855557425, "avg_entropy_of_first_answer": 3.030364476144314, "lex_sim": 1.0}
{"question": "where is the oldest civilization known to man", "answer": ["Sumer"], "scores": [[0.4042178988456726, 0.9986336827278137]], "normalized_score": [0.6353468298912048], "tokens": [["\u2581S", "umer"]], "entropy": [[2.96962571144104, 0.015637218952178955]], "avg_entropy": [1.4926314651966095], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mesopotamia", "score_of_biggest_cluster": 0.6353468298912048, "score_of_first_answer": 0.6353468298912048, "max_score_of_first_answer": 0.9986336827278137, "min_score_of_first_answer": 0.4042178988456726, "avg_score": 0.6353468298912048, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.29720789194107056, "std_of_first_answer": 0.29720789194107056, "avg_entropy_of_first_answer": 1.4926314651966095, "lex_sim": 1.0}
{"question": "what does the m number mean on a pint glass", "answer": ["Measurement"], "scores": [[0.010216750204563141, 0.8333674073219299, 0.32899054884910583]], "normalized_score": [0.14096486568450928], "tokens": [["\u2581Me", "asure", "ment"]], "entropy": [[4.650287628173828, 0.6630104780197144, 1.700384497642517]], "avg_entropy": [2.3378942012786865], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "imperial fluid ounces", "score_of_biggest_cluster": 0.14096486568450928, "score_of_first_answer": 0.14096486568450928, "max_score_of_first_answer": 0.8333674073219299, "min_score_of_first_answer": 0.010216750204563141, "avg_score": 0.14096486568450928, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3388853877670049, "std_of_first_answer": 0.3388853877670049, "avg_entropy_of_first_answer": 2.3378942012786865, "lex_sim": 1.0}
{"question": "when did ole miss beat alabama in football", "answer": ["September 24, 2005"], "scores": [[0.060278311371803284, 0.9793187975883484, 0.41244542598724365, 0.11983945965766907, 0.6979591250419617, 0.9917994737625122, 0.49160081148147583, 0.9992647767066956, 0.30920884013175964, 0.6363105773925781]], "normalized_score": [0.4256493151187897], "tokens": [["\u2581September", "\u2581", "2", "4", ",", "\u2581", "2", "0", "0", "5"]], "entropy": [[1.757444977760315, 0.12815484404563904, 1.4080528020858765, 2.3400306701660156, 0.7039728164672852, 0.05551523715257645, 0.6959140300750732, 0.008073221892118454, 0.6364436149597168, 1.387326717376709]], "avg_entropy": [0.9120928931981325], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "October 3, 1970", "score_of_biggest_cluster": 0.4256493151187897, "score_of_first_answer": 0.4256493151187897, "max_score_of_first_answer": 0.9992647767066956, "min_score_of_first_answer": 0.060278311371803284, "avg_score": 0.4256493151187897, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33345731072113527, "std_of_first_answer": 0.33345731072113527, "avg_entropy_of_first_answer": 0.9120928931981325, "lex_sim": 1.0}
{"question": "who wrote put your hand in the hand of the man who stilled the water", "answer": ["Katherine Kidd"], "scores": [[0.02310783974826336, 0.8393322825431824, 0.24859049916267395, 0.0015488776843994856]], "normalized_score": [0.05227556452155113], "tokens": [["\u2581K", "atherine", "\u2581K", "idd"]], "entropy": [[5.553108215332031, 1.0880002975463867, 3.1204535961151123, 1.2438212633132935]], "avg_entropy": [2.751345843076706], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gene MacLellan", "score_of_biggest_cluster": 0.05227556452155113, "score_of_first_answer": 0.05227556452155113, "max_score_of_first_answer": 0.8393322825431824, "min_score_of_first_answer": 0.0015488776843994856, "avg_score": 0.05227556452155113, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3381397649630915, "std_of_first_answer": 0.3381397649630915, "avg_entropy_of_first_answer": 2.751345843076706, "lex_sim": 1.0}
{"question": "when was the canadian pacific railway started and finished", "answer": ["started (1881-1885) and finished (1885)"], "scores": [[0.013379819691181183, 0.0016238794196397066, 0.8171052932739258, 0.9816642999649048, 0.7233811020851135, 0.8309970498085022, 0.08196649700403214, 0.8448533415794373, 0.9568570852279663, 0.9798471927642822, 0.920476496219635, 0.5964787602424622, 0.1712474822998047, 0.846604585647583, 0.95912766456604, 0.9613176584243774, 0.704683244228363, 0.8540093302726746, 0.9318842887878418, 0.8535889387130737]], "normalized_score": [0.415831983089447], "tokens": [["\u2581started", "\u2581(", "1", "8", "8", "1", "-", "1", "8", "8", "5", ")", "\u2581and", "\u2581finished", "\u2581(", "1", "8", "8", "5", ")"]], "entropy": [[2.1420578956604004, 1.6662695407867432, 1.29208505153656, 0.1316428780555725, 0.670455813407898, 0.5791568756103516, 1.2737386226654053, 0.6471179723739624, 0.19043919444084167, 0.10942396521568298, 0.41286182403564453, 1.0872619152069092, 2.0025599002838135, 0.7173896431922913, 0.2712017893791199, 0.29582861065864563, 0.6272346377372742, 0.4474974274635315, 0.3723217844963074, 0.5460711717605591]], "avg_entropy": [0.7741308256983757], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "between 1881 and 1885", "score_of_biggest_cluster": 0.415831983089447, "score_of_first_answer": 0.415831983089447, "max_score_of_first_answer": 0.9816642999649048, "min_score_of_first_answer": 0.0016238794196397066, "avg_score": 0.415831983089447, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.332720212838344, "std_of_first_answer": 0.332720212838344, "avg_entropy_of_first_answer": 0.7741308256983757, "lex_sim": 1.0}
{"question": "who did america declare war on in ww1", "answer": ["Germany"], "scores": [[0.685576319694519]], "normalized_score": [0.685576319694519], "tokens": [["\u2581Germany"]], "entropy": [[1.6546037197113037]], "avg_entropy": [1.6546037197113037], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Austria-Hungary", "score_of_biggest_cluster": 0.685576319694519, "score_of_first_answer": 0.685576319694519, "max_score_of_first_answer": 0.685576319694519, "min_score_of_first_answer": 0.685576319694519, "avg_score": 0.685576319694519, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.6546037197113037, "lex_sim": 1.0}
{"question": "who has the most followers in the world on instagram", "answer": ["David Beckham"], "scores": [[0.007026765961199999, 0.4153132438659668, 0.994158148765564]], "normalized_score": [0.14262498915195465], "tokens": [["\u2581David", "\u2581Beck", "ham"]], "entropy": [[4.029575347900391, 3.932086706161499, 0.051107242703437805]], "avg_entropy": [2.670923098921776], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Instagram's own account", "score_of_biggest_cluster": 0.14262498915195465, "score_of_first_answer": 0.14262498915195465, "max_score_of_first_answer": 0.994158148765564, "min_score_of_first_answer": 0.007026765961199999, "avg_score": 0.14262498915195465, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40499487498614095, "std_of_first_answer": 0.40499487498614095, "avg_entropy_of_first_answer": 2.670923098921776, "lex_sim": 1.0}
{"question": "when did the jurassic park movies come out", "answer": ["1993, 1997 and 2015"], "scores": [[0.8244950175285339, 0.9804314970970154, 0.9945266842842102, 0.9851388335227966, 0.8939787149429321, 0.542087733745575, 0.9426262378692627, 0.9638341069221497, 0.999405026435852, 0.9994311928749084, 0.9474145174026489, 0.04615504667162895, 0.9963733553886414, 0.910536527633667, 0.99934321641922, 0.42642268538475037, 0.955811083316803]], "normalized_score": [0.7373405694961548], "tokens": [["\u2581", "1", "9", "9", "3", ",", "\u2581", "1", "9", "9", "7", "\u2581and", "\u2581", "2", "0", "1", "5"]], "entropy": [[1.103991985321045, 0.12744837999343872, 0.05151183158159256, 0.09312918037176132, 0.4641006588935852, 1.3405537605285645, 0.31539660692214966, 0.17275093495845795, 0.00672170938923955, 0.005931737832725048, 0.2778523862361908, 0.3996084928512573, 0.038794536143541336, 0.303488552570343, 0.007820225320756435, 0.6847481727600098, 0.2642415463924408]], "avg_entropy": [0.3328288645921823], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jurassic Park (1993)", "score_of_biggest_cluster": 0.7373405694961548, "score_of_first_answer": 0.7373405694961548, "max_score_of_first_answer": 0.9994311928749084, "min_score_of_first_answer": 0.04615504667162895, "avg_score": 0.7373405694961548, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2559074220523266, "std_of_first_answer": 0.2559074220523266, "avg_entropy_of_first_answer": 0.3328288645921823, "lex_sim": 1.0}
{"question": "who ordered the forbidden city to be built", "answer": ["Hongwu Emperor"], "scores": [[0.012265287339687347, 0.8648041486740112, 0.983727216720581, 0.32179170846939087]], "normalized_score": [0.24071946740150452], "tokens": [["\u2581Hong", "w", "u", "\u2581Emperor"]], "entropy": [[3.497258186340332, 0.8440263867378235, 0.13184289634227753, 1.1046220064163208]], "avg_entropy": [1.3944373689591885], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Zhu Di", "score_of_biggest_cluster": 0.24071946740150452, "score_of_first_answer": 0.24071946740150452, "max_score_of_first_answer": 0.983727216720581, "min_score_of_first_answer": 0.012265287339687347, "avg_score": 0.24071946740150452, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3963530007387183, "std_of_first_answer": 0.3963530007387183, "avg_entropy_of_first_answer": 1.3944373689591885, "lex_sim": 1.0}
{"question": "where are haploid cells found in the human body", "answer": ["sperm and egg"], "scores": [[0.015328431501984596, 0.9276421666145325, 0.20725677907466888, 0.7009459137916565]], "normalized_score": [0.21319052577018738], "tokens": [["\u2581s", "perm", "\u2581and", "\u2581egg"]], "entropy": [[3.8562684059143066, 0.3109174966812134, 1.5424867868423462, 1.2123913764953613]], "avg_entropy": [1.7305160164833069], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "sperm", "score_of_biggest_cluster": 0.21319052577018738, "score_of_first_answer": 0.21319052577018738, "max_score_of_first_answer": 0.9276421666145325, "min_score_of_first_answer": 0.015328431501984596, "avg_score": 0.21319052577018738, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36685312398669156, "std_of_first_answer": 0.36685312398669156, "avg_entropy_of_first_answer": 1.7305160164833069, "lex_sim": 1.0}
{"question": "when do the new episodes of supernatural start", "answer": ["January 25"], "scores": [[0.027053317055106163, 0.9627805948257446, 0.7095685601234436, 0.1872195601463318]], "normalized_score": [0.24253441393375397], "tokens": [["\u2581January", "\u2581", "2", "5"]], "entropy": [[2.504707098007202, 0.20172034204006195, 0.8930121064186096, 2.085242748260498]], "avg_entropy": [1.421170573681593], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "May\u00a010,\u00a02018", "score_of_biggest_cluster": 0.24253441393375397, "score_of_first_answer": 0.24253441393375397, "max_score_of_first_answer": 0.9627805948257446, "min_score_of_first_answer": 0.027053317055106163, "avg_score": 0.24253441393375397, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37959892910304094, "std_of_first_answer": 0.37959892910304094, "avg_entropy_of_first_answer": 1.421170573681593, "lex_sim": 1.0}
{"question": "what states do not allow daylight savings time", "answer": ["Arizona, Hawaii, and parts of Indiana"], "scores": [[0.3040768504142761, 0.7759780287742615, 0.824527382850647, 0.9974685907363892, 0.831713080406189, 0.6086481213569641, 0.056600410491228104, 0.9966707825660706, 0.7638264894485474]], "normalized_score": [0.5448709726333618], "tokens": [["\u2581Arizona", ",", "\u2581Hawai", "i", ",", "\u2581and", "\u2581parts", "\u2581of", "\u2581Indiana"]], "entropy": [[3.1366305351257324, 0.8331583738327026, 1.0892173051834106, 0.024450579658150673, 0.6213311553001404, 2.127021312713623, 3.646915912628174, 0.035491347312927246, 1.3433560132980347]], "avg_entropy": [1.4286191705614328], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Navajo", "score_of_biggest_cluster": 0.5448709726333618, "score_of_first_answer": 0.5448709726333618, "max_score_of_first_answer": 0.9974685907363892, "min_score_of_first_answer": 0.056600410491228104, "avg_score": 0.5448709726333618, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29740979635748316, "std_of_first_answer": 0.29740979635748316, "avg_entropy_of_first_answer": 1.4286191705614328, "lex_sim": 1.0}
{"question": "what nfl team is robert griffin the third playing for", "answer": ["Washington Redskins"], "scores": [[0.5932523608207703, 0.7022519111633301, 0.9983810186386108, 0.9976867437362671]], "normalized_score": [0.8026121854782104], "tokens": [["\u2581Washington", "\u2581Red", "sk", "ins"]], "entropy": [[1.9553701877593994, 0.8854342103004456, 0.016223059967160225, 0.018835658207535744]], "avg_entropy": [0.7189657790586352], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "currently a free agent", "score_of_biggest_cluster": 0.8026121854782104, "score_of_first_answer": 0.8026121854782104, "max_score_of_first_answer": 0.9983810186386108, "min_score_of_first_answer": 0.5932523608207703, "avg_score": 0.8026121854782104, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17933069523466097, "std_of_first_answer": 0.17933069523466097, "avg_entropy_of_first_answer": 0.7189657790586352, "lex_sim": 1.0}
{"question": "who is the girl that died in eyes wide shut", "answer": ["Kirsten Dunst"], "scores": [[0.01414527278393507, 0.052966922521591187, 0.9834064245223999, 0.9160116314888, 0.9996423721313477]], "normalized_score": [0.23217685520648956], "tokens": [["\u2581K", "irst", "en", "\u2581D", "unst"]], "entropy": [[4.616477012634277, 3.196211814880371, 0.11492255330085754, 0.7201802730560303, 0.004725283477455378]], "avg_entropy": [1.7305033874697984], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mandy", "score_of_biggest_cluster": 0.23217685520648956, "score_of_first_answer": 0.23217685520648956, "max_score_of_first_answer": 0.9996423721313477, "min_score_of_first_answer": 0.01414527278393507, "avg_score": 0.23217685520648956, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4579999899956475, "std_of_first_answer": 0.4579999899956475, "avg_entropy_of_first_answer": 1.7305033874697984, "lex_sim": 1.0}
{"question": "how many seasons of the rugrats are there", "answer": ["there are two seasons"], "scores": [[0.002252463949844241, 0.5934194326400757, 0.06466877460479736, 0.3055357038974762]], "normalized_score": [0.07168757915496826], "tokens": [["\u2581there", "\u2581are", "\u2581two", "\u2581seasons"]], "entropy": [[1.4319648742675781, 1.4117354154586792, 2.330598831176758, 3.0958340167999268]], "avg_entropy": [2.0675332844257355], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "9 seasons", "score_of_biggest_cluster": 0.07168757915496826, "score_of_first_answer": 0.07168757915496826, "max_score_of_first_answer": 0.5934194326400757, "min_score_of_first_answer": 0.002252463949844241, "avg_score": 0.07168757915496826, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23262444003340346, "std_of_first_answer": 0.23262444003340346, "avg_entropy_of_first_answer": 2.0675332844257355, "lex_sim": 1.0}
{"question": "who wrote the theme song for mission impossible", "answer": ["Lalo Schiffrin"], "scores": [[0.5914457440376282, 0.997343122959137, 0.9778553247451782, 0.9804353713989258, 0.09842681139707565, 0.9975718855857849]], "normalized_score": [0.6176632642745972], "tokens": [["\u2581L", "alo", "\u2581Sch", "if", "fr", "in"]], "entropy": [[2.4736294746398926, 0.025057688355445862, 0.12591461837291718, 0.1400369107723236, 0.3908638656139374, 0.0237174853682518]], "avg_entropy": [0.5298700071871281], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lalo Schifrin", "score_of_biggest_cluster": 0.6176632642745972, "score_of_first_answer": 0.6176632642745972, "max_score_of_first_answer": 0.9975718855857849, "min_score_of_first_answer": 0.09842681139707565, "avg_score": 0.6176632642745972, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3351030046909612, "std_of_first_answer": 0.3351030046909612, "avg_entropy_of_first_answer": 0.5298700071871281, "lex_sim": 1.0}
{"question": "who played kelly taylor's mom on 90210", "answer": ["Barbara Williams"], "scores": [[0.1364542692899704, 0.0257719699293375]], "normalized_score": [0.05930174142122269], "tokens": [["\u2581Barbara", "\u2581Williams"]], "entropy": [[4.334056854248047, 2.561426877975464]], "avg_entropy": [3.4477418661117554], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ann Gillespie", "score_of_biggest_cluster": 0.05930174142122269, "score_of_first_answer": 0.05930174142122269, "max_score_of_first_answer": 0.1364542692899704, "min_score_of_first_answer": 0.0257719699293375, "avg_score": 0.05930174142122269, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05534114968031645, "std_of_first_answer": 0.05534114968031645, "avg_entropy_of_first_answer": 3.4477418661117554, "lex_sim": 1.0}
{"question": "describe the various uses of forests to human beings", "answer": ["forests provide products that humans require including wood, animals such as timber and furs, food such as fruits and nuts, and medicine."], "scores": [[0.0512695387005806, 0.9000470638275146, 0.33437925577163696, 0.004738213494420052, 0.09150102734565735, 0.0872735008597374, 0.023851590231060982, 0.02258351445198059, 0.11911014467477798, 0.5877522826194763, 0.015005105175077915, 0.013382512144744396, 0.9954919815063477, 0.023410536348819733, 0.9961663484573364, 0.2378404438495636, 0.011945945210754871, 0.34929126501083374, 0.8025316596031189, 0.06916831433773041, 0.2755388617515564, 0.9951435327529907, 0.13992226123809814, 0.9979044198989868, 0.4044238030910492, 0.6201319098472595, 0.9997593760490417, 0.8454679846763611, 0.3259344696998596, 0.12094958126544952, 0.29189297556877136, 0.0009883744642138481, 0.00010514612949918956, 0.265024334192276, 0.6357898712158203, 0.945722222328186, 0.9991349577903748, 0.551539957523346, 0.2462148219347, 0.2757778763771057, 0.07473310828208923, 0.05080583319067955, 0.059997547417879105]], "normalized_score": [0.13695161044597626], "tokens": [["\u2581for", "ests", "\u2581provide", "\u2581products", "\u2581that", "\u2581humans", "\u2581require", "\u2581including", "\u2581wood", ",", "\u2581animals", "\u2581such", "\u2581as", "\u2581tim", "ber", "\u2581and", "\u2581f", "urs", ",", "\u2581food", "\u2581such", "\u2581as", "\u2581f", "ruits", "\u2581and", "\u2581n", "uts", ",", "\u2581and", "\u2581medicine", ".", "\u2581F", "RE", "QU", "EN", "C", "Y", ":", "\u2581", "1", ".", "4", "2"]], "entropy": [[3.8407182693481445, 0.7795453071594238, 2.174696683883667, 3.887413263320923, 2.4465324878692627, 2.754715919494629, 1.9416378736495972, 2.3746750354766846, 3.4512786865234375, 1.502237319946289, 4.110639572143555, 1.1606444120407104, 0.047452036291360855, 3.5743298530578613, 0.03678714483976364, 2.191481351852417, 4.006576061248779, 1.3644694089889526, 1.0412616729736328, 3.867508888244629, 2.5345940589904785, 0.05233444646000862, 3.0140390396118164, 0.0210212804377079, 0.8951947689056396, 1.9040944576263428, 0.003417744766920805, 0.7464129328727722, 3.4811811447143555, 4.332392692565918, 2.2320163249969482, 2.605759859085083, 2.9275200366973877, 2.6142725944519043, 0.6884548664093018, 0.25064799189567566, 0.009150879457592964, 2.175062417984009, 5.16166877746582, 2.1790947914123535, 3.0077853202819824, 3.3658242225646973, 3.2652180194854736]], "avg_entropy": [2.186552556220789], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "a source of lumber", "score_of_biggest_cluster": 0.13695161044597626, "score_of_first_answer": 0.13695161044597626, "max_score_of_first_answer": 0.9997593760490417, "min_score_of_first_answer": 0.00010514612949918956, "avg_score": 0.13695161044597626, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36195622693597634, "std_of_first_answer": 0.36195622693597634, "avg_entropy_of_first_answer": 2.186552556220789, "lex_sim": 1.0}
{"question": "who was the captain of the mayflower when it took the pilgrims to the new world", "answer": ["Christopher Jones"], "scores": [[0.5780683159828186, 0.9688197374343872]], "normalized_score": [0.7483608722686768], "tokens": [["\u2581Christopher", "\u2581Jones"]], "entropy": [[1.8333876132965088, 0.25304359197616577]], "avg_entropy": [1.0432156026363373], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Captain Jones", "score_of_biggest_cluster": 0.7483608722686768, "score_of_first_answer": 0.7483608722686768, "max_score_of_first_answer": 0.9688197374343872, "min_score_of_first_answer": 0.5780683159828186, "avg_score": 0.7483608722686768, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1953757107257843, "std_of_first_answer": 0.1953757107257843, "avg_entropy_of_first_answer": 1.0432156026363373, "lex_sim": 1.0}
{"question": "where will the cricket world cup 2019 be held", "answer": ["England and Wales"], "scores": [[0.668768584728241, 0.37998488545417786, 0.9946146607398987]], "normalized_score": [0.6322647929191589], "tokens": [["\u2581England", "\u2581and", "\u2581Wales"]], "entropy": [[1.915999174118042, 1.2286131381988525, 0.0547894611954689]], "avg_entropy": [1.0664672578374546], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "England and Wales", "score_of_biggest_cluster": 0.6322647929191589, "score_of_first_answer": 0.6322647929191589, "max_score_of_first_answer": 0.9946146607398987, "min_score_of_first_answer": 0.37998488545417786, "avg_score": 0.6322647929191589, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.25107357285739157, "std_of_first_answer": 0.25107357285739157, "avg_entropy_of_first_answer": 1.0664672578374546, "lex_sim": 1.0}
{"question": "who has played in the most masters tournaments", "answer": ["Sam Snead"], "scores": [[0.03135044500231743, 0.7112029790878296, 0.9966193437576294, 0.99962317943573]], "normalized_score": [0.3860563635826111], "tokens": [["\u2581Sam", "\u2581S", "ne", "ad"]], "entropy": [[3.8242900371551514, 1.3416475057601929, 0.023326708003878593, 0.004014035686850548]], "avg_entropy": [1.2983195716515183], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gary Player", "score_of_biggest_cluster": 0.3860563635826111, "score_of_first_answer": 0.3860563635826111, "max_score_of_first_answer": 0.99962317943573, "min_score_of_first_answer": 0.03135044500231743, "avg_score": 0.3860563635826111, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39498048523572427, "std_of_first_answer": 0.39498048523572427, "avg_entropy_of_first_answer": 1.2983195716515183, "lex_sim": 1.0}
{"question": "who were the nationalist in the spanish civil war", "answer": ["Nationalists (Nationalists)"], "scores": [[0.2325785607099533, 0.8097654581069946, 0.017250755801796913, 0.053863562643527985, 0.5688632726669312, 0.6099515557289124]], "normalized_score": [0.19825425744056702], "tokens": [["\u2581National", "ists", "\u2581(", "National", "ists", ")"]], "entropy": [[3.572251081466675, 0.7051049470901489, 0.4991264343261719, 4.24353551864624, 1.5518512725830078, 2.4302759170532227]], "avg_entropy": [2.1670241951942444], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the CEDA", "score_of_biggest_cluster": 0.19825425744056702, "score_of_first_answer": 0.19825425744056702, "max_score_of_first_answer": 0.8097654581069946, "min_score_of_first_answer": 0.017250755801796913, "avg_score": 0.19825425744056702, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2980227531379671, "std_of_first_answer": 0.2980227531379671, "avg_entropy_of_first_answer": 2.1670241951942444, "lex_sim": 1.0}
{"question": "who was the first british team to win the european cup", "answer": ["Nottingham Forest"], "scores": [[0.10972284525632858, 0.9844051003456116, 0.99598628282547, 0.9623327255249023]], "normalized_score": [0.5672341585159302], "tokens": [["\u2581Not", "ting", "ham", "\u2581Forest"]], "entropy": [[3.4941675662994385, 0.12711381912231445, 0.03814125061035156, 0.2532083988189697]], "avg_entropy": [0.9781577587127686], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Celtic", "score_of_biggest_cluster": 0.5672341585159302, "score_of_first_answer": 0.5672341585159302, "max_score_of_first_answer": 0.99598628282547, "min_score_of_first_answer": 0.10972284525632858, "avg_score": 0.5672341585159302, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37742792453563045, "std_of_first_answer": 0.37742792453563045, "avg_entropy_of_first_answer": 0.9781577587127686, "lex_sim": 1.0}
{"question": "how many casinos are in atlantic city new jersey", "answer": ["Seven"], "scores": [[0.05836298316717148]], "normalized_score": [0.05836297944188118], "tokens": [["\u2581Seven"]], "entropy": [[3.166661024093628]], "avg_entropy": [3.166661024093628], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "eleven", "score_of_biggest_cluster": 0.05836297944188118, "score_of_first_answer": 0.05836297944188118, "max_score_of_first_answer": 0.05836298316717148, "min_score_of_first_answer": 0.05836298316717148, "avg_score": 0.05836297944188118, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.166661024093628, "lex_sim": 1.0}
{"question": "when did the continental congress vote to adopt the declaration of independence", "answer": ["July 1, 1776"], "scores": [[0.8393099904060364, 0.9826086759567261, 0.08786404877901077, 0.355960488319397, 0.9775851368904114, 0.9980478286743164, 0.9971816539764404, 0.9990478157997131, 0.9928901195526123]], "normalized_score": [0.6634176969528198], "tokens": [["\u2581July", "\u2581", "1", ",", "\u2581", "1", "7", "7", "6"]], "entropy": [[0.8793256282806396, 0.11727498471736908, 0.9351168274879456, 1.629713535308838, 0.12313829362392426, 0.015916824340820312, 0.022668777033686638, 0.009713508188724518, 0.05371760204434395]], "avg_entropy": [0.42073177566958797], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "July 2, 1776", "score_of_biggest_cluster": 0.6634176969528198, "score_of_first_answer": 0.6634176969528198, "max_score_of_first_answer": 0.9990478157997131, "min_score_of_first_answer": 0.08786404877901077, "avg_score": 0.6634176969528198, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32068732289996577, "std_of_first_answer": 0.32068732289996577, "avg_entropy_of_first_answer": 0.42073177566958797, "lex_sim": 1.0}
{"question": "who was the captain of the mayflower which brought the pilgrims to plymouth", "answer": ["Christopher Jones"], "scores": [[0.4824256896972656, 0.9494717121124268]], "normalized_score": [0.6767935752868652], "tokens": [["\u2581Christopher", "\u2581Jones"]], "entropy": [[2.072052001953125, 0.358852356672287]], "avg_entropy": [1.215452179312706], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Master Christopher Jones", "score_of_biggest_cluster": 0.6767935752868652, "score_of_first_answer": 0.6767935752868652, "max_score_of_first_answer": 0.9494717121124268, "min_score_of_first_answer": 0.4824256896972656, "avg_score": 0.6767935752868652, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23352301120758057, "std_of_first_answer": 0.23352301120758057, "avg_entropy_of_first_answer": 1.215452179312706, "lex_sim": 1.0}
{"question": "how many episodes in season 4 of last ship", "answer": ["ten"], "scores": [[0.006420073565095663]], "normalized_score": [0.006420074496418238], "tokens": [["\u2581ten"]], "entropy": [[0.9280401468276978]], "avg_entropy": [0.9280401468276978], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "10", "score_of_biggest_cluster": 0.006420074496418238, "score_of_first_answer": 0.006420074496418238, "max_score_of_first_answer": 0.006420073565095663, "min_score_of_first_answer": 0.006420073565095663, "avg_score": 0.006420074496418238, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.9280401468276978, "lex_sim": 1.0}
{"question": "how many times has the saints won the super bowl", "answer": ["6"], "scores": [[0.198220357298851, 0.008541988208889961]], "normalized_score": [0.04114845395088196], "tokens": [["\u2581", "6"]], "entropy": [[3.027336359024048, 1.7573859691619873]], "avg_entropy": [2.3923611640930176], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1", "score_of_biggest_cluster": 0.04114845395088196, "score_of_first_answer": 0.04114845395088196, "max_score_of_first_answer": 0.198220357298851, "min_score_of_first_answer": 0.008541988208889961, "avg_score": 0.04114845395088196, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09483918454498053, "std_of_first_answer": 0.09483918454498053, "avg_entropy_of_first_answer": 2.3923611640930176, "lex_sim": 1.0}
{"question": "who plays the short cop on will and grace", "answer": ["Shelley Morrison"], "scores": [[0.054367102682590485, 0.9240830540657043, 0.9994694590568542, 0.9440190196037292, 0.9967893362045288, 0.998908281326294]], "normalized_score": [0.6011567711830139], "tokens": [["\u2581Sh", "elle", "y", "\u2581Mor", "r", "ison"]], "entropy": [[3.372541904449463, 0.377424418926239, 0.007374714594334364, 0.3913634419441223, 0.03203628212213516, 0.01130575593560934]], "avg_entropy": [0.6986744196619838], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ryan Pinkston", "score_of_biggest_cluster": 0.6011567711830139, "score_of_first_answer": 0.6011567711830139, "max_score_of_first_answer": 0.9994694590568542, "min_score_of_first_answer": 0.054367102682590485, "avg_score": 0.6011567711830139, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.343481860477376, "std_of_first_answer": 0.343481860477376, "avg_entropy_of_first_answer": 0.6986744196619838, "lex_sim": 1.0}
{"question": "where was the original planet of the apes filmed at", "answer": ["Aviara Golf Club, Carlsbad, California, USA"], "scores": [[0.007673761807382107, 0.004826785996556282, 0.17586299777030945, 0.14585614204406738, 0.9997945427894592, 0.5091502070426941, 0.2714432179927826, 0.5937938094139099, 0.9918143153190613, 0.9991186261177063, 0.8700565099716187, 0.8818606734275818, 0.23980098962783813, 0.49045631289482117]], "normalized_score": [0.26144322752952576], "tokens": [["\u2581A", "vi", "ara", "\u2581G", "olf", "\u2581Club", ",", "\u2581Car", "ls", "bad", ",", "\u2581California", ",", "\u2581USA"]], "entropy": [[5.828654766082764, 3.9222359657287598, 2.8865699768066406, 2.5944526195526123, 0.002734237350523472, 0.9751244187355042, 1.6823804378509521, 2.0609030723571777, 0.048337120562791824, 0.010241101495921612, 0.5571842193603516, 0.5206679105758667, 0.8068506121635437, 1.205126404762268]], "avg_entropy": [1.650104490241834], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Utah", "score_of_biggest_cluster": 0.26144322752952576, "score_of_first_answer": 0.26144322752952576, "max_score_of_first_answer": 0.9997945427894592, "min_score_of_first_answer": 0.004826785996556282, "avg_score": 0.26144322752952576, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36529631556440084, "std_of_first_answer": 0.36529631556440084, "avg_entropy_of_first_answer": 1.650104490241834, "lex_sim": 1.0}
{"question": "what disney cartoon character's middle name is fauntleroy", "answer": ["Timothy"], "scores": [[0.10106566548347473, 0.6744257807731628]], "normalized_score": [0.2610771656036377], "tokens": [["\u2581Tim", "othy"]], "entropy": [[4.898741722106934, 1.329914927482605]], "avg_entropy": [3.1143283247947693], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Donald Duck", "score_of_biggest_cluster": 0.2610771656036377, "score_of_first_answer": 0.2610771656036377, "max_score_of_first_answer": 0.6744257807731628, "min_score_of_first_answer": 0.10106566548347473, "avg_score": 0.2610771656036377, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28668005764484406, "std_of_first_answer": 0.28668005764484406, "avg_entropy_of_first_answer": 3.1143283247947693, "lex_sim": 1.0}
{"question": "where are the san juan mountains in new mexico", "answer": ["New Mexico"], "scores": [[0.03132655844092369, 0.9672243595123291]], "normalized_score": [0.174068421125412], "tokens": [["\u2581New", "\u2581Mexico"]], "entropy": [[4.789775848388672, 0.2652037441730499]], "avg_entropy": [2.527489796280861], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "northwestern New Mexico", "score_of_biggest_cluster": 0.174068421125412, "score_of_first_answer": 0.174068421125412, "max_score_of_first_answer": 0.9672243595123291, "min_score_of_first_answer": 0.03132655844092369, "avg_score": 0.174068421125412, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4679489005357027, "std_of_first_answer": 0.4679489005357027, "avg_entropy_of_first_answer": 2.527489796280861, "lex_sim": 1.0}
{"question": "what is the rank of pakistan in population", "answer": ["6th"], "scores": [[0.7798634171485901, 0.3834604322910309, 0.9083072543144226]], "normalized_score": [0.6476255059242249], "tokens": [["\u2581", "6", "th"]], "entropy": [[1.5204777717590332, 1.744508981704712, 0.4151153564453125]], "avg_entropy": [1.2267007033030193], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "5", "score_of_biggest_cluster": 0.6476255059242249, "score_of_first_answer": 0.6476255059242249, "max_score_of_first_answer": 0.9083072543144226, "min_score_of_first_answer": 0.3834604322910309, "avg_score": 0.6476255059242249, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.223382415576199, "std_of_first_answer": 0.223382415576199, "avg_entropy_of_first_answer": 1.2267007033030193, "lex_sim": 1.0}
{"question": "where did the battle of issus take place", "answer": ["Syria"], "scores": [[0.4091621935367584, 0.9391860365867615]], "normalized_score": [0.6199027299880981], "tokens": [["\u2581Sy", "ria"]], "entropy": [[3.2143607139587402, 0.2937653660774231]], "avg_entropy": [1.7540630400180817], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "southern Anatolia", "score_of_biggest_cluster": 0.6199027299880981, "score_of_first_answer": 0.6199027299880981, "max_score_of_first_answer": 0.9391860365867615, "min_score_of_first_answer": 0.4091621935367584, "avg_score": 0.6199027299880981, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2650119215250015, "std_of_first_answer": 0.2650119215250015, "avg_entropy_of_first_answer": 1.7540630400180817, "lex_sim": 1.0}
{"question": "when was coffee first made into a drink", "answer": ["15th Century"], "scores": [[0.5738798379898071, 0.9299061298370361, 0.36371558904647827, 0.6173989772796631, 0.12000773847103119]], "normalized_score": [0.42811399698257446], "tokens": [["\u2581", "1", "5", "th", "\u2581Century"]], "entropy": [[2.8483972549438477, 0.3806154727935791, 1.791697382926941, 1.350163459777832, 0.721287190914154]], "avg_entropy": [1.4184321522712708], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "15th century", "score_of_biggest_cluster": 0.42811399698257446, "score_of_first_answer": 0.42811399698257446, "max_score_of_first_answer": 0.9299061298370361, "min_score_of_first_answer": 0.12000773847103119, "avg_score": 0.42811399698257446, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.27012087707433025, "std_of_first_answer": 0.27012087707433025, "avg_entropy_of_first_answer": 1.4184321522712708, "lex_sim": 1.0}
{"question": "who won the american league east in 2017", "answer": ["Boston Red Sox Grammar and spelling are not checked at the time of speech synthesis."], "scores": [[0.3973676264286041, 0.9691500067710876, 0.9812135100364685, 0.9967201352119446, 1.0908476788529242e-08, 0.1089792549610138, 0.01802190952003002, 0.07784570753574371, 0.9984690546989441, 0.1225028783082962, 0.15470509231090546, 0.09562799334526062, 0.005186484195291996, 0.18757015466690063, 0.33927005529403687, 0.8793239593505859, 0.01023562066257, 0.04992644116282463, 0.9730207920074463, 0.631423830986023, 0.01631556823849678, 0.8531531691551208, 0.06978491693735123, 0.3569573163986206, 0.012348842807114124, 0.2494392842054367, 0.0563628263771534, 0.06743454933166504, 0.00911740493029356, 0.9958551526069641, 0.9987375140190125, 0.9997435212135315, 0.9228591918945312, 0.8442596793174744, 0.6760300397872925, 0.0005414669867604971, 0.0029740629252046347, 0.5865049958229065, 0.01921779476106167, 0.11545970290899277, 0.022278564050793648, 0.09849739819765091, 0.05882296711206436, 0.017662884667515755, 0.9462510347366333]], "normalized_score": [0.09007856994867325], "tokens": [["\u2581Boston", "\u2581Red", "\u2581So", "x", "\u2581Gram", "mar", "\u2581and", "\u2581sp", "elling", "\u2581are", "\u2581not", "\u2581checked", "\u2581at", "\u2581the", "\u2581time", "\u2581of", "\u2581speech", "\u2581synth", "esis", ".", "\u2581Spe", "ech", "\u2581to", "\u2581Text", "\u2581trans", "cript", "\u2581is", "\u2581not", "\u2581", "1", "0", "0", "%", "\u2581accurate", ".", "\u2581Ask", "\u2581more", "\u2581questions", "\u2581like", "\u2581these", ",", "\u2581or", "\u2581try", "\u2581one", "\u2581of"]], "entropy": [[2.190047025680542, 0.18219122290611267, 0.12677733600139618, 0.02613258734345436, 0.1525261402130127, 2.5821545124053955, 3.72969126701355, 4.095676422119141, 0.01571003906428814, 4.252078533172607, 4.704429626464844, 4.715977668762207, 2.710881233215332, 2.3811206817626953, 3.240846633911133, 0.7546635270118713, 4.699858665466309, 3.311169147491455, 0.14249461889266968, 1.9827244281768799, 4.121756076812744, 0.6583852767944336, 4.432188987731934, 0.8656449913978577, 4.756975173950195, 0.9062379598617554, 1.934464454650879, 3.7009525299072266, 3.349830389022827, 0.04028209298849106, 0.0137704536318779, 0.0034469941165298223, 0.5095144510269165, 1.0772788524627686, 1.7115975618362427, 3.9594037532806396, 4.005759239196777, 2.3269195556640625, 3.239943504333496, 2.2590785026550293, 3.282764434814453, 4.449044704437256, 4.8936309814453125, 4.604888916015625, 0.4138439893722534]], "avg_entropy": [2.3892167809884994], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Yankees", "score_of_biggest_cluster": 0.09007856994867325, "score_of_first_answer": 0.09007856994867325, "max_score_of_first_answer": 0.9997435212135315, "min_score_of_first_answer": 1.0908476788529242e-08, "avg_score": 0.09007856994867325, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39941650935280565, "std_of_first_answer": 0.39941650935280565, "avg_entropy_of_first_answer": 2.3892167809884994, "lex_sim": 1.0}
{"question": "public companies are part of what economic sector", "answer": ["financial sector."], "scores": [[0.04663165658712387, 0.07741860300302505, 0.001807680120691657]], "normalized_score": [0.0186874158680439], "tokens": [["\u2581financial", "\u2581sector", "."]], "entropy": [[4.296561241149902, 1.4116251468658447, 0.18947771191596985]], "avg_entropy": [1.9658880333105724], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "public sector", "score_of_biggest_cluster": 0.0186874158680439, "score_of_first_answer": 0.0186874158680439, "max_score_of_first_answer": 0.07741860300302505, "min_score_of_first_answer": 0.001807680120691657, "avg_score": 0.0186874158680439, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.031044836026118038, "std_of_first_answer": 0.031044836026118038, "avg_entropy_of_first_answer": 1.9658880333105724, "lex_sim": 1.0}
{"question": "when is the running of the bulls held in spain", "answer": ["July 7"], "scores": [[0.6214559674263, 0.5756804347038269, 0.747310221195221]], "normalized_score": [0.6442151069641113], "tokens": [["\u2581July", "\u2581", "7"]], "entropy": [[2.3485631942749023, 1.080557107925415, 0.8294380903244019]], "avg_entropy": [1.4195194641749065], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "6\u201314 July", "score_of_biggest_cluster": 0.6442151069641113, "score_of_first_answer": 0.6442151069641113, "max_score_of_first_answer": 0.747310221195221, "min_score_of_first_answer": 0.5756804347038269, "avg_score": 0.6442151069641113, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07256527804431573, "std_of_first_answer": 0.07256527804431573, "avg_entropy_of_first_answer": 1.4195194641749065, "lex_sim": 1.0}
{"question": "where did the california gold rush take place", "answer": ["California"], "scores": [[0.12644028663635254]], "normalized_score": [0.12644030153751373], "tokens": [["\u2581California"]], "entropy": [[3.504230499267578]], "avg_entropy": [3.504230499267578], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Northern California", "score_of_biggest_cluster": 0.12644030153751373, "score_of_first_answer": 0.12644030153751373, "max_score_of_first_answer": 0.12644028663635254, "min_score_of_first_answer": 0.12644028663635254, "avg_score": 0.12644030153751373, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.504230499267578, "lex_sim": 1.0}
{"question": "book series about the end of the world", "answer": ["In a World Just Right"], "scores": [[0.002758242655545473, 0.04435957223176956, 0.12941858172416687, 0.17010457813739777, 0.9527230262756348]], "normalized_score": [0.07618331164121628], "tokens": [["\u2581In", "\u2581a", "\u2581World", "\u2581Just", "\u2581Right"]], "entropy": [[4.050923824310303, 3.232776403427124, 4.2920989990234375, 3.3335018157958984, 0.3118095397949219]], "avg_entropy": [3.044222116470337], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Left Behind", "score_of_biggest_cluster": 0.07618331164121628, "score_of_first_answer": 0.07618331164121628, "max_score_of_first_answer": 0.9527230262756348, "min_score_of_first_answer": 0.002758242655545473, "avg_score": 0.07618331164121628, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3514745850595876, "std_of_first_answer": 0.3514745850595876, "avg_entropy_of_first_answer": 3.044222116470337, "lex_sim": 1.0}
{"question": "who is the present president of assam sahitya sabha", "answer": ["Lakshminath Bezbarua"], "scores": [[0.033890701830387115, 0.9693344831466675, 0.6305488348007202, 0.9329254031181335, 0.9936178922653198, 0.9883870482444763, 0.18661847710609436]], "normalized_score": [0.44656914472579956], "tokens": [["\u2581Lak", "sh", "min", "ath", "\u2581Bez", "bar", "ua"]], "entropy": [[4.731829643249512, 0.23965874314308167, 1.3777891397476196, 0.3019293248653412, 0.06273654848337173, 0.07635101675987244, 1.1828722953796387]], "avg_entropy": [1.1390238159469195], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dhrubajyoti Bora", "score_of_biggest_cluster": 0.44656914472579956, "score_of_first_answer": 0.44656914472579956, "max_score_of_first_answer": 0.9936178922653198, "min_score_of_first_answer": 0.033890701830387115, "avg_score": 0.44656914472579956, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37879233511779625, "std_of_first_answer": 0.37879233511779625, "avg_entropy_of_first_answer": 1.1390238159469195, "lex_sim": 1.0}
{"question": "where was 2017 beauty and the beast filmed", "answer": ["Stellenbosch"], "scores": [[0.008906296454370022, 0.00805321242660284, 0.9882603883743286, 0.9990491271018982]], "normalized_score": [0.0917341560125351], "tokens": [["\u2581St", "ellen", "bos", "ch"]], "entropy": [[5.243463039398193, 3.2276341915130615, 0.09423376619815826, 0.00934914592653513]], "avg_entropy": [2.143670035758987], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Surrey, United Kingdom", "score_of_biggest_cluster": 0.0917341560125351, "score_of_first_answer": 0.0917341560125351, "max_score_of_first_answer": 0.9990491271018982, "min_score_of_first_answer": 0.00805321242660284, "avg_score": 0.0917341560125351, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.49260236231678906, "std_of_first_answer": 0.49260236231678906, "avg_entropy_of_first_answer": 2.143670035758987, "lex_sim": 1.0}
{"question": "what are some elements that are similar to silver", "answer": ["Copper, gold, lead, iron, and zinc"], "scores": [[0.018169129267334938, 0.9921746850013733, 0.6321150660514832, 0.1167701706290245, 0.8025388717651367, 0.15036576986312866, 0.6587037444114685, 0.057020027190446854, 0.6273083090782166, 0.10652714222669601, 0.15603706240653992, 0.9981439113616943]], "normalized_score": [0.2510799765586853], "tokens": [["\u2581Co", "pper", ",", "\u2581gold", ",", "\u2581lead", ",", "\u2581iron", ",", "\u2581and", "\u2581z", "inc"]], "entropy": [[5.050844669342041, 0.07170405983924866, 1.1263799667358398, 3.478522777557373, 0.7071222066879272, 3.0309767723083496, 0.9374290108680725, 2.8780856132507324, 0.9685962200164795, 2.8809428215026855, 2.5008673667907715, 0.017808247357606888]], "avg_entropy": [1.9707733110214274], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Roentgenium (Rg)", "score_of_biggest_cluster": 0.2510799765586853, "score_of_first_answer": 0.2510799765586853, "max_score_of_first_answer": 0.9981439113616943, "min_score_of_first_answer": 0.018169129267334938, "avg_score": 0.2510799765586853, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36198436617211693, "std_of_first_answer": 0.36198436617211693, "avg_entropy_of_first_answer": 1.9707733110214274, "lex_sim": 1.0}
{"question": "when did the first train run in england", "answer": ["December 21, 1830"], "scores": [[0.021180137991905212, 0.9735906720161438, 0.7326155304908752, 0.09560935199260712, 0.7310081124305725, 0.9822415709495544, 0.9940127730369568, 0.9836999773979187, 0.5480901598930359, 0.9737251400947571]], "normalized_score": [0.47134241461753845], "tokens": [["\u2581December", "\u2581", "2", "1", ",", "\u2581", "1", "8", "3", "0"]], "entropy": [[1.5383650064468384, 0.15454089641571045, 0.9503993988037109, 2.2392332553863525, 0.76822429895401, 0.09556236118078232, 0.04046349599957466, 0.10394637286663055, 1.2009553909301758, 0.17484596371650696]], "avg_entropy": [0.7266536440700293], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1560s", "score_of_biggest_cluster": 0.47134241461753845, "score_of_first_answer": 0.47134241461753845, "max_score_of_first_answer": 0.9940127730369568, "min_score_of_first_answer": 0.021180137991905212, "avg_score": 0.47134241461753845, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35318906585995913, "std_of_first_answer": 0.35318906585995913, "avg_entropy_of_first_answer": 0.7266536440700293, "lex_sim": 1.0}
{"question": "list of strict nature reserve in the philippines", "answer": ["Abra"], "scores": [[0.008756722323596478, 0.7052140831947327]], "normalized_score": [0.0785834938287735], "tokens": [["\u2581Ab", "ra"]], "entropy": [[5.74564266204834, 1.5530893802642822]], "avg_entropy": [3.649366021156311], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Palawan", "score_of_biggest_cluster": 0.0785834938287735, "score_of_first_answer": 0.0785834938287735, "max_score_of_first_answer": 0.7052140831947327, "min_score_of_first_answer": 0.008756722323596478, "avg_score": 0.0785834938287735, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3482286804355681, "std_of_first_answer": 0.3482286804355681, "avg_entropy_of_first_answer": 3.649366021156311, "lex_sim": 1.0}
{"question": "where does florida natural orange juice come from", "answer": ["Florida"], "scores": [[0.32334908843040466]], "normalized_score": [0.3233490586280823], "tokens": [["\u2581Florida"]], "entropy": [[3.647911310195923]], "avg_entropy": [3.647911310195923], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lake Wales, Florida", "score_of_biggest_cluster": 0.3233490586280823, "score_of_first_answer": 0.3233490586280823, "max_score_of_first_answer": 0.32334908843040466, "min_score_of_first_answer": 0.32334908843040466, "avg_score": 0.3233490586280823, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.647911310195923, "lex_sim": 1.0}
{"question": "the man in the high castle episode 1 season 1", "answer": ["The Man in the High Castle"], "scores": [[0.16805952787399292, 0.0353945754468441, 0.9006698727607727, 0.9074369072914124, 0.9799270629882812, 0.9915034770965576]], "normalized_score": [0.4096173048019409], "tokens": [["\u2581The", "\u2581Man", "\u2581in", "\u2581the", "\u2581High", "\u2581Castle"]], "entropy": [[5.440401077270508, 2.876567840576172, 0.5413782596588135, 0.49264031648635864, 0.17799711227416992, 0.06024422496557236]], "avg_entropy": [1.5982048052052658], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The New World", "score_of_biggest_cluster": 0.4096173048019409, "score_of_first_answer": 0.4096173048019409, "max_score_of_first_answer": 0.9915034770965576, "min_score_of_first_answer": 0.0353945754468441, "avg_score": 0.4096173048019409, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40071688993506616, "std_of_first_answer": 0.40071688993506616, "avg_entropy_of_first_answer": 1.5982048052052658, "lex_sim": 1.0}
{"question": "who played junior on in the heat of the night", "answer": ["Carroll O'Connor"], "scores": [[0.11530032753944397, 0.9830091595649719, 0.9732137322425842, 0.5230405926704407, 0.9437276124954224, 0.9843590259552002]], "normalized_score": [0.6140286922454834], "tokens": [["\u2581Car", "roll", "\u2581O", "'", "Con", "nor"]], "entropy": [[4.139293193817139, 0.13226447999477386, 0.20125702023506165, 1.3697657585144043, 0.23443706333637238, 0.08911725878715515]], "avg_entropy": [1.027689129114151], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Christian LeBlanc", "score_of_biggest_cluster": 0.6140286922454834, "score_of_first_answer": 0.6140286922454834, "max_score_of_first_answer": 0.9843590259552002, "min_score_of_first_answer": 0.11530032753944397, "avg_score": 0.6140286922454834, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32935313349011364, "std_of_first_answer": 0.32935313349011364, "avg_entropy_of_first_answer": 1.027689129114151, "lex_sim": 1.0}
{"question": "where do the elves go on the boat in lord of the rings", "answer": ["Helms Deep"], "scores": [[0.006501773837953806, 0.49869686365127563, 0.9053304195404053]], "normalized_score": [0.1431831270456314], "tokens": [["\u2581Hel", "ms", "\u2581Deep"]], "entropy": [[5.0197038650512695, 1.0236085653305054, 0.5220606327056885]], "avg_entropy": [2.1884576876958213], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Valinor", "score_of_biggest_cluster": 0.1431831270456314, "score_of_first_answer": 0.1431831270456314, "max_score_of_first_answer": 0.9053304195404053, "min_score_of_first_answer": 0.006501773837953806, "avg_score": 0.1431831270456314, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36749902336176876, "std_of_first_answer": 0.36749902336176876, "avg_entropy_of_first_answer": 2.1884576876958213, "lex_sim": 1.0}
{"question": "three movies made from agatha christie's novels", "answer": ["And Then There Were None, Murder on the Orient Express and Death on the Nile"], "scores": [[0.18617595732212067, 0.9294478297233582, 0.9920700192451477, 0.9797481894493103, 0.9991070628166199, 0.9967048764228821, 0.7424703240394592, 0.6952506303787231, 0.999171257019043, 0.9006065726280212, 0.9923533797264099, 0.9930898547172546, 0.9973570108413696, 0.18818596005439758, 0.3617027997970581, 0.9742386937141418, 0.9926795959472656, 0.9957570433616638, 0.9999139308929443]], "normalized_score": [0.756742537021637], "tokens": [["\u2581And", "\u2581Then", "\u2581There", "\u2581W", "ere", "\u2581None", ",", "\u2581Mur", "der", "\u2581on", "\u2581the", "\u2581Orient", "\u2581Express", "\u2581and", "\u2581Death", "\u2581on", "\u2581the", "\u2581N", "ile"]], "entropy": [[2.7968177795410156, 0.3027268052101135, 0.05867791175842285, 0.13046865165233612, 0.00963635928928852, 0.029697898775339127, 0.9819292426109314, 1.5299298763275146, 0.007145348936319351, 0.43141698837280273, 0.05868110805749893, 0.06031978502869606, 0.02531769871711731, 0.6607546806335449, 2.4302573204040527, 0.15077930688858032, 0.05575764924287796, 0.04261999577283859, 0.0011302821803838015]], "avg_entropy": [0.5138981415473513], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alibi", "score_of_biggest_cluster": 0.756742537021637, "score_of_first_answer": 0.756742537021637, "max_score_of_first_answer": 0.9999139308929443, "min_score_of_first_answer": 0.18617595732212067, "avg_score": 0.756742537021637, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27179590592282826, "std_of_first_answer": 0.27179590592282826, "avg_entropy_of_first_answer": 0.5138981415473513, "lex_sim": 1.0}
{"question": "during the great depression in the 1930s the u.s. economy experienced a", "answer": ["recession"], "scores": [[0.09533379971981049, 0.9978163242340088, 0.9956432580947876]], "normalized_score": [0.45582741498947144], "tokens": [["\u2581re", "cess", "ion"]], "entropy": [[5.054686546325684, 0.02220917120575905, 0.03931598365306854]], "avg_entropy": [1.7054039003948371], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "economic recession", "score_of_biggest_cluster": 0.45582741498947144, "score_of_first_answer": 0.45582741498947144, "max_score_of_first_answer": 0.9978163242340088, "min_score_of_first_answer": 0.09533379971981049, "avg_score": 0.45582741498947144, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.42492307148332553, "std_of_first_answer": 0.42492307148332553, "avg_entropy_of_first_answer": 1.7054039003948371, "lex_sim": 1.0}
{"question": "what is the meaning of lord shiva lingam", "answer": ["Lord Shiva was once bathing in Pushkari Tirtha, when some drops from his body turned into a lingam after dropping into sand."], "scores": [[0.02624056115746498, 0.7970142364501953, 0.9918932914733887, 0.0019751987420022488, 0.01824725605547428, 0.0025287182070314884, 0.9991402626037598, 0.6640468239784241, 0.011105400510132313, 0.5111342072486877, 0.3508867621421814, 0.005833941511809826, 0.03938892483711243, 0.9140076637268066, 0.8033389449119568, 0.2569842040538788, 0.10906510055065155, 0.018159301951527596, 0.5764153003692627, 0.9817735552787781, 0.03896576911211014, 0.7082834839820862, 0.49003079533576965, 0.012998860329389572, 0.9254448413848877, 0.3380976617336273, 0.4687337279319763, 0.6652086973190308, 0.0006286486168392003, 0.018736770376563072, 0.9803871512413025, 0.4520299434661865, 0.003963778261095285, 0.6994255185127258, 0.13841812312602997, 0.5915313363075256, 0.9306071400642395, 0.008907102048397064, 0.2750033140182495, 0.006786929443478584, 0.15550297498703003, 0.3962112367153168, 0.016728827729821205, 0.01601356454193592, 0.999896764755249, 0.06469357758760452, 0.004940014332532883, 0.0050515420734882355, 0.0008613670361228287, 0.03271813318133354]], "normalized_score": [0.0889502763748169], "tokens": [["\u2581Lord", "\u2581Sh", "iva", "\u2581was", "\u2581once", "\u2581bath", "ing", "\u2581in", "\u2581P", "ush", "k", "ari", "\u2581T", "irth", "a", ",", "\u2581when", "\u2581some", "\u2581dro", "ps", "\u2581from", "\u2581his", "\u2581body", "\u2581turned", "\u2581into", "\u2581a", "\u2581ling", "am", "\u2581after", "\u2581dro", "pping", "\u2581into", "\u2581sand", ".", "\u2581The", "\u2581ling", "am", "\u2581started", "\u2581to", "\u2581dance", ",", "\u2581and", "\u2581hence", "for", "th", "\u2581the", "\u2581term", "\u2581Ar", "das", "\u2581became"]], "entropy": [[4.9689507484436035, 1.0386598110198975, 0.0631905049085617, 2.7398102283477783, 3.990541934967041, 4.972026348114014, 0.009624105878174305, 1.5565540790557861, 1.1139816045761108, 2.0826520919799805, 1.0096617937088013, 0.15991011261940002, 3.4701297283172607, 0.41998836398124695, 1.0665934085845947, 2.7079360485076904, 2.887019634246826, 3.0935513973236084, 2.609520435333252, 0.10130516439676285, 0.40977200865745544, 1.1538164615631104, 2.5037477016448975, 1.6588321924209595, 0.4411058723926544, 2.8379805088043213, 3.0318803787231445, 0.7775529623031616, 1.8388947248458862, 3.5939955711364746, 0.12022016942501068, 1.6613998413085938, 0.90630042552948, 1.6178455352783203, 3.0610852241516113, 2.778169631958008, 0.3310060501098633, 3.7431020736694336, 3.0244908332824707, 3.281017780303955, 2.845187187194824, 3.493715286254883, 4.340958595275879, 3.1341371536254883, 0.0014000058872625232, 3.0822839736938477, 3.839078664779663, 3.373461961746216, 0.4508943259716034, 4.337521553039551]], "avg_entropy": [2.1546492439857685], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "reminds of the Omnipotent Lord", "score_of_biggest_cluster": 0.0889502763748169, "score_of_first_answer": 0.0889502763748169, "max_score_of_first_answer": 0.999896764755249, "min_score_of_first_answer": 0.0006286486168392003, "avg_score": 0.0889502763748169, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3656928305119028, "std_of_first_answer": 0.3656928305119028, "avg_entropy_of_first_answer": 2.1546492439857685, "lex_sim": 1.0}
{"question": "who is the guy who walked across the twin towers", "answer": ["Nik Wallenda"], "scores": [[0.016311299055814743, 0.5893123745918274, 0.9914705157279968]], "normalized_score": [0.21201731264591217], "tokens": [["\u2581Nik", "\u2581Wall", "enda"]], "entropy": [[5.063033103942871, 1.935148000717163, 0.059149160981178284]], "avg_entropy": [2.352443421880404], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Philippe Petit", "score_of_biggest_cluster": 0.21201731264591217, "score_of_first_answer": 0.21201731264591217, "max_score_of_first_answer": 0.9914705157279968, "min_score_of_first_answer": 0.016311299055814743, "avg_score": 0.21201731264591217, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4001384345043364, "std_of_first_answer": 0.4001384345043364, "avg_entropy_of_first_answer": 2.352443421880404, "lex_sim": 1.0}
{"question": "who played tom hagon in the godfather series", "answer": ["James Caan"], "scores": [[0.1994175761938095, 0.9149463772773743, 0.9982215762138367]], "normalized_score": [0.5668419599533081], "tokens": [["\u2581James", "\u2581Ca", "an"]], "entropy": [[2.5132269859313965, 0.5887117385864258, 0.018956013023853302]], "avg_entropy": [1.0402982458472252], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Robert Duvall", "score_of_biggest_cluster": 0.5668419599533081, "score_of_first_answer": 0.5668419599533081, "max_score_of_first_answer": 0.9982215762138367, "min_score_of_first_answer": 0.1994175761938095, "avg_score": 0.5668419599533081, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3585470764667973, "std_of_first_answer": 0.3585470764667973, "avg_entropy_of_first_answer": 1.0402982458472252, "lex_sim": 1.0}
{"question": "who sings find out who your friends are with tracy lawrence", "answer": ["Trace Adkins"], "scores": [[0.2416585385799408, 0.9425380229949951, 0.9930282235145569]], "normalized_score": [0.6092855334281921], "tokens": [["\u2581Trace", "\u2581Ad", "kins"]], "entropy": [[2.295621156692505, 0.3369852304458618, 0.049907803535461426]], "avg_entropy": [0.894171396891276], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kenny Chesney", "score_of_biggest_cluster": 0.6092855334281921, "score_of_first_answer": 0.6092855334281921, "max_score_of_first_answer": 0.9930282235145569, "min_score_of_first_answer": 0.2416585385799408, "avg_score": 0.6092855334281921, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.342918473540993, "std_of_first_answer": 0.342918473540993, "avg_entropy_of_first_answer": 0.894171396891276, "lex_sim": 1.0}
{"question": "who discovered that neural communication between cells occurs through chemicals", "answer": ["Santiago Ram\u00f3n y Cajal"], "scores": [[0.29144933819770813, 0.9808121919631958, 0.4288654327392578, 0.9612687826156616, 0.9957642555236816, 0.9989724159240723, 0.999687671661377]], "normalized_score": [0.7361802458763123], "tokens": [["\u2581Santiago", "\u2581Ram", "\u00f3n", "\u2581y", "\u2581C", "aj", "al"]], "entropy": [[4.189858436584473, 0.17186477780342102, 0.9646332263946533, 0.22581717371940613, 0.041987910866737366, 0.010983750224113464, 0.003653666703030467]], "avg_entropy": [0.8012569917565477], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charles Sherrington", "score_of_biggest_cluster": 0.7361802458763123, "score_of_first_answer": 0.7361802458763123, "max_score_of_first_answer": 0.999687671661377, "min_score_of_first_answer": 0.29144933819770813, "avg_score": 0.7361802458763123, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28595527282658306, "std_of_first_answer": 0.28595527282658306, "avg_entropy_of_first_answer": 0.8012569917565477, "lex_sim": 1.0}
{"question": "who was tammy from basketball wives married to", "answer": ["Gloria Govan"], "scores": [[0.033299628645181656, 0.3476848006248474, 0.9059330224990845, 0.9906648397445679]], "normalized_score": [0.31927287578582764], "tokens": [["\u2581Gl", "oria", "\u2581G", "ovan"]], "entropy": [[5.493183135986328, 0.9679608345031738, 0.6868836283683777, 0.0789492204785347]], "avg_entropy": [1.8067442048341036], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "basketball player Kenny Anderson", "score_of_biggest_cluster": 0.31927287578582764, "score_of_first_answer": 0.31927287578582764, "max_score_of_first_answer": 0.9906648397445679, "min_score_of_first_answer": 0.033299628645181656, "avg_score": 0.31927287578582764, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39600498044839216, "std_of_first_answer": 0.39600498044839216, "avg_entropy_of_first_answer": 1.8067442048341036, "lex_sim": 1.0}
{"question": "who is the girl in the stone sour video say you'll haunt me", "answer": ["Jennifer Carroll"], "scores": [[0.0857391208410263, 0.8946163058280945, 0.024829618632793427, 0.2049332708120346]], "normalized_score": [0.14055605232715607], "tokens": [["\u2581Jenn", "ifer", "\u2581Car", "roll"]], "entropy": [[4.95147705078125, 0.871537983417511, 5.437003135681152, 3.344686985015869]], "avg_entropy": [3.6511762887239456], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joanna Moskawa", "score_of_biggest_cluster": 0.14055605232715607, "score_of_first_answer": 0.14055605232715607, "max_score_of_first_answer": 0.8946163058280945, "min_score_of_first_answer": 0.024829618632793427, "avg_score": 0.14055605232715607, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34792495677755453, "std_of_first_answer": 0.34792495677755453, "avg_entropy_of_first_answer": 3.6511762887239456, "lex_sim": 1.0}
{"question": "islamic term for the community of the faithful", "answer": ["ummah"], "scores": [[0.17475420236587524, 0.9779269099235535, 0.8975409269332886]], "normalized_score": [0.5352985858917236], "tokens": [["\u2581um", "ma", "h"]], "entropy": [[1.3316295146942139, 0.14187279343605042, 0.3783408999443054]], "avg_entropy": [0.6172810693581899], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ummat al-Islamiyah", "score_of_biggest_cluster": 0.5352985858917236, "score_of_first_answer": 0.5352985858917236, "max_score_of_first_answer": 0.9779269099235535, "min_score_of_first_answer": 0.17475420236587524, "avg_score": 0.5352985858917236, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.36116615961166265, "std_of_first_answer": 0.36116615961166265, "avg_entropy_of_first_answer": 0.6172810693581899, "lex_sim": 1.0}
{"question": "when was the first nuclear power plant opened", "answer": ["January 17,1956"], "scores": [[0.04035322368144989, 0.9751807451248169, 0.7248088121414185, 0.1534338891506195, 0.7420420050621033, 0.010481308214366436, 0.999244213104248, 0.3988586664199829, 0.03003937378525734]], "normalized_score": [0.1949911117553711], "tokens": [["\u2581January", "\u2581", "1", "7", ",", "1", "9", "5", "6"]], "entropy": [[2.663816213607788, 0.14793336391448975, 0.790274441242218, 1.6679011583328247, 0.671189546585083, 0.06428155303001404, 0.00776124931871891, 0.817014217376709, 1.3596676588058472]], "avg_entropy": [0.9099821558015214], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "December 20, 1951", "score_of_biggest_cluster": 0.1949911117553711, "score_of_first_answer": 0.1949911117553711, "max_score_of_first_answer": 0.999244213104248, "min_score_of_first_answer": 0.010481308214366436, "avg_score": 0.1949911117553711, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3896076430570322, "std_of_first_answer": 0.3896076430570322, "avg_entropy_of_first_answer": 0.9099821558015214, "lex_sim": 1.0}
{"question": "what was the book wealth of nations about", "answer": ["published in 1776, book written by Adam Smith on the philosophy of economics"], "scores": [[0.0007197090308181942, 0.7454545497894287, 0.9571946859359741, 0.9992263317108154, 0.9914593696594238, 0.9938743710517883, 0.9932108521461487, 0.49980565905570984, 0.002139885211363435, 0.04947500303387642, 0.9179667830467224, 0.8739978075027466, 0.9798431992530823, 0.01508755050599575, 0.4349297881126404, 0.011832451447844505, 0.9826474785804749, 0.49447277188301086, 0.998660683631897]], "normalized_score": [0.23064763844013214], "tokens": [["\u2581published", "\u2581in", "\u2581", "1", "7", "7", "6", ",", "\u2581book", "\u2581written", "\u2581by", "\u2581Adam", "\u2581Smith", "\u2581on", "\u2581the", "\u2581philosophy", "\u2581of", "\u2581econom", "ics"]], "entropy": [[3.792748212814331, 1.3834620714187622, 0.35824623703956604, 0.007759863510727882, 0.059658195823431015, 0.048739608377218246, 0.05389101430773735, 1.6902198791503906, 2.689467430114746, 4.13432502746582, 0.5017769932746887, 0.8166556358337402, 0.14932745695114136, 1.836758017539978, 2.537994384765625, 3.673473358154297, 0.1332724690437317, 2.392976999282837, 0.013961129821836948]], "avg_entropy": [1.3828796834047687], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "what builds nations' wealth", "score_of_biggest_cluster": 0.23064763844013214, "score_of_first_answer": 0.23064763844013214, "max_score_of_first_answer": 0.9992263317108154, "min_score_of_first_answer": 0.0007197090308181942, "avg_score": 0.23064763844013214, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4063705723207125, "std_of_first_answer": 0.4063705723207125, "avg_entropy_of_first_answer": 1.3828796834047687, "lex_sim": 1.0}
{"question": "when does the first nba basketball game start", "answer": ["October 16, 2009"], "scores": [[0.4829411208629608, 0.7890313267707825, 0.725469708442688, 0.6416342854499817, 0.24003787338733673, 0.9899126887321472, 0.4380801022052765, 0.9981786012649536, 0.1646367609500885, 0.05102524906396866]], "normalized_score": [0.41589581966400146], "tokens": [["\u2581October", "\u2581", "1", "6", ",", "\u2581", "2", "0", "0", "9"]], "entropy": [[2.390388011932373, 0.6177865266799927, 0.9531136155128479, 1.3507732152938843, 1.145628571510315, 0.08422177284955978, 0.701973021030426, 0.02030365727841854, 0.4677589237689972, 1.6259666681289673]], "avg_entropy": [0.9357913983985782], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "October 17, 2017", "score_of_biggest_cluster": 0.41589581966400146, "score_of_first_answer": 0.41589581966400146, "max_score_of_first_answer": 0.9981786012649536, "min_score_of_first_answer": 0.05102524906396866, "avg_score": 0.41589581966400146, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31642469759694924, "std_of_first_answer": 0.31642469759694924, "avg_entropy_of_first_answer": 0.9357913983985782, "lex_sim": 1.0}
{"question": "only player to play for manchester united manchester city liverpool and everton", "answer": ["Paul Scholes"], "scores": [[0.11455994844436646, 0.33673393726348877, 0.9980583786964417]], "normalized_score": [0.3376695513725281], "tokens": [["\u2581Paul", "\u2581Sch", "oles"]], "entropy": [[4.511689186096191, 2.6324808597564697, 0.017641428858041763]], "avg_entropy": [2.3872704915702343], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Peter Andrew Beardsley", "score_of_biggest_cluster": 0.3376695513725281, "score_of_first_answer": 0.3376695513725281, "max_score_of_first_answer": 0.9980583786964417, "min_score_of_first_answer": 0.11455994844436646, "avg_score": 0.3376695513725281, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3752452197344925, "std_of_first_answer": 0.3752452197344925, "avg_entropy_of_first_answer": 2.3872704915702343, "lex_sim": 1.0}
{"question": "skin that covers the palms fingertips and soles of the feet", "answer": ["skin"], "scores": [[0.07621777802705765]], "normalized_score": [0.07621777802705765], "tokens": [["\u2581skin"]], "entropy": [[3.6829214096069336]], "avg_entropy": [3.6829214096069336], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "stratum lucidum", "score_of_biggest_cluster": 0.07621777802705765, "score_of_first_answer": 0.07621777802705765, "max_score_of_first_answer": 0.07621777802705765, "min_score_of_first_answer": 0.07621777802705765, "avg_score": 0.07621777802705765, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.6829214096069336, "lex_sim": 1.0}
{"question": "who does dwyane wade play for right now", "answer": ["Cleveland Cavaliers"], "scores": [[0.09152306616306305, 0.934607744216919, 0.9827583432197571, 0.9980224370956421]], "normalized_score": [0.5381913781166077], "tokens": [["\u2581Cleveland", "\u2581Cav", "ali", "ers"]], "entropy": [[2.1797661781311035, 0.33503156900405884, 0.12530139088630676, 0.01736241951584816]], "avg_entropy": [0.6643653893843293], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Miami Heat", "score_of_biggest_cluster": 0.5381913781166077, "score_of_first_answer": 0.5381913781166077, "max_score_of_first_answer": 0.9980224370956421, "min_score_of_first_answer": 0.09152306616306305, "avg_score": 0.5381913781166077, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38188726240211784, "std_of_first_answer": 0.38188726240211784, "avg_entropy_of_first_answer": 0.6643653893843293, "lex_sim": 1.0}
{"question": "who wrote the song to make you feel my love", "answer": ["Alicia Keys"], "scores": [[0.6476746797561646, 0.6263008117675781, 0.9997116923332214, 0.8454098701477051, 0.9978669881820679]], "normalized_score": [0.8069208264350891], "tokens": [["\u2581A", "lic", "ia", "\u2581Ke", "ys"]], "entropy": [[2.22568416595459, 1.1090445518493652, 0.0036995874252170324, 1.1057209968566895, 0.02304082363843918]], "avg_entropy": [0.8934380251448601], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bob Dylan", "score_of_biggest_cluster": 0.8069208264350891, "score_of_first_answer": 0.8069208264350891, "max_score_of_first_answer": 0.9997116923332214, "min_score_of_first_answer": 0.6263008117675781, "avg_score": 0.8069208264350891, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.16231848639063298, "std_of_first_answer": 0.16231848639063298, "avg_entropy_of_first_answer": 0.8934380251448601, "lex_sim": 1.0}
{"question": "when was the biltmore house opened to the public", "answer": ["October 8,1930"], "scores": [[0.04441511258482933, 0.9765555262565613, 0.010761282406747341, 0.6771388649940491, 0.009964827448129654, 0.8991474509239197, 0.4777032136917114, 0.9972406625747681]], "normalized_score": [0.1846093088388443], "tokens": [["\u2581October", "\u2581", "8", ",", "1", "9", "3", "0"]], "entropy": [[1.761527419090271, 0.13946561515331268, 1.469325065612793, 0.7128878831863403, 0.060010336339473724, 0.3500356078147888, 1.6553165912628174, 0.02423201873898506]], "avg_entropy": [0.7716000671498477], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "March 1930", "score_of_biggest_cluster": 0.1846093088388443, "score_of_first_answer": 0.1846093088388443, "max_score_of_first_answer": 0.9972406625747681, "min_score_of_first_answer": 0.009964827448129654, "avg_score": 0.1846093088388443, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41102229465112766, "std_of_first_answer": 0.41102229465112766, "avg_entropy_of_first_answer": 0.7716000671498477, "lex_sim": 1.0}
{"question": "who killed missy in a bend in the road", "answer": ["Joe"], "scores": [[0.002349064452573657]], "normalized_score": [0.002349064452573657], "tokens": [["\u2581Joe"]], "entropy": [[6.0428547859191895]], "avg_entropy": [6.0428547859191895], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brian", "score_of_biggest_cluster": 0.002349064452573657, "score_of_first_answer": 0.002349064452573657, "max_score_of_first_answer": 0.002349064452573657, "min_score_of_first_answer": 0.002349064452573657, "avg_score": 0.002349064452573657, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.0428547859191895, "lex_sim": 1.0}
{"question": "when was the last time vancouver had an earthquake", "answer": ["1913"], "scores": [[0.37017205357551575, 0.5300071239471436, 0.8157448172569275, 0.016823237761855125, 0.11552475392818451]], "normalized_score": [0.19886796176433563], "tokens": [["\u2581", "1", "9", "1", "3"]], "entropy": [[2.8786237239837646, 1.0795353651046753, 0.8128300309181213, 1.7651054859161377, 2.3193578720092773]], "avg_entropy": [1.7710904955863953], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1946", "score_of_biggest_cluster": 0.19886796176433563, "score_of_first_answer": 0.19886796176433563, "max_score_of_first_answer": 0.8157448172569275, "min_score_of_first_answer": 0.016823237761855125, "avg_score": 0.19886796176433563, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2876739927313065, "std_of_first_answer": 0.2876739927313065, "avg_entropy_of_first_answer": 1.7710904955863953, "lex_sim": 1.0}
{"question": "how many episodes of corrie has there been", "answer": ["over 8000"], "scores": [[0.022458845749497414, 0.9605233669281006, 0.06590701639652252, 0.41917261481285095, 0.9775459170341492, 0.9234234690666199]], "normalized_score": [0.28518474102020264], "tokens": [["\u2581over", "\u2581", "8", "0", "0", "0"]], "entropy": [[0.9586178064346313, 0.27459344267845154, 1.6376570463180542, 1.7451753616333008, 0.1608181595802307, 0.3417782783508301]], "avg_entropy": [0.8531066824992498], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "9,436", "score_of_biggest_cluster": 0.28518474102020264, "score_of_first_answer": 0.28518474102020264, "max_score_of_first_answer": 0.9775459170341492, "min_score_of_first_answer": 0.022458845749497414, "avg_score": 0.28518474102020264, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4122574652004518, "std_of_first_answer": 0.4122574652004518, "avg_entropy_of_first_answer": 0.8531066824992498, "lex_sim": 1.0}
{"question": "how many seasons of the smurfs are there", "answer": ["82 We also checked the response time \u2014 the slower the better."], "scores": [[0.5660167336463928, 0.030264632776379585, 0.03674301505088806, 4.054837063449668e-06, 0.0026949401944875717, 0.0003701976966112852, 0.17979592084884644, 0.001173787284642458, 0.08190154284238815, 0.00024291551380883902, 0.36328479647636414, 0.0026630910579115152, 0.519838809967041, 0.49091631174087524, 0.4861506223678589, 0.10155566036701202, 0.1603478193283081, 0.6615241169929504, 0.3895869553089142, 0.17711898684501648, 0.6313250064849854, 0.011380820535123348, 0.022423988208174706, 0.11868014186620712, 0.675558865070343, 0.0006018582498654723, 0.7480438351631165, 0.5944855213165283, 0.2617272436618805, 0.9686245322227478, 0.09457708150148392, 0.08855795860290527, 0.03432796522974968, 0.04916452616453171, 0.042636144906282425, 0.04632095247507095, 0.33118727803230286, 0.5340133309364319, 0.009936620481312275, 0.9640084505081177, 0.004807198885828257, 0.2935185730457306, 0.3265390992164612, 0.585870623588562, 0.26563507318496704, 0.009563502855598927, 0.4771854281425476, 0.0003128390235360712, 0.052008647471666336]], "normalized_score": [0.05776968225836754], "tokens": [["\u2581", "8", "2", "\u2581We", "\u2581also", "\u2581checked", "\u2581the", "\u2581response", "\u2581time", "\u2581\u2014", "\u2581the", "\u2581slower", "\u2581the", "\u2581better", ".", "\u2581We", "\u2581found", "\u2581that", "\u2581the", "\u2581average", "\u2581response", "\u2581to", "\u2581each", "\u2581query", "\u2581was", "\u2581completed", "\u2581in", "\u2581", "0", ".", "0", "3", "0", "3", "3", "6", "\u2581seconds", ".", "\u2581However", ",", "\u2581during", "\u2581our", "\u2581testing", ",", "\u2581the", "\u2581actual", "\u2581response", "\u2581at", "\u2581different"]], "entropy": [[2.283743381500244, 1.9280357360839844, 2.816771984100342, 0.6917159557342529, 2.84049654006958, 3.3360893726348877, 4.307847023010254, 6.5538835525512695, 3.8583388328552246, 2.9695026874542236, 3.3630356788635254, 2.812049388885498, 1.9146274328231812, 2.6423168182373047, 1.6494145393371582, 4.308796405792236, 4.4198808670043945, 1.873199701309204, 4.197723865509033, 4.825519561767578, 2.4788949489593506, 0.6171563267707825, 3.1864840984344482, 2.749429702758789, 1.6630160808563232, 2.1886439323425293, 1.1470298767089844, 1.8466064929962158, 1.9693005084991455, 0.22801601886749268, 2.2994556427001953, 2.3216724395751953, 2.356562614440918, 2.1155333518981934, 1.9636871814727783, 2.301406145095825, 2.3659653663635254, 1.7535996437072754, 3.616537094116211, 0.2935871481895447, 3.4127306938171387, 2.4158577919006348, 3.0024499893188477, 1.98458993434906, 2.7498550415039062, 3.9890475273132324, 2.187835216522217, 0.962988018989563, 2.2385201454162598]], "avg_entropy": [2.5714173122328154], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "9", "score_of_biggest_cluster": 0.05776968225836754, "score_of_first_answer": 0.05776968225836754, "max_score_of_first_answer": 0.9686245322227478, "min_score_of_first_answer": 4.054837063449668e-06, "avg_score": 0.05776968225836754, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27430075535978493, "std_of_first_answer": 0.27430075535978493, "avg_entropy_of_first_answer": 2.5714173122328154, "lex_sim": 1.0}
{"question": "who plays dr. sloan on kevin saves the world", "answer": ["Imogene Wolodarsky"], "scores": [[0.0028513509314507246, 0.32021841406822205, 0.24639371037483215, 0.9854957461357117, 0.9943740367889404, 0.944469153881073]], "normalized_score": [0.24345560371875763], "tokens": [["\u2581Im", "ogene", "\u2581Wol", "od", "ar", "sky"]], "entropy": [[4.439047336578369, 2.364183187484741, 3.180137872695923, 0.12697277963161469, 0.05524926260113716, 0.28733882308006287]], "avg_entropy": [1.7421548770119746], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Richard Masur", "score_of_biggest_cluster": 0.24345560371875763, "score_of_first_answer": 0.24345560371875763, "max_score_of_first_answer": 0.9943740367889404, "min_score_of_first_answer": 0.0028513509314507246, "avg_score": 0.24345560371875763, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40431399669090906, "std_of_first_answer": 0.40431399669090906, "avg_entropy_of_first_answer": 1.7421548770119746, "lex_sim": 1.0}
{"question": "who wrote the book my country my life", "answer": ["Inderjit Singh Grewal"], "scores": [[0.009461269713938236, 0.13938604295253754, 0.4419465661048889, 0.985947847366333, 0.5741427540779114, 0.04962692782282829, 0.6115623712539673, 0.9921532273292542]], "normalized_score": [0.23694288730621338], "tokens": [["\u2581Ind", "er", "j", "it", "\u2581Singh", "\u2581G", "rew", "al"]], "entropy": [[5.83282995223999, 1.9142000675201416, 2.390472888946533, 0.109238401055336, 1.7813925743103027, 3.4052388668060303, 1.993431568145752, 0.06203566491603851]], "avg_entropy": [2.1861049979925156], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "L. K. Advani", "score_of_biggest_cluster": 0.23694288730621338, "score_of_first_answer": 0.23694288730621338, "max_score_of_first_answer": 0.9921532273292542, "min_score_of_first_answer": 0.009461269713938236, "avg_score": 0.23694288730621338, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3654431067780949, "std_of_first_answer": 0.3654431067780949, "avg_entropy_of_first_answer": 2.1861049979925156, "lex_sim": 1.0}
{"question": "who sang the american anthem at the super bowl", "answer": ["Chris Martin"], "scores": [[0.0033551868982613087, 0.27324727177619934]], "normalized_score": [0.030278632417321205], "tokens": [["\u2581Chris", "\u2581Martin"]], "entropy": [[4.090828895568848, 3.6182591915130615]], "avg_entropy": [3.8545440435409546], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pink", "score_of_biggest_cluster": 0.030278632417321205, "score_of_first_answer": 0.030278632417321205, "max_score_of_first_answer": 0.27324727177619934, "min_score_of_first_answer": 0.0033551868982613087, "avg_score": 0.030278632417321205, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13494604243896902, "std_of_first_answer": 0.13494604243896902, "avg_entropy_of_first_answer": 3.8545440435409546, "lex_sim": 1.0}
{"question": "in the texas legislature the house has members and the senate has", "answer": ["150 senators and 50 representatives"], "scores": [[0.12305545061826706, 0.9101498126983643, 0.7795423269271851, 0.9121283292770386, 0.31158214807510376, 0.8962192535400391, 0.5140819549560547, 0.9533478617668152, 0.025338172912597656, 0.4718368947505951, 0.7114279270172119, 0.9989951252937317]], "normalized_score": [0.4611974060535431], "tokens": [["\u2581", "1", "5", "0", "\u2581sen", "ators", "\u2581and", "\u2581", "5", "0", "\u2581represent", "atives"]], "entropy": [[3.7858939170837402, 0.48481884598731995, 0.8601823449134827, 0.3340262770652771, 2.121636390686035, 0.40299493074417114, 1.2167260646820068, 0.36076620221138, 0.44975370168685913, 1.9827675819396973, 1.4531190395355225, 0.010624721646308899]], "avg_entropy": [1.12194250151515], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "member", "score_of_biggest_cluster": 0.4611974060535431, "score_of_first_answer": 0.4611974060535431, "max_score_of_first_answer": 0.9989951252937317, "min_score_of_first_answer": 0.025338172912597656, "avg_score": 0.4611974060535431, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3241552098876504, "std_of_first_answer": 0.3241552098876504, "avg_entropy_of_first_answer": 1.12194250151515, "lex_sim": 1.0}
{"question": "who played the female lead in the 1942 film casablanca", "answer": ["Ingrid Bergman"], "scores": [[0.8533035516738892, 0.9983775615692139, 0.9776248931884766, 0.9861899018287659]], "normalized_score": [0.9519909024238586], "tokens": [["\u2581In", "grid", "\u2581Berg", "man"]], "entropy": [[1.0278598070144653, 0.017384599894285202, 0.1858062446117401, 0.10244046896696091]], "avg_entropy": [0.3333727801218629], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "romantic drama", "score_of_biggest_cluster": 0.9519909024238586, "score_of_first_answer": 0.9519909024238586, "max_score_of_first_answer": 0.9983775615692139, "min_score_of_first_answer": 0.8533035516738892, "avg_score": 0.9519909024238586, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.058530770768704415, "std_of_first_answer": 0.058530770768704415, "avg_entropy_of_first_answer": 0.3333727801218629, "lex_sim": 1.0}
{"question": "where do most star formation occur in the milky way today", "answer": ["galactic plane"], "scores": [[0.008631378412246704, 0.9723886251449585, 0.09787435084581375]], "normalized_score": [0.09365472197532654], "tokens": [["\u2581gal", "actic", "\u2581plane"]], "entropy": [[4.888376235961914, 0.20078444480895996, 3.2312769889831543]], "avg_entropy": [2.7734792232513428], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "molecular clouds in interstellar space", "score_of_biggest_cluster": 0.09365472197532654, "score_of_first_answer": 0.09365472197532654, "max_score_of_first_answer": 0.9723886251449585, "min_score_of_first_answer": 0.008631378412246704, "avg_score": 0.09365472197532654, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43481382410707375, "std_of_first_answer": 0.43481382410707375, "avg_entropy_of_first_answer": 2.7734792232513428, "lex_sim": 1.0}
{"question": "vikram samvat calender is official in which country", "answer": ["India"], "scores": [[0.5603954195976257]], "normalized_score": [0.5603954195976257], "tokens": [["\u2581India"]], "entropy": [[2.221426010131836]], "avg_entropy": [2.221426010131836], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nepal", "score_of_biggest_cluster": 0.5603954195976257, "score_of_first_answer": 0.5603954195976257, "max_score_of_first_answer": 0.5603954195976257, "min_score_of_first_answer": 0.5603954195976257, "avg_score": 0.5603954195976257, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.221426010131836, "lex_sim": 1.0}
{"question": "who sang buddy can you spare a dime", "answer": ["Yip Harburg, Jay Graydon, Michael Kamen"], "scores": [[0.7555890083312988, 0.9851626753807068, 0.9606080651283264, 0.9741030335426331, 0.020032629370689392, 0.03117724135518074, 0.0203891359269619, 0.8175386786460876, 0.34601131081581116, 0.01851438730955124, 0.020204192027449608, 0.8892300724983215]], "normalized_score": [0.17516884207725525], "tokens": [["\u2581Y", "ip", "\u2581Har", "burg", ",", "\u2581Jay", "\u2581Gray", "don", ",", "\u2581Michael", "\u2581K", "amen"]], "entropy": [[1.7910209894180298, 0.10355526953935623, 0.3264705538749695, 0.2211596667766571, 0.5506001710891724, 4.659697532653809, 1.6796308755874634, 0.5639306306838989, 1.149399757385254, 4.826314926147461, 4.189129829406738, 0.8117485046386719]], "avg_entropy": [1.7393882256001234], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rudy Vallee", "score_of_biggest_cluster": 0.17516884207725525, "score_of_first_answer": 0.17516884207725525, "max_score_of_first_answer": 0.9851626753807068, "min_score_of_first_answer": 0.01851438730955124, "avg_score": 0.17516884207725525, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42364843638902533, "std_of_first_answer": 0.42364843638902533, "avg_entropy_of_first_answer": 1.7393882256001234, "lex_sim": 1.0}
{"question": "how many episodes are there in season six of nashville", "answer": ["24"], "scores": [[0.9187281131744385, 0.5712398290634155, 0.10574983060359955]], "normalized_score": [0.3814418911933899], "tokens": [["\u2581", "2", "4"]], "entropy": [[0.6655691862106323, 0.9031845927238464, 1.4178365468978882]], "avg_entropy": [0.9955301086107889], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "16", "score_of_biggest_cluster": 0.3814418911933899, "score_of_first_answer": 0.3814418911933899, "max_score_of_first_answer": 0.9187281131744385, "min_score_of_first_answer": 0.10574983060359955, "avg_score": 0.3814418911933899, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33306034391389416, "std_of_first_answer": 0.33306034391389416, "avg_entropy_of_first_answer": 0.9955301086107889, "lex_sim": 1.0}
{"question": "who is covering the winter olympics in canada", "answer": ["Tara Cleary"], "scores": [[0.052341774106025696, 0.24675367772579193, 0.5787873268127441, 0.9616451859474182]], "normalized_score": [0.29117995500564575], "tokens": [["\u2581T", "ara", "\u2581Cle", "ary"]], "entropy": [[3.679654598236084, 1.832808017730713, 1.3609566688537598, 0.28854668140411377]], "avg_entropy": [1.7904914915561676], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "TLN", "score_of_biggest_cluster": 0.29117995500564575, "score_of_first_answer": 0.29117995500564575, "max_score_of_first_answer": 0.9616451859474182, "min_score_of_first_answer": 0.052341774106025696, "avg_score": 0.29117995500564575, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3454770213030379, "std_of_first_answer": 0.3454770213030379, "avg_entropy_of_first_answer": 1.7904914915561676, "lex_sim": 1.0}
{"question": "the first political society started under british rule in india was", "answer": ["Bengal nawab"], "scores": [[0.05915866047143936, 0.9697815179824829, 0.00578248081728816, 0.8091924786567688, 0.8286027908325195]], "normalized_score": [0.18596917390823364], "tokens": [["\u2581Beng", "al", "\u2581na", "w", "ab"]], "entropy": [[4.791606903076172, 0.1787262260913849, 2.0815653800964355, 1.1231311559677124, 0.596734881401062]], "avg_entropy": [1.7543529093265533], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The British Indian Association", "score_of_biggest_cluster": 0.18596917390823364, "score_of_first_answer": 0.18596917390823364, "max_score_of_first_answer": 0.9697815179824829, "min_score_of_first_answer": 0.00578248081728816, "avg_score": 0.18596917390823364, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41398406089955936, "std_of_first_answer": 0.41398406089955936, "avg_entropy_of_first_answer": 1.7543529093265533, "lex_sim": 1.0}
{"question": "when does the dlc for rainbow six siege come out", "answer": ["September 7"], "scores": [[0.07618598639965057, 0.9141057729721069, 0.0602608397603035]], "normalized_score": [0.16130046546459198], "tokens": [["\u2581September", "\u2581", "7"]], "entropy": [[3.635852813720703, 0.36150485277175903, 1.689225196838379]], "avg_entropy": [1.8955276211102803], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "January 2018", "score_of_biggest_cluster": 0.16130046546459198, "score_of_first_answer": 0.16130046546459198, "max_score_of_first_answer": 0.9141057729721069, "min_score_of_first_answer": 0.0602608397603035, "avg_score": 0.16130046546459198, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3988057657273755, "std_of_first_answer": 0.3988057657273755, "avg_entropy_of_first_answer": 1.8955276211102803, "lex_sim": 1.0}
{"question": "who has the highest corporate taxes in the world", "answer": ["Swit"], "scores": [[0.003060499671846628, 0.03569778427481651]], "normalized_score": [0.010452420450747013], "tokens": [["\u2581Sw", "it"]], "entropy": [[4.36142635345459, 2.7759571075439453]], "avg_entropy": [3.5686917304992676], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Argentina", "score_of_biggest_cluster": 0.010452420450747013, "score_of_first_answer": 0.010452420450747013, "max_score_of_first_answer": 0.03569778427481651, "min_score_of_first_answer": 0.003060499671846628, "avg_score": 0.010452420450747013, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.016318642301484942, "std_of_first_answer": 0.016318642301484942, "avg_entropy_of_first_answer": 3.5686917304992676, "lex_sim": 1.0}
{"question": "points on a sphere or angles in a circle are measured in units called", "answer": ["Radians"], "scores": [[0.03428183123469353, 0.835902214050293]], "normalized_score": [0.16928158700466156], "tokens": [["\u2581Rad", "ians"]], "entropy": [[1.827672004699707, 0.6142400503158569]], "avg_entropy": [1.220956027507782], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "radians", "score_of_biggest_cluster": 0.16928158700466156, "score_of_first_answer": 0.16928158700466156, "max_score_of_first_answer": 0.835902214050293, "min_score_of_first_answer": 0.03428183123469353, "avg_score": 0.16928158700466156, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4008101914077997, "std_of_first_answer": 0.4008101914077997, "avg_entropy_of_first_answer": 1.220956027507782, "lex_sim": 1.0}
{"question": "when is the new season of chicago pd", "answer": ["2019"], "scores": [[0.10094335675239563, 0.5896216630935669, 0.8050205707550049, 0.9565808773040771, 0.059271588921546936]], "normalized_score": [0.3067646324634552], "tokens": [["\u2581", "2", "0", "1", "9"]], "entropy": [[3.1485588550567627, 1.3685448169708252, 1.0244015455245972, 0.26951223611831665, 1.848083257675171]], "avg_entropy": [1.5318201422691344], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "September 27, 2017", "score_of_biggest_cluster": 0.3067646324634552, "score_of_first_answer": 0.3067646324634552, "max_score_of_first_answer": 0.9565808773040771, "min_score_of_first_answer": 0.059271588921546936, "avg_score": 0.3067646324634552, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36414207656411157, "std_of_first_answer": 0.36414207656411157, "avg_entropy_of_first_answer": 1.5318201422691344, "lex_sim": 1.0}
{"question": "what are the main crops grown in the united states", "answer": ["corn, soybeans, and wheat"], "scores": [[0.3764086961746216, 0.900802493095398, 0.29558321833610535, 0.9960343241691589, 0.7599717974662781, 0.9623820781707764, 0.13744589686393738, 0.8304773569107056, 0.9997430443763733]], "normalized_score": [0.5874496698379517], "tokens": [["\u2581corn", ",", "\u2581so", "y", "beans", ",", "\u2581and", "\u2581whe", "at"]], "entropy": [[2.4191274642944336, 0.5180269479751587, 1.3234913349151611, 0.02730761095881462, 0.7668821811676025, 0.20996996760368347, 1.505465030670166, 0.916770875453949, 0.002983544720336795]], "avg_entropy": [0.8544472175288118], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Corn", "score_of_biggest_cluster": 0.5874496698379517, "score_of_first_answer": 0.5874496698379517, "max_score_of_first_answer": 0.9997430443763733, "min_score_of_first_answer": 0.13744589686393738, "avg_score": 0.5874496698379517, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3147860639086679, "std_of_first_answer": 0.3147860639086679, "avg_entropy_of_first_answer": 0.8544472175288118, "lex_sim": 1.0}
{"question": "the vascular layer of the eye is the", "answer": ["retina"], "scores": [[0.1756739467382431, 0.9490832686424255]], "normalized_score": [0.40832486748695374], "tokens": [["\u2581ret", "ina"]], "entropy": [[3.114842414855957, 0.2570093870162964]], "avg_entropy": [1.6859259009361267], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "middle layer", "score_of_biggest_cluster": 0.40832486748695374, "score_of_first_answer": 0.40832486748695374, "max_score_of_first_answer": 0.9490832686424255, "min_score_of_first_answer": 0.1756739467382431, "avg_score": 0.40832486748695374, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3867046609520912, "std_of_first_answer": 0.3867046609520912, "avg_entropy_of_first_answer": 1.6859259009361267, "lex_sim": 1.0}
{"question": "who had the best strike rate among batsmen who scored over 200 runs in the world cup", "answer": ["Sachin Tendulkar"], "scores": [[0.2004818618297577, 0.9979175925254822, 0.9408257007598877, 0.9992347955703735, 0.9943088889122009, 0.996674656867981]], "normalized_score": [0.7557941675186157], "tokens": [["\u2581Sach", "in", "\u2581T", "end", "ul", "kar"]], "entropy": [[4.286407947540283, 0.022396178916096687, 0.34847378730773926, 0.009298417717218399, 0.049632035195827484, 0.026402389630675316]], "avg_entropy": [0.7904351260513067], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chris Gayle", "score_of_biggest_cluster": 0.7557941675186157, "score_of_first_answer": 0.7557941675186157, "max_score_of_first_answer": 0.9992347955703735, "min_score_of_first_answer": 0.2004818618297577, "avg_score": 0.7557941675186157, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2933904627863282, "std_of_first_answer": 0.2933904627863282, "avg_entropy_of_first_answer": 0.7904351260513067, "lex_sim": 1.0}
{"question": "who is the president of usa right now", "answer": ["Donald Trump"], "scores": [[0.522580087184906, 0.7312880158424377]], "normalized_score": [0.6181881427764893], "tokens": [["\u2581Donald", "\u2581Trump"]], "entropy": [[1.2667427062988281, 0.9260647892951965]], "avg_entropy": [1.0964037477970123], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Donald Trump", "score_of_biggest_cluster": 0.6181881427764893, "score_of_first_answer": 0.6181881427764893, "max_score_of_first_answer": 0.7312880158424377, "min_score_of_first_answer": 0.522580087184906, "avg_score": 0.6181881427764893, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.10435396432876587, "std_of_first_answer": 0.10435396432876587, "avg_entropy_of_first_answer": 1.0964037477970123, "lex_sim": 1.0}
{"question": "when was the $1 000 bill discontinued", "answer": ["August 30, 1969"], "scores": [[0.014427204616367817, 0.9653281569480896, 0.4204261898994446, 0.2965503931045532, 0.6667096018791199, 0.9868305325508118, 0.9119775295257568, 0.9964931607246399, 0.6447312235832214, 0.5956372022628784]], "normalized_score": [0.4571559429168701], "tokens": [["\u2581August", "\u2581", "3", "0", ",", "\u2581", "1", "9", "6", "9"]], "entropy": [[2.1970157623291016, 0.1973678022623062, 1.5237149000167847, 0.9196608066558838, 0.849716305732727, 0.08063000440597534, 0.29875099658966064, 0.02486809343099594, 1.223745584487915, 0.9283182621002197]], "avg_entropy": [0.824378851801157], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1969", "score_of_biggest_cluster": 0.4571559429168701, "score_of_first_answer": 0.4571559429168701, "max_score_of_first_answer": 0.9964931607246399, "min_score_of_first_answer": 0.014427204616367817, "avg_score": 0.4571559429168701, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.31407357949034626, "std_of_first_answer": 0.31407357949034626, "avg_entropy_of_first_answer": 0.824378851801157, "lex_sim": 1.0}
{"question": "how old is the actress who plays phyllis on y&r", "answer": ["65"], "scores": [[0.22336076200008392, 0.16031810641288757, 0.10394426435232162]], "normalized_score": [0.15497559309005737], "tokens": [["\u2581", "6", "5"]], "entropy": [[4.109157562255859, 1.7366529703140259, 2.3050591945648193]], "avg_entropy": [2.7169565757115683], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gina Tognoni", "score_of_biggest_cluster": 0.15497559309005737, "score_of_first_answer": 0.15497559309005737, "max_score_of_first_answer": 0.22336076200008392, "min_score_of_first_answer": 0.10394426435232162, "avg_score": 0.15497559309005737, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.048776914396752975, "std_of_first_answer": 0.048776914396752975, "avg_entropy_of_first_answer": 2.7169565757115683, "lex_sim": 1.0}
{"question": "who won nfl football coach of the year", "answer": ["Norv Turner"], "scores": [[0.0009013485978357494, 0.9837115406990051, 0.9728837013244629]], "normalized_score": [0.09519345313310623], "tokens": [["\u2581Nor", "v", "\u2581Turner"]], "entropy": [[3.6782736778259277, 0.12224045395851135, 0.19727137684822083]], "avg_entropy": [1.33259516954422], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sean McVay", "score_of_biggest_cluster": 0.09519345313310623, "score_of_first_answer": 0.09519345313310623, "max_score_of_first_answer": 0.9837115406990051, "min_score_of_first_answer": 0.0009013485978357494, "avg_score": 0.09519345313310623, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4607702259159171, "std_of_first_answer": 0.4607702259159171, "avg_entropy_of_first_answer": 1.33259516954422, "lex_sim": 1.0}
{"question": "where is the largest salt mine in the us", "answer": ["New Rock Salt"], "scores": [[0.03449266031384468, 0.005352078936994076, 0.5843065977096558]], "normalized_score": [0.04760252684354782], "tokens": [["\u2581New", "\u2581Rock", "\u2581Salt"]], "entropy": [[4.723592281341553, 1.411301612854004, 2.205892562866211]], "avg_entropy": [2.7802621523539224], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Western New York", "score_of_biggest_cluster": 0.04760252684354782, "score_of_first_answer": 0.04760252684354782, "max_score_of_first_answer": 0.5843065977096558, "min_score_of_first_answer": 0.005352078936994076, "avg_score": 0.04760252684354782, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26631912228355537, "std_of_first_answer": 0.26631912228355537, "avg_entropy_of_first_answer": 2.7802621523539224, "lex_sim": 1.0}
{"question": "when is if loving you is wrong coming back season 4", "answer": ["2019"], "scores": [[0.14967671036720276, 0.8889768719673157, 0.972871720790863, 0.8150867819786072, 0.30759450793266296]], "normalized_score": [0.5037981271743774], "tokens": [["\u2581", "2", "0", "1", "9"]], "entropy": [[3.815096139907837, 0.5530377626419067, 0.20404942333698273, 0.5211883783340454, 1.564155101776123]], "avg_entropy": [1.331505361199379], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "September\u00a019,\u00a02017", "score_of_biggest_cluster": 0.5037981271743774, "score_of_first_answer": 0.5037981271743774, "max_score_of_first_answer": 0.972871720790863, "min_score_of_first_answer": 0.14967671036720276, "avg_score": 0.5037981271743774, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3327140007674493, "std_of_first_answer": 0.3327140007674493, "avg_entropy_of_first_answer": 1.331505361199379, "lex_sim": 1.0}
{"question": "what are the zords in the power rangers movie", "answer": ["zords"], "scores": [[0.013197450898587704, 0.9327569603919983]], "normalized_score": [0.11095049977302551], "tokens": [["\u2581z", "ords"]], "entropy": [[4.036571502685547, 0.3343658447265625]], "avg_entropy": [2.1854686737060547], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "animal-themed Ninja Zords", "score_of_biggest_cluster": 0.11095049977302551, "score_of_first_answer": 0.11095049977302551, "max_score_of_first_answer": 0.9327569603919983, "min_score_of_first_answer": 0.013197450898587704, "avg_score": 0.11095049977302551, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4597797547467053, "std_of_first_answer": 0.4597797547467053, "avg_entropy_of_first_answer": 2.1854686737060547, "lex_sim": 1.0}
{"question": "a drug that might be used in surgery for its amnesic properties is", "answer": ["ketamine"], "scores": [[0.16033223271369934, 0.9942672848701477, 0.9846935272216797, 0.9980851411819458]], "normalized_score": [0.6291412711143494], "tokens": [["\u2581k", "et", "am", "ine"]], "entropy": [[4.955528259277344, 0.055014900863170624, 0.13879933953285217, 0.021602671593427658]], "avg_entropy": [1.2927362928166986], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "benzodiazepines", "score_of_biggest_cluster": 0.6291412711143494, "score_of_first_answer": 0.6291412711143494, "max_score_of_first_answer": 0.9980851411819458, "min_score_of_first_answer": 0.16033223271369934, "avg_score": 0.6291412711143494, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36030670259272235, "std_of_first_answer": 0.36030670259272235, "avg_entropy_of_first_answer": 1.2927362928166986, "lex_sim": 1.0}
{"question": "how many stores does lowes have in canada", "answer": ["51"], "scores": [[0.7998409867286682, 0.09130902588367462, 0.091964490711689]], "normalized_score": [0.188674196600914], "tokens": [["\u2581", "5", "1"]], "entropy": [[1.4278558492660522, 2.060027837753296, 2.414534330368042]], "avg_entropy": [1.9674726724624634], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "62", "score_of_biggest_cluster": 0.188674196600914, "score_of_first_answer": 0.188674196600914, "max_score_of_first_answer": 0.7998409867286682, "min_score_of_first_answer": 0.09130902588367462, "avg_score": 0.188674196600914, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3338507821677671, "std_of_first_answer": 0.3338507821677671, "avg_entropy_of_first_answer": 1.9674726724624634, "lex_sim": 1.0}
{"question": "which nfl coach has the most superbowl rings", "answer": ["George Schembechler"], "scores": [[0.022540081292390823, 0.011142936535179615, 0.3959859013557434, 0.9242697954177856, 0.996769905090332, 0.998767614364624]], "normalized_score": [0.2122831493616104], "tokens": [["\u2581George", "\u2581Sch", "em", "be", "ch", "ler"]], "entropy": [[2.9972639083862305, 2.0456104278564453, 3.131836414337158, 0.5794938802719116, 0.029391402378678322, 0.012086303904652596]], "avg_entropy": [1.4659470561891794], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bill Belichick", "score_of_biggest_cluster": 0.2122831493616104, "score_of_first_answer": 0.2122831493616104, "max_score_of_first_answer": 0.998767614364624, "min_score_of_first_answer": 0.011142936535179615, "avg_score": 0.2122831493616104, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43454321992047384, "std_of_first_answer": 0.43454321992047384, "avg_entropy_of_first_answer": 1.4659470561891794, "lex_sim": 1.0}
{"question": "who does the music for shimmer and shine", "answer": ["Micheal Todd"], "scores": [[0.003957661800086498, 0.14762012660503387, 0.7132691144943237, 0.009928587824106216, 0.9815580248832703]], "normalized_score": [0.08350803703069687], "tokens": [["\u2581M", "iche", "al", "\u2581Tod", "d"]], "entropy": [[5.818225383758545, 4.399350643157959, 0.6564710736274719, 4.573594093322754, 0.15037311613559723]], "avg_entropy": [3.119602862000465], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bobby Tahouri", "score_of_biggest_cluster": 0.08350803703069687, "score_of_first_answer": 0.08350803703069687, "max_score_of_first_answer": 0.9815580248832703, "min_score_of_first_answer": 0.003957661800086498, "avg_score": 0.08350803703069687, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.401228140437138, "std_of_first_answer": 0.401228140437138, "avg_entropy_of_first_answer": 3.119602862000465, "lex_sim": 1.0}
{"question": "who sings jungle book i wanna be like you", "answer": ["Christopher Walken"], "scores": [[0.06390301138162613, 0.16709576547145844, 0.9988306164741516]], "normalized_score": [0.22012001276016235], "tokens": [["\u2581Christopher", "\u2581Wal", "ken"]], "entropy": [[3.682880401611328, 2.748065948486328, 0.011209346354007721]], "avg_entropy": [2.1473852321505547], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Louis Prima.", "score_of_biggest_cluster": 0.22012001276016235, "score_of_first_answer": 0.22012001276016235, "max_score_of_first_answer": 0.9988306164741516, "min_score_of_first_answer": 0.06390301138162613, "avg_score": 0.22012001276016235, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41853198917076845, "std_of_first_answer": 0.41853198917076845, "avg_entropy_of_first_answer": 2.1473852321505547, "lex_sim": 1.0}
{"question": "how did early humans make use of stones during the prehistoric period", "answer": ["as weapons"], "scores": [[0.02796727418899536, 0.17266808450222015]], "normalized_score": [0.0694914162158966], "tokens": [["\u2581as", "\u2581weapons"]], "entropy": [[5.187084197998047, 2.390878200531006]], "avg_entropy": [3.7889811992645264], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "hunt", "score_of_biggest_cluster": 0.0694914162158966, "score_of_first_answer": 0.0694914162158966, "max_score_of_first_answer": 0.17266808450222015, "min_score_of_first_answer": 0.02796727418899536, "avg_score": 0.0694914162158966, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0723504051566124, "std_of_first_answer": 0.0723504051566124, "avg_entropy_of_first_answer": 3.7889811992645264, "lex_sim": 1.0}
{"question": "who owns the biggest house in beverly hills", "answer": ["Dr. Dre"], "scores": [[0.003930706065148115, 0.882939338684082, 0.48238757252693176]], "normalized_score": [0.11874058842658997], "tokens": [["\u2581Dr", ".", "\u2581Dre"]], "entropy": [[5.16823673248291, 0.8637252449989319, 3.390871286392212]], "avg_entropy": [3.1409444212913513], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Anthony Pritzker", "score_of_biggest_cluster": 0.11874058842658997, "score_of_first_answer": 0.11874058842658997, "max_score_of_first_answer": 0.882939338684082, "min_score_of_first_answer": 0.003930706065148115, "avg_score": 0.11874058842658997, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3593232631017259, "std_of_first_answer": 0.3593232631017259, "avg_entropy_of_first_answer": 3.1409444212913513, "lex_sim": 1.0}
{"question": "who played sandy's jock boyfriend in grease", "answer": ["Dean Paul Martin"], "scores": [[0.0051348572596907616, 0.06562787294387817, 0.9496158361434937]], "normalized_score": [0.06839979439973831], "tokens": [["\u2581Dean", "\u2581Paul", "\u2581Martin"]], "entropy": [[4.323137283325195, 2.2233197689056396, 0.3181077837944031]], "avg_entropy": [2.2881882786750793], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Travolta", "score_of_biggest_cluster": 0.06839979439973831, "score_of_first_answer": 0.06839979439973831, "max_score_of_first_answer": 0.9496158361434937, "min_score_of_first_answer": 0.0051348572596907616, "avg_score": 0.06839979439973831, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43168126661303124, "std_of_first_answer": 0.43168126661303124, "avg_entropy_of_first_answer": 2.2881882786750793, "lex_sim": 1.0}
{"question": "who did the broncos beat in the super bowl", "answer": ["Seahawks"], "scores": [[0.061658576130867004, 0.993069052696228, 0.9956555366516113, 0.9997429251670837]], "normalized_score": [0.4968697726726532], "tokens": [["\u2581Se", "ah", "aw", "ks"]], "entropy": [[2.827873706817627, 0.062484513968229294, 0.03494669124484062, 0.0031071961857378483]], "avg_entropy": [0.7321030270541087], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Carolina Panthers", "score_of_biggest_cluster": 0.4968697726726532, "score_of_first_answer": 0.4968697726726532, "max_score_of_first_answer": 0.9997429251670837, "min_score_of_first_answer": 0.061658576130867004, "avg_score": 0.4968697726726532, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4046561797719677, "std_of_first_answer": 0.4046561797719677, "avg_entropy_of_first_answer": 0.7321030270541087, "lex_sim": 1.0}
{"question": "when did the east india company take control of india", "answer": ["November 1, 1905"], "scores": [[0.0036477050743997097, 0.9668752551078796, 0.47048038244247437, 0.3687860071659088, 0.9894033670425415, 0.9993036985397339, 0.027411652728915215, 0.015958044677972794, 0.5987436175346375]], "normalized_score": [0.17556460201740265], "tokens": [["\u2581November", "\u2581", "1", ",", "\u2581", "1", "9", "0", "5"]], "entropy": [[1.4602395296096802, 0.19083701074123383, 1.4371140003204346, 1.9010660648345947, 0.06628119945526123, 0.006245272234082222, 1.0641248226165771, 1.0093265771865845, 1.5459978580474854]], "avg_entropy": [0.9645813705606593], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in 1757", "score_of_biggest_cluster": 0.17556460201740265, "score_of_first_answer": 0.17556460201740265, "max_score_of_first_answer": 0.9993036985397339, "min_score_of_first_answer": 0.0036477050743997097, "avg_score": 0.17556460201740265, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39975564956275256, "std_of_first_answer": 0.39975564956275256, "avg_entropy_of_first_answer": 0.9645813705606593, "lex_sim": 1.0}
{"question": "what is the main mineral in lithium batteries", "answer": ["carbon dioxide"], "scores": [[0.0267172958701849, 0.01783074624836445, 0.9995377063751221, 0.9987278580665588]], "normalized_score": [0.1476733237504959], "tokens": [["\u2581carbon", "\u2581dio", "x", "ide"]], "entropy": [[3.179478645324707, 0.8104541897773743, 0.004834960214793682, 0.013992948457598686]], "avg_entropy": [1.0021901859436184], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lithium", "score_of_biggest_cluster": 0.1476733237504959, "score_of_first_answer": 0.1476733237504959, "max_score_of_first_answer": 0.9995377063751221, "min_score_of_first_answer": 0.01783074624836445, "avg_score": 0.1476733237504959, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.48843956959058427, "std_of_first_answer": 0.48843956959058427, "avg_entropy_of_first_answer": 1.0021901859436184, "lex_sim": 1.0}
{"question": "who turned out to be the mother on how i met your mother", "answer": ["Barney Stinson"], "scores": [[0.06435753405094147, 0.932753324508667, 0.7346572875976562, 0.9947596192359924]], "normalized_score": [0.4576593041419983], "tokens": [["\u2581Bar", "ney", "\u2581St", "inson"]], "entropy": [[4.180075645446777, 0.34942662715911865, 0.9898492097854614, 0.0452486127614975]], "avg_entropy": [1.3911500237882137], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tracy McConnell", "score_of_biggest_cluster": 0.4576593041419983, "score_of_first_answer": 0.4576593041419983, "max_score_of_first_answer": 0.9947596192359924, "min_score_of_first_answer": 0.06435753405094147, "avg_score": 0.4576593041419983, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36910373643085886, "std_of_first_answer": 0.36910373643085886, "avg_entropy_of_first_answer": 1.3911500237882137, "lex_sim": 1.0}
{"question": "who plays reggie the robot in justin's house", "answer": ["Johnny Pemberton"], "scores": [[0.006405363325029612, 0.13393361866474152, 0.9720075726509094, 0.998980700969696]], "normalized_score": [0.1698889136314392], "tokens": [["\u2581Johnny", "\u2581P", "ember", "ton"]], "entropy": [[6.079362869262695, 5.014059066772461, 0.26326799392700195, 0.010545851662755013]], "avg_entropy": [2.8418089454062283], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ben Faulks", "score_of_biggest_cluster": 0.1698889136314392, "score_of_first_answer": 0.1698889136314392, "max_score_of_first_answer": 0.998980700969696, "min_score_of_first_answer": 0.006405363325029612, "avg_score": 0.1698889136314392, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45997682279049806, "std_of_first_answer": 0.45997682279049806, "avg_entropy_of_first_answer": 2.8418089454062283, "lex_sim": 1.0}
{"question": "who was the first imperial power that was fighting in vietnam", "answer": ["USA"], "scores": [[0.048282962292432785]], "normalized_score": [0.04828295856714249], "tokens": [["\u2581USA"]], "entropy": [[2.819972515106201]], "avg_entropy": [2.819972515106201], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "France", "score_of_biggest_cluster": 0.04828295856714249, "score_of_first_answer": 0.04828295856714249, "max_score_of_first_answer": 0.048282962292432785, "min_score_of_first_answer": 0.048282962292432785, "avg_score": 0.04828295856714249, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.819972515106201, "lex_sim": 1.0}
{"question": "what is the point of a menstrual cup", "answer": ["It collects the menstrual blood and can be reused for"], "scores": [[0.15362390875816345, 0.36793768405914307, 0.9962857961654663, 0.12341076135635376, 0.30557382106781006, 0.9877073168754578, 0.9847954511642456, 0.5480934977531433, 0.14548294246196747, 0.11172415316104889, 0.7899730801582336, 0.31264981627464294, 0.8660988211631775, 0.3340813219547272]], "normalized_score": [0.38012465834617615], "tokens": [["\u2581It", "\u2581collect", "s", "\u2581the", "\u2581men", "stru", "al", "\u2581blood", "\u2581and", "\u2581can", "\u2581be", "\u2581re", "used", "\u2581for"]], "entropy": [[4.592691421508789, 2.527399778366089, 0.03775762394070625, 2.1974425315856934, 1.8791043758392334, 0.0831158310174942, 0.09482674300670624, 1.2994356155395508, 3.1524908542633057, 4.362936496734619, 1.2275142669677734, 2.0285141468048096, 0.5417567491531372, 2.455379009246826]], "avg_entropy": [1.891454674569624], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "to collect menstrual flow", "score_of_biggest_cluster": 0.38012465834617615, "score_of_first_answer": 0.38012465834617615, "max_score_of_first_answer": 0.9962857961654663, "min_score_of_first_answer": 0.11172415316104889, "avg_score": 0.38012465834617615, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.33760331883533995, "std_of_first_answer": 0.33760331883533995, "avg_entropy_of_first_answer": 1.891454674569624, "lex_sim": 1.0}
{"question": "what is the name of the protease which is released in the stomach", "answer": ["pepsin"], "scores": [[0.4645739197731018, 0.9550130367279053, 0.9793279767036438]], "normalized_score": [0.7574095129966736], "tokens": [["\u2581pe", "ps", "in"]], "entropy": [[2.2468159198760986, 0.20033305883407593, 0.16836971044540405]], "avg_entropy": [0.8718395630518595], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pepsin", "score_of_biggest_cluster": 0.7574095129966736, "score_of_first_answer": 0.7574095129966736, "max_score_of_first_answer": 0.9793279767036438, "min_score_of_first_answer": 0.4645739197731018, "avg_score": 0.7574095129966736, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2371341586697011, "std_of_first_answer": 0.2371341586697011, "avg_entropy_of_first_answer": 0.8718395630518595, "lex_sim": 1.0}
{"question": "who is mowgli's main enemy in the jungle book", "answer": ["Shere Khan"], "scores": [[0.653317391872406, 0.9925954937934875, 0.9091655015945435]], "normalized_score": [0.8385194540023804], "tokens": [["\u2581Sh", "ere", "\u2581Khan"]], "entropy": [[1.7307465076446533, 0.07220068573951721, 0.393221378326416]], "avg_entropy": [0.7320561905701956], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Shere Khan", "score_of_biggest_cluster": 0.8385194540023804, "score_of_first_answer": 0.8385194540023804, "max_score_of_first_answer": 0.9925954937934875, "min_score_of_first_answer": 0.653317391872406, "avg_score": 0.8385194540023804, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1443485170188529, "std_of_first_answer": 0.1443485170188529, "avg_entropy_of_first_answer": 0.7320561905701956, "lex_sim": 1.0}
{"question": "who sang how long has this been going on song", "answer": ["The Stylistics"], "scores": [[0.03640418127179146, 0.30001696944236755, 0.3819495141506195, 0.9950206279754639, 0.9994540810585022]], "normalized_score": [0.3338717222213745], "tokens": [["\u2581The", "\u2581St", "y", "list", "ics"]], "entropy": [[5.54822301864624, 3.8759360313415527, 0.9658763408660889, 0.04038235545158386, 0.00497405044734478]], "avg_entropy": [2.087078359350562], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ace", "score_of_biggest_cluster": 0.3338717222213745, "score_of_first_answer": 0.3338717222213745, "max_score_of_first_answer": 0.9994540810585022, "min_score_of_first_answer": 0.03640418127179146, "avg_score": 0.3338717222213745, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38840426965515296, "std_of_first_answer": 0.38840426965515296, "avg_entropy_of_first_answer": 2.087078359350562, "lex_sim": 1.0}
{"question": "where was the first high level language operating system created", "answer": ["IBM JOSS"], "scores": [[0.2013249546289444, 0.0033171053510159254, 0.08740762621164322, 0.9893403649330139]], "normalized_score": [0.08717420697212219], "tokens": [["\u2581IBM", "\u2581J", "O", "SS"]], "entropy": [[4.472776412963867, 2.2144970893859863, 2.8811964988708496, 0.09560592472553253]], "avg_entropy": [2.416018981486559], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "at IBM", "score_of_biggest_cluster": 0.08717420697212219, "score_of_first_answer": 0.08717420697212219, "max_score_of_first_answer": 0.9893403649330139, "min_score_of_first_answer": 0.0033171053510159254, "avg_score": 0.08717420697212219, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3925834546850237, "std_of_first_answer": 0.3925834546850237, "avg_entropy_of_first_answer": 2.416018981486559, "lex_sim": 1.0}
{"question": "who plays harley quinn in the lego batman movie", "answer": ["Margot Robbie"], "scores": [[0.8070294857025146, 0.993059515953064, 0.8679049015045166, 0.9917730689048767]], "normalized_score": [0.9113543629646301], "tokens": [["\u2581Mar", "got", "\u2581Rob", "bie"]], "entropy": [[1.0896035432815552, 0.05699871852993965, 0.9097955226898193, 0.06551644951105118]], "avg_entropy": [0.5304785585030913], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jenny Slate", "score_of_biggest_cluster": 0.9113543629646301, "score_of_first_answer": 0.9113543629646301, "max_score_of_first_answer": 0.993059515953064, "min_score_of_first_answer": 0.8070294857025146, "avg_score": 0.9113543629646301, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0804098234427202, "std_of_first_answer": 0.0804098234427202, "avg_entropy_of_first_answer": 0.5304785585030913, "lex_sim": 1.0}
{"question": "who plays the robot on the orville show", "answer": ["Alara Kitan"], "scores": [[0.009282157756388187, 0.9312373995780945, 0.721832811832428, 0.9858296513557434]], "normalized_score": [0.2800509035587311], "tokens": [["\u2581Al", "ara", "\u2581K", "itan"]], "entropy": [[4.11415958404541, 0.5040756464004517, 1.4967725276947021, 0.13015027344226837]], "avg_entropy": [1.561289507895708], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mark Jackson", "score_of_biggest_cluster": 0.2800509035587311, "score_of_first_answer": 0.2800509035587311, "max_score_of_first_answer": 0.9858296513557434, "min_score_of_first_answer": 0.009282157756388187, "avg_score": 0.2800509035587311, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3895429883532819, "std_of_first_answer": 0.3895429883532819, "avg_entropy_of_first_answer": 1.561289507895708, "lex_sim": 1.0}
{"question": "when did television come out in the us", "answer": ["1939"], "scores": [[0.6618356704711914, 0.9855397939682007, 0.9789894223213196, 0.1621662676334381, 0.8167040944099426]], "normalized_score": [0.6101624965667725], "tokens": [["\u2581", "1", "9", "3", "9"]], "entropy": [[1.9425795078277588, 0.0980372428894043, 0.13621786236763, 1.2646456956863403, 0.8191790580749512]], "avg_entropy": [0.8521318733692169], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "928,", "score_of_biggest_cluster": 0.6101624965667725, "score_of_first_answer": 0.6101624965667725, "max_score_of_first_answer": 0.9855397939682007, "min_score_of_first_answer": 0.1621662676334381, "avg_score": 0.6101624965667725, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30380739476194096, "std_of_first_answer": 0.30380739476194096, "avg_entropy_of_first_answer": 0.8521318733692169, "lex_sim": 1.0}
{"question": "when did vat change from 17.5 to 20", "answer": ["April 1, 2015"], "scores": [[0.04296179115772247, 0.9521817564964294, 0.8687461018562317, 0.47256413102149963, 0.9862375259399414, 0.3236609697341919, 0.9995550513267517, 0.8383035659790039, 0.14337502419948578]], "normalized_score": [0.44201233983039856], "tokens": [["\u2581April", "\u2581", "1", ",", "\u2581", "2", "0", "1", "5"]], "entropy": [[2.750546455383301, 0.2566149830818176, 0.5573275685310364, 1.3129515647888184, 0.08857275545597076, 0.6330257654190063, 0.005276982206851244, 0.4520808756351471, 2.205742359161377]], "avg_entropy": [0.9180154788514806], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "4 January 2011", "score_of_biggest_cluster": 0.44201233983039856, "score_of_first_answer": 0.44201233983039856, "max_score_of_first_answer": 0.9995550513267517, "min_score_of_first_answer": 0.04296179115772247, "avg_score": 0.44201233983039856, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3601719910170048, "std_of_first_answer": 0.3601719910170048, "avg_entropy_of_first_answer": 0.9180154788514806, "lex_sim": 1.0}
{"question": "which approach to psychology focuses on the body especially the brain and nervous system", "answer": ["biological psychology"], "scores": [[0.1197071224451065, 0.6307805776596069, 0.10881204903125763, 0.9911296367645264]], "normalized_score": [0.30040106177330017], "tokens": [["\u2581bi", "ological", "\u2581psych", "ology"]], "entropy": [[3.1783387660980225, 0.9660271406173706, 1.0507903099060059, 0.07208635658025742]], "avg_entropy": [1.316810643300414], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "neuropsychology", "score_of_biggest_cluster": 0.30040106177330017, "score_of_first_answer": 0.30040106177330017, "max_score_of_first_answer": 0.9911296367645264, "min_score_of_first_answer": 0.10881204903125763, "avg_score": 0.30040106177330017, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37093453706584634, "std_of_first_answer": 0.37093453706584634, "avg_entropy_of_first_answer": 1.316810643300414, "lex_sim": 1.0}
{"question": "jharkhand which festival is associated with cattle worship", "answer": ["Chhath Puja"], "scores": [[0.020551666617393494, 0.5353885889053345, 0.8564159274101257, 0.341996431350708, 0.9963956475257874]], "normalized_score": [0.317198246717453], "tokens": [["\u2581Ch", "h", "ath", "\u2581Pu", "ja"]], "entropy": [[4.812699794769287, 1.9448010921478271, 0.7049378156661987, 1.226698875427246, 0.03328506648540497]], "avg_entropy": [1.7444845288991928], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sohrai", "score_of_biggest_cluster": 0.317198246717453, "score_of_first_answer": 0.317198246717453, "max_score_of_first_answer": 0.9963956475257874, "min_score_of_first_answer": 0.020551666617393494, "avg_score": 0.317198246717453, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35127032278519205, "std_of_first_answer": 0.35127032278519205, "avg_entropy_of_first_answer": 1.7444845288991928, "lex_sim": 1.0}
{"question": "who are the nbc olympic ice skating commentators", "answer": ["Sandra Bezic"], "scores": [[0.01038240734487772, 0.9596458673477173, 0.9795935153961182, 0.9912910461425781]], "normalized_score": [0.3136274218559265], "tokens": [["\u2581Sand", "ra", "\u2581Bez", "ic"]], "entropy": [[3.5992512702941895, 0.20263908803462982, 0.19627708196640015, 0.08278437703847885]], "avg_entropy": [1.0202379543334246], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jonny Moseley", "score_of_biggest_cluster": 0.3136274218559265, "score_of_first_answer": 0.3136274218559265, "max_score_of_first_answer": 0.9912910461425781, "min_score_of_first_answer": 0.01038240734487772, "avg_score": 0.3136274218559265, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41864283735987434, "std_of_first_answer": 0.41864283735987434, "avg_entropy_of_first_answer": 1.0202379543334246, "lex_sim": 1.0}
{"question": "who sings the theme tune to mum on bbc2", "answer": ["Rachel Allen"], "scores": [[0.009641709737479687, 0.9451852440834045, 0.01885557547211647]], "normalized_score": [0.05559515953063965], "tokens": [["\u2581Rach", "el", "\u2581Allen"]], "entropy": [[6.046138763427734, 0.25588980317115784, 4.719198226928711]], "avg_entropy": [3.673742264509201], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lulu and the Lampshades", "score_of_biggest_cluster": 0.05559515953063965, "score_of_first_answer": 0.05559515953063965, "max_score_of_first_answer": 0.9451852440834045, "min_score_of_first_answer": 0.009641709737479687, "avg_score": 0.05559515953063965, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43886384308774945, "std_of_first_answer": 0.43886384308774945, "avg_entropy_of_first_answer": 3.673742264509201, "lex_sim": 1.0}
{"question": "who won the oscar for best actor when titanic was nominated", "answer": ["Jack Nicholson"], "scores": [[0.2461891919374466, 0.9277300238609314, 0.9962455630302429, 0.9995946288108826]], "normalized_score": [0.6905899047851562], "tokens": [["\u2581Jack", "\u2581Nich", "ol", "son"]], "entropy": [[2.997318744659424, 0.47222045063972473, 0.030452903360128403, 0.004454711452126503]], "avg_entropy": [0.8761117025278509], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jack Nicholson", "score_of_biggest_cluster": 0.6905899047851562, "score_of_first_answer": 0.6905899047851562, "max_score_of_first_answer": 0.9995946288108826, "min_score_of_first_answer": 0.2461891919374466, "avg_score": 0.6905899047851562, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.31667928714436194, "std_of_first_answer": 0.31667928714436194, "avg_entropy_of_first_answer": 0.8761117025278509, "lex_sim": 1.0}
{"question": "when did cristiano ronaldo go to manchester united", "answer": ["August 17, 2003"], "scores": [[0.13675320148468018, 0.9482769966125488, 0.5278841257095337, 0.00883873924612999, 0.6496737003326416, 0.9888448119163513, 0.9948335886001587, 0.9993488192558289, 0.9119470715522766, 0.9504578709602356]], "normalized_score": [0.4492618143558502], "tokens": [["\u2581August", "\u2581", "1", "7", ",", "\u2581", "2", "0", "0", "3"]], "entropy": [[1.9708077907562256, 0.2691369652748108, 1.275792121887207, 0.9598366022109985, 0.807356595993042, 0.06661953032016754, 0.03637389838695526, 0.007848618552088737, 0.30922719836235046, 0.28149110078811646]], "avg_entropy": [0.5984490422531963], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2003", "score_of_biggest_cluster": 0.4492618143558502, "score_of_first_answer": 0.4492618143558502, "max_score_of_first_answer": 0.9993488192558289, "min_score_of_first_answer": 0.00883873924612999, "avg_score": 0.4492618143558502, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.35426006173641694, "std_of_first_answer": 0.35426006173641694, "avg_entropy_of_first_answer": 0.5984490422531963, "lex_sim": 1.0}
{"question": "who is the first wife on sister wives", "answer": ["Janelle (Kody Brown)"], "scores": [[0.05335485562682152, 0.9990770816802979, 0.001898334943689406, 0.014386242255568504, 0.9358702898025513, 0.4326136112213135, 0.8136023283004761]], "normalized_score": [0.12510080635547638], "tokens": [["\u2581Jan", "elle", "\u2581(", "K", "ody", "\u2581Brown", ")"]], "entropy": [[2.4782891273498535, 0.008745917119085789, 0.9200103282928467, 4.998232841491699, 0.4763283431529999, 1.9066674709320068, 1.0387742519378662]], "avg_entropy": [1.6895783257537655], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Meri", "score_of_biggest_cluster": 0.12510080635547638, "score_of_first_answer": 0.12510080635547638, "max_score_of_first_answer": 0.9990770816802979, "min_score_of_first_answer": 0.001898334943689406, "avg_score": 0.12510080635547638, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4168752364734459, "std_of_first_answer": 0.4168752364734459, "avg_entropy_of_first_answer": 1.6895783257537655, "lex_sim": 1.0}
{"question": "who played the girl in silence of the lambs", "answer": ["Jodie Foster This is an AI that will learn from me, in order to have a human conversation with another human."], "scores": [[0.8924129605293274, 0.835094690322876, 0.9940444827079773, 0.9815160036087036, 0.9991881251335144, 1.1984018328803359e-06, 0.26277852058410645, 0.12111524492502213, 0.15130850672721863, 0.9346320033073425, 0.013402733020484447, 0.046358440071344376, 0.03638041019439697, 0.2614324986934662, 0.00367962964810431, 0.08728163689374924, 0.003778060432523489, 0.20143678784370422, 0.8370184302330017, 0.008520195260643959, 0.30901190638542175, 0.01751808077096939, 0.16441763937473297, 0.17697416245937347, 0.009818049147725105, 0.5707329511642456, 0.4699193239212036, 0.008696544915437698, 0.028677400201559067, 0.02522246539592743, 0.7497127652168274, 0.14003708958625793, 0.6465364098548889, 0.028058238327503204, 0.014879791997373104, 0.9390329718589783, 0.013489700853824615, 0.9927818775177002, 0.985678493976593, 0.2320389747619629, 0.03038090467453003, 0.22594843804836273, 0.23517216742038727, 0.09908817708492279, 0.23645654320716858, 0.008365673944354057, 0.057593222707509995, 0.2201431393623352, 0.004434923641383648]], "normalized_score": [0.08760510385036469], "tokens": [["\u2581J", "od", "ie", "\u2581F", "oster", "\u2581This", "\u2581is", "\u2581an", "\u2581A", "I", "\u2581that", "\u2581will", "\u2581learn", "\u2581from", "\u2581me", ",", "\u2581in", "\u2581order", "\u2581to", "\u2581have", "\u2581a", "\u2581human", "\u2581conversation", "\u2581with", "\u2581another", "\u2581human", ".", "\u2581When", "ever", "\u2581possible", ",", "\u2581it", "\u2581will", "\u2581be", "\u2581trying", "\u2581to", "\u2581m", "im", "ic", "\u2581the", "\u2581language", "\u2581of", "\u2581a", "\u2581person", ".", "\u2581A", "\u2581human", ".", "\u2581Let"]], "entropy": [[0.8695931434631348, 0.5888119339942932, 0.045110832899808884, 0.17247651517391205, 0.0077551547437906265, 0.1546059250831604, 4.701408386230469, 3.7211241722106934, 4.302153587341309, 0.49759340286254883, 3.6392056941986084, 4.058073043823242, 4.483913421630859, 3.016096591949463, 3.0342037677764893, 2.839576244354248, 4.184614181518555, 3.4561257362365723, 0.565740704536438, 4.085803508758545, 3.1960997581481934, 4.081926345825195, 3.6455953121185303, 2.270703077316284, 2.5939502716064453, 1.6811647415161133, 1.9798343181610107, 4.028348445892334, 2.9604058265686035, 2.3726253509521484, 1.288678526878357, 3.279846429824829, 1.7077338695526123, 4.469047546386719, 5.249532699584961, 0.4443574547767639, 4.729574680328369, 0.06349387764930725, 0.08418586105108261, 2.976900577545166, 4.133271217346191, 2.702968120574951, 2.299152135848999, 2.2225465774536133, 3.2727646827697754, 3.9399993419647217, 3.8907670974731445, 3.8573172092437744, 4.138211250305176]], "avg_entropy": [2.6935711949683574], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Masha Skorobogatov", "score_of_biggest_cluster": 0.08760510385036469, "score_of_first_answer": 0.08760510385036469, "max_score_of_first_answer": 0.9991881251335144, "min_score_of_first_answer": 1.1984018328803359e-06, "avg_score": 0.08760510385036469, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3580172533119394, "std_of_first_answer": 0.3580172533119394, "avg_entropy_of_first_answer": 2.6935711949683574, "lex_sim": 1.0}
{"question": "which greek god flew too close to the sun", "answer": ["Ikaros"], "scores": [[0.4516707956790924, 0.002227382967248559, 0.77125084400177]], "normalized_score": [0.09189053624868393], "tokens": [["\u2581I", "kar", "os"]], "entropy": [[2.3390462398529053, 0.07734908163547516, 0.6402362585067749]], "avg_entropy": [1.0188771933317184], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Icarus", "score_of_biggest_cluster": 0.09189053624868393, "score_of_first_answer": 0.09189053624868393, "max_score_of_first_answer": 0.77125084400177, "min_score_of_first_answer": 0.002227382967248559, "avg_score": 0.09189053624868393, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.31544111479667253, "std_of_first_answer": 0.31544111479667253, "avg_entropy_of_first_answer": 1.0188771933317184, "lex_sim": 1.0}
{"question": "who are the judges on do you think you can dance", "answer": ["Judging panel"], "scores": [[0.0056586056016385555, 0.0018928500358015299, 0.3123798370361328]], "normalized_score": [0.014956717379391193], "tokens": [["\u2581Jud", "ging", "\u2581panel"]], "entropy": [[4.413335800170898, 1.0034953355789185, 2.3623030185699463]], "avg_entropy": [2.593044718106588], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nigel Lythgoe", "score_of_biggest_cluster": 0.014956717379391193, "score_of_first_answer": 0.014956717379391193, "max_score_of_first_answer": 0.3123798370361328, "min_score_of_first_answer": 0.0018928500358015299, "avg_score": 0.014956717379391193, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1454854952007313, "std_of_first_answer": 0.1454854952007313, "avg_entropy_of_first_answer": 2.593044718106588, "lex_sim": 1.0}
{"question": "in which sea pearl is found in india", "answer": ["Andaman"], "scores": [[0.0248375553637743, 0.9674116373062134]], "normalized_score": [0.1550101488828659], "tokens": [["\u2581And", "aman"]], "entropy": [[2.1158194541931152, 0.18155723810195923]], "avg_entropy": [1.1486883461475372], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Indian Ocean", "score_of_biggest_cluster": 0.1550101488828659, "score_of_first_answer": 0.1550101488828659, "max_score_of_first_answer": 0.9674116373062134, "min_score_of_first_answer": 0.0248375553637743, "avg_score": 0.1550101488828659, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.47128704097121954, "std_of_first_answer": 0.47128704097121954, "avg_entropy_of_first_answer": 1.1486883461475372, "lex_sim": 1.0}
{"question": "when was catch me if you can made", "answer": ["2002"], "scores": [[0.7886327505111694, 0.9671928882598877, 0.9947846531867981, 0.9967849254608154, 0.8039436936378479]], "normalized_score": [0.905292272567749], "tokens": [["\u2581", "2", "0", "0", "2"]], "entropy": [[1.1332387924194336, 0.18319010734558105, 0.046016812324523926, 0.02746308222413063, 0.5654022097587585]], "avg_entropy": [0.39106220081448556], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2002", "score_of_biggest_cluster": 0.905292272567749, "score_of_first_answer": 0.905292272567749, "max_score_of_first_answer": 0.9967849254608154, "min_score_of_first_answer": 0.7886327505111694, "avg_score": 0.905292272567749, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.09377491988997502, "std_of_first_answer": 0.09377491988997502, "avg_entropy_of_first_answer": 0.39106220081448556, "lex_sim": 1.0}
{"question": "who has the most yards per carry in nfl history", "answer": ["Barry Sanders"], "scores": [[0.20132601261138916, 0.9718348383903503, 0.9987730383872986]], "normalized_score": [0.5803006291389465], "tokens": [["\u2581Barry", "\u2581Sand", "ers"]], "entropy": [[3.0908594131469727, 0.2576977014541626, 0.014945008791983128]], "avg_entropy": [1.1211673744643729], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Emmitt Smith", "score_of_biggest_cluster": 0.5803006291389465, "score_of_first_answer": 0.5803006291389465, "max_score_of_first_answer": 0.9987730383872986, "min_score_of_first_answer": 0.20132601261138916, "avg_score": 0.5803006291389465, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36973433046824206, "std_of_first_answer": 0.36973433046824206, "avg_entropy_of_first_answer": 1.1211673744643729, "lex_sim": 1.0}
{"question": "who plays the dad in pretty in pink", "answer": ["Andrew McCarthy"], "scores": [[0.05824537202715874, 0.9614324569702148, 0.9842479228973389, 0.9984287619590759]], "normalized_score": [0.4843401610851288], "tokens": [["\u2581Andrew", "\u2581Mc", "Car", "thy"]], "entropy": [[2.633955478668213, 0.3018726706504822, 0.11848848313093185, 0.015229807235300541]], "avg_entropy": [0.7673866099212319], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Harry Dean Stanton", "score_of_biggest_cluster": 0.4843401610851288, "score_of_first_answer": 0.4843401610851288, "max_score_of_first_answer": 0.9984287619590759, "min_score_of_first_answer": 0.05824537202715874, "avg_score": 0.4843401610851288, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3999424028527954, "std_of_first_answer": 0.3999424028527954, "avg_entropy_of_first_answer": 0.7673866099212319, "lex_sim": 1.0}
{"question": "who has the most conference championships in college basketball", "answer": ["Kentucky Wildcats"], "scores": [[0.18654265999794006, 0.2305920422077179, 0.9938344955444336, 0.9988143444061279]], "normalized_score": [0.4545747935771942], "tokens": [["\u2581Kentucky", "\u2581Wild", "c", "ats"]], "entropy": [[2.7563443183898926, 0.8550499081611633, 0.0466272197663784, 0.01168771181255579]], "avg_entropy": [0.9174272895324975], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kansas", "score_of_biggest_cluster": 0.4545747935771942, "score_of_first_answer": 0.4545747935771942, "max_score_of_first_answer": 0.9988143444061279, "min_score_of_first_answer": 0.18654265999794006, "avg_score": 0.4545747935771942, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3941902373988957, "std_of_first_answer": 0.3941902373988957, "avg_entropy_of_first_answer": 0.9174272895324975, "lex_sim": 1.0}
{"question": "who came up with the first working periodic table", "answer": ["Dmitri Ivanenko Mendeleev"], "scores": [[0.6025581955909729, 0.9989646673202515, 0.9396222233772278, 0.1596154272556305, 0.015629390254616737, 0.9976997971534729, 0.3676857352256775, 0.992007851600647, 0.8499467968940735, 0.9739064574241638]], "normalized_score": [0.46008914709091187], "tokens": [["\u2581D", "mit", "ri", "\u2581Ivan", "en", "ko", "\u2581M", "ende", "le", "ev"]], "entropy": [[1.9340918064117432, 0.011831182986497879, 0.2781890332698822, 0.6961182355880737, 0.22420665621757507, 0.02412685751914978, 1.1270897388458252, 0.05540398135781288, 0.5017988085746765, 0.18194922804832458]], "avg_entropy": [0.5034805528819561], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dmitri Mendeleev", "score_of_biggest_cluster": 0.46008914709091187, "score_of_first_answer": 0.46008914709091187, "max_score_of_first_answer": 0.9989646673202515, "min_score_of_first_answer": 0.015629390254616737, "avg_score": 0.46008914709091187, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.36020267797341865, "std_of_first_answer": 0.36020267797341865, "avg_entropy_of_first_answer": 0.5034805528819561, "lex_sim": 1.0}
{"question": "how many times has south korea held the winter olympics", "answer": ["2002, 1988"], "scores": [[0.14664818346500397, 0.738339364528656, 0.041114453226327896, 0.18536366522312164, 0.8358390927314758, 0.22221525013446808, 0.9189302325248718, 0.07379230856895447, 0.9937286376953125, 0.9183928966522217, 0.9943806529045105]], "normalized_score": [0.34925657510757446], "tokens": [["\u2581", "2", "0", "0", "2", ",", "\u2581", "1", "9", "8", "8"]], "entropy": [[3.6438536643981934, 1.1019680500030518, 1.3382209539413452, 0.851389467716217, 0.9056283235549927, 2.2314867973327637, 0.5367438793182373, 0.27735644578933716, 0.0560164675116539, 0.35015633702278137, 0.04262281954288483]], "avg_entropy": [1.0304948369210416], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "two", "score_of_biggest_cluster": 0.34925657510757446, "score_of_first_answer": 0.34925657510757446, "max_score_of_first_answer": 0.9943806529045105, "min_score_of_first_answer": 0.041114453226327896, "avg_score": 0.34925657510757446, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3898915145285828, "std_of_first_answer": 0.3898915145285828, "avg_entropy_of_first_answer": 1.0304948369210416, "lex_sim": 1.0}
{"question": "who began the age of exploration in portugal", "answer": ["Henry the Navigator"], "scores": [[0.39037713408470154, 0.9260890483856201, 0.9371122121810913, 0.9975111484527588]], "normalized_score": [0.7624504566192627], "tokens": [["\u2581Henry", "\u2581the", "\u2581N", "avigator"]], "entropy": [[2.1400301456451416, 0.4528772532939911, 0.2916009724140167, 0.01904395967721939]], "avg_entropy": [0.7258880827575922], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "King Dinis of Portugal", "score_of_biggest_cluster": 0.7624504566192627, "score_of_first_answer": 0.7624504566192627, "max_score_of_first_answer": 0.9975111484527588, "min_score_of_first_answer": 0.39037713408470154, "avg_score": 0.7624504566192627, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24538093729043312, "std_of_first_answer": 0.24538093729043312, "avg_entropy_of_first_answer": 0.7258880827575922, "lex_sim": 1.0}
{"question": "tujhe dekha toh yeh jana sanam movie name", "answer": ["Tu"], "scores": [[0.21091939508914948]], "normalized_score": [0.21091940999031067], "tokens": [["\u2581Tu"]], "entropy": [[4.0546369552612305]], "avg_entropy": [4.0546369552612305], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dilwale Dulhania Le Jayenge", "score_of_biggest_cluster": 0.21091940999031067, "score_of_first_answer": 0.21091940999031067, "max_score_of_first_answer": 0.21091939508914948, "min_score_of_first_answer": 0.21091939508914948, "avg_score": 0.21091940999031067, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.0546369552612305, "lex_sim": 1.0}
{"question": "who drove the number 18 car in nascar", "answer": ["Tony Stewart"], "scores": [[0.08787935972213745, 0.9315785765647888]], "normalized_score": [0.28612327575683594], "tokens": [["\u2581Tony", "\u2581Stewart"]], "entropy": [[3.8444817066192627, 0.48349690437316895]], "avg_entropy": [2.163989305496216], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kyle Busch", "score_of_biggest_cluster": 0.28612327575683594, "score_of_first_answer": 0.28612327575683594, "max_score_of_first_answer": 0.9315785765647888, "min_score_of_first_answer": 0.08787935972213745, "avg_score": 0.28612327575683594, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4218496084213257, "std_of_first_answer": 0.4218496084213257, "avg_entropy_of_first_answer": 2.163989305496216, "lex_sim": 1.0}
{"question": "who played bubba in the heat of the night", "answer": ["Sidney Poitier"], "scores": [[0.035304855555295944, 0.9227420091629028, 0.9692769050598145, 0.9940418601036072, 0.9994043111801147]], "normalized_score": [0.5003820061683655], "tokens": [["\u2581Sid", "ney", "\u2581Po", "it", "ier"]], "entropy": [[4.514836311340332, 0.48625046014785767, 0.2116595208644867, 0.044569604098796844, 0.005712677258998156]], "avg_entropy": [1.0526057147420942], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Carlos Alan Autry Jr.", "score_of_biggest_cluster": 0.5003820061683655, "score_of_first_answer": 0.5003820061683655, "max_score_of_first_answer": 0.9994043111801147, "min_score_of_first_answer": 0.035304855555295944, "avg_score": 0.5003820061683655, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3754031906308454, "std_of_first_answer": 0.3754031906308454, "avg_entropy_of_first_answer": 1.0526057147420942, "lex_sim": 1.0}
{"question": "who won the ncaa women's championship last year 2017", "answer": ["South Carolina Gamecocks"], "scores": [[0.38174664974212646, 0.9709004163742065, 0.2935256063938141, 0.9829487204551697, 0.9997656941413879]], "normalized_score": [0.6394476294517517], "tokens": [["\u2581South", "\u2581Carolina", "\u2581Game", "co", "cks"]], "entropy": [[2.475252866744995, 0.2386002391576767, 0.8679003715515137, 0.12068894505500793, 0.0027437005192041397]], "avg_entropy": [0.7410372246056796], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "South Carolina Gamecocks", "score_of_biggest_cluster": 0.6394476294517517, "score_of_first_answer": 0.6394476294517517, "max_score_of_first_answer": 0.9997656941413879, "min_score_of_first_answer": 0.2935256063938141, "avg_score": 0.6394476294517517, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.31827369880224515, "std_of_first_answer": 0.31827369880224515, "avg_entropy_of_first_answer": 0.7410372246056796, "lex_sim": 1.0}
{"question": "who signed the declaration of independence for maryland", "answer": ["Charles Carroll"], "scores": [[0.2139730006456375, 0.9977723956108093, 0.9336984157562256]], "normalized_score": [0.5841607451438904], "tokens": [["\u2581Charles", "\u2581Car", "roll"]], "entropy": [[2.580747604370117, 0.023095395416021347, 0.30888843536376953]], "avg_entropy": [0.9709104783833027], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "William Paca", "score_of_biggest_cluster": 0.5841607451438904, "score_of_first_answer": 0.5841607451438904, "max_score_of_first_answer": 0.9977723956108093, "min_score_of_first_answer": 0.2139730006456375, "avg_score": 0.5841607451438904, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3553482860515993, "std_of_first_answer": 0.3553482860515993, "avg_entropy_of_first_answer": 0.9709104783833027, "lex_sim": 1.0}
{"question": "what are the spices in chinese 5 spice powder", "answer": ["Five"], "scores": [[0.01208985224366188]], "normalized_score": [0.012089849449694157], "tokens": [["\u2581Five"]], "entropy": [[3.640763521194458]], "avg_entropy": [3.640763521194458], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sichuan pepper", "score_of_biggest_cluster": 0.012089849449694157, "score_of_first_answer": 0.012089849449694157, "max_score_of_first_answer": 0.01208985224366188, "min_score_of_first_answer": 0.01208985224366188, "avg_score": 0.012089849449694157, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.640763521194458, "lex_sim": 1.0}
{"question": "where is natural gas found in the us", "answer": ["Gulf of Mexico"], "scores": [[0.009313724935054779, 0.9249002933502197, 0.6652940511703491, 0.994082510471344]], "normalized_score": [0.274734765291214], "tokens": [["\u2581G", "ulf", "\u2581of", "\u2581Mexico"]], "entropy": [[5.230869293212891, 0.5261829495429993, 0.9066721200942993, 0.05540911853313446]], "avg_entropy": [1.679783370345831], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Louisiana", "score_of_biggest_cluster": 0.274734765291214, "score_of_first_answer": 0.274734765291214, "max_score_of_first_answer": 0.994082510471344, "min_score_of_first_answer": 0.009313724935054779, "avg_score": 0.274734765291214, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38880119797515966, "std_of_first_answer": 0.38880119797515966, "avg_entropy_of_first_answer": 1.679783370345831, "lex_sim": 1.0}
{"question": "swan lake the sleeping beauty and the nutcracker are three famous ballets by", "answer": ["Tchaikovsky"], "scores": [[0.21756981313228607, 0.9960594177246094, 0.997062623500824, 0.9945955872535706, 0.9960740804672241]], "normalized_score": [0.7346980571746826], "tokens": [["\u2581T", "cha", "ik", "ov", "sky"]], "entropy": [[3.114332914352417, 0.03951764851808548, 0.029300972819328308, 0.04344319552183151, 0.032905854284763336]], "avg_entropy": [0.6519001170992851], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pyotr Ilyich Tchaikovsky", "score_of_biggest_cluster": 0.7346980571746826, "score_of_first_answer": 0.7346980571746826, "max_score_of_first_answer": 0.997062623500824, "min_score_of_first_answer": 0.21756981313228607, "avg_score": 0.7346980571746826, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.311352241208948, "std_of_first_answer": 0.311352241208948, "avg_entropy_of_first_answer": 0.6519001170992851, "lex_sim": 1.0}
{"question": "when did they start vaccinating for whooping cough", "answer": ["2010"], "scores": [[0.7371124625205994, 0.08438565582036972, 0.9818629622459412, 0.3364652693271637, 0.17217950522899628]], "normalized_score": [0.32341113686561584], "tokens": [["\u2581", "2", "0", "1", "0"]], "entropy": [[1.8392057418823242, 0.3809922933578491, 0.1434958577156067, 0.7751317620277405, 2.1304659843444824]], "avg_entropy": [1.0538583278656006], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the 1920s", "score_of_biggest_cluster": 0.32341113686561584, "score_of_first_answer": 0.32341113686561584, "max_score_of_first_answer": 0.9818629622459412, "min_score_of_first_answer": 0.08438565582036972, "avg_score": 0.32341113686561584, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3430134232309066, "std_of_first_answer": 0.3430134232309066, "avg_entropy_of_first_answer": 1.0538583278656006, "lex_sim": 1.0}
{"question": "who wrote the book the origin of species", "answer": ["Charles Robert Darwin"], "scores": [[0.9492590427398682, 0.2360726296901703, 0.9910008311271667, 0.9980216026306152]], "normalized_score": [0.6861373782157898], "tokens": [["\u2581Charles", "\u2581Robert", "\u2581Dar", "win"]], "entropy": [[0.42222440242767334, 0.7384045124053955, 0.07062961906194687, 0.018522238358855247]], "avg_entropy": [0.31244519306346774], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charles Darwin", "score_of_biggest_cluster": 0.6861373782157898, "score_of_first_answer": 0.6861373782157898, "max_score_of_first_answer": 0.9980216026306152, "min_score_of_first_answer": 0.2360726296901703, "avg_score": 0.6861373782157898, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.32242122505214005, "std_of_first_answer": 0.32242122505214005, "avg_entropy_of_first_answer": 0.31244519306346774, "lex_sim": 1.0}
{"question": "what was the date of the signing of the declaration of independence", "answer": ["July 4, 1776"], "scores": [[0.900657057762146, 0.9860040545463562, 0.9526410102844238, 0.3013012111186981, 0.9893544912338257, 0.9984935522079468, 0.9982098340988159, 0.9989405274391174, 0.9982953667640686]], "normalized_score": [0.8574953675270081], "tokens": [["\u2581July", "\u2581", "4", ",", "\u2581", "1", "7", "7", "6"]], "entropy": [[0.5909742116928101, 0.102132648229599, 0.256925106048584, 0.8516777753829956, 0.065813347697258, 0.0121000362560153, 0.015204531140625477, 0.010423380881547928, 0.016270238906145096]], "avg_entropy": [0.21350236402617562], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "August\u00a02,\u00a01776", "score_of_biggest_cluster": 0.8574953675270081, "score_of_first_answer": 0.8574953675270081, "max_score_of_first_answer": 0.9989405274391174, "min_score_of_first_answer": 0.3013012111186981, "avg_score": 0.8574953675270081, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21481945288251378, "std_of_first_answer": 0.21481945288251378, "avg_entropy_of_first_answer": 0.21350236402617562, "lex_sim": 1.0}
{"question": "all time highest goal scorer in spain national team", "answer": ["David Villa"], "scores": [[0.21092619001865387, 0.99162358045578]], "normalized_score": [0.45733946561813354], "tokens": [["\u2581David", "\u2581Villa"]], "entropy": [[2.510974645614624, 0.08965320140123367]], "avg_entropy": [1.3003139235079288], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "David Villa", "score_of_biggest_cluster": 0.45733946561813354, "score_of_first_answer": 0.45733946561813354, "max_score_of_first_answer": 0.99162358045578, "min_score_of_first_answer": 0.21092619001865387, "avg_score": 0.45733946561813354, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3903486952185631, "std_of_first_answer": 0.3903486952185631, "avg_entropy_of_first_answer": 1.3003139235079288, "lex_sim": 1.0}
{"question": "when did the dallas cowboys win their last playoff game", "answer": ["2007"], "scores": [[0.5806013941764832, 0.5660753846168518, 0.9956004619598389, 0.6763411164283752, 0.0731051117181778]], "normalized_score": [0.4383189380168915], "tokens": [["\u2581", "2", "0", "0", "7"]], "entropy": [[1.58543062210083, 0.8249022960662842, 0.04219523072242737, 0.6525335907936096, 1.1956692934036255]], "avg_entropy": [0.8601462066173553], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2014", "score_of_biggest_cluster": 0.4383189380168915, "score_of_first_answer": 0.4383189380168915, "max_score_of_first_answer": 0.9956004619598389, "min_score_of_first_answer": 0.0731051117181778, "avg_score": 0.4383189380168915, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2963539569019626, "std_of_first_answer": 0.2963539569019626, "avg_entropy_of_first_answer": 0.8601462066173553, "lex_sim": 1.0}
{"question": "who had the most governmental power under the articles of confederation", "answer": ["the Continental Congress"], "scores": [[0.057202212512493134, 0.06972210109233856, 0.9967921376228333, 0.9810318350791931]], "normalized_score": [0.24990086257457733], "tokens": [["\u2581the", "\u2581Cont", "inental", "\u2581Congress"]], "entropy": [[3.648373603820801, 3.349067211151123, 0.03342512622475624, 0.12727124989032745]], "avg_entropy": [1.7895342977717519], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the states", "score_of_biggest_cluster": 0.24990086257457733, "score_of_first_answer": 0.24990086257457733, "max_score_of_first_answer": 0.9967921376228333, "min_score_of_first_answer": 0.057202212512493134, "avg_score": 0.24990086257457733, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4627796328666289, "std_of_first_answer": 0.4627796328666289, "avg_entropy_of_first_answer": 1.7895342977717519, "lex_sim": 1.0}
{"question": "who has trained the most melbourne cup winners", "answer": ["Bart Cummings"], "scores": [[0.7215421795845032, 0.9656937718391418, 0.9988899827003479, 0.9992806315422058]], "normalized_score": [0.9132224321365356], "tokens": [["\u2581Bart", "\u2581C", "umm", "ings"]], "entropy": [[2.0301876068115234, 0.25463610887527466, 0.009176217950880527, 0.005964758340269327]], "avg_entropy": [0.574991172994487], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bart Cummings", "score_of_biggest_cluster": 0.9132224321365356, "score_of_first_answer": 0.9132224321365356, "max_score_of_first_answer": 0.9992806315422058, "min_score_of_first_answer": 0.7215421795845032, "avg_score": 0.9132224321365356, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.11616278173222437, "std_of_first_answer": 0.11616278173222437, "avg_entropy_of_first_answer": 0.574991172994487, "lex_sim": 1.0}
{"question": "what nfl player has the most nfl rings", "answer": ["John Elway"], "scores": [[0.057288117706775665, 0.7492652535438538, 0.9994432330131531]], "normalized_score": [0.35006827116012573], "tokens": [["\u2581John", "\u2581El", "way"]], "entropy": [[3.765246868133545, 1.5657877922058105, 0.006403463426977396]], "avg_entropy": [1.7791460412554443], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charles Haley", "score_of_biggest_cluster": 0.35006827116012573, "score_of_first_answer": 0.35006827116012573, "max_score_of_first_answer": 0.9994432330131531, "min_score_of_first_answer": 0.057288117706775665, "avg_score": 0.35006827116012573, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3984801178966845, "std_of_first_answer": 0.3984801178966845, "avg_entropy_of_first_answer": 1.7791460412554443, "lex_sim": 1.0}
{"question": "who sings i feel love with the blue man group", "answer": ["Blue Man Group (Narrator)"], "scores": [[0.0061026327311992645, 0.8402304649353027, 0.8413577079772949, 0.01473560743033886, 0.02260761894285679, 0.15976229310035706, 0.9089915752410889, 0.8233427405357361]], "normalized_score": [0.14268942177295685], "tokens": [["\u2581Blue", "\u2581Man", "\u2581Group", "\u2581(", "N", "arr", "ator", ")"]], "entropy": [[4.596763610839844, 1.0010548830032349, 0.7691972255706787, 0.9019036293029785, 5.314232349395752, 3.4826927185058594, 0.4517199695110321, 1.0215293169021606]], "avg_entropy": [2.1923867128789425], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Annette Strean", "score_of_biggest_cluster": 0.14268942177295685, "score_of_first_answer": 0.14268942177295685, "max_score_of_first_answer": 0.9089915752410889, "min_score_of_first_answer": 0.0061026327311992645, "avg_score": 0.14268942177295685, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4044848877047169, "std_of_first_answer": 0.4044848877047169, "avg_entropy_of_first_answer": 2.1923867128789425, "lex_sim": 1.0}
{"question": "who has the best nba record this season", "answer": ["Golden State Warriors"], "scores": [[0.4798719584941864, 0.98512864112854, 0.7392039895057678, 0.9968254566192627]], "normalized_score": [0.7682462334632874], "tokens": [["\u2581Golden", "\u2581State", "\u2581War", "riors"]], "entropy": [[2.4641265869140625, 0.10165736079216003, 0.6799927353858948, 0.027468310669064522]], "avg_entropy": [0.8183112484402955], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Houston Rockets", "score_of_biggest_cluster": 0.7682462334632874, "score_of_first_answer": 0.7682462334632874, "max_score_of_first_answer": 0.9968254566192627, "min_score_of_first_answer": 0.4798719584941864, "avg_score": 0.7682462334632874, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21165462831065324, "std_of_first_answer": 0.21165462831065324, "avg_entropy_of_first_answer": 0.8183112484402955, "lex_sim": 1.0}
{"question": "actress who plays penelope garcia on criminal minds", "answer": ["Kirsten Vangsness"], "scores": [[0.8098067045211792, 0.9968298077583313, 0.9928656816482544, 0.932127833366394, 0.9900001883506775, 0.9981281161308289]], "normalized_score": [0.9506740570068359], "tokens": [["\u2581K", "irst", "en", "\u2581V", "angs", "ness"]], "entropy": [[1.3155369758605957, 0.03316009044647217, 0.057141512632369995, 0.46845418214797974, 0.06190691888332367, 0.013981282711029053]], "avg_entropy": [0.3250301604469617], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kirsten Simone Vangsness", "score_of_biggest_cluster": 0.9506740570068359, "score_of_first_answer": 0.9506740570068359, "max_score_of_first_answer": 0.9981281161308289, "min_score_of_first_answer": 0.8098067045211792, "avg_score": 0.9506740570068359, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06813630621352863, "std_of_first_answer": 0.06813630621352863, "avg_entropy_of_first_answer": 0.3250301604469617, "lex_sim": 1.0}
{"question": "where does the electron transport chain get its electrons from", "answer": ["NADH"], "scores": [[0.20467756688594818, 0.9837768077850342, 0.7082837820053101]], "normalized_score": [0.5224660038948059], "tokens": [["\u2581N", "AD", "H"]], "entropy": [[4.378934860229492, 0.11354997009038925, 1.001063585281372]], "avg_entropy": [1.8311828052004178], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "electron donors", "score_of_biggest_cluster": 0.5224660038948059, "score_of_first_answer": 0.5224660038948059, "max_score_of_first_answer": 0.9837768077850342, "min_score_of_first_answer": 0.20467756688594818, "avg_score": 0.5224660038948059, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32257837192940825, "std_of_first_answer": 0.32257837192940825, "avg_entropy_of_first_answer": 1.8311828052004178, "lex_sim": 1.0}
{"question": "who is considered the father of modern behaviorism", "answer": ["John B. Watson"], "scores": [[0.3817819356918335, 0.7961441874504089, 0.7052797675132751, 0.9860674738883972]], "normalized_score": [0.6780610084533691], "tokens": [["\u2581John", "\u2581B", ".", "\u2581Watson"]], "entropy": [[1.5836200714111328, 0.9457864761352539, 0.9485399723052979, 0.10272365063428879]], "avg_entropy": [0.8951675426214933], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John B. Watson", "score_of_biggest_cluster": 0.6780610084533691, "score_of_first_answer": 0.6780610084533691, "max_score_of_first_answer": 0.9860674738883972, "min_score_of_first_answer": 0.3817819356918335, "avg_score": 0.6780610084533691, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.21861451063207715, "std_of_first_answer": 0.21861451063207715, "avg_entropy_of_first_answer": 0.8951675426214933, "lex_sim": 1.0}
{"question": "who makes the decisions about what to produce in a market economy", "answer": ["consumers"], "scores": [[0.33254677057266235, 0.9998747110366821]], "normalized_score": [0.576632559299469], "tokens": [["\u2581consum", "ers"]], "entropy": [[3.3911259174346924, 0.001756453886628151]], "avg_entropy": [1.6964411856606603], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "capital and financial markets", "score_of_biggest_cluster": 0.576632559299469, "score_of_first_answer": 0.576632559299469, "max_score_of_first_answer": 0.9998747110366821, "min_score_of_first_answer": 0.33254677057266235, "avg_score": 0.576632559299469, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3336639702320099, "std_of_first_answer": 0.3336639702320099, "avg_entropy_of_first_answer": 1.6964411856606603, "lex_sim": 1.0}
{"question": "what is the dogs name in wizard of oz", "answer": ["Toto"], "scores": [[0.8944342136383057, 0.9982993006706238]], "normalized_score": [0.9449407458305359], "tokens": [["\u2581T", "oto"]], "entropy": [[0.9252758622169495, 0.0172216035425663]], "avg_entropy": [0.4712487328797579], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Toto", "score_of_biggest_cluster": 0.9449407458305359, "score_of_first_answer": 0.9449407458305359, "max_score_of_first_answer": 0.9982993006706238, "min_score_of_first_answer": 0.8944342136383057, "avg_score": 0.9449407458305359, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.05193254351615906, "std_of_first_answer": 0.05193254351615906, "avg_entropy_of_first_answer": 0.4712487328797579, "lex_sim": 1.0}
{"question": "who sang theme song for license to kill", "answer": ["Gladys Knight"], "scores": [[0.45338788628578186, 0.9982128143310547, 0.9846243858337402]], "normalized_score": [0.7638144493103027], "tokens": [["\u2581Glad", "ys", "\u2581Knight"]], "entropy": [[2.811587333679199, 0.019470207393169403, 0.13941147923469543]], "avg_entropy": [0.9901563401023546], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gladys Knight", "score_of_biggest_cluster": 0.7638144493103027, "score_of_first_answer": 0.7638144493103027, "max_score_of_first_answer": 0.9982128143310547, "min_score_of_first_answer": 0.45338788628578186, "avg_score": 0.7638144493103027, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.25369077109752936, "std_of_first_answer": 0.25369077109752936, "avg_entropy_of_first_answer": 0.9901563401023546, "lex_sim": 1.0}
{"question": "who plays max voice in a goofy movie", "answer": ["Patrick Warburton"], "scores": [[0.05158140882849693, 0.8625505566596985, 0.9991908669471741, 0.999840259552002]], "normalized_score": [0.459160178899765], "tokens": [["\u2581Patrick", "\u2581War", "bur", "ton"]], "entropy": [[4.260580062866211, 0.975563645362854, 0.008060652762651443, 0.002144239842891693]], "avg_entropy": [1.311587150208652], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jason Marsden", "score_of_biggest_cluster": 0.459160178899765, "score_of_first_answer": 0.459160178899765, "max_score_of_first_answer": 0.999840259552002, "min_score_of_first_answer": 0.05158140882849693, "avg_score": 0.459160178899765, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.394679375447018, "std_of_first_answer": 0.394679375447018, "avg_entropy_of_first_answer": 1.311587150208652, "lex_sim": 1.0}
{"question": "who won the fountain of youth stakes at gulfstream park", "answer": ["Tizway"], "scores": [[0.01654594950377941, 0.16497540473937988, 0.07310188561677933]], "normalized_score": [0.05843590945005417], "tokens": [["\u2581T", "iz", "way"]], "entropy": [[6.67164945602417, 3.569007158279419, 2.4506402015686035]], "avg_entropy": [4.2304322719573975], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gunnevera", "score_of_biggest_cluster": 0.05843590945005417, "score_of_first_answer": 0.05843590945005417, "max_score_of_first_answer": 0.16497540473937988, "min_score_of_first_answer": 0.01654594950377941, "avg_score": 0.05843590945005417, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06116518672789622, "std_of_first_answer": 0.06116518672789622, "avg_entropy_of_first_answer": 4.2304322719573975, "lex_sim": 1.0}
{"question": "who's winning the election in virginia for governor", "answer": ["Ed Gillespie"], "scores": [[0.24387642741203308, 0.9414516091346741, 0.9961005449295044, 0.9957438111305237]], "normalized_score": [0.6908037066459656], "tokens": [["\u2581Ed", "\u2581G", "illes", "pie"]], "entropy": [[2.837951183319092, 0.4506400525569916, 0.0292058102786541, 0.03570019453763962]], "avg_entropy": [0.8383743101730943], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ralph Northam", "score_of_biggest_cluster": 0.6908037066459656, "score_of_first_answer": 0.6908037066459656, "max_score_of_first_answer": 0.9961005449295044, "min_score_of_first_answer": 0.24387642741203308, "avg_score": 0.6908037066459656, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31856034519283694, "std_of_first_answer": 0.31856034519283694, "avg_entropy_of_first_answer": 0.8383743101730943, "lex_sim": 1.0}
{"question": "who wrote hit me baby one more time", "answer": ["Tiffany"], "scores": [[0.02067159116268158, 0.2400580644607544, 0.9749194979667664]], "normalized_score": [0.1691296398639679], "tokens": [["\u2581T", "iff", "any"]], "entropy": [[3.527808904647827, 2.328300952911377, 0.18618761003017426]], "avg_entropy": [2.0140991558631263], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Max Martin", "score_of_biggest_cluster": 0.1691296398639679, "score_of_first_answer": 0.1691296398639679, "max_score_of_first_answer": 0.9749194979667664, "min_score_of_first_answer": 0.02067159116268158, "avg_score": 0.1691296398639679, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40807690162526067, "std_of_first_answer": 0.40807690162526067, "avg_entropy_of_first_answer": 2.0140991558631263, "lex_sim": 1.0}
{"question": "who was the declaration of independence written for", "answer": ["George Washington"], "scores": [[0.045888502150774, 0.7804315090179443]], "normalized_score": [0.18924279510974884], "tokens": [["\u2581George", "\u2581Washington"]], "entropy": [[3.884199619293213, 1.1411933898925781]], "avg_entropy": [2.5126965045928955], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Second Continental Congress", "score_of_biggest_cluster": 0.18924279510974884, "score_of_first_answer": 0.18924279510974884, "max_score_of_first_answer": 0.7804315090179443, "min_score_of_first_answer": 0.045888502150774, "avg_score": 0.18924279510974884, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36727150343358517, "std_of_first_answer": 0.36727150343358517, "avg_entropy_of_first_answer": 2.5126965045928955, "lex_sim": 1.0}
{"question": "where does the papillary layer of the skin lie", "answer": ["On the Dermis"], "scores": [[0.008143030107021332, 0.722281813621521, 0.019269082695245743, 0.9827744364738464, 0.9417024254798889]], "normalized_score": [0.16000893712043762], "tokens": [["\u2581On", "\u2581the", "\u2581D", "erm", "is"]], "entropy": [[4.8068928718566895, 1.0861457586288452, 3.236760377883911, 0.11739607155323029, 0.2616518437862396]], "avg_entropy": [1.9017693847417831], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "adjacent to the epidermis", "score_of_biggest_cluster": 0.16000893712043762, "score_of_first_answer": 0.16000893712043762, "max_score_of_first_answer": 0.9827744364738464, "min_score_of_first_answer": 0.008143030107021332, "avg_score": 0.16000893712043762, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.43463543224971724, "std_of_first_answer": 0.43463543224971724, "avg_entropy_of_first_answer": 1.9017693847417831, "lex_sim": 1.0}
{"question": "which body part(s) occupy the greatest portion of the primary motor cortex", "answer": ["primary motor cortices"], "scores": [[0.016322322189807892, 0.8230118155479431, 0.0036255435552448034, 0.8286405205726624]], "normalized_score": [0.07970431447029114], "tokens": [["\u2581primary", "\u2581motor", "\u2581cort", "ices"]], "entropy": [[4.435978889465332, 1.0304315090179443, 0.41734907031059265, 0.6791644096374512]], "avg_entropy": [1.64073096960783], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "face", "score_of_biggest_cluster": 0.07970431447029114, "score_of_first_answer": 0.07970431447029114, "max_score_of_first_answer": 0.8286405205726624, "min_score_of_first_answer": 0.0036255435552448034, "avg_score": 0.07970431447029114, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40795567005339745, "std_of_first_answer": 0.40795567005339745, "avg_entropy_of_first_answer": 1.64073096960783, "lex_sim": 1.0}
{"question": "what is the corporate tax rate in great britain", "answer": ["19%"], "scores": [[0.8943929672241211, 0.19941827654838562, 0.8156302571296692, 0.6212867498397827]], "normalized_score": [0.5483018159866333], "tokens": [["\u2581", "1", "9", "%"]], "entropy": [[0.8980036377906799, 0.7176504731178284, 0.6928601264953613, 1.10574209690094]], "avg_entropy": [0.8535640835762024], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "20%", "score_of_biggest_cluster": 0.5483018159866333, "score_of_first_answer": 0.5483018159866333, "max_score_of_first_answer": 0.8943929672241211, "min_score_of_first_answer": 0.19941827654838562, "avg_score": 0.5483018159866333, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26917002105431603, "std_of_first_answer": 0.26917002105431603, "avg_entropy_of_first_answer": 0.8535640835762024, "lex_sim": 1.0}
{"question": "india's first arctic research station set up in 2008 what is the name", "answer": ["Hut Point"], "scores": [[0.06779590249061584, 0.3528439700603485, 0.6138799786567688]], "normalized_score": [0.24488182365894318], "tokens": [["\u2581H", "ut", "\u2581Point"]], "entropy": [[4.385461807250977, 3.2659854888916016, 2.2296981811523438]], "avg_entropy": [3.293715159098307], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Himadri Station", "score_of_biggest_cluster": 0.24488182365894318, "score_of_first_answer": 0.24488182365894318, "max_score_of_first_answer": 0.6138799786567688, "min_score_of_first_answer": 0.06779590249061584, "avg_score": 0.24488182365894318, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22300971998035077, "std_of_first_answer": 0.22300971998035077, "avg_entropy_of_first_answer": 3.293715159098307, "lex_sim": 1.0}
{"question": "who won the champions league final in 2016", "answer": ["Real Madrid"], "scores": [[0.8812702298164368, 0.9868436455726624]], "normalized_score": [0.9325641989707947], "tokens": [["\u2581Real", "\u2581Madrid"]], "entropy": [[0.8056514859199524, 0.09653646498918533]], "avg_entropy": [0.45109397545456886], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Real Madrid", "score_of_biggest_cluster": 0.9325641989707947, "score_of_first_answer": 0.9325641989707947, "max_score_of_first_answer": 0.9868436455726624, "min_score_of_first_answer": 0.8812702298164368, "avg_score": 0.9325641989707947, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.05278670787811279, "std_of_first_answer": 0.05278670787811279, "avg_entropy_of_first_answer": 0.45109397545456886, "lex_sim": 1.0}
{"question": "when does shadowhunters season 2 episode 17 come out", "answer": ["October 21, 2019"], "scores": [[0.011833473108708858, 0.9590190649032593, 0.4304802715778351, 0.10839580744504929, 0.2773769795894623, 0.9881080985069275, 0.998456597328186, 0.9995725750923157, 0.9829188585281372, 0.02466602250933647]], "normalized_score": [0.2848092019557953], "tokens": [["\u2581October", "\u2581", "2", "1", ",", "\u2581", "2", "0", "1", "9"]], "entropy": [[3.247279167175293, 0.22396937012672424, 1.5761229991912842, 2.3988990783691406, 1.1105225086212158, 0.07558979094028473, 0.01324414275586605, 0.005268897861242294, 0.09826844930648804, 0.9804459810256958]], "avg_entropy": [0.9729610385373235], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "February\u00a013,\u00a02017", "score_of_biggest_cluster": 0.2848092019557953, "score_of_first_answer": 0.2848092019557953, "max_score_of_first_answer": 0.9995725750923157, "min_score_of_first_answer": 0.011833473108708858, "avg_score": 0.2848092019557953, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42322783169587486, "std_of_first_answer": 0.42322783169587486, "avg_entropy_of_first_answer": 0.9729610385373235, "lex_sim": 1.0}
{"question": "who won best boy band i heart radio music awards", "answer": ["One Directions (Niall Horan, Zayn Malik, Liam Payne, Louis Tomlinson, Harry Styles)"], "scores": [[0.2531582713127136, 0.0059082405641674995, 0.9468467831611633, 0.0014736357843503356, 0.05270347744226456, 0.5386497378349304, 0.9985291957855225, 0.4280116856098175, 0.9942712783813477, 0.6536673307418823, 0.4370643198490143, 0.9278888702392578, 0.9638686776161194, 0.9766223430633545, 0.9964117407798767, 0.9871713519096375, 0.7444421052932739, 0.9996054768562317, 0.9976039528846741, 0.9972537159919739, 0.9772929549217224, 0.15552257001399994, 0.9908866882324219, 0.9920567870140076, 0.9991447925567627, 0.6255683302879333, 0.757859468460083, 0.9918060302734375, 0.9906609654426575, 0.9436478018760681]], "normalized_score": [0.48148253560066223], "tokens": [["\u2581One", "\u2581Dire", "ctions", "\u2581(", "N", "i", "all", "\u2581Hor", "an", ",", "\u2581Z", "ay", "n", "\u2581Mal", "ik", ",", "\u2581Li", "am", "\u2581Pay", "ne", ",", "\u2581Louis", "\u2581Tom", "lin", "son", ",", "\u2581Harry", "\u2581St", "yles", ")"]], "entropy": [[3.4418954849243164, 0.3449293076992035, 0.32710129022598267, 0.46567416191101074, 4.71534538269043, 1.6357874870300293, 0.016580849885940552, 1.3229793310165405, 0.055862993001937866, 1.263193964958191, 1.4447166919708252, 0.3378376364707947, 0.16579769551753998, 0.15944945812225342, 0.0344385951757431, 0.10616249591112137, 0.8119065761566162, 0.005220985040068626, 0.02621239423751831, 0.02745109796524048, 0.1597408652305603, 0.5692375898361206, 0.09408850967884064, 0.06064896285533905, 0.008987624198198318, 1.0761810541152954, 0.7548331618309021, 0.07983465492725372, 0.0666700154542923, 0.3629135489463806]], "avg_entropy": [0.6647226622328162], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "BTS", "score_of_biggest_cluster": 0.48148253560066223, "score_of_first_answer": 0.48148253560066223, "max_score_of_first_answer": 0.9996054768562317, "min_score_of_first_answer": 0.0014736357843503356, "avg_score": 0.48148253560066223, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3388922272933973, "std_of_first_answer": 0.3388922272933973, "avg_entropy_of_first_answer": 0.6647226622328162, "lex_sim": 1.0}
{"question": "when did italy enter the second world war", "answer": ["June 10, 1940"], "scores": [[0.2298664003610611, 0.9750586748123169, 0.9866445660591125, 0.9019812941551208, 0.6378390192985535, 0.9877090454101562, 0.9999189376831055, 0.9995154142379761, 0.999876856803894, 0.9995402097702026]], "normalized_score": [0.8125882744789124], "tokens": [["\u2581June", "\u2581", "1", "0", ",", "\u2581", "1", "9", "4", "0"]], "entropy": [[2.18121337890625, 0.1503378450870514, 0.08701037615537643, 0.402897447347641, 0.833791971206665, 0.07005922496318817, 0.0009766254806891084, 0.0054565793834626675, 0.0014121667481958866, 0.004852956160902977]], "avg_entropy": [0.3738008571439423], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "10 June 1940", "score_of_biggest_cluster": 0.8125882744789124, "score_of_first_answer": 0.8125882744789124, "max_score_of_first_answer": 0.9999189376831055, "min_score_of_first_answer": 0.2298664003610611, "avg_score": 0.8125882744789124, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23884322553933898, "std_of_first_answer": 0.23884322553933898, "avg_entropy_of_first_answer": 0.3738008571439423, "lex_sim": 1.0}
{"question": "who is command sergeant major of the army", "answer": ["Raymond F. Chandler III"], "scores": [[0.08194288611412048, 0.7028217315673828, 0.7839465141296387, 0.7855164408683777, 0.9998524188995361, 0.9998843669891357, 0.5943830609321594]], "normalized_score": [0.5761502385139465], "tokens": [["\u2581Raymond", "\u2581F", ".", "\u2581Ch", "and", "ler", "\u2581III"]], "entropy": [[4.698084354400635, 1.7330236434936523, 1.0685127973556519, 1.1318953037261963, 0.0016207408625632524, 0.0014670303789898753, 0.9387508630752563]], "avg_entropy": [1.367622104756135], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Daniel A. Dailey", "score_of_biggest_cluster": 0.5761502385139465, "score_of_first_answer": 0.5761502385139465, "max_score_of_first_answer": 0.9998843669891357, "min_score_of_first_answer": 0.08194288611412048, "avg_score": 0.5761502385139465, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2895873312341933, "std_of_first_answer": 0.2895873312341933, "avg_entropy_of_first_answer": 1.367622104756135, "lex_sim": 1.0}
{"question": "how many nuclear power plants in washington state", "answer": ["3"], "scores": [[0.35412415862083435, 0.2987699508666992]], "normalized_score": [0.3252716660499573], "tokens": [["\u2581", "3"]], "entropy": [[2.9411118030548096, 1.6159708499908447]], "avg_entropy": [2.278541326522827], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "3", "score_of_biggest_cluster": 0.3252716660499573, "score_of_first_answer": 0.3252716660499573, "max_score_of_first_answer": 0.35412415862083435, "min_score_of_first_answer": 0.2987699508666992, "avg_score": 0.3252716660499573, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.027677103877067566, "std_of_first_answer": 0.027677103877067566, "avg_entropy_of_first_answer": 2.278541326522827, "lex_sim": 1.0}
{"question": "who becomes chief resident on grey's anatomy season 7", "answer": ["Callie Torres I have a new challenge for you in the comments below: name a movie that includes the following phrases in the title, in consecutive order: \u201cIce.\u201d \u201cMen.\u201d \u201cChamber.\u201d If no one"], "scores": [[0.14931297302246094, 0.9992006421089172, 0.7857567071914673, 0.9945530295372009, 6.262120223254897e-06, 0.007123716175556183, 0.12036795169115067, 0.029596049338579178, 0.002251595724374056, 0.8409203886985779, 0.9737122654914856, 0.004100872669368982, 0.3253922164440155, 0.0004012311401311308, 0.2707335650920868, 0.03771059215068817, 0.002725753001868725, 0.15762659907341003, 0.09270884096622467, 0.33161863684654236, 0.016834920272231102, 0.3279796838760376, 0.23031079769134521, 0.010485359467566013, 0.9980620741844177, 0.09820056706666946, 0.367578387260437, 0.6485694050788879, 0.031773459166288376, 0.0988931730389595, 0.00034282138221897185, 0.9806391596794128, 0.5438895225524902, 0.11314809322357178, 0.042180366814136505, 0.013096977025270462, 0.002547272015362978, 0.8003437519073486, 0.024327365681529045, 0.9153986573219299, 0.8961073756217957, 0.00509036285802722, 0.09199225902557373, 0.9792255163192749, 0.8210604786872864, 0.014705928042531013, 0.01144473347812891, 0.877723217010498]], "normalized_score": [0.07077925652265549], "tokens": [["\u2581Call", "ie", "\u2581Tor", "res", "\u2581I", "\u2581have", "\u2581a", "\u2581new", "\u2581challenge", "\u2581for", "\u2581you", "\u2581in", "\u2581the", "\u2581comments", "\u2581below", ":", "\u2581name", "\u2581a", "\u2581movie", "\u2581that", "\u2581includes", "\u2581the", "\u2581following", "\u2581phr", "ases", "\u2581in", "\u2581the", "\u2581title", ",", "\u2581in", "\u2581consecutive", "\u2581order", ":", "\u2581\u201c", "I", "ce", ".\u201d", "\u2581\u201c", "Men", ".\u201d", "\u2581\u201c", "Ch", "am", "ber", ".\u201d", "\u2581If", "\u2581no", "\u2581one"]], "entropy": [[3.3285341262817383, 0.01003462914377451, 0.7881895899772644, 0.04769223928451538, 0.24477308988571167, 4.031601428985596, 4.596406936645508, 5.837173938751221, 5.593514442443848, 1.0886540412902832, 0.23060962557792664, 2.72833251953125, 3.8964853286743164, 5.799904823303223, 2.5242719650268555, 2.4763247966766357, 4.20095157623291, 2.950967788696289, 5.714104652404785, 3.0865345001220703, 3.5893561840057373, 2.8209755420684814, 2.994755506515503, 3.41715145111084, 0.018171165138483047, 2.4030814170837402, 2.080007553100586, 1.8979482650756836, 1.876755714416504, 4.648758411407471, 1.8629316091537476, 0.17869022488594055, 1.6929543018341064, 4.77481746673584, 5.4675397872924805, 4.287941932678223, 3.003354549407959, 0.8690899610519409, 4.0899152755737305, 0.5393602252006531, 0.602005124092102, 4.684114933013916, 3.6135525703430176, 0.17874747514724731, 0.8790985345840454, 2.8997690677642822, 0.8226596713066101, 0.8789588212966919]], "avg_entropy": [2.63015676625461], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "April Kepner M.D.", "score_of_biggest_cluster": 0.07077925652265549, "score_of_first_answer": 0.07077925652265549, "max_score_of_first_answer": 0.9992006421089172, "min_score_of_first_answer": 6.262120223254897e-06, "avg_score": 0.07077925652265549, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37996353941456423, "std_of_first_answer": 0.37996353941456423, "avg_entropy_of_first_answer": 2.63015676625461, "lex_sim": 1.0}
{"question": "how many grams of alcohol in one beer", "answer": ["one"], "scores": [[0.012038900516927242]], "normalized_score": [0.012038903310894966], "tokens": [["\u2581one"]], "entropy": [[2.2622952461242676]], "avg_entropy": [2.2622952461242676], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "14", "score_of_biggest_cluster": 0.012038903310894966, "score_of_first_answer": 0.012038903310894966, "max_score_of_first_answer": 0.012038900516927242, "min_score_of_first_answer": 0.012038900516927242, "avg_score": 0.012038903310894966, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.2622952461242676, "lex_sim": 1.0}
{"question": "to whom do the powers not given to the federal government by the constitution go", "answer": ["to the states"], "scores": [[0.11863190680742264, 0.9620005488395691, 0.7968277931213379]], "normalized_score": [0.449690580368042], "tokens": [["\u2581to", "\u2581the", "\u2581states"]], "entropy": [[3.0089645385742188, 0.25671157240867615, 0.9143121838569641]], "avg_entropy": [1.3933294316132863], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the states or the people", "score_of_biggest_cluster": 0.449690580368042, "score_of_first_answer": 0.449690580368042, "max_score_of_first_answer": 0.9620005488395691, "min_score_of_first_answer": 0.11863190680742264, "avg_score": 0.449690580368042, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3649204427463636, "std_of_first_answer": 0.3649204427463636, "avg_entropy_of_first_answer": 1.3933294316132863, "lex_sim": 1.0}
{"question": "who does the voice of the elephant in sing", "answer": ["Matthew McConaughey"], "scores": [[0.7083984613418579, 0.8045345544815063, 0.9396359920501709, 0.9826196432113647, 0.9950418472290039, 0.9994266033172607]], "normalized_score": [0.89769047498703], "tokens": [["\u2581Matthew", "\u2581Mc", "Con", "aug", "he", "y"]], "entropy": [[2.1014418601989746, 1.4273500442504883, 0.38880014419555664, 0.12464721500873566, 0.03330257907509804, 0.00534760532900691]], "avg_entropy": [0.6801482413429767], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tori Kelly", "score_of_biggest_cluster": 0.89769047498703, "score_of_first_answer": 0.89769047498703, "max_score_of_first_answer": 0.9994266033172607, "min_score_of_first_answer": 0.7083984613418579, "avg_score": 0.89769047498703, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11029795236346221, "std_of_first_answer": 0.11029795236346221, "avg_entropy_of_first_answer": 0.6801482413429767, "lex_sim": 1.0}
{"question": "when did they stop making jello pudding pops", "answer": ["August 8, 2010"], "scores": [[0.021339911967515945, 0.9405806660652161, 0.019761530682444572, 0.5055627226829529, 0.9914931654930115, 0.8471031785011292, 0.9997779726982117, 0.7137975692749023, 0.0866708979010582]], "normalized_score": [0.2795232832431793], "tokens": [["\u2581August", "\u2581", "8", ",", "\u2581", "2", "0", "1", "0"]], "entropy": [[2.56111478805542, 0.30576789379119873, 1.401565432548523, 0.8452954292297363, 0.05565562844276428, 0.42920351028442383, 0.0026703656185418367, 0.61470627784729, 2.247467517852783]], "avg_entropy": [0.9403829826300757], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the 90s", "score_of_biggest_cluster": 0.2795232832431793, "score_of_first_answer": 0.2795232832431793, "max_score_of_first_answer": 0.9997779726982117, "min_score_of_first_answer": 0.019761530682444572, "avg_score": 0.2795232832431793, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39982041595225487, "std_of_first_answer": 0.39982041595225487, "avg_entropy_of_first_answer": 0.9403829826300757, "lex_sim": 1.0}
{"question": "where does a girl wear an engagement ring", "answer": ["Rings are worn on the ring finger of the left hand."], "scores": [[0.002965723630040884, 0.5376474261283875, 0.38483837246894836, 0.39730674028396606, 0.9033839106559753, 0.9098880290985107, 0.06477414071559906, 0.9632099270820618, 0.6132135391235352, 0.9208563566207886, 0.9621138572692871, 0.9806082248687744, 0.35944414138793945, 0.017215918749570847, 0.6657362580299377, 0.13565267622470856, 0.09482069313526154, 0.8036403059959412, 0.0670061931014061, 0.7678682804107666, 0.059609245508909225, 0.9932418465614319, 0.0338844358921051, 0.49195602536201477, 0.7992016673088074, 0.6639727354049683, 0.0005640601739287376, 0.0010049614356830716, 0.004777166992425919, 0.0003983943024650216, 0.7875730395317078, 0.16582612693309784, 0.004581686109304428, 0.004889185074716806, 0.009041744284331799, 0.05394590273499489, 0.6286410093307495]], "normalized_score": [0.11233513802289963], "tokens": [["\u2581R", "ings", "\u2581are", "\u2581worn", "\u2581on", "\u2581the", "\u2581ring", "\u2581finger", "\u2581of", "\u2581the", "\u2581left", "\u2581hand", ".", "\u2581It", "\u2581is", "\u2581a", "\u2581symbol", "\u2581of", "\u2581et", "ernal", "\u2581commit", "ment", "\u2581between", "\u2581two", "\u2581people", ".", "\u2581", "\u2581(", "G", "ENT", "LE", "MAN", "):", "\u2581Are", "\u2581they", "\u2581married", "?"]], "entropy": [[4.234718322753906, 1.6820259094238281, 2.964193105697632, 2.695544719696045, 0.6685321927070618, 0.6217443943023682, 1.8453154563903809, 0.2637903392314911, 1.626143217086792, 0.47326093912124634, 0.26925766468048096, 0.14860951900482178, 1.9716544151306152, 1.983187198638916, 1.8251607418060303, 3.86358642578125, 3.9878416061401367, 0.8832525014877319, 3.467238664627075, 0.5486422777175903, 1.1138231754302979, 0.04380837082862854, 1.9911816120147705, 1.7093416452407837, 1.0532881021499634, 1.5195538997650146, 2.0109963417053223, 3.6696059703826904, 5.826667785644531, 4.161848068237305, 1.2475311756134033, 3.753676414489746, 3.3415422439575195, 5.183487892150879, 0.6289700269699097, 5.314614295959473, 1.6557422876358032]], "avg_entropy": [2.168902132962201], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "left hand ring finger", "score_of_biggest_cluster": 0.11233513802289963, "score_of_first_answer": 0.11233513802289963, "max_score_of_first_answer": 0.9932418465614319, "min_score_of_first_answer": 0.0003983943024650216, "avg_score": 0.11233513802289963, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.37457409102742206, "std_of_first_answer": 0.37457409102742206, "avg_entropy_of_first_answer": 2.168902132962201, "lex_sim": 1.0}
{"question": "ahana from dil sambhal ja zara real name", "answer": ["Anusha Dandekar"], "scores": [[0.019492963328957558, 0.12643103301525116, 0.18879972398281097, 0.517508864402771, 0.9852958917617798, 0.9962247610092163]], "normalized_score": [0.24865418672561646], "tokens": [["\u2581An", "ush", "a", "\u2581Dan", "de", "kar"]], "entropy": [[4.905889511108398, 3.192368507385254, 0.6925061345100403, 2.4195361137390137, 0.1361057460308075, 0.033633992075920105]], "avg_entropy": [1.8966733341415722], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Smriti Kalra", "score_of_biggest_cluster": 0.24865418672561646, "score_of_first_answer": 0.24865418672561646, "max_score_of_first_answer": 0.9962247610092163, "min_score_of_first_answer": 0.019492963328957558, "avg_score": 0.24865418672561646, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39680762394030256, "std_of_first_answer": 0.39680762394030256, "avg_entropy_of_first_answer": 1.8966733341415722, "lex_sim": 1.0}
{"question": "where did the potter's wheel first develop", "answer": ["Mesopotamia"], "scores": [[0.23347418010234833, 0.9973728656768799, 0.997033953666687, 0.9954426288604736, 0.980902910232544]], "normalized_score": [0.7431726455688477], "tokens": [["\u2581Mes", "op", "ot", "am", "ia"]], "entropy": [[3.7750093936920166, 0.022688064724206924, 0.02747572585940361, 0.03930441290140152, 0.09995374828577042]], "avg_entropy": [0.7928862690925598], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in the Near East", "score_of_biggest_cluster": 0.7431726455688477, "score_of_first_answer": 0.7431726455688477, "max_score_of_first_answer": 0.9973728656768799, "min_score_of_first_answer": 0.23347418010234833, "avg_score": 0.7431726455688477, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3037472369953393, "std_of_first_answer": 0.3037472369953393, "avg_entropy_of_first_answer": 0.7928862690925598, "lex_sim": 1.0}
{"question": "mark who went to golf majors in 1998", "answer": ["Mark Brooks"], "scores": [[0.21651451289653778, 0.10531862825155258, 0.9379702210426331]], "normalized_score": [0.2775835692882538], "tokens": [["\u2581Mark", "\u2581Bro", "oks"]], "entropy": [[3.4455325603485107, 2.1157169342041016, 0.3482268452644348]], "avg_entropy": [1.9698254466056824], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mark O'Meara", "score_of_biggest_cluster": 0.2775835692882538, "score_of_first_answer": 0.2775835692882538, "max_score_of_first_answer": 0.9379702210426331, "min_score_of_first_answer": 0.10531862825155258, "avg_score": 0.2775835692882538, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36910876725131786, "std_of_first_answer": 0.36910876725131786, "avg_entropy_of_first_answer": 1.9698254466056824, "lex_sim": 1.0}
{"question": "who does brant daugherty play in pretty little liars", "answer": ["Ezra Fitz"], "scores": [[0.013861430808901787, 0.997205913066864, 0.8947934508323669]], "normalized_score": [0.23126250505447388], "tokens": [["\u2581Ez", "ra", "\u2581Fitz"]], "entropy": [[3.8741371631622314, 0.024373110383749008, 0.4715750813484192]], "avg_entropy": [1.4566951182981331], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Noel Kahn", "score_of_biggest_cluster": 0.23126250505447388, "score_of_first_answer": 0.23126250505447388, "max_score_of_first_answer": 0.997205913066864, "min_score_of_first_answer": 0.013861430808901787, "avg_score": 0.23126250505447388, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4413987749958754, "std_of_first_answer": 0.4413987749958754, "avg_entropy_of_first_answer": 1.4566951182981331, "lex_sim": 1.0}
{"question": "who ran the fastest 40 yard dash in the nfl", "answer": ["Johnny Bailey"], "scores": [[0.011774998158216476, 0.07562743127346039, 0.9678353071212769]], "normalized_score": [0.09516572952270508], "tokens": [["\u2581Johnny", "\u2581Ba", "iley"]], "entropy": [[4.346147537231445, 3.640184164047241, 0.23216260969638824]], "avg_entropy": [2.739498103658358], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jakeem Grant", "score_of_biggest_cluster": 0.09516572952270508, "score_of_first_answer": 0.09516572952270508, "max_score_of_first_answer": 0.9678353071212769, "min_score_of_first_answer": 0.011774998158216476, "avg_score": 0.09516572952270508, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43642020287717004, "std_of_first_answer": 0.43642020287717004, "avg_entropy_of_first_answer": 2.739498103658358, "lex_sim": 1.0}
{"question": "who sings the wizards of waverly place theme song", "answer": ["Selena Gomez"], "scores": [[0.48053470253944397, 0.9987567663192749, 0.9872767329216003, 0.9988781809806824, 0.9990251064300537]], "normalized_score": [0.86087965965271], "tokens": [["\u2581Sel", "ena", "\u2581G", "ome", "z"]], "entropy": [[2.888728141784668, 0.011715685948729515, 0.11460532248020172, 0.01227595191448927, 0.010069429874420166]], "avg_entropy": [0.6074789064005017], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Selena Gomez", "score_of_biggest_cluster": 0.86087965965271, "score_of_first_answer": 0.86087965965271, "max_score_of_first_answer": 0.9990251064300537, "min_score_of_first_answer": 0.48053470253944397, "avg_score": 0.86087965965271, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.20622884092117316, "std_of_first_answer": 0.20622884092117316, "avg_entropy_of_first_answer": 0.6074789064005017, "lex_sim": 1.0}
{"question": "who won the 7 man elimination chamber match", "answer": ["Kane won the match."], "scores": [[0.02498541958630085, 0.7748114466667175, 0.007254298776388168, 0.26505956053733826, 0.6769034266471863, 0.056334707885980606]], "normalized_score": [0.106011763215065], "tokens": [["\u2581K", "ane", "\u2581won", "\u2581the", "\u2581match", "."]], "entropy": [[4.012809753417969, 0.6427727937698364, 0.6725276708602905, 1.744863510131836, 1.6380336284637451, 1.411975622177124]], "avg_entropy": [1.6871638298034668], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Roman Reigns", "score_of_biggest_cluster": 0.106011763215065, "score_of_first_answer": 0.106011763215065, "max_score_of_first_answer": 0.7748114466667175, "min_score_of_first_answer": 0.007254298776388168, "avg_score": 0.106011763215065, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31342835662927077, "std_of_first_answer": 0.31342835662927077, "avg_entropy_of_first_answer": 1.6871638298034668, "lex_sim": 1.0}
{"question": "when does a cell have condensed visible chromosomes also known as sister chromatids", "answer": ["when it divides"], "scores": [[0.055025286972522736, 0.0797329694032669, 0.5661110877990723, 0.9983415603637695]], "normalized_score": [0.22314918041229248], "tokens": [["\u2581when", "\u2581it", "\u2581div", "ides"]], "entropy": [[4.598762512207031, 3.1533331871032715, 1.8186744451522827, 0.015462549403309822]], "avg_entropy": [2.396558173466474], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "metaphase", "score_of_biggest_cluster": 0.22314918041229248, "score_of_first_answer": 0.22314918041229248, "max_score_of_first_answer": 0.9983415603637695, "min_score_of_first_answer": 0.055025286972522736, "avg_score": 0.22314918041229248, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.38881979726125615, "std_of_first_answer": 0.38881979726125615, "avg_entropy_of_first_answer": 2.396558173466474, "lex_sim": 1.0}
{"question": "how much square feet is a basketball court", "answer": ["approx 4000"], "scores": [[0.003259598044678569, 0.9932118654251099, 0.354610413312912, 0.1283123642206192, 0.34352439641952515, 0.5161244869232178, 0.6686614751815796]], "normalized_score": [0.20907701551914215], "tokens": [["\u2581appro", "x", "\u2581", "4", "0", "0", "0"]], "entropy": [[0.9325642585754395, 0.05745464563369751, 0.9402590990066528, 1.9009943008422852, 2.179781436920166, 1.654905915260315, 1.2708523273468018]], "avg_entropy": [1.2766874262264796], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "94 by 50 feet", "score_of_biggest_cluster": 0.20907701551914215, "score_of_first_answer": 0.20907701551914215, "max_score_of_first_answer": 0.9932118654251099, "min_score_of_first_answer": 0.003259598044678569, "avg_score": 0.20907701551914215, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30888462933665634, "std_of_first_answer": 0.30888462933665634, "avg_entropy_of_first_answer": 1.2766874262264796, "lex_sim": 1.0}
{"question": "where is the nfl game in london played", "answer": ["Wembley Stadium"], "scores": [[0.6783949732780457, 0.9811205267906189, 0.9998008608818054, 0.8412202596664429]], "normalized_score": [0.8649819493293762], "tokens": [["\u2581W", "emble", "y", "\u2581Stadium"]], "entropy": [[1.5391886234283447, 0.11347942054271698, 0.002481105737388134, 0.6299283504486084]], "avg_entropy": [0.5712693750392646], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Twickenham Stadium", "score_of_biggest_cluster": 0.8649819493293762, "score_of_first_answer": 0.8649819493293762, "max_score_of_first_answer": 0.9998008608818054, "min_score_of_first_answer": 0.6783949732780457, "avg_score": 0.8649819493293762, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1290652514061003, "std_of_first_answer": 0.1290652514061003, "avg_entropy_of_first_answer": 0.5712693750392646, "lex_sim": 1.0}
{"question": "where are antibodies made and by what type of lymphocyte", "answer": ["B Lymphocytes"], "scores": [[0.36191225051879883, 0.029020514339208603, 0.9887935519218445, 0.9939815402030945, 0.9848610162734985, 0.6729486584663391]], "normalized_score": [0.43570399284362793], "tokens": [["\u2581B", "\u2581L", "ym", "ph", "ocy", "tes"]], "entropy": [[3.4360878467559814, 1.8565127849578857, 0.06758954375982285, 0.056866515427827835, 0.12293476611375809, 0.6396682858467102]], "avg_entropy": [1.0299432904769976], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "B cells", "score_of_biggest_cluster": 0.43570399284362793, "score_of_first_answer": 0.43570399284362793, "max_score_of_first_answer": 0.9939815402030945, "min_score_of_first_answer": 0.029020514339208603, "avg_score": 0.43570399284362793, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.36776128461960655, "std_of_first_answer": 0.36776128461960655, "avg_entropy_of_first_answer": 1.0299432904769976, "lex_sim": 1.0}
{"question": "when was the last time arsenal win premier league", "answer": ["2004"], "scores": [[0.7708243131637573, 0.946283757686615, 0.993133008480072, 0.9478465914726257, 0.7958792448043823]], "normalized_score": [0.886163592338562], "tokens": [["\u2581", "2", "0", "0", "4"]], "entropy": [[1.4250211715698242, 0.2795688807964325, 0.06345706433057785, 0.20990416407585144, 0.8062146306037903]], "avg_entropy": [0.5568331822752952], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2003\u201304", "score_of_biggest_cluster": 0.886163592338562, "score_of_first_answer": 0.886163592338562, "max_score_of_first_answer": 0.993133008480072, "min_score_of_first_answer": 0.7708243131637573, "avg_score": 0.886163592338562, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08967599519389044, "std_of_first_answer": 0.08967599519389044, "avg_entropy_of_first_answer": 0.5568331822752952, "lex_sim": 1.0}
{"question": "who got the most rings in the nba", "answer": ["LeBron James"], "scores": [[0.08032483607530594, 0.9901421666145325, 0.9992484450340271, 0.9831532835960388]], "normalized_score": [0.5287014842033386], "tokens": [["\u2581Le", "B", "ron", "\u2581James"]], "entropy": [[3.062994956970215, 0.07601702958345413, 0.009363380260765553, 0.12665115296840668]], "avg_entropy": [0.8187566299457103], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bill Russell", "score_of_biggest_cluster": 0.5287014842033386, "score_of_first_answer": 0.5287014842033386, "max_score_of_first_answer": 0.9992484450340271, "min_score_of_first_answer": 0.08032483607530594, "avg_score": 0.5287014842033386, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3943093806516461, "std_of_first_answer": 0.3943093806516461, "avg_entropy_of_first_answer": 0.8187566299457103, "lex_sim": 1.0}
{"question": "how long did it take to build the great pyramid of egypt", "answer": ["20 years"], "scores": [[0.6757923364639282, 0.8411563038825989, 0.6987879276275635, 0.691944420337677]], "normalized_score": [0.7240634560585022], "tokens": [["\u2581", "2", "0", "\u2581years"]], "entropy": [[2.1206936836242676, 0.6665459871292114, 0.9304654598236084, 1.4012045860290527]], "avg_entropy": [1.279727429151535], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "20-year period", "score_of_biggest_cluster": 0.7240634560585022, "score_of_first_answer": 0.7240634560585022, "max_score_of_first_answer": 0.8411563038825989, "min_score_of_first_answer": 0.6757923364639282, "avg_score": 0.7240634560585022, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06648059072363632, "std_of_first_answer": 0.06648059072363632, "avg_entropy_of_first_answer": 1.279727429151535, "lex_sim": 1.0}
{"question": "which term is used to identify having official power to act", "answer": ["jurisdiction"], "scores": [[0.03352454677224159, 0.9787198901176453]], "normalized_score": [0.18113845586776733], "tokens": [["\u2581juris", "diction"]], "entropy": [[5.375818252563477, 0.13012683391571045]], "avg_entropy": [2.7529725432395935], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "authority", "score_of_biggest_cluster": 0.18113845586776733, "score_of_first_answer": 0.18113845586776733, "max_score_of_first_answer": 0.9787198901176453, "min_score_of_first_answer": 0.03352454677224159, "avg_score": 0.18113845586776733, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.47259767167270184, "std_of_first_answer": 0.47259767167270184, "avg_entropy_of_first_answer": 2.7529725432395935, "lex_sim": 1.0}
{"question": "where did zendaya place in dancing with the stars", "answer": ["4th"], "scores": [[0.5415186882019043, 0.11324432492256165, 0.9376673102378845]], "normalized_score": [0.38597533106803894], "tokens": [["\u2581", "4", "th"]], "entropy": [[2.5723912715911865, 1.8227509260177612, 0.32480761408805847]], "avg_entropy": [1.573316603899002], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Runner-up", "score_of_biggest_cluster": 0.38597533106803894, "score_of_first_answer": 0.38597533106803894, "max_score_of_first_answer": 0.9376673102378845, "min_score_of_first_answer": 0.11324432492256165, "avg_score": 0.38597533106803894, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33665444197270183, "std_of_first_answer": 0.33665444197270183, "avg_entropy_of_first_answer": 1.573316603899002, "lex_sim": 1.0}
{"question": "when does closure of the ductus arteriosus occur", "answer": ["Ductus Arteriosus"], "scores": [[0.0022782410960644484, 0.6529393196105957, 0.7620654702186584, 0.2325051724910736, 0.9933865666389465, 0.9928250312805176, 0.9828575849533081]], "normalized_score": [0.30673855543136597], "tokens": [["\u2581D", "uct", "us", "\u2581Ar", "ter", "ios", "us"]], "entropy": [[4.56989860534668, 2.0644664764404297, 0.7746051549911499, 1.909657597541809, 0.05729062855243683, 0.05559564754366875, 0.11208631098270416]], "avg_entropy": [1.3633714887712682], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "at birth", "score_of_biggest_cluster": 0.30673855543136597, "score_of_first_answer": 0.30673855543136597, "max_score_of_first_answer": 0.9933865666389465, "min_score_of_first_answer": 0.0022782410960644484, "avg_score": 0.30673855543136597, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3687765275718604, "std_of_first_answer": 0.3687765275718604, "avg_entropy_of_first_answer": 1.3633714887712682, "lex_sim": 1.0}
{"question": "who was the main character in their eyes were watching god", "answer": ["Janie Starks"], "scores": [[0.9498324990272522, 0.9957879185676575, 0.42504429817199707, 0.9659901261329651]], "normalized_score": [0.789414644241333], "tokens": [["\u2581Jan", "ie", "\u2581St", "arks"]], "entropy": [[0.43215590715408325, 0.033921029418706894, 1.3389055728912354, 0.19380781054496765]], "avg_entropy": [0.4996975800022483], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Janie Crawford", "score_of_biggest_cluster": 0.789414644241333, "score_of_first_answer": 0.789414644241333, "max_score_of_first_answer": 0.9957879185676575, "min_score_of_first_answer": 0.42504429817199707, "avg_score": 0.789414644241333, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23677972393931102, "std_of_first_answer": 0.23677972393931102, "avg_entropy_of_first_answer": 0.4996975800022483, "lex_sim": 1.0}
{"question": "locations for the film an englishman who went up a hill", "answer": ["Calderdale, England"], "scores": [[0.0027399887330830097, 0.0737919956445694, 0.12854869663715363, 0.6801555156707764, 0.0945030227303505]], "normalized_score": [0.06991592049598694], "tokens": [["\u2581Cal", "der", "dale", ",", "\u2581England"]], "entropy": [[6.029897689819336, 3.4077627658843994, 3.4931182861328125, 1.35042405128479, 4.490999221801758]], "avg_entropy": [3.7544404029846192], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Llanrhaeadr-ym-Mochnant", "score_of_biggest_cluster": 0.06991592049598694, "score_of_first_answer": 0.06991592049598694, "max_score_of_first_answer": 0.6801555156707764, "min_score_of_first_answer": 0.0027399887330830097, "avg_score": 0.06991592049598694, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2455777240536206, "std_of_first_answer": 0.2455777240536206, "avg_entropy_of_first_answer": 3.7544404029846192, "lex_sim": 1.0}
{"question": "where was the statue of liberty originally built", "answer": ["France"], "scores": [[0.3625636100769043]], "normalized_score": [0.3625636100769043], "tokens": [["\u2581France"]], "entropy": [[2.7260239124298096]], "avg_entropy": [2.7260239124298096], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paris", "score_of_biggest_cluster": 0.3625636100769043, "score_of_first_answer": 0.3625636100769043, "max_score_of_first_answer": 0.3625636100769043, "min_score_of_first_answer": 0.3625636100769043, "avg_score": 0.3625636100769043, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.7260239124298096, "lex_sim": 1.0}
{"question": "how many ammendments to the constitution have there been", "answer": ["27"], "scores": [[0.7554073929786682, 0.9115058183670044, 0.9392545223236084]], "normalized_score": [0.8647847175598145], "tokens": [["\u2581", "2", "7"]], "entropy": [[1.2257637977600098, 0.36378854513168335, 0.31769153475761414]], "avg_entropy": [0.6357479592164358], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Twenty-seven", "score_of_biggest_cluster": 0.8647847175598145, "score_of_first_answer": 0.8647847175598145, "max_score_of_first_answer": 0.9392545223236084, "min_score_of_first_answer": 0.7554073929786682, "avg_score": 0.8647847175598145, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08092278620756975, "std_of_first_answer": 0.08092278620756975, "avg_entropy_of_first_answer": 0.6357479592164358, "lex_sim": 1.0}
{"question": "season 2 this is us number of episodes", "answer": ["208"], "scores": [[0.864182710647583, 0.493787944316864, 0.17136338353157043, 0.0005744387744925916]], "normalized_score": [0.08050575852394104], "tokens": [["\u2581", "2", "0", "8"]], "entropy": [[1.0404167175292969, 1.0273537635803223, 1.7835088968276978, 0.7299731373786926]], "avg_entropy": [1.1453131288290024], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "15", "score_of_biggest_cluster": 0.08050575852394104, "score_of_first_answer": 0.08050575852394104, "max_score_of_first_answer": 0.864182710647583, "min_score_of_first_answer": 0.0005744387744925916, "avg_score": 0.08050575852394104, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32971542401142295, "std_of_first_answer": 0.32971542401142295, "avg_entropy_of_first_answer": 1.1453131288290024, "lex_sim": 1.0}
{"question": "when did the crucifix become the symbol of christianity", "answer": ["529 AD"], "scores": [[0.6349450349807739, 0.0444704033434391, 0.03513186052441597, 0.054560352116823196, 0.3407333195209503]], "normalized_score": [0.11302120238542557], "tokens": [["\u2581", "5", "2", "9", "\u2581AD"]], "entropy": [[2.364680528640747, 1.5751488208770752, 1.6213910579681396, 2.3652331829071045, 2.12107253074646]], "avg_entropy": [2.0095052242279055], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in the 2nd century", "score_of_biggest_cluster": 0.11302120238542557, "score_of_first_answer": 0.11302120238542557, "max_score_of_first_answer": 0.6349450349807739, "min_score_of_first_answer": 0.03513186052441597, "avg_score": 0.11302120238542557, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23625989627738736, "std_of_first_answer": 0.23625989627738736, "avg_entropy_of_first_answer": 2.0095052242279055, "lex_sim": 1.0}
{"question": "who sings love will keep us alive by the eagles", "answer": ["Don Henley"], "scores": [[0.4189926087856293, 0.8227189183235168, 0.9896118640899658]], "normalized_score": [0.6987270712852478], "tokens": [["\u2581Don", "\u2581Hen", "ley"]], "entropy": [[3.161454439163208, 0.5436564683914185, 0.0698433667421341]], "avg_entropy": [1.2583180914322536], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Timothy B. Schmit", "score_of_biggest_cluster": 0.6987270712852478, "score_of_first_answer": 0.6987270712852478, "max_score_of_first_answer": 0.9896118640899658, "min_score_of_first_answer": 0.4189926087856293, "avg_score": 0.6987270712852478, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23954923476390697, "std_of_first_answer": 0.23954923476390697, "avg_entropy_of_first_answer": 1.2583180914322536, "lex_sim": 1.0}
{"question": "when does the second part of vikings season 4 start", "answer": ["June 2017"], "scores": [[0.05410071089863777, 0.9500107169151306, 0.3157877027988434, 0.18842953443527222, 0.6255427002906799, 0.5570666193962097]], "normalized_score": [0.31959986686706543], "tokens": [["\u2581June", "\u2581", "2", "0", "1", "7"]], "entropy": [[3.3193743228912354, 0.2532757818698883, 0.7462168335914612, 1.9059869050979614, 1.1308927536010742, 1.1338623762130737]], "avg_entropy": [1.414934828877449], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "November 30, 2016", "score_of_biggest_cluster": 0.31959986686706543, "score_of_first_answer": 0.31959986686706543, "max_score_of_first_answer": 0.9500107169151306, "min_score_of_first_answer": 0.05410071089863777, "avg_score": 0.31959986686706543, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2987317155476369, "std_of_first_answer": 0.2987317155476369, "avg_entropy_of_first_answer": 1.414934828877449, "lex_sim": 1.0}
{"question": "who gave a speech to the democratic national convention in 1984", "answer": ["Rev."], "scores": [[0.006270839367061853, 0.852605402469635, 0.9233099818229675, 0.9979267120361328, 0.9163822531700134]], "normalized_score": [0.33956214785575867], "tokens": [["\u2581Rev", ".", "\u2581J", "esse", "\u2581Jackson"]], "entropy": [[3.775038242340088, 0.6516189575195312, 0.5297784209251404, 0.018381495028734207, 0.4342730641365051]], "avg_entropy": [1.0818180359899998], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mario Cuomo", "score_of_biggest_cluster": 0.33956214785575867, "score_of_first_answer": 0.33956214785575867, "max_score_of_first_answer": 0.9979267120361328, "min_score_of_first_answer": 0.006270839367061853, "avg_score": 0.33956214785575867, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3693982778896742, "std_of_first_answer": 0.3693982778896742, "avg_entropy_of_first_answer": 1.0818180359899998, "lex_sim": 1.0}
{"question": "when did the first junie b jones book come out", "answer": ["1991"], "scores": [[0.6142950654029846, 0.871552586555481, 0.9929920434951782, 0.9702085256576538, 0.22864529490470886]], "normalized_score": [0.6521218419075012], "tokens": [["\u2581", "1", "9", "9", "1"]], "entropy": [[1.9340561628341675, 0.42268672585487366, 0.06146351248025894, 0.15865065157413483, 1.606673002243042]], "avg_entropy": [0.8367060109972954], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1992", "score_of_biggest_cluster": 0.6521218419075012, "score_of_first_answer": 0.6521218419075012, "max_score_of_first_answer": 0.9929920434951782, "min_score_of_first_answer": 0.22864529490470886, "avg_score": 0.6521218419075012, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2868772809221255, "std_of_first_answer": 0.2868772809221255, "avg_entropy_of_first_answer": 0.8367060109972954, "lex_sim": 1.0}
{"question": "who was the temple of vesta built for", "answer": ["Roman Pagan religion"], "scores": [[0.12003876268863678, 0.003375766333192587, 0.595117449760437, 0.025755945593118668]], "normalized_score": [0.04992217943072319], "tokens": [["\u2581Roman", "\u2581P", "agan", "\u2581religion"]], "entropy": [[3.8126590251922607, 2.728437662124634, 1.1114541292190552, 2.1170318126678467]], "avg_entropy": [2.442395657300949], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Vesta", "score_of_biggest_cluster": 0.04992217943072319, "score_of_first_answer": 0.04992217943072319, "max_score_of_first_answer": 0.595117449760437, "min_score_of_first_answer": 0.003375766333192587, "avg_score": 0.04992217943072319, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.24018623997648283, "std_of_first_answer": 0.24018623997648283, "avg_entropy_of_first_answer": 2.442395657300949, "lex_sim": 1.0}
{"question": "who wrote the song city of new orleans", "answer": ["User Answered Questions: 224 User Questions: 90 User Answers: 94 User Answers Were: 14 User Tags: User Search Terms: User R"], "scores": [[1.0716833003243664e-06, 5.077840614831075e-05, 0.28666773438453674, 0.018427139148116112, 0.3623819053173065, 0.1263645738363266, 0.6062631011009216, 0.14798064529895782, 0.0805181935429573, 0.043282683938741684, 0.009750972501933575, 0.08114489167928696, 0.9564881920814514, 0.6525409817695618, 0.9628961682319641, 0.02881070226430893, 0.09513632953166962, 0.06264558434486389, 0.2918736934661865, 0.999322772026062, 0.9782842993736267, 0.9811939597129822, 0.07482621818780899, 0.06716577708721161, 0.1635708510875702, 0.035091277211904526, 0.9961147308349609, 0.0009829855989664793, 0.2689060568809509, 0.24685460329055786, 0.7542166709899902, 0.21757926046848297, 0.059136372059583664, 0.1670474261045456, 0.002094669034704566, 0.9425973296165466, 0.030748436227440834, 0.007937846705317497, 0.0355447456240654, 0.9430434107780457, 0.9650464057922363, 0.0727389007806778, 0.011926263570785522]], "normalized_score": [0.0789555236697197], "tokens": [["\u2581User", "\u2581Answer", "ed", "\u2581Question", "s", ":", "\u2581", "2", "2", "4", "\u2581User", "\u2581Question", "s", ":", "\u2581", "9", "0", "\u2581User", "\u2581An", "swers", ":", "\u2581", "9", "4", "\u2581User", "\u2581An", "swers", "\u2581W", "ere", ":", "\u2581", "1", "4", "\u2581User", "\u2581Tags", ":", "\u2581User", "\u2581Search", "\u2581Term", "s", ":", "\u2581User", "\u2581R"]], "entropy": [[4.109930992126465, 2.730194330215454, 2.4110584259033203, 2.9615731239318848, 2.4685025215148926, 2.5378170013427734, 2.346372604370117, 2.020658254623413, 2.8512492179870605, 3.341149091720581, 3.364567279815674, 4.807906150817871, 0.33839213848114014, 2.0526223182678223, 0.36686602234840393, 1.9040801525115967, 3.0387091636657715, 3.5726280212402344, 4.4036664962768555, 0.007525327615439892, 0.1931462585926056, 0.18765702843666077, 2.0208842754364014, 2.5187227725982666, 3.2524101734161377, 5.22689151763916, 0.03758471831679344, 1.6360797882080078, 1.4751962423324585, 3.5063233375549316, 1.9149073362350464, 2.1807780265808105, 2.9658241271972656, 3.6972179412841797, 4.745817184448242, 0.4387643337249756, 4.693412780761719, 3.508002996444702, 2.1629295349121094, 0.2686532735824585, 0.3052980303764343, 5.490902900695801, 5.276544570922852]], "avg_entropy": [2.5427771577784837], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Steve Goodman", "score_of_biggest_cluster": 0.0789555236697197, "score_of_first_answer": 0.0789555236697197, "max_score_of_first_answer": 0.999322772026062, "min_score_of_first_answer": 1.0716833003243664e-06, "avg_score": 0.0789555236697197, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3734232229311472, "std_of_first_answer": 0.3734232229311472, "avg_entropy_of_first_answer": 2.5427771577784837, "lex_sim": 1.0}
{"question": "who sings my anaconda don't want none", "answer": ["Rapper Big Poppa"], "scores": [[0.054843854159116745, 0.03295521438121796, 0.01158494595438242, 0.14618586003780365, 0.7719202637672424]], "normalized_score": [0.07493501156568527], "tokens": [["\u2581R", "apper", "\u2581Big", "\u2581Po", "ppa"]], "entropy": [[3.6025052070617676, 2.2305731773376465, 2.863002300262451, 3.367586612701416, 0.7413449883460999]], "avg_entropy": [2.561002457141876], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sir Mix-a-Lot", "score_of_biggest_cluster": 0.07493501156568527, "score_of_first_answer": 0.07493501156568527, "max_score_of_first_answer": 0.7719202637672424, "min_score_of_first_answer": 0.01158494595438242, "avg_score": 0.07493501156568527, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2878895859348621, "std_of_first_answer": 0.2878895859348621, "avg_entropy_of_first_answer": 2.561002457141876, "lex_sim": 1.0}
{"question": "who plays the woodsman in over the garden wall", "answer": ["Peter Serafinowicz"], "scores": [[0.061017170548439026, 0.3015899062156677, 0.8366549015045166, 0.9979866743087769, 0.9996318817138672, 0.9947823882102966]], "normalized_score": [0.498139888048172], "tokens": [["\u2581Peter", "\u2581S", "era", "fin", "ow", "icz"]], "entropy": [[4.3379411697387695, 3.7169060707092285, 1.061223030090332, 0.015804782509803772, 0.004489526618272066, 0.043491996824741364]], "avg_entropy": [1.5299760960818578], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Christopher Lloyd", "score_of_biggest_cluster": 0.498139888048172, "score_of_first_answer": 0.498139888048172, "max_score_of_first_answer": 0.9996318817138672, "min_score_of_first_answer": 0.061017170548439026, "avg_score": 0.498139888048172, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37664408836359625, "std_of_first_answer": 0.37664408836359625, "avg_entropy_of_first_answer": 1.5299760960818578, "lex_sim": 1.0}
{"question": "who sang the song suddenly with olivia newton john", "answer": ["John Travolta"], "scores": [[0.09035160392522812, 0.7338388562202454, 0.9964627623558044, 0.9982402324676514]], "normalized_score": [0.5067667961120605], "tokens": [["\u2581John", "\u2581Tra", "vol", "ta"]], "entropy": [[4.908683776855469, 1.2650643587112427, 0.029825259000062943, 0.016592111438512802]], "avg_entropy": [1.5550413765013218], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cliff Richard", "score_of_biggest_cluster": 0.5067667961120605, "score_of_first_answer": 0.5067667961120605, "max_score_of_first_answer": 0.9982402324676514, "min_score_of_first_answer": 0.09035160392522812, "avg_score": 0.5067667961120605, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37066305457000415, "std_of_first_answer": 0.37066305457000415, "avg_entropy_of_first_answer": 1.5550413765013218, "lex_sim": 1.0}
{"question": "active absorption of water from the soil by the root is mainly affected by", "answer": ["soil moisture content"], "scores": [[0.05045873671770096, 0.26912206411361694, 0.9992939233779907, 0.9987980127334595, 0.0940968245267868]], "normalized_score": [0.2637098431587219], "tokens": [["\u2581soil", "\u2581mo", "ist", "ure", "\u2581content"]], "entropy": [[5.2310471534729, 3.0827407836914062, 0.007326978258788586, 0.012684397399425507, 1.3016555309295654]], "avg_entropy": [1.9270909687504172], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "SURFACE AREA OF ROOTS", "score_of_biggest_cluster": 0.2637098431587219, "score_of_first_answer": 0.2637098431587219, "max_score_of_first_answer": 0.9992939233779907, "min_score_of_first_answer": 0.05045873671770096, "avg_score": 0.2637098431587219, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.428179007870358, "std_of_first_answer": 0.428179007870358, "avg_entropy_of_first_answer": 1.9270909687504172, "lex_sim": 1.0}
{"question": "when does planet of the apes come out 2017", "answer": ["August 2017"], "scores": [[0.20005610585212708, 0.9243873357772827, 0.027424974367022514, 0.606207549571991, 0.9804307818412781, 0.9358910918235779]], "normalized_score": [0.3758973777294159], "tokens": [["\u2581August", "\u2581", "2", "0", "1", "7"]], "entropy": [[2.117121696472168, 0.3203809857368469, 0.6068080067634583, 1.552983283996582, 0.12157665193080902, 0.3333408534526825]], "avg_entropy": [0.8420352463920912], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "July 14, 2017", "score_of_biggest_cluster": 0.3758973777294159, "score_of_first_answer": 0.3758973777294159, "max_score_of_first_answer": 0.9804307818412781, "min_score_of_first_answer": 0.027424974367022514, "avg_score": 0.3758973777294159, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3763178741999419, "std_of_first_answer": 0.3763178741999419, "avg_entropy_of_first_answer": 0.8420352463920912, "lex_sim": 1.0}
{"question": "ganglion axons forming the optic nerve run to the", "answer": ["brain"], "scores": [[0.04575815424323082]], "normalized_score": [0.04575814679265022], "tokens": [["\u2581brain"]], "entropy": [[4.269822597503662]], "avg_entropy": [4.269822597503662], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "pretectal nucleus", "score_of_biggest_cluster": 0.04575814679265022, "score_of_first_answer": 0.04575814679265022, "max_score_of_first_answer": 0.04575815424323082, "min_score_of_first_answer": 0.04575815424323082, "avg_score": 0.04575814679265022, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.269822597503662, "lex_sim": 1.0}
{"question": "turkey stuffed with a duck stuffed with a chicken", "answer": ["Sausage"], "scores": [[0.008630502969026566, 0.08596877753734589, 0.9439016580581665]], "normalized_score": [0.08880443125963211], "tokens": [["\u2581S", "aus", "age"]], "entropy": [[6.097867965698242, 5.289040565490723, 0.4600100815296173]], "avg_entropy": [3.948972870906194], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Turducken", "score_of_biggest_cluster": 0.08880443125963211, "score_of_first_answer": 0.08880443125963211, "max_score_of_first_answer": 0.9439016580581665, "min_score_of_first_answer": 0.008630502969026566, "avg_score": 0.08880443125963211, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4238398767005362, "std_of_first_answer": 0.4238398767005362, "avg_entropy_of_first_answer": 3.948972870906194, "lex_sim": 1.0}
{"question": "when did the great fire of london end", "answer": ["After a month."], "scores": [[0.0012093179393559694, 0.03648895397782326, 0.17067226767539978, 0.016517329961061478]], "normalized_score": [0.018780242651700974], "tokens": [["\u2581After", "\u2581a", "\u2581month", "."]], "entropy": [[1.7754650115966797, 3.1126132011413574, 3.0893375873565674, 1.4712471961975098]], "avg_entropy": [2.3621657490730286], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "5 September 1666", "score_of_biggest_cluster": 0.018780242651700974, "score_of_first_answer": 0.018780242651700974, "max_score_of_first_answer": 0.17067226767539978, "min_score_of_first_answer": 0.0012093179393559694, "avg_score": 0.018780242651700974, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06725160385947357, "std_of_first_answer": 0.06725160385947357, "avg_entropy_of_first_answer": 2.3621657490730286, "lex_sim": 1.0}
{"question": "where was the tv show friday night lights filmed", "answer": ["Houston, Texas"], "scores": [[0.002044726861640811, 0.7412921786308289, 0.8857215046882629]], "normalized_score": [0.11031655967235565], "tokens": [["\u2581Houston", ",", "\u2581Texas"]], "entropy": [[3.9774134159088135, 0.8514398336410522, 0.49475347995758057]], "avg_entropy": [1.7745355765024822], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Austin", "score_of_biggest_cluster": 0.11031655967235565, "score_of_first_answer": 0.11031655967235565, "max_score_of_first_answer": 0.8857215046882629, "min_score_of_first_answer": 0.002044726861640811, "avg_score": 0.11031655967235565, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3870445384710889, "std_of_first_answer": 0.3870445384710889, "avg_entropy_of_first_answer": 1.7745355765024822, "lex_sim": 1.0}
{"question": "when was the nobel prize for literature first awarded", "answer": ["1901"], "scores": [[0.8191126585006714, 0.9979352951049805, 0.9905325174331665, 0.9947755336761475, 0.9943037033081055]], "normalized_score": [0.9565590620040894], "tokens": [["\u2581", "1", "9", "0", "1"]], "entropy": [[1.1110520362854004, 0.019163114950060844, 0.07115363329648972, 0.047176528722047806, 0.045030899345874786]], "avg_entropy": [0.2587152425199747], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1901", "score_of_biggest_cluster": 0.9565590620040894, "score_of_first_answer": 0.9565590620040894, "max_score_of_first_answer": 0.9979352951049805, "min_score_of_first_answer": 0.8191126585006714, "avg_score": 0.9565590620040894, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.07014900501542438, "std_of_first_answer": 0.07014900501542438, "avg_entropy_of_first_answer": 0.2587152425199747, "lex_sim": 1.0}
{"question": "premier league players to score 5 goals in one game", "answer": ["Alan Shearer"], "scores": [[0.14643977582454681, 0.9510034322738647, 0.9964979887008667, 0.998798131942749]], "normalized_score": [0.6101671457290649], "tokens": [["\u2581Alan", "\u2581She", "ar", "er"]], "entropy": [[4.810423851013184, 0.38127145171165466, 0.035600535571575165, 0.013069496490061283]], "avg_entropy": [1.3100913336966187], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alan Shearer", "score_of_biggest_cluster": 0.6101671457290649, "score_of_first_answer": 0.6101671457290649, "max_score_of_first_answer": 0.998798131942749, "min_score_of_first_answer": 0.14643977582454681, "avg_score": 0.6101671457290649, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3623530554274695, "std_of_first_answer": 0.3623530554274695, "avg_entropy_of_first_answer": 1.3100913336966187, "lex_sim": 1.0}
{"question": "who wrote old flames cant hold a candle to you", "answer": ["Dolly Parton"], "scores": [[0.1423850953578949, 0.9941945672035217, 0.9811481833457947, 0.99906986951828]], "normalized_score": [0.6103326678276062], "tokens": [["\u2581Dol", "ly", "\u2581Part", "on"]], "entropy": [[5.104001522064209, 0.039322953671216965, 0.19229403138160706, 0.009358452633023262]], "avg_entropy": [1.336244239937514], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pebe Sebert", "score_of_biggest_cluster": 0.6103326678276062, "score_of_first_answer": 0.6103326678276062, "max_score_of_first_answer": 0.99906986951828, "min_score_of_first_answer": 0.1423850953578949, "avg_score": 0.6103326678276062, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3677233045982775, "std_of_first_answer": 0.3677233045982775, "avg_entropy_of_first_answer": 1.336244239937514, "lex_sim": 1.0}
{"question": "when was the last time clemson won the national championship", "answer": ["1981"], "scores": [[0.8245672583580017, 0.2051907330751419, 0.9745447039604187, 0.9773557186126709, 0.9151699542999268]], "normalized_score": [0.681942880153656], "tokens": [["\u2581", "1", "9", "8", "1"]], "entropy": [[1.0156822204589844, 0.5875140428543091, 0.19421687722206116, 0.15006887912750244, 0.4490094780921936]], "avg_entropy": [0.4792982995510101], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2016", "score_of_biggest_cluster": 0.681942880153656, "score_of_first_answer": 0.681942880153656, "max_score_of_first_answer": 0.9773557186126709, "min_score_of_first_answer": 0.2051907330751419, "avg_score": 0.681942880153656, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29238938423422156, "std_of_first_answer": 0.29238938423422156, "avg_entropy_of_first_answer": 0.4792982995510101, "lex_sim": 1.0}
{"question": "at what age are you no longer a toddler", "answer": ["three years old"], "scores": [[0.02824961580336094, 0.2447802722454071, 0.5827719569206238]], "normalized_score": [0.15913383662700653], "tokens": [["\u2581three", "\u2581years", "\u2581old"]], "entropy": [[3.704461097717285, 0.815955638885498, 1.0098439455032349]], "avg_entropy": [1.8434202273686726], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "36 months", "score_of_biggest_cluster": 0.15913383662700653, "score_of_first_answer": 0.15913383662700653, "max_score_of_first_answer": 0.5827719569206238, "min_score_of_first_answer": 0.02824961580336094, "avg_score": 0.15913383662700653, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.22818582361433815, "std_of_first_answer": 0.22818582361433815, "avg_entropy_of_first_answer": 1.8434202273686726, "lex_sim": 1.0}
{"question": "who starred in an officer and a gentleman", "answer": ["John Gammon"], "scores": [[0.0142007265239954, 0.00025559726054780185, 0.5962166786193848]], "normalized_score": [0.01293471734970808], "tokens": [["\u2581John", "\u2581Gam", "mon"]], "entropy": [[3.943227529525757, 0.9758207201957703, 2.1465377807617188]], "avg_entropy": [2.3551953434944153], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Richard Gere", "score_of_biggest_cluster": 0.01293471734970808, "score_of_first_answer": 0.01293471734970808, "max_score_of_first_answer": 0.5962166786193848, "min_score_of_first_answer": 0.00025559726054780185, "avg_score": 0.01293471734970808, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2777102097947249, "std_of_first_answer": 0.2777102097947249, "avg_entropy_of_first_answer": 2.3551953434944153, "lex_sim": 1.0}
{"question": "when did the ship hector arrived in pictou", "answer": ["1773"], "scores": [[0.5199432373046875, 0.9597510695457458, 0.5743193030357361, 0.8216397762298584, 0.6985592842102051]], "normalized_score": [0.6969963908195496], "tokens": [["\u2581", "1", "7", "7", "3"]], "entropy": [[2.320321559906006, 0.2388540506362915, 1.1917924880981445, 0.8299560546875, 1.2381739616394043]], "avg_entropy": [1.1638196229934692], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1773", "score_of_biggest_cluster": 0.6969963908195496, "score_of_first_answer": 0.6969963908195496, "max_score_of_first_answer": 0.9597510695457458, "min_score_of_first_answer": 0.5199432373046875, "avg_score": 0.6969963908195496, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1608623636825874, "std_of_first_answer": 0.1608623636825874, "avg_entropy_of_first_answer": 1.1638196229934692, "lex_sim": 1.0}
{"question": "where did the tea come from in the boston tea party", "answer": ["China"], "scores": [[0.13814616203308105]], "normalized_score": [0.13814616203308105], "tokens": [["\u2581China"]], "entropy": [[3.7225818634033203]], "avg_entropy": [3.7225818634033203], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "England", "score_of_biggest_cluster": 0.13814616203308105, "score_of_first_answer": 0.13814616203308105, "max_score_of_first_answer": 0.13814616203308105, "min_score_of_first_answer": 0.13814616203308105, "avg_score": 0.13814616203308105, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.7225818634033203, "lex_sim": 1.0}
{"question": "when was the drinking age set to 21", "answer": ["July 1, 1988"], "scores": [[0.05391194298863411, 0.9680031538009644, 0.9700626730918884, 0.06909701228141785, 0.9871290922164917, 0.9993528723716736, 0.9993805885314941, 0.9937877655029297, 0.0619838573038578]], "normalized_score": [0.390762597322464], "tokens": [["\u2581July", "\u2581", "1", ",", "\u2581", "1", "9", "8", "8"]], "entropy": [[2.259521245956421, 0.18188753724098206, 0.18671084940433502, 0.8733389377593994, 0.07374188303947449, 0.00581634696573019, 0.005835868418216705, 0.04645019769668579, 0.4948188066482544]], "avg_entropy": [0.45868018590327764], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "By mid-1988", "score_of_biggest_cluster": 0.390762597322464, "score_of_first_answer": 0.390762597322464, "max_score_of_first_answer": 0.9993805885314941, "min_score_of_first_answer": 0.05391194298863411, "avg_score": 0.390762597322464, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43601259889577293, "std_of_first_answer": 0.43601259889577293, "avg_entropy_of_first_answer": 0.45868018590327764, "lex_sim": 1.0}
{"question": "who conducted the opening concert at carnegie hall", "answer": ["Ignacy Jan Paderewski"], "scores": [[0.15112954378128052, 0.3580300211906433, 0.9003847241401672, 0.9879466891288757, 0.9977834820747375, 0.9954777359962463, 0.9902476668357849]], "normalized_score": [0.6467685699462891], "tokens": [["\u2581Ign", "acy", "\u2581Jan", "\u2581Pad", "ere", "w", "ski"]], "entropy": [[4.646365165710449, 0.9601780772209167, 0.6216968297958374, 0.1140541136264801, 0.026482973247766495, 0.0383979007601738, 0.06958231329917908]], "avg_entropy": [0.9252510533801147], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pyotr Ilyich Tchaikovsky", "score_of_biggest_cluster": 0.6467685699462891, "score_of_first_answer": 0.6467685699462891, "max_score_of_first_answer": 0.9977834820747375, "min_score_of_first_answer": 0.15112954378128052, "avg_score": 0.6467685699462891, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33132716410436946, "std_of_first_answer": 0.33132716410436946, "avg_entropy_of_first_answer": 0.9252510533801147, "lex_sim": 1.0}
{"question": "what age do you need to be to buy a bb gun", "answer": ["Sixteen"], "scores": [[0.005751799326390028, 0.996881365776062]], "normalized_score": [0.07572226971387863], "tokens": [["\u2581Six", "teen"]], "entropy": [[2.7030558586120605, 0.025504592806100845]], "avg_entropy": [1.3642802257090807], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "18", "score_of_biggest_cluster": 0.07572226971387863, "score_of_first_answer": 0.07572226971387863, "max_score_of_first_answer": 0.996881365776062, "min_score_of_first_answer": 0.005751799326390028, "avg_score": 0.07572226971387863, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.495564783224836, "std_of_first_answer": 0.495564783224836, "avg_entropy_of_first_answer": 1.3642802257090807, "lex_sim": 1.0}
{"question": "what do you call initials that stand for something", "answer": ["acronyms"], "scores": [[0.1540774255990982, 0.9990724325180054, 0.9993422627449036, 0.34019234776496887]], "normalized_score": [0.47829264402389526], "tokens": [["\u2581ac", "ron", "ym", "s"]], "entropy": [[4.662863731384277, 0.009868822991847992, 0.0070966570638120174, 0.7739793658256531]], "avg_entropy": [1.3634521443163976], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "acronym", "score_of_biggest_cluster": 0.47829264402389526, "score_of_first_answer": 0.47829264402389526, "max_score_of_first_answer": 0.9993422627449036, "min_score_of_first_answer": 0.1540774255990982, "avg_score": 0.47829264402389526, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3817500507691839, "std_of_first_answer": 0.3817500507691839, "avg_entropy_of_first_answer": 1.3634521443163976, "lex_sim": 1.0}
{"question": "when will the flash season 4 be released", "answer": ["January 2020"], "scores": [[0.021749142557382584, 0.9100376963615417, 0.35633862018585205, 0.6807505488395691, 0.026944415643811226, 0.9646589756011963]], "normalized_score": [0.22354546189308167], "tokens": [["\u2581January", "\u2581", "2", "0", "2", "0"]], "entropy": [[3.2262325286865234, 0.44598478078842163, 0.8020955324172974, 1.1413049697875977, 0.5087345838546753, 0.19822828471660614]], "avg_entropy": [1.0537634467085202], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "October 10, 2017", "score_of_biggest_cluster": 0.22354546189308167, "score_of_first_answer": 0.22354546189308167, "max_score_of_first_answer": 0.9646589756011963, "min_score_of_first_answer": 0.021749142557382584, "avg_score": 0.22354546189308167, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38505267703564744, "std_of_first_answer": 0.38505267703564744, "avg_entropy_of_first_answer": 1.0537634467085202, "lex_sim": 1.0}
{"question": "the grand tour season 2 episode 2 celebrity guests", "answer": ["Michael Kors"], "scores": [[0.008820031769573689, 0.06415253132581711, 0.3632119596004486]], "normalized_score": [0.059013042598962784], "tokens": [["\u2581Michael", "\u2581K", "ors"]], "entropy": [[6.246485233306885, 3.7833845615386963, 3.0617473125457764]], "avg_entropy": [4.363872369130452], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brian Wilson", "score_of_biggest_cluster": 0.059013042598962784, "score_of_first_answer": 0.059013042598962784, "max_score_of_first_answer": 0.3632119596004486, "min_score_of_first_answer": 0.008820031769573689, "avg_score": 0.059013042598962784, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.15566768938235606, "std_of_first_answer": 0.15566768938235606, "avg_entropy_of_first_answer": 4.363872369130452, "lex_sim": 1.0}
{"question": "who wrote the song oh lord won't you buy me a mercedes benz", "answer": ["Janis Joplin"], "scores": [[0.7222138047218323, 0.9729515314102173, 0.6502709984779358, 0.9989134073257446, 0.9991102814674377]], "normalized_score": [0.8546701073646545], "tokens": [["\u2581Jan", "is", "\u2581J", "op", "lin"]], "entropy": [[1.9550325870513916, 0.2190428376197815, 1.095221757888794, 0.011662635952234268, 0.009268684312701225]], "avg_entropy": [0.6580457005649805], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Janis Joplin", "score_of_biggest_cluster": 0.8546701073646545, "score_of_first_answer": 0.8546701073646545, "max_score_of_first_answer": 0.9991102814674377, "min_score_of_first_answer": 0.6502709984779358, "avg_score": 0.8546701073646545, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.15099681357866193, "std_of_first_answer": 0.15099681357866193, "avg_entropy_of_first_answer": 0.6580457005649805, "lex_sim": 1.0}
{"question": "the old man and the sea page count", "answer": ["18"], "scores": [[0.9070899486541748, 0.6541016101837158, 0.09268268942832947]], "normalized_score": [0.3802752196788788], "tokens": [["\u2581", "1", "8"]], "entropy": [[0.8482189178466797, 1.160144567489624, 2.2400243282318115]], "avg_entropy": [1.4161292711893718], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "127", "score_of_biggest_cluster": 0.3802752196788788, "score_of_first_answer": 0.3802752196788788, "max_score_of_first_answer": 0.9070899486541748, "min_score_of_first_answer": 0.09268268942832947, "avg_score": 0.3802752196788788, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34033537182934054, "std_of_first_answer": 0.34033537182934054, "avg_entropy_of_first_answer": 1.4161292711893718, "lex_sim": 1.0}
{"question": "who represents the insured in the investigation of a claim", "answer": ["the insurer"], "scores": [[0.08028703182935715, 0.3994757831096649, 0.0875774547457695]], "normalized_score": [0.14109428226947784], "tokens": [["\u2581the", "\u2581ins", "urer"]], "entropy": [[3.9357407093048096, 2.4630420207977295, 0.9313790798187256]], "avg_entropy": [2.443387269973755], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Public adjusters", "score_of_biggest_cluster": 0.14109428226947784, "score_of_first_answer": 0.14109428226947784, "max_score_of_first_answer": 0.3994757831096649, "min_score_of_first_answer": 0.08028703182935715, "avg_score": 0.14109428226947784, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14877842452839973, "std_of_first_answer": 0.14877842452839973, "avg_entropy_of_first_answer": 2.443387269973755, "lex_sim": 1.0}
{"question": "who plays stacey's mum in gavin and stacey", "answer": ["Ruth Jones"], "scores": [[0.09983550757169724, 0.9681434631347656]], "normalized_score": [0.3108940124511719], "tokens": [["\u2581Ruth", "\u2581Jones"]], "entropy": [[3.344180107116699, 0.23755714297294617]], "avg_entropy": [1.7908686250448227], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Melanie Walters", "score_of_biggest_cluster": 0.3108940124511719, "score_of_first_answer": 0.3108940124511719, "max_score_of_first_answer": 0.9681434631347656, "min_score_of_first_answer": 0.09983550757169724, "avg_score": 0.3108940124511719, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4341539777815342, "std_of_first_answer": 0.4341539777815342, "avg_entropy_of_first_answer": 1.7908686250448227, "lex_sim": 1.0}
{"question": "who said i will not go quietly into the night", "answer": ["William Shatner (\u201cJim Kirk\u201d)"], "scores": [[0.033626995980739594, 0.033187590539455414, 0.9484209418296814, 0.9993051290512085, 0.004630381241440773, 0.004071095492690802, 0.06771335750818253, 0.7223525047302246, 0.40591469407081604, 0.9399635791778564, 0.41227880120277405]], "normalized_score": [0.12817537784576416], "tokens": [["\u2581William", "\u2581Sh", "at", "ner", "\u2581(", "\u201c", "J", "im", "\u2581Kirk", "\u201d", ")"]], "entropy": [[5.118858337402344, 4.71873664855957, 0.34630122780799866, 0.00841944757848978, 0.4268379807472229, 4.023599624633789, 3.242182731628418, 1.0726267099380493, 1.1300079822540283, 0.3878744840621948, 1.8098385334014893]], "avg_entropy": [2.0259348825466903], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Welsh poet Dylan Thomas", "score_of_biggest_cluster": 0.12817537784576416, "score_of_first_answer": 0.12817537784576416, "max_score_of_first_answer": 0.9993051290512085, "min_score_of_first_answer": 0.004071095492690802, "avg_score": 0.12817537784576416, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39831334620753, "std_of_first_answer": 0.39831334620753, "avg_entropy_of_first_answer": 2.0259348825466903, "lex_sim": 1.0}
{"question": "who said have you no sense of decency", "answer": ["Arthur Fierro"], "scores": [[0.015617667697370052, 0.041276946663856506, 0.0015141194453462958, 0.8287786841392517]], "normalized_score": [0.02999028004705906], "tokens": [["\u2581Arthur", "\u2581F", "ier", "ro"]], "entropy": [[3.895291328430176, 4.539330959320068, 2.166388511657715, 0.910759687423706]], "avg_entropy": [2.8779426217079163], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joseph Nye Welch", "score_of_biggest_cluster": 0.02999028004705906, "score_of_first_answer": 0.02999028004705906, "max_score_of_first_answer": 0.8287786841392517, "min_score_of_first_answer": 0.0015141194453462958, "avg_score": 0.02999028004705906, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3507309221722316, "std_of_first_answer": 0.3507309221722316, "avg_entropy_of_first_answer": 2.8779426217079163, "lex_sim": 1.0}
{"question": "where was it happened at the world fair filmed", "answer": ["Flushing Meadows Park"], "scores": [[0.11810566484928131, 0.9695941805839539, 0.8102036714553833, 0.9998825788497925, 0.9588727355003357, 0.14857016503810883]], "normalized_score": [0.4862385392189026], "tokens": [["\u2581Fl", "ushing", "\u2581M", "ead", "ows", "\u2581Park"]], "entropy": [[4.164783954620361, 0.14777624607086182, 0.7537288665771484, 0.0015321893151849508, 0.17564690113067627, 1.871752142906189]], "avg_entropy": [1.1858700501034036], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Seattle, Washington", "score_of_biggest_cluster": 0.4862385392189026, "score_of_first_answer": 0.4862385392189026, "max_score_of_first_answer": 0.9998825788497925, "min_score_of_first_answer": 0.11810566484928131, "avg_score": 0.4862385392189026, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.382562365694518, "std_of_first_answer": 0.382562365694518, "avg_entropy_of_first_answer": 1.1858700501034036, "lex_sim": 1.0}
{"question": "who wrote catch 22 (both names)", "answer": ["Joseph Heller"], "scores": [[0.8490731120109558, 0.9452541470527649, 0.9995957016944885]], "normalized_score": [0.9291931986808777], "tokens": [["\u2581Joseph", "\u2581H", "eller"]], "entropy": [[1.16517972946167, 0.4667177200317383, 0.004901118576526642]], "avg_entropy": [0.5455995226899782], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joseph Heller.", "score_of_biggest_cluster": 0.9291931986808777, "score_of_first_answer": 0.9291931986808777, "max_score_of_first_answer": 0.9995957016944885, "min_score_of_first_answer": 0.8490731120109558, "avg_score": 0.9291931986808777, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06223686480573881, "std_of_first_answer": 0.06223686480573881, "avg_entropy_of_first_answer": 0.5455995226899782, "lex_sim": 1.0}
{"question": "where is the new stadium being built in las vegas", "answer": ["Las Vegas, Nevada, U.S."], "scores": [[0.2774832844734192, 0.9953489899635315, 0.9997206330299377, 0.05890971049666405, 0.8207581043243408, 0.9989290833473206, 0.13507522642612457, 0.06489201635122299, 0.9865585565567017, 0.9935142993927002, 0.9221790432929993]], "normalized_score": [0.43504783511161804], "tokens": [["\u2581Las", "\u2581Veg", "as", ",", "\u2581Nev", "ada", ",", "\u2581U", ".", "S", "."]], "entropy": [[3.9627442359924316, 0.044009365141391754, 0.0035480912774801254, 2.1045644283294678, 0.8338041305541992, 0.01055697537958622, 0.7500131130218506, 1.3529797792434692, 0.10257646441459656, 0.04848795011639595, 0.33547335863113403]], "avg_entropy": [0.8680688992820003], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paradise, Nevada", "score_of_biggest_cluster": 0.43504783511161804, "score_of_first_answer": 0.43504783511161804, "max_score_of_first_answer": 0.9997206330299377, "min_score_of_first_answer": 0.05890971049666405, "avg_score": 0.43504783511161804, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4036784703064306, "std_of_first_answer": 0.4036784703064306, "avg_entropy_of_first_answer": 0.8680688992820003, "lex_sim": 1.0}
{"question": "when was the last time the denver nuggets made the playoffs", "answer": ["2012-2013"], "scores": [[0.8410489559173584, 0.9378165602684021, 0.9977147579193115, 0.628054141998291, 0.2659510672092438, 0.3832625150680542, 0.5522235631942749, 0.9967578053474426, 0.9998536109924316, 0.9970581531524658]], "normalized_score": [0.6984900236129761], "tokens": [["\u2581", "2", "0", "1", "2", "-", "2", "0", "1", "3"]], "entropy": [[1.1070278882980347, 0.2952921688556671, 0.02301323227584362, 0.6750456690788269, 1.8226969242095947, 1.2672674655914307, 0.7344354391098022, 0.03497503325343132, 0.0016669731121510267, 0.02489396743476391]], "avg_entropy": [0.5986314761219547], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2012\u201313", "score_of_biggest_cluster": 0.6984900236129761, "score_of_first_answer": 0.6984900236129761, "max_score_of_first_answer": 0.9998536109924316, "min_score_of_first_answer": 0.2659510672092438, "avg_score": 0.6984900236129761, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.26665947958912556, "std_of_first_answer": 0.26665947958912556, "avg_entropy_of_first_answer": 0.5986314761219547, "lex_sim": 1.0}
{"question": "where is wind power used most in the united states", "answer": ["Iowa"], "scores": [[0.02162288874387741]], "normalized_score": [0.02162288688123226], "tokens": [["\u2581Iowa"]], "entropy": [[4.407530784606934]], "avg_entropy": [4.407530784606934], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Texas", "score_of_biggest_cluster": 0.02162288688123226, "score_of_first_answer": 0.02162288688123226, "max_score_of_first_answer": 0.02162288874387741, "min_score_of_first_answer": 0.02162288874387741, "avg_score": 0.02162288688123226, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.407530784606934, "lex_sim": 1.0}
{"question": "who is the best director in bollywood 2017", "answer": ["AR Rajnikanth"], "scores": [[0.00877547450363636, 0.011204196140170097, 0.5103939175605774, 0.8736919164657593]], "normalized_score": [0.0813727006316185], "tokens": [["\u2581AR", "\u2581Raj", "nik", "anth"]], "entropy": [[3.781547784805298, 1.6291983127593994, 2.196781635284424, 0.4239557385444641]], "avg_entropy": [2.0078708678483963], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nitesh Tiwari", "score_of_biggest_cluster": 0.0813727006316185, "score_of_first_answer": 0.0813727006316185, "max_score_of_first_answer": 0.8736919164657593, "min_score_of_first_answer": 0.00877547450363636, "avg_score": 0.0813727006316185, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.364414624292577, "std_of_first_answer": 0.364414624292577, "avg_entropy_of_first_answer": 2.0078708678483963, "lex_sim": 1.0}
{"question": "what is a buckeye the mascot for ohio state university", "answer": ["I don't know your username!"], "scores": [[0.003267330350354314, 0.19593548774719238, 0.5099973082542419, 0.9993138313293457, 0.8117349147796631, 0.0012394975638017058, 0.01677524670958519, 0.008699637837707996, 0.022239385172724724, 0.017165103927254677, 0.3035915493965149, 0.7390106916427612, 0.2383192777633667]], "normalized_score": [0.07004959881305695], "tokens": [["\u2581I", "\u2581don", "'", "t", "\u2581know", "\u2581your", "\u2581username", "!", "\u2581Please", "\u2581type", "\u2581it", "\u2581in", "."]], "entropy": [[4.670003890991211, 3.6646883487701416, 0.8379756808280945, 0.008704669773578644, 0.858460009098053, 1.4326462745666504, 3.5275015830993652, 2.1166131496429443, 1.229907512664795, 3.6551899909973145, 2.11684513092041, 1.297620177268982, 2.354279041290283]], "avg_entropy": [2.1361873430701404], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Anthropomorphic buckeye nut", "score_of_biggest_cluster": 0.07004959881305695, "score_of_first_answer": 0.07004959881305695, "max_score_of_first_answer": 0.9993138313293457, "min_score_of_first_answer": 0.0012394975638017058, "avg_score": 0.07004959881305695, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3398905995242509, "std_of_first_answer": 0.3398905995242509, "avg_entropy_of_first_answer": 2.1361873430701404, "lex_sim": 1.0}
{"question": "who is the sixth president of the united states", "answer": ["james monroe"], "scores": [[0.000379606761271134, 0.2538282871246338, 0.5299647450447083, 0.9975672960281372]], "normalized_score": [0.08448231220245361], "tokens": [["\u2581j", "ames", "\u2581mon", "roe"]], "entropy": [[1.2176796197891235, 0.6509931683540344, 0.9743748307228088, 0.018851514905691147]], "avg_entropy": [0.7154747834429145], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Quincy Adams", "score_of_biggest_cluster": 0.08448231220245361, "score_of_first_answer": 0.08448231220245361, "max_score_of_first_answer": 0.9975672960281372, "min_score_of_first_answer": 0.000379606761271134, "avg_score": 0.08448231220245361, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3697238109671892, "std_of_first_answer": 0.3697238109671892, "avg_entropy_of_first_answer": 0.7154747834429145, "lex_sim": 1.0}
{"question": "roman god of the underworld also called orcus or pluto", "answer": ["Pluto Amazon\u2019s new Alexa-enabled smart plug has a built-in camera and mic Amazon\u2019s smart plug with camera and mic is now available for pre-order."], "scores": [[0.30345508456230164, 0.9399641156196594, 1.90251476794856e-08, 0.002564737806096673, 0.9982373714447021, 0.007195994257926941, 0.03453364223241806, 0.9898355603218079, 0.12827695906162262, 0.25620824098587036, 0.05684676021337509, 0.11768884211778641, 0.029687173664569855, 0.2880263924598694, 0.06078018620610237, 0.9350016117095947, 0.9969654679298401, 0.03501317277550697, 0.18167021870613098, 0.11929158121347427, 0.01170872151851654, 0.23219504952430725, 0.9978694915771484, 0.0510023757815361, 0.6675212383270264, 0.04401002079248428, 0.07939261943101883, 0.6491120457649231, 0.5600970983505249, 0.27632465958595276, 0.19852353632450104, 0.6861178874969482, 0.456213116645813, 0.741176426410675, 0.5254926681518555, 0.9884117245674133, 0.11378862708806992, 0.0006213218439370394, 0.002986536594107747, 0.9942353367805481, 0.9996914863586426, 0.6658879518508911, 0.9133488535881042]], "normalized_score": [0.11393879354000092], "tokens": [["\u2581Pl", "uto", "\u2581Amazon", "\u2019", "s", "\u2581new", "\u2581Alex", "a", "-", "enabled", "\u2581smart", "\u2581plug", "\u2581has", "\u2581a", "\u2581built", "-", "in", "\u2581camera", "\u2581and", "\u2581mic", "\u2581Amazon", "\u2019", "s", "\u2581smart", "\u2581plug", "\u2581with", "\u2581camera", "\u2581and", "\u2581mic", "\u2581is", "\u2581now", "\u2581available", "\u2581for", "\u2581pre", "-", "order", ".", "\u2581|", "\u2581Cour", "tes", "y", "\u2581of", "\u2581Amazon"]], "entropy": [[1.696532130241394, 0.2528536915779114, 0.4293621778488159, 3.526115894317627, 0.02163585275411606, 4.112513542175293, 5.009395599365234, 0.10804829746484756, 3.9868383407592773, 1.5994055271148682, 2.8950345516204834, 1.8844162225723267, 4.094477653503418, 4.013787269592285, 5.517215251922607, 0.2665160894393921, 0.03215306997299194, 4.6079421043396, 2.9665884971618652, 3.000575065612793, 3.1032090187072754, 3.244804620742798, 0.024015814065933228, 2.5150368213653564, 1.5158867835998535, 3.8040175437927246, 2.2388105392456055, 2.1165056228637695, 1.6101353168487549, 4.068224906921387, 4.180732727050781, 1.553161859512329, 2.1502299308776855, 1.2597672939300537, 0.7510315179824829, 0.08433186262845993, 3.6679978370666504, 3.9508018493652344, 5.0159101486206055, 0.05102762579917908, 0.003735259175300598, 1.2428123950958252, 0.7705355882644653]], "avg_entropy": [2.30102627239255], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dis Pater", "score_of_biggest_cluster": 0.11393879354000092, "score_of_first_answer": 0.11393879354000092, "max_score_of_first_answer": 0.9996914863586426, "min_score_of_first_answer": 1.90251476794856e-08, "avg_score": 0.11393879354000092, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37920606478066676, "std_of_first_answer": 0.37920606478066676, "avg_entropy_of_first_answer": 2.30102627239255, "lex_sim": 1.0}
{"question": "who's playing in the world series and when is it", "answer": ["Houston Astros v. Washington Nationals, October 29-November 1"], "scores": [[0.05823346599936485, 0.9433098435401917, 0.9975415468215942, 0.036958593875169754, 0.4960610270500183, 0.30691784620285034, 0.954391598701477, 0.9962769150733948, 0.2108447402715683, 0.22003668546676636, 0.9844132661819458, 0.9412195682525635, 0.013685924932360649, 0.11179950833320618, 0.8787822723388672, 0.971863865852356, 0.9960830211639404, 0.4683893620967865]], "normalized_score": [0.35415884852409363], "tokens": [["\u2581Houston", "\u2581Ast", "ros", "\u2581v", ".", "\u2581Washington", "\u2581National", "s", ",", "\u2581October", "\u2581", "2", "9", "-", "Nov", "ember", "\u2581", "1"]], "entropy": [[3.1424286365509033, 0.35888800024986267, 0.02066565304994583, 2.2153072357177734, 2.2180333137512207, 2.0616133213043213, 0.31084656715393066, 0.034489475190639496, 2.670137405395508, 3.563819408416748, 0.11867508292198181, 0.2915858328342438, 1.5171505212783813, 1.5158828496932983, 0.5950919389724731, 0.16332276165485382, 0.03632160276174545, 1.3148448467254639]], "avg_entropy": [1.2305058029790719], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Houston Astros", "score_of_biggest_cluster": 0.35415884852409363, "score_of_first_answer": 0.35415884852409363, "max_score_of_first_answer": 0.9975415468215942, "min_score_of_first_answer": 0.013685924932360649, "avg_score": 0.35415884852409363, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39409244424375256, "std_of_first_answer": 0.39409244424375256, "avg_entropy_of_first_answer": 1.2305058029790719, "lex_sim": 1.0}
{"question": "what was generally the preferred way for european powers to control colonial regions", "answer": ["colonialism"], "scores": [[0.015338419936597347, 0.7529727220535278]], "normalized_score": [0.10746819525957108], "tokens": [["\u2581colonial", "ism"]], "entropy": [[5.760743141174316, 1.7149560451507568]], "avg_entropy": [3.7378495931625366], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Indirect rule", "score_of_biggest_cluster": 0.10746819525957108, "score_of_first_answer": 0.10746819525957108, "max_score_of_first_answer": 0.7529727220535278, "min_score_of_first_answer": 0.015338419936597347, "avg_score": 0.10746819525957108, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36881715105846524, "std_of_first_answer": 0.36881715105846524, "avg_entropy_of_first_answer": 3.7378495931625366, "lex_sim": 1.0}
{"question": "when did the movie the post begin filming", "answer": ["15 July 2017"], "scores": [[0.18574096262454987, 0.11403743922710419, 0.05760600417852402, 0.04452139139175415, 0.9104936122894287, 0.9798312783241272, 0.9993451237678528, 0.9711636900901794, 0.6447646617889404]], "normalized_score": [0.31476104259490967], "tokens": [["\u2581", "1", "5", "\u2581July", "\u2581", "2", "0", "1", "7"]], "entropy": [[2.954981803894043, 0.8076145052909851, 2.2784433364868164, 2.977511167526245, 0.4109792113304138, 0.10109776258468628, 0.008074277080595493, 0.1333431601524353, 0.9083387851715088]], "avg_entropy": [1.1755982232797477], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "May 2017", "score_of_biggest_cluster": 0.31476104259490967, "score_of_first_answer": 0.31476104259490967, "max_score_of_first_answer": 0.9993451237678528, "min_score_of_first_answer": 0.04452139139175415, "avg_score": 0.31476104259490967, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4114322711941441, "std_of_first_answer": 0.4114322711941441, "avg_entropy_of_first_answer": 1.1755982232797477, "lex_sim": 1.0}
{"question": "how many states have a cracker barrel restaurant", "answer": ["4 in South Carolina"], "scores": [[0.6394898295402527, 0.07484373450279236, 0.001309413812123239, 0.005539000499993563, 0.8842612504959106]], "normalized_score": [0.04982137307524681], "tokens": [["\u2581", "4", "\u2581in", "\u2581South", "\u2581Carolina"]], "entropy": [[2.284188747406006, 1.4967867136001587, 2.4809465408325195, 4.405301094055176, 0.5636732578277588]], "avg_entropy": [2.2461792707443236], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "43", "score_of_biggest_cluster": 0.04982137307524681, "score_of_first_answer": 0.04982137307524681, "max_score_of_first_answer": 0.8842612504959106, "min_score_of_first_answer": 0.001309413812123239, "avg_score": 0.04982137307524681, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36905543992377526, "std_of_first_answer": 0.36905543992377526, "avg_entropy_of_first_answer": 2.2461792707443236, "lex_sim": 1.0}
{"question": "when are the opening ceremonies for the 2018 olympic games", "answer": ["August 9, 2018"], "scores": [[0.09737170487642288, 0.9687151312828064, 0.1194823682308197, 0.17362484335899353, 0.9823607206344604, 0.9885239005088806, 0.999479353427887, 0.9944743514060974, 0.7108659148216248]], "normalized_score": [0.4796055853366852], "tokens": [["\u2581August", "\u2581", "9", ",", "\u2581", "2", "0", "1", "8"]], "entropy": [[2.3722403049468994, 0.17849068343639374, 1.5405598878860474, 1.1540851593017578, 0.10948197543621063, 0.08055691421031952, 0.00590574461966753, 0.0415201336145401, 0.786697506904602]], "avg_entropy": [0.696615367817382], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "9\u00a0February\u00a02018", "score_of_biggest_cluster": 0.4796055853366852, "score_of_first_answer": 0.4796055853366852, "max_score_of_first_answer": 0.999479353427887, "min_score_of_first_answer": 0.09737170487642288, "avg_score": 0.4796055853366852, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3917377459160646, "std_of_first_answer": 0.3917377459160646, "avg_entropy_of_first_answer": 0.696615367817382, "lex_sim": 1.0}
{"question": "when did day light savings start in the us", "answer": ["March 8 to November 1"], "scores": [[0.3366082012653351, 0.82504802942276, 0.4434788227081299, 0.001443694462068379, 0.7360635995864868, 0.9892240166664124, 0.9247428774833679]], "normalized_score": [0.2752583622932434], "tokens": [["\u2581March", "\u2581", "8", "\u2581to", "\u2581November", "\u2581", "1"]], "entropy": [[3.0181822776794434, 0.6591413021087646, 0.9629092812538147, 1.1482421159744263, 1.1275287866592407, 0.07633804529905319, 0.3486330211162567]], "avg_entropy": [1.0487106900129999], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "March 31, 1918", "score_of_biggest_cluster": 0.2752583622932434, "score_of_first_answer": 0.2752583622932434, "max_score_of_first_answer": 0.9892240166664124, "min_score_of_first_answer": 0.001443694462068379, "avg_score": 0.2752583622932434, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33333791496999104, "std_of_first_answer": 0.33333791496999104, "avg_entropy_of_first_answer": 1.0487106900129999, "lex_sim": 1.0}
{"question": "who won season 2 of real chance of love", "answer": ["Megan"], "scores": [[0.007072580046951771, 0.8896424770355225]], "normalized_score": [0.07932254672050476], "tokens": [["\u2581Meg", "an"]], "entropy": [[6.078830718994141, 0.5068109035491943]], "avg_entropy": [3.2928208112716675], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jackie DaFonseca", "score_of_biggest_cluster": 0.07932254672050476, "score_of_first_answer": 0.07932254672050476, "max_score_of_first_answer": 0.8896424770355225, "min_score_of_first_answer": 0.007072580046951771, "avg_score": 0.07932254672050476, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44128494849428535, "std_of_first_answer": 0.44128494849428535, "avg_entropy_of_first_answer": 3.2928208112716675, "lex_sim": 1.0}
{"question": "name three large lakes other than the great lakes in the united states", "answer": ["Lake Superior, Lake Huron, Lake Michigan, Lake Ontario, Lake Champlain"], "scores": [[0.2171657681465149, 0.06275561451911926, 0.9988916516304016, 0.8846248388290405, 0.8763728737831116, 0.09215086698532104, 0.9992272853851318, 0.9070612788200378, 0.7725450396537781, 0.7051006555557251, 0.45567452907562256, 0.928388237953186, 0.11909730732440948, 0.6325212121009827, 0.8897609114646912, 0.043467000126838684, 0.9979289770126343]], "normalized_score": [0.4301348924636841], "tokens": [["\u2581Lake", "\u2581Super", "ior", ",", "\u2581Lake", "\u2581Hur", "on", ",", "\u2581Lake", "\u2581Michigan", ",", "\u2581Lake", "\u2581Ontario", ",", "\u2581Lake", "\u2581Champ", "lain"]], "entropy": [[4.683384418487549, 3.3700101375579834, 0.011640099808573723, 0.6014828681945801, 0.9314671754837036, 2.098508834838867, 0.009169714525341988, 0.39180082082748413, 0.7462060451507568, 1.2372143268585205, 0.9235482215881348, 0.5128539800643921, 1.4738082885742188, 0.8365141153335571, 0.8096888661384583, 0.9911447167396545, 0.018511895090341568]], "avg_entropy": [1.1557032073683597], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Iliamna Lake", "score_of_biggest_cluster": 0.4301348924636841, "score_of_first_answer": 0.4301348924636841, "max_score_of_first_answer": 0.9992272853851318, "min_score_of_first_answer": 0.043467000126838684, "avg_score": 0.4301348924636841, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3604338793976783, "std_of_first_answer": 0.3604338793976783, "avg_entropy_of_first_answer": 1.1557032073683597, "lex_sim": 1.0}
{"question": "where is the hotel used in the movie the shining", "answer": ["Overlook Hotel"], "scores": [[0.1266818642616272, 0.9116620421409607, 0.8676722645759583]], "normalized_score": [0.46448105573654175], "tokens": [["\u2581Over", "look", "\u2581Hotel"]], "entropy": [[3.133188486099243, 0.5554813146591187, 0.6896883249282837]], "avg_entropy": [1.459452708562215], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Stanley Hotel", "score_of_biggest_cluster": 0.46448105573654175, "score_of_first_answer": 0.46448105573654175, "max_score_of_first_answer": 0.9116620421409607, "min_score_of_first_answer": 0.1266818642616272, "avg_score": 0.46448105573654175, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3601227805364413, "std_of_first_answer": 0.3601227805364413, "avg_entropy_of_first_answer": 1.459452708562215, "lex_sim": 1.0}
{"question": "who is the head of parliament in uk", "answer": ["Speaker of the House of Commons"], "scores": [[0.06964754313230515, 0.9964261651039124, 0.8562345504760742, 0.9789265990257263, 0.9450927376747131, 0.6749768853187561, 0.9801157712936401]], "normalized_score": [0.6228602528572083], "tokens": [["\u2581Spe", "aker", "\u2581of", "\u2581the", "\u2581House", "\u2581of", "\u2581Commons"]], "entropy": [[3.371030807495117, 0.02644648402929306, 0.6220455169677734, 0.14181911945343018, 0.35048234462738037, 0.9140830039978027, 0.14104434847831726]], "avg_entropy": [0.7952788035784449], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Lord Fowler", "score_of_biggest_cluster": 0.6228602528572083, "score_of_first_answer": 0.6228602528572083, "max_score_of_first_answer": 0.9964261651039124, "min_score_of_first_answer": 0.06964754313230515, "avg_score": 0.6228602528572083, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31050905069321894, "std_of_first_answer": 0.31050905069321894, "avg_entropy_of_first_answer": 0.7952788035784449, "lex_sim": 1.0}
{"question": "who wrote knock knock knocking on heavens door", "answer": ["Bob Dylan"], "scores": [[0.8385053873062134, 0.9380510449409485, 0.9997250437736511, 0.9996510744094849]], "normalized_score": [0.941597580909729], "tokens": [["\u2581Bob", "\u2581D", "yl", "an"]], "entropy": [[1.1148550510406494, 0.4770289659500122, 0.0035896955523639917, 0.0038188863545656204]], "avg_entropy": [0.3998231497243978], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bob Dylan", "score_of_biggest_cluster": 0.941597580909729, "score_of_first_answer": 0.941597580909729, "max_score_of_first_answer": 0.9997250437736511, "min_score_of_first_answer": 0.8385053873062134, "avg_score": 0.941597580909729, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06589162470081251, "std_of_first_answer": 0.06589162470081251, "avg_entropy_of_first_answer": 0.3998231497243978, "lex_sim": 1.0}
{"question": "who is the winner of bigg boss kannada season", "answer": ["Vishal Raghavendra"], "scores": [[0.021755972877144814, 0.17987331748008728, 0.590305745601654, 0.009403141215443611, 0.06964462250471115, 0.49421390891075134, 0.15098625421524048]], "normalized_score": [0.10174644738435745], "tokens": [["\u2581V", "ish", "al", "\u2581R", "ag", "hav", "endra"]], "entropy": [[4.70896053314209, 2.8539085388183594, 1.2649753093719482, 2.370513916015625, 2.9248790740966797, 1.5980806350708008, 1.482915997505188]], "avg_entropy": [2.4577477148600986], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chandan Shetty", "score_of_biggest_cluster": 0.10174644738435745, "score_of_first_answer": 0.10174644738435745, "max_score_of_first_answer": 0.590305745601654, "min_score_of_first_answer": 0.009403141215443611, "avg_score": 0.10174644738435745, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21542983467947324, "std_of_first_answer": 0.21542983467947324, "avg_entropy_of_first_answer": 2.4577477148600986, "lex_sim": 1.0}
{"question": "when did the song the joker come out", "answer": ["14 Septmeber 2011"], "scores": [[0.7237403392791748, 0.7429282069206238, 0.0006539298337884247, 0.0018084971234202385, 0.007150896824896336, 0.9992040991783142, 0.8299973607063293, 0.535854697227478, 0.9957082271575928, 0.6875864863395691, 0.044476430863142014]], "normalized_score": [0.1179606169462204], "tokens": [["\u2581", "1", "4", "\u2581Sept", "me", "ber", "\u2581", "2", "0", "1", "1"]], "entropy": [[1.6862858533859253, 0.6528005599975586, 0.09718804061412811, 2.8224072456359863, 1.2352811098098755, 0.009334596805274487, 0.5950465798377991, 0.76506108045578, 0.040027666836977005, 0.6294140815734863, 1.4710595607757568]], "avg_entropy": [0.9094460341571406], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1973", "score_of_biggest_cluster": 0.1179606169462204, "score_of_first_answer": 0.1179606169462204, "max_score_of_first_answer": 0.9992040991783142, "min_score_of_first_answer": 0.0006539298337884247, "avg_score": 0.1179606169462204, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39276256793593745, "std_of_first_answer": 0.39276256793593745, "avg_entropy_of_first_answer": 0.9094460341571406, "lex_sim": 1.0}
{"question": "who was the first president of the constituent assembly", "answer": ["Abhinandan Bhave"], "scores": [[0.0008071570191532373, 0.515727162361145, 0.22557328641414642, 0.9855255484580994, 0.007982837036252022, 0.09724348783493042]], "normalized_score": [0.06447508931159973], "tokens": [["\u2581Ab", "hin", "and", "an", "\u2581B", "have"]], "entropy": [[3.192906141281128, 2.1874194145202637, 1.8170368671417236, 0.09550231695175171, 3.6129348278045654, 1.884747862815857]], "avg_entropy": [2.1317579050858817], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dr. Rajendra Prasad", "score_of_biggest_cluster": 0.06447508931159973, "score_of_first_answer": 0.06447508931159973, "max_score_of_first_answer": 0.9855255484580994, "min_score_of_first_answer": 0.0008071570191532373, "avg_score": 0.06447508931159973, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35065259357486617, "std_of_first_answer": 0.35065259357486617, "avg_entropy_of_first_answer": 2.1317579050858817, "lex_sim": 1.0}
{"question": "where is the inscription on the statue of liberty", "answer": ["Liberty Island"], "scores": [[0.029035432264208794, 0.9865423440933228, 0.011717683635652065]], "normalized_score": [0.06949633359909058], "tokens": [["\u2581Liber", "ty", "\u2581Island"]], "entropy": [[4.34100341796875, 0.09984425455331802, 0.8772032260894775]], "avg_entropy": [1.7726836328705151], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The New Colossus", "score_of_biggest_cluster": 0.06949633359909058, "score_of_first_answer": 0.06949633359909058, "max_score_of_first_answer": 0.9865423440933228, "min_score_of_first_answer": 0.011717683635652065, "avg_score": 0.06949633359909058, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.455509788802551, "std_of_first_answer": 0.455509788802551, "avg_entropy_of_first_answer": 1.7726836328705151, "lex_sim": 1.0}
{"question": "panic at the disco la devotee video cast", "answer": ["Jared Leto"], "scores": [[0.024389320984482765, 0.5897664427757263, 0.11763839423656464, 0.998101532459259]], "normalized_score": [0.2027219980955124], "tokens": [["\u2581J", "ared", "\u2581Let", "o"]], "entropy": [[6.138157844543457, 1.6430974006652832, 3.093132495880127, 0.02002703584730625]], "avg_entropy": [2.7236036942340434], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Noah Schnapp", "score_of_biggest_cluster": 0.2027219980955124, "score_of_first_answer": 0.2027219980955124, "max_score_of_first_answer": 0.998101532459259, "min_score_of_first_answer": 0.024389320984482765, "avg_score": 0.2027219980955124, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39061809415780663, "std_of_first_answer": 0.39061809415780663, "avg_entropy_of_first_answer": 2.7236036942340434, "lex_sim": 1.0}
{"question": "what is money as a medium of exchange", "answer": ["A means of exchange to promote or facilitate trade"], "scores": [[0.05587925761938095, 0.05749577656388283, 0.8366616368293762, 0.36418041586875916, 0.009412999264895916, 0.03435461223125458, 0.006381068844348192, 0.7380669713020325, 0.9985966086387634, 0.22790679335594177]], "normalized_score": [0.11299528181552887], "tokens": [["\u2581A", "\u2581means", "\u2581of", "\u2581exchange", "\u2581to", "\u2581promote", "\u2581or", "\u2581facil", "itate", "\u2581trade"]], "entropy": [[5.2001800537109375, 4.466204643249512, 0.8165358901023865, 2.0565152168273926, 2.5852861404418945, 3.2040984630584717, 3.497377872467041, 1.793633222579956, 0.013144515454769135, 2.4693965911865234]], "avg_entropy": [2.6102372609078883], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "fiat money", "score_of_biggest_cluster": 0.11299528181552887, "score_of_first_answer": 0.11299528181552887, "max_score_of_first_answer": 0.9985966086387634, "min_score_of_first_answer": 0.006381068844348192, "avg_score": 0.11299528181552887, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36415187108951114, "std_of_first_answer": 0.36415187108951114, "avg_entropy_of_first_answer": 2.6102372609078883, "lex_sim": 1.0}
{"question": "what are the active materials of a lead acid battery", "answer": ["Electrolyte Plates"], "scores": [[0.013124371878802776, 0.9949386119842529, 0.523898720741272, 0.718108057975769, 0.0014781025238335133, 0.520433783531189]], "normalized_score": [0.1248045265674591], "tokens": [["\u2581Elect", "ro", "ly", "te", "\u2581Pl", "ates"]], "entropy": [[3.6873855590820312, 0.045619070529937744, 1.0790767669677734, 0.6985277533531189, 2.5624804496765137, 1.3040308952331543]], "avg_entropy": [1.5628534158070881], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lead", "score_of_biggest_cluster": 0.1248045265674591, "score_of_first_answer": 0.1248045265674591, "max_score_of_first_answer": 0.9949386119842529, "min_score_of_first_answer": 0.0014781025238335133, "avg_score": 0.1248045265674591, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35833657542202724, "std_of_first_answer": 0.35833657542202724, "avg_entropy_of_first_answer": 1.5628534158070881, "lex_sim": 1.0}
{"question": "when did the first pair of yeezys come out", "answer": ["August 2015"], "scores": [[0.029334578663110733, 0.9331347346305847, 0.48524561524391174, 0.6129885911941528, 0.8057518005371094, 0.7427315711975098]], "normalized_score": [0.41174519062042236], "tokens": [["\u2581August", "\u2581", "2", "0", "1", "5"]], "entropy": [[2.4075827598571777, 0.3285643458366394, 1.4303638935089111, 1.51885986328125, 0.6237029433250427, 0.9471926093101501]], "avg_entropy": [1.2093777358531952], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "February 14, 2015", "score_of_biggest_cluster": 0.41174519062042236, "score_of_first_answer": 0.41174519062042236, "max_score_of_first_answer": 0.9331347346305847, "min_score_of_first_answer": 0.029334578663110733, "avg_score": 0.41174519062042236, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29230893622841525, "std_of_first_answer": 0.29230893622841525, "avg_entropy_of_first_answer": 1.2093777358531952, "lex_sim": 1.0}
{"question": "what is the scientific name for the calf muscle", "answer": ["gastrocnemius"], "scores": [[0.30301734805107117, 0.9671505093574524, 0.999666690826416, 0.9952186942100525, 0.988955020904541]], "normalized_score": [0.7797984480857849], "tokens": [["\u2581g", "ast", "roc", "nem", "ius"]], "entropy": [[3.1224255561828613, 0.20715126395225525, 0.004037173464894295, 0.04507466405630112, 0.0884932279586792]], "avg_entropy": [0.6934363771229982], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "gastrocnemius", "score_of_biggest_cluster": 0.7797984480857849, "score_of_first_answer": 0.7797984480857849, "max_score_of_first_answer": 0.999666690826416, "min_score_of_first_answer": 0.30301734805107117, "avg_score": 0.7797984480857849, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2741197313096458, "std_of_first_answer": 0.2741197313096458, "avg_entropy_of_first_answer": 0.6934363771229982, "lex_sim": 1.0}
{"question": "how many episodes in season 2 of fargo", "answer": ["6"], "scores": [[0.8086373209953308, 0.007203221321105957]], "normalized_score": [0.0763203352689743], "tokens": [["\u2581", "6"]], "entropy": [[1.12705659866333, 0.7126315832138062]], "avg_entropy": [0.9198440909385681], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "10", "score_of_biggest_cluster": 0.0763203352689743, "score_of_first_answer": 0.0763203352689743, "max_score_of_first_answer": 0.8086373209953308, "min_score_of_first_answer": 0.007203221321105957, "avg_score": 0.0763203352689743, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4007170498371124, "std_of_first_answer": 0.4007170498371124, "avg_entropy_of_first_answer": 0.9198440909385681, "lex_sim": 1.0}
{"question": "the king of kalinga in the mauryan period was", "answer": ["Ashoka"], "scores": [[0.5782719850540161, 0.9561522603034973]], "normalized_score": [0.7435832619667053], "tokens": [["\u2581Ash", "oka"]], "entropy": [[2.3163461685180664, 0.2584485709667206]], "avg_entropy": [1.2873973697423935], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ashoka", "score_of_biggest_cluster": 0.7435832619667053, "score_of_first_answer": 0.7435832619667053, "max_score_of_first_answer": 0.9561522603034973, "min_score_of_first_answer": 0.5782719850540161, "avg_score": 0.7435832619667053, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1889401376247406, "std_of_first_answer": 0.1889401376247406, "avg_entropy_of_first_answer": 1.2873973697423935, "lex_sim": 1.0}
{"question": "if there is a line that lies parallel to the horizon it is said to be", "answer": ["perpendicular"], "scores": [[0.16924938559532166, 0.9943219423294067, 0.9963395595550537]], "normalized_score": [0.5514258742332458], "tokens": [["\u2581per", "pend", "icular"]], "entropy": [[4.810834884643555, 0.048093970865011215, 0.03785888850688934]], "avg_entropy": [1.632262581338485], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A vanishing point", "score_of_biggest_cluster": 0.5514258742332458, "score_of_first_answer": 0.5514258742332458, "max_score_of_first_answer": 0.9963395595550537, "min_score_of_first_answer": 0.16924938559532166, "avg_score": 0.5514258742332458, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3894193612883059, "std_of_first_answer": 0.3894193612883059, "avg_entropy_of_first_answer": 1.632262581338485, "lex_sim": 1.0}
{"question": "branch of science that deals with the structure of human body parts", "answer": ["Anatomy"], "scores": [[0.5188218951225281, 0.9947690367698669, 0.98829585313797]], "normalized_score": [0.7989921569824219], "tokens": [["\u2581An", "atom", "y"]], "entropy": [[2.086275815963745, 0.03954526409506798, 0.08268126845359802]], "avg_entropy": [0.736167449504137], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Anatomy", "score_of_biggest_cluster": 0.7989921569824219, "score_of_first_answer": 0.7989921569824219, "max_score_of_first_answer": 0.9947690367698669, "min_score_of_first_answer": 0.5188218951225281, "avg_score": 0.7989921569824219, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.222853559541609, "std_of_first_answer": 0.222853559541609, "avg_entropy_of_first_answer": 0.736167449504137, "lex_sim": 1.0}
{"question": "who is the new york state senate majority leader", "answer": ["Andrea Stewart-Cousins"], "scores": [[0.07882343232631683, 0.8972182273864746, 0.8764693737030029, 0.9944561123847961, 0.9976702332496643, 0.9967905879020691]], "normalized_score": [0.6279298663139343], "tokens": [["\u2581Andrea", "\u2581Stewart", "-", "C", "ous", "ins"]], "entropy": [[1.8990272283554077, 0.7833706736564636, 0.44789397716522217, 0.0466141402721405, 0.021248752251267433, 0.029429316520690918]], "avg_entropy": [0.5379306813701987], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John J. Flanagan", "score_of_biggest_cluster": 0.6279298663139343, "score_of_first_answer": 0.6279298663139343, "max_score_of_first_answer": 0.9976702332496643, "min_score_of_first_answer": 0.07882343232631683, "avg_score": 0.6279298663139343, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32932298755214434, "std_of_first_answer": 0.32932298755214434, "avg_entropy_of_first_answer": 0.5379306813701987, "lex_sim": 1.0}
{"question": "where was uncle tom's cabin first published", "answer": ["1852"], "scores": [[0.029305119067430496, 0.9664888381958008, 0.921064019203186, 0.964642345905304, 0.8620595932006836]], "normalized_score": [0.4648006856441498], "tokens": [["\u2581", "1", "8", "5", "2"]], "entropy": [[4.530033111572266, 0.21742165088653564, 0.4706341326236725, 0.22736579179763794, 0.5281227827072144]], "avg_entropy": [1.1947154939174651], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The National Era", "score_of_biggest_cluster": 0.4648006856441498, "score_of_first_answer": 0.4648006856441498, "max_score_of_first_answer": 0.9664888381958008, "min_score_of_first_answer": 0.029305119067430496, "avg_score": 0.4648006856441498, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36170480264388916, "std_of_first_answer": 0.36170480264388916, "avg_entropy_of_first_answer": 1.1947154939174651, "lex_sim": 1.0}
{"question": "who was elected senator of illinois in 1858", "answer": ["Abraham Lincoln"], "scores": [[0.7212968468666077, 0.9897412061691284]], "normalized_score": [0.8449243903160095], "tokens": [["\u2581Abraham", "\u2581Lincoln"]], "entropy": [[1.0746262073516846, 0.09850554168224335]], "avg_entropy": [0.586565874516964], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stephen A. Douglas", "score_of_biggest_cluster": 0.8449243903160095, "score_of_first_answer": 0.8449243903160095, "max_score_of_first_answer": 0.9897412061691284, "min_score_of_first_answer": 0.7212968468666077, "avg_score": 0.8449243903160095, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13422217965126038, "std_of_first_answer": 0.13422217965126038, "avg_entropy_of_first_answer": 0.586565874516964, "lex_sim": 1.0}
{"question": "who was the inventor who developed a way to record sound", "answer": ["Thomas Alva Edison"], "scores": [[0.8397448062896729, 0.37138792872428894, 0.9877755641937256, 0.9974579215049744, 0.9967659711837769]], "normalized_score": [0.7892675399780273], "tokens": [["\u2581Thomas", "\u2581Al", "va", "\u2581Ed", "ison"]], "entropy": [[1.0638883113861084, 1.226264238357544, 0.09633475542068481, 0.0236081350594759, 0.02607811987400055]], "avg_entropy": [0.4872347120195627], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Thomas Edison", "score_of_biggest_cluster": 0.7892675399780273, "score_of_first_answer": 0.7892675399780273, "max_score_of_first_answer": 0.9974579215049744, "min_score_of_first_answer": 0.37138792872428894, "avg_score": 0.7892675399780273, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.24116139378024243, "std_of_first_answer": 0.24116139378024243, "avg_entropy_of_first_answer": 0.4872347120195627, "lex_sim": 1.0}
{"question": "what was the first video game for nintendo", "answer": ["donkey kong"], "scores": [[0.006210367660969496, 0.9976373910903931, 0.9265879392623901, 0.9996259212493896]], "normalized_score": [0.2752349078655243], "tokens": [["\u2581don", "key", "\u2581k", "ong"]], "entropy": [[3.0047695636749268, 0.02336270734667778, 0.4205741882324219, 0.00448201410472393]], "avg_entropy": [0.8632971183396876], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Magnavox Odyssey", "score_of_biggest_cluster": 0.2752349078655243, "score_of_first_answer": 0.2752349078655243, "max_score_of_first_answer": 0.9996259212493896, "min_score_of_first_answer": 0.006210367660969496, "avg_score": 0.2752349078655243, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42036319154351726, "std_of_first_answer": 0.42036319154351726, "avg_entropy_of_first_answer": 0.8632971183396876, "lex_sim": 1.0}
{"question": "when does brooklyn nine nine season 5 episode 12", "answer": ["January 14, 2020"], "scores": [[0.07817734032869339, 0.9691339135169983, 0.5199940800666809, 0.04532278701663017, 0.3777938485145569, 0.9892112016677856, 0.9961718916893005, 0.9992117881774902, 0.5225347876548767, 0.9609991908073425]], "normalized_score": [0.44907134771347046], "tokens": [["\u2581January", "\u2581", "1", "4", ",", "\u2581", "2", "0", "2", "0"]], "entropy": [[3.306847095489502, 0.17933334410190582, 1.4035191535949707, 2.310655117034912, 1.058064341545105, 0.06981763243675232, 0.028190666809678078, 0.009431960061192513, 0.7209444642066956, 0.19604280591011047]], "avg_entropy": [0.9282846581190825], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "March\u00a018,\u00a02018", "score_of_biggest_cluster": 0.44907134771347046, "score_of_first_answer": 0.44907134771347046, "max_score_of_first_answer": 0.9992117881774902, "min_score_of_first_answer": 0.04532278701663017, "avg_score": 0.44907134771347046, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.368117377136366, "std_of_first_answer": 0.368117377136366, "avg_entropy_of_first_answer": 0.9282846581190825, "lex_sim": 1.0}
{"question": "when was the last time unc did not make the ncaa tournament", "answer": ["2012"], "scores": [[0.8375844955444336, 0.6111443042755127, 0.9953644275665283, 0.44906798005104065, 0.13646192848682404]], "normalized_score": [0.49991434812545776], "tokens": [["\u2581", "2", "0", "1", "2"]], "entropy": [[1.193131446838379, 0.7601727247238159, 0.0438227541744709, 0.7067462801933289, 2.134322166442871]], "avg_entropy": [0.9676390744745731], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2003", "score_of_biggest_cluster": 0.49991434812545776, "score_of_first_answer": 0.49991434812545776, "max_score_of_first_answer": 0.9953644275665283, "min_score_of_first_answer": 0.13646192848682404, "avg_score": 0.49991434812545776, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3001186868482287, "std_of_first_answer": 0.3001186868482287, "avg_entropy_of_first_answer": 0.9676390744745731, "lex_sim": 1.0}
{"question": "what is in a pat o brien hurricane", "answer": ["a tequila."], "scores": [[0.014180988073348999, 0.0037176215555518866, 0.9737478494644165, 0.9919448494911194, 0.0006916305865161121]], "normalized_score": [0.03231140971183777], "tokens": [["\u2581a", "\u2581te", "qu", "ila", "."]], "entropy": [[5.018375396728516, 4.733267307281494, 0.1555873155593872, 0.06173071265220642, 2.7279577255249023]], "avg_entropy": [2.5393836915493013], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "fruit juice", "score_of_biggest_cluster": 0.03231140971183777, "score_of_first_answer": 0.03231140971183777, "max_score_of_first_answer": 0.9919448494911194, "min_score_of_first_answer": 0.0006916305865161121, "avg_score": 0.03231140971183777, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.47851417987708317, "std_of_first_answer": 0.47851417987708317, "avg_entropy_of_first_answer": 2.5393836915493013, "lex_sim": 1.0}
{"question": "where was the first cell phone call made", "answer": ["Japan"], "scores": [[0.0035672972444444895]], "normalized_score": [0.0035672965459525585], "tokens": [["\u2581Japan"]], "entropy": [[4.8881707191467285]], "avg_entropy": [4.8881707191467285], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "April 3, 1973", "score_of_biggest_cluster": 0.0035672965459525585, "score_of_first_answer": 0.0035672965459525585, "max_score_of_first_answer": 0.0035672972444444895, "min_score_of_first_answer": 0.0035672972444444895, "avg_score": 0.0035672965459525585, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.8881707191467285, "lex_sim": 1.0}
{"question": "who has won the most games in nfl 2017", "answer": ["Ben Roethlisberger"], "scores": [[0.06682374328374863, 0.9680615663528442, 0.9983906745910645, 0.9928354620933533, 0.9873867630958557]], "normalized_score": [0.5758373141288757], "tokens": [["\u2581Ben", "\u2581Ro", "eth", "lis", "berger"]], "entropy": [[2.6702654361724854, 0.2557426989078522, 0.01613912172615528, 0.06457173824310303, 0.0921817421913147]], "avg_entropy": [0.6197801474481821], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dallas Cowboys", "score_of_biggest_cluster": 0.5758373141288757, "score_of_first_answer": 0.5758373141288757, "max_score_of_first_answer": 0.9983906745910645, "min_score_of_first_answer": 0.06682374328374863, "avg_score": 0.5758373141288757, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3680798411635527, "std_of_first_answer": 0.3680798411635527, "avg_entropy_of_first_answer": 0.6197801474481821, "lex_sim": 1.0}
{"question": "who wrote the song fly me to the moon", "answer": ["Bart Howard"], "scores": [[0.921309232711792, 0.9965619444847107]], "normalized_score": [0.9581971168518066], "tokens": [["\u2581Bart", "\u2581Howard"]], "entropy": [[0.6805615425109863, 0.03841467201709747]], "avg_entropy": [0.3594881072640419], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bart Howard", "score_of_biggest_cluster": 0.9581971168518066, "score_of_first_answer": 0.9581971168518066, "max_score_of_first_answer": 0.9965619444847107, "min_score_of_first_answer": 0.921309232711792, "avg_score": 0.9581971168518066, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03762635588645935, "std_of_first_answer": 0.03762635588645935, "avg_entropy_of_first_answer": 0.3594881072640419, "lex_sim": 1.0}
{"question": "who is the supreme court judge was a former chief justice of kerala high court", "answer": ["Kerala High Court"], "scores": [[0.0076648942194879055, 0.9938637018203735, 0.2968006730079651, 0.9704886674880981]], "normalized_score": [0.21643231809139252], "tokens": [["\u2581Ker", "ala", "\u2581High", "\u2581Court"]], "entropy": [[4.408609867095947, 0.05993754416704178, 3.402395248413086, 0.19101661443710327]], "avg_entropy": [2.0154898185282946], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Konakuppakatil Gopinathan Balakrishnan", "score_of_biggest_cluster": 0.21643231809139252, "score_of_first_answer": 0.21643231809139252, "max_score_of_first_answer": 0.9938637018203735, "min_score_of_first_answer": 0.0076648942194879055, "avg_score": 0.21643231809139252, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42745730615276506, "std_of_first_answer": 0.42745730615276506, "avg_entropy_of_first_answer": 2.0154898185282946, "lex_sim": 1.0}
{"question": "when does the heart develop and begin pumping blood", "answer": ["Week 4"], "scores": [[0.03949853032827377, 0.9328372478485107, 0.17345677316188812]], "normalized_score": [0.18557780981063843], "tokens": [["\u2581Week", "\u2581", "4"]], "entropy": [[3.6604721546173096, 0.5071138739585876, 2.0511622428894043]], "avg_entropy": [2.072916090488434], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "by week 4 of development", "score_of_biggest_cluster": 0.18557780981063843, "score_of_first_answer": 0.18557780981063843, "max_score_of_first_answer": 0.9328372478485107, "min_score_of_first_answer": 0.03949853032827377, "avg_score": 0.18557780981063843, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3933697130604083, "std_of_first_answer": 0.3933697130604083, "avg_entropy_of_first_answer": 2.072916090488434, "lex_sim": 1.0}
{"question": "who won in the brown vs board of education case", "answer": ["Brown"], "scores": [[0.25771233439445496]], "normalized_score": [0.25771233439445496], "tokens": [["\u2581Brown"]], "entropy": [[3.7544145584106445]], "avg_entropy": [3.7544145584106445], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "defendants", "score_of_biggest_cluster": 0.25771233439445496, "score_of_first_answer": 0.25771233439445496, "max_score_of_first_answer": 0.25771233439445496, "min_score_of_first_answer": 0.25771233439445496, "avg_score": 0.25771233439445496, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.7544145584106445, "lex_sim": 1.0}
{"question": "how many episodes are in season 7 of pretty little liars", "answer": ["20"], "scores": [[0.8987093567848206, 0.666140615940094, 0.5804153680801392]], "normalized_score": [0.7030313611030579], "tokens": [["\u2581", "2", "0"]], "entropy": [[0.8149951696395874, 0.9962513446807861, 1.4845114946365356]], "avg_entropy": [1.0985860029856365], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "20", "score_of_biggest_cluster": 0.7030313611030579, "score_of_first_answer": 0.7030313611030579, "max_score_of_first_answer": 0.8987093567848206, "min_score_of_first_answer": 0.5804153680801392, "avg_score": 0.7030313611030579, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.13447350031565, "std_of_first_answer": 0.13447350031565, "avg_entropy_of_first_answer": 1.0985860029856365, "lex_sim": 1.0}
{"question": "who did the steelers lose to in the playoffs last year", "answer": ["the Baltimore Ravens"], "scores": [[0.043662913143634796, 0.06066101789474487, 0.9293537139892578, 0.9997337460517883]], "normalized_score": [0.22272653877735138], "tokens": [["\u2581the", "\u2581Baltimore", "\u2581Rav", "ens"]], "entropy": [[3.0857415199279785, 4.321142196655273, 0.3561658263206482, 0.0033924055751413107]], "avg_entropy": [1.9416104871197604], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "New England Patriots", "score_of_biggest_cluster": 0.22272653877735138, "score_of_first_answer": 0.22272653877735138, "max_score_of_first_answer": 0.9997337460517883, "min_score_of_first_answer": 0.043662913143634796, "avg_score": 0.22272653877735138, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4569085319934649, "std_of_first_answer": 0.4569085319934649, "avg_entropy_of_first_answer": 1.9416104871197604, "lex_sim": 1.0}
{"question": "who sings far from the home i love in fiddler on the roof", "answer": ["Tevie Topol"], "scores": [[0.08796311914920807, 0.0003254724433645606, 0.02149740606546402, 0.9647834897041321]], "normalized_score": [0.0277592521160841], "tokens": [["\u2581Te", "vie", "\u2581Top", "ol"]], "entropy": [[3.8823506832122803, 0.10210537165403366, 3.278904914855957, 0.28172558546066284]], "avg_entropy": [1.8862716387957335], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hodel", "score_of_biggest_cluster": 0.0277592521160841, "score_of_first_answer": 0.0277592521160841, "max_score_of_first_answer": 0.9647834897041321, "min_score_of_first_answer": 0.0003254724433645606, "avg_score": 0.0277592521160841, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40321583579503273, "std_of_first_answer": 0.40321583579503273, "avg_entropy_of_first_answer": 1.8862716387957335, "lex_sim": 1.0}
{"question": "baga beach is in north or south goa", "answer": ["North"], "scores": [[0.6225235462188721]], "normalized_score": [0.6225235462188721], "tokens": [["\u2581North"]], "entropy": [[1.511993646621704]], "avg_entropy": [1.511993646621704], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "North Goa", "score_of_biggest_cluster": 0.6225235462188721, "score_of_first_answer": 0.6225235462188721, "max_score_of_first_answer": 0.6225235462188721, "min_score_of_first_answer": 0.6225235462188721, "avg_score": 0.6225235462188721, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.511993646621704, "lex_sim": 1.0}
{"question": "how long has tom brady been playing professional football", "answer": ["Since 2001"], "scores": [[0.08576729893684387, 0.9149500727653503, 0.9301513433456421, 0.9986490607261658, 0.998112678527832, 0.054559990763664246]], "normalized_score": [0.39791467785835266], "tokens": [["\u2581Since", "\u2581", "2", "0", "0", "1"]], "entropy": [[1.806974172592163, 0.5372684001922607, 0.2624970078468323, 0.015617545694112778, 0.016951603814959526, 0.29647356271743774]], "avg_entropy": [0.4892970488096277], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "18-season career", "score_of_biggest_cluster": 0.39791467785835266, "score_of_first_answer": 0.39791467785835266, "max_score_of_first_answer": 0.9986490607261658, "min_score_of_first_answer": 0.054559990763664246, "avg_score": 0.39791467785835266, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4209519743138112, "std_of_first_answer": 0.4209519743138112, "avg_entropy_of_first_answer": 0.4892970488096277, "lex_sim": 1.0}
{"question": "what kind of beast is the beast from beauty and the beast", "answer": ["A beast"], "scores": [[0.0794411301612854, 0.13556011021137238, 0.9965386390686035]], "normalized_score": [0.22057542204856873], "tokens": [["\u2581A", "\u2581be", "ast"]], "entropy": [[5.45291805267334, 5.038084983825684, 0.03798794001340866]], "avg_entropy": [3.509663658837477], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "a chimera", "score_of_biggest_cluster": 0.22057542204856873, "score_of_first_answer": 0.22057542204856873, "max_score_of_first_answer": 0.9965386390686035, "min_score_of_first_answer": 0.0794411301612854, "avg_score": 0.22057542204856873, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4197222901870861, "std_of_first_answer": 0.4197222901870861, "avg_entropy_of_first_answer": 3.509663658837477, "lex_sim": 1.0}
{"question": "who has won the most world series in baseball", "answer": ["The New York Yankees"], "scores": [[0.03998642414808273, 0.7117550373077393, 0.9967209696769714, 0.9674937129020691, 0.9988790154457092, 0.9996135830879211]], "normalized_score": [0.549079179763794], "tokens": [["\u2581The", "\u2581New", "\u2581York", "\u2581Yan", "ke", "es"]], "entropy": [[3.0886712074279785, 1.2450528144836426, 0.029154233634471893, 0.19240662455558777, 0.009952036663889885, 0.004087952896952629]], "avg_entropy": [0.7615541449437538], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the New York Yankees", "score_of_biggest_cluster": 0.549079179763794, "score_of_first_answer": 0.549079179763794, "max_score_of_first_answer": 0.9996135830879211, "min_score_of_first_answer": 0.03998642414808273, "avg_score": 0.549079179763794, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3488886407234914, "std_of_first_answer": 0.3488886407234914, "avg_entropy_of_first_answer": 0.7615541449437538, "lex_sim": 1.0}
{"question": "who won last year's grand national 2017", "answer": ["One of the Best"], "scores": [[0.7267882823944092, 0.1372864842414856, 0.6248922348022461, 0.015559795312583447]], "normalized_score": [0.17648637294769287], "tokens": [["\u2581One", "\u2581of", "\u2581the", "\u2581Best"]], "entropy": [[1.7597335577011108, 3.8397021293640137, 1.8841077089309692, 5.676453590393066]], "avg_entropy": [3.28999924659729], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "One For Arthur", "score_of_biggest_cluster": 0.17648637294769287, "score_of_first_answer": 0.17648637294769287, "max_score_of_first_answer": 0.7267882823944092, "min_score_of_first_answer": 0.015559795312583447, "avg_score": 0.17648637294769287, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3049184226937876, "std_of_first_answer": 0.3049184226937876, "avg_entropy_of_first_answer": 3.28999924659729, "lex_sim": 1.0}
{"question": "when did canada gain independence from great britain", "answer": ["75 years ago"], "scores": [[0.34078899025917053, 0.0017368674743920565, 0.0241107027977705, 0.2995956540107727, 0.4700063169002533]], "normalized_score": [0.07254716008901596], "tokens": [["\u2581", "7", "5", "\u2581years", "\u2581ago"]], "entropy": [[1.2935264110565186, 0.10241051763296127, 2.891256809234619, 2.7243995666503906, 1.9567739963531494]], "avg_entropy": [1.7936734601855278], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "April 17, 1982", "score_of_biggest_cluster": 0.07254716008901596, "score_of_first_answer": 0.07254716008901596, "max_score_of_first_answer": 0.4700063169002533, "min_score_of_first_answer": 0.0017368674743920565, "avg_score": 0.07254716008901596, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18394421564230676, "std_of_first_answer": 0.18394421564230676, "avg_entropy_of_first_answer": 1.7936734601855278, "lex_sim": 1.0}
{"question": "who sang the songs in the movie beyond the sea", "answer": ["Bobby Darin"], "scores": [[0.22568193078041077, 0.9530880451202393, 0.992699921131134]], "normalized_score": [0.5976991057395935], "tokens": [["\u2581Bobby", "\u2581Dar", "in"]], "entropy": [[4.117950439453125, 0.326201856136322, 0.06324981898069382]], "avg_entropy": [1.5024673715233803], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kevin Spacey", "score_of_biggest_cluster": 0.5976991057395935, "score_of_first_answer": 0.5976991057395935, "max_score_of_first_answer": 0.992699921131134, "min_score_of_first_answer": 0.22568193078041077, "avg_score": 0.5976991057395935, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35261016456984084, "std_of_first_answer": 0.35261016456984084, "avg_entropy_of_first_answer": 1.5024673715233803, "lex_sim": 1.0}
{"question": "where was the summer olympics held in 2012", "answer": ["London"], "scores": [[0.9173596501350403]], "normalized_score": [0.9173596501350403], "tokens": [["\u2581London"]], "entropy": [[0.6584218144416809]], "avg_entropy": [0.6584218144416809], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "London", "score_of_biggest_cluster": 0.9173596501350403, "score_of_first_answer": 0.9173596501350403, "max_score_of_first_answer": 0.9173596501350403, "min_score_of_first_answer": 0.9173596501350403, "avg_score": 0.9173596501350403, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.6584218144416809, "lex_sim": 1.0}
{"question": "where did the book small steps take place", "answer": ["the moon"], "scores": [[0.05807895213365555, 0.8014177680015564]], "normalized_score": [0.21574406325817108], "tokens": [["\u2581the", "\u2581moon"]], "entropy": [[5.40113639831543, 1.6304209232330322]], "avg_entropy": [3.515778660774231], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Austin, Texas", "score_of_biggest_cluster": 0.21574406325817108, "score_of_first_answer": 0.21574406325817108, "max_score_of_first_answer": 0.8014177680015564, "min_score_of_first_answer": 0.05807895213365555, "avg_score": 0.21574406325817108, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3716694079339504, "std_of_first_answer": 0.3716694079339504, "avg_entropy_of_first_answer": 3.515778660774231, "lex_sim": 1.0}
{"question": "when did the nfl start playing in london", "answer": ["2007"], "scores": [[0.7960761785507202, 0.9835577011108398, 0.9986762404441833, 0.9830113649368286, 0.9669468402862549]], "normalized_score": [0.9423843622207642], "tokens": [["\u2581", "2", "0", "0", "7"]], "entropy": [[1.262521743774414, 0.10058487206697464, 0.014242248609662056, 0.08830193430185318, 0.1894145905971527]], "avg_entropy": [0.33101307787001133], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "since 2007", "score_of_biggest_cluster": 0.9423843622207642, "score_of_first_answer": 0.9423843622207642, "max_score_of_first_answer": 0.9986762404441833, "min_score_of_first_answer": 0.7960761785507202, "avg_score": 0.9423843622207642, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.07545930157515605, "std_of_first_answer": 0.07545930157515605, "avg_entropy_of_first_answer": 0.33101307787001133, "lex_sim": 1.0}
{"question": "who is tinker air force base named after", "answer": ["Edgar Allan Poe"], "scores": [[0.009890628978610039, 0.8331718444824219, 0.30646228790283203, 0.9994798302650452, 0.6885987520217896, 0.997701108455658]], "normalized_score": [0.34661397337913513], "tokens": [["\u2581Ed", "gar", "\u2581All", "an", "\u2581Po", "e"]], "entropy": [[4.610638618469238, 0.5746838450431824, 3.9353485107421875, 0.005727527197450399, 1.5817440748214722, 0.023468870669603348]], "avg_entropy": [1.7886019078238558], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Major General Clarence L. Tinker", "score_of_biggest_cluster": 0.34661397337913513, "score_of_first_answer": 0.34661397337913513, "max_score_of_first_answer": 0.9994798302650452, "min_score_of_first_answer": 0.009890628978610039, "avg_score": 0.34661397337913513, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3663183000482845, "std_of_first_answer": 0.3663183000482845, "avg_entropy_of_first_answer": 1.7886019078238558, "lex_sim": 1.0}
{"question": "which us ship was captured off the coast of massachusetts by the british", "answer": ["Lexington"], "scores": [[0.03277185186743736, 0.9994518160820007]], "normalized_score": [0.1809803545475006], "tokens": [["\u2581Lex", "ington"]], "entropy": [[3.177764892578125, 0.006820934824645519]], "avg_entropy": [1.5922929137013853], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "USS Chesapeake", "score_of_biggest_cluster": 0.1809803545475006, "score_of_first_answer": 0.1809803545475006, "max_score_of_first_answer": 0.9994518160820007, "min_score_of_first_answer": 0.03277185186743736, "avg_score": 0.1809803545475006, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4833399821072817, "std_of_first_answer": 0.4833399821072817, "avg_entropy_of_first_answer": 1.5922929137013853, "lex_sim": 1.0}
{"question": "who is the current defence against the dark arts teacher", "answer": ["Severus Snape"], "scores": [[0.31987839937210083, 0.990894079208374, 0.8784048557281494, 0.9983024597167969, 0.9991497993469238]], "normalized_score": [0.7739624977111816], "tokens": [["\u2581Sever", "us", "\u2581S", "na", "pe"]], "entropy": [[3.6060986518859863, 0.08161237835884094, 0.9734208583831787, 0.01862534135580063, 0.010118847712874413]], "avg_entropy": [0.9379752155393362], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Amycus Carrow", "score_of_biggest_cluster": 0.7739624977111816, "score_of_first_answer": 0.7739624977111816, "max_score_of_first_answer": 0.9991497993469238, "min_score_of_first_answer": 0.31987839937210083, "avg_score": 0.7739624977111816, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2627253354182833, "std_of_first_answer": 0.2627253354182833, "avg_entropy_of_first_answer": 0.9379752155393362, "lex_sim": 1.0}
{"question": "rain sleet or snow that contains a high concentration of acids is called", "answer": ["acid rain"], "scores": [[0.5041497945785522, 0.6650310158729553]], "normalized_score": [0.5790295600891113], "tokens": [["\u2581acid", "\u2581rain"]], "entropy": [[2.6956541538238525, 1.4269869327545166]], "avg_entropy": [2.0613205432891846], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Acid rain", "score_of_biggest_cluster": 0.5790295600891113, "score_of_first_answer": 0.5790295600891113, "max_score_of_first_answer": 0.6650310158729553, "min_score_of_first_answer": 0.5041497945785522, "avg_score": 0.5790295600891113, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08044061064720154, "std_of_first_answer": 0.08044061064720154, "avg_entropy_of_first_answer": 2.0613205432891846, "lex_sim": 1.0}
{"question": "who translated the play neel darpan into english", "answer": ["Abhyuday Joshi also known as Abhaya Joshi"], "scores": [[0.008140227757394314, 0.01641530729830265, 0.5229150652885437, 0.21452856063842773, 0.042022522538900375, 0.9981405735015869, 8.539835835108534e-05, 0.7239878177642822, 0.9228437542915344, 0.18592575192451477, 0.39318230748176575, 0.038855165243148804, 0.6947848796844482, 0.9967976212501526]], "normalized_score": [0.1149783656001091], "tokens": [["\u2581Ab", "hy", "ud", "ay", "\u2581Jos", "hi", "\u2581also", "\u2581known", "\u2581as", "\u2581Ab", "h", "aya", "\u2581Jos", "hi"]], "entropy": [[5.415773391723633, 3.0548453330993652, 2.257924795150757, 0.9312499761581421, 4.46052360534668, 0.01766999252140522, 0.315981388092041, 1.7795370817184448, 0.369905948638916, 4.915799617767334, 1.6633697748184204, 1.454529881477356, 1.6522985696792603, 0.031046297401189804]], "avg_entropy": [2.022889689542353], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Michael Madhusudan Dutta", "score_of_biggest_cluster": 0.1149783656001091, "score_of_first_answer": 0.1149783656001091, "max_score_of_first_answer": 0.9981405735015869, "min_score_of_first_answer": 8.539835835108534e-05, "avg_score": 0.1149783656001091, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37720329657433865, "std_of_first_answer": 0.37720329657433865, "avg_entropy_of_first_answer": 2.022889689542353, "lex_sim": 1.0}
{"question": "what is the final season of downton abbey", "answer": ["series six"], "scores": [[0.025635099038481712, 0.09772761166095734]], "normalized_score": [0.05005254969000816], "tokens": [["\u2581series", "\u2581six"]], "entropy": [[3.0141777992248535, 0.9747592210769653]], "avg_entropy": [1.9944685101509094], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the sixth", "score_of_biggest_cluster": 0.05005254969000816, "score_of_first_answer": 0.05005254969000816, "max_score_of_first_answer": 0.09772761166095734, "min_score_of_first_answer": 0.025635099038481712, "avg_score": 0.05005254969000816, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03604625631123781, "std_of_first_answer": 0.03604625631123781, "avg_entropy_of_first_answer": 1.9944685101509094, "lex_sim": 1.0}
{"question": "when was the peak to peak gondola built", "answer": ["1996"], "scores": [[0.6618064045906067, 0.5890806317329407, 0.9848565459251404, 0.36571964621543884, 0.2911125421524048]], "normalized_score": [0.5275912284851074], "tokens": [["\u2581", "1", "9", "9", "6"]], "entropy": [[1.8963050842285156, 0.7384721636772156, 0.11874589323997498, 1.363407850265503, 2.0309412479400635]], "avg_entropy": [1.2295744478702546], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in 2007 and 2008", "score_of_biggest_cluster": 0.5275912284851074, "score_of_first_answer": 0.5275912284851074, "max_score_of_first_answer": 0.9848565459251404, "min_score_of_first_answer": 0.2911125421524048, "avg_score": 0.5275912284851074, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24496717248090275, "std_of_first_answer": 0.24496717248090275, "avg_entropy_of_first_answer": 1.2295744478702546, "lex_sim": 1.0}
{"question": "where does the synthesis of new dna from existing dna occurs", "answer": ["cell nucleus"], "scores": [[0.057662900537252426, 0.26037082076072693, 0.9949501752853394]], "normalized_score": [0.2462805062532425], "tokens": [["\u2581cell", "\u2581nucle", "us"]], "entropy": [[4.610663414001465, 1.6118507385253906, 0.04394547641277313]], "avg_entropy": [2.0888198763132095], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "origins of replication", "score_of_biggest_cluster": 0.2462805062532425, "score_of_first_answer": 0.2462805062532425, "max_score_of_first_answer": 0.9949501752853394, "min_score_of_first_answer": 0.057662900537252426, "avg_score": 0.2462805062532425, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40265849390434216, "std_of_first_answer": 0.40265849390434216, "avg_entropy_of_first_answer": 2.0888198763132095, "lex_sim": 1.0}
{"question": "which is the only test playing nation that india has never beat in t20", "answer": ["Australia"], "scores": [[0.1384752094745636]], "normalized_score": [0.1384752094745636], "tokens": [["\u2581Australia"]], "entropy": [[3.007521152496338]], "avg_entropy": [3.007521152496338], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Scotland", "score_of_biggest_cluster": 0.1384752094745636, "score_of_first_answer": 0.1384752094745636, "max_score_of_first_answer": 0.1384752094745636, "min_score_of_first_answer": 0.1384752094745636, "avg_score": 0.1384752094745636, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.007521152496338, "lex_sim": 1.0}
{"question": "in florida it is illegal to sell alcohol before 1 pm on any sunday. this is an example of", "answer": ["Blue Law"], "scores": [[0.040133435279130936, 0.06861933320760727]], "normalized_score": [0.05247789993882179], "tokens": [["\u2581Blue", "\u2581Law"]], "entropy": [[5.493006706237793, 1.3233497142791748]], "avg_entropy": [3.408178210258484], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Blue laws", "score_of_biggest_cluster": 0.05247789993882179, "score_of_first_answer": 0.05247789993882179, "max_score_of_first_answer": 0.06861933320760727, "min_score_of_first_answer": 0.040133435279130936, "avg_score": 0.05247789993882179, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.014242948964238167, "std_of_first_answer": 0.014242948964238167, "avg_entropy_of_first_answer": 3.408178210258484, "lex_sim": 1.0}
{"question": "when did the name of bombay change to mumbai", "answer": ["1996"], "scores": [[0.5372856855392456, 0.9784181714057922, 0.9825361967086792, 0.9806234240531921, 0.24324798583984375]], "normalized_score": [0.6578486561775208], "tokens": [["\u2581", "1", "9", "9", "6"]], "entropy": [[2.362410545349121, 0.12780587375164032, 0.13258770108222961, 0.1520134061574936, 0.8244694471359253]], "avg_entropy": [0.719857394695282], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1995", "score_of_biggest_cluster": 0.6578486561775208, "score_of_first_answer": 0.6578486561775208, "max_score_of_first_answer": 0.9825361967086792, "min_score_of_first_answer": 0.24324798583984375, "avg_score": 0.6578486561775208, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30375139073555035, "std_of_first_answer": 0.30375139073555035, "avg_entropy_of_first_answer": 0.719857394695282, "lex_sim": 1.0}
{"question": "nuclear power plant that blew up in russia", "answer": ["Chernobyl"], "scores": [[0.7171117663383484, 0.96240234375, 0.9965403079986572, 0.9992572665214539]], "normalized_score": [0.9104978442192078], "tokens": [["\u2581Ch", "ern", "ob", "yl"]], "entropy": [[2.058687925338745, 0.2542419135570526, 0.032369498163461685, 0.00810939446091652]], "avg_entropy": [0.588352182880044], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chernobyl Nuclear Power Plant", "score_of_biggest_cluster": 0.9104978442192078, "score_of_first_answer": 0.9104978442192078, "max_score_of_first_answer": 0.9992572665214539, "min_score_of_first_answer": 0.7171117663383484, "avg_score": 0.9104978442192078, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.11736293290847578, "std_of_first_answer": 0.11736293290847578, "avg_entropy_of_first_answer": 0.588352182880044, "lex_sim": 1.0}
{"question": "when does middle-earth shadow of mordor come out", "answer": ["August 2014"], "scores": [[0.12278460711240768, 0.9559557437896729, 0.7501928210258484, 0.1955936998128891, 0.8634058833122253, 0.849920392036438]], "normalized_score": [0.482633113861084], "tokens": [["\u2581August", "\u2581", "2", "0", "1", "4"]], "entropy": [[2.403411865234375, 0.22087787091732025, 0.9219098687171936, 1.9159656763076782, 0.6074400544166565, 0.6657084226608276]], "avg_entropy": [1.122552293042342], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "September 2014", "score_of_biggest_cluster": 0.482633113861084, "score_of_first_answer": 0.482633113861084, "max_score_of_first_answer": 0.9559557437896729, "min_score_of_first_answer": 0.12278460711240768, "avg_score": 0.482633113861084, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33397030187906845, "std_of_first_answer": 0.33397030187906845, "avg_entropy_of_first_answer": 1.122552293042342, "lex_sim": 1.0}
{"question": "where was the capital of the habsburg empire located", "answer": ["Vienna"], "scores": [[0.8319072127342224]], "normalized_score": [0.8319072127342224], "tokens": [["\u2581Vienna"]], "entropy": [[1.187500238418579]], "avg_entropy": [1.187500238418579], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Prague", "score_of_biggest_cluster": 0.8319072127342224, "score_of_first_answer": 0.8319072127342224, "max_score_of_first_answer": 0.8319072127342224, "min_score_of_first_answer": 0.8319072127342224, "avg_score": 0.8319072127342224, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.187500238418579, "lex_sim": 1.0}
{"question": "who has the power (judicial) to make decisions in courts of law", "answer": ["Judiciary power"], "scores": [[0.15708665549755096, 0.09364937990903854, 0.9885899424552917, 0.0006254646577872336]], "normalized_score": [0.05491812899708748], "tokens": [["\u2581Jud", "ici", "ary", "\u2581power"]], "entropy": [[3.224978446960449, 0.5744823217391968, 0.07377003878355026, 0.3251572251319885]], "avg_entropy": [1.0495970081537962], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "judges", "score_of_biggest_cluster": 0.05491812899708748, "score_of_first_answer": 0.05491812899708748, "max_score_of_first_answer": 0.9885899424552917, "min_score_of_first_answer": 0.0006254646577872336, "avg_score": 0.05491812899708748, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.39572307055278483, "std_of_first_answer": 0.39572307055278483, "avg_entropy_of_first_answer": 1.0495970081537962, "lex_sim": 1.0}
{"question": "when did the battle of badr take place", "answer": ["between 624-625"], "scores": [[0.001089933910407126, 0.6147913336753845, 0.5600101947784424, 0.9376317858695984, 0.3029186427593231, 0.11362683027982712, 0.9236277341842651, 0.7634627223014832, 0.39259499311447144]], "normalized_score": [0.24643954634666443], "tokens": [["\u2581between", "\u2581", "6", "2", "4", "-", "6", "2", "5"]], "entropy": [[1.7425332069396973, 1.9706621170043945, 1.4152079820632935, 0.35533156991004944, 1.2825368642807007, 1.3580206632614136, 0.44281843304634094, 0.6109231114387512, 1.2556936740875244]], "avg_entropy": [1.159303069114685], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "13 March 624", "score_of_biggest_cluster": 0.24643954634666443, "score_of_first_answer": 0.24643954634666443, "max_score_of_first_answer": 0.9376317858695984, "min_score_of_first_answer": 0.001089933910407126, "avg_score": 0.24643954634666443, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3170010256167612, "std_of_first_answer": 0.3170010256167612, "avg_entropy_of_first_answer": 1.159303069114685, "lex_sim": 1.0}
{"question": "south african tree with thorns on the trunk", "answer": ["baobab"], "scores": [[0.03586746007204056, 0.9943089485168457, 0.945335328578949]], "normalized_score": [0.3230496644973755], "tokens": [["\u2581ba", "ob", "ab"]], "entropy": [[4.7392778396606445, 0.049240775406360626, 0.292967289686203]], "avg_entropy": [1.693828634917736], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "silk floss tree", "score_of_biggest_cluster": 0.3230496644973755, "score_of_first_answer": 0.3230496644973755, "max_score_of_first_answer": 0.9943089485168457, "min_score_of_first_answer": 0.03586746007204056, "avg_score": 0.3230496644973755, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44072419015230097, "std_of_first_answer": 0.44072419015230097, "avg_entropy_of_first_answer": 1.693828634917736, "lex_sim": 1.0}
{"question": "who wrote the music phantom of the opera", "answer": ["Andrew Lloyd Webber"], "scores": [[0.8326340913772583, 0.9755032658576965, 0.9347532987594604, 0.9985929131507874]], "normalized_score": [0.9331299066543579], "tokens": [["\u2581Andrew", "\u2581Lloyd", "\u2581Web", "ber"]], "entropy": [[1.118687391281128, 0.22101610898971558, 0.29763689637184143, 0.013560116291046143]], "avg_entropy": [0.41272512823343277], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Andrew Lloyd Webber", "score_of_biggest_cluster": 0.9331299066543579, "score_of_first_answer": 0.9331299066543579, "max_score_of_first_answer": 0.9985929131507874, "min_score_of_first_answer": 0.8326340913772583, "avg_score": 0.9331299066543579, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06356662452390457, "std_of_first_answer": 0.06356662452390457, "avg_entropy_of_first_answer": 0.41272512823343277, "lex_sim": 1.0}
{"question": "who is the most selling music artist of all time", "answer": ["POP/Rock/R&B (820.77 million)"], "scores": [[0.003359374590218067, 0.010149612091481686, 0.047001421451568604, 0.7893341779708862, 0.5726660490036011, 0.0637994259595871, 0.2809435725212097, 0.6467776298522949, 0.9880517721176147, 0.006904026959091425, 0.0052847592160105705, 0.0829395055770874, 0.1155804768204689, 0.1164998933672905, 0.06911260634660721, 0.03667474910616875, 0.48732104897499084, 0.5246421694755554]], "normalized_score": [0.09191146492958069], "tokens": [["\u2581P", "OP", "/", "R", "ock", "/", "R", "&", "B", "\u2581(", "8", "2", "0", ".", "7", "7", "\u2581million", ")"]], "entropy": [[3.0051231384277344, 1.981717824935913, 3.6685233116149902, 1.1650460958480835, 1.304722785949707, 3.3555898666381836, 3.224691867828369, 1.2997872829437256, 0.09509886056184769, 2.638735771179199, 4.976977348327637, 2.605747699737549, 2.84224796295166, 2.193905830383301, 2.103823661804199, 2.447188377380371, 2.363284111022949, 1.988673210144043]], "avg_entropy": [2.4033825004266367], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Beatles", "score_of_biggest_cluster": 0.09191146492958069, "score_of_first_answer": 0.09191146492958069, "max_score_of_first_answer": 0.9880517721176147, "min_score_of_first_answer": 0.003359374590218067, "avg_score": 0.09191146492958069, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3053606083050641, "std_of_first_answer": 0.3053606083050641, "avg_entropy_of_first_answer": 2.4033825004266367, "lex_sim": 1.0}
{"question": "which domain of life are humans members of", "answer": ["Eukaryota"], "scores": [[0.4731442332267761, 0.8255119323730469, 0.9923223853111267, 0.2282005399465561]], "normalized_score": [0.5453453063964844], "tokens": [["\u2581E", "uk", "ary", "ota"]], "entropy": [[2.7576775550842285, 0.7067083120346069, 0.05823095887899399, 1.4892315864562988]], "avg_entropy": [1.252962103113532], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eukarya", "score_of_biggest_cluster": 0.5453453063964844, "score_of_first_answer": 0.5453453063964844, "max_score_of_first_answer": 0.9923223853111267, "min_score_of_first_answer": 0.2282005399465561, "avg_score": 0.5453453063964844, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.29813956287034743, "std_of_first_answer": 0.29813956287034743, "avg_entropy_of_first_answer": 1.252962103113532, "lex_sim": 1.0}
{"question": "how many nfl games were played in london", "answer": ["15"], "scores": [[0.3302498757839203, 0.268527626991272, 0.05588795989751816]], "normalized_score": [0.17049695551395416], "tokens": [["\u2581", "1", "5"]], "entropy": [[2.9409756660461426, 2.072796583175659, 2.365088701248169]], "avg_entropy": [2.4596203168233237], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "21", "score_of_biggest_cluster": 0.17049695551395416, "score_of_first_answer": 0.17049695551395416, "max_score_of_first_answer": 0.3302498757839203, "min_score_of_first_answer": 0.05588795989751816, "avg_score": 0.17049695551395416, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11752055371372334, "std_of_first_answer": 0.11752055371372334, "avg_entropy_of_first_answer": 2.4596203168233237, "lex_sim": 1.0}
{"question": "who has hit a double century in the ranji trophy for jharkhand in 2016", "answer": ["Dhruv Pratap Singh"], "scores": [[0.014513371512293816, 0.5640388131141663, 0.49754762649536133, 0.9558691382408142, 0.02844870463013649, 0.723888635635376, 0.9986338019371033, 0.9929383397102356]], "normalized_score": [0.3072887659072876], "tokens": [["\u2581D", "h", "ru", "v", "\u2581Pr", "at", "ap", "\u2581Singh"]], "entropy": [[3.727024793624878, 2.0438342094421387, 1.9149240255355835, 0.2705157995223999, 2.855288028717041, 0.7526208162307739, 0.013289378024637699, 0.057826701551675797]], "avg_entropy": [1.454415469081141], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jaydev Shah", "score_of_biggest_cluster": 0.3072887659072876, "score_of_first_answer": 0.3072887659072876, "max_score_of_first_answer": 0.9986338019371033, "min_score_of_first_answer": 0.014513371512293816, "avg_score": 0.3072887659072876, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3769089950453966, "std_of_first_answer": 0.3769089950453966, "avg_entropy_of_first_answer": 1.454415469081141, "lex_sim": 1.0}
{"question": "who designed the earth day flag in 1969", "answer": ["J.W."], "scores": [[0.02135532721877098, 0.5391597151756287, 0.0060513517819345, 0.8146160244941711, 0.010697294026613235]], "normalized_score": [0.0571032352745533], "tokens": [["\u2581J", ".", "W", ".", "\u2581Graham"]], "entropy": [[4.3888959884643555, 2.6175222396850586, 4.532587051391602, 1.0090162754058838, 4.732382774353027]], "avg_entropy": [3.4560808658599855], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John McConnell", "score_of_biggest_cluster": 0.0571032352745533, "score_of_first_answer": 0.0571032352745533, "max_score_of_first_answer": 0.8146160244941711, "min_score_of_first_answer": 0.0060513517819345, "avg_score": 0.0571032352745533, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3368779422478984, "std_of_first_answer": 0.3368779422478984, "avg_entropy_of_first_answer": 3.4560808658599855, "lex_sim": 1.0}
{"question": "role of malonyl coa in fatty acid synthesis", "answer": ["a precusor"], "scores": [[0.032453492283821106, 0.005864778067916632, 0.0020043503027409315, 0.9804332852363586]], "normalized_score": [0.02473011612892151], "tokens": [["\u2581a", "\u2581prec", "us", "or"]], "entropy": [[4.830506324768066, 0.708950400352478, 0.04336153715848923, 0.17348556220531464]], "avg_entropy": [1.439075956121087], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "chain elongation", "score_of_biggest_cluster": 0.02473011612892151, "score_of_first_answer": 0.02473011612892151, "max_score_of_first_answer": 0.9804332852363586, "min_score_of_first_answer": 0.0020043503027409315, "avg_score": 0.02473011612892151, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41888405739235923, "std_of_first_answer": 0.41888405739235923, "avg_entropy_of_first_answer": 1.439075956121087, "lex_sim": 1.0}
{"question": "who was elected governor of tennessee in 1865", "answer": ["Isham Garrott Reavis"], "scores": [[0.21258041262626648, 0.8321550488471985, 0.93650221824646, 0.013288400135934353, 0.8587408065795898, 0.1745837777853012, 0.9801652431488037, 0.9977232813835144]], "normalized_score": [0.36610913276672363], "tokens": [["\u2581I", "sh", "am", "\u2581Gar", "rott", "\u2581Re", "av", "is"]], "entropy": [[2.8807623386383057, 0.8461958169937134, 0.3372156023979187, 1.4072933197021484, 0.6400347948074341, 2.1016106605529785, 0.15304848551750183, 0.0196312814950943]], "avg_entropy": [1.0482240375131369], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Edward H. East", "score_of_biggest_cluster": 0.36610913276672363, "score_of_first_answer": 0.36610913276672363, "max_score_of_first_answer": 0.9977232813835144, "min_score_of_first_answer": 0.013288400135934353, "avg_score": 0.36610913276672363, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38839384951490674, "std_of_first_answer": 0.38839384951490674, "avg_entropy_of_first_answer": 1.0482240375131369, "lex_sim": 1.0}
{"question": "who wrote i want to dance with somebody by whitney houston", "answer": ["Whitney Houston, Alan George Brown"], "scores": [[0.038607560098171234, 0.9986811280250549, 0.8730085492134094, 0.047713760286569595, 0.0039451997727155685, 0.03788759559392929, 0.2881052792072296]], "normalized_score": [0.09486925601959229], "tokens": [["\u2581Whit", "ney", "\u2581Houston", ",", "\u2581Alan", "\u2581George", "\u2581Brown"]], "entropy": [[4.426018238067627, 0.01541955303400755, 0.6705615520477295, 1.0338648557662964, 3.501089572906494, 4.4532012939453125, 3.403369426727295]], "avg_entropy": [2.500503498927823], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George Merrill", "score_of_biggest_cluster": 0.09486925601959229, "score_of_first_answer": 0.09486925601959229, "max_score_of_first_answer": 0.9986811280250549, "min_score_of_first_answer": 0.0039451997727155685, "avg_score": 0.09486925601959229, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3963971501453297, "std_of_first_answer": 0.3963971501453297, "avg_entropy_of_first_answer": 2.500503498927823, "lex_sim": 1.0}
{"question": "where did an independence movement occur because of the congress of vienna", "answer": ["Polska"], "scores": [[0.004084465559571981, 0.06764674186706543]], "normalized_score": [0.01662229187786579], "tokens": [["\u2581Pol", "ska"]], "entropy": [[5.134451389312744, 3.002840042114258]], "avg_entropy": [4.068645715713501], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Italy", "score_of_biggest_cluster": 0.01662229187786579, "score_of_first_answer": 0.01662229187786579, "max_score_of_first_answer": 0.06764674186706543, "min_score_of_first_answer": 0.004084465559571981, "avg_score": 0.01662229187786579, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.031781138153746724, "std_of_first_answer": 0.031781138153746724, "avg_entropy_of_first_answer": 4.068645715713501, "lex_sim": 1.0}
{"question": "how many episodes in 1st season of the crown", "answer": ["10 episodes"], "scores": [[0.7209069728851318, 0.8341201543807983, 0.9853779077529907, 0.02387954294681549]], "normalized_score": [0.34489282965660095], "tokens": [["\u2581", "1", "0", "\u2581episodes"]], "entropy": [[1.2941477298736572, 0.6330950260162354, 0.11381181329488754, 0.345069944858551]], "avg_entropy": [0.5965311285108328], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "10", "score_of_biggest_cluster": 0.34489282965660095, "score_of_first_answer": 0.34489282965660095, "max_score_of_first_answer": 0.9853779077529907, "min_score_of_first_answer": 0.02387954294681549, "avg_score": 0.34489282965660095, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3684814445665591, "std_of_first_answer": 0.3684814445665591, "avg_entropy_of_first_answer": 0.5965311285108328, "lex_sim": 1.0}
{"question": "who came out first batman or spider man", "answer": ["Spider-Man"], "scores": [[0.3223762810230255, 0.6395396590232849, 0.9667176008224487]], "normalized_score": [0.5841307640075684], "tokens": [["\u2581Spider", "-", "Man"]], "entropy": [[2.4295809268951416, 0.9262670278549194, 0.15522339940071106]], "avg_entropy": [1.1703571180502574], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Batman", "score_of_biggest_cluster": 0.5841307640075684, "score_of_first_answer": 0.5841307640075684, "max_score_of_first_answer": 0.9667176008224487, "min_score_of_first_answer": 0.3223762810230255, "avg_score": 0.5841307640075684, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2630618326860595, "std_of_first_answer": 0.2630618326860595, "avg_entropy_of_first_answer": 1.1703571180502574, "lex_sim": 1.0}
{"question": "what happens when iron reacts with oxygen and water", "answer": ["Iron III oxides become rusty when interacting with water and oxygen."], "scores": [[0.2098773717880249, 0.0033584400080144405, 0.5668072700500488, 0.05223239213228226, 0.0016119558131322265, 0.395243376493454, 0.04278915002942085, 0.02920721285045147, 0.0023816276807338, 0.8958098292350769, 0.976554811000824, 0.370708167552948, 0.41670647263526917, 0.8516795039176941, 0.9990656971931458, 0.40539148449897766]], "normalized_score": [0.1199830025434494], "tokens": [["\u2581Iron", "\u2581III", "\u2581ox", "ides", "\u2581become", "\u2581rust", "y", "\u2581when", "\u2581interact", "ing", "\u2581with", "\u2581water", "\u2581and", "\u2581o", "xygen", "."]], "entropy": [[2.9544930458068848, 2.326070785522461, 1.8944833278656006, 0.27124714851379395, 2.5994882583618164, 3.0119147300720215, 1.9710845947265625, 2.054443359375, 2.626666784286499, 0.43502700328826904, 0.17833343148231506, 1.7192848920822144, 1.6340489387512207, 0.8060017824172974, 0.009732551872730255, 1.372509241104126]], "avg_entropy": [1.6165518672205508], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rust", "score_of_biggest_cluster": 0.1199830025434494, "score_of_first_answer": 0.1199830025434494, "max_score_of_first_answer": 0.9990656971931458, "min_score_of_first_answer": 0.0016119558131322265, "avg_score": 0.1199830025434494, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3592027778453909, "std_of_first_answer": 0.3592027778453909, "avg_entropy_of_first_answer": 1.6165518672205508, "lex_sim": 1.0}
{"question": "what culture region is germany a part of", "answer": ["West"], "scores": [[0.06167908385396004]], "normalized_score": [0.06167908012866974], "tokens": [["\u2581West"]], "entropy": [[3.3993942737579346]], "avg_entropy": [3.3993942737579346], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Central Germany", "score_of_biggest_cluster": 0.06167908012866974, "score_of_first_answer": 0.06167908012866974, "max_score_of_first_answer": 0.06167908385396004, "min_score_of_first_answer": 0.06167908385396004, "avg_score": 0.06167908012866974, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.3993942737579346, "lex_sim": 1.0}
{"question": "the elements in each period have the same number of", "answer": ["electrons"], "scores": [[0.4391147196292877]], "normalized_score": [0.4391147196292877], "tokens": [["\u2581electrons"]], "entropy": [[2.4749646186828613]], "avg_entropy": [2.4749646186828613], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "electron shells", "score_of_biggest_cluster": 0.4391147196292877, "score_of_first_answer": 0.4391147196292877, "max_score_of_first_answer": 0.4391147196292877, "min_score_of_first_answer": 0.4391147196292877, "avg_score": 0.4391147196292877, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.4749646186828613, "lex_sim": 1.0}
{"question": "where did they film the royal tv series", "answer": ["Sandringham, Norfolk"], "scores": [[0.01739620417356491, 0.9949217438697815, 0.9990731477737427, 0.1406427025794983, 0.5011956095695496, 0.9960474371910095]], "normalized_score": [0.3266185522079468], "tokens": [["\u2581Sand", "ring", "ham", ",", "\u2581Nor", "folk"]], "entropy": [[4.865599632263184, 0.04530327022075653, 0.010776993818581104, 1.8375673294067383, 2.3153207302093506, 0.03412158787250519]], "avg_entropy": [1.5181149239651859], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "City of Bradford", "score_of_biggest_cluster": 0.3266185522079468, "score_of_first_answer": 0.3266185522079468, "max_score_of_first_answer": 0.9990731477737427, "min_score_of_first_answer": 0.01739620417356491, "avg_score": 0.3266185522079468, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41470271470238296, "std_of_first_answer": 0.41470271470238296, "avg_entropy_of_first_answer": 1.5181149239651859, "lex_sim": 1.0}
{"question": "who talks for belle in beauty and the beast", "answer": ["Gaston"], "scores": [[0.001747025758959353, 0.9980206489562988]], "normalized_score": [0.041756048798561096], "tokens": [["\u2581Gast", "on"]], "entropy": [[1.4218887090682983, 0.021167457103729248]], "avg_entropy": [0.7215280830860138], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Julie Nathanson", "score_of_biggest_cluster": 0.041756048798561096, "score_of_first_answer": 0.041756048798561096, "max_score_of_first_answer": 0.9980206489562988, "min_score_of_first_answer": 0.001747025758959353, "avg_score": 0.041756048798561096, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.49813681159866974, "std_of_first_answer": 0.49813681159866974, "avg_entropy_of_first_answer": 0.7215280830860138, "lex_sim": 1.0}
{"question": "where does the saying get out of dodge come from", "answer": ["Texas"], "scores": [[0.039849165827035904]], "normalized_score": [0.039849162101745605], "tokens": [["\u2581Texas"]], "entropy": [[5.572983741760254]], "avg_entropy": [5.572983741760254], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the drama series Gunsmoke", "score_of_biggest_cluster": 0.039849162101745605, "score_of_first_answer": 0.039849162101745605, "max_score_of_first_answer": 0.039849165827035904, "min_score_of_first_answer": 0.039849165827035904, "avg_score": 0.039849162101745605, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.572983741760254, "lex_sim": 1.0}
{"question": "who were the twins that played for kentucky", "answer": ["Dakari Johnson and Enes Kanter"], "scores": [[0.0031093761790543795, 0.8819991946220398, 0.4038086235523224, 0.6023812294006348, 0.009041154757142067, 0.9892895817756653, 0.9754481315612793, 0.9980997443199158]], "normalized_score": [0.2215716689825058], "tokens": [["\u2581Dak", "ari", "\u2581Johnson", "\u2581and", "\u2581En", "es", "\u2581Kan", "ter"]], "entropy": [[4.803788185119629, 0.5769095420837402, 1.3163869380950928, 1.028485655784607, 3.5417490005493164, 0.08862540125846863, 0.22364085912704468, 0.021120328456163406]], "avg_entropy": [1.4500882388092577], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Aaron Harrison", "score_of_biggest_cluster": 0.2215716689825058, "score_of_first_answer": 0.2215716689825058, "max_score_of_first_answer": 0.9980997443199158, "min_score_of_first_answer": 0.0031093761790543795, "avg_score": 0.2215716689825058, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39906503432267143, "std_of_first_answer": 0.39906503432267143, "avg_entropy_of_first_answer": 1.4500882388092577, "lex_sim": 1.0}
{"question": "who sang the song a white sports coat and a pink carnation", "answer": ["Marty Robbins"], "scores": [[0.3928661048412323, 0.9339988827705383, 0.9910175800323486, 0.9896174669265747, 0.9998443126678467]], "normalized_score": [0.8151065707206726], "tokens": [["\u2581Mart", "y", "\u2581Rob", "b", "ins"]], "entropy": [[3.435586452484131, 0.3130417466163635, 0.0863993912935257, 0.06839422881603241, 0.0019638126250356436]], "avg_entropy": [0.7810771263670176], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marty Robbins", "score_of_biggest_cluster": 0.8151065707206726, "score_of_first_answer": 0.8151065707206726, "max_score_of_first_answer": 0.9998443126678467, "min_score_of_first_answer": 0.3928661048412323, "avg_score": 0.8151065707206726, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23545778753982866, "std_of_first_answer": 0.23545778753982866, "avg_entropy_of_first_answer": 0.7810771263670176, "lex_sim": 1.0}
{"question": "the region that stretches between the black and caspian seas", "answer": ["Caucasus Mountains"], "scores": [[0.5759060382843018, 0.9432999491691589, 0.9802001714706421, 0.9167965650558472, 0.037426989525556564]], "normalized_score": [0.4491121768951416], "tokens": [["\u2581C", "au", "cas", "us", "\u2581Mountains"]], "entropy": [[2.271080493927002, 0.2595532238483429, 0.11506868153810501, 0.36651402711868286, 0.4345947802066803]], "avg_entropy": [0.6893622413277626], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Caucasus Mountains", "score_of_biggest_cluster": 0.4491121768951416, "score_of_first_answer": 0.4491121768951416, "max_score_of_first_answer": 0.9802001714706421, "min_score_of_first_answer": 0.037426989525556564, "avg_score": 0.4491121768951416, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.35740173945504367, "std_of_first_answer": 0.35740173945504367, "avg_entropy_of_first_answer": 0.6893622413277626, "lex_sim": 1.0}
{"question": "who plays cesar in the show on my block", "answer": ["Michael Angarano"], "scores": [[0.01696230098605156, 0.06829217821359634, 0.950348436832428, 0.9975269436836243]], "normalized_score": [0.1820395588874817], "tokens": [["\u2581Michael", "\u2581Ang", "ar", "ano"]], "entropy": [[5.886733055114746, 4.565437316894531, 0.2120238095521927, 0.024133607745170593]], "avg_entropy": [2.67208194732666], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Diego Tinoco", "score_of_biggest_cluster": 0.1820395588874817, "score_of_first_answer": 0.1820395588874817, "max_score_of_first_answer": 0.9975269436836243, "min_score_of_first_answer": 0.01696230098605156, "avg_score": 0.1820395588874817, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4663071518343985, "std_of_first_answer": 0.4663071518343985, "avg_entropy_of_first_answer": 2.67208194732666, "lex_sim": 1.0}
{"question": "who is the original singer of fool if you think it's over", "answer": ["Lee Brilleaux"], "scores": [[0.025484640151262283, 0.01497907005250454, 0.2917094826698303, 0.9310576915740967]], "normalized_score": [0.10090731084346771], "tokens": [["\u2581Lee", "\u2581Br", "ille", "aux"]], "entropy": [[4.1691179275512695, 4.028925895690918, 0.7925858497619629, 0.3907151520252228]], "avg_entropy": [2.3453362062573433], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chris Rea", "score_of_biggest_cluster": 0.10090731084346771, "score_of_first_answer": 0.10090731084346771, "max_score_of_first_answer": 0.9310576915740967, "min_score_of_first_answer": 0.01497907005250454, "avg_score": 0.10090731084346771, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37212182813106964, "std_of_first_answer": 0.37212182813106964, "avg_entropy_of_first_answer": 2.3453362062573433, "lex_sim": 1.0}
{"question": "who beat university of virginia in basketball this year", "answer": ["Virginia Cavaliers"], "scores": [[0.19501088559627533, 0.3844165802001953, 0.9663011431694031, 0.9999239444732666]], "normalized_score": [0.5187822580337524], "tokens": [["\u2581Virginia", "\u2581Cav", "ali", "ers"]], "entropy": [[3.993126392364502, 1.686082363128662, 0.2304036021232605, 0.0009062591707333922]], "avg_entropy": [1.4776296541967895], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Virginia Tech", "score_of_biggest_cluster": 0.5187822580337524, "score_of_first_answer": 0.5187822580337524, "max_score_of_first_answer": 0.9999239444732666, "min_score_of_first_answer": 0.19501088559627533, "avg_score": 0.5187822580337524, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3533073789023959, "std_of_first_answer": 0.3533073789023959, "avg_entropy_of_first_answer": 1.4776296541967895, "lex_sim": 1.0}
{"question": "who used morse code in world war 2", "answer": ["World War II was widely known as the Great Patriotic War."], "scores": [[0.016345111653208733, 0.968254029750824, 0.45230287313461304, 0.010415669530630112, 0.0012253150343894958, 0.2173696607351303, 0.7291982173919678, 0.5776309967041016, 0.040331147611141205, 0.32373297214508057, 0.9970324039459229, 0.9866845011711121, 0.05654647946357727, 0.12216606736183167, 0.21500685811042786, 0.8371850252151489, 0.15847115218639374, 0.20600545406341553, 0.0022353846579790115, 0.019566990435123444, 0.10922666639089584, 0.21453239023685455]], "normalized_score": [0.1188540831208229], "tokens": [["\u2581World", "\u2581War", "\u2581II", "\u2581was", "\u2581widely", "\u2581known", "\u2581as", "\u2581the", "\u2581Great", "\u2581Patri", "otic", "\u2581War", ".", "\u2581The", "\u2581Soviet", "\u2581Union", "\u2581used", "\u2581the", "\u2581codes", "\u2581as", "\u2581well", "."]], "entropy": [[5.409348487854004, 0.2378859966993332, 1.3823059797286987, 2.07034969329834, 3.2938194274902344, 2.9735701084136963, 1.0134849548339844, 1.8728597164154053, 3.09741473197937, 1.8858458995819092, 0.02211998775601387, 0.1033942699432373, 1.3663642406463623, 3.4702816009521484, 4.4856438636779785, 1.1581066846847534, 4.039061546325684, 3.2969179153442383, 4.4116973876953125, 4.431866645812988, 2.8431220054626465, 1.2323663234710693]], "avg_entropy": [2.4589921576394276], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "naval bases", "score_of_biggest_cluster": 0.1188540831208229, "score_of_first_answer": 0.1188540831208229, "max_score_of_first_answer": 0.9970324039459229, "min_score_of_first_answer": 0.0012253150343894958, "avg_score": 0.1188540831208229, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34532678844060777, "std_of_first_answer": 0.34532678844060777, "avg_entropy_of_first_answer": 2.4589921576394276, "lex_sim": 1.0}
{"question": "which british city is farthest from the sea", "answer": ["New Castle"], "scores": [[0.06116052344441414, 0.0015663611702620983]], "normalized_score": [0.009787717834115028], "tokens": [["\u2581New", "\u2581Castle"]], "entropy": [[4.097378730773926, 0.3494839370250702]], "avg_entropy": [2.223431333899498], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Coton in the Elms", "score_of_biggest_cluster": 0.009787717834115028, "score_of_first_answer": 0.009787717834115028, "max_score_of_first_answer": 0.06116052344441414, "min_score_of_first_answer": 0.0015663611702620983, "avg_score": 0.009787717834115028, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02979708113707602, "std_of_first_answer": 0.02979708113707602, "avg_entropy_of_first_answer": 2.223431333899498, "lex_sim": 1.0}
{"question": "when was the death penalty reinstated in oregon", "answer": ["1984"], "scores": [[0.6387913823127747, 0.8783925771713257, 0.9761409759521484, 0.48860761523246765, 0.9390289783477783]], "normalized_score": [0.7586472630500793], "tokens": [["\u2581", "1", "9", "8", "4"]], "entropy": [[1.8261451721191406, 0.40400221943855286, 0.14505615830421448, 1.483338713645935, 0.32823336124420166]], "avg_entropy": [0.837355124950409], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1984", "score_of_biggest_cluster": 0.7586472630500793, "score_of_first_answer": 0.7586472630500793, "max_score_of_first_answer": 0.9761409759521484, "min_score_of_first_answer": 0.48860761523246765, "avg_score": 0.7586472630500793, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.188787772969607, "std_of_first_answer": 0.188787772969607, "avg_entropy_of_first_answer": 0.837355124950409, "lex_sim": 1.0}
{"question": "the fertile crescent is located between what two bodies of water", "answer": ["Red Sea and Mediterranean Sea"], "scores": [[0.06603101640939713, 0.8070079684257507, 0.8456738591194153, 0.2573815882205963, 0.9875827431678772, 0.999292254447937, 0.8178730607032776]], "normalized_score": [0.5130905508995056], "tokens": [["\u2581Red", "\u2581Sea", "\u2581and", "\u2581Mediter", "rane", "an", "\u2581Sea"]], "entropy": [[3.3378422260284424, 0.7269647121429443, 0.7275262475013733, 2.1624109745025635, 0.09259975701570511, 0.007617766037583351, 0.5658330917358398]], "avg_entropy": [1.0886849678520645], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tigris and Euphrates rivers", "score_of_biggest_cluster": 0.5130905508995056, "score_of_first_answer": 0.5130905508995056, "max_score_of_first_answer": 0.999292254447937, "min_score_of_first_answer": 0.06603101640939713, "avg_score": 0.5130905508995056, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34114192615203615, "std_of_first_answer": 0.34114192615203615, "avg_entropy_of_first_answer": 1.0886849678520645, "lex_sim": 1.0}
{"question": "a player that can shoot pass or dribble is considered to be", "answer": ["a combo guard"], "scores": [[0.07966199517250061, 0.1769181787967682, 0.9979909658432007, 0.7113797068595886]], "normalized_score": [0.31627359986305237], "tokens": [["\u2581a", "\u2581com", "bo", "\u2581guard"]], "entropy": [[3.9058423042297363, 4.0349650382995605, 0.02189595438539982, 1.0797086954116821]], "avg_entropy": [2.2606029980815947], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Triple threat", "score_of_biggest_cluster": 0.31627359986305237, "score_of_first_answer": 0.31627359986305237, "max_score_of_first_answer": 0.9979909658432007, "min_score_of_first_answer": 0.07966199517250061, "avg_score": 0.31627359986305237, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3786332155983089, "std_of_first_answer": 0.3786332155983089, "avg_entropy_of_first_answer": 2.2606029980815947, "lex_sim": 1.0}
{"question": "who wrote the guitar solo in beat it", "answer": ["Eddie Van Halen"], "scores": [[0.8015942573547363, 0.9831477403640747, 0.9279380440711975, 0.9977511763572693, 0.9981056451797485]], "normalized_score": [0.9385517835617065], "tokens": [["\u2581Edd", "ie", "\u2581Van", "\u2581Hal", "en"]], "entropy": [[1.1030559539794922, 0.08881736546754837, 0.4178319573402405, 0.02461269125342369, 0.020518239587545395]], "avg_entropy": [0.33096724152565005], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eddie Van Halen", "score_of_biggest_cluster": 0.9385517835617065, "score_of_first_answer": 0.9385517835617065, "max_score_of_first_answer": 0.9981056451797485, "min_score_of_first_answer": 0.8015942573547363, "avg_score": 0.9385517835617065, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.07464617423800275, "std_of_first_answer": 0.07464617423800275, "avg_entropy_of_first_answer": 0.33096724152565005, "lex_sim": 1.0}
{"question": "what nfl coach has the most wins ever", "answer": ["Don Shula"], "scores": [[0.15102894604206085, 0.9520545601844788, 0.9997995495796204]], "normalized_score": [0.5238556861877441], "tokens": [["\u2581Don", "\u2581Sh", "ula"]], "entropy": [[3.2470827102661133, 0.2764643430709839, 0.002517131855711341]], "avg_entropy": [1.175354728397603], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Shula, Don", "score_of_biggest_cluster": 0.5238556861877441, "score_of_first_answer": 0.5238556861877441, "max_score_of_first_answer": 0.9997995495796204, "min_score_of_first_answer": 0.15102894604206085, "avg_score": 0.5238556861877441, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3893489089284675, "std_of_first_answer": 0.3893489089284675, "avg_entropy_of_first_answer": 1.175354728397603, "lex_sim": 1.0}
{"question": "who is the queen of hearts in alice in wonderland", "answer": ["The Queen of Hearts"], "scores": [[0.17919300496578217, 0.5637660026550293, 0.9802944660186768, 0.9736087322235107, 0.9997262358665466]], "normalized_score": [0.6263375282287598], "tokens": [["\u2581The", "\u2581Queen", "\u2581of", "\u2581He", "arts"]], "entropy": [[2.0904221534729004, 1.6081125736236572, 0.12641391158103943, 0.18846487998962402, 0.0035102779511362314]], "avg_entropy": [0.8033847593236715], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the primary antagonist", "score_of_biggest_cluster": 0.6263375282287598, "score_of_first_answer": 0.6263375282287598, "max_score_of_first_answer": 0.9997262358665466, "min_score_of_first_answer": 0.17919300496578217, "avg_score": 0.6263375282287598, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3241397129391003, "std_of_first_answer": 0.3241397129391003, "avg_entropy_of_first_answer": 0.8033847593236715, "lex_sim": 1.0}
{"question": "where does the sweetness of fruit come from", "answer": ["sugars"], "scores": [[0.047071829438209534, 0.9792045950889587]], "normalized_score": [0.21469268202781677], "tokens": [["\u2581sug", "ars"]], "entropy": [[4.734548568725586, 0.10849098861217499]], "avg_entropy": [2.4215197786688805], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fructose, or fruit sugar", "score_of_biggest_cluster": 0.21469268202781677, "score_of_first_answer": 0.21469268202781677, "max_score_of_first_answer": 0.9792045950889587, "min_score_of_first_answer": 0.047071829438209534, "avg_score": 0.21469268202781677, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4660663828253746, "std_of_first_answer": 0.4660663828253746, "avg_entropy_of_first_answer": 2.4215197786688805, "lex_sim": 1.0}
{"question": "who was the first chief minister of west bengal", "answer": ["Bidhan Chandra Roy"], "scores": [[0.49189209938049316, 0.9720384478569031, 0.9889405369758606, 0.9878339767456055, 0.994268536567688, 0.9573429822921753]], "normalized_score": [0.8736345171928406], "tokens": [["\u2581B", "id", "han", "\u2581Ch", "andra", "\u2581Roy"]], "entropy": [[2.453660488128662, 0.19653703272342682, 0.07377735525369644, 0.10777822881937027, 0.05002541095018387, 0.2028510570526123]], "avg_entropy": [0.5141049288213253], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Prafulla Chandra Ghosh", "score_of_biggest_cluster": 0.8736345171928406, "score_of_first_answer": 0.8736345171928406, "max_score_of_first_answer": 0.994268536567688, "min_score_of_first_answer": 0.49189209938049316, "avg_score": 0.8736345171928406, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18236033859429504, "std_of_first_answer": 0.18236033859429504, "avg_entropy_of_first_answer": 0.5141049288213253, "lex_sim": 1.0}
{"question": "who sings the song rock you like a hurricane", "answer": ["Dio"], "scores": [[0.3152530789375305, 0.5853127241134644]], "normalized_score": [0.4295597970485687], "tokens": [["\u2581D", "io"]], "entropy": [[3.3441684246063232, 1.7807765007019043]], "avg_entropy": [2.5624724626541138], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "German rock band Scorpions", "score_of_biggest_cluster": 0.4295597970485687, "score_of_first_answer": 0.4295597970485687, "max_score_of_first_answer": 0.5853127241134644, "min_score_of_first_answer": 0.3152530789375305, "avg_score": 0.4295597970485687, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13502982258796692, "std_of_first_answer": 0.13502982258796692, "avg_entropy_of_first_answer": 2.5624724626541138, "lex_sim": 1.0}
{"question": "who propounded the idea of basic education in india", "answer": ["Maulana Abul Kasem Azad"], "scores": [[0.2793196737766266, 0.9881095290184021, 0.9892917275428772, 0.9496229290962219, 0.998595654964447, 0.002223312621936202, 0.8488245606422424, 0.38231921195983887, 0.9954684972763062]], "normalized_score": [0.38502928614616394], "tokens": [["\u2581Ma", "ul", "ana", "\u2581Ab", "ul", "\u2581Kas", "em", "\u2581Az", "ad"]], "entropy": [[3.6256203651428223, 0.0877743661403656, 0.08705554902553558, 0.26910439133644104, 0.014982707798480988, 0.18482699990272522, 0.500630259513855, 1.6286628246307373, 0.039240237325429916]], "avg_entropy": [0.715321966757377], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mahatma Gandhi", "score_of_biggest_cluster": 0.38502928614616394, "score_of_first_answer": 0.38502928614616394, "max_score_of_first_answer": 0.998595654964447, "min_score_of_first_answer": 0.002223312621936202, "avg_score": 0.38502928614616394, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36368714649881767, "std_of_first_answer": 0.36368714649881767, "avg_entropy_of_first_answer": 0.715321966757377, "lex_sim": 1.0}
{"question": "when were birth certificates first issued in the united states", "answer": ["1910"], "scores": [[0.735447883605957, 0.9931463003158569, 0.5623028874397278, 0.27977263927459717, 0.0909377932548523]], "normalized_score": [0.40162158012390137], "tokens": [["\u2581", "1", "9", "1", "0"]], "entropy": [[1.8381141424179077, 0.04994594678282738, 0.9988497495651245, 1.8006832599639893, 1.5607469081878662]], "avg_entropy": [1.249668001383543], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1902", "score_of_biggest_cluster": 0.40162158012390137, "score_of_first_answer": 0.40162158012390137, "max_score_of_first_answer": 0.9931463003158569, "min_score_of_first_answer": 0.0909377932548523, "avg_score": 0.40162158012390137, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3203495626747695, "std_of_first_answer": 0.3203495626747695, "avg_entropy_of_first_answer": 1.249668001383543, "lex_sim": 1.0}
{"question": "who played in last train from gun hill", "answer": ["Gregory Peck"], "scores": [[0.020667193457484245, 0.7411780953407288, 0.9968101382255554]], "normalized_score": [0.24808786809444427], "tokens": [["\u2581Gregory", "\u2581Pe", "ck"]], "entropy": [[5.72496223449707, 1.4469903707504272, 0.02980225905776024]], "avg_entropy": [2.4005849547684193], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bing Russell", "score_of_biggest_cluster": 0.24808786809444427, "score_of_first_answer": 0.24808786809444427, "max_score_of_first_answer": 0.9968101382255554, "min_score_of_first_answer": 0.020667193457484245, "avg_score": 0.24808786809444427, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4132982175543678, "std_of_first_answer": 0.4132982175543678, "avg_entropy_of_first_answer": 2.4005849547684193, "lex_sim": 1.0}
{"question": "delhi is on the banks of which river", "answer": ["Yamuna River A recent study in Boston University School of Medicine, reported that smokers who quit have a good chance of improving their memory to more normal levels within five years."], "scores": [[0.7482355833053589, 0.9982646107673645, 0.04388701543211937, 5.251401489658747e-06, 0.00015950569650158286, 0.17909936606884003, 0.027106815949082375, 0.0006831381469964981, 0.05923653021454811, 0.07211398333311081, 0.980897068977356, 0.8307532072067261, 0.054936591535806656, 0.012209120206534863, 0.6037447452545166, 0.005210832692682743, 0.18542125821113586, 0.8998259902000427, 0.4831595718860626, 0.08418773114681244, 0.03725447505712509, 0.35872238874435425, 0.00461469404399395, 0.7883845567703247, 0.845424234867096, 0.023057296872138977, 0.9995214939117432, 0.6971575617790222, 0.06884409487247467, 0.0033839682582765818, 0.0016274305526167154, 0.08252201229333878, 0.8843171000480652, 0.27646559476852417, 0.10851289331912994, 0.9594635367393494, 0.4229101538658142, 0.14250542223453522, 0.002806307515129447, 0.6365451812744141, 0.4421180784702301, 0.9894199371337891, 0.00806520227342844, 0.9707265496253967, 0.02639898844063282, 0.852085530757904, 0.9938945174217224, 0.9988345503807068]], "normalized_score": [0.09214017540216446], "tokens": [["\u2581Yam", "una", "\u2581River", "\u2581A", "\u2581recent", "\u2581study", "\u2581in", "\u2581Boston", "\u2581University", "\u2581School", "\u2581of", "\u2581Medicine", ",", "\u2581reported", "\u2581that", "\u2581sm", "ok", "ers", "\u2581who", "\u2581quit", "\u2581have", "\u2581a", "\u2581good", "\u2581chance", "\u2581of", "\u2581impro", "ving", "\u2581their", "\u2581memory", "\u2581to", "\u2581more", "\u2581normal", "\u2581levels", "\u2581within", "\u2581five", "\u2581years", ".", "\u2581The", "\u2581Boston", "\u2581University", "\u2581School", "\u2581of", "\u2581Public", "\u2581Health", "\u2581(", "B", "US", "PH"]], "entropy": [[1.4642274379730225, 0.018063440918922424, 0.4876726567745209, 0.2190987765789032, 3.436903476715088, 5.713197708129883, 3.689187526702881, 4.191877365112305, 3.7364118099212646, 3.9877257347106934, 0.15764549374580383, 0.7002021074295044, 3.664365768432617, 4.899680137634277, 2.017993688583374, 5.769611835479736, 1.193557858467102, 0.3877420127391815, 2.7104218006134033, 4.622182369232178, 3.6609487533569336, 3.3928496837615967, 3.348801374435425, 1.5844048261642456, 0.6655684113502502, 4.16489315032959, 0.005438930355012417, 1.739905595779419, 3.6537251472473145, 2.9142165184020996, 2.9663987159729004, 2.0773563385009766, 0.772255539894104, 2.721571445465088, 2.3870205879211426, 0.22447815537452698, 1.6745407581329346, 4.32533073425293, 2.972194194793701, 1.4646259546279907, 1.779327154159546, 0.09880736470222473, 0.17775094509124756, 0.1849939227104187, 3.5239007472991943, 1.0026533603668213, 0.05146021768450737, 0.011454042978584766]], "avg_entropy": [2.221138366187612], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yamuna", "score_of_biggest_cluster": 0.09214017540216446, "score_of_first_answer": 0.09214017540216446, "max_score_of_first_answer": 0.9995214939117432, "min_score_of_first_answer": 5.251401489658747e-06, "avg_score": 0.09214017540216446, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3940830707970783, "std_of_first_answer": 0.3940830707970783, "avg_entropy_of_first_answer": 2.221138366187612, "lex_sim": 1.0}
{"question": "who played the hobbits in the lord of the rings", "answer": ["James Nesbitt"], "scores": [[0.0057490491308271885, 0.29035547375679016, 0.9969629645347595, 0.9840255379676819, 0.9998003840446472]], "normalized_score": [0.2772202491760254], "tokens": [["\u2581James", "\u2581N", "es", "b", "itt"]], "entropy": [[3.2521309852600098, 4.060258388519287, 0.02945220284163952, 0.08883163332939148, 0.0024003288708627224]], "avg_entropy": [1.4866147077642382], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dominic Monaghan", "score_of_biggest_cluster": 0.2772202491760254, "score_of_first_answer": 0.2772202491760254, "max_score_of_first_answer": 0.9998003840446472, "min_score_of_first_answer": 0.0057490491308271885, "avg_score": 0.2772202491760254, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4239281753574448, "std_of_first_answer": 0.4239281753574448, "avg_entropy_of_first_answer": 1.4866147077642382, "lex_sim": 1.0}
{"question": "dynamin is associated with which type of vesicle", "answer": ["endocytic"], "scores": [[0.2262815237045288, 0.16265776753425598, 0.9903596639633179, 0.8742786049842834]], "normalized_score": [0.42251452803611755], "tokens": [["\u2581end", "oc", "yt", "ic"]], "entropy": [[3.935746669769287, 0.8782268762588501, 0.09195302426815033, 0.4949150085449219]], "avg_entropy": [1.3502103947103024], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "newly formed vesicles", "score_of_biggest_cluster": 0.42251452803611755, "score_of_first_answer": 0.42251452803611755, "max_score_of_first_answer": 0.9903596639633179, "min_score_of_first_answer": 0.16265776753425598, "avg_score": 0.42251452803611755, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37188145473529793, "std_of_first_answer": 0.37188145473529793, "avg_entropy_of_first_answer": 1.3502103947103024, "lex_sim": 1.0}
{"question": "where is season 3 of the detour filmed", "answer": ["California"], "scores": [[0.005059146322309971]], "normalized_score": [0.005059145390987396], "tokens": [["\u2581California"]], "entropy": [[6.068753242492676]], "avg_entropy": [6.068753242492676], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alaska", "score_of_biggest_cluster": 0.005059145390987396, "score_of_first_answer": 0.005059145390987396, "max_score_of_first_answer": 0.005059146322309971, "min_score_of_first_answer": 0.005059146322309971, "avg_score": 0.005059145390987396, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.068753242492676, "lex_sim": 1.0}
{"question": "in the song i drive your truck who is he talking about", "answer": ["Hank Williams III"], "scores": [[0.02334485575556755, 0.9709171056747437, 0.8440788984298706, 0.005762103479355574]], "normalized_score": [0.10246707499027252], "tokens": [["\u2581H", "ank", "\u2581Williams", "\u2581III"]], "entropy": [[4.946499824523926, 0.23322634398937225, 0.7549551725387573, 1.2145676612854004]], "avg_entropy": [1.787312250584364], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "his brother", "score_of_biggest_cluster": 0.10246707499027252, "score_of_first_answer": 0.10246707499027252, "max_score_of_first_answer": 0.9709171056747437, "min_score_of_first_answer": 0.005762103479355574, "avg_score": 0.10246707499027252, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4487617581032681, "std_of_first_answer": 0.4487617581032681, "avg_entropy_of_first_answer": 1.787312250584364, "lex_sim": 1.0}
{"question": "what is the current population of bora bora", "answer": ["1000"], "scores": [[0.8745670318603516, 0.20749686658382416, 0.21211646497249603, 0.2348622828722, 0.8271443247795105]], "normalized_score": [0.3756251931190491], "tokens": [["\u2581", "1", "0", "0", "0"]], "entropy": [[1.0132179260253906, 1.7788039445877075, 2.3416452407836914, 1.1505286693572998, 0.8012633323669434]], "avg_entropy": [1.4170918226242066], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "10,605", "score_of_biggest_cluster": 0.3756251931190491, "score_of_first_answer": 0.3756251931190491, "max_score_of_first_answer": 0.8745670318603516, "min_score_of_first_answer": 0.20749686658382416, "avg_score": 0.3756251931190491, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3104578731409377, "std_of_first_answer": 0.3104578731409377, "avg_entropy_of_first_answer": 1.4170918226242066, "lex_sim": 1.0}
{"question": "who has been chosen as the brand ambassador of the campaign 'beti bachao-beti padhao", "answer": ["Gauhar Khan"], "scores": [[0.006287899799644947, 0.40136539936065674, 0.9205917716026306, 0.9908393025398254]], "normalized_score": [0.2190428227186203], "tokens": [["\u2581G", "au", "har", "\u2581Khan"]], "entropy": [[5.0755181312561035, 2.569579601287842, 0.3656032085418701, 0.08435061573982239]], "avg_entropy": [2.0237628892064095], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sakshi Malik", "score_of_biggest_cluster": 0.2190428227186203, "score_of_first_answer": 0.2190428227186203, "max_score_of_first_answer": 0.9908393025398254, "min_score_of_first_answer": 0.006287899799644947, "avg_score": 0.2190428227186203, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4018231489920803, "std_of_first_answer": 0.4018231489920803, "avg_entropy_of_first_answer": 2.0237628892064095, "lex_sim": 1.0}
{"question": "when did world war 2 officially start and end", "answer": ["December 1939 \u2013 September 1945"], "scores": [[0.008509274572134018, 0.9864280819892883, 0.20926694571971893, 0.5759020447731018, 0.6491520404815674, 0.9674320220947266, 0.12356884032487869, 0.6357024312019348, 0.9949737787246704, 0.9397728443145752, 0.9932089447975159, 0.9968704581260681, 0.9916585087776184]], "normalized_score": [0.463694304227829], "tokens": [["\u2581December", "\u2581", "1", "9", "3", "9", "\u2581\u2013", "\u2581September", "\u2581", "1", "9", "4", "5"]], "entropy": [[1.7251875400543213, 0.08672808855772018, 0.7338087558746338, 1.4646010398864746, 0.7079373002052307, 0.18765270709991455, 2.4791393280029297, 1.3683099746704102, 0.04269598796963692, 0.24899108707904816, 0.05609027296304703, 0.025789756327867508, 0.06627769023180008]], "avg_entropy": [0.7071699637633103], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1939", "score_of_biggest_cluster": 0.463694304227829, "score_of_first_answer": 0.463694304227829, "max_score_of_first_answer": 0.9968704581260681, "min_score_of_first_answer": 0.008509274572134018, "avg_score": 0.463694304227829, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35415582720953004, "std_of_first_answer": 0.35415582720953004, "avg_entropy_of_first_answer": 0.7071699637633103, "lex_sim": 1.0}
{"question": "who are the australia's got talent judges", "answer": ["Dannii Minogue, Steve Anthony, Redfoo"], "scores": [[0.05022144690155983, 0.8801138401031494, 0.9998377561569214, 0.9781534671783447, 0.9987456798553467, 0.998932421207428, 0.8175169825553894, 0.006052968557924032, 0.11837013810873032, 0.8173113465309143, 0.05480731651186943, 0.8176237344741821]], "normalized_score": [0.3141860365867615], "tokens": [["\u2581D", "ann", "ii", "\u2581Min", "og", "ue", ",", "\u2581Steve", "\u2581Anthony", ",", "\u2581Red", "foo"]], "entropy": [[5.075430393218994, 0.5825515985488892, 0.002220642752945423, 0.16094110906124115, 0.013983902521431446, 0.011542373336851597, 0.8055716156959534, 4.036383628845215, 3.7308878898620605, 0.7344461679458618, 4.438505172729492, 0.6811547875404358]], "avg_entropy": [1.6894682735049475], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kelly Osbourne", "score_of_biggest_cluster": 0.3141860365867615, "score_of_first_answer": 0.3141860365867615, "max_score_of_first_answer": 0.9998377561569214, "min_score_of_first_answer": 0.006052968557924032, "avg_score": 0.3141860365867615, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40988884540495407, "std_of_first_answer": 0.40988884540495407, "avg_entropy_of_first_answer": 1.6894682735049475, "lex_sim": 1.0}
{"question": "who supported states rights during the civil war", "answer": ["Confederacy"], "scores": [[0.05198708921670914, 0.32149869203567505]], "normalized_score": [0.12928178906440735], "tokens": [["\u2581Confeder", "acy"]], "entropy": [[3.819336414337158, 1.1130571365356445]], "avg_entropy": [2.4661967754364014], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Confederacy", "score_of_biggest_cluster": 0.12928178906440735, "score_of_first_answer": 0.12928178906440735, "max_score_of_first_answer": 0.32149869203567505, "min_score_of_first_answer": 0.05198708921670914, "avg_score": 0.12928178906440735, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.13475580140948296, "std_of_first_answer": 0.13475580140948296, "avg_entropy_of_first_answer": 2.4661967754364014, "lex_sim": 1.0}
{"question": "bowler to take hattrick in both inning in test match", "answer": ["Nilesh Raikar Sorry."], "scores": [[0.006858127191662788, 0.03120410442352295, 0.9912974834442139, 0.015595703385770321, 0.3229670226573944, 0.9688858389854431, 4.599246850034433e-08, 0.02978929504752159, 0.040673837065696716, 0.0049026464112102985, 0.4206046164035797, 0.9990018010139465, 0.3049473166465759, 0.09307462722063065, 0.00023258353758137673, 0.7120203971862793, 0.11160744726657867, 0.7479851245880127, 0.3275239169597626, 0.033981841057538986, 0.9981770515441895, 0.6086941361427307, 0.034748099744319916, 0.2975175976753235, 0.9942476749420166, 0.04640975221991539, 0.5284996032714844, 0.15524698793888092, 0.8835926055908203, 0.13567891716957092, 0.6221581101417542, 0.5493419766426086, 0.6229816675186157, 0.9975337982177734, 0.004609521944075823, 0.006539225112646818, 0.8648733496665955, 0.05240946635603905, 0.02391608990728855, 0.0924171581864357, 0.5211156606674194, 0.5536943078041077, 0.9991528987884521, 0.0741976872086525, 0.01308081392198801, 0.8713229894638062, 0.021279770880937576, 0.024386243894696236]], "normalized_score": [0.09568020701408386], "tokens": [["\u2581N", "iles", "h", "\u2581Ra", "ik", "ar", "\u2581Sorry", ".", "\u2581That", "\u2581wasn", "\u2019", "t", "\u2581a", "\u2581real", "\u2581assistant", ".", "\u2581It", "\u2581was", "\u2581a", "\u2581jo", "ke", ".", "\u2581It", "\u2019", "s", "\u2581called", "\u2581a", "\u2581chat", "bot", ",", "\u2581and", "\u2581it", "\u2019", "s", "\u2581only", "\u2581fun", "ny", "\u2581for", "\u2581now", "\u2581because", "\u2581it", "\u2019", "s", "\u2581a", "\u2581one", "-", "direction", "\u2581conversation"]], "entropy": [[5.800234794616699, 2.1384239196777344, 0.09038259088993073, 1.3766582012176514, 2.4419867992401123, 0.19799759984016418, 0.6948390007019043, 2.0304999351501465, 3.636544704437256, 3.0224339962005615, 0.7682197093963623, 0.01266153808683157, 3.1336121559143066, 3.0988106727600098, 2.3227782249450684, 1.4736545085906982, 3.3443636894226074, 0.889436662197113, 2.7606430053710938, 5.274219512939453, 0.01748817414045334, 2.5050408840179443, 3.642681360244751, 2.4386935234069824, 0.04157557711005211, 4.212576866149902, 2.6416027545928955, 3.941816806793213, 0.4402465224266052, 1.2425934076309204, 2.0005228519439697, 2.4480204582214355, 1.898989200592041, 0.01953970454633236, 3.8431315422058105, 3.9262638092041016, 0.6964904069900513, 1.9670552015304565, 2.3228797912597656, 1.0079389810562134, 2.320098638534546, 2.3517608642578125, 0.008096108213067055, 4.2466254234313965, 4.825156211853027, 0.7056344747543335, 2.149576425552368, 0.5730056166648865]], "avg_entropy": [2.1446563085191883], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "leg spinner Jimmy Matthews", "score_of_biggest_cluster": 0.09568020701408386, "score_of_first_answer": 0.09568020701408386, "max_score_of_first_answer": 0.9991528987884521, "min_score_of_first_answer": 4.599246850034433e-08, "avg_score": 0.09568020701408386, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37209154655408955, "std_of_first_answer": 0.37209154655408955, "avg_entropy_of_first_answer": 2.1446563085191883, "lex_sim": 1.0}
{"question": "three largest cities in the world by population", "answer": ["Tokyo, New Delhi, Shanghai"], "scores": [[0.5039169788360596, 0.8648126125335693, 0.0955239087343216, 0.12865054607391357, 0.9991115927696228, 0.8939502835273743, 0.12852518260478973, 0.9959337115287781]], "normalized_score": [0.3966147303581238], "tokens": [["\u2581Tokyo", ",", "\u2581New", "\u2581Del", "hi", ",", "\u2581Shang", "hai"]], "entropy": [[2.5195260047912598, 0.7468842267990112, 2.5193984508514404, 0.5733410120010376, 0.009876669384539127, 0.3926139771938324, 2.132784366607666, 0.033880334347486496]], "avg_entropy": [1.1160381302470341], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Beijing", "score_of_biggest_cluster": 0.3966147303581238, "score_of_first_answer": 0.3966147303581238, "max_score_of_first_answer": 0.9991115927696228, "min_score_of_first_answer": 0.0955239087343216, "avg_score": 0.3966147303581238, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38345152844752545, "std_of_first_answer": 0.38345152844752545, "avg_entropy_of_first_answer": 1.1160381302470341, "lex_sim": 1.0}
{"question": "what are the importance of rigor mortis in meat processing", "answer": ["Temperature and time"], "scores": [[0.004047753755003214, 0.9641223549842834, 0.8410688638687134, 0.15485627949237823, 0.28705093264579773]], "normalized_score": [0.170927956700325], "tokens": [["\u2581Tem", "per", "ature", "\u2581and", "\u2581time"]], "entropy": [[5.279081344604492, 0.1908176988363266, 0.8404140472412109, 2.783295154571533, 4.40812873840332]], "avg_entropy": [2.7003473967313765], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "tenderness of meat", "score_of_biggest_cluster": 0.170927956700325, "score_of_first_answer": 0.170927956700325, "max_score_of_first_answer": 0.9641223549842834, "min_score_of_first_answer": 0.004047753755003214, "avg_score": 0.170927956700325, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38204502605632784, "std_of_first_answer": 0.38204502605632784, "avg_entropy_of_first_answer": 2.7003473967313765, "lex_sim": 1.0}
{"question": "who's flag is red white and green", "answer": ["Argentina's"], "scores": [[0.009466638788580894, 0.06670030951499939, 0.9990481734275818]], "normalized_score": [0.08576366305351257], "tokens": [["\u2581Argentina", "'", "s"]], "entropy": [[4.849853515625, 0.5407778024673462, 0.010507081635296345]], "avg_entropy": [1.8003794665758808], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hungary", "score_of_biggest_cluster": 0.08576366305351257, "score_of_first_answer": 0.08576366305351257, "max_score_of_first_answer": 0.9990481734275818, "min_score_of_first_answer": 0.009466638788580894, "avg_score": 0.08576366305351257, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4536052917222264, "std_of_first_answer": 0.4536052917222264, "avg_entropy_of_first_answer": 1.8003794665758808, "lex_sim": 1.0}
{"question": "theme of the song roar by katy perry", "answer": ["breakup"], "scores": [[0.002742466749623418, 0.010677877813577652]], "normalized_score": [0.00541144423186779], "tokens": [["\u2581break", "up"]], "entropy": [[5.504944801330566, 1.884941577911377]], "avg_entropy": [3.6949431896209717], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "self-empowerment", "score_of_biggest_cluster": 0.00541144423186779, "score_of_first_answer": 0.00541144423186779, "max_score_of_first_answer": 0.010677877813577652, "min_score_of_first_answer": 0.002742466749623418, "avg_score": 0.00541144423186779, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.003967705531977117, "std_of_first_answer": 0.003967705531977117, "avg_entropy_of_first_answer": 3.6949431896209717, "lex_sim": 1.0}
{"question": "who has won the most olympic medals in curling", "answer": ["Sweden"], "scores": [[0.22410881519317627]], "normalized_score": [0.22410881519317627], "tokens": [["\u2581Sweden"]], "entropy": [[3.979386329650879]], "avg_entropy": [3.979386329650879], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Torger Nerg\u00e5rd", "score_of_biggest_cluster": 0.22410881519317627, "score_of_first_answer": 0.22410881519317627, "max_score_of_first_answer": 0.22410881519317627, "min_score_of_first_answer": 0.22410881519317627, "avg_score": 0.22410881519317627, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.979386329650879, "lex_sim": 1.0}
{"question": "when was the last god of war made", "answer": ["2018"], "scores": [[0.606069028377533, 0.9073596596717834, 0.9944487810134888, 0.7583826184272766, 0.24373185634613037]], "normalized_score": [0.632319986820221], "tokens": [["\u2581", "2", "0", "1", "8"]], "entropy": [[2.266240119934082, 0.44166308641433716, 0.05205531790852547, 0.5864317417144775, 1.7776862382888794]], "avg_entropy": [1.0248153008520604], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2018", "score_of_biggest_cluster": 0.632319986820221, "score_of_first_answer": 0.632319986820221, "max_score_of_first_answer": 0.9944487810134888, "min_score_of_first_answer": 0.24373185634613037, "avg_score": 0.632319986820221, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2646092673316322, "std_of_first_answer": 0.2646092673316322, "avg_entropy_of_first_answer": 1.0248153008520604, "lex_sim": 1.0}
{"question": "where do some of the natural air pollutants come from", "answer": ["Air pollution: is created both from man-made and natural sources."], "scores": [[0.007129893638193607, 0.33679163455963135, 0.4614444375038147, 0.001681838184595108, 0.011303042992949486, 0.0275043323636055, 0.0425051711499691, 0.05394696816802025, 0.02802216447889805, 0.7817626595497131, 0.9854100942611694, 0.4406811594963074, 0.9458147883415222, 0.7891799211502075, 0.743657648563385, 0.014705292880535126, 0.2685704827308655, 0.785922646522522, 0.2833283543586731, 0.9611234664916992, 0.993548572063446, 0.7744624614715576, 0.9924715757369995, 0.8823307752609253, 0.6072260737419128, 0.034412678331136703, 0.22368304431438446, 0.6102704405784607, 0.03891849145293236, 0.7183146476745605, 0.006515071261674166, 0.5292637348175049, 0.9998416900634766, 0.8447161912918091, 0.5470010638237, 0.2434600293636322, 0.7628006935119629, 0.052946582436561584, 0.4477854371070862, 0.7955648303031921, 0.8495476841926575, 0.00755756301805377, 0.9351773262023926, 0.14064213633537292, 0.8539875745773315, 0.7131427526473999, 0.8073299527168274, 0.5460667610168457, 0.962526261806488, 0.8840488791465759]], "normalized_score": [0.245869979262352], "tokens": [["\u2581Air", "\u2581poll", "ution", ":", "\u2581is", "\u2581created", "\u2581both", "\u2581from", "\u2581man", "-", "made", "\u2581and", "\u2581natural", "\u2581sources", ".", "\u2581Major", "\u2581sources", "\u2581of", "\u2581man", "-", "made", "\u2581air", "\u2581poll", "ution", "\u2581include", "\u2581industrial", "\u2581processes", ",", "\u2581autom", "obile", "\u2581tail", "pip", "es", ",", "\u2581and", "\u2581power", "\u2581plants", ",", "\u2581while", "\u2581natural", "\u2581sources", "\u2581come", "\u2581from", "\u2581forest", "\u2581fires", ",", "\u2581vol", "cano", "es", ","]], "entropy": [[5.582422256469727, 3.148773431777954, 0.7228317260742188, 2.692591667175293, 5.301181793212891, 2.8466570377349854, 1.6823344230651855, 1.8905037641525269, 1.4626212120056152, 0.8377987742424011, 0.1142556369304657, 1.8437025547027588, 0.29781845211982727, 0.9970701336860657, 1.2202565670013428, 3.7745585441589355, 2.284498691558838, 0.9148607850074768, 1.6798577308654785, 0.20545916259288788, 0.054709337651729584, 0.8741650581359863, 0.0697331577539444, 0.37491023540496826, 1.297314167022705, 3.273409843444824, 2.807328224182129, 1.3090227842330933, 3.083613872528076, 0.6862245798110962, 1.019826889038086, 1.0625386238098145, 0.001995452679693699, 0.537601113319397, 2.261538505554199, 3.4509365558624268, 0.9693197011947632, 0.8422775864601135, 2.410829544067383, 1.1436282396316528, 0.7368104457855225, 1.2421579360961914, 0.37468838691711426, 3.2544078826904297, 0.5472679734230042, 0.796360433101654, 1.2479381561279297, 0.6940934062004089, 0.18080824613571167, 0.4306778907775879]], "avg_entropy": [1.5307237714715303], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "wildfires", "score_of_biggest_cluster": 0.245869979262352, "score_of_first_answer": 0.245869979262352, "max_score_of_first_answer": 0.9998416900634766, "min_score_of_first_answer": 0.001681838184595108, "avg_score": 0.245869979262352, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36132494267236204, "std_of_first_answer": 0.36132494267236204, "avg_entropy_of_first_answer": 1.5307237714715303, "lex_sim": 1.0}
{"question": "who hit 6 sixes in an over in test cricket", "answer": ["Yuvraj Singh"], "scores": [[0.12330860644578934, 0.9530961513519287, 0.9958544969558716, 0.9590798020362854]], "normalized_score": [0.5788221955299377], "tokens": [["\u2581Y", "uv", "raj", "\u2581Singh"]], "entropy": [[3.9433910846710205, 0.35179686546325684, 0.03984001278877258, 0.2713448405265808]], "avg_entropy": [1.1515932008624077], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ravi Shastri", "score_of_biggest_cluster": 0.5788221955299377, "score_of_first_answer": 0.5788221955299377, "max_score_of_first_answer": 0.9958544969558716, "min_score_of_first_answer": 0.12330860644578934, "avg_score": 0.5788221955299377, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36670949422592036, "std_of_first_answer": 0.36670949422592036, "avg_entropy_of_first_answer": 1.1515932008624077, "lex_sim": 1.0}
{"question": "what was the last episode of the flash", "answer": ["Last Laugh"], "scores": [[0.000928371911868453, 0.02039712481200695, 0.9488183259963989]], "normalized_score": [0.026191361248493195], "tokens": [["\u2581Last", "\u2581L", "augh"]], "entropy": [[4.514897346496582, 4.343856334686279, 0.3828050196170807]], "avg_entropy": [3.080519566933314], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Elongated Knight Rises", "score_of_biggest_cluster": 0.026191361248493195, "score_of_first_answer": 0.026191361248493195, "max_score_of_first_answer": 0.9488183259963989, "min_score_of_first_answer": 0.000928371911868453, "avg_score": 0.026191361248493195, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44232219581589177, "std_of_first_answer": 0.44232219581589177, "avg_entropy_of_first_answer": 3.080519566933314, "lex_sim": 1.0}
{"question": "where's the tv show the crossing filmed", "answer": ["Bakersfield, California"], "scores": [[0.004280155524611473, 0.03127052262425423, 0.961107075214386, 0.7796006202697754, 0.7317488789558411]], "normalized_score": [0.14897732436656952], "tokens": [["\u2581B", "akers", "field", ",", "\u2581California"]], "entropy": [[5.961857795715332, 4.184659481048584, 0.2989892065525055, 0.7854553461074829, 0.9123848676681519]], "avg_entropy": [2.4286693394184113], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "British Columbia, Canada", "score_of_biggest_cluster": 0.14897732436656952, "score_of_first_answer": 0.14897732436656952, "max_score_of_first_answer": 0.961107075214386, "min_score_of_first_answer": 0.004280155524611473, "avg_score": 0.14897732436656952, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4024764461466376, "std_of_first_answer": 0.4024764461466376, "avg_entropy_of_first_answer": 2.4286693394184113, "lex_sim": 1.0}
{"question": "why wasnt agent matthews in miss congeniality 2", "answer": ["We don\u2019t know what reasons were behind director Donald Petrie\u2019s decision in letting the role go to an uncredited actress."], "scores": [[0.0024688998237252235, 0.1616312861442566, 0.39186328649520874, 0.9992913007736206, 0.5654550194740295, 0.014289505779743195, 0.003288760781288147, 0.1050051897764206, 0.15016499161720276, 0.005449166055768728, 0.05437452718615532, 0.9911278486251831, 0.9983819723129272, 0.8174042701721191, 0.9989917874336243, 0.7552794218063354, 0.005602132994681597, 0.0024157955776900053, 0.16756446659564972, 0.044143084436655045, 0.5952802300453186, 0.5221390128135681, 0.008328841999173164, 0.09316620230674744, 0.4183391332626343, 0.999174177646637, 0.30181804299354553, 0.47638559341430664]], "normalized_score": [0.12017722427845001], "tokens": [["\u2581We", "\u2581don", "\u2019", "t", "\u2581know", "\u2581what", "\u2581reasons", "\u2581were", "\u2581behind", "\u2581director", "\u2581Donald", "\u2581Pet", "rie", "\u2019", "s", "\u2581decision", "\u2581in", "\u2581letting", "\u2581the", "\u2581role", "\u2581go", "\u2581to", "\u2581an", "\u2581un", "cred", "ited", "\u2581actress", "."]], "entropy": [[4.492132663726807, 4.0185956954956055, 0.868340253829956, 0.00928498525172472, 1.969763994216919, 2.188375949859619, 3.2716665267944336, 3.871487617492676, 2.6376805305480957, 2.920132875442505, 4.823137283325195, 0.07018912583589554, 0.016598068177700043, 1.05427086353302, 0.013537234626710415, 1.1953222751617432, 1.8477904796600342, 3.360661029815674, 4.17679500579834, 2.789856433868408, 1.6658165454864502, 1.908408761024475, 5.378544807434082, 2.6284677982330322, 2.2743477821350098, 0.008296317420899868, 3.325571060180664, 2.3943114280700684]], "avg_entropy": [2.3278351211587767], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "relocated to Miami", "score_of_biggest_cluster": 0.12017722427845001, "score_of_first_answer": 0.12017722427845001, "max_score_of_first_answer": 0.9992913007736206, "min_score_of_first_answer": 0.0024157955776900053, "avg_score": 0.12017722427845001, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3711831972922579, "std_of_first_answer": 0.3711831972922579, "avg_entropy_of_first_answer": 2.3278351211587767, "lex_sim": 1.0}
{"question": "when was the last time dallas cowboys won the super bowl", "answer": ["1995"], "scores": [[0.5338811874389648, 0.8462475538253784, 0.9871089458465576, 0.9820059537887573, 0.5095032453536987]], "normalized_score": [0.7408215999603271], "tokens": [["\u2581", "1", "9", "9", "5"]], "entropy": [[2.40914249420166, 0.592725932598114, 0.10337459295988083, 0.11511147022247314, 0.9522989392280579]], "avg_entropy": [0.8345306858420372], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1995", "score_of_biggest_cluster": 0.7408215999603271, "score_of_first_answer": 0.7408215999603271, "max_score_of_first_answer": 0.9871089458465576, "min_score_of_first_answer": 0.5095032453536987, "avg_score": 0.7408215999603271, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.21047178409254808, "std_of_first_answer": 0.21047178409254808, "avg_entropy_of_first_answer": 0.8345306858420372, "lex_sim": 1.0}
{"question": "name of black man in to kill a mockingbird", "answer": ["Calpurny"], "scores": [[0.06989837437868118, 0.888188362121582, 0.9972108006477356, 0.00900269579142332]], "normalized_score": [0.15365013480186462], "tokens": [["\u2581Cal", "p", "urn", "y"]], "entropy": [[1.457896113395691, 0.5950933694839478, 0.034079428762197495, 0.4243992269039154]], "avg_entropy": [0.6278670346364379], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Thomas \"Tom\" Robinson", "score_of_biggest_cluster": 0.15365013480186462, "score_of_first_answer": 0.15365013480186462, "max_score_of_first_answer": 0.9972108006477356, "min_score_of_first_answer": 0.00900269579142332, "avg_score": 0.15365013480186462, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45377745854315654, "std_of_first_answer": 0.45377745854315654, "avg_entropy_of_first_answer": 0.6278670346364379, "lex_sim": 1.0}
{"question": "how do you say evil eye in greek", "answer": ["mavros o dromos (Image description: A gif of a smiling face with the text 'Evil Eye')"], "scores": [[0.11147385835647583, 0.14026851952075958, 0.3797276020050049, 0.49764567613601685, 0.03557403385639191, 0.30254992842674255, 0.9945306181907654, 0.008160186000168324, 4.640418410417624e-06, 0.0003353965003043413, 0.6434065699577332, 0.07228179275989532, 0.03236141800880432, 0.029917670413851738, 0.7034862041473389, 0.42347583174705505, 0.01624157465994358, 0.5975657105445862, 0.18654167652130127, 0.4471329152584076, 0.14839380979537964, 0.11420667171478271, 0.06530460715293884, 0.03084748238325119, 0.9604666233062744, 0.6819537878036499, 0.9832475781440735, 0.30106544494628906]], "normalized_score": [0.09566041082143784], "tokens": [["\u2581m", "av", "ros", "\u2581o", "\u2581d", "rom", "os", "\u2581(", "Image", "\u2581description", ":", "\u2581A", "\u2581g", "if", "\u2581of", "\u2581a", "\u2581sm", "iling", "\u2581face", "\u2581with", "\u2581the", "\u2581text", "\u2581'", "E", "vil", "\u2581E", "ye", "')"]], "entropy": [[4.276878356933594, 3.7585673332214355, 1.505337119102478, 2.49586820602417, 4.55449104309082, 2.303030490875244, 0.04902247339487076, 0.37990349531173706, 4.827704429626465, 3.705042839050293, 1.7060444355010986, 5.488726615905762, 5.82908296585083, 0.6873613595962524, 1.679656982421875, 3.291171073913574, 5.397702693939209, 1.1576060056686401, 4.574306011199951, 3.1000399589538574, 3.3366498947143555, 3.2572827339172363, 3.009927272796631, 3.2777185440063477, 0.30572348833084106, 1.0534497499465942, 0.10378149896860123, 2.3957104682922363]], "avg_entropy": [2.7681352693055357], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "matiasma", "score_of_biggest_cluster": 0.09566041082143784, "score_of_first_answer": 0.09566041082143784, "max_score_of_first_answer": 0.9945306181907654, "min_score_of_first_answer": 4.640418410417624e-06, "avg_score": 0.09566041082143784, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31872926774774646, "std_of_first_answer": 0.31872926774774646, "avg_entropy_of_first_answer": 2.7681352693055357, "lex_sim": 1.0}
{"question": "who recorded i can't help falling in love with you", "answer": ["Elvis Presley"], "scores": [[0.8007673621177673, 0.9958167672157288, 0.9603623747825623, 0.9962170720100403]], "normalized_score": [0.9345848560333252], "tokens": [["\u2581El", "vis", "\u2581Pres", "ley"]], "entropy": [[1.577253818511963, 0.032522667199373245, 0.25091034173965454, 0.030046695843338966]], "avg_entropy": [0.4726833808235824], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Elvis Presley", "score_of_biggest_cluster": 0.9345848560333252, "score_of_first_answer": 0.9345848560333252, "max_score_of_first_answer": 0.9962170720100403, "min_score_of_first_answer": 0.8007673621177673, "avg_score": 0.9345848560333252, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08072258090736051, "std_of_first_answer": 0.08072258090736051, "avg_entropy_of_first_answer": 0.4726833808235824, "lex_sim": 1.0}
{"question": "who wrote the song if i were a boy", "answer": ["Beyonce"], "scores": [[0.430330365896225, 0.4625353217124939]], "normalized_score": [0.4461423456668854], "tokens": [["\u2581Bey", "once"]], "entropy": [[3.353618621826172, 0.903766393661499]], "avg_entropy": [2.1286925077438354], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "BC Jean and Toby Gad", "score_of_biggest_cluster": 0.4461423456668854, "score_of_first_answer": 0.4461423456668854, "max_score_of_first_answer": 0.4625353217124939, "min_score_of_first_answer": 0.430330365896225, "avg_score": 0.4461423456668854, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01610247790813446, "std_of_first_answer": 0.01610247790813446, "avg_entropy_of_first_answer": 2.1286925077438354, "lex_sim": 1.0}
{"question": "how many times has psg won champions league", "answer": ["Seven"], "scores": [[0.0019866693764925003]], "normalized_score": [0.0019866691436618567], "tokens": [["\u2581Seven"]], "entropy": [[2.9637842178344727]], "avg_entropy": [2.9637842178344727], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "46", "score_of_biggest_cluster": 0.0019866691436618567, "score_of_first_answer": 0.0019866691436618567, "max_score_of_first_answer": 0.0019866693764925003, "min_score_of_first_answer": 0.0019866693764925003, "avg_score": 0.0019866691436618567, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.9637842178344727, "lex_sim": 1.0}
{"question": "who plays young agent o in mib 3", "answer": ["Josh."], "scores": [[0.18719545006752014, 0.0030165587086230516, 0.4601067900657654, 0.9083334803581238, 0.9970443844795227]], "normalized_score": [0.18807241320610046], "tokens": [["\u2581Josh", ".", "\u2581B", "rol", "in"]], "entropy": [[4.156484603881836, 1.2276735305786133, 2.8260161876678467, 0.4348905384540558, 0.02979481779038906]], "avg_entropy": [1.734971935674548], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alice Eve", "score_of_biggest_cluster": 0.18807241320610046, "score_of_first_answer": 0.18807241320610046, "max_score_of_first_answer": 0.9970443844795227, "min_score_of_first_answer": 0.0030165587086230516, "avg_score": 0.18807241320610046, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3897690657889969, "std_of_first_answer": 0.3897690657889969, "avg_entropy_of_first_answer": 1.734971935674548, "lex_sim": 1.0}
{"question": "when does the world cup of hockey start", "answer": ["September"], "scores": [[0.16330991685390472]], "normalized_score": [0.16330991685390472], "tokens": [["\u2581September"]], "entropy": [[2.901400089263916]], "avg_entropy": [2.901400089263916], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2020", "score_of_biggest_cluster": 0.16330991685390472, "score_of_first_answer": 0.16330991685390472, "max_score_of_first_answer": 0.16330991685390472, "min_score_of_first_answer": 0.16330991685390472, "avg_score": 0.16330991685390472, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.901400089263916, "lex_sim": 1.0}
{"question": "chemicals produced in one part of a plant that affect the growth and response of other parts", "answer": ["Second messengers"], "scores": [[0.021550657227635384, 0.5745141506195068, 0.7507427930831909]], "normalized_score": [0.2102571576833725], "tokens": [["\u2581Second", "\u2581mess", "engers"]], "entropy": [[2.5911319255828857, 0.9537489414215088, 0.6023732423782349]], "avg_entropy": [1.3824180364608765], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Plant hormones", "score_of_biggest_cluster": 0.2102571576833725, "score_of_first_answer": 0.2102571576833725, "max_score_of_first_answer": 0.7507427930831909, "min_score_of_first_answer": 0.021550657227635384, "avg_score": 0.2102571576833725, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31065277695028687, "std_of_first_answer": 0.31065277695028687, "avg_entropy_of_first_answer": 1.3824180364608765, "lex_sim": 1.0}
{"question": "who played violet in a series of unfortunate events netflix", "answer": ["Emily Browning"], "scores": [[0.0646098256111145, 0.9976444840431213, 0.5986729264259338, 0.9841878414154053]], "normalized_score": [0.44145405292510986], "tokens": [["\u2581Em", "ily", "\u2581Brow", "ning"]], "entropy": [[4.463686466217041, 0.020458396524190903, 2.121936321258545, 0.1075645163655281]], "avg_entropy": [1.6784114250913262], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Malina Weissman", "score_of_biggest_cluster": 0.44145405292510986, "score_of_first_answer": 0.44145405292510986, "max_score_of_first_answer": 0.9976444840431213, "min_score_of_first_answer": 0.0646098256111145, "avg_score": 0.44145405292510986, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3799162680229242, "std_of_first_answer": 0.3799162680229242, "avg_entropy_of_first_answer": 1.6784114250913262, "lex_sim": 1.0}
{"question": "when's the last time the philadelphia eagles played the new england patriots", "answer": ["Sunday, September 10, 2017"], "scores": [[0.01028478518128395, 0.7244559526443481, 0.10987594723701477, 0.9922268390655518, 0.33110281825065613, 0.15304462611675262, 0.6569666266441345, 0.9967551827430725, 0.9697821736335754, 0.999619722366333, 0.6731733083724976, 0.9707100987434387]], "normalized_score": [0.4005867540836334], "tokens": [["\u2581Sunday", ",", "\u2581September", "\u2581", "1", "0", ",", "\u2581", "2", "0", "1", "7"]], "entropy": [[2.8663594722747803, 1.3373405933380127, 2.1616930961608887, 0.05447080358862877, 1.5666389465332031, 2.239046573638916, 0.7760991454124451, 0.02680092304944992, 0.1363656222820282, 0.004560828674584627, 0.6391308903694153, 0.19129234552383423]], "avg_entropy": [0.9999832700705156], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in Super Bowl LII", "score_of_biggest_cluster": 0.4005867540836334, "score_of_first_answer": 0.4005867540836334, "max_score_of_first_answer": 0.999619722366333, "min_score_of_first_answer": 0.01028478518128395, "avg_score": 0.4005867540836334, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36705146674429523, "std_of_first_answer": 0.36705146674429523, "avg_entropy_of_first_answer": 0.9999832700705156, "lex_sim": 1.0}
{"question": "who sang what are we doing in love", "answer": ["Al Martino"], "scores": [[0.027606314048171043, 0.05932721868157387, 0.9966818690299988]], "normalized_score": [0.11774423718452454], "tokens": [["\u2581Al", "\u2581Mart", "ino"]], "entropy": [[5.788405418395996, 1.7237141132354736, 0.02710566110908985]], "avg_entropy": [2.5130750642468533], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dottie West", "score_of_biggest_cluster": 0.11774423718452454, "score_of_first_answer": 0.11774423718452454, "max_score_of_first_answer": 0.9966818690299988, "min_score_of_first_answer": 0.027606314048171043, "avg_score": 0.11774423718452454, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4495364757890951, "std_of_first_answer": 0.4495364757890951, "avg_entropy_of_first_answer": 2.5130750642468533, "lex_sim": 1.0}
{"question": "who created the borders of africa's countries", "answer": ["L.S."], "scores": [[0.005199159495532513, 0.1600772887468338, 0.6928675174713135, 0.9739310145378113, 0.002422038931399584, 0.9216748476028442, 0.9978082776069641]], "normalized_score": [0.14346618950366974], "tokens": [["\u2581L", ".", "S", ".", "\u2581Le", "ake", "y"]], "entropy": [[6.009521484375, 4.424089431762695, 1.7926371097564697, 0.23777499794960022, 4.056975364685059, 0.5139937996864319, 0.018969722092151642]], "avg_entropy": [2.436280272901058], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "European imperial powers", "score_of_biggest_cluster": 0.14346618950366974, "score_of_first_answer": 0.14346618950366974, "max_score_of_first_answer": 0.9978082776069641, "min_score_of_first_answer": 0.002422038931399584, "avg_score": 0.14346618950366974, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42864825113155625, "std_of_first_answer": 0.42864825113155625, "avg_entropy_of_first_answer": 2.436280272901058, "lex_sim": 1.0}
{"question": "when does the champions league round of 16 start", "answer": ["February 12 of each year, except years divisible by 3"], "scores": [[0.48635566234588623, 0.8924332857131958, 0.9507046341896057, 0.09791208058595657, 0.00013150088489055634, 0.06419417262077332, 0.9428739547729492, 0.013258152641355991, 0.061625562608242035, 0.01344121154397726, 0.16600792109966278, 0.9959558844566345, 0.9959027171134949, 0.5820023417472839, 0.15066520869731903]], "normalized_score": [0.12996697425842285], "tokens": [["\u2581February", "\u2581", "1", "2", "\u2581of", "\u2581each", "\u2581year", ",", "\u2581except", "\u2581years", "\u2581divis", "ible", "\u2581by", "\u2581", "3"]], "entropy": [[2.3370845317840576, 0.4160594046115875, 0.20542819797992706, 1.8703118562698364, 0.9736328125, 1.5073497295379639, 0.3933649957180023, 0.5819085240364075, 4.106892108917236, 2.278761625289917, 2.2592880725860596, 0.03512658551335335, 0.03983717039227486, 1.2871897220611572, 1.6651654243469238]], "avg_entropy": [1.3304933841029802], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "13 February", "score_of_biggest_cluster": 0.12996697425842285, "score_of_first_answer": 0.12996697425842285, "max_score_of_first_answer": 0.9959558844566345, "min_score_of_first_answer": 0.00013150088489055634, "avg_score": 0.12996697425842285, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40614109706425383, "std_of_first_answer": 0.40614109706425383, "avg_entropy_of_first_answer": 1.3304933841029802, "lex_sim": 1.0}
{"question": "who created separation of powers and checks and balances", "answer": ["James Madison"], "scores": [[0.3612908124923706, 0.969723105430603, 0.9988788962364197]], "normalized_score": [0.7047024965286255], "tokens": [["\u2581James", "\u2581Mad", "ison"]], "entropy": [[2.807234764099121, 0.25683194398880005, 0.012519825249910355]], "avg_entropy": [1.0255288444459438], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Montesquieu in the Enlightenment", "score_of_biggest_cluster": 0.7047024965286255, "score_of_first_answer": 0.7047024965286255, "max_score_of_first_answer": 0.9988788962364197, "min_score_of_first_answer": 0.3612908124923706, "avg_score": 0.7047024965286255, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29393092154450096, "std_of_first_answer": 0.29393092154450096, "avg_entropy_of_first_answer": 1.0255288444459438, "lex_sim": 1.0}
{"question": "who was president when the panama canal was built", "answer": ["Calvin Coolidge"], "scores": [[0.002492760308086872, 0.9891604781150818, 0.9734512567520142, 0.9997941851615906, 0.997841477394104]], "normalized_score": [0.29912087321281433], "tokens": [["\u2581Cal", "vin", "\u2581C", "ool", "idge"]], "entropy": [[2.425821304321289, 0.10055698454380035, 0.23933619260787964, 0.0025006297510117292, 0.021813606843352318]], "avg_entropy": [0.5580057436134667], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Theodore Roosevelt", "score_of_biggest_cluster": 0.29912087321281433, "score_of_first_answer": 0.29912087321281433, "max_score_of_first_answer": 0.9997941851615906, "min_score_of_first_answer": 0.002492760308086872, "avg_score": 0.29912087321281433, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3951369711235019, "std_of_first_answer": 0.3951369711235019, "avg_entropy_of_first_answer": 0.5580057436134667, "lex_sim": 1.0}
{"question": "who is the original singer of just when i needed you most", "answer": ["Randy Travis"], "scores": [[0.1130344346165657, 0.9698702096939087, 0.8063309788703918, 0.9996416568756104]], "normalized_score": [0.5452184677124023], "tokens": [["\u2581R", "andy", "\u2581Tra", "vis"]], "entropy": [[4.303508758544922, 0.19472165405750275, 1.095813274383545, 0.004372071009129286]], "avg_entropy": [1.3996039394987747], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Randy VanWarmer", "score_of_biggest_cluster": 0.5452184677124023, "score_of_first_answer": 0.5452184677124023, "max_score_of_first_answer": 0.9996416568756104, "min_score_of_first_answer": 0.1130344346165657, "avg_score": 0.5452184677124023, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35933101449125787, "std_of_first_answer": 0.35933101449125787, "avg_entropy_of_first_answer": 1.3996039394987747, "lex_sim": 1.0}
{"question": "when does god of war ghost of sparta take place", "answer": ["400BC"], "scores": [[0.46045905351638794, 0.3983191251754761, 0.3437020778656006, 0.9518933892250061, 0.03311983495950699]], "normalized_score": [0.28817489743232727], "tokens": [["\u2581", "4", "0", "0", "BC"]], "entropy": [[3.268622398376465, 1.7735446691513062, 2.007784366607666, 0.33195507526397705, 1.7283353805541992]], "avg_entropy": [1.8220483779907226], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ancient Greece", "score_of_biggest_cluster": 0.28817489743232727, "score_of_first_answer": 0.28817489743232727, "max_score_of_first_answer": 0.9518933892250061, "min_score_of_first_answer": 0.03311983495950699, "avg_score": 0.28817489743232727, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29630531079789274, "std_of_first_answer": 0.29630531079789274, "avg_entropy_of_first_answer": 1.8220483779907226, "lex_sim": 1.0}
{"question": "where does the this is us family live", "answer": ["Pittsburgh"], "scores": [[0.2151222974061966, 0.9193709492683411]], "normalized_score": [0.4447214901447296], "tokens": [["\u2581Pitts", "burgh"]], "entropy": [[4.5000386238098145, 0.34307944774627686]], "avg_entropy": [2.4215590357780457], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pittsburgh", "score_of_biggest_cluster": 0.4447214901447296, "score_of_first_answer": 0.4447214901447296, "max_score_of_first_answer": 0.9193709492683411, "min_score_of_first_answer": 0.2151222974061966, "avg_score": 0.4447214901447296, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.35212432593107224, "std_of_first_answer": 0.35212432593107224, "avg_entropy_of_first_answer": 2.4215590357780457, "lex_sim": 1.0}
{"question": "how many books are in the one piece series", "answer": ["97"], "scores": [[0.817908525466919, 0.1015075147151947, 0.033672649413347244]], "normalized_score": [0.14087265729904175], "tokens": [["\u2581", "9", "7"]], "entropy": [[1.31389582157135, 1.8240044116973877, 2.0837209224700928]], "avg_entropy": [1.7405403852462769], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "83", "score_of_biggest_cluster": 0.14087265729904175, "score_of_first_answer": 0.14087265729904175, "max_score_of_first_answer": 0.817908525466919, "min_score_of_first_answer": 0.033672649413347244, "avg_score": 0.14087265729904175, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35478598970930897, "std_of_first_answer": 0.35478598970930897, "avg_entropy_of_first_answer": 1.7405403852462769, "lex_sim": 1.0}
{"question": "is aluminium a ferrous or non ferrous metal", "answer": ["Non ferrous metal"], "scores": [[0.2486068159341812, 0.16167865693569183, 0.9998076558113098, 0.9966955184936523, 0.07563382387161255]], "normalized_score": [0.31352499127388], "tokens": [["\u2581Non", "\u2581f", "err", "ous", "\u2581metal"]], "entropy": [[2.234592914581299, 1.6351786851882935, 0.002699607517570257, 0.032446183264255524, 0.6829538941383362]], "avg_entropy": [0.9175742569379508], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "non-ferrous", "score_of_biggest_cluster": 0.31352499127388, "score_of_first_answer": 0.31352499127388, "max_score_of_first_answer": 0.9998076558113098, "min_score_of_first_answer": 0.07563382387161255, "avg_score": 0.31352499127388, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.413327679354739, "std_of_first_answer": 0.413327679354739, "avg_entropy_of_first_answer": 0.9175742569379508, "lex_sim": 1.0}
{"question": "when does the new season on the 100 come out", "answer": ["May"], "scores": [[0.02709289640188217]], "normalized_score": [0.027092894539237022], "tokens": [["\u2581May"]], "entropy": [[3.2792139053344727]], "avg_entropy": [3.2792139053344727], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "April 24, 2018", "score_of_biggest_cluster": 0.027092894539237022, "score_of_first_answer": 0.027092894539237022, "max_score_of_first_answer": 0.02709289640188217, "min_score_of_first_answer": 0.02709289640188217, "avg_score": 0.027092894539237022, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.2792139053344727, "lex_sim": 1.0}
{"question": "an edge that is between a vertex and itself is a", "answer": ["Self loop"], "scores": [[0.03547760844230652, 0.024491488933563232]], "normalized_score": [0.02947710081934929], "tokens": [["\u2581Self", "\u2581loop"]], "entropy": [[4.6704230308532715, 1.8597946166992188]], "avg_entropy": [3.265108823776245], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "loop", "score_of_biggest_cluster": 0.02947710081934929, "score_of_first_answer": 0.02947710081934929, "max_score_of_first_answer": 0.03547760844230652, "min_score_of_first_answer": 0.024491488933563232, "avg_score": 0.02947710081934929, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.005493059754371643, "std_of_first_answer": 0.005493059754371643, "avg_entropy_of_first_answer": 3.265108823776245, "lex_sim": 1.0}
{"question": "who expanded the palace of versailles to its present size", "answer": ["Jean-Baptiste Colbert"], "scores": [[0.0030376454815268517, 0.723297119140625, 0.8633095622062683, 0.9258020520210266, 0.981289803981781, 0.701603353023529, 0.9992675185203552]], "normalized_score": [0.38296443223953247], "tokens": [["\u2581Jean", "-", "B", "apt", "iste", "\u2581Col", "bert"]], "entropy": [[1.7562243938446045, 1.6375073194503784, 0.8561204671859741, 0.3389471769332886, 0.10905778408050537, 1.4384737014770508, 0.00862317904829979]], "avg_entropy": [0.8778505745743003], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Louis XIV", "score_of_biggest_cluster": 0.38296443223953247, "score_of_first_answer": 0.38296443223953247, "max_score_of_first_answer": 0.9992675185203552, "min_score_of_first_answer": 0.0030376454815268517, "avg_score": 0.38296443223953247, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3207113923158807, "std_of_first_answer": 0.3207113923158807, "avg_entropy_of_first_answer": 0.8778505745743003, "lex_sim": 1.0}
{"question": "when did las vegas become a gambling town", "answer": ["1941"], "scores": [[0.7077481150627136, 0.9934036135673523, 0.9661306142807007, 0.06329468637704849, 0.7964236736297607]], "normalized_score": [0.5092257261276245], "tokens": [["\u2581", "1", "9", "4", "1"]], "entropy": [[1.860252022743225, 0.05144481733441353, 0.1721096783876419, 0.5319519639015198, 0.8922581076622009]], "avg_entropy": [0.7016033180058002], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1931", "score_of_biggest_cluster": 0.5092257261276245, "score_of_first_answer": 0.5092257261276245, "max_score_of_first_answer": 0.9934036135673523, "min_score_of_first_answer": 0.06329468637704849, "avg_score": 0.5092257261276245, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33808773749995114, "std_of_first_answer": 0.33808773749995114, "avg_entropy_of_first_answer": 0.7016033180058002, "lex_sim": 1.0}
{"question": "how many pages are in the book inside out and back again", "answer": ["272"], "scores": [[0.9245936274528503, 0.4439280331134796, 0.06888795644044876, 0.43553265929222107]], "normalized_score": [0.33312472701072693], "tokens": [["\u2581", "2", "7", "2"]], "entropy": [[0.6751092672348022, 1.3309874534606934, 2.149096727371216, 1.949228048324585]], "avg_entropy": [1.526105374097824], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "263", "score_of_biggest_cluster": 0.33312472701072693, "score_of_first_answer": 0.33312472701072693, "max_score_of_first_answer": 0.9245936274528503, "min_score_of_first_answer": 0.06888795644044876, "avg_score": 0.33312472701072693, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30389205722431206, "std_of_first_answer": 0.30389205722431206, "avg_entropy_of_first_answer": 1.526105374097824, "lex_sim": 1.0}
{"question": "who is the founder of the ubuntu project", "answer": ["Mark Shuttleworth"], "scores": [[0.922479510307312, 0.9849547147750854, 0.9986751675605774, 0.9979793429374695, 0.9982041120529175]], "normalized_score": [0.9800034761428833], "tokens": [["\u2581Mark", "\u2581Sh", "utt", "le", "worth"]], "entropy": [[0.5924241542816162, 0.13289763033390045, 0.013290042988955975, 0.02010444551706314, 0.01927109807729721]], "avg_entropy": [0.1555974742397666], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mark Richard Shuttleworth", "score_of_biggest_cluster": 0.9800034761428833, "score_of_first_answer": 0.9800034761428833, "max_score_of_first_answer": 0.9986751675605774, "min_score_of_first_answer": 0.922479510307312, "avg_score": 0.9800034761428833, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.02944660542218428, "std_of_first_answer": 0.02944660542218428, "avg_entropy_of_first_answer": 0.1555974742397666, "lex_sim": 1.0}
{"question": "song have i told you lately i love you", "answer": ["Rod Stewart"], "scores": [[0.22401531040668488, 0.9884017109870911]], "normalized_score": [0.4705498218536377], "tokens": [["\u2581Rod", "\u2581Stewart"]], "entropy": [[4.5564374923706055, 0.09028949588537216]], "avg_entropy": [2.323363494127989], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Have I Told You Lately", "score_of_biggest_cluster": 0.4705498218536377, "score_of_first_answer": 0.4705498218536377, "max_score_of_first_answer": 0.9884017109870911, "min_score_of_first_answer": 0.22401531040668488, "avg_score": 0.4705498218536377, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3821932002902031, "std_of_first_answer": 0.3821932002902031, "avg_entropy_of_first_answer": 2.323363494127989, "lex_sim": 1.0}
{"question": "when did fortnite battle royal come out on xbox", "answer": ["mid-2015"], "scores": [[0.00039277508039958775, 0.3736472725868225, 0.6059976816177368, 0.9995877146720886, 0.9968005418777466, 0.015640202909708023]], "normalized_score": [0.10559047758579254], "tokens": [["\u2581mid", "-", "2", "0", "1", "5"]], "entropy": [[2.929421901702881, 2.2637624740600586, 1.967107892036438, 0.0047264788299798965, 0.025907687842845917, 0.6376632452011108]], "avg_entropy": [1.304764946612219], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "September 2017", "score_of_biggest_cluster": 0.10559047758579254, "score_of_first_answer": 0.10559047758579254, "max_score_of_first_answer": 0.9995877146720886, "min_score_of_first_answer": 0.00039277508039958775, "avg_score": 0.10559047758579254, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40983746465928284, "std_of_first_answer": 0.40983746465928284, "avg_entropy_of_first_answer": 1.304764946612219, "lex_sim": 1.0}
{"question": "another name for the torso is the blank region of the body", "answer": ["Trunk"], "scores": [[0.010244622826576233, 0.973515510559082]], "normalized_score": [0.09986640512943268], "tokens": [["\u2581Tr", "unk"]], "entropy": [[5.4258294105529785, 0.1674138456583023]], "avg_entropy": [2.7966216281056404], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "trunk", "score_of_biggest_cluster": 0.09986640512943268, "score_of_first_answer": 0.09986640512943268, "max_score_of_first_answer": 0.973515510559082, "min_score_of_first_answer": 0.010244622826576233, "avg_score": 0.09986640512943268, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4816354438662529, "std_of_first_answer": 0.4816354438662529, "avg_entropy_of_first_answer": 2.7966216281056404, "lex_sim": 1.0}
{"question": "the gulf stream the world's fastest ocean current flows along the western side of this water body", "answer": ["Atlantic Ocean"], "scores": [[0.4524472951889038, 0.555324375629425]], "normalized_score": [0.5012534260749817], "tokens": [["\u2581Atlantic", "\u2581Ocean"]], "entropy": [[2.881744146347046, 0.9514838457107544]], "avg_entropy": [1.9166139960289001], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Atlantic ocean", "score_of_biggest_cluster": 0.5012534260749817, "score_of_first_answer": 0.5012534260749817, "max_score_of_first_answer": 0.555324375629425, "min_score_of_first_answer": 0.4524472951889038, "avg_score": 0.5012534260749817, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.05143854022026062, "std_of_first_answer": 0.05143854022026062, "avg_entropy_of_first_answer": 1.9166139960289001, "lex_sim": 1.0}
{"question": "where did the allies go after north africa", "answer": ["Suez Canal"], "scores": [[0.0047343517653644085, 0.30694580078125, 0.8212023973464966]], "normalized_score": [0.10606955736875534], "tokens": [["\u2581S", "uez", "\u2581Canal"]], "entropy": [[3.974947452545166, 2.115345001220703, 0.7360896468162537]], "avg_entropy": [2.275460700194041], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Italy", "score_of_biggest_cluster": 0.10606955736875534, "score_of_first_answer": 0.10606955736875534, "max_score_of_first_answer": 0.8212023973464966, "min_score_of_first_answer": 0.0047343517653644085, "avg_score": 0.10606955736875534, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33704791556530056, "std_of_first_answer": 0.33704791556530056, "avg_entropy_of_first_answer": 2.275460700194041, "lex_sim": 1.0}
{"question": "who has won the canada open women's doubles", "answer": ["Aleksandra Wozniak"], "scores": [[0.008478531613945961, 0.9742825627326965, 0.3134118318557739, 0.9925472140312195, 0.9943692088127136, 0.9858747124671936]], "normalized_score": [0.36887022852897644], "tokens": [["\u2581Aleks", "andra", "\u2581W", "oz", "nia", "k"]], "entropy": [[4.902555465698242, 0.18683862686157227, 2.172081470489502, 0.0678204894065857, 0.05577883496880531, 0.09136588871479034]], "avg_entropy": [1.2460734626899164], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wakana Nagahara", "score_of_biggest_cluster": 0.36887022852897644, "score_of_first_answer": 0.36887022852897644, "max_score_of_first_answer": 0.9943692088127136, "min_score_of_first_answer": 0.008478531613945961, "avg_score": 0.36887022852897644, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39917666480304725, "std_of_first_answer": 0.39917666480304725, "avg_entropy_of_first_answer": 1.2460734626899164, "lex_sim": 1.0}
{"question": "who played the original wonder woman on tv", "answer": ["Lynda Carter"], "scores": [[0.7661589980125427, 0.9992733597755432, 0.9995501637458801, 0.987006664276123]], "normalized_score": [0.932249128818512], "tokens": [["\u2581Ly", "nd", "a", "\u2581Carter"]], "entropy": [[1.0925495624542236, 0.007542512379586697, 0.005254920106381178, 0.12417066097259521]], "avg_entropy": [0.3073794139781967], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lynda Carter", "score_of_biggest_cluster": 0.932249128818512, "score_of_first_answer": 0.932249128818512, "max_score_of_first_answer": 0.9995501637458801, "min_score_of_first_answer": 0.7661589980125427, "avg_score": 0.932249128818512, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.09934011067239792, "std_of_first_answer": 0.09934011067239792, "avg_entropy_of_first_answer": 0.3073794139781967, "lex_sim": 1.0}
{"question": "who wrote the song going to kansas city", "answer": ["Leon Russell"], "scores": [[0.02199767716228962, 0.11116992682218552]], "normalized_score": [0.0494518019258976], "tokens": [["\u2581Leon", "\u2581Russell"]], "entropy": [[5.213995933532715, 3.9913971424102783]], "avg_entropy": [4.602696537971497], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jerry Leiber and Mike Stoller", "score_of_biggest_cluster": 0.0494518019258976, "score_of_first_answer": 0.0494518019258976, "max_score_of_first_answer": 0.11116992682218552, "min_score_of_first_answer": 0.02199767716228962, "avg_score": 0.0494518019258976, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04458612482994795, "std_of_first_answer": 0.04458612482994795, "avg_entropy_of_first_answer": 4.602696537971497, "lex_sim": 1.0}
{"question": "what is the legal age for marriage in australia", "answer": ["18"], "scores": [[0.8064072728157043, 0.9959891438484192, 0.8776851892471313]], "normalized_score": [0.8899848461151123], "tokens": [["\u2581", "1", "8"]], "entropy": [[1.4662760496139526, 0.03154893219470978, 0.47296470403671265]], "avg_entropy": [0.6569298952817917], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "18", "score_of_biggest_cluster": 0.8899848461151123, "score_of_first_answer": 0.8899848461151123, "max_score_of_first_answer": 0.9959891438484192, "min_score_of_first_answer": 0.8064072728157043, "avg_score": 0.8899848461151123, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.07818614033856355, "std_of_first_answer": 0.07818614033856355, "avg_entropy_of_first_answer": 0.6569298952817917, "lex_sim": 1.0}
{"question": "what kind of sentence contains an independent clause and a dependent clause", "answer": ["Complex"], "scores": [[0.14985598623752594]], "normalized_score": [0.14985597133636475], "tokens": [["\u2581Complex"]], "entropy": [[3.6048216819763184]], "avg_entropy": [3.6048216819763184], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A complex sentence", "score_of_biggest_cluster": 0.14985597133636475, "score_of_first_answer": 0.14985597133636475, "max_score_of_first_answer": 0.14985598623752594, "min_score_of_first_answer": 0.14985598623752594, "avg_score": 0.14985597133636475, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.6048216819763184, "lex_sim": 1.0}
{"question": "who does the voice of amy on futurama", "answer": ["Tress MacNeille"], "scores": [[0.739536464214325, 0.8930293917655945, 0.9709416627883911, 0.8843533396720886, 0.9883589744567871]], "normalized_score": [0.8906596899032593], "tokens": [["\u2581T", "ress", "\u2581Mac", "Ne", "ille"]], "entropy": [[1.3883123397827148, 0.5468323230743408, 0.24194912612438202, 0.5129452347755432, 0.0789957270026207]], "avg_entropy": [0.5538069501519203], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lauren Tom", "score_of_biggest_cluster": 0.8906596899032593, "score_of_first_answer": 0.8906596899032593, "max_score_of_first_answer": 0.9883589744567871, "min_score_of_first_answer": 0.739536464214325, "avg_score": 0.8906596899032593, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08805553053859208, "std_of_first_answer": 0.08805553053859208, "avg_entropy_of_first_answer": 0.5538069501519203, "lex_sim": 1.0}
{"question": "who played truman capote in in cold blood", "answer": ["Capote"], "scores": [[0.002483905991539359, 0.9857712984085083]], "normalized_score": [0.049482960253953934], "tokens": [["\u2581Cap", "ote"]], "entropy": [[2.6048669815063477, 0.11431898176670074]], "avg_entropy": [1.3595929816365242], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Philip Seymour Hoffman", "score_of_biggest_cluster": 0.049482960253953934, "score_of_first_answer": 0.049482960253953934, "max_score_of_first_answer": 0.9857712984085083, "min_score_of_first_answer": 0.002483905991539359, "avg_score": 0.049482960253953934, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.49164369620848447, "std_of_first_answer": 0.49164369620848447, "avg_entropy_of_first_answer": 1.3595929816365242, "lex_sim": 1.0}
{"question": "1. what was the precursor to the present day internet", "answer": ["ARPANET"], "scores": [[0.46352502703666687, 0.7498602867126465, 0.9872370958328247, 0.8973785042762756]], "normalized_score": [0.7449252009391785], "tokens": [["\u2581AR", "P", "AN", "ET"]], "entropy": [[2.884568214416504, 0.6496660709381104, 0.0893917977809906, 0.33892226219177246]], "avg_entropy": [0.9906370863318443], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the ARPANET project", "score_of_biggest_cluster": 0.7449252009391785, "score_of_first_answer": 0.7449252009391785, "max_score_of_first_answer": 0.9872370958328247, "min_score_of_first_answer": 0.46352502703666687, "avg_score": 0.7449252009391785, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1985376211431848, "std_of_first_answer": 0.1985376211431848, "avg_entropy_of_first_answer": 0.9906370863318443, "lex_sim": 1.0}
{"question": "who was originally cast to play indiana jones", "answer": ["Sean Connery"], "scores": [[0.07956834137439728, 0.9996616840362549, 0.926045298576355, 0.998251736164093, 0.9978557229042053]], "normalized_score": [0.5930710434913635], "tokens": [["\u2581Se", "an", "\u2581Con", "n", "ery"]], "entropy": [[3.9613852500915527, 0.004121794365346432, 0.4409131109714508, 0.017362412065267563, 0.01843041367828846]], "avg_entropy": [0.8884425962343812], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tom Selleck", "score_of_biggest_cluster": 0.5930710434913635, "score_of_first_answer": 0.5930710434913635, "max_score_of_first_answer": 0.9996616840362549, "min_score_of_first_answer": 0.07956834137439728, "avg_score": 0.5930710434913635, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3614482634280209, "std_of_first_answer": 0.3614482634280209, "avg_entropy_of_first_answer": 0.8884425962343812, "lex_sim": 1.0}
{"question": "what has been the origin of most classical dances of india", "answer": ["Bhakti movements"], "scores": [[0.011552471667528152, 0.6733223795890808, 0.9512679576873779, 0.009347702376544476]], "normalized_score": [0.09119611978530884], "tokens": [["\u2581Bh", "ak", "ti", "\u2581movements"]], "entropy": [[5.011197090148926, 1.3135207891464233, 0.2912372946739197, 2.6022579669952393]], "avg_entropy": [2.304553285241127], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Sanskrit text Natya Shastra", "score_of_biggest_cluster": 0.09119611978530884, "score_of_first_answer": 0.09119611978530884, "max_score_of_first_answer": 0.9512679576873779, "min_score_of_first_answer": 0.009347702376544476, "avg_score": 0.09119611978530884, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4127907574086691, "std_of_first_answer": 0.4127907574086691, "avg_entropy_of_first_answer": 2.304553285241127, "lex_sim": 1.0}
{"question": "what is the weight of a honda fit", "answer": ["2385lbs"], "scores": [[0.8954776525497437, 0.625558078289032, 0.08549767732620239, 0.07149478048086166, 0.14488214254379272, 0.058349549770355225, 0.9985408782958984]], "normalized_score": [0.2246813327074051], "tokens": [["\u2581", "2", "3", "8", "5", "l", "bs"]], "entropy": [[0.8767590522766113, 1.1297521591186523, 1.9492597579956055, 2.236509084701538, 2.2339017391204834, 2.0496654510498047, 0.013698950409889221]], "avg_entropy": [1.4985065992389406], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1,020\u20131,080\u00a0kg", "score_of_biggest_cluster": 0.2246813327074051, "score_of_first_answer": 0.2246813327074051, "max_score_of_first_answer": 0.9985408782958984, "min_score_of_first_answer": 0.058349549770355225, "avg_score": 0.2246813327074051, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38588553782019164, "std_of_first_answer": 0.38588553782019164, "avg_entropy_of_first_answer": 1.4985065992389406, "lex_sim": 1.0}
{"question": "who did university of miami play last week", "answer": ["San Diego State"], "scores": [[0.0056691644713282585, 0.8460783362388611, 0.8570513129234314]], "normalized_score": [0.16019372642040253], "tokens": [["\u2581San", "\u2581Diego", "\u2581State"]], "entropy": [[4.5838093757629395, 0.5599350929260254, 0.6161575317382812]], "avg_entropy": [1.9199673334757488], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "FIU", "score_of_biggest_cluster": 0.16019372642040253, "score_of_first_answer": 0.16019372642040253, "max_score_of_first_answer": 0.8570513129234314, "min_score_of_first_answer": 0.0056691644713282585, "avg_score": 0.16019372642040253, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3987842001928737, "std_of_first_answer": 0.3987842001928737, "avg_entropy_of_first_answer": 1.9199673334757488, "lex_sim": 1.0}
{"question": "who won the battle of the first battle of bull run", "answer": ["North (Union Army)"], "scores": [[0.008068239316344261, 0.004581234883517027, 0.37755176424980164, 0.04613494127988815, 0.9704685807228088]], "normalized_score": [0.05743146687746048], "tokens": [["\u2581North", "\u2581(", "Union", "\u2581Army", ")"]], "entropy": [[2.7338273525238037, 0.9755599498748779, 2.526144027709961, 0.6306976079940796, 0.24954691529273987]], "avg_entropy": [1.4231551706790924], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Confederate victory", "score_of_biggest_cluster": 0.05743146687746048, "score_of_first_answer": 0.05743146687746048, "max_score_of_first_answer": 0.9704685807228088, "min_score_of_first_answer": 0.004581234883517027, "avg_score": 0.05743146687746048, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3716851331591348, "std_of_first_answer": 0.3716851331591348, "avg_entropy_of_first_answer": 1.4231551706790924, "lex_sim": 1.0}
{"question": "when does season 8 of hawaii five o premiere", "answer": ["September 30, 2018"], "scores": [[0.5729910135269165, 0.9101911187171936, 0.016892313957214355, 0.9744957089424133, 0.31583353877067566, 0.9887153506278992, 0.9972851276397705, 0.999406099319458, 0.9915816187858582, 0.42634910345077515]], "normalized_score": [0.5073333978652954], "tokens": [["\u2581September", "\u2581", "3", "0", ",", "\u2581", "2", "0", "1", "8"]], "entropy": [[2.0772407054901123, 0.37756380438804626, 0.18036486208438873, 0.1634370982646942, 1.182158350944519, 0.07413056492805481, 0.023727737367153168, 0.007169500458985567, 0.057286642491817474, 1.6951682567596436]], "avg_entropy": [0.5838247523177416], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "September 29, 2017", "score_of_biggest_cluster": 0.5073333978652954, "score_of_first_answer": 0.5073333978652954, "max_score_of_first_answer": 0.999406099319458, "min_score_of_first_answer": 0.016892313957214355, "avg_score": 0.5073333978652954, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34167375863075633, "std_of_first_answer": 0.34167375863075633, "avg_entropy_of_first_answer": 0.5838247523177416, "lex_sim": 1.0}
{"question": "whens the last time the patriots played the eagles", "answer": ["February 3, 2013"], "scores": [[0.017662888392806053, 0.9708362817764282, 0.07380320131778717, 0.49995487928390503, 0.9916627407073975, 0.9541032314300537, 0.9998323917388916, 0.7201664447784424, 0.4571225941181183]], "normalized_score": [0.3875119388103485], "tokens": [["\u2581February", "\u2581", "3", ",", "\u2581", "2", "0", "1", "3"]], "entropy": [[2.7566099166870117, 0.17490345239639282, 1.9018709659576416, 0.8775595426559448, 0.054137155413627625, 0.18734651803970337, 0.0021317016799002886, 0.635643482208252, 1.6314400434494019]], "avg_entropy": [0.9135158642764307], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Super Bowl LII", "score_of_biggest_cluster": 0.3875119388103485, "score_of_first_answer": 0.3875119388103485, "max_score_of_first_answer": 0.9998323917388916, "min_score_of_first_answer": 0.017662888392806053, "avg_score": 0.3875119388103485, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3693842718868971, "std_of_first_answer": 0.3693842718868971, "avg_entropy_of_first_answer": 0.9135158642764307, "lex_sim": 1.0}
{"question": "what is the largest city in the united states per square mile", "answer": ["New York City"], "scores": [[0.5919842720031738, 0.9837919473648071, 0.5930104851722717]], "normalized_score": [0.7016038298606873], "tokens": [["\u2581New", "\u2581York", "\u2581City"]], "entropy": [[2.3382210731506348, 0.10686540603637695, 0.9278100728988647]], "avg_entropy": [1.1242988506952922], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sitka", "score_of_biggest_cluster": 0.7016038298606873, "score_of_first_answer": 0.7016038298606873, "max_score_of_first_answer": 0.9837919473648071, "min_score_of_first_answer": 0.5919842720031738, "avg_score": 0.7016038298606873, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18445850445111903, "std_of_first_answer": 0.18445850445111903, "avg_entropy_of_first_answer": 1.1242988506952922, "lex_sim": 1.0}
{"question": "when did the rational dress society begin to work", "answer": ["March 23, 1881"], "scores": [[0.008981282822787762, 0.9588689804077148, 0.3349805474281311, 0.07570374757051468, 0.6603205800056458, 0.9867291450500488, 0.9963692426681519, 0.9667861461639404, 0.8177099823951721, 0.7160546779632568]], "normalized_score": [0.38944700360298157], "tokens": [["\u2581March", "\u2581", "2", "3", ",", "\u2581", "1", "8", "8", "1"]], "entropy": [[1.4428648948669434, 0.23288753628730774, 1.435865044593811, 2.318474769592285, 0.8835152387619019, 0.07665680348873138, 0.025018790736794472, 0.15388594567775726, 0.754845142364502, 0.9617060422897339]], "avg_entropy": [0.8285720208659768], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1881", "score_of_biggest_cluster": 0.38944700360298157, "score_of_first_answer": 0.38944700360298157, "max_score_of_first_answer": 0.9963692426681519, "min_score_of_first_answer": 0.008981282822787762, "avg_score": 0.38944700360298157, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3607791352373851, "std_of_first_answer": 0.3607791352373851, "avg_entropy_of_first_answer": 0.8285720208659768, "lex_sim": 1.0}
{"question": "who plays the dragon queen from game of thrones", "answer": ["Emilia Clarke"], "scores": [[0.578018307685852, 0.9963343143463135, 0.9888954162597656, 0.999723494052887]], "normalized_score": [0.8686488270759583], "tokens": [["\u2581Em", "ilia", "\u2581Clar", "ke"]], "entropy": [[1.343196988105774, 0.02987532690167427, 0.09404569864273071, 0.0034564912784844637]], "avg_entropy": [0.36764362623216584], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Emilia Clarke", "score_of_biggest_cluster": 0.8686488270759583, "score_of_first_answer": 0.8686488270759583, "max_score_of_first_answer": 0.999723494052887, "min_score_of_first_answer": 0.578018307685852, "avg_score": 0.8686488270759583, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1805940916461191, "std_of_first_answer": 0.1805940916461191, "avg_entropy_of_first_answer": 0.36764362623216584, "lex_sim": 1.0}
{"question": "who was executed for being an american spy during the revolutionary war", "answer": ["Nathan Hale"], "scores": [[0.895586371421814, 0.9709332585334778, 0.9982080459594727]], "normalized_score": [0.9539068341255188], "tokens": [["\u2581Nathan", "\u2581H", "ale"]], "entropy": [[0.7078160643577576, 0.2262071669101715, 0.015553132630884647]], "avg_entropy": [0.3165254546329379], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nathan Hale", "score_of_biggest_cluster": 0.9539068341255188, "score_of_first_answer": 0.9539068341255188, "max_score_of_first_answer": 0.9982080459594727, "min_score_of_first_answer": 0.895586371421814, "avg_score": 0.9539068341255188, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.04340030147292139, "std_of_first_answer": 0.04340030147292139, "avg_entropy_of_first_answer": 0.3165254546329379, "lex_sim": 1.0}
{"question": "what is the main character's name in layer cake", "answer": ["Brian Cox"], "scores": [[0.31787827610969543, 0.1746339350938797, 0.9169833660125732]], "normalized_score": [0.3706098794937134], "tokens": [["\u2581Brian", "\u2581C", "ox"]], "entropy": [[3.253952980041504, 2.129981279373169, 0.5330562591552734]], "avg_entropy": [1.9723301728566487], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "XXXX", "score_of_biggest_cluster": 0.3706098794937134, "score_of_first_answer": 0.3706098794937134, "max_score_of_first_answer": 0.9169833660125732, "min_score_of_first_answer": 0.1746339350938797, "avg_score": 0.3706098794937134, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3215463553182234, "std_of_first_answer": 0.3215463553182234, "avg_entropy_of_first_answer": 1.9723301728566487, "lex_sim": 1.0}
{"question": "cast of law & order special victim unit", "answer": ["Mariska Hargitay, Ice-T, Dann Floreck, Danny Pino, Richard Belzer, Christopher Meloni"], "scores": [[0.41160815954208374, 0.9990131855010986, 0.9729887843132019, 0.9998006224632263, 0.9975805282592773, 0.9963430762290955, 0.7342154383659363, 0.3400387167930603, 0.6593404412269592, 0.9911820888519287, 0.885182797908783, 0.1665155440568924, 0.7081301808357239, 0.9951320886611938, 0.9931923747062683, 0.00034659940865822136, 0.7760254144668579, 0.067884162068367, 0.32107487320899963, 0.9773409366607666, 0.9998728036880493, 0.8216606378555298, 0.6724863648414612, 0.9418438673019409, 0.9948863387107849, 0.8353998064994812, 0.06575201451778412, 0.9974508881568909, 0.9852885603904724]], "normalized_score": [0.4905579090118408], "tokens": [["\u2581Mar", "iska", "\u2581H", "arg", "it", "ay", ",", "\u2581Ice", "-", "T", ",", "\u2581D", "ann", "\u2581Fl", "ore", "ck", ",", "\u2581D", "anny", "\u2581P", "ino", ",", "\u2581Richard", "\u2581Bel", "zer", ",", "\u2581Christopher", "\u2581Mel", "oni"]], "entropy": [[3.64473032951355, 0.010289661586284637, 0.2295483648777008, 0.0025538220070302486, 0.023211369290947914, 0.03676125407218933, 1.0171375274658203, 2.3407037258148193, 0.7326105237007141, 0.06419064104557037, 0.48480868339538574, 2.391119956970215, 0.6934239268302917, 0.050263650715351105, 0.05197175592184067, 0.02828921377658844, 0.7324251532554626, 2.1073076725006104, 1.095888376235962, 0.16350716352462769, 0.0015252642333507538, 0.6021590232849121, 1.378703236579895, 0.2655864953994751, 0.044594839215278625, 0.5893195867538452, 2.810997247695923, 0.025950055569410324, 0.08120916038751602]], "avg_entropy": [0.7483030235041576], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kelli Giddish", "score_of_biggest_cluster": 0.4905579090118408, "score_of_first_answer": 0.4905579090118408, "max_score_of_first_answer": 0.9998728036880493, "min_score_of_first_answer": 0.00034659940865822136, "avg_score": 0.4905579090118408, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32927958666704266, "std_of_first_answer": 0.32927958666704266, "avg_entropy_of_first_answer": 0.7483030235041576, "lex_sim": 1.0}
{"question": "who played the beast in the movie beauty and the beast", "answer": ["Robby Benson"], "scores": [[0.006948582828044891, 0.8943809270858765, 0.966752290725708, 0.9999078512191772]], "normalized_score": [0.27840274572372437], "tokens": [["\u2581Rob", "by", "\u2581B", "enson"]], "entropy": [[1.0514299869537354, 0.6448804140090942, 0.3026394844055176, 0.0012485357001423836]], "avg_entropy": [0.5000496052671224], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dan Stevens", "score_of_biggest_cluster": 0.27840274572372437, "score_of_first_answer": 0.27840274572372437, "max_score_of_first_answer": 0.9999078512191772, "min_score_of_first_answer": 0.006948582828044891, "avg_score": 0.27840274572372437, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4117189779750159, "std_of_first_answer": 0.4117189779750159, "avg_entropy_of_first_answer": 0.5000496052671224, "lex_sim": 1.0}
{"question": "what was the name of darth vader star destroyer", "answer": ["Executor"], "scores": [[0.3920513987541199, 0.975709855556488]], "normalized_score": [0.6184887886047363], "tokens": [["\u2581Exec", "utor"]], "entropy": [[2.279904842376709, 0.14393895864486694]], "avg_entropy": [1.211921900510788], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Devastator", "score_of_biggest_cluster": 0.6184887886047363, "score_of_first_answer": 0.6184887886047363, "max_score_of_first_answer": 0.975709855556488, "min_score_of_first_answer": 0.3920513987541199, "avg_score": 0.6184887886047363, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2918292284011841, "std_of_first_answer": 0.2918292284011841, "avg_entropy_of_first_answer": 1.211921900510788, "lex_sim": 1.0}
{"question": "the temperature of gas is proportional to the", "answer": ["cube of its pressure"], "scores": [[0.003903165925294161, 0.7319990992546082, 0.25034284591674805, 0.23762965202331543]], "normalized_score": [0.11418022215366364], "tokens": [["\u2581cube", "\u2581of", "\u2581its", "\u2581pressure"]], "entropy": [[3.619987726211548, 0.9366267323493958, 1.8644124269485474, 1.7902870178222656]], "avg_entropy": [2.052828475832939], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "volume", "score_of_biggest_cluster": 0.11418022215366364, "score_of_first_answer": 0.11418022215366364, "max_score_of_first_answer": 0.7319990992546082, "min_score_of_first_answer": 0.003903165925294161, "avg_score": 0.11418022215366364, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26481594980558126, "std_of_first_answer": 0.26481594980558126, "avg_entropy_of_first_answer": 2.052828475832939, "lex_sim": 1.0}
{"question": "who owns the majority of the us debt", "answer": ["The USA"], "scores": [[0.13249266147613525, 0.006642101798206568]], "normalized_score": [0.029665295034646988], "tokens": [["\u2581The", "\u2581USA"]], "entropy": [[3.143217086791992, 2.0787699222564697]], "avg_entropy": [2.610993504524231], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "China", "score_of_biggest_cluster": 0.029665295034646988, "score_of_first_answer": 0.029665295034646988, "max_score_of_first_answer": 0.13249266147613525, "min_score_of_first_answer": 0.006642101798206568, "avg_score": 0.029665295034646988, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06292527983896434, "std_of_first_answer": 0.06292527983896434, "avg_entropy_of_first_answer": 2.610993504524231, "lex_sim": 1.0}
{"question": "most of new zealands land supports which economic activity", "answer": ["Horticulture and pastoral farming"], "scores": [[0.004303479101508856, 0.9516048431396484, 0.9953649044036865, 0.9612829685211182, 0.06525703519582748, 0.05424472317099571, 0.972861111164093, 0.40991777181625366, 0.9870339632034302]], "normalized_score": [0.2601606547832489], "tokens": [["\u2581H", "ort", "icult", "ure", "\u2581and", "\u2581past", "oral", "\u2581far", "ming"]], "entropy": [[3.289971351623535, 0.28220275044441223, 0.04240236058831215, 0.16798561811447144, 0.7599611878395081, 3.3653664588928223, 0.17587575316429138, 1.6425845623016357, 0.0766361802816391]], "avg_entropy": [1.089220691472292], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "agriculture", "score_of_biggest_cluster": 0.2601606547832489, "score_of_first_answer": 0.2601606547832489, "max_score_of_first_answer": 0.9953649044036865, "min_score_of_first_answer": 0.004303479101508856, "avg_score": 0.2601606547832489, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4312861287652165, "std_of_first_answer": 0.4312861287652165, "avg_entropy_of_first_answer": 1.089220691472292, "lex_sim": 1.0}
{"question": "where is fe best absorbed in the body", "answer": ["stomach"], "scores": [[0.05308685824275017, 0.9982594847679138, 0.9981258511543274]], "normalized_score": [0.3753806948661804], "tokens": [["\u2581st", "om", "ach"]], "entropy": [[5.126320838928223, 0.01725626364350319, 0.018687542527914047]], "avg_entropy": [1.72075488169988], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in the duodenum", "score_of_biggest_cluster": 0.3753806948661804, "score_of_first_answer": 0.3753806948661804, "max_score_of_first_answer": 0.9982594847679138, "min_score_of_first_answer": 0.05308685824275017, "avg_score": 0.3753806948661804, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44552715466733556, "std_of_first_answer": 0.44552715466733556, "avg_entropy_of_first_answer": 1.72075488169988, "lex_sim": 1.0}
{"question": "latest series of keeping up with the kardashians", "answer": ["KUWTK"], "scores": [[0.16092871129512787, 0.15731807053089142, 0.9870136380195618, 0.9977256655693054]], "normalized_score": [0.39736223220825195], "tokens": [["\u2581K", "U", "WT", "K"]], "entropy": [[3.806180000305176, 1.9951967000961304, 0.07293074578046799, 0.021963588893413544]], "avg_entropy": [1.474067758768797], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "14", "score_of_biggest_cluster": 0.39736223220825195, "score_of_first_answer": 0.39736223220825195, "max_score_of_first_answer": 0.9977256655693054, "min_score_of_first_answer": 0.15731807053089142, "avg_score": 0.39736223220825195, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41664229964183647, "std_of_first_answer": 0.41664229964183647, "avg_entropy_of_first_answer": 1.474067758768797, "lex_sim": 1.0}
{"question": "when did how you remind me come out", "answer": ["December 23, 2008"], "scores": [[0.016763128340244293, 0.9652279615402222, 0.4494604170322418, 0.05518821254372597, 0.6821475028991699, 0.9923632144927979, 0.9826206564903259, 0.999775230884552, 0.9018427729606628, 0.12121648341417313]], "normalized_score": [0.3519696891307831], "tokens": [["\u2581December", "\u2581", "2", "3", ",", "\u2581", "2", "0", "0", "8"]], "entropy": [[2.0821146965026855, 0.18945252895355225, 1.4512765407562256, 2.02668833732605, 0.7773113250732422, 0.05230306088924408, 0.0914926528930664, 0.0028638632502406836, 0.3229382634162903, 1.8024158477783203]], "avg_entropy": [0.8798857116838917], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "August\u00a021,\u00a02001", "score_of_biggest_cluster": 0.3519696891307831, "score_of_first_answer": 0.3519696891307831, "max_score_of_first_answer": 0.999775230884552, "min_score_of_first_answer": 0.016763128340244293, "avg_score": 0.3519696891307831, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3971802666122143, "std_of_first_answer": 0.3971802666122143, "avg_entropy_of_first_answer": 0.8798857116838917, "lex_sim": 1.0}
{"question": "when did the battles of lexington and concord happen", "answer": ["Massachusetts"], "scores": [[0.0019512242870405316]], "normalized_score": [0.0019512241706252098], "tokens": [["\u2581Massachusetts"]], "entropy": [[1.328183889389038]], "avg_entropy": [1.328183889389038], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "April 19, 1775", "score_of_biggest_cluster": 0.0019512241706252098, "score_of_first_answer": 0.0019512241706252098, "max_score_of_first_answer": 0.0019512242870405316, "min_score_of_first_answer": 0.0019512242870405316, "avg_score": 0.0019512241706252098, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.328183889389038, "lex_sim": 1.0}
{"question": "what is the meaning of veronica in english", "answer": ["(English) True image"], "scores": [[0.00606051879003644, 0.005188242997974157, 0.3810078203678131, 0.013024996966123581, 0.2291703075170517]], "normalized_score": [0.032410066574811935], "tokens": [["\u2581(", "English", ")", "\u2581True", "\u2581image"]], "entropy": [[5.880006790161133, 6.003334045410156, 3.5480518341064453, 4.850736618041992, 2.810927391052246]], "avg_entropy": [4.6186113357543945], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "she who brings victory", "score_of_biggest_cluster": 0.032410066574811935, "score_of_first_answer": 0.032410066574811935, "max_score_of_first_answer": 0.3810078203678131, "min_score_of_first_answer": 0.005188242997974157, "avg_score": 0.032410066574811935, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.153240626490701, "std_of_first_answer": 0.153240626490701, "avg_entropy_of_first_answer": 4.6186113357543945, "lex_sim": 1.0}
{"question": "who heads the executive department of west virginia government", "answer": ["Joe Manchin III"], "scores": [[0.008060340769588947, 0.971047580242157, 0.9965928196907043, 0.1750534474849701]], "normalized_score": [0.1922297328710556], "tokens": [["\u2581Joe", "\u2581Man", "chin", "\u2581III"]], "entropy": [[4.195593357086182, 0.2665005922317505, 0.02549399435520172, 0.6761441230773926]], "avg_entropy": [1.2909330166876316], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Governor of West Virginia", "score_of_biggest_cluster": 0.1922297328710556, "score_of_first_answer": 0.1922297328710556, "max_score_of_first_answer": 0.9965928196907043, "min_score_of_first_answer": 0.008060340769588947, "avg_score": 0.1922297328710556, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45011205144427086, "std_of_first_answer": 0.45011205144427086, "avg_entropy_of_first_answer": 1.2909330166876316, "lex_sim": 1.0}
{"question": "when was half life 2 episode 2 released", "answer": ["March 19, 2006"], "scores": [[0.040861088782548904, 0.985382080078125, 0.5949375629425049, 0.8782786726951599, 0.6398313641548157, 0.9943222999572754, 0.9993447661399841, 0.9996918439865112, 0.8316425681114197, 0.009930198080837727]], "normalized_score": [0.40207651257514954], "tokens": [["\u2581March", "\u2581", "1", "9", ",", "\u2581", "2", "0", "0", "6"]], "entropy": [[2.154611110687256, 0.09462054818868637, 1.0072054862976074, 0.6335296630859375, 0.8048874139785767, 0.03710047900676727, 0.006246764212846756, 0.003692668629810214, 0.45519569516181946, 0.6486098766326904]], "avg_entropy": [0.5845699705881998], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in 2007", "score_of_biggest_cluster": 0.40207651257514954, "score_of_first_answer": 0.40207651257514954, "max_score_of_first_answer": 0.9996918439865112, "min_score_of_first_answer": 0.009930198080837727, "avg_score": 0.40207651257514954, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3635734392269709, "std_of_first_answer": 0.3635734392269709, "avg_entropy_of_first_answer": 0.5845699705881998, "lex_sim": 1.0}
{"question": "what is the purpose of a jake brake", "answer": ["To slow down the vehicle and save the engine."], "scores": [[0.19943293929100037, 0.2366502285003662, 0.3259899914264679, 0.2918631434440613, 0.235160693526268, 0.044729579240083694, 0.012457609176635742, 0.21469169855117798, 0.07922396063804626, 0.09175761789083481]], "normalized_score": [0.1248244047164917], "tokens": [["\u2581To", "\u2581slow", "\u2581down", "\u2581the", "\u2581vehicle", "\u2581and", "\u2581save", "\u2581the", "\u2581engine", "."]], "entropy": [[4.192885398864746, 3.644082546234131, 1.9801549911499023, 2.356426954269409, 3.0938527584075928, 2.7107388973236084, 4.327415466308594, 2.0228497982025146, 2.927717685699463, 2.009152412414551]], "avg_entropy": [2.9265276908874513], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "slowing the vehicle", "score_of_biggest_cluster": 0.1248244047164917, "score_of_first_answer": 0.1248244047164917, "max_score_of_first_answer": 0.3259899914264679, "min_score_of_first_answer": 0.012457609176635742, "avg_score": 0.1248244047164917, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10271690849042478, "std_of_first_answer": 0.10271690849042478, "avg_entropy_of_first_answer": 2.9265276908874513, "lex_sim": 1.0}
{"question": "when was the last time minnesota vikings was in the super bowl", "answer": ["1980"], "scores": [[0.5856643915176392, 0.47983798384666443, 0.9720182418823242, 0.039912424981594086, 0.27776628732681274]], "normalized_score": [0.31350255012512207], "tokens": [["\u2581", "1", "9", "8", "0"]], "entropy": [[2.1171083450317383, 0.9275267124176025, 0.19724515080451965, 1.1291768550872803, 1.6349711418151855]], "avg_entropy": [1.2012056410312653], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1969", "score_of_biggest_cluster": 0.31350255012512207, "score_of_first_answer": 0.31350255012512207, "max_score_of_first_answer": 0.9720182418823242, "min_score_of_first_answer": 0.039912424981594086, "avg_score": 0.31350255012512207, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31222462056721245, "std_of_first_answer": 0.31222462056721245, "avg_entropy_of_first_answer": 1.2012056410312653, "lex_sim": 1.0}
{"question": "who sings don't take your guns to town", "answer": ["Elvis Presley"], "scores": [[0.0026054270565509796, 0.41580647230148315, 0.87714022397995, 0.9968061447143555]], "normalized_score": [0.17543348670005798], "tokens": [["\u2581El", "vis", "\u2581Pres", "ley"]], "entropy": [[4.006779670715332, 2.22701096534729, 0.6669639348983765, 0.025490570813417435]], "avg_entropy": [1.731561285443604], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "U2", "score_of_biggest_cluster": 0.17543348670005798, "score_of_first_answer": 0.17543348670005798, "max_score_of_first_answer": 0.9968061447143555, "min_score_of_first_answer": 0.0026054270565509796, "avg_score": 0.17543348670005798, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39438961258027067, "std_of_first_answer": 0.39438961258027067, "avg_entropy_of_first_answer": 1.731561285443604, "lex_sim": 1.0}
{"question": "who sponsored the phoenix open before waste management", "answer": ["Waste Management"], "scores": [[0.10763929039239883, 0.9706662893295288, 0.990423858165741]], "normalized_score": [0.4694838225841522], "tokens": [["\u2581W", "aste", "\u2581Management"]], "entropy": [[4.881825923919678, 0.2476818859577179, 0.08095769584178925]], "avg_entropy": [1.7368218352397282], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Friedman Billings Ramsey", "score_of_biggest_cluster": 0.4694838225841522, "score_of_first_answer": 0.4694838225841522, "max_score_of_first_answer": 0.990423858165741, "min_score_of_first_answer": 0.10763929039239883, "avg_score": 0.4694838225841522, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4115707790512281, "std_of_first_answer": 0.4115707790512281, "avg_entropy_of_first_answer": 1.7368218352397282, "lex_sim": 1.0}
{"question": "who does tony end up with on skins", "answer": ["Franky"], "scores": [[0.009465157985687256, 0.9832252860069275]], "normalized_score": [0.09646958857774734], "tokens": [["\u2581Fran", "ky"]], "entropy": [[4.103285789489746, 0.12742221355438232]], "avg_entropy": [2.115354001522064], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Michelle", "score_of_biggest_cluster": 0.09646958857774734, "score_of_first_answer": 0.09646958857774734, "max_score_of_first_answer": 0.9832252860069275, "min_score_of_first_answer": 0.009465157985687256, "avg_score": 0.09646958857774734, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4868800640106201, "std_of_first_answer": 0.4868800640106201, "avg_entropy_of_first_answer": 2.115354001522064, "lex_sim": 1.0}
{"question": "when was it was not death for i stood up published", "answer": ["1892"], "scores": [[0.6545850038528442, 0.9389733672142029, 0.3480309247970581, 0.239854097366333, 0.18475301563739777]], "normalized_score": [0.39387205243110657], "tokens": [["\u2581", "1", "8", "9", "2"]], "entropy": [[2.1418042182922363, 0.28624656796455383, 1.0040236711502075, 1.947106957435608, 2.198511838912964]], "avg_entropy": [1.515538650751114], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1890", "score_of_biggest_cluster": 0.39387205243110657, "score_of_first_answer": 0.39387205243110657, "max_score_of_first_answer": 0.9389733672142029, "min_score_of_first_answer": 0.18475301563739777, "avg_score": 0.39387205243110657, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28395936750331874, "std_of_first_answer": 0.28395936750331874, "avg_entropy_of_first_answer": 1.515538650751114, "lex_sim": 1.0}
{"question": "when did they stop saying mass in latin", "answer": ["in the beginning"], "scores": [[0.024614861235022545, 0.41196274757385254, 0.001957534346729517]], "normalized_score": [0.027076227590441704], "tokens": [["\u2581in", "\u2581the", "\u2581beginning"]], "entropy": [[2.202237367630005, 0.9515585899353027, 1.5150928497314453]], "avg_entropy": [1.5562962690989177], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "from 1967 onwards", "score_of_biggest_cluster": 0.027076227590441704, "score_of_first_answer": 0.027076227590441704, "max_score_of_first_answer": 0.41196274757385254, "min_score_of_first_answer": 0.001957534346729517, "avg_score": 0.027076227590441704, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18816541611226031, "std_of_first_answer": 0.18816541611226031, "avg_entropy_of_first_answer": 1.5562962690989177, "lex_sim": 1.0}
{"question": "when do new episodes of ncis los angeles return", "answer": ["September 26"], "scores": [[0.21845494210720062, 0.9155530333518982, 0.9262102842330933, 0.08745948225259781]], "normalized_score": [0.3567717373371124], "tokens": [["\u2581September", "\u2581", "2", "6"]], "entropy": [[3.4353621006011963, 0.35343271493911743, 0.36312225461006165, 2.2135775089263916]], "avg_entropy": [1.5913736447691917], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "May\u00a013,\u00a02018", "score_of_biggest_cluster": 0.3567717373371124, "score_of_first_answer": 0.3567717373371124, "max_score_of_first_answer": 0.9262102842330933, "min_score_of_first_answer": 0.08745948225259781, "avg_score": 0.3567717373371124, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38676370352409445, "std_of_first_answer": 0.38676370352409445, "avg_entropy_of_first_answer": 1.5913736447691917, "lex_sim": 1.0}
{"question": "what are bulls used for on a farm", "answer": ["animal husbandry"], "scores": [[0.00659826397895813, 0.608311653137207, 0.9974773526191711]], "normalized_score": [0.15878869593143463], "tokens": [["\u2581animal", "\u2581husband", "ry"]], "entropy": [[4.2449631690979, 2.2069735527038574, 0.02494088187813759]], "avg_entropy": [2.1589592012266317], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "as work oxen", "score_of_biggest_cluster": 0.15878869593143463, "score_of_first_answer": 0.15878869593143463, "max_score_of_first_answer": 0.9974773526191711, "min_score_of_first_answer": 0.00659826397895813, "avg_score": 0.15878869593143463, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40761505705615764, "std_of_first_answer": 0.40761505705615764, "avg_entropy_of_first_answer": 2.1589592012266317, "lex_sim": 1.0}
{"question": "who plays the voice of johnny in sing", "answer": ["Taron Egerton"], "scores": [[0.7698430418968201, 0.9277582168579102, 0.9334983229637146, 0.9985803365707397]], "normalized_score": [0.9033028483390808], "tokens": [["\u2581T", "aron", "\u2581Eg", "erton"]], "entropy": [[1.5865609645843506, 0.588104248046875, 0.4135574698448181, 0.013845402747392654]], "avg_entropy": [0.6505170213058591], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Taron Egerton", "score_of_biggest_cluster": 0.9033028483390808, "score_of_first_answer": 0.9033028483390808, "max_score_of_first_answer": 0.9985803365707397, "min_score_of_first_answer": 0.7698430418968201, "avg_score": 0.9033028483390808, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08415959297396255, "std_of_first_answer": 0.08415959297396255, "avg_entropy_of_first_answer": 0.6505170213058591, "lex_sim": 1.0}
{"question": "what is the name of the restaurant in seinfeld", "answer": ["Monk's"], "scores": [[0.3652980923652649, 0.9786968231201172, 0.5844160318374634, 0.9981316924095154]], "normalized_score": [0.6757738590240479], "tokens": [["\u2581Mon", "k", "'", "s"]], "entropy": [[3.5541088581085205, 0.13048040866851807, 0.881950855255127, 0.021008221432566643]], "avg_entropy": [1.146887085866183], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Monk\u2019s", "score_of_biggest_cluster": 0.6757738590240479, "score_of_first_answer": 0.6757738590240479, "max_score_of_first_answer": 0.9981316924095154, "min_score_of_first_answer": 0.3652980923652649, "avg_score": 0.6757738590240479, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2682984280559282, "std_of_first_answer": 0.2682984280559282, "avg_entropy_of_first_answer": 1.146887085866183, "lex_sim": 1.0}
{"question": "where is the boy who played charlie in willy wonka", "answer": ["Michael Borden"], "scores": [[0.009535735473036766, 0.647544264793396, 0.03877295181155205]], "normalized_score": [0.062094178050756454], "tokens": [["\u2581Michael", "\u2581B", "orden"]], "entropy": [[3.3630051612854004, 2.3435168266296387, 2.818931818008423]], "avg_entropy": [2.841817935307821], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Peter Gardner[4] Ostrum", "score_of_biggest_cluster": 0.062094178050756454, "score_of_first_answer": 0.062094178050756454, "max_score_of_first_answer": 0.647544264793396, "min_score_of_first_answer": 0.009535735473036766, "avg_score": 0.062094178050756454, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2941111297554777, "std_of_first_answer": 0.2941111297554777, "avg_entropy_of_first_answer": 2.841817935307821, "lex_sim": 1.0}
{"question": "who came up with the initial concept of protons and neutrons", "answer": ["Geiger & H.A.W."], "scores": [[0.0022177109494805336, 0.2510281801223755, 0.01546644326299429, 0.016173800453543663, 0.2926914393901825, 0.051502443850040436, 0.9858369827270508, 0.05044809728860855, 0.8337721824645996, 0.057013221085071564, 0.005001244600862265, 0.13715699315071106, 0.9922448992729187, 0.97870934009552, 0.3747689127922058, 0.3498852252960205]], "normalized_score": [0.10960046201944351], "tokens": [["\u2581Ge", "iger", "\u2581&", "\u2581H", ".", "A", ".", "W", ".", "\u2581Koch", ",", "\u2581", "1", "9", "2", "8"]], "entropy": [[3.6967597007751465, 3.4190497398376465, 1.407320261001587, 3.991415500640869, 2.4989898204803467, 3.1138856410980225, 0.13048158586025238, 3.4719440937042236, 1.1308562755584717, 3.6882402896881104, 0.5075734853744507, 5.145397186279297, 0.06363719701766968, 0.12632514536380768, 0.953999400138855, 1.8892793655395508]], "avg_entropy": [2.202197168022394], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dmitri Ivanenko", "score_of_biggest_cluster": 0.10960046201944351, "score_of_first_answer": 0.10960046201944351, "max_score_of_first_answer": 0.9922448992729187, "min_score_of_first_answer": 0.0022177109494805336, "avg_score": 0.10960046201944351, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3733649172899543, "std_of_first_answer": 0.3733649172899543, "avg_entropy_of_first_answer": 2.202197168022394, "lex_sim": 1.0}
{"question": "where can i get a state issued id", "answer": ["Minnesota"], "scores": [[0.0011186494957655668]], "normalized_score": [0.0011186492629349232], "tokens": [["\u2581Minnesota"]], "entropy": [[5.098875522613525]], "avg_entropy": [5.098875522613525], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "each state's DMV", "score_of_biggest_cluster": 0.0011186492629349232, "score_of_first_answer": 0.0011186492629349232, "max_score_of_first_answer": 0.0011186494957655668, "min_score_of_first_answer": 0.0011186494957655668, "avg_score": 0.0011186492629349232, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.098875522613525, "lex_sim": 1.0}
{"question": "who sold the most records elvis or the beatles", "answer": ["The Beatles"], "scores": [[0.3923768103122711, 0.9957700371742249, 0.9995177984237671]], "normalized_score": [0.7309443354606628], "tokens": [["\u2581The", "\u2581Beat", "les"]], "entropy": [[1.5025010108947754, 0.0456816703081131, 0.004948639776557684]], "avg_entropy": [0.5177104403264821], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Beatles", "score_of_biggest_cluster": 0.7309443354606628, "score_of_first_answer": 0.7309443354606628, "max_score_of_first_answer": 0.9995177984237671, "min_score_of_first_answer": 0.3923768103122711, "avg_score": 0.7309443354606628, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2853297529725268, "std_of_first_answer": 0.2853297529725268, "avg_entropy_of_first_answer": 0.5177104403264821, "lex_sim": 1.0}
{"question": "who sings somebody's watching me with michael jackson", "answer": ["Michael Jackson"], "scores": [[0.26417893171310425, 0.939814031124115]], "normalized_score": [0.49827611446380615], "tokens": [["\u2581Michael", "\u2581Jackson"]], "entropy": [[4.004169464111328, 0.45706456899642944]], "avg_entropy": [2.230617016553879], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jermaine Jackson", "score_of_biggest_cluster": 0.49827611446380615, "score_of_first_answer": 0.49827611446380615, "max_score_of_first_answer": 0.939814031124115, "min_score_of_first_answer": 0.26417893171310425, "avg_score": 0.49827611446380615, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33781754970550537, "std_of_first_answer": 0.33781754970550537, "avg_entropy_of_first_answer": 2.230617016553879, "lex_sim": 1.0}
{"question": "who has won india's next super star", "answer": ["Diljit Dosanjh"], "scores": [[0.028130166232585907, 0.07837136834859848, 0.38867560029029846, 0.9792948365211487, 0.823724627494812, 0.9962764382362366, 0.9956240057945251, 0.9993645548820496, 0.9601244330406189]], "normalized_score": [0.4430572986602783], "tokens": [["\u2581D", "il", "j", "it", "\u2581D", "os", "an", "j", "h"]], "entropy": [[4.898468494415283, 3.3109002113342285, 1.7448434829711914, 0.13475260138511658, 0.7776970863342285, 0.03111083433032036, 0.043319497257471085, 0.007048200815916061, 0.2203359156847]], "avg_entropy": [1.2409418138364952], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Natasha Bharadwaj", "score_of_biggest_cluster": 0.4430572986602783, "score_of_first_answer": 0.4430572986602783, "max_score_of_first_answer": 0.9993645548820496, "min_score_of_first_answer": 0.028130166232585907, "avg_score": 0.4430572986602783, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3887647448851121, "std_of_first_answer": 0.3887647448851121, "avg_entropy_of_first_answer": 1.2409418138364952, "lex_sim": 1.0}
{"question": "who sings the song i want to go outside in the rain", "answer": ["Layla Fowler"], "scores": [[0.01361135020852089, 0.011947146616876125, 0.1725437045097351, 0.006725084036588669, 0.0027889113407582045, 0.962997555732727]], "normalized_score": [0.0282360278069973], "tokens": [["\u2581L", "ay", "la", "\u2581F", "ow", "ler"]], "entropy": [[6.252362251281738, 4.262495040893555, 2.840630054473877, 3.187594413757324, 3.592690944671631, 0.2516099810600281]], "avg_entropy": [3.397897114356359], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Milira", "score_of_biggest_cluster": 0.0282360278069973, "score_of_first_answer": 0.0282360278069973, "max_score_of_first_answer": 0.962997555732727, "min_score_of_first_answer": 0.0027889113407582045, "avg_score": 0.0282360278069973, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3485989108861497, "std_of_first_answer": 0.3485989108861497, "avg_entropy_of_first_answer": 3.397897114356359, "lex_sim": 1.0}
{"question": "who said that religion is the opiate of the masses", "answer": ["Karl Marx"], "scores": [[0.8544361591339111, 0.9661476612091064]], "normalized_score": [0.9085766077041626], "tokens": [["\u2581Karl", "\u2581Marx"]], "entropy": [[0.949982762336731, 0.22760887444019318]], "avg_entropy": [0.5887958183884621], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Karl Marx.", "score_of_biggest_cluster": 0.9085766077041626, "score_of_first_answer": 0.9085766077041626, "max_score_of_first_answer": 0.9661476612091064, "min_score_of_first_answer": 0.8544361591339111, "avg_score": 0.9085766077041626, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.055855751037597656, "std_of_first_answer": 0.055855751037597656, "avg_entropy_of_first_answer": 0.5887958183884621, "lex_sim": 1.0}
{"question": "what was the immediate catalyst to the civil war", "answer": ["The election of Abraham Lincoln as 16th President of the USA."], "scores": [[0.0888163149356842, 0.17111696302890778, 0.9800038933753967, 0.5676869750022888, 0.9966159462928772, 0.23771102726459503, 0.010804377496242523, 0.997663140296936, 0.9860109090805054, 0.9921684861183167, 0.50654137134552, 0.753517210483551, 0.9394420981407166, 0.01983795501291752, 0.08325850963592529]], "normalized_score": [0.29726946353912354], "tokens": [["\u2581The", "\u2581election", "\u2581of", "\u2581Abraham", "\u2581Lincoln", "\u2581as", "\u2581", "1", "6", "th", "\u2581President", "\u2581of", "\u2581the", "\u2581USA", "."]], "entropy": [[4.582851409912109, 4.330309867858887, 0.16182181239128113, 1.5807247161865234, 0.028996901586651802, 1.5264395475387573, 1.1927666664123535, 0.020461970940232277, 0.09594152122735977, 0.0655391737818718, 1.070772409439087, 0.8235566020011902, 0.3273944854736328, 0.5429350137710571, 1.1915466785430908]], "avg_entropy": [1.1694705851376057], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "slavery", "score_of_biggest_cluster": 0.29726946353912354, "score_of_first_answer": 0.29726946353912354, "max_score_of_first_answer": 0.997663140296936, "min_score_of_first_answer": 0.010804377496242523, "avg_score": 0.29726946353912354, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4007205824012704, "std_of_first_answer": 0.4007205824012704, "avg_entropy_of_first_answer": 1.1694705851376057, "lex_sim": 1.0}
{"question": "when did bihar bifurcate from bengal and some parts of chota nagpur merged into bengal", "answer": ["January 1, 1912"], "scores": [[0.042659275233745575, 0.9785818457603455, 0.5362451076507568, 0.5101370811462402, 0.9843196272850037, 0.5913918614387512, 0.9981855750083923, 0.9701598286628723, 0.9939350485801697]], "normalized_score": [0.5704684853553772], "tokens": [["\u2581January", "\u2581", "1", ",", "\u2581", "1", "9", "1", "2"]], "entropy": [[2.516632556915283, 0.1370951533317566, 1.0104225873947144, 1.3123024702072144, 0.08867698162794113, 0.6767207384109497, 0.014811480417847633, 0.18221014738082886, 0.04868720471858978]], "avg_entropy": [0.6652843689339029], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1912", "score_of_biggest_cluster": 0.5704684853553772, "score_of_first_answer": 0.5704684853553772, "max_score_of_first_answer": 0.9981855750083923, "min_score_of_first_answer": 0.042659275233745575, "avg_score": 0.5704684853553772, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3167758712785624, "std_of_first_answer": 0.3167758712785624, "avg_entropy_of_first_answer": 0.6652843689339029, "lex_sim": 1.0}
{"question": "the outer layer of the skin that contains no blood or nerve supply is the", "answer": ["epidermis"], "scores": [[0.45258837938308716, 0.9947405457496643, 0.9983450174331665, 0.991965651512146]], "normalized_score": [0.8171421885490417], "tokens": [["\u2581e", "pid", "erm", "is"]], "entropy": [[2.552201509475708, 0.04179796203970909, 0.017679188400506973, 0.05996163189411163]], "avg_entropy": [0.6679100729525089], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "epidermis", "score_of_biggest_cluster": 0.8171421885490417, "score_of_first_answer": 0.8171421885490417, "max_score_of_first_answer": 0.9983450174331665, "min_score_of_first_answer": 0.45258837938308716, "avg_score": 0.8171421885490417, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2348894034600092, "std_of_first_answer": 0.2348894034600092, "avg_entropy_of_first_answer": 0.6679100729525089, "lex_sim": 1.0}
{"question": "what happened to western rome after it fell", "answer": ["Constantinople"], "scores": [[0.00701691210269928, 0.9918638467788696]], "normalized_score": [0.08342552930116653], "tokens": [["\u2581Constantin", "ople"]], "entropy": [[4.781463623046875, 0.0724683627486229]], "avg_entropy": [2.426965992897749], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "divided into several successor polities", "score_of_biggest_cluster": 0.08342552930116653, "score_of_first_answer": 0.08342552930116653, "max_score_of_first_answer": 0.9918638467788696, "min_score_of_first_answer": 0.00701691210269928, "avg_score": 0.08342552930116653, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4924234673380852, "std_of_first_answer": 0.4924234673380852, "avg_entropy_of_first_answer": 2.426965992897749, "lex_sim": 1.0}
{"question": "the most important battles in the revolutionary war", "answer": ["Battle of Trenton, Saratoga, and Cowpens"], "scores": [[0.04453408718109131, 0.9791364669799805, 0.1165689080953598, 0.999953031539917, 0.9983604550361633, 0.5863277912139893, 0.044576942920684814, 0.9981441497802734, 0.9991275668144226, 0.7892463803291321, 0.344274640083313, 0.06714732944965363, 0.991384744644165, 0.9998792409896851]], "normalized_score": [0.3967205882072449], "tokens": [["\u2581Battle", "\u2581of", "\u2581Tr", "ent", "on", ",", "\u2581Sar", "at", "oga", ",", "\u2581and", "\u2581Cow", "p", "ens"]], "entropy": [[4.060505390167236, 0.16093188524246216, 2.6069562435150146, 0.0006703957915306091, 0.01733769290149212, 1.2710614204406738, 1.2315471172332764, 0.019497159868478775, 0.008157645352184772, 0.9004127979278564, 2.7639620304107666, 1.8480932712554932, 0.08349914103746414, 0.0015454624081030488]], "avg_entropy": [1.0695841181108594], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Philadelphia campaign", "score_of_biggest_cluster": 0.3967205882072449, "score_of_first_answer": 0.3967205882072449, "max_score_of_first_answer": 0.999953031539917, "min_score_of_first_answer": 0.04453408718109131, "avg_score": 0.3967205882072449, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4059453869836293, "std_of_first_answer": 0.4059453869836293, "avg_entropy_of_first_answer": 1.0695841181108594, "lex_sim": 1.0}
{"question": "who sings the song you'll never find another love like mine", "answer": ["Lou Rawls"], "scores": [[0.5510089993476868, 0.8753358721733093, 0.9979180693626404]], "normalized_score": [0.7836872339248657], "tokens": [["\u2581Lou", "\u2581Raw", "ls"]], "entropy": [[2.948800563812256, 0.7441801428794861, 0.018635939806699753]], "avg_entropy": [1.237205548832814], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lou Rawls", "score_of_biggest_cluster": 0.7836872339248657, "score_of_first_answer": 0.7836872339248657, "max_score_of_first_answer": 0.9979180693626404, "min_score_of_first_answer": 0.5510089993476868, "avg_score": 0.7836872339248657, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1885447286650047, "std_of_first_answer": 0.1885447286650047, "avg_entropy_of_first_answer": 1.237205548832814, "lex_sim": 1.0}
{"question": "who sings the theme song for living single", "answer": ["Queen Latifah"], "scores": [[0.6370893716812134, 0.9759953618049622, 0.9909700155258179, 0.9989519119262695]], "normalized_score": [0.8857543468475342], "tokens": [["\u2581Queen", "\u2581Lat", "if", "ah"]], "entropy": [[2.0314788818359375, 0.16684871912002563, 0.05529765784740448, 0.010797763243317604]], "avg_entropy": [0.5661057555116713], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Queen Latifah", "score_of_biggest_cluster": 0.8857543468475342, "score_of_first_answer": 0.8857543468475342, "max_score_of_first_answer": 0.9989519119262695, "min_score_of_first_answer": 0.6370893716812134, "avg_score": 0.8857543468475342, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.15244839954471365, "std_of_first_answer": 0.15244839954471365, "avg_entropy_of_first_answer": 0.5661057555116713, "lex_sim": 1.0}
{"question": "who is the richest club in the championship", "answer": ["Chelsea"], "scores": [[0.0335274338722229, 0.9735258221626282, 0.9990503191947937]], "normalized_score": [0.3194810450077057], "tokens": [["\u2581Ch", "else", "a"]], "entropy": [[3.9724178314208984, 0.16383004188537598, 0.010575341060757637]], "avg_entropy": [1.3822744047890108], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Manchester City", "score_of_biggest_cluster": 0.3194810450077057, "score_of_first_answer": 0.3194810450077057, "max_score_of_first_answer": 0.9990503191947937, "min_score_of_first_answer": 0.0335274338722229, "avg_score": 0.3194810450077057, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4492565354769696, "std_of_first_answer": 0.4492565354769696, "avg_entropy_of_first_answer": 1.3822744047890108, "lex_sim": 1.0}
{"question": "what is the full scientific name for the e. coli bacteria", "answer": ["Escherichia Coli"], "scores": [[0.8421279191970825, 0.9865911602973938, 0.9928978681564331, 0.9984729886054993, 0.24515293538570404, 0.9972370862960815]], "normalized_score": [0.7655941843986511], "tokens": [["\u2581E", "scher", "ich", "ia", "\u2581Col", "i"]], "entropy": [[0.9603462815284729, 0.09820420295000076, 0.05863042175769806, 0.017454303801059723, 0.8343172669410706, 0.03052276186645031]], "avg_entropy": [0.33324587314079207], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Escherichia coli", "score_of_biggest_cluster": 0.7655941843986511, "score_of_first_answer": 0.7655941843986511, "max_score_of_first_answer": 0.9984729886054993, "min_score_of_first_answer": 0.24515293538570404, "avg_score": 0.7655941843986511, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2733944797532887, "std_of_first_answer": 0.2733944797532887, "avg_entropy_of_first_answer": 0.33324587314079207, "lex_sim": 1.0}
{"question": "who win road march in trinidad and tobago", "answer": ["Machel Montano"], "scores": [[0.2932807207107544, 0.9995970129966736, 0.9827825427055359, 0.9960965514183044]], "normalized_score": [0.7319254279136658], "tokens": [["\u2581Mach", "el", "\u2581Mont", "ano"]], "entropy": [[3.9180760383605957, 0.005139597691595554, 0.1348641961812973, 0.031447287648916245]], "avg_entropy": [1.0223817799706012], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Superblue", "score_of_biggest_cluster": 0.7319254279136658, "score_of_first_answer": 0.7319254279136658, "max_score_of_first_answer": 0.9995970129966736, "min_score_of_first_answer": 0.2932807207107544, "avg_score": 0.7319254279136658, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30297667006370227, "std_of_first_answer": 0.30297667006370227, "avg_entropy_of_first_answer": 1.0223817799706012, "lex_sim": 1.0}
{"question": "who discovered gold in the sierra nevada of california", "answer": ["John Mackay"], "scores": [[0.1789761483669281, 0.0004613918426912278, 0.9821167588233948]], "normalized_score": [0.043285518884658813], "tokens": [["\u2581John", "\u2581Mack", "ay"]], "entropy": [[1.4811859130859375, 2.5627541542053223, 0.15137845277786255]], "avg_entropy": [1.3984395066897075], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "James W. Marshall", "score_of_biggest_cluster": 0.043285518884658813, "score_of_first_answer": 0.043285518884658813, "max_score_of_first_answer": 0.9821167588233948, "min_score_of_first_answer": 0.0004613918426912278, "avg_score": 0.043285518884658813, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42694647309356565, "std_of_first_answer": 0.42694647309356565, "avg_entropy_of_first_answer": 1.3984395066897075, "lex_sim": 1.0}
{"question": "what type of energy do satellites generally use to communicate with earth", "answer": ["Microwave"], "scores": [[0.08812817186117172, 0.999197781085968, 0.9334704875946045]], "normalized_score": [0.4347994029521942], "tokens": [["\u2581Mic", "row", "ave"]], "entropy": [[2.995927572250366, 0.008316967636346817, 0.2511959969997406]], "avg_entropy": [1.0851468456288178], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "radio and microwave frequencies", "score_of_biggest_cluster": 0.4347994029521942, "score_of_first_answer": 0.4347994029521942, "max_score_of_first_answer": 0.999197781085968, "min_score_of_first_answer": 0.08812817186117172, "avg_score": 0.4347994029521942, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4148589506931891, "std_of_first_answer": 0.4148589506931891, "avg_entropy_of_first_answer": 1.0851468456288178, "lex_sim": 1.0}
{"question": "who wrote ain't living long like this", "answer": ["Paul McCartney"], "scores": [[0.005508542526513338, 0.07468704134225845, 0.99481201171875, 0.99854576587677]], "normalized_score": [0.14218303561210632], "tokens": [["\u2581Paul", "\u2581McC", "art", "ney"]], "entropy": [[5.408209800720215, 3.467647075653076, 0.04911308363080025, 0.01411377266049385]], "avg_entropy": [2.2347709331661463], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rodney Crowell", "score_of_biggest_cluster": 0.14218303561210632, "score_of_first_answer": 0.14218303561210632, "max_score_of_first_answer": 0.99854576587677, "min_score_of_first_answer": 0.005508542526513338, "avg_score": 0.14218303561210632, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4789173200265345, "std_of_first_answer": 0.4789173200265345, "avg_entropy_of_first_answer": 2.2347709331661463, "lex_sim": 1.0}
{"question": "who does demetrius love in a midsummer night dream", "answer": ["Hermia"], "scores": [[0.49392566084861755, 0.9852783679962158]], "normalized_score": [0.697606086730957], "tokens": [["\u2581Herm", "ia"]], "entropy": [[1.6131534576416016, 0.11679080873727798]], "avg_entropy": [0.8649721331894398], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Helena", "score_of_biggest_cluster": 0.697606086730957, "score_of_first_answer": 0.697606086730957, "max_score_of_first_answer": 0.9852783679962158, "min_score_of_first_answer": 0.49392566084861755, "avg_score": 0.697606086730957, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24567635357379913, "std_of_first_answer": 0.24567635357379913, "avg_entropy_of_first_answer": 0.8649721331894398, "lex_sim": 1.0}
{"question": "what is the movie about six degrees of separation", "answer": ["the movie is about six degrees of separation"], "scores": [[0.016519540920853615, 0.046584926545619965, 0.21816566586494446, 0.8262125849723816, 0.06661270558834076, 0.7513447403907776, 0.9755673408508301, 0.9620678424835205]], "normalized_score": [0.2247747778892517], "tokens": [["\u2581the", "\u2581movie", "\u2581is", "\u2581about", "\u2581six", "\u2581degrees", "\u2581of", "\u2581separation"]], "entropy": [[4.730152130126953, 5.393558502197266, 3.9504170417785645, 0.9150083661079407, 2.238358974456787, 1.338667631149292, 0.19315722584724426, 0.3258606493473053]], "avg_entropy": [2.385647565126419], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Six Degrees of Separation", "score_of_biggest_cluster": 0.2247747778892517, "score_of_first_answer": 0.2247747778892517, "max_score_of_first_answer": 0.9755673408508301, "min_score_of_first_answer": 0.016519540920853615, "avg_score": 0.2247747778892517, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4052021089539516, "std_of_first_answer": 0.4052021089539516, "avg_entropy_of_first_answer": 2.385647565126419, "lex_sim": 1.0}
{"question": "who was the chicago bears quarterback last year", "answer": ["Jay Cutler"], "scores": [[0.5738124251365662, 0.992212176322937, 0.9872575402259827, 0.999046266078949]], "normalized_score": [0.865660548210144], "tokens": [["\u2581Jay", "\u2581C", "ut", "ler"]], "entropy": [[1.9346548318862915, 0.08851204812526703, 0.11055825650691986, 0.010243118740618229]], "avg_entropy": [0.5359920638147742], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Matt Barkley", "score_of_biggest_cluster": 0.865660548210144, "score_of_first_answer": 0.865660548210144, "max_score_of_first_answer": 0.999046266078949, "min_score_of_first_answer": 0.5738124251365662, "avg_score": 0.865660548210144, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18149195259804435, "std_of_first_answer": 0.18149195259804435, "avg_entropy_of_first_answer": 0.5359920638147742, "lex_sim": 1.0}
{"question": "when did sweet caroline start at red sox games", "answer": ["1988"], "scores": [[0.7307921648025513, 0.47020623087882996, 0.9798055291175842, 0.2900683581829071, 0.1599198579788208]], "normalized_score": [0.4352361857891083], "tokens": [["\u2581", "1", "9", "8", "8"]], "entropy": [[1.7788197994232178, 0.7731532454490662, 0.13861055672168732, 1.145323395729065, 2.149627447128296]], "avg_entropy": [1.1971068888902665], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1997", "score_of_biggest_cluster": 0.4352361857891083, "score_of_first_answer": 0.4352361857891083, "max_score_of_first_answer": 0.9798055291175842, "min_score_of_first_answer": 0.1599198579788208, "avg_score": 0.4352361857891083, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29687388507251616, "std_of_first_answer": 0.29687388507251616, "avg_entropy_of_first_answer": 1.1971068888902665, "lex_sim": 1.0}
{"question": "who does claire from mcleods daughters end up with", "answer": ["Matt Parkinson"], "scores": [[0.016391128301620483, 0.01012648269534111, 0.9205596446990967]], "normalized_score": [0.05346133932471275], "tokens": [["\u2581Matt", "\u2581Park", "inson"]], "entropy": [[5.435633659362793, 3.4713664054870605, 0.3986987769603729]], "avg_entropy": [3.1018996139367423], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alex Ryan", "score_of_biggest_cluster": 0.05346133932471275, "score_of_first_answer": 0.05346133932471275, "max_score_of_first_answer": 0.9205596446990967, "min_score_of_first_answer": 0.01012648269534111, "avg_score": 0.05346133932471275, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4277133638191122, "std_of_first_answer": 0.4277133638191122, "avg_entropy_of_first_answer": 3.1018996139367423, "lex_sim": 1.0}
{"question": "who played the king in the ten commandments", "answer": ["Yul Brynner"], "scores": [[0.6498665809631348, 0.9699822068214417, 0.959966778755188, 0.9580109119415283, 0.9995521903038025]], "normalized_score": [0.8966109156608582], "tokens": [["\u2581Y", "ul", "\u2581Bry", "n", "ner"]], "entropy": [[1.5948892831802368, 0.22505450248718262, 0.26581811904907227, 0.18206842243671417, 0.0049460032023489475]], "avg_entropy": [0.45455526607111096], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yul Brynner", "score_of_biggest_cluster": 0.8966109156608582, "score_of_first_answer": 0.8966109156608582, "max_score_of_first_answer": 0.9995521903038025, "min_score_of_first_answer": 0.6498665809631348, "avg_score": 0.8966109156608582, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.12965856907083914, "std_of_first_answer": 0.12965856907083914, "avg_entropy_of_first_answer": 0.45455526607111096, "lex_sim": 1.0}
{"question": "who plays lois's dad on family guy", "answer": ["Nathan-Louis Jackson"], "scores": [[0.0056444439105689526, 0.03204868733882904, 0.009996877051889896, 0.06447020173072815]], "normalized_score": [0.018478374928236008], "tokens": [["\u2581Nathan", "-", "Louis", "\u2581Jackson"]], "entropy": [[3.4494540691375732, 2.0562143325805664, 3.5703749656677246, 3.8371243476867676]], "avg_entropy": [3.228291928768158], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "MacFarlane", "score_of_biggest_cluster": 0.018478374928236008, "score_of_first_answer": 0.018478374928236008, "max_score_of_first_answer": 0.06447020173072815, "min_score_of_first_answer": 0.0056444439105689526, "avg_score": 0.018478374928236008, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02329347757735241, "std_of_first_answer": 0.02329347757735241, "avg_entropy_of_first_answer": 3.228291928768158, "lex_sim": 1.0}
{"question": "who developed a set of postulates to prove that specific microorganisms cause disease", "answer": ["Louis Pasteur"], "scores": [[0.46666407585144043, 0.9791719317436218, 0.99925297498703, 0.9992722868919373]], "normalized_score": [0.8218747973442078], "tokens": [["\u2581Louis", "\u2581P", "aste", "ur"]], "entropy": [[2.5277152061462402, 0.18683737516403198, 0.00868823379278183, 0.007947411388158798]], "avg_entropy": [0.6827970566228032], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Robert Koch", "score_of_biggest_cluster": 0.8218747973442078, "score_of_first_answer": 0.8218747973442078, "max_score_of_first_answer": 0.9992722868919373, "min_score_of_first_answer": 0.46666407585144043, "avg_score": 0.8218747973442078, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22786975683068392, "std_of_first_answer": 0.22786975683068392, "avg_entropy_of_first_answer": 0.6827970566228032, "lex_sim": 1.0}
{"question": "who sang let me tell you about the birds and the bees", "answer": ["Billie Holiday"], "scores": [[0.12532810866832733, 0.28754642605781555, 0.29749271273612976, 0.9991756081581116]], "normalized_score": [0.32171300053596497], "tokens": [["\u2581Bill", "ie", "\u2581Hol", "iday"]], "entropy": [[4.068026542663574, 3.1259512901306152, 2.8008949756622314, 0.008835900574922562]], "avg_entropy": [2.500927177257836], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jewel Akens", "score_of_biggest_cluster": 0.32171300053596497, "score_of_first_answer": 0.32171300053596497, "max_score_of_first_answer": 0.9991756081581116, "min_score_of_first_answer": 0.12532810866832733, "avg_score": 0.32171300053596497, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3371237525855906, "std_of_first_answer": 0.3371237525855906, "avg_entropy_of_first_answer": 2.500927177257836, "lex_sim": 1.0}
{"question": "when did the first episode of that 70s show air", "answer": ["1998"], "scores": [[0.16466861963272095, 0.7126370668411255, 0.9653789401054382, 0.9686050415039062, 0.955022931098938]], "normalized_score": [0.6368945240974426], "tokens": [["\u2581", "1", "9", "9", "8"]], "entropy": [[2.183849811553955, 0.851087749004364, 0.2425205111503601, 0.1802872121334076, 0.24851951003074646]], "avg_entropy": [0.7412529587745667], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "August 23, 1998", "score_of_biggest_cluster": 0.6368945240974426, "score_of_first_answer": 0.6368945240974426, "max_score_of_first_answer": 0.9686050415039062, "min_score_of_first_answer": 0.16466861963272095, "avg_score": 0.6368945240974426, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3098923192745344, "std_of_first_answer": 0.3098923192745344, "avg_entropy_of_first_answer": 0.7412529587745667, "lex_sim": 1.0}
{"question": "name the process of fusion of an egg with a sperm", "answer": ["Fusion of sperm in egg to form a new human being"], "scores": [[0.19340495765209198, 0.009654462337493896, 0.07197871804237366, 0.08262232691049576, 0.9829331636428833, 0.0021323873661458492, 0.29672080278396606, 0.027351289987564087, 0.5666574239730835, 0.5028990507125854, 0.14745230972766876, 0.021746551617980003, 0.3202201724052429]], "normalized_score": [0.09554163366556168], "tokens": [["\u2581F", "usion", "\u2581of", "\u2581s", "perm", "\u2581in", "\u2581egg", "\u2581to", "\u2581form", "\u2581a", "\u2581new", "\u2581human", "\u2581being"]], "entropy": [[1.6709572076797485, 0.1918305605649948, 0.8190456628799438, 2.7338624000549316, 0.09026680886745453, 1.4442188739776611, 2.134519100189209, 1.692636489868164, 1.7641311883926392, 1.7655037641525269, 1.5860964059829712, 2.4711732864379883, 1.9885809421539307]], "avg_entropy": [1.5656017454770894], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "fertilization", "score_of_biggest_cluster": 0.09554163366556168, "score_of_first_answer": 0.09554163366556168, "max_score_of_first_answer": 0.9829331636428833, "min_score_of_first_answer": 0.0021323873661458492, "avg_score": 0.09554163366556168, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.27773697302480016, "std_of_first_answer": 0.27773697302480016, "avg_entropy_of_first_answer": 1.5656017454770894, "lex_sim": 1.0}
{"question": "where was the first t20 cricket match played", "answer": ["Bangladesh v South Africa."], "scores": [[0.01181588601320982, 0.5612826347351074, 0.9961018562316895, 0.034585703164339066, 0.03438843414187431, 0.9902539849281311, 0.0034890740644186735, 0.022680873051285744, 0.9945663213729858, 0.9996581077575684, 0.03336479887366295, 0.5489874482154846, 0.5688438415527344, 0.9845332503318787, 0.9637889266014099]], "normalized_score": [0.17870138585567474], "tokens": [["\u2581Bang", "l", "adesh", "\u2581v", "\u2581South", "\u2581Africa", ".", "\u2581D", "h", "aka", "\u2581", "2", "0", "0", "6"]], "entropy": [[4.3149566650390625, 0.7670120596885681, 0.02857391908764839, 0.8086483478546143, 2.5605952739715576, 0.07759896665811539, 1.7713491916656494, 2.37135910987854, 0.05632501095533371, 0.003654263447970152, 2.009716272354126, 1.1756651401519775, 1.5227689743041992, 0.12051402032375336, 0.21186214685440063]], "avg_entropy": [1.1867066241490345], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lord's", "score_of_biggest_cluster": 0.17870138585567474, "score_of_first_answer": 0.17870138585567474, "max_score_of_first_answer": 0.9996581077575684, "min_score_of_first_answer": 0.0034890740644186735, "avg_score": 0.17870138585567474, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4321426746772462, "std_of_first_answer": 0.4321426746772462, "avg_entropy_of_first_answer": 1.1867066241490345, "lex_sim": 1.0}
{"question": "what was the first capital city of australia", "answer": ["Sydney"], "scores": [[0.7643504738807678]], "normalized_score": [0.7643504738807678], "tokens": [["\u2581Sydney"]], "entropy": [[1.6214501857757568]], "avg_entropy": [1.6214501857757568], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "New South Wales", "score_of_biggest_cluster": 0.7643504738807678, "score_of_first_answer": 0.7643504738807678, "max_score_of_first_answer": 0.7643504738807678, "min_score_of_first_answer": 0.7643504738807678, "avg_score": 0.7643504738807678, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.6214501857757568, "lex_sim": 1.0}
{"question": "who was the movie citizen kane based on", "answer": ["Charles Foster Kane"], "scores": [[0.04938900098204613, 0.9415088891983032, 0.9717025756835938, 0.9735108613967896, 0.9997047781944275]], "normalized_score": [0.5353528261184692], "tokens": [["\u2581Charles", "\u2581F", "oster", "\u2581K", "ane"]], "entropy": [[2.240238904953003, 0.4656243324279785, 0.14123843610286713, 0.2379554808139801, 0.0037919520400464535]], "avg_entropy": [0.6177698212675751], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "William Randolph Hearst", "score_of_biggest_cluster": 0.5353528261184692, "score_of_first_answer": 0.5353528261184692, "max_score_of_first_answer": 0.9997047781944275, "min_score_of_first_answer": 0.04938900098204613, "avg_score": 0.5353528261184692, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.369347401522686, "std_of_first_answer": 0.369347401522686, "avg_entropy_of_first_answer": 0.6177698212675751, "lex_sim": 1.0}
{"question": "where is the world's largest thermometer located", "answer": ["Baker, California"], "scores": [[0.8750772476196289, 0.7006690502166748, 0.8870158195495605]], "normalized_score": [0.8162631988525391], "tokens": [["\u2581Baker", ",", "\u2581California"]], "entropy": [[0.7747637629508972, 1.0220301151275635, 0.5337383151054382]], "avg_entropy": [0.7768440643946329], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Baker, California, USA", "score_of_biggest_cluster": 0.8162631988525391, "score_of_first_answer": 0.8162631988525391, "max_score_of_first_answer": 0.8870158195495605, "min_score_of_first_answer": 0.7006690502166748, "avg_score": 0.8162631988525391, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08517033081607552, "std_of_first_answer": 0.08517033081607552, "avg_entropy_of_first_answer": 0.7768440643946329, "lex_sim": 1.0}
{"question": "what mlb teams did deion sanders play for", "answer": ["Atlanta Braves, San Francisco Giants"], "scores": [[0.4192226231098175, 0.9580960273742676, 0.9973946809768677, 0.45939308404922485, 0.37975186109542847, 0.9707841277122498, 0.9795498251914978, 0.9990867376327515]], "normalized_score": [0.7124751210212708], "tokens": [["\u2581Atlanta", "\u2581Bra", "ves", ",", "\u2581San", "\u2581Francisco", "\u2581Gi", "ants"]], "entropy": [[2.7811458110809326, 0.26880431175231934, 0.019971271976828575, 1.4070515632629395, 2.429349899291992, 0.17091906070709229, 0.1504477560520172, 0.008265848271548748]], "avg_entropy": [0.9044944402994588], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "San Francisco Giants", "score_of_biggest_cluster": 0.7124751210212708, "score_of_first_answer": 0.7124751210212708, "max_score_of_first_answer": 0.9990867376327515, "min_score_of_first_answer": 0.37975186109542847, "avg_score": 0.7124751210212708, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2728571685260502, "std_of_first_answer": 0.2728571685260502, "avg_entropy_of_first_answer": 0.9044944402994588, "lex_sim": 1.0}
{"question": "who won the most medals in the 1924 winter olympics", "answer": ["USA"], "scores": [[0.040250007063150406]], "normalized_score": [0.040250007063150406], "tokens": [["\u2581USA"]], "entropy": [[4.202580451965332]], "avg_entropy": [4.202580451965332], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Norway", "score_of_biggest_cluster": 0.040250007063150406, "score_of_first_answer": 0.040250007063150406, "max_score_of_first_answer": 0.040250007063150406, "min_score_of_first_answer": 0.040250007063150406, "avg_score": 0.040250007063150406, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.202580451965332, "lex_sim": 1.0}
{"question": "who sings the song let me love you", "answer": ["Justin Bieber"], "scores": [[0.5931031107902527, 0.9086030125617981, 0.9954728484153748, 0.9994688630104065]], "normalized_score": [0.8557083606719971], "tokens": [["\u2581Justin", "\u2581B", "ie", "ber"]], "entropy": [[2.0414552688598633, 0.5089156627655029, 0.04588643088936806, 0.006294023711234331]], "avg_entropy": [0.6506378465564921], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "DJ Snake featuring Justin Bieber", "score_of_biggest_cluster": 0.8557083606719971, "score_of_first_answer": 0.8557083606719971, "max_score_of_first_answer": 0.9994688630104065, "min_score_of_first_answer": 0.5931031107902527, "avg_score": 0.8557083606719971, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.16628169954241728, "std_of_first_answer": 0.16628169954241728, "avg_entropy_of_first_answer": 0.6506378465564921, "lex_sim": 1.0}
{"question": "what is the name of the under butler in downton abbey", "answer": ["Carson"], "scores": [[0.16821610927581787, 0.9997878670692444]], "normalized_score": [0.4100980758666992], "tokens": [["\u2581Car", "son"]], "entropy": [[3.103822946548462, 0.0028944790828973055]], "avg_entropy": [1.5533587128156796], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charles Carson", "score_of_biggest_cluster": 0.4100980758666992, "score_of_first_answer": 0.4100980758666992, "max_score_of_first_answer": 0.9997878670692444, "min_score_of_first_answer": 0.16821610927581787, "avg_score": 0.4100980758666992, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.41578587889671326, "std_of_first_answer": 0.41578587889671326, "avg_entropy_of_first_answer": 1.5533587128156796, "lex_sim": 1.0}
{"question": "where is the extensor pollicis longus tendon located", "answer": ["the wrist"], "scores": [[0.013332006521522999, 0.1756078153848648, 0.999466598033905]], "normalized_score": [0.13276061415672302], "tokens": [["\u2581the", "\u2581w", "rist"]], "entropy": [[3.7465884685516357, 3.5466723442077637, 0.006680099759250879]], "avg_entropy": [2.4333136375062168], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "dorsally on the forearm", "score_of_biggest_cluster": 0.13276061415672302, "score_of_first_answer": 0.13276061415672302, "max_score_of_first_answer": 0.999466598033905, "min_score_of_first_answer": 0.013332006521522999, "avg_score": 0.13276061415672302, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.431732705948111, "std_of_first_answer": 0.431732705948111, "avg_entropy_of_first_answer": 2.4333136375062168, "lex_sim": 1.0}
{"question": "what does zonia receive from reuben in the play", "answer": ["a doll to hold, an antelope skin to wear"], "scores": [[0.16915075480937958, 0.01049230620265007, 0.0017616314580664039, 0.01117675006389618, 0.018422361463308334, 0.019076086580753326, 0.0021032390650361776, 0.2600162923336029, 0.9977160692214966, 0.21373820304870605, 0.20772698521614075, 0.3229219913482666]], "normalized_score": [0.0462610088288784], "tokens": [["\u2581a", "\u2581doll", "\u2581to", "\u2581hold", ",", "\u2581an", "\u2581ant", "el", "ope", "\u2581skin", "\u2581to", "\u2581wear"]], "entropy": [[5.200795650482178, 5.760514259338379, 1.3458142280578613, 4.405246734619141, 2.8204658031463623, 3.5000674724578857, 4.208860874176025, 2.6026010513305664, 0.020535841584205627, 2.6976568698883057, 2.8400378227233887, 3.049121618270874]], "avg_entropy": [3.204309852172931], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "kiss", "score_of_biggest_cluster": 0.0462610088288784, "score_of_first_answer": 0.0462610088288784, "max_score_of_first_answer": 0.9977160692214966, "min_score_of_first_answer": 0.0017616314580664039, "avg_score": 0.0462610088288784, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2692130069779125, "std_of_first_answer": 0.2692130069779125, "avg_entropy_of_first_answer": 3.204309852172931, "lex_sim": 1.0}
{"question": "when does the black panther movie soundtrack come out", "answer": ["May 10, 2018"], "scores": [[0.02367779240012169, 0.9651110172271729, 0.9614987969398499, 0.8807881474494934, 0.23172609508037567, 0.9909379482269287, 0.9994438290596008, 0.9996930360794067, 0.9974143505096436, 0.9954133629798889]], "normalized_score": [0.5813348889350891], "tokens": [["\u2581May", "\u2581", "1", "0", ",", "\u2581", "2", "0", "1", "8"]], "entropy": [[1.9015567302703857, 0.1977381706237793, 0.24006974697113037, 0.49251338839530945, 1.0582853555679321, 0.06257058680057526, 0.0057757580652832985, 0.003998413681983948, 0.019455280154943466, 0.03623677045106888]], "avg_entropy": [0.4018200200982392], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "February 9, 2018", "score_of_biggest_cluster": 0.5813348889350891, "score_of_first_answer": 0.5813348889350891, "max_score_of_first_answer": 0.9996930360794067, "min_score_of_first_answer": 0.02367779240012169, "avg_score": 0.5813348889350891, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3432992869375478, "std_of_first_answer": 0.3432992869375478, "avg_entropy_of_first_answer": 0.4018200200982392, "lex_sim": 1.0}
{"question": "who drives the number 95 car in nascar", "answer": ["Mark Martin"], "scores": [[0.013497988693416119, 0.9323701858520508]], "normalized_score": [0.11218343675136566], "tokens": [["\u2581Mark", "\u2581Martin"]], "entropy": [[3.840939998626709, 0.5624884366989136]], "avg_entropy": [2.2017142176628113], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Michael Christopher McDowell", "score_of_biggest_cluster": 0.11218343675136566, "score_of_first_answer": 0.11218343675136566, "max_score_of_first_answer": 0.9323701858520508, "min_score_of_first_answer": 0.013497988693416119, "avg_score": 0.11218343675136566, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45943609857931733, "std_of_first_answer": 0.45943609857931733, "avg_entropy_of_first_answer": 2.2017142176628113, "lex_sim": 1.0}
{"question": "when was the term prime minister first used", "answer": ["1486"], "scores": [[0.7017214298248291, 0.9852409958839417, 0.012798227369785309, 0.1907990276813507, 0.024715173989534378]], "normalized_score": [0.13306976854801178], "tokens": [["\u2581", "1", "4", "8", "6"]], "entropy": [[2.1508681774139404, 0.10704696923494339, 1.4851045608520508, 1.9639928340911865, 0.8098294734954834]], "avg_entropy": [1.303368403017521], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1624", "score_of_biggest_cluster": 0.13306976854801178, "score_of_first_answer": 0.13306976854801178, "max_score_of_first_answer": 0.9852409958839417, "min_score_of_first_answer": 0.012798227369785309, "avg_score": 0.13306976854801178, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39157020688810723, "std_of_first_answer": 0.39157020688810723, "avg_entropy_of_first_answer": 1.303368403017521, "lex_sim": 1.0}
{"question": "who is the founder of google and when was it founded", "answer": ["Larry Page and Sergey Brin"], "scores": [[0.8189803957939148, 0.9618925452232361, 0.731960117816925, 0.9173660278320312, 0.9795128703117371, 0.9906443357467651, 0.9979248046875]], "normalized_score": [0.9088531136512756], "tokens": [["\u2581Larry", "\u2581Page", "\u2581and", "\u2581Serge", "y", "\u2581Br", "in"]], "entropy": [[0.8524290323257446, 0.2779989242553711, 1.0027378797531128, 0.5458198189735413, 0.10550004988908768, 0.08796177804470062, 0.020172808319330215]], "avg_entropy": [0.41323147022298407], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1998", "score_of_biggest_cluster": 0.9088531136512756, "score_of_first_answer": 0.9088531136512756, "max_score_of_first_answer": 0.9979248046875, "min_score_of_first_answer": 0.731960117816925, "avg_score": 0.9088531136512756, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0938706723198389, "std_of_first_answer": 0.0938706723198389, "avg_entropy_of_first_answer": 0.41323147022298407, "lex_sim": 1.0}
{"question": "blood alcohol concentration means the parts of alcohol in the blood in relation to what", "answer": ["a percentage"], "scores": [[0.006113535258919001, 0.06915654242038727]], "normalized_score": [0.020561879500746727], "tokens": [["\u2581a", "\u2581percentage"]], "entropy": [[4.5607757568359375, 4.908822059631348]], "avg_entropy": [4.734798908233643], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "volume of blood", "score_of_biggest_cluster": 0.020561879500746727, "score_of_first_answer": 0.020561879500746727, "max_score_of_first_answer": 0.06915654242038727, "min_score_of_first_answer": 0.006113535258919001, "avg_score": 0.020561879500746727, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.031521503580734134, "std_of_first_answer": 0.031521503580734134, "avg_entropy_of_first_answer": 4.734798908233643, "lex_sim": 1.0}
{"question": "what is a e8 in the marine corps", "answer": ["E-8"], "scores": [[0.1643781214952469, 0.10152130573987961, 0.9259966015815735]], "normalized_score": [0.2490788698196411], "tokens": [["\u2581E", "-", "8"]], "entropy": [[4.977237224578857, 1.900266408920288, 0.5490952730178833]], "avg_entropy": [2.4755329688390098], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Master sergeant", "score_of_biggest_cluster": 0.2490788698196411, "score_of_first_answer": 0.2490788698196411, "max_score_of_first_answer": 0.9259966015815735, "min_score_of_first_answer": 0.10152130573987961, "avg_score": 0.2490788698196411, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.37472555912429795, "std_of_first_answer": 0.37472555912429795, "avg_entropy_of_first_answer": 2.4755329688390098, "lex_sim": 1.0}
{"question": "where does the sciatic nerve run in the foot", "answer": ["calf"], "scores": [[0.0254069771617651, 0.8905035257339478]], "normalized_score": [0.15041610598564148], "tokens": [["\u2581cal", "f"]], "entropy": [[4.953539848327637, 0.5780267715454102]], "avg_entropy": [2.7657833099365234], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "on the posterior aspect", "score_of_biggest_cluster": 0.15041610598564148, "score_of_first_answer": 0.15041610598564148, "max_score_of_first_answer": 0.8905035257339478, "min_score_of_first_answer": 0.0254069771617651, "avg_score": 0.15041610598564148, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4325482742860913, "std_of_first_answer": 0.4325482742860913, "avg_entropy_of_first_answer": 2.7657833099365234, "lex_sim": 1.0}
{"question": "what is the name of the dragon in eragon", "answer": ["F'nor"], "scores": [[0.16326205432415009, 0.5959774255752563, 0.23982080817222595]], "normalized_score": [0.28575947880744934], "tokens": [["\u2581F", "'", "nor"]], "entropy": [[4.543597221374512, 2.4380240440368652, 2.0396053791046143]], "avg_entropy": [3.007075548171997], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Saphira", "score_of_biggest_cluster": 0.28575947880744934, "score_of_first_answer": 0.28575947880744934, "max_score_of_first_answer": 0.5959774255752563, "min_score_of_first_answer": 0.16326205432415009, "avg_score": 0.28575947880744934, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18854748044155337, "std_of_first_answer": 0.18854748044155337, "avg_entropy_of_first_answer": 3.007075548171997, "lex_sim": 1.0}
{"question": "who wrote the song balls to the wall", "answer": ["David Lee Roth"], "scores": [[0.45631781220436096, 0.37611180543899536, 0.9933342933654785]], "normalized_score": [0.5544894337654114], "tokens": [["\u2581David", "\u2581Lee", "\u2581Roth"]], "entropy": [[3.41158390045166, 3.1859076023101807, 0.067169688642025]], "avg_entropy": [2.221553730467955], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Deaffy", "score_of_biggest_cluster": 0.5544894337654114, "score_of_first_answer": 0.5544894337654114, "max_score_of_first_answer": 0.9933342933654785, "min_score_of_first_answer": 0.37611180543899536, "avg_score": 0.5544894337654114, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2740201339241075, "std_of_first_answer": 0.2740201339241075, "avg_entropy_of_first_answer": 2.221553730467955, "lex_sim": 1.0}
{"question": "who plays the saint of killers on preacher", "answer": ["Ian Colletti"], "scores": [[0.30647996068000793, 0.9623134732246399, 0.9931796789169312, 0.9895868301391602]], "normalized_score": [0.7337533235549927], "tokens": [["\u2581Ian", "\u2581Col", "lett", "i"]], "entropy": [[3.1983892917633057, 0.2193022519350052, 0.06231822073459625, 0.08430276811122894]], "avg_entropy": [0.891078133136034], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Graham McTavish", "score_of_biggest_cluster": 0.7337533235549927, "score_of_first_answer": 0.7337533235549927, "max_score_of_first_answer": 0.9931796789169312, "min_score_of_first_answer": 0.30647996068000793, "avg_score": 0.7337533235549927, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29261948003327937, "std_of_first_answer": 0.29261948003327937, "avg_entropy_of_first_answer": 0.891078133136034, "lex_sim": 1.0}
{"question": "how many indian astronauts have gone to space", "answer": ["One"], "scores": [[0.08366769552230835]], "normalized_score": [0.08366768807172775], "tokens": [["\u2581One"]], "entropy": [[3.208165168762207]], "avg_entropy": [3.208165168762207], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2", "score_of_biggest_cluster": 0.08366768807172775, "score_of_first_answer": 0.08366768807172775, "max_score_of_first_answer": 0.08366769552230835, "min_score_of_first_answer": 0.08366769552230835, "avg_score": 0.08366768807172775, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.208165168762207, "lex_sim": 1.0}
{"question": "who plays gram on the young and the restless", "answer": ["Daniel Hall"], "scores": [[0.03048519976437092, 0.18414132297039032]], "normalized_score": [0.07492386549711227], "tokens": [["\u2581Daniel", "\u2581Hall"]], "entropy": [[4.697887420654297, 1.1191320419311523]], "avg_entropy": [2.9085097312927246], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Max Shippee", "score_of_biggest_cluster": 0.07492386549711227, "score_of_first_answer": 0.07492386549711227, "max_score_of_first_answer": 0.18414132297039032, "min_score_of_first_answer": 0.03048519976437092, "avg_score": 0.07492386549711227, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0768280616030097, "std_of_first_answer": 0.0768280616030097, "avg_entropy_of_first_answer": 2.9085097312927246, "lex_sim": 1.0}
{"question": "when does the new episodes of supernatural start", "answer": ["8 October 2018"], "scores": [[0.05831597000360489, 0.048558179289102554, 0.03457067534327507, 0.6356319785118103, 0.987959623336792, 0.9983845949172974, 0.9920205473899841, 0.09229929000139236]], "normalized_score": [0.22065560519695282], "tokens": [["\u2581", "8", "\u2581October", "\u2581", "2", "0", "1", "8"]], "entropy": [[2.5642282962799072, 1.6112265586853027, 2.3736863136291504, 1.0021989345550537, 0.08927202224731445, 0.01668678969144821, 0.05117747560143471, 1.7560126781463623]], "avg_entropy": [1.1830611336044967], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "October\u00a012,\u00a02017", "score_of_biggest_cluster": 0.22065560519695282, "score_of_first_answer": 0.22065560519695282, "max_score_of_first_answer": 0.9983845949172974, "min_score_of_first_answer": 0.03457067534327507, "avg_score": 0.22065560519695282, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43672116681744344, "std_of_first_answer": 0.43672116681744344, "avg_entropy_of_first_answer": 1.1830611336044967, "lex_sim": 1.0}
{"question": "type of disappearing lake found in limestone areas in the west of ireland", "answer": ["meromictic"], "scores": [[0.03349591791629791, 0.8638631701469421, 0.9550870656967163, 0.9984613656997681]], "normalized_score": [0.4075705409049988], "tokens": [["\u2581mer", "om", "ict", "ic"]], "entropy": [[5.433252334594727, 0.8549291491508484, 0.35665178298950195, 0.01639469526708126]], "avg_entropy": [1.6653069905005395], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "turlough", "score_of_biggest_cluster": 0.4075705409049988, "score_of_first_answer": 0.4075705409049988, "max_score_of_first_answer": 0.9984613656997681, "min_score_of_first_answer": 0.03349591791629791, "avg_score": 0.4075705409049988, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39515172212126, "std_of_first_answer": 0.39515172212126, "avg_entropy_of_first_answer": 1.6653069905005395, "lex_sim": 1.0}
{"question": "how many senators are there in the us senate", "answer": ["100"], "scores": [[0.863949179649353, 0.9851523637771606, 0.9980472326278687, 0.9987154006958008]], "normalized_score": [0.9597234725952148], "tokens": [["\u2581", "1", "0", "0"]], "entropy": [[0.8462601900100708, 0.10242900252342224, 0.01979666016995907, 0.012433085590600967]], "avg_entropy": [0.24522973457351327], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "100", "score_of_biggest_cluster": 0.9597234725952148, "score_of_first_answer": 0.9597234725952148, "max_score_of_first_answer": 0.9987154006958008, "min_score_of_first_answer": 0.863949179649353, "avg_score": 0.9597234725952148, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.05656031849505247, "std_of_first_answer": 0.05656031849505247, "avg_entropy_of_first_answer": 0.24522973457351327, "lex_sim": 1.0}
{"question": "who is the prime minister of india full name", "answer": ["Narendra Damodardas Modi"], "scores": [[0.871649444103241, 0.9824798107147217, 0.2549611032009125, 0.9941799640655518, 0.8288528323173523, 0.9932739734649658, 0.9974889755249023, 0.9978158473968506]], "normalized_score": [0.8058675527572632], "tokens": [["\u2581Nar", "endra", "\u2581Dam", "od", "ard", "as", "\u2581Mod", "i"]], "entropy": [[0.9351181983947754, 0.1272875964641571, 0.7991788387298584, 0.059660058468580246, 0.6426708102226257, 0.061410773545503616, 0.025171251967549324, 0.022977115586400032]], "avg_entropy": [0.33418433042243123], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Narendra Modi", "score_of_biggest_cluster": 0.8058675527572632, "score_of_first_answer": 0.8058675527572632, "max_score_of_first_answer": 0.9978158473968506, "min_score_of_first_answer": 0.2549611032009125, "avg_score": 0.8058675527572632, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23865110153730698, "std_of_first_answer": 0.23865110153730698, "avg_entropy_of_first_answer": 0.33418433042243123, "lex_sim": 1.0}
{"question": "who is the cast of the new tomb raider movie", "answer": ["Alicia Vikander"], "scores": [[0.5347694754600525, 0.9895119071006775, 0.9996284246444702, 0.9954408407211304, 0.9987242817878723]], "normalized_score": [0.8793817758560181], "tokens": [["\u2581A", "lic", "ia", "\u2581Vik", "ander"]], "entropy": [[3.1325159072875977, 0.08571002632379532, 0.004897866398096085, 0.04309895262122154, 0.01393554825335741]], "avg_entropy": [0.6560316601768136], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alicia Vikander as Lara Croft", "score_of_biggest_cluster": 0.8793817758560181, "score_of_first_answer": 0.8793817758560181, "max_score_of_first_answer": 0.9996284246444702, "min_score_of_first_answer": 0.5347694754600525, "avg_score": 0.8793817758560181, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1844568447311821, "std_of_first_answer": 0.1844568447311821, "avg_entropy_of_first_answer": 0.6560316601768136, "lex_sim": 1.0}
{"question": "who won the womens 2017 ncaa basketball tournament", "answer": ["UConn"], "scores": [[0.13984809815883636, 0.5234053134918213, 0.9983890056610107]], "normalized_score": [0.4180852472782135], "tokens": [["\u2581U", "Con", "n"]], "entropy": [[2.3614206314086914, 1.2260961532592773, 0.01661861501634121]], "avg_entropy": [1.2013784665614367], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "South Carolina", "score_of_biggest_cluster": 0.4180852472782135, "score_of_first_answer": 0.4180852472782135, "max_score_of_first_answer": 0.9983890056610107, "min_score_of_first_answer": 0.13984809815883636, "avg_score": 0.4180852472782135, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3511596874983305, "std_of_first_answer": 0.3511596874983305, "avg_entropy_of_first_answer": 1.2013784665614367, "lex_sim": 1.0}
