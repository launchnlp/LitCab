{"question": "how long has tom brady been playing professional football", "answer": ["The New England Patriots quarterback made his first playoff appearance in 2001."], "scores": [[0.0015720219817012548, 0.4139634668827057, 0.9974290728569031, 0.9811534881591797, 0.9864776134490967, 0.35920989513397217, 0.9986593723297119, 0.03133297711610794, 0.9623686075210571, 0.08671484142541885, 0.0049764844588935375, 0.6381503939628601, 0.46246692538261414, 0.5804556608200073, 0.5796687602996826, 0.990655243396759, 0.9994035959243774, 0.992895781993866, 0.5955622792243958, 0.3041679859161377, 0.11814885586500168, 0.12437178194522858, 0.18129144608974457, 0.27176448702812195, 0.9946605563163757, 0.9523034691810608, 0.999634861946106, 0.2051592916250229, 0.09951601922512054, 0.02145361714065075, 0.5822551846504211, 0.5483136177062988, 0.9922971129417419, 0.8507976531982422, 0.9829089641571045, 0.1821938008069992, 0.0952463150024414, 0.5433182120323181, 0.06922846287488937, 0.0016909444238990545, 0.9991648197174072, 0.11772261559963226, 0.9418924450874329, 0.9387755393981934, 0.5127912163734436, 0.04972198233008385, 0.014255039393901825, 0.016056852415204048, 0.881220817565918, 0.049970101565122604]], "normalized_score": [0.24082699418067932], "tokens": [["\u2581The", "\u2581New", "\u2581England", "\u2581Patri", "ots", "\u2581quarter", "back", "\u2581made", "\u2581his", "\u2581first", "\u2581play", "off", "\u2581appearance", "\u2581in", "\u2581", "2", "0", "0", "1", ".", "\u2581He", "\u2581was", "\u2581named", "\u2581Super", "\u2581Bowl", "\u2581M", "VP", "\u2581after", "\u2581his", "\u2581performance", "\u2581in", "\u2581Super", "\u2581Bowl", "\u2581XXX", "VI", ",", "\u2581where", "\u2581he", "\u2581helped", "\u2581prop", "el", "\u2581his", "\u2581team", "\u2581to", "\u2581a", "\u2581victory", "\u2581with", "\u2581one", "\u2581of", "\u2581his"]], "entropy": [[1.6314654350280762, 3.1804256439208984, 0.024108177050948143, 0.1304509937763214, 0.07250209897756577, 2.245117664337158, 0.012798971496522427, 2.9261767864227295, 0.26387497782707214, 1.569173812866211, 2.3882083892822266, 1.3560856580734253, 0.9963597059249878, 1.8676413297653198, 1.2503793239593506, 0.05917838215827942, 0.006889831740409136, 0.04451672360301018, 1.1568922996520996, 2.1885414123535156, 1.485783338546753, 3.043917655944824, 3.0568435192108154, 2.057464361190796, 0.04431988298892975, 0.2840436100959778, 0.0038659521378576756, 2.1870341300964355, 2.500051498413086, 2.206862449645996, 1.4570958614349365, 1.3618091344833374, 0.06363437324762344, 0.6833210587501526, 0.09815803170204163, 2.057995319366455, 3.1481871604919434, 1.12344491481781, 2.527677297592163, 2.1962947845458984, 0.00840685609728098, 0.7166361808776855, 0.3902127146720886, 0.38895660638809204, 1.5226415395736694, 0.8640736937522888, 0.9239331483840942, 1.8284966945648193, 0.7988425493240356, 0.2677135467529297]], "avg_entropy": [1.253370109666139], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "18-season career", "score_of_biggest_cluster": 0.24082699418067932, "score_of_first_answer": 0.24082699418067932, "max_score_of_first_answer": 0.999634861946106, "min_score_of_first_answer": 0.0015720219817012548, "avg_score": 0.24082699418067932, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3906236361290472, "std_of_first_answer": 0.3906236361290472, "avg_entropy_of_first_answer": 1.253370109666139, "lex_sim": 1.0}
{"question": "what disney cartoon character's middle name is fauntleroy", "answer": ["Cinderella"], "scores": [[0.2601288855075836, 0.9673939943313599, 0.9946640729904175]], "normalized_score": [0.6302160620689392], "tokens": [["\u2581C", "inder", "ella"]], "entropy": [[3.788403272628784, 0.24405591189861298, 0.04541788995265961]], "avg_entropy": [1.359292358160019], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Donald Duck", "score_of_biggest_cluster": 0.6302160620689392, "score_of_first_answer": 0.6302160620689392, "max_score_of_first_answer": 0.9946640729904175, "min_score_of_first_answer": 0.2601288855075836, "avg_score": 0.6302160620689392, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3400178970779857, "std_of_first_answer": 0.3400178970779857, "avg_entropy_of_first_answer": 1.359292358160019, "lex_sim": 1.0}
{"question": "when did jelly beans became associated with easter", "answer": ["1990\u2019s"], "scores": [[0.524389922618866, 0.9421437978744507, 0.8992695808410645, 0.04541996493935585, 0.5928701162338257, 0.0486472062766552, 0.9954108595848083]], "normalized_score": [0.3447951674461365], "tokens": [["\u2581", "1", "9", "9", "0", "\u2019", "s"]], "entropy": [[2.9473354816436768, 0.28489670157432556, 0.4555319547653198, 2.1478421688079834, 1.5283334255218506, 1.0801305770874023, 0.03641793876886368]], "avg_entropy": [1.2114983211670602], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "during the 1930s", "score_of_biggest_cluster": 0.3447951674461365, "score_of_first_answer": 0.3447951674461365, "max_score_of_first_answer": 0.9954108595848083, "min_score_of_first_answer": 0.04541996493935585, "avg_score": 0.3447951674461365, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37359203269990365, "std_of_first_answer": 0.37359203269990365, "avg_entropy_of_first_answer": 1.2114983211670602, "lex_sim": 1.0}
{"question": "who appoints the chair of the federal reserve system", "answer": ["The President with Senate confirmation"], "scores": [[0.2749042809009552, 0.8156373500823975, 0.012906844727694988, 0.3021092116832733, 0.5887424349784851, 0.998636782169342]], "normalized_score": [0.28303006291389465], "tokens": [["\u2581The", "\u2581President", "\u2581with", "\u2581Senate", "\u2581confirm", "ation"]], "entropy": [[2.133652687072754, 0.7478446960449219, 0.9740837812423706, 1.236191987991333, 1.210827350616455, 0.013402966782450676]], "avg_entropy": [1.052667244958381], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "President of the United States", "score_of_biggest_cluster": 0.28303006291389465, "score_of_first_answer": 0.28303006291389465, "max_score_of_first_answer": 0.998636782169342, "min_score_of_first_answer": 0.012906844727694988, "avg_score": 0.28303006291389465, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33744655657913647, "std_of_first_answer": 0.33744655657913647, "avg_entropy_of_first_answer": 1.052667244958381, "lex_sim": 1.0}
{"question": "who is nebula on guardians of the galaxy", "answer": ["Mining Guild"], "scores": [[0.0025270136538892984, 0.02504216879606247, 0.03575414419174194, 0.9931291937828064]], "normalized_score": [0.038717109709978104], "tokens": [["\u2581M", "ining", "\u2581Gu", "ild"]], "entropy": [[2.5432181358337402, 4.305148124694824, 4.6112165451049805, 0.06698516756296158]], "avg_entropy": [2.8816419932991266], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Karen Gillan", "score_of_biggest_cluster": 0.038717109709978104, "score_of_first_answer": 0.038717109709978104, "max_score_of_first_answer": 0.9931291937828064, "min_score_of_first_answer": 0.0025270136538892984, "avg_score": 0.038717109709978104, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42106842330218985, "std_of_first_answer": 0.42106842330218985, "avg_entropy_of_first_answer": 2.8816419932991266, "lex_sim": 1.0}
{"question": "what is the year round weather in dubai", "answer": ["Arid climate"], "scores": [[0.04393031820654869, 0.9935497641563416, 0.04954194277524948]], "normalized_score": [0.12931305170059204], "tokens": [["\u2581Ar", "id", "\u2581climate"]], "entropy": [[3.7305748462677, 0.04549407213926315, 1.8169887065887451]], "avg_entropy": [1.8643525416652362], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "tropical desert climate", "score_of_biggest_cluster": 0.12931305170059204, "score_of_first_answer": 0.12931305170059204, "max_score_of_first_answer": 0.9935497641563416, "min_score_of_first_answer": 0.04393031820654869, "avg_score": 0.12931305170059204, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.44633810668159124, "std_of_first_answer": 0.44633810668159124, "avg_entropy_of_first_answer": 1.8643525416652362, "lex_sim": 1.0}
{"question": "epidemiologists attempt to explain the link between health and variables such as", "answer": ["Age, gender, socioeconomic status, level of physical activity, diet, etc."], "scores": [[0.008157002739608288, 0.7959985136985779, 0.16078130900859833, 0.953406572341919, 0.10672715306282043, 0.9998328685760498, 0.8063622117042542, 0.9953606724739075, 0.9663668274879456, 0.9343838095664978, 0.8494153618812561, 0.010197796858847141, 0.9923257827758789, 0.14198574423789978, 0.9185187816619873, 0.8441663980484009, 0.20815905928611755, 0.9951157569885254, 0.5975948572158813, 0.038858458399772644, 0.8277342915534973]], "normalized_score": [0.35142281651496887], "tokens": [["\u2581Age", ",", "\u2581gender", ",", "\u2581so", "cio", "e", "conom", "ic", "\u2581status", ",", "\u2581level", "\u2581of", "\u2581physical", "\u2581activity", ",", "\u2581di", "et", ",", "\u2581etc", "."]], "entropy": [[4.863330364227295, 0.9980499148368835, 3.5159196853637695, 0.2763326168060303, 3.6589417457580566, 0.0022377846762537956, 0.536994457244873, 0.03969366103410721, 0.15740437805652618, 0.44142836332321167, 0.6633425354957581, 3.2829039096832275, 0.06351860612630844, 1.477186918258667, 0.4754372835159302, 0.6792585849761963, 3.8950235843658447, 0.03640696406364441, 1.2998347282409668, 3.7365949153900146, 0.6799545288085938]], "avg_entropy": [1.4657045490596265], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "biological agents", "score_of_biggest_cluster": 0.35142281651496887, "score_of_first_answer": 0.35142281651496887, "max_score_of_first_answer": 0.9998328685760498, "min_score_of_first_answer": 0.008157002739608288, "avg_score": 0.35142281651496887, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38738353478609067, "std_of_first_answer": 0.38738353478609067, "avg_entropy_of_first_answer": 1.4657045490596265, "lex_sim": 1.0}
{"question": "who is the father of genetics and what did he study", "answer": ["Gregor Mendel and the genetics of"], "scores": [[0.9271538257598877, 0.9996927976608276, 0.8846994042396545, 0.999650239944458, 0.9990492463111877, 0.19925235211849213, 0.011049475520849228, 0.12054242193698883, 0.6457579731941223, 0.9938217401504517, 0.8340634107589722]], "normalized_score": [0.4388715326786041], "tokens": [["\u2581Greg", "or", "\u2581M", "end", "el", "\u2581and", "\u2581the", "\u2581gen", "et", "ics", "\u2581of"]], "entropy": [[0.5427497029304504, 0.004066755063831806, 0.44671204686164856, 0.004151715897023678, 0.009736152365803719, 1.904497742652893, 2.8984570503234863, 3.3203463554382324, 0.8864201903343201, 0.04546312615275383, 0.7908923625946045]], "avg_entropy": [0.9866812000559135], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the common edible pea", "score_of_biggest_cluster": 0.4388715326786041, "score_of_first_answer": 0.4388715326786041, "max_score_of_first_answer": 0.9996927976608276, "min_score_of_first_answer": 0.011049475520849228, "avg_score": 0.4388715326786041, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3720345531457302, "std_of_first_answer": 0.3720345531457302, "avg_entropy_of_first_answer": 0.9866812000559135, "lex_sim": 1.0}
{"question": "where was the statue of liberty originally built", "answer": ["France"], "scores": [[0.74335777759552]], "normalized_score": [0.74335777759552], "tokens": [["\u2581France"]], "entropy": [[1.6511473655700684]], "avg_entropy": [1.6511473655700684], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Paris", "score_of_biggest_cluster": 0.74335777759552, "score_of_first_answer": 0.74335777759552, "max_score_of_first_answer": 0.74335777759552, "min_score_of_first_answer": 0.74335777759552, "avg_score": 0.74335777759552, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.6511473655700684, "lex_sim": 1.0}
{"question": "when was the last amendment to the constitution passed", "answer": ["April 8, 1992"], "scores": [[0.034448735415935516, 0.9959222078323364, 0.5223847031593323, 0.9884310960769653, 0.9971243739128113, 0.9900928139686584, 0.99431312084198, 0.9890025854110718, 0.9075140357017517]], "normalized_score": [0.6298823356628418], "tokens": [["\u2581April", "\u2581", "8", ",", "\u2581", "1", "9", "9", "2"]], "entropy": [[2.4363698959350586, 0.03229335695505142, 1.421218752861023, 0.08040282875299454, 0.021481789648532867, 0.05595770105719566, 0.03907939791679382, 0.07711642980575562, 0.41531479358673096]], "avg_entropy": [0.5088038829465708], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "May 5, 1992", "score_of_biggest_cluster": 0.6298823356628418, "score_of_first_answer": 0.6298823356628418, "max_score_of_first_answer": 0.9971243739128113, "min_score_of_first_answer": 0.034448735415935516, "avg_score": 0.6298823356628418, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31477020435130726, "std_of_first_answer": 0.31477020435130726, "avg_entropy_of_first_answer": 0.5088038829465708, "lex_sim": 1.0}
{"question": "most passing yards in nfl history in a game", "answer": ["554"], "scores": [[0.39072248339653015, 0.794009804725647, 0.865712583065033, 0.9885472059249878]], "normalized_score": [0.7178213000297546], "tokens": [["\u2581", "5", "5", "4"]], "entropy": [[2.9649548530578613, 0.7996047735214233, 0.6683411598205566, 0.08488082885742188]], "avg_entropy": [1.1294454038143158], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Norm Van Brocklin", "score_of_biggest_cluster": 0.7178213000297546, "score_of_first_answer": 0.7178213000297546, "max_score_of_first_answer": 0.9885472059249878, "min_score_of_first_answer": 0.39072248339653015, "avg_score": 0.7178213000297546, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22412679037966765, "std_of_first_answer": 0.22412679037966765, "avg_entropy_of_first_answer": 1.1294454038143158, "lex_sim": 1.0}
{"question": "what age do you have to be to buy cigarettes in california", "answer": ["18 / January 1, 2017"], "scores": [[0.8414456248283386, 0.5847019553184509, 0.990581750869751, 0.00042529875645413995, 0.0038208512123674154, 0.970099687576294, 0.9254139065742493, 0.8994008898735046, 0.9985900521278381, 0.7689037919044495, 0.9991734623908997, 0.7679244875907898, 0.0633372962474823]], "normalized_score": [0.2592199444770813], "tokens": [["\u2581", "1", "8", "\u2581/", "\u2581January", "\u2581", "1", ",", "\u2581", "2", "0", "1", "7"]], "entropy": [[1.1688365936279297, 0.685982346534729, 0.06795006990432739, 0.9968647360801697, 1.355346441268921, 0.19106590747833252, 0.36366215348243713, 0.5752332806587219, 0.013442966155707836, 0.5428346395492554, 0.008079946041107178, 0.6135424971580505, 2.0547430515289307]], "avg_entropy": [0.6644295868822016], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "21", "score_of_biggest_cluster": 0.2592199444770813, "score_of_first_answer": 0.2592199444770813, "max_score_of_first_answer": 0.9991734623908997, "min_score_of_first_answer": 0.00042529875645413995, "avg_score": 0.2592199444770813, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.376460856652371, "std_of_first_answer": 0.376460856652371, "avg_entropy_of_first_answer": 0.6644295868822016, "lex_sim": 1.0}
{"question": "who plays the dad in girl meets world", "answer": ["Daniel Roebuck"], "scores": [[0.007499993313103914, 0.011581502854824066, 0.9990359544754028, 0.9973637461662292]], "normalized_score": [0.09645286947488785], "tokens": [["\u2581Daniel", "\u2581Ro", "eb", "uck"]], "entropy": [[2.9669861793518066, 3.798891544342041, 0.010953123681247234, 0.023837970569729805]], "avg_entropy": [1.7001672044862062], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ben Savage", "score_of_biggest_cluster": 0.09645286947488785, "score_of_first_answer": 0.09645286947488785, "max_score_of_first_answer": 0.9990359544754028, "min_score_of_first_answer": 0.007499993313103914, "avg_score": 0.09645286947488785, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4943320108834149, "std_of_first_answer": 0.4943320108834149, "avg_entropy_of_first_answer": 1.7001672044862062, "lex_sim": 1.0}
{"question": "what type of car is a g wagon", "answer": ["Mercedes"], "scores": [[0.3803083598613739, 0.995159924030304]], "normalized_score": [0.6151972413063049], "tokens": [["\u2581Mer", "cedes"]], "entropy": [[3.0575408935546875, 0.03676919639110565]], "avg_entropy": [1.5471550449728966], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Mercedes-Benz", "score_of_biggest_cluster": 0.6151972413063049, "score_of_first_answer": 0.6151972413063049, "max_score_of_first_answer": 0.995159924030304, "min_score_of_first_answer": 0.3803083598613739, "avg_score": 0.6151972413063049, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.307425782084465, "std_of_first_answer": 0.307425782084465, "avg_entropy_of_first_answer": 1.5471550449728966, "lex_sim": 1.0}
{"question": "who has won the 2017 mens singles mutua madrid open tennis", "answer": ["Rafael Nadal"], "scores": [[0.9097820520401001, 0.9908791184425354, 0.9995929598808289]], "normalized_score": [0.9658886790275574], "tokens": [["\u2581Rafael", "\u2581Nad", "al"]], "entropy": [[0.572284460067749, 0.0859261155128479, 0.005131580866873264]], "avg_entropy": [0.22111405214915672], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Rafael Nadal", "score_of_biggest_cluster": 0.9658886790275574, "score_of_first_answer": 0.9658886790275574, "max_score_of_first_answer": 0.9995929598808289, "min_score_of_first_answer": 0.9097820520401001, "avg_score": 0.9658886790275574, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.04044016757145823, "std_of_first_answer": 0.04044016757145823, "avg_entropy_of_first_answer": 0.22111405214915672, "lex_sim": 1.0}
{"question": "where do the blood vessels that supply the retinal cells lie", "answer": ["arteries and veins"], "scores": [[0.0034815368708223104, 0.8086822628974915, 0.7546972036361694, 0.3636227250099182, 0.9055452346801758, 0.9935566782951355]], "normalized_score": [0.29763171076774597], "tokens": [["\u2581ar", "ter", "ies", "\u2581and", "\u2581ve", "ins"]], "entropy": [[3.4158225059509277, 0.5301734209060669, 0.9806582927703857, 2.1367053985595703, 0.5714693665504456, 0.04085763171315193]], "avg_entropy": [1.279281102741758], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "optic nerve", "score_of_biggest_cluster": 0.29763171076774597, "score_of_first_answer": 0.29763171076774597, "max_score_of_first_answer": 0.9935566782951355, "min_score_of_first_answer": 0.0034815368708223104, "avg_score": 0.29763171076774597, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3460847262170408, "std_of_first_answer": 0.3460847262170408, "avg_entropy_of_first_answer": 1.279281102741758, "lex_sim": 1.0}
{"question": "who sings i will go down with this ship", "answer": ["Wreck of the Edmund Fitzgerald"], "scores": [[0.22884537279605865, 0.35532522201538086, 0.12970924377441406, 0.9548829793930054, 0.08542852848768234, 0.9674643874168396, 0.9884212017059326, 0.9945698380470276]], "normalized_score": [0.4112567603588104], "tokens": [["\u2581W", "reck", "\u2581of", "\u2581the", "\u2581Edmund", "\u2581Fitz", "ger", "ald"]], "entropy": [[4.721214294433594, 2.663422107696533, 2.6534018516540527, 0.2736014723777771, 1.4410431385040283, 0.2666285037994385, 0.07888541370630264, 0.04845700412988663]], "avg_entropy": [1.5183317232877016], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dido", "score_of_biggest_cluster": 0.4112567603588104, "score_of_first_answer": 0.4112567603588104, "max_score_of_first_answer": 0.9945698380470276, "min_score_of_first_answer": 0.08542852848768234, "avg_score": 0.4112567603588104, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3952800528349628, "std_of_first_answer": 0.3952800528349628, "avg_entropy_of_first_answer": 1.5183317232877016, "lex_sim": 1.0}
{"question": "who played ben stone son on law and order", "answer": ["Brendan Fraser"], "scores": [[0.015350709669291973, 0.2845858931541443, 0.9966103434562683, 0.16726896166801453, 0.8361338973045349, 0.9820154905319214]], "normalized_score": [0.2902548313140869], "tokens": [["\u2581Br", "end", "an", "\u2581F", "ras", "er"]], "entropy": [[4.489343643188477, 1.4391963481903076, 0.024258345365524292, 1.3308275938034058, 0.6950394511222839, 0.09794244170188904]], "avg_entropy": [1.3461013038953145], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Michael Moriarty", "score_of_biggest_cluster": 0.2902548313140869, "score_of_first_answer": 0.2902548313140869, "max_score_of_first_answer": 0.9966103434562683, "min_score_of_first_answer": 0.015350709669291973, "avg_score": 0.2902548313140869, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40222177571768214, "std_of_first_answer": 0.40222177571768214, "avg_entropy_of_first_answer": 1.3461013038953145, "lex_sim": 1.0}
{"question": "sending money home to the native country is an example of", "answer": ["remittance."], "scores": [[0.6974661350250244, 0.9872982501983643, 0.6505254507064819, 0.013213417492806911]], "normalized_score": [0.2773720920085907], "tokens": [["\u2581rem", "itt", "ance", "."]], "entropy": [[1.5751996040344238, 0.09784310311079025, 0.660489022731781, 0.25699353218078613]], "avg_entropy": [0.6476313155144453], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "international capital flows", "score_of_biggest_cluster": 0.2773720920085907, "score_of_first_answer": 0.2773720920085907, "max_score_of_first_answer": 0.9872982501983643, "min_score_of_first_answer": 0.013213417492806911, "avg_score": 0.2773720920085907, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3555657175319293, "std_of_first_answer": 0.3555657175319293, "avg_entropy_of_first_answer": 0.6476313155144453, "lex_sim": 1.0}
{"question": "who has the most subscribers in faze clan", "answer": ["FaZe Diva"], "scores": [[0.40913841128349304, 0.9995108842849731, 0.00557853514328599, 0.010289330035448074]], "normalized_score": [0.06960513442754745], "tokens": [["\u2581Fa", "Ze", "\u2581D", "iva"]], "entropy": [[3.7871575355529785, 0.0054456451907753944, 3.2762844562530518, 3.3226218223571777]], "avg_entropy": [2.597877364838496], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "FaZe Rug", "score_of_biggest_cluster": 0.06960513442754745, "score_of_first_answer": 0.06960513442754745, "max_score_of_first_answer": 0.9995108842849731, "min_score_of_first_answer": 0.00557853514328599, "avg_score": 0.06960513442754745, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40596827229563665, "std_of_first_answer": 0.40596827229563665, "avg_entropy_of_first_answer": 2.597877364838496, "lex_sim": 1.0}
{"question": "who did the united states win its independence from", "answer": ["Great Britain"], "scores": [[0.7114881277084351, 0.9837645888328552]], "normalized_score": [0.8366222381591797], "tokens": [["\u2581Great", "\u2581Britain"]], "entropy": [[1.3253095149993896, 0.10644806921482086]], "avg_entropy": [0.7158787921071053], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "the British Empire", "score_of_biggest_cluster": 0.8366222381591797, "score_of_first_answer": 0.8366222381591797, "max_score_of_first_answer": 0.9837645888328552, "min_score_of_first_answer": 0.7114881277084351, "avg_score": 0.8366222381591797, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.13613823056221008, "std_of_first_answer": 0.13613823056221008, "avg_entropy_of_first_answer": 0.7158787921071053, "lex_sim": 1.0}
{"question": "who used the word physiology for the first time", "answer": ["Hippocrates of Cos"], "scores": [[0.15346592664718628, 0.9999337196350098, 0.9973700046539307, 0.997307300567627, 0.134750634431839, 0.541204035282135]], "normalized_score": [0.47252708673477173], "tokens": [["\u2581Hi", "pp", "ocr", "ates", "\u2581of", "\u2581Cos"]], "entropy": [[4.9044671058654785, 0.0008187489584088326, 0.025819703936576843, 0.023510536178946495, 0.827149510383606, 1.1695715188980103]], "avg_entropy": [1.158556187370171], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jean Fernel", "score_of_biggest_cluster": 0.47252708673477173, "score_of_first_answer": 0.47252708673477173, "max_score_of_first_answer": 0.9999337196350098, "min_score_of_first_answer": 0.134750634431839, "avg_score": 0.47252708673477173, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.384413965275628, "std_of_first_answer": 0.384413965275628, "avg_entropy_of_first_answer": 1.158556187370171, "lex_sim": 1.0}
{"question": "what is the highest base on the ph scale", "answer": ["a pH"], "scores": [[0.0016606830758973956, 0.2274908870458603, 0.9951083064079285]], "normalized_score": [0.07217282801866531], "tokens": [["\u2581a", "\u2581p", "H"]], "entropy": [[0.917244553565979, 3.788771390914917, 0.049681585282087326]], "avg_entropy": [1.5852325099209945], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "greater than 14", "score_of_biggest_cluster": 0.07217282801866531, "score_of_first_answer": 0.07217282801866531, "max_score_of_first_answer": 0.9951083064079285, "min_score_of_first_answer": 0.0016606830758973956, "avg_score": 0.07217282801866531, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42520243072333047, "std_of_first_answer": 0.42520243072333047, "avg_entropy_of_first_answer": 1.5852325099209945, "lex_sim": 1.0}
{"question": "when did equus first appear in fossil record", "answer": ["33 million years ago"], "scores": [[0.3392747640609741, 0.0693274587392807, 0.05525529012084007, 0.35912397503852844, 0.9443764686584473, 0.8944936394691467]], "normalized_score": [0.27079012989997864], "tokens": [["\u2581", "3", "3", "\u2581million", "\u2581years", "\u2581ago"]], "entropy": [[3.624802589416504, 1.5449132919311523, 2.366616725921631, 2.0366241931915283, 0.36446866393089294, 0.4659956097602844]], "avg_entropy": [1.7339035123586655], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1820s", "score_of_biggest_cluster": 0.27079012989997864, "score_of_first_answer": 0.27079012989997864, "max_score_of_first_answer": 0.9443764686584473, "min_score_of_first_answer": 0.05525529012084007, "avg_score": 0.27079012989997864, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.356602869124177, "std_of_first_answer": 0.356602869124177, "avg_entropy_of_first_answer": 1.7339035123586655, "lex_sim": 1.0}
{"question": "who is the supreme justice of the supreme court", "answer": ["John Glover Roberts"], "scores": [[0.5614413022994995, 0.2716429531574249, 0.9715893268585205, 0.9719179272651672]], "normalized_score": [0.6160327196121216], "tokens": [["\u2581John", "\u2581Glo", "ver", "\u2581Roberts"]], "entropy": [[2.247199535369873, 1.5558099746704102, 0.23719340562820435, 0.1832476556301117]], "avg_entropy": [1.0558626428246498], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "John Roberts", "score_of_biggest_cluster": 0.6160327196121216, "score_of_first_answer": 0.6160327196121216, "max_score_of_first_answer": 0.9719179272651672, "min_score_of_first_answer": 0.2716429531574249, "avg_score": 0.6160327196121216, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.29591020790261324, "std_of_first_answer": 0.29591020790261324, "avg_entropy_of_first_answer": 1.0558626428246498, "lex_sim": 1.0}
{"question": "when did the not in this lifetime tour start", "answer": ["September 22, 2017"], "scores": [[0.021072575822472572, 0.9837960004806519, 0.3455137014389038, 0.05871572718024254, 0.8406014442443848, 0.9887999296188354, 0.9536662101745605, 0.9990742206573486, 0.8895344734191895, 0.36981767416000366]], "normalized_score": [0.4017617106437683], "tokens": [["\u2581September", "\u2581", "2", "2", ",", "\u2581", "2", "0", "1", "7"]], "entropy": [[2.9156317710876465, 0.11034246534109116, 1.5915467739105225, 2.2152037620544434, 0.6186432242393494, 0.0845339223742485, 0.1904067099094391, 0.009743825532495975, 0.3624160885810852, 1.5789790153503418]], "avg_entropy": [0.9677447558380663], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "April 1, 2016", "score_of_biggest_cluster": 0.4017617106437683, "score_of_first_answer": 0.4017617106437683, "max_score_of_first_answer": 0.9990742206573486, "min_score_of_first_answer": 0.021072575822472572, "avg_score": 0.4017617106437683, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38080356325826653, "std_of_first_answer": 0.38080356325826653, "avg_entropy_of_first_answer": 0.9677447558380663, "lex_sim": 1.0}
{"question": "who was the first coronated king of manipur at kangla", "answer": ["Bhagyachandra"], "scores": [[0.06311975419521332, 0.780256450176239, 0.9901790618896484, 0.9969006776809692]], "normalized_score": [0.46956104040145874], "tokens": [["\u2581Bh", "agy", "ach", "andra"]], "entropy": [[3.28556752204895, 0.8669358491897583, 0.08975279331207275, 0.026589959859848022]], "avg_entropy": [1.0672115311026573], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pakhangba", "score_of_biggest_cluster": 0.46956104040145874, "score_of_first_answer": 0.46956104040145874, "max_score_of_first_answer": 0.9969006776809692, "min_score_of_first_answer": 0.06311975419521332, "avg_score": 0.46956104040145874, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38215819843602766, "std_of_first_answer": 0.38215819843602766, "avg_entropy_of_first_answer": 1.0672115311026573, "lex_sim": 1.0}
{"question": "who is currently serving as president of the senate", "answer": ["Mitch McConnell"], "scores": [[0.036169640719890594, 0.969510018825531, 0.9937974810600281, 0.9632331728935242]], "normalized_score": [0.4280371367931366], "tokens": [["\u2581Mitch", "\u2581Mc", "Con", "nell"]], "entropy": [[5.182203769683838, 0.22558020055294037, 0.046431075781583786, 0.1875644326210022]], "avg_entropy": [1.410444869659841], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Orrin Hatch", "score_of_biggest_cluster": 0.4280371367931366, "score_of_first_answer": 0.4280371367931366, "max_score_of_first_answer": 0.9937974810600281, "min_score_of_first_answer": 0.036169640719890594, "avg_score": 0.4280371367931366, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4069079741810097, "std_of_first_answer": 0.4069079741810097, "avg_entropy_of_first_answer": 1.410444869659841, "lex_sim": 1.0}
{"question": "who played bubba in the heat of the night", "answer": ["Alan Autry"], "scores": [[0.16830408573150635, 0.6868360042572021, 0.9990456700325012]], "normalized_score": [0.4869798719882965], "tokens": [["\u2581Alan", "\u2581Aut", "ry"]], "entropy": [[4.025762557983398, 1.7626659870147705, 0.010085484944283962]], "avg_entropy": [1.9328380099808176], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Carlos Alan Autry Jr.", "score_of_biggest_cluster": 0.4869798719882965, "score_of_first_answer": 0.4869798719882965, "max_score_of_first_answer": 0.9990456700325012, "min_score_of_first_answer": 0.16830408573150635, "avg_score": 0.4869798719882965, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.34261766929534826, "std_of_first_answer": 0.34261766929534826, "avg_entropy_of_first_answer": 1.9328380099808176, "lex_sim": 1.0}
{"question": "the site of secretion of h+ and cl-", "answer": ["duodenum"], "scores": [[0.04455530643463135, 0.9480507373809814, 0.9296155571937561]], "normalized_score": [0.33989500999450684], "tokens": [["\u2581du", "oden", "um"]], "entropy": [[4.243975639343262, 0.2616220712661743, 0.2717890441417694]], "avg_entropy": [1.5924622515837352], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Parietal cells", "score_of_biggest_cluster": 0.33989500999450684, "score_of_first_answer": 0.33989500999450684, "max_score_of_first_answer": 0.9480507373809814, "min_score_of_first_answer": 0.04455530643463135, "avg_score": 0.33989500999450684, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4216337928006656, "std_of_first_answer": 0.4216337928006656, "avg_entropy_of_first_answer": 1.5924622515837352, "lex_sim": 1.0}
{"question": "is it marley and me or marley and i", "answer": ["Marley and Marley"], "scores": [[0.8662957549095154, 0.9989307522773743, 0.12777267396450043, 0.0005613676039502025, 0.9830134510993958]], "normalized_score": [0.143578439950943], "tokens": [["\u2581Mar", "ley", "\u2581and", "\u2581Mar", "ley"]], "entropy": [[0.7518923282623291, 0.011299468576908112, 0.4795704483985901, 0.3093099594116211, 0.1347573697566986]], "avg_entropy": [0.3373659148812294], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marley & Me", "score_of_biggest_cluster": 0.143578439950943, "score_of_first_answer": 0.143578439950943, "max_score_of_first_answer": 0.9989307522773743, "min_score_of_first_answer": 0.0005613676039502025, "avg_score": 0.143578439950943, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43794384203580555, "std_of_first_answer": 0.43794384203580555, "avg_entropy_of_first_answer": 0.3373659148812294, "lex_sim": 1.0}
{"question": "songs on 4 your eyez only j cole", "answer": ["4 Your Eyes Only, I Got It, Change, Neighbor, She's My Cole, High for Hours, Once An Addict, Deja Vu, No Longer Safe, Cage, 19"], "scores": [[0.3019813001155853, 0.5887666344642639, 0.9369102120399475, 0.9991505146026611, 0.10967723280191422, 0.997753918170929, 0.29398399591445923, 0.002450656145811081, 0.11808082461357117, 0.10191544890403748, 0.8103740215301514, 0.11092735081911087, 0.8997151255607605, 0.4413825571537018, 0.9997578263282776, 0.4282473921775818, 0.8642760515213013, 0.6571940183639526, 0.6498022675514221, 0.9998517036437988, 0.06020532175898552, 0.00435953214764595, 0.5800214409828186, 0.06086453050374985, 0.35757994651794434, 0.79116290807724, 0.9941208958625793, 0.9040342569351196, 0.04369819909334183, 0.11859743297100067, 0.9633621573448181, 0.9987882971763611, 0.8244481086730957, 0.16399547457695007, 0.9874894022941589, 0.9853132367134094, 0.9982397556304932, 0.9095051288604736, 0.22480781376361847, 0.007332607172429562, 0.6376848220825195, 0.07383772730827332, 0.9998688697814941, 0.6337060332298279, 0.32498160004615784, 0.18457993865013123, 0.07473526149988174, 0.04523991793394089, 0.3623301684856415, 0.8964416980743408]], "normalized_score": [0.2965829074382782], "tokens": [["\u2581", "4", "\u2581Your", "\u2581E", "yes", "\u2581Only", ",", "\u2581I", "\u2581Got", "\u2581It", ",", "\u2581Change", ",", "\u2581Ne", "igh", "bor", ",", "\u2581She", "'", "s", "\u2581My", "\u2581Cole", ",", "\u2581High", "\u2581for", "\u2581H", "ours", ",", "\u2581Once", "\u2581An", "\u2581Add", "ict", ",", "\u2581De", "ja", "\u2581V", "u", ",", "\u2581No", "\u2581Long", "er", "\u2581Sa", "fe", ",", "\u2581C", "age", ",", "\u2581", "1", "9"]], "entropy": [[3.9114677906036377, 1.2044110298156738, 0.4370594620704651, 0.010197937488555908, 0.3486136198043823, 0.02093920111656189, 1.4391108751296997, 2.570767402648926, 3.671875476837158, 1.7867355346679688, 1.1454145908355713, 4.161359786987305, 0.5520429611206055, 2.797746419906616, 0.002818326000124216, 0.788191556930542, 0.6746941804885864, 1.583229899406433, 0.9649146795272827, 0.0021778945811092854, 0.6505035161972046, 2.2525994777679443, 2.543386936187744, 4.805569648742676, 1.6290661096572876, 0.8423380255699158, 0.045671071857213974, 0.4931333363056183, 3.721006155014038, 0.6197192072868347, 0.266048789024353, 0.012720038183033466, 0.8437826037406921, 3.7912099361419678, 0.08098116517066956, 0.1123528927564621, 0.01789216697216034, 0.5017561912536621, 3.2367148399353027, 3.549983501434326, 0.7996151447296143, 3.4343228340148926, 0.0015556152211502194, 1.3936246633529663, 4.0767011642456055, 3.672672986984253, 2.9283859729766846, 5.331254005432129, 1.0838899612426758, 0.5138696432113647]], "avg_entropy": [1.626521924531553], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Immortal", "score_of_biggest_cluster": 0.2965829074382782, "score_of_first_answer": 0.2965829074382782, "max_score_of_first_answer": 0.9998688697814941, "min_score_of_first_answer": 0.002450656145811081, "avg_score": 0.2965829074382782, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3764724186540509, "std_of_first_answer": 0.3764724186540509, "avg_entropy_of_first_answer": 1.626521924531553, "lex_sim": 1.0}
{"question": "new york heart association functional classification of angina", "answer": ["mild"], "scores": [[0.008272518403828144, 0.9674303531646729]], "normalized_score": [0.08945994824171066], "tokens": [["\u2581m", "ild"]], "entropy": [[4.186958312988281, 0.3093166947364807]], "avg_entropy": [2.248137503862381], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "II", "score_of_biggest_cluster": 0.08945994824171066, "score_of_first_answer": 0.08945994824171066, "max_score_of_first_answer": 0.9674303531646729, "min_score_of_first_answer": 0.008272518403828144, "avg_score": 0.08945994824171066, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.47957891738042235, "std_of_first_answer": 0.47957891738042235, "avg_entropy_of_first_answer": 2.248137503862381, "lex_sim": 1.0}
{"question": "who cooks for the president of the united states", "answer": ["White House Executive Chef"], "scores": [[0.40626439452171326, 0.9903518557548523, 0.47190091013908386, 0.959587037563324, 0.9676589965820312]], "normalized_score": [0.7067261934280396], "tokens": [["\u2581White", "\u2581House", "\u2581Executive", "\u2581Che", "f"]], "entropy": [[3.222977638244629, 0.07859981805086136, 1.880466103553772, 0.23618653416633606, 0.14513249695301056]], "avg_entropy": [1.1126725181937218], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Cristeta Comerford", "score_of_biggest_cluster": 0.7067261934280396, "score_of_first_answer": 0.7067261934280396, "max_score_of_first_answer": 0.9903518557548523, "min_score_of_first_answer": 0.40626439452171326, "avg_score": 0.7067261934280396, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.26235304302873136, "std_of_first_answer": 0.26235304302873136, "avg_entropy_of_first_answer": 1.1126725181937218, "lex_sim": 1.0}
{"question": "where does patience is a virtue come from", "answer": ["\u201cPatience is bitter, but her fruit is sweet.\u201d from Aristotle"], "scores": [[0.008932462893426418, 0.5680493116378784, 0.9893067479133606, 0.9044695496559143, 0.04340587183833122, 0.8012630343437195, 0.9942027926445007, 0.011691498570144176, 0.9219957590103149, 0.9961287975311279, 0.9913149476051331, 0.7118372917175293, 0.007436224725097418, 0.24185588955879211, 0.9957518577575684, 0.9867767691612244]], "normalized_score": [0.28685781359672546], "tokens": [["\u2581\u201c", "P", "atience", "\u2581is", "\u2581bitter", ",", "\u2581but", "\u2581her", "\u2581fruit", "\u2581is", "\u2581sweet", ".\u201d", "\u2581from", "\u2581Arist", "ot", "le"]], "entropy": [[4.268385887145996, 2.5308167934417725, 0.10077249258756638, 0.5465520024299622, 0.3628549575805664, 0.6098697781562805, 0.05459943413734436, 0.7294183969497681, 0.32099446654319763, 0.034261398017406464, 0.07414986193180084, 0.9567747116088867, 2.439067840576172, 4.076711654663086, 0.0386650487780571, 0.0971425324678421]], "avg_entropy": [1.0775648285634816], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\"Psychomachia,\"", "score_of_biggest_cluster": 0.28685781359672546, "score_of_first_answer": 0.28685781359672546, "max_score_of_first_answer": 0.9961287975311279, "min_score_of_first_answer": 0.007436224725097418, "avg_score": 0.28685781359672546, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40602312758011266, "std_of_first_answer": 0.40602312758011266, "avg_entropy_of_first_answer": 1.0775648285634816, "lex_sim": 1.0}
{"question": "how many gold medals won by india in 2018 commonwealth games", "answer": ["16"], "scores": [[0.8947762846946716, 0.12342886626720428, 0.49323028326034546]], "normalized_score": [0.37907660007476807], "tokens": [["\u2581", "1", "6"]], "entropy": [[0.8237871527671814, 1.4435276985168457, 1.9213557243347168]], "avg_entropy": [1.396223525206248], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "26", "score_of_biggest_cluster": 0.37907660007476807, "score_of_first_answer": 0.37907660007476807, "max_score_of_first_answer": 0.8947762846946716, "min_score_of_first_answer": 0.12342886626720428, "avg_score": 0.37907660007476807, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31499014432425754, "std_of_first_answer": 0.31499014432425754, "avg_entropy_of_first_answer": 1.396223525206248, "lex_sim": 1.0}
{"question": "when do new episodes of riverdale season 2 come out", "answer": ["Last New: September 13, 2017"], "scores": [[0.0009134178981184959, 0.005965062882751226, 0.009720703586935997, 0.003647875739261508, 0.993055522441864, 0.14087724685668945, 0.25865837931632996, 0.8835128545761108, 0.9931395649909973, 0.9981642365455627, 0.999714195728302, 0.9939165711402893, 0.556955873966217]], "normalized_score": [0.13112786412239075], "tokens": [["\u2581Last", "\u2581New", ":", "\u2581September", "\u2581", "1", "3", ",", "\u2581", "2", "0", "1", "7"]], "entropy": [[2.3286876678466797, 3.491239070892334, 1.0454087257385254, 2.962378978729248, 0.05675432085990906, 0.7462854385375977, 2.1546642780303955, 0.5715014934539795, 0.064610555768013, 0.015960458666086197, 0.0034077062737196684, 0.04426521062850952, 0.7769478559494019]], "avg_entropy": [1.097085520105723], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "February\u00a07,\u00a02018", "score_of_biggest_cluster": 0.13112786412239075, "score_of_first_answer": 0.13112786412239075, "max_score_of_first_answer": 0.999714195728302, "min_score_of_first_answer": 0.0009134178981184959, "avg_score": 0.13112786412239075, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4417416095934549, "std_of_first_answer": 0.4417416095934549, "avg_entropy_of_first_answer": 1.097085520105723, "lex_sim": 1.0}
{"question": "when are general elections held in the us", "answer": ["November 3-8, 2020"], "scores": [[0.34621500968933105, 0.4291580319404602, 0.12799644470214844, 0.001318314578384161, 0.368462473154068, 0.5261019468307495, 0.9664471745491028, 0.9880020618438721, 0.999408483505249, 0.8034451007843018, 0.9799139499664307]], "normalized_score": [0.32038989663124084], "tokens": [["\u2581November", "\u2581", "3", "-", "8", ",", "\u2581", "2", "0", "2", "0"]], "entropy": [[3.224691152572632, 1.7620980739593506, 1.5460578203201294, 0.9829375743865967, 2.467008590698242, 1.437211513519287, 0.24968364834785461, 0.07049235701560974, 0.006859014742076397, 0.6128522157669067, 0.11774548888206482]], "avg_entropy": [1.1343306772918864], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "every two to six years", "score_of_biggest_cluster": 0.32038989663124084, "score_of_first_answer": 0.32038989663124084, "max_score_of_first_answer": 0.999408483505249, "min_score_of_first_answer": 0.001318314578384161, "avg_score": 0.32038989663124084, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3520828060134072, "std_of_first_answer": 0.3520828060134072, "avg_entropy_of_first_answer": 1.1343306772918864, "lex_sim": 1.0}
{"question": "tools made from high-speed tool steel are generally used for what type of machining operations", "answer": ["machining"], "scores": [[0.12898138165473938, 0.9949782490730286]], "normalized_score": [0.3582369089126587], "tokens": [["\u2581mach", "ining"]], "entropy": [[4.1469950675964355, 0.04088810831308365]], "avg_entropy": [2.0939415879547596], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "milling", "score_of_biggest_cluster": 0.3582369089126587, "score_of_first_answer": 0.3582369089126587, "max_score_of_first_answer": 0.9949782490730286, "min_score_of_first_answer": 0.12898138165473938, "avg_score": 0.3582369089126587, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4329984337091446, "std_of_first_answer": 0.4329984337091446, "avg_entropy_of_first_answer": 2.0939415879547596, "lex_sim": 1.0}
{"question": "where can you find dna in the body", "answer": ["The nucleus of a cell"], "scores": [[0.0154217928647995, 0.6411734819412231, 0.9896857142448425, 0.8047769069671631, 0.3041430115699768, 0.8261531591415405]], "normalized_score": [0.354326456785202], "tokens": [["\u2581The", "\u2581nucle", "us", "\u2581of", "\u2581a", "\u2581cell"]], "entropy": [[3.7369439601898193, 2.130061149597168, 0.08667388558387756, 0.8341851234436035, 2.1951193809509277, 0.7721714973449707]], "avg_entropy": [1.6258591661850612], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "chromosomes in a cell", "score_of_biggest_cluster": 0.354326456785202, "score_of_first_answer": 0.354326456785202, "max_score_of_first_answer": 0.9896857142448425, "min_score_of_first_answer": 0.0154217928647995, "avg_score": 0.354326456785202, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3356173757025455, "std_of_first_answer": 0.3356173757025455, "avg_entropy_of_first_answer": 1.6258591661850612, "lex_sim": 1.0}
{"question": "who was the qb for the saints before drew brees", "answer": ["Aaron Brooks"], "scores": [[0.7487452626228333, 0.9993709921836853, 0.995513379573822, 0.979470431804657]], "normalized_score": [0.9242188334465027], "tokens": [["\u2581A", "aron", "\u2581Bro", "oks"]], "entropy": [[1.2519757747650146, 0.007361846510320902, 0.04282589256763458, 0.1057480126619339]], "avg_entropy": [0.351977881626226], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Aaron Brooks", "score_of_biggest_cluster": 0.9242188334465027, "score_of_first_answer": 0.9242188334465027, "max_score_of_first_answer": 0.9993709921836853, "min_score_of_first_answer": 0.7487452626228333, "avg_score": 0.9242188334465027, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.10535954904723832, "std_of_first_answer": 0.10535954904723832, "avg_entropy_of_first_answer": 0.351977881626226, "lex_sim": 1.0}
{"question": "jharkhand which festival is associated with cattle worship", "answer": ["Holi"], "scores": [[0.003888827981427312, 0.9885998964309692]], "normalized_score": [0.06200399622321129], "tokens": [["\u2581Hol", "i"]], "entropy": [[4.795284271240234, 0.08917063474655151]], "avg_entropy": [2.442227452993393], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sohrai", "score_of_biggest_cluster": 0.06200399622321129, "score_of_first_answer": 0.06200399622321129, "max_score_of_first_answer": 0.9885998964309692, "min_score_of_first_answer": 0.003888827981427312, "avg_score": 0.06200399622321129, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.49235553422477096, "std_of_first_answer": 0.49235553422477096, "avg_entropy_of_first_answer": 2.442227452993393, "lex_sim": 1.0}
{"question": "who plays the mother of howard on big bang theory", "answer": ["Sara Gilbert"], "scores": [[0.030650600790977478, 0.7572606801986694, 0.44053444266319275]], "normalized_score": [0.21704745292663574], "tokens": [["\u2581S", "ara", "\u2581Gilbert"]], "entropy": [[3.901495933532715, 1.375427007675171, 1.0114495754241943]], "avg_entropy": [2.0961241722106934], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Carol Ann Susi", "score_of_biggest_cluster": 0.21704745292663574, "score_of_first_answer": 0.21704745292663574, "max_score_of_first_answer": 0.7572606801986694, "min_score_of_first_answer": 0.030650600790977478, "avg_score": 0.21704745292663574, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2974488715856444, "std_of_first_answer": 0.2974488715856444, "avg_entropy_of_first_answer": 2.0961241722106934, "lex_sim": 1.0}
{"question": "who played the oldest brother in 7th heaven", "answer": ["Geoff Stults"], "scores": [[0.015123123303055763, 0.9438650012016296, 0.997495174407959, 0.9674475193023682]], "normalized_score": [0.3425882160663605], "tokens": [["\u2581Geoff", "\u2581St", "ult", "s"]], "entropy": [[1.7833799123764038, 0.2613077461719513, 0.027301305904984474, 0.16456711292266846]], "avg_entropy": [0.559139019344002], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Barry Watson", "score_of_biggest_cluster": 0.3425882160663605, "score_of_first_answer": 0.3425882160663605, "max_score_of_first_answer": 0.997495174407959, "min_score_of_first_answer": 0.015123123303055763, "avg_score": 0.3425882160663605, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4137385398704066, "std_of_first_answer": 0.4137385398704066, "avg_entropy_of_first_answer": 0.559139019344002, "lex_sim": 1.0}
{"question": "which mode is used for short wave broadcast service", "answer": ["AM"], "scores": [[0.3496705889701843]], "normalized_score": [0.3496705889701843], "tokens": [["\u2581AM"]], "entropy": [[3.2909770011901855]], "avg_entropy": [3.2909770011901855], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Olivia", "score_of_biggest_cluster": 0.3496705889701843, "score_of_first_answer": 0.3496705889701843, "max_score_of_first_answer": 0.3496705889701843, "min_score_of_first_answer": 0.3496705889701843, "avg_score": 0.3496705889701843, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.2909770011901855, "lex_sim": 1.0}
{"question": "where was percy jackson and the olympians filmed", "answer": ["New Zealand"], "scores": [[0.1330585926771164, 0.625261664390564]], "normalized_score": [0.2884379029273987], "tokens": [["\u2581New", "\u2581Zealand"]], "entropy": [[4.683273792266846, 1.2419793605804443]], "avg_entropy": [2.962626576423645], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Vancouver", "score_of_biggest_cluster": 0.2884379029273987, "score_of_first_answer": 0.2884379029273987, "max_score_of_first_answer": 0.625261664390564, "min_score_of_first_answer": 0.1330585926771164, "avg_score": 0.2884379029273987, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24610153585672379, "std_of_first_answer": 0.24610153585672379, "avg_entropy_of_first_answer": 2.962626576423645, "lex_sim": 1.0}
{"question": "what does the sun represent on the uruguay flag", "answer": ["Sun of May"], "scores": [[0.08670624345541, 0.6730977296829224, 0.9900321960449219]], "normalized_score": [0.38659772276878357], "tokens": [["\u2581Sun", "\u2581of", "\u2581May"]], "entropy": [[3.5668978691101074, 0.9791805744171143, 0.09225547313690186]], "avg_entropy": [1.5461113055547078], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the May Revolution of 1810", "score_of_biggest_cluster": 0.38659772276878357, "score_of_first_answer": 0.38659772276878357, "max_score_of_first_answer": 0.9900321960449219, "min_score_of_first_answer": 0.08670624345541, "avg_score": 0.38659772276878357, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37421031040342145, "std_of_first_answer": 0.37421031040342145, "avg_entropy_of_first_answer": 1.5461113055547078, "lex_sim": 1.0}
{"question": "what city was zeus the patron god of", "answer": ["Athens"], "scores": [[0.6080520153045654, 0.9708138108253479]], "normalized_score": [0.7683132886886597], "tokens": [["\u2581Ath", "ens"]], "entropy": [[1.818861722946167, 0.14305195212364197]], "avg_entropy": [0.9809568375349045], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Olympia", "score_of_biggest_cluster": 0.7683132886886597, "score_of_first_answer": 0.7683132886886597, "max_score_of_first_answer": 0.9708138108253479, "min_score_of_first_answer": 0.6080520153045654, "avg_score": 0.7683132886886597, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18138089776039124, "std_of_first_answer": 0.18138089776039124, "avg_entropy_of_first_answer": 0.9809568375349045, "lex_sim": 1.0}
{"question": "who was charlie writing to in perks of being a wallflower movie", "answer": ["Stephanie"], "scores": [[0.001982486806809902, 0.8202592134475708]], "normalized_score": [0.0403255820274353], "tokens": [["\u2581Steph", "anie"]], "entropy": [[3.9112043380737305, 0.8448371887207031]], "avg_entropy": [2.378020763397217], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "an unknown recipient", "score_of_biggest_cluster": 0.0403255820274353, "score_of_first_answer": 0.0403255820274353, "max_score_of_first_answer": 0.8202592134475708, "min_score_of_first_answer": 0.001982486806809902, "avg_score": 0.0403255820274353, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40913836332038045, "std_of_first_answer": 0.40913836332038045, "avg_entropy_of_first_answer": 2.378020763397217, "lex_sim": 1.0}
{"question": "who played david on the assassination of gianni versace", "answer": ["Cody Fern"], "scores": [[0.1903357207775116, 0.9925385117530823, 0.9986139535903931]], "normalized_score": [0.5735286474227905], "tokens": [["\u2581C", "ody", "\u2581Fern"]], "entropy": [[1.8206170797348022, 0.057096417993307114, 0.016413796693086624]], "avg_entropy": [0.6313757648070654], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Cody Fern", "score_of_biggest_cluster": 0.5735286474227905, "score_of_first_answer": 0.5735286474227905, "max_score_of_first_answer": 0.9986139535903931, "min_score_of_first_answer": 0.1903357207775116, "avg_score": 0.5735286474227905, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3796021207178831, "std_of_first_answer": 0.3796021207178831, "avg_entropy_of_first_answer": 0.6313757648070654, "lex_sim": 1.0}
{"question": "what breed of dog is the teddy bear dog", "answer": ["Shih Tzu"], "scores": [[0.5271807312965393, 0.9391187429428101, 0.9570814371109009, 0.9743077754974365]], "normalized_score": [0.8242923617362976], "tokens": [["\u2581Sh", "ih", "\u2581T", "zu"]], "entropy": [[2.39324688911438, 0.28315407037734985, 0.24245235323905945, 0.12693913280963898]], "avg_entropy": [0.761448111385107], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Zuchon", "score_of_biggest_cluster": 0.8242923617362976, "score_of_first_answer": 0.8242923617362976, "max_score_of_first_answer": 0.9743077754974365, "min_score_of_first_answer": 0.5271807312965393, "avg_score": 0.8242923617362976, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18646176033618708, "std_of_first_answer": 0.18646176033618708, "avg_entropy_of_first_answer": 0.761448111385107, "lex_sim": 1.0}
{"question": "when was the last year the raiders won the superbowl", "answer": ["1983"], "scores": [[0.5654181838035583, 0.5112255811691284, 0.9924589991569519, 0.980130672454834, 0.5046459436416626]], "normalized_score": [0.6766952872276306], "tokens": [["\u2581", "1", "9", "8", "3"]], "entropy": [[1.5288355350494385, 0.8205395936965942, 0.0661190003156662, 0.11766809970140457, 0.8984577059745789]], "avg_entropy": [0.6863239869475365], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "1983", "score_of_biggest_cluster": 0.6766952872276306, "score_of_first_answer": 0.6766952872276306, "max_score_of_first_answer": 0.9924589991569519, "min_score_of_first_answer": 0.5046459436416626, "avg_score": 0.6766952872276306, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2259805815137644, "std_of_first_answer": 0.2259805815137644, "avg_entropy_of_first_answer": 0.6863239869475365, "lex_sim": 1.0}
{"question": "where is the slide placed on the microscope", "answer": ["on a microscope"], "scores": [[0.30590078234672546, 0.044026900082826614, 0.35257861018180847, 0.9955281615257263, 0.9221919178962708]], "normalized_score": [0.3371987044811249], "tokens": [["\u2581on", "\u2581a", "\u2581mic", "ros", "cope"]], "entropy": [[3.530484914779663, 0.9350253343582153, 3.007101058959961, 0.03689625859260559, 0.3079734146595001]], "avg_entropy": [1.563496196269989], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "stage", "score_of_biggest_cluster": 0.3371987044811249, "score_of_first_answer": 0.3371987044811249, "max_score_of_first_answer": 0.9955281615257263, "min_score_of_first_answer": 0.044026900082826614, "avg_score": 0.3371987044811249, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37100467345119903, "std_of_first_answer": 0.37100467345119903, "avg_entropy_of_first_answer": 1.563496196269989, "lex_sim": 1.0}
{"question": "when is the new dumping ground coming out 2017", "answer": ["October 6, 2017"], "scores": [[0.1102277934551239, 0.9332042932510376, 0.03178110346198082, 0.6349815726280212, 0.9944275617599487, 0.9969878792762756, 0.9995977282524109, 0.9942141175270081, 0.8544079661369324]], "normalized_score": [0.4938596189022064], "tokens": [["\u2581October", "\u2581", "6", ",", "\u2581", "2", "0", "1", "7"]], "entropy": [[4.016928672790527, 0.3496600389480591, 1.5702000856399536, 1.0412204265594482, 0.05400427430868149, 0.0249472763389349, 0.004798672627657652, 0.04007010534405708, 0.6240860223770142]], "avg_entropy": [0.8584350638815926], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "15\u00a0December\u00a02017", "score_of_biggest_cluster": 0.4938596189022064, "score_of_first_answer": 0.4938596189022064, "max_score_of_first_answer": 0.9995977282524109, "min_score_of_first_answer": 0.03178110346198082, "avg_score": 0.4938596189022064, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3683410766607145, "std_of_first_answer": 0.3683410766607145, "avg_entropy_of_first_answer": 0.8584350638815926, "lex_sim": 1.0}
{"question": "author of the hymn great is thy faithfulness", "answer": ["Thomas Obadiah Chisam"], "scores": [[0.8863146901130676, 0.5351833701133728, 0.5278000235557556, 0.9993759989738464, 0.9932588338851929, 0.45576179027557373, 0.008800729177892208]], "normalized_score": [0.37258872389793396], "tokens": [["\u2581Thomas", "\u2581Ob", "ad", "iah", "\u2581Ch", "is", "am"]], "entropy": [[0.887442946434021, 1.1814154386520386, 1.0898698568344116, 0.006128197535872459, 0.06262323260307312, 0.7935805916786194, 1.6117007732391357]], "avg_entropy": [0.804680148139596], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "William M. Runyan", "score_of_biggest_cluster": 0.37258872389793396, "score_of_first_answer": 0.37258872389793396, "max_score_of_first_answer": 0.9993759989738464, "min_score_of_first_answer": 0.008800729177892208, "avg_score": 0.37258872389793396, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3316137148423034, "std_of_first_answer": 0.3316137148423034, "avg_entropy_of_first_answer": 0.804680148139596, "lex_sim": 1.0}
{"question": "when was the first ford f 150 made", "answer": ["November 1, 1976"], "scores": [[0.026719139888882637, 0.9579261541366577, 0.8491310477256775, 0.07410372048616409, 0.996629536151886, 0.9805352091789246, 0.9999126195907593, 0.7391791343688965, 0.7358701229095459]], "normalized_score": [0.45620954036712646], "tokens": [["\u2581November", "\u2581", "1", ",", "\u2581", "1", "9", "7", "6"]], "entropy": [[2.057183265686035, 0.2071380317211151, 0.6247023940086365, 0.7254647016525269, 0.023995989933609962, 0.09636330604553223, 0.001100506167858839, 0.8054080009460449, 1.0128772258758545]], "avg_entropy": [0.6171370468930237], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1975", "score_of_biggest_cluster": 0.45620954036712646, "score_of_first_answer": 0.45620954036712646, "max_score_of_first_answer": 0.9999126195907593, "min_score_of_first_answer": 0.026719139888882637, "avg_score": 0.45620954036712646, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36406310282359566, "std_of_first_answer": 0.36406310282359566, "avg_entropy_of_first_answer": 0.6171370468930237, "lex_sim": 1.0}
{"question": "who wrote the song mary had a little lamb", "answer": ["Mary Had a Little Lamb"], "scores": [[0.09151792526245117, 0.6736304759979248, 0.8763017654418945, 0.9973714351654053, 0.995631217956543]], "normalized_score": [0.5570669770240784], "tokens": [["\u2581Mary", "\u2581Had", "\u2581a", "\u2581Little", "\u2581Lamb"]], "entropy": [[3.389800548553467, 1.1596345901489258, 0.38743481040000916, 0.026106009259819984, 0.04146893694996834]], "avg_entropy": [1.000888979062438], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Roulstone", "score_of_biggest_cluster": 0.5570669770240784, "score_of_first_answer": 0.5570669770240784, "max_score_of_first_answer": 0.9973714351654053, "min_score_of_first_answer": 0.09151792526245117, "avg_score": 0.5570669770240784, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3388923648895035, "std_of_first_answer": 0.3388923648895035, "avg_entropy_of_first_answer": 1.000888979062438, "lex_sim": 1.0}
{"question": "where are unipolar neurons found in spinal cord", "answer": ["motor nuclei"], "scores": [[0.04868622124195099, 0.013469595462083817, 0.9950634837150574]], "normalized_score": [0.08673689514398575], "tokens": [["\u2581motor", "\u2581nuc", "lei"]], "entropy": [[4.897181987762451, 0.9544084072113037, 0.04591486603021622]], "avg_entropy": [1.9658350870013237], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the distal dorsal root", "score_of_biggest_cluster": 0.08673689514398575, "score_of_first_answer": 0.08673689514398575, "max_score_of_first_answer": 0.9950634837150574, "min_score_of_first_answer": 0.013469595462083817, "avg_score": 0.08673689514398575, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4546545324765217, "std_of_first_answer": 0.4546545324765217, "avg_entropy_of_first_answer": 1.9658350870013237, "lex_sim": 1.0}
{"question": "most home runs by 2 teammates in a season", "answer": ["Roger Maris and Mickey Mantle in 1961"], "scores": [[0.028392812237143517, 0.9615665078163147, 0.9985542893409729, 0.5816168785095215, 0.9958659410476685, 0.99935382604599, 0.9989224672317505, 0.9972307085990906, 0.1342829167842865, 0.9603639245033264, 0.998420238494873, 0.9999125003814697, 0.9987629652023315, 0.9901082515716553]], "normalized_score": [0.641559898853302], "tokens": [["\u2581Roger", "\u2581Mar", "is", "\u2581and", "\u2581Mic", "key", "\u2581Mant", "le", "\u2581in", "\u2581", "1", "9", "6", "1"]], "entropy": [[3.410691261291504, 0.26511865854263306, 0.015820225700736046, 1.5975520610809326, 0.041897356510162354, 0.00844665989279747, 0.011924901977181435, 0.023570146411657333, 1.9859787225723267, 0.23391227424144745, 0.013780580833554268, 0.0011586893815547228, 0.011940847150981426, 0.0669795498251915]], "avg_entropy": [0.5491979953866186], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "115", "score_of_biggest_cluster": 0.641559898853302, "score_of_first_answer": 0.641559898853302, "max_score_of_first_answer": 0.9999125003814697, "min_score_of_first_answer": 0.028392812237143517, "avg_score": 0.641559898853302, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3245888576190968, "std_of_first_answer": 0.3245888576190968, "avg_entropy_of_first_answer": 0.5491979953866186, "lex_sim": 1.0}
{"question": "who played forrest gumps mother in the 1994 film", "answer": ["Sally Field"], "scores": [[0.9018809795379639, 0.9827696084976196, 0.9360204935073853]], "normalized_score": [0.9396412968635559], "tokens": [["\u2581S", "ally", "\u2581Field"]], "entropy": [[0.7864294648170471, 0.11108075082302094, 0.32569262385368347]], "avg_entropy": [0.4077342798312505], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Sally Field", "score_of_biggest_cluster": 0.9396412968635559, "score_of_first_answer": 0.9396412968635559, "max_score_of_first_answer": 0.9827696084976196, "min_score_of_first_answer": 0.9018809795379639, "avg_score": 0.9396412968635559, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03315612305008489, "std_of_first_answer": 0.03315612305008489, "avg_entropy_of_first_answer": 0.4077342798312505, "lex_sim": 1.0}
{"question": "who does luke skywalker fall in love with", "answer": ["Princess Leia"], "scores": [[0.6149312257766724, 0.986868679523468, 0.9946739673614502]], "normalized_score": [0.8451274633407593], "tokens": [["\u2581Princess", "\u2581Le", "ia"]], "entropy": [[1.6323671340942383, 0.11406997591257095, 0.04253307357430458]], "avg_entropy": [0.596323394527038], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mara Jade", "score_of_biggest_cluster": 0.8451274633407593, "score_of_first_answer": 0.8451274633407593, "max_score_of_first_answer": 0.9946739673614502, "min_score_of_first_answer": 0.6149312257766724, "avg_score": 0.8451274633407593, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17720137377326986, "std_of_first_answer": 0.17720137377326986, "avg_entropy_of_first_answer": 0.596323394527038, "lex_sim": 1.0}
{"question": "who plays bianca in that's so raven", "answer": ["Bianca Lawson"], "scores": [[0.22277410328388214, 0.9975540041923523, 0.5969441533088684, 0.9998893737792969]], "normalized_score": [0.603492259979248], "tokens": [["\u2581Bian", "ca", "\u2581Law", "son"]], "entropy": [[2.620303153991699, 0.02212080918252468, 2.31781005859375, 0.0015230216085910797]], "avg_entropy": [1.2404392608441412], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Erica Rivera", "score_of_biggest_cluster": 0.603492259979248, "score_of_first_answer": 0.603492259979248, "max_score_of_first_answer": 0.9998893737792969, "min_score_of_first_answer": 0.22277410328388214, "avg_score": 0.603492259979248, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3227860961709548, "std_of_first_answer": 0.3227860961709548, "avg_entropy_of_first_answer": 1.2404392608441412, "lex_sim": 1.0}
{"question": "what is the job of the whip in congress", "answer": ["Whip"], "scores": [[0.06895656883716583, 0.8310226202011108]], "normalized_score": [0.2393835186958313], "tokens": [["\u2581Wh", "ip"]], "entropy": [[4.63047981262207, 0.5093930959701538]], "avg_entropy": [2.569936454296112], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "to ensure party discipline", "score_of_biggest_cluster": 0.2393835186958313, "score_of_first_answer": 0.2393835186958313, "max_score_of_first_answer": 0.8310226202011108, "min_score_of_first_answer": 0.06895656883716583, "avg_score": 0.2393835186958313, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3810330256819725, "std_of_first_answer": 0.3810330256819725, "avg_entropy_of_first_answer": 2.569936454296112, "lex_sim": 1.0}
{"question": "who played cory's older brother on boy meets world", "answer": ["Lee Norris"], "scores": [[0.0841483399271965, 0.9906703233718872, 0.9628404974937439]], "normalized_score": [0.43136313557624817], "tokens": [["\u2581Lee", "\u2581Nor", "ris"]], "entropy": [[3.6473214626312256, 0.07864320278167725, 0.2847575545310974]], "avg_entropy": [1.336907406648], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Will Friedle", "score_of_biggest_cluster": 0.43136313557624817, "score_of_first_answer": 0.43136313557624817, "max_score_of_first_answer": 0.9906703233718872, "min_score_of_first_answer": 0.0841483399271965, "avg_score": 0.43136313557624817, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42093236636013126, "std_of_first_answer": 0.42093236636013126, "avg_entropy_of_first_answer": 1.336907406648, "lex_sim": 1.0}
{"question": "who wrote hit me baby one more time", "answer": ["Rami Yacoub"], "scores": [[0.2351331263780594, 0.9607470035552979, 0.8006671071052551, 0.9956344962120056, 0.9993471503257751, 0.9995357990264893]], "normalized_score": [0.7513314485549927], "tokens": [["\u2581R", "ami", "\u2581Y", "ac", "ou", "b"]], "entropy": [[1.9138753414154053, 0.2801688611507416, 0.8057557344436646, 0.03703438490629196, 0.006595010869204998, 0.005627891048789024]], "avg_entropy": [0.5081762039723495], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Max Martin", "score_of_biggest_cluster": 0.7513314485549927, "score_of_first_answer": 0.7513314485549927, "max_score_of_first_answer": 0.9995357990264893, "min_score_of_first_answer": 0.2351331263780594, "avg_score": 0.7513314485549927, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2758795090717154, "std_of_first_answer": 0.2758795090717154, "avg_entropy_of_first_answer": 0.5081762039723495, "lex_sim": 1.0}
{"question": "what is the average height of a chinese man", "answer": ["The average height of a Chinese man is 5\u20195.96 feet or 167.18 centimeters."], "scores": [[0.009817374870181084, 0.8237938284873962, 0.8225376009941101, 0.7498986721038818, 0.6123497486114502, 0.7314009070396423, 0.8262451887130737, 0.8037176728248596, 0.7948132157325745, 0.5469163060188293, 0.08854324370622635, 0.05964869260787964, 0.18721961975097656, 0.0731331929564476, 0.028706466779112816, 0.01481934916228056, 0.07966986298561096, 0.949979841709137, 0.9774407744407654, 0.5615741014480591, 0.7085080146789551, 0.9126576781272888, 0.03749576956033707, 0.09337301552295685, 0.4111877977848053, 0.9044400453567505, 0.9991119503974915, 0.5958706736564636]], "normalized_score": [0.2822117805480957], "tokens": [["\u2581The", "\u2581average", "\u2581height", "\u2581of", "\u2581a", "\u2581Chinese", "\u2581man", "\u2581is", "\u2581", "5", "\u2019", "5", ".", "9", "6", "\u2581feet", "\u2581or", "\u2581", "1", "6", "7", ".", "1", "8", "\u2581cent", "im", "eters", "."]], "entropy": [[0.9343311786651611, 1.2915409803390503, 0.9848966598510742, 0.8309412002563477, 1.485846996307373, 1.134488821029663, 0.8276404142379761, 1.1154483556747437, 1.189597249031067, 0.8630520701408386, 2.1466994285583496, 2.149440288543701, 2.2658820152282715, 2.5324671268463135, 2.5622613430023193, 2.4808759689331055, 2.0249338150024414, 0.3496183454990387, 0.13667866587638855, 0.910140335559845, 0.838703453540802, 0.43303436040878296, 2.1261680126190186, 2.5675220489501953, 1.6444523334503174, 0.3888758420944214, 0.009953614324331284, 1.3727340698242188]], "avg_entropy": [1.3427937497783984], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "5\u00a0ft 6\u00a0in", "score_of_biggest_cluster": 0.2822117805480957, "score_of_first_answer": 0.2822117805480957, "max_score_of_first_answer": 0.9991119503974915, "min_score_of_first_answer": 0.009817374870181084, "avg_score": 0.2822117805480957, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.35827782770207245, "std_of_first_answer": 0.35827782770207245, "avg_entropy_of_first_answer": 1.3427937497783984, "lex_sim": 1.0}
{"question": "when is winter big brother going to start", "answer": ["January 5, 2011"], "scores": [[0.25126025080680847, 0.9611520171165466, 0.25945815443992615, 0.7017938494682312, 0.997418999671936, 0.9968020915985107, 0.9997310042381287, 0.93499356508255, 0.042735811322927475]], "normalized_score": [0.4938257038593292], "tokens": [["\u2581January", "\u2581", "5", ",", "\u2581", "2", "0", "1", "1"]], "entropy": [[3.152132511138916, 0.22185827791690826, 1.6741459369659424, 0.8221648335456848, 0.024465903639793396, 0.026460785418748856, 0.003193800337612629, 0.2811706066131592, 1.4712278842926025]], "avg_entropy": [0.8529800599854853], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "February 7, 2018", "score_of_biggest_cluster": 0.4938257038593292, "score_of_first_answer": 0.4938257038593292, "max_score_of_first_answer": 0.9997310042381287, "min_score_of_first_answer": 0.042735811322927475, "avg_score": 0.4938257038593292, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3673625113263993, "std_of_first_answer": 0.3673625113263993, "avg_entropy_of_first_answer": 0.8529800599854853, "lex_sim": 1.0}
{"question": "who is the founder of the ubuntu project", "answer": ["Mark Shuttleworth"], "scores": [[0.8237715363502502, 0.9942173957824707, 0.9999247789382935, 0.9988207221031189, 0.993535041809082]], "normalized_score": [0.9593680500984192], "tokens": [["\u2581Mark", "\u2581Sh", "utt", "le", "worth"]], "entropy": [[0.8757519721984863, 0.06117375195026398, 0.0009382015559822321, 0.012438375502824783, 0.05791936069726944]], "avg_entropy": [0.20164433238096535], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Mark Richard Shuttleworth", "score_of_biggest_cluster": 0.9593680500984192, "score_of_first_answer": 0.9593680500984192, "max_score_of_first_answer": 0.9999247789382935, "min_score_of_first_answer": 0.8237715363502502, "avg_score": 0.9593680500984192, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06918607883994485, "std_of_first_answer": 0.06918607883994485, "avg_entropy_of_first_answer": 0.20164433238096535, "lex_sim": 1.0}
{"question": "how many pages are in the book inside out and back again", "answer": ["247 pages"], "scores": [[0.9661946296691895, 0.5059065818786621, 0.15361087024211884, 0.03382503613829613, 0.1703823208808899]], "normalized_score": [0.2124437391757965], "tokens": [["\u2581", "2", "4", "7", "\u2581pages"]], "entropy": [[0.33042219281196594, 1.1832458972930908, 2.196004629135132, 1.5543596744537354, 0.7666560411453247]], "avg_entropy": [1.2061376869678497], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "263", "score_of_biggest_cluster": 0.2124437391757965, "score_of_first_answer": 0.2124437391757965, "max_score_of_first_answer": 0.9661946296691895, "min_score_of_first_answer": 0.03382503613829613, "avg_score": 0.2124437391757965, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3386806960186738, "std_of_first_answer": 0.3386806960186738, "avg_entropy_of_first_answer": 1.2061376869678497, "lex_sim": 1.0}
{"question": "who sings the song you'll never find another love like mine", "answer": ["Lou Rawl"], "scores": [[0.6603484153747559, 0.9967132806777954, 0.0018656215397641063]], "normalized_score": [0.10708344727754593], "tokens": [["\u2581Lou", "\u2581Raw", "l"]], "entropy": [[1.6878341436386108, 0.030779791995882988, 0.0236856359988451]], "avg_entropy": [0.5807665238777796], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Lou Rawls", "score_of_biggest_cluster": 0.10708344727754593, "score_of_first_answer": 0.10708344727754593, "max_score_of_first_answer": 0.9967132806777954, "min_score_of_first_answer": 0.0018656215397641063, "avg_score": 0.10708344727754593, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.41318045254390684, "std_of_first_answer": 0.41318045254390684, "avg_entropy_of_first_answer": 0.5807665238777796, "lex_sim": 1.0}
{"question": "swan lake the sleeping beauty and the nutcracker are three famous ballets by", "answer": ["Tchaikovsky"], "scores": [[0.3183706998825073, 0.9957267045974731, 0.9986844658851624, 0.9974457025527954, 0.9918935894966125]], "normalized_score": [0.7928137183189392], "tokens": [["\u2581T", "cha", "ik", "ov", "sky"]], "entropy": [[1.6320228576660156, 0.04136066138744354, 0.013326277025043964, 0.022314824163913727, 0.0621492974460125]], "avg_entropy": [0.3542347835376859], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Pyotr Ilyich Tchaikovsky", "score_of_biggest_cluster": 0.7928137183189392, "score_of_first_answer": 0.7928137183189392, "max_score_of_first_answer": 0.9986844658851624, "min_score_of_first_answer": 0.3183706998825073, "avg_score": 0.7928137183189392, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.27103643976374914, "std_of_first_answer": 0.27103643976374914, "avg_entropy_of_first_answer": 0.3542347835376859, "lex_sim": 1.0}
{"question": "who made delhi as capital for the first time", "answer": ["Sultan of Mughal Empire, Akbar [1556 to 1605]"], "scores": [[0.011713421903550625, 0.05627477914094925, 0.005762295797467232, 0.14542511105537415, 0.9774503707885742, 0.26022869348526, 0.16827307641506195, 0.014563947916030884, 0.9882760643959045, 0.0008368423441424966, 0.5294402241706848, 0.8728645443916321, 0.28250542283058167, 0.9505179524421692, 0.058763839304447174, 0.9907671809196472, 0.9942323565483093, 0.9022014737129211, 0.9971959590911865, 0.9738527536392212, 0.7242210507392883]], "normalized_score": [0.1954825222492218], "tokens": [["\u2581Sultan", "\u2581of", "\u2581M", "ug", "hal", "\u2581Empire", ",", "\u2581Ak", "bar", "\u2581[", "1", "5", "5", "6", "\u2581to", "\u2581", "1", "6", "0", "5", "]"]], "entropy": [[4.241314888000488, 2.9639549255371094, 0.6646472215652466, 2.1862082481384277, 0.16529719531536102, 2.827578544616699, 2.5414676666259766, 3.414857864379883, 0.08760324865579605, 1.6783506870269775, 2.9304490089416504, 0.6661313772201538, 1.1589624881744385, 0.24969594180583954, 1.8108043670654297, 0.07487565279006958, 0.04293898493051529, 0.3720322251319885, 0.026098616421222687, 0.16452962160110474, 1.201556921005249]], "avg_entropy": [1.4033026521404584], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Pandavas", "score_of_biggest_cluster": 0.1954825222492218, "score_of_first_answer": 0.1954825222492218, "max_score_of_first_answer": 0.9971959590911865, "min_score_of_first_answer": 0.0008368423441424966, "avg_score": 0.1954825222492218, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4180428557232569, "std_of_first_answer": 0.4180428557232569, "avg_entropy_of_first_answer": 1.4033026521404584, "lex_sim": 1.0}
{"question": "what is the filename extension used for all java source files", "answer": [".java"], "scores": [[0.8470229506492615, 0.972027599811554]], "normalized_score": [0.907375156879425], "tokens": [["\u2581.", "java"]], "entropy": [[0.8363193273544312, 0.1872318536043167]], "avg_entropy": [0.5117755904793739], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": ".java", "score_of_biggest_cluster": 0.907375156879425, "score_of_first_answer": 0.907375156879425, "max_score_of_first_answer": 0.972027599811554, "min_score_of_first_answer": 0.8470229506492615, "avg_score": 0.907375156879425, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06250232458114624, "std_of_first_answer": 0.06250232458114624, "avg_entropy_of_first_answer": 0.5117755904793739, "lex_sim": 1.0}
{"question": "who voices hiccup in how to train your dragon 2", "answer": ["Jay Baruchel"], "scores": [[0.9929172992706299, 0.9922078251838684, 0.9934089183807373, 0.9920300245285034]], "normalized_score": [0.9926408529281616], "tokens": [["\u2581Jay", "\u2581Bar", "uch", "el"]], "entropy": [[0.07681076228618622, 0.07055602967739105, 0.06365681439638138, 0.0719076544046402]], "avg_entropy": [0.07073281519114971], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Jay Baruchel", "score_of_biggest_cluster": 0.9926408529281616, "score_of_first_answer": 0.9926408529281616, "max_score_of_first_answer": 0.9934089183807373, "min_score_of_first_answer": 0.9920300245285034, "avg_score": 0.9926408529281616, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0005538436993844586, "std_of_first_answer": 0.0005538436993844586, "avg_entropy_of_first_answer": 0.07073281519114971, "lex_sim": 1.0}
{"question": "who has won the most superbowls as a player", "answer": ["Tom Brady"], "scores": [[0.3236367106437683, 0.9959509372711182, 0.9991329312324524]], "normalized_score": [0.6854456067085266], "tokens": [["\u2581Tom", "\u2581Br", "ady"]], "entropy": [[1.6981174945831299, 0.041421521455049515, 0.010592099279165268]], "avg_entropy": [0.5833770384391149], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charles Haley", "score_of_biggest_cluster": 0.6854456067085266, "score_of_first_answer": 0.6854456067085266, "max_score_of_first_answer": 0.9991329312324524, "min_score_of_first_answer": 0.3236367106437683, "avg_score": 0.6854456067085266, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.317684624962958, "std_of_first_answer": 0.317684624962958, "avg_entropy_of_first_answer": 0.5833770384391149, "lex_sim": 1.0}
{"question": "what are some special products made in mississippi", "answer": ["Catfish and hushpuppies"], "scores": [[0.08939486742019653, 0.8954167366027832, 0.04165489599108696, 0.05504707247018814, 0.468966543674469, 0.5838975310325623, 0.9841512441635132, 0.9982155561447144]], "normalized_score": [0.28952616453170776], "tokens": [["\u2581Cat", "fish", "\u2581and", "\u2581h", "ush", "pu", "pp", "ies"]], "entropy": [[5.149213790893555, 0.6118309497833252, 2.0440902709960938, 5.2609663009643555, 1.0941319465637207, 0.8055092096328735, 0.0853910893201828, 0.017580300569534302]], "avg_entropy": [1.883589232340455], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "farm-raised catfish", "score_of_biggest_cluster": 0.28952616453170776, "score_of_first_answer": 0.28952616453170776, "max_score_of_first_answer": 0.9982155561447144, "min_score_of_first_answer": 0.04165489599108696, "avg_score": 0.28952616453170776, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.390821841293215, "std_of_first_answer": 0.390821841293215, "avg_entropy_of_first_answer": 1.883589232340455, "lex_sim": 1.0}
{"question": "when did we first put a rover on mars", "answer": ["January 4, 2004"], "scores": [[0.2877413332462311, 0.9953201413154602, 0.425449401140213, 0.9840314984321594, 0.9989657402038574, 0.8596128821372986, 0.9998929500579834, 0.9456285238265991, 0.995520830154419]], "normalized_score": [0.7715743184089661], "tokens": [["\u2581January", "\u2581", "4", ",", "\u2581", "2", "0", "0", "4"]], "entropy": [[1.7849526405334473, 0.03531331568956375, 1.3842886686325073, 0.10861598700284958, 0.010271448642015457, 0.4062901735305786, 0.0013604953419417143, 0.22671854496002197, 0.034651584923267365]], "avg_entropy": [0.44360698436179924], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "January 2004", "score_of_biggest_cluster": 0.7715743184089661, "score_of_first_answer": 0.7715743184089661, "max_score_of_first_answer": 0.9998929500579834, "min_score_of_first_answer": 0.2877413332462311, "avg_score": 0.7715743184089661, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.25986419115679965, "std_of_first_answer": 0.25986419115679965, "avg_entropy_of_first_answer": 0.44360698436179924, "lex_sim": 1.0}
{"question": "who won the king of dance season 2", "answer": ["Niharika Tiwari"], "scores": [[0.011219966225326061, 0.02816946804523468, 0.9459919333457947, 0.13862153887748718, 0.00311014405451715, 0.31723982095718384, 0.9763514995574951]], "normalized_score": [0.08770765364170074], "tokens": [["\u2581N", "ih", "ar", "ika", "\u2581T", "iw", "ari"]], "entropy": [[4.375231742858887, 3.920875072479248, 0.4390082359313965, 4.087820053100586, 2.902646541595459, 4.026120185852051, 0.15592074394226074]], "avg_entropy": [2.8439460822514127], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "LAAB Crew From Team Sherif", "score_of_biggest_cluster": 0.08770765364170074, "score_of_first_answer": 0.08770765364170074, "max_score_of_first_answer": 0.9763514995574951, "min_score_of_first_answer": 0.00311014405451715, "avg_score": 0.08770765364170074, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4021075617773887, "std_of_first_answer": 0.4021075617773887, "avg_entropy_of_first_answer": 2.8439460822514127, "lex_sim": 1.0}
{"question": "who sings gone gone gone she been gone so long", "answer": ["CARS"], "scores": [[0.004243828821927309, 0.006224115379154682, 0.032707106322050095]], "normalized_score": [0.009524142369627953], "tokens": [["\u2581C", "AR", "S"]], "entropy": [[5.231739521026611, 4.275467872619629, 2.6632981300354004]], "avg_entropy": [4.056835174560547], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chilliwack", "score_of_biggest_cluster": 0.009524142369627953, "score_of_first_answer": 0.009524142369627953, "max_score_of_first_answer": 0.032707106322050095, "min_score_of_first_answer": 0.004243828821927309, "avg_score": 0.009524142369627953, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.012976168370342147, "std_of_first_answer": 0.012976168370342147, "avg_entropy_of_first_answer": 4.056835174560547, "lex_sim": 1.0}
{"question": "who cracked the enigma code in world war 2", "answer": ["Alan Turing"], "scores": [[0.8729552626609802, 0.956420361995697, 0.9997686743736267]], "normalized_score": [0.9415573477745056], "tokens": [["\u2581Alan", "\u2581T", "uring"]], "entropy": [[0.9382033348083496, 0.24057208001613617, 0.0026679220609366894]], "avg_entropy": [0.3938144456284742], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Turing", "score_of_biggest_cluster": 0.9415573477745056, "score_of_first_answer": 0.9415573477745056, "max_score_of_first_answer": 0.9997686743736267, "min_score_of_first_answer": 0.8729552626609802, "avg_score": 0.9415573477745056, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.052627770851487295, "std_of_first_answer": 0.052627770851487295, "avg_entropy_of_first_answer": 0.3938144456284742, "lex_sim": 1.0}
{"question": "who won the mens single ice skating 2018", "answer": ["Yuzuru Hanyu"], "scores": [[0.7505016922950745, 0.9999508857727051, 0.9945834279060364, 0.9922471046447754, 0.9951574206352234, 0.7411483526229858]], "normalized_score": [0.9041310548782349], "tokens": [["\u2581Y", "uz", "uru", "\u2581H", "any", "u"]], "entropy": [[1.546026587486267, 0.0006521014147438109, 0.042765602469444275, 0.07177837193012238, 0.04534659534692764, 0.887323260307312]], "avg_entropy": [0.43231541982580285], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Yuzuru Hanyu", "score_of_biggest_cluster": 0.9041310548782349, "score_of_first_answer": 0.9041310548782349, "max_score_of_first_answer": 0.9999508857727051, "min_score_of_first_answer": 0.7411483526229858, "avg_score": 0.9041310548782349, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1177438627755321, "std_of_first_answer": 0.1177438627755321, "avg_entropy_of_first_answer": 0.43231541982580285, "lex_sim": 1.0}
{"question": "when does the miz and maryse show start", "answer": ["August 20, 2018"], "scores": [[0.046206988394260406, 0.9889916181564331, 0.8660688996315002, 0.5755217671394348, 0.5648849010467529, 0.9957213401794434, 0.99431312084198, 0.9996836185455322, 0.9893742203712463, 0.9250544905662537]], "normalized_score": [0.6406968235969543], "tokens": [["\u2581August", "\u2581", "2", "0", ",", "\u2581", "2", "0", "1", "8"]], "entropy": [[3.0872159004211426, 0.07638205587863922, 0.6131722927093506, 1.53854501247406, 1.1344971656799316, 0.03861364349722862, 0.04478657990694046, 0.0036786426790058613, 0.06935328245162964, 0.3939906656742096]], "avg_entropy": [0.7000235241372138], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2018", "score_of_biggest_cluster": 0.6406968235969543, "score_of_first_answer": 0.6406968235969543, "max_score_of_first_answer": 0.9996836185455322, "min_score_of_first_answer": 0.046206988394260406, "avg_score": 0.6406968235969543, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2968336379147836, "std_of_first_answer": 0.2968336379147836, "avg_entropy_of_first_answer": 0.7000235241372138, "lex_sim": 1.0}
{"question": "name two fibres which are made of proteins", "answer": ["silk"], "scores": [[0.08457744121551514, 0.9916815161705017]], "normalized_score": [0.2896098792552948], "tokens": [["\u2581sil", "k"]], "entropy": [[2.336111307144165, 0.061776868999004364]], "avg_entropy": [1.1989440880715847], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "feathers", "score_of_biggest_cluster": 0.2896098792552948, "score_of_first_answer": 0.2896098792552948, "max_score_of_first_answer": 0.9916815161705017, "min_score_of_first_answer": 0.08457744121551514, "avg_score": 0.2896098792552948, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4535520374774933, "std_of_first_answer": 0.4535520374774933, "avg_entropy_of_first_answer": 1.1989440880715847, "lex_sim": 1.0}
{"question": "when does the nba season end this year", "answer": ["June 16, 2019"], "scores": [[0.387928307056427, 0.9681077599525452, 0.8691323399543762, 0.04179074615240097, 0.401655912399292, 0.9948666095733643, 0.9977133274078369, 0.9996224641799927, 0.7856458425521851, 0.8294124603271484]], "normalized_score": [0.5687611699104309], "tokens": [["\u2581June", "\u2581", "1", "6", ",", "\u2581", "2", "0", "1", "9"]], "entropy": [[2.0158863067626953, 0.18041051924228668, 0.6063615679740906, 1.2256602048873901, 1.0256811380386353, 0.04641163349151611, 0.017721721902489662, 0.004618290811777115, 0.6579133868217468, 0.7565662860870361]], "avg_entropy": [0.6537231056019663], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "April 11, 2018", "score_of_biggest_cluster": 0.5687611699104309, "score_of_first_answer": 0.5687611699104309, "max_score_of_first_answer": 0.9996224641799927, "min_score_of_first_answer": 0.04179074615240097, "avg_score": 0.5687611699104309, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31653595251870414, "std_of_first_answer": 0.31653595251870414, "avg_entropy_of_first_answer": 0.6537231056019663, "lex_sim": 1.0}
{"question": "who picks the chief justice of the illinois supreme court", "answer": ["Gov."], "scores": [[0.004679083824157715, 0.6865628957748413, 0.8845678567886353, 0.37160101532936096, 0.9948963522911072, 0.9992266893386841, 0.9992313385009766]], "normalized_score": [0.3753132224082947], "tokens": [["\u2581G", "ov", ".", "\u2581Bruce", "\u2581Ra", "un", "er"]], "entropy": [[3.4349334239959717, 1.4640703201293945, 0.7744004726409912, 2.375025510787964, 0.053047291934490204, 0.009166296571493149, 0.008985666558146477]], "avg_entropy": [1.1599469975169217], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the court", "score_of_biggest_cluster": 0.3753132224082947, "score_of_first_answer": 0.3753132224082947, "max_score_of_first_answer": 0.9992313385009766, "min_score_of_first_answer": 0.004679083824157715, "avg_score": 0.3753132224082947, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3568443803861938, "std_of_first_answer": 0.3568443803861938, "avg_entropy_of_first_answer": 1.1599469975169217, "lex_sim": 1.0}
{"question": "when did ireland gain its independence from england", "answer": ["1922"], "scores": [[0.273713618516922, 0.6378297209739685, 0.8385794758796692, 0.829041063785553, 0.6550388932228088]], "normalized_score": [0.6026673913002014], "tokens": [["\u2581", "1", "9", "2", "2"]], "entropy": [[2.474484920501709, 0.9343948364257812, 0.7271566390991211, 0.6057106256484985, 0.7484132051467896]], "avg_entropy": [1.09803204536438], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "In 1922", "score_of_biggest_cluster": 0.6026673913002014, "score_of_first_answer": 0.6026673913002014, "max_score_of_first_answer": 0.8385794758796692, "min_score_of_first_answer": 0.273713618516922, "avg_score": 0.6026673913002014, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.20461327317186517, "std_of_first_answer": 0.20461327317186517, "avg_entropy_of_first_answer": 1.09803204536438, "lex_sim": 1.0}
{"question": "how many wars held between india and pakistan", "answer": ["2"], "scores": [[0.71501225233078, 0.15816281735897064]], "normalized_score": [0.33628609776496887], "tokens": [["\u2581", "2"]], "entropy": [[1.807267189025879, 1.6288131475448608]], "avg_entropy": [1.7180401682853699], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "four", "score_of_biggest_cluster": 0.33628609776496887, "score_of_first_answer": 0.33628609776496887, "max_score_of_first_answer": 0.71501225233078, "min_score_of_first_answer": 0.15816281735897064, "avg_score": 0.33628609776496887, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2784247174859047, "std_of_first_answer": 0.2784247174859047, "avg_entropy_of_first_answer": 1.7180401682853699, "lex_sim": 1.0}
{"question": "where does aarp fall on the political spectrum", "answer": ["neither on a political scale nor on a left-wing (liberal), center-wing (moderate), or right-wing (conservative) scale."], "scores": [[0.005069661419838667, 0.002701211255043745, 0.016282036900520325, 0.10858357697725296, 0.07029115408658981, 0.03739834204316139, 0.3959195613861084, 0.5365382432937622, 0.0386984683573246, 0.5653988122940063, 0.031000681221485138, 0.0014500065008178353, 0.700904369354248, 0.9978833794593811, 0.995043158531189, 0.03190140426158905, 0.16867618262767792, 0.41375407576560974, 0.8430118560791016, 0.9182848334312439, 0.8867692351341248, 0.9990400671958923, 0.9958659410476685, 0.7284377217292786, 0.7207803130149841, 0.9794331789016724, 0.995431661605835, 0.9961197376251221, 0.9906013011932373, 0.9916900396347046, 0.9995249509811401, 0.997840166091919, 0.9866923689842224, 0.6879593729972839, 0.2844236493110657]], "normalized_score": [0.2533908784389496], "tokens": [["\u2581neither", "\u2581on", "\u2581a", "\u2581political", "\u2581scale", "\u2581nor", "\u2581on", "\u2581a", "\u2581left", "-", "wing", "\u2581(", "li", "ber", "al", "),", "\u2581center", "-", "wing", "\u2581(", "mod", "er", "ate", "),", "\u2581or", "\u2581right", "-", "wing", "\u2581(", "con", "serv", "ative", ")", "\u2581scale", "."]], "entropy": [[4.036741256713867, 2.3680920600891113, 1.0416016578674316, 3.5978262424468994, 1.2897601127624512, 1.7297388315200806, 2.819027900695801, 1.5242550373077393, 4.097192287445068, 1.483942985534668, 0.7977328300476074, 2.1591391563415527, 1.746102213859558, 0.017677146941423416, 0.03779538720846176, 0.6858917474746704, 2.139770746231079, 1.4035868644714355, 0.8937671780586243, 0.37688857316970825, 0.7030540704727173, 0.009327077306807041, 0.03171376883983612, 0.6827093362808228, 0.9073903560638428, 0.14629116654396057, 0.03658178448677063, 0.03672420233488083, 0.0839383527636528, 0.08156765252351761, 0.0054518380202353, 0.01970422826707363, 0.09818577021360397, 1.5634820461273193, 1.1063551902770996]], "avg_entropy": [1.1359716301916964], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "non-partisan", "score_of_biggest_cluster": 0.2533908784389496, "score_of_first_answer": 0.2533908784389496, "max_score_of_first_answer": 0.9995249509811401, "min_score_of_first_answer": 0.0014500065008178353, "avg_score": 0.2533908784389496, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.40381982343204614, "std_of_first_answer": 0.40381982343204614, "avg_entropy_of_first_answer": 1.1359716301916964, "lex_sim": 1.0}
{"question": "who plays christian in 50 shades of grey", "answer": ["Jamie Dornan"], "scores": [[0.9775739312171936, 0.9993256330490112, 0.9836276769638062, 0.9933581948280334, 0.9997552037239075]], "normalized_score": [0.9906890988349915], "tokens": [["\u2581Jam", "ie", "\u2581D", "orn", "an"]], "entropy": [[0.21096760034561157, 0.008310998789966106, 0.1479823887348175, 0.04669760912656784, 0.003019282128661871]], "avg_entropy": [0.08339557582512498], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "James \"Jamie\" Dornan", "score_of_biggest_cluster": 0.9906890988349915, "score_of_first_answer": 0.9906890988349915, "max_score_of_first_answer": 0.9997552037239075, "min_score_of_first_answer": 0.9775739312171936, "avg_score": 0.9906890988349915, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.008783754762412662, "std_of_first_answer": 0.008783754762412662, "avg_entropy_of_first_answer": 0.08339557582512498, "lex_sim": 1.0}
{"question": "who played in the stanley cup finals last year", "answer": ["Washington Capitals"], "scores": [[0.12220875173807144, 0.973520815372467, 0.9980611205101013]], "normalized_score": [0.49151289463043213], "tokens": [["\u2581Washington", "\u2581Capit", "als"]], "entropy": [[3.5869598388671875, 0.19388817250728607, 0.015244893729686737]], "avg_entropy": [1.2653643017013867], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nashville Predators", "score_of_biggest_cluster": 0.49151289463043213, "score_of_first_answer": 0.49151289463043213, "max_score_of_first_answer": 0.9980611205101013, "min_score_of_first_answer": 0.12220875173807144, "avg_score": 0.49151289463043213, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4072198189006464, "std_of_first_answer": 0.4072198189006464, "avg_entropy_of_first_answer": 1.2653643017013867, "lex_sim": 1.0}
{"question": "who pays medical bills in great britain where does the money come from to pay these bills", "answer": ["National Health Service"], "scores": [[0.20658381283283234, 0.8498929142951965, 0.9402150511741638]], "normalized_score": [0.548566460609436], "tokens": [["\u2581National", "\u2581Health", "\u2581Service"]], "entropy": [[3.586237907409668, 0.5831716060638428, 0.3590477406978607]], "avg_entropy": [1.5094857513904572], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "general taxation", "score_of_biggest_cluster": 0.548566460609436, "score_of_first_answer": 0.548566460609436, "max_score_of_first_answer": 0.9402150511741638, "min_score_of_first_answer": 0.20658381283283234, "avg_score": 0.548566460609436, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3266359649356405, "std_of_first_answer": 0.3266359649356405, "avg_entropy_of_first_answer": 1.5094857513904572, "lex_sim": 1.0}
{"question": "who was the first signatory of the israeli declaration of independence", "answer": ["Mordechai Modai"], "scores": [[0.13224242627620697, 0.9985021352767944, 0.8114246726036072, 0.9949566721916199, 0.016630711033940315, 0.17461445927619934]], "normalized_score": [0.2600921392440796], "tokens": [["\u2581Mor", "de", "ch", "ai", "\u2581Mod", "ai"]], "entropy": [[2.1597485542297363, 0.0136692114174366, 0.6477497220039368, 0.03680182248353958, 2.697035312652588, 2.7520570755004883]], "avg_entropy": [1.3845102830479543], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "David Ben-Gurion", "score_of_biggest_cluster": 0.2600921392440796, "score_of_first_answer": 0.2600921392440796, "max_score_of_first_answer": 0.9985021352767944, "min_score_of_first_answer": 0.016630711033940315, "avg_score": 0.2600921392440796, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4208112291291807, "std_of_first_answer": 0.4208112291291807, "avg_entropy_of_first_answer": 1.3845102830479543, "lex_sim": 1.0}
{"question": "when was the movie cool hand luke made", "answer": ["1967"], "scores": [[0.5197361707687378, 0.9656436443328857, 0.981583297252655, 0.9895517230033875, 0.969595730304718]], "normalized_score": [0.8608177900314331], "tokens": [["\u2581", "1", "9", "6", "7"]], "entropy": [[1.7542645931243896, 0.21383139491081238, 0.14149683713912964, 0.0773276686668396, 0.18166743218898773]], "avg_entropy": [0.4737175852060318], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "1967", "score_of_biggest_cluster": 0.8608177900314331, "score_of_first_answer": 0.8608177900314331, "max_score_of_first_answer": 0.9895517230033875, "min_score_of_first_answer": 0.5197361707687378, "avg_score": 0.8608177900314331, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.18294078239786551, "std_of_first_answer": 0.18294078239786551, "avg_entropy_of_first_answer": 0.4737175852060318, "lex_sim": 1.0}
{"question": "when did the us not go to the olympics", "answer": ["After not going to 1980 Olympics, the United States of America boycotted the 1984 Summer Olympics in Moscow, but the countries still attended the 1984 Winter Olympics in Sarajevo and went to"], "scores": [[0.0043711052276194096, 0.004278019070625305, 0.07095328718423843, 0.7236060500144958, 0.06852217018604279, 0.9792831540107727, 0.9939411878585815, 0.9305090308189392, 0.9682044982910156, 0.079157754778862, 0.34196680784225464, 0.48729419708251953, 0.3036167323589325, 0.9955899715423584, 0.00688669690862298, 0.9852176308631897, 0.23026180267333984, 0.9799789190292358, 0.9799875020980835, 0.8878353834152222, 0.8425509333610535, 0.9966078996658325, 0.999295711517334, 0.999057948589325, 0.9961177110671997, 0.669640302658081, 0.9295231699943542, 0.5089181661605835, 0.16894765198230743, 0.3969392776489258, 0.005674303974956274, 0.37013527750968933, 0.0019411544781178236, 0.004245912656188011, 0.07487913966178894, 0.8505986332893372, 0.7933640480041504, 0.9979779124259949, 0.9995275735855103, 0.9933411478996277, 0.9433834552764893, 0.955619752407074, 0.971197783946991, 0.651515543460846, 0.948181688785553, 0.9988127946853638, 0.9994246959686279, 0.026160528883337975, 0.022310946136713028, 0.41522642970085144]], "normalized_score": [0.28562626242637634], "tokens": [["\u2581After", "\u2581not", "\u2581going", "\u2581to", "\u2581", "1", "9", "8", "0", "\u2581Olympics", ",", "\u2581the", "\u2581United", "\u2581States", "\u2581of", "\u2581America", "\u2581boy", "c", "otted", "\u2581the", "\u2581", "1", "9", "8", "4", "\u2581Summer", "\u2581Olympics", "\u2581in", "\u2581Moscow", ",", "\u2581but", "\u2581the", "\u2581countries", "\u2581still", "\u2581attended", "\u2581the", "\u2581", "1", "9", "8", "4", "\u2581Winter", "\u2581Olympics", "\u2581in", "\u2581Sar", "aje", "vo", "\u2581and", "\u2581went", "\u2581to"]], "entropy": [[1.6330041885375977, 1.7937204837799072, 2.4559741020202637, 0.7843616008758545, 0.8141977787017822, 0.1330995261669159, 0.04664267972111702, 0.3809007406234741, 0.15325681865215302, 2.645322322845459, 2.4045872688293457, 2.554551362991333, 1.7564778327941895, 0.03912699967622757, 3.132206678390503, 0.12565037608146667, 3.528140068054199, 0.10409620404243469, 0.10290329158306122, 0.5799077749252319, 0.8345410823822021, 0.02775839902460575, 0.0075697703287005424, 0.009424572810530663, 0.029483316466212273, 1.2825806140899658, 0.3414197862148285, 2.0113742351531982, 1.199320673942566, 2.186760425567627, 2.6641364097595215, 2.800351858139038, 2.3333280086517334, 3.027606725692749, 2.368295192718506, 0.8142440319061279, 1.0808978080749512, 0.01696927659213543, 0.0052967676892876625, 0.043843064457178116, 0.24101005494594574, 0.2752566337585449, 0.1750427633523941, 1.1499946117401123, 0.3493693172931671, 0.012386264279484749, 0.0069445474073290825, 1.2456562519073486, 2.499325752258301, 0.9898234009742737]], "avg_entropy": [1.1039627943374217], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The 1980 Summer Olympics", "score_of_biggest_cluster": 0.28562626242637634, "score_of_first_answer": 0.28562626242637634, "max_score_of_first_answer": 0.9995275735855103, "min_score_of_first_answer": 0.0019411544781178236, "avg_score": 0.28562626242637634, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3989502210022448, "std_of_first_answer": 0.3989502210022448, "avg_entropy_of_first_answer": 1.1039627943374217, "lex_sim": 1.0}
{"question": "what is the song in red dead redemption", "answer": ["Faraway Horses are the Ones That Never Come Home"], "scores": [[0.2256036400794983, 0.02799898386001587, 0.03356574475765228, 0.23641541600227356, 0.9889686107635498, 0.0002929733891505748, 0.8793051242828369, 0.02752496488392353, 0.999515175819397, 0.055806685239076614, 0.6926028728485107, 0.9881821274757385, 0.9324873685836792]], "normalized_score": [0.14498953521251678], "tokens": [["\u2581Far", "away", "\u2581H", "ors", "es", "\u2581are", "\u2581the", "\u2581On", "es", "\u2581That", "\u2581Never", "\u2581Come", "\u2581Home"]], "entropy": [[3.5093441009521484, 0.521774411201477, 4.406269073486328, 1.2245774269104004, 0.07294617593288422, 1.011421799659729, 0.8964398503303528, 4.22198486328125, 0.005909639410674572, 2.245692491531372, 1.510016679763794, 0.08851736783981323, 0.3393024206161499]], "avg_entropy": [1.542630484685875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Far Away", "score_of_biggest_cluster": 0.14498953521251678, "score_of_first_answer": 0.14498953521251678, "max_score_of_first_answer": 0.999515175819397, "min_score_of_first_answer": 0.0002929733891505748, "avg_score": 0.14498953521251678, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42401600745264784, "std_of_first_answer": 0.42401600745264784, "avg_entropy_of_first_answer": 1.542630484685875, "lex_sim": 1.0}
{"question": "where was the remake of wake in fright filmed", "answer": ["Broken Hill, New South Wales"], "scores": [[0.15039478242397308, 0.9914862513542175, 0.9755029082298279, 0.4049937427043915, 0.8063717484474182, 0.9981828331947327, 0.9989513158798218]], "normalized_score": [0.6468204259872437], "tokens": [["\u2581Bro", "ken", "\u2581Hill", ",", "\u2581New", "\u2581South", "\u2581Wales"]], "entropy": [[3.704312801361084, 0.053229596465826035, 0.16896510124206543, 1.107352614402771, 0.9637242555618286, 0.015926018357276917, 0.010857154615223408]], "avg_entropy": [0.8606239345722965], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Broken", "score_of_biggest_cluster": 0.6468204259872437, "score_of_first_answer": 0.6468204259872437, "max_score_of_first_answer": 0.9989513158798218, "min_score_of_first_answer": 0.15039478242397308, "avg_score": 0.6468204259872437, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3192959977512904, "std_of_first_answer": 0.3192959977512904, "avg_entropy_of_first_answer": 0.8606239345722965, "lex_sim": 1.0}
{"question": "who started the guinness book of world records", "answer": ["Hugh Beaver"], "scores": [[0.18501144647598267, 0.967231035232544, 0.9896408915519714]], "normalized_score": [0.5615677237510681], "tokens": [["\u2581Hugh", "\u2581Be", "aver"]], "entropy": [[2.468327760696411, 0.2800706624984741, 0.08406175673007965]], "avg_entropy": [0.9441533933083216], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Sir Hugh Beaver", "score_of_biggest_cluster": 0.5615677237510681, "score_of_first_answer": 0.5615677237510681, "max_score_of_first_answer": 0.9896408915519714, "min_score_of_first_answer": 0.18501144647598267, "avg_score": 0.5615677237510681, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3741357790633465, "std_of_first_answer": 0.3741357790633465, "avg_entropy_of_first_answer": 0.9441533933083216, "lex_sim": 1.0}
{"question": "when did the movie napoleon dynamite come out", "answer": ["June 11, 2004"], "scores": [[0.8116574287414551, 0.9970743656158447, 0.9661032557487488, 0.9322215914726257, 0.986164927482605, 0.9993871450424194, 0.9999096393585205, 0.999988317489624, 0.9981093406677246, 0.9996449947357178]], "normalized_score": [0.9672318696975708], "tokens": [["\u2581June", "\u2581", "1", "1", ",", "\u2581", "2", "0", "0", "4"]], "entropy": [[0.9399131536483765, 0.025251097977161407, 0.19994111359119415, 0.3311190903186798, 0.09279289096593857, 0.006002599373459816, 0.0009711082675494254, 0.00017543586727697402, 0.013925906270742416, 0.0039322832599282265]], "avg_entropy": [0.16140246795403074], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "June\u00a011,\u00a02004", "score_of_biggest_cluster": 0.9672318696975708, "score_of_first_answer": 0.9672318696975708, "max_score_of_first_answer": 0.999988317489624, "min_score_of_first_answer": 0.8116574287414551, "avg_score": 0.9672318696975708, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0564145856543321, "std_of_first_answer": 0.0564145856543321, "avg_entropy_of_first_answer": 0.16140246795403074, "lex_sim": 1.0}
{"question": "when did the nba start playing zone defense", "answer": ["1983 NBA Playoffs"], "scores": [[0.5695387125015259, 0.2913215756416321, 0.9814563989639282, 0.21223458647727966, 0.2033943086862564, 0.011387106031179428, 0.3457346558570862, 0.9714077115058899]], "normalized_score": [0.26833972334861755], "tokens": [["\u2581", "1", "9", "8", "3", "\u2581NBA", "\u2581Play", "offs"]], "entropy": [[2.602344036102295, 0.6746025681495667, 0.1451067477464676, 2.069582223892212, 1.8688766956329346, 1.610172986984253, 1.7898061275482178, 0.14341555535793304]], "avg_entropy": [1.362988367676735], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2002", "score_of_biggest_cluster": 0.26833972334861755, "score_of_first_answer": 0.26833972334861755, "max_score_of_first_answer": 0.9814563989639282, "min_score_of_first_answer": 0.011387106031179428, "avg_score": 0.26833972334861755, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3380748212021429, "std_of_first_answer": 0.3380748212021429, "avg_entropy_of_first_answer": 1.362988367676735, "lex_sim": 1.0}
{"question": "who declared war on the united states four days after pearl harbor", "answer": ["Bulgaria, Hungary, Germany, and Italy"], "scores": [[0.0024266152177006006, 0.9994134902954102, 0.07872150093317032, 0.5291401743888855, 0.9991970658302307, 0.7817990779876709, 0.003034171648323536, 0.7161152958869934, 0.14241868257522583, 0.4972877502441406]], "normalized_score": [0.16159778833389282], "tokens": [["\u2581Bulg", "aria", ",", "\u2581Hung", "ary", ",", "\u2581Germany", ",", "\u2581and", "\u2581Italy"]], "entropy": [[2.866912841796875, 0.006261967122554779, 0.7177083492279053, 2.2410755157470703, 0.009153748862445354, 0.6994866728782654, 1.7433805465698242, 0.9419313669204712, 2.2507176399230957, 1.7610633373260498]], "avg_entropy": [1.3237691986374558], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fascist Italy", "score_of_biggest_cluster": 0.16159778833389282, "score_of_first_answer": 0.16159778833389282, "max_score_of_first_answer": 0.9994134902954102, "min_score_of_first_answer": 0.0024266152177006006, "avg_score": 0.16159778833389282, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3767235782160612, "std_of_first_answer": 0.3767235782160612, "avg_entropy_of_first_answer": 1.3237691986374558, "lex_sim": 1.0}
{"question": "who wrote the poem for whom the bell tolls", "answer": ["John Donne"], "scores": [[0.8428861498832703, 0.9939671754837036, 0.9992716908454895]], "normalized_score": [0.9424858689308167], "tokens": [["\u2581John", "\u2581Don", "ne"]], "entropy": [[0.8112840056419373, 0.06420955061912537, 0.008276182226836681]], "avg_entropy": [0.29458991282929975], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ernest Hemingway", "score_of_biggest_cluster": 0.9424858689308167, "score_of_first_answer": 0.9424858689308167, "max_score_of_first_answer": 0.9992716908454895, "min_score_of_first_answer": 0.8428861498832703, "avg_score": 0.9424858689308167, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07250291304626486, "std_of_first_answer": 0.07250291304626486, "avg_entropy_of_first_answer": 0.29458991282929975, "lex_sim": 1.0}
{"question": "how many countries does cadbury sell its products", "answer": ["50 countries"], "scores": [[0.7853618264198303, 0.5930719971656799, 0.828235924243927, 0.21155282855033875]], "normalized_score": [0.5344874262809753], "tokens": [["\u2581", "5", "0", "\u2581countries"]], "entropy": [[1.3983296155929565, 1.3890191316604614, 0.8414380550384521, 1.581418752670288]], "avg_entropy": [1.3025513887405396], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "more than 50 countries worldwide", "score_of_biggest_cluster": 0.5344874262809753, "score_of_first_answer": 0.5344874262809753, "max_score_of_first_answer": 0.828235924243927, "min_score_of_first_answer": 0.21155282855033875, "avg_score": 0.5344874262809753, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24357074458317576, "std_of_first_answer": 0.24357074458317576, "avg_entropy_of_first_answer": 1.3025513887405396, "lex_sim": 1.0}
{"question": "where did they live in sex and the city", "answer": ["Manhattan"], "scores": [[0.18136979639530182, 0.9989976286888123]], "normalized_score": [0.4256618320941925], "tokens": [["\u2581Manh", "attan"]], "entropy": [[2.066328287124634, 0.00956663303077221]], "avg_entropy": [1.037947460077703], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "New York City", "score_of_biggest_cluster": 0.4256618320941925, "score_of_first_answer": 0.4256618320941925, "max_score_of_first_answer": 0.9989976286888123, "min_score_of_first_answer": 0.18136979639530182, "avg_score": 0.4256618320941925, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4088139161467552, "std_of_first_answer": 0.4088139161467552, "avg_entropy_of_first_answer": 1.037947460077703, "lex_sim": 1.0}
{"question": "who died in the first episode of stranger things", "answer": ["Barb"], "scores": [[0.4712921977043152]], "normalized_score": [0.4712921977043152], "tokens": [["\u2581Barb"]], "entropy": [[1.8492529392242432]], "avg_entropy": [1.8492529392242432], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Benny", "score_of_biggest_cluster": 0.4712921977043152, "score_of_first_answer": 0.4712921977043152, "max_score_of_first_answer": 0.4712921977043152, "min_score_of_first_answer": 0.4712921977043152, "avg_score": 0.4712921977043152, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.8492529392242432, "lex_sim": 1.0}
{"question": "1. what was the precursor to the present day internet", "answer": ["The original ARPANET from 1969 used packet switching, and TCP/IP from 1974."], "scores": [[0.07359343022108078, 0.008557155728340149, 0.32025808095932007, 0.8169112801551819, 0.9959617257118225, 0.9868288636207581, 0.0052925581112504005, 0.47452330589294434, 0.9932886958122253, 0.9985244870185852, 0.9124177098274231, 0.9570125937461853, 0.017282677814364433, 0.8473404049873352, 0.9336280822753906, 0.12855754792690277, 0.3772207200527191, 0.23495301604270935, 0.8754753470420837, 0.9983106851577759, 0.23445679247379303, 0.6447641849517822, 0.9991409778594971, 0.9995484948158264, 0.8868947625160217, 0.10235404223203659, 0.019760530441999435]], "normalized_score": [0.3059742748737335], "tokens": [["\u2581The", "\u2581original", "\u2581AR", "P", "AN", "ET", "\u2581from", "\u2581", "1", "9", "6", "9", "\u2581used", "\u2581packet", "\u2581switching", ",", "\u2581and", "\u2581TCP", "/", "IP", "\u2581from", "\u2581", "1", "9", "7", "4", "."]], "entropy": [[3.020778179168701, 3.5543856620788574, 3.4708597660064697, 0.520412802696228, 0.034471526741981506, 0.08130461722612381, 2.916977882385254, 1.4145017862319946, 0.04868908226490021, 0.016333773732185364, 0.3432081341743469, 0.2539236545562744, 2.5342257022857666, 1.0356615781784058, 0.31135743856430054, 2.3378984928131104, 2.6611740589141846, 2.800835609436035, 0.8210639953613281, 0.017659172415733337, 2.9595658779144287, 1.2027716636657715, 0.008616721257567406, 0.00530257448554039, 0.3654899001121521, 0.9939438104629517, 0.9835343360900879]], "avg_entropy": [1.2857388073785438], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "the ARPANET project", "score_of_biggest_cluster": 0.3059742748737335, "score_of_first_answer": 0.3059742748737335, "max_score_of_first_answer": 0.9995484948158264, "min_score_of_first_answer": 0.0052925581112504005, "avg_score": 0.3059742748737335, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3960378398051097, "std_of_first_answer": 0.3960378398051097, "avg_entropy_of_first_answer": 1.2857388073785438, "lex_sim": 1.0}
{"question": "when was the first epistle of john written", "answer": ["Not Available"], "scores": [[0.0020424651447683573, 0.009179847314953804, 0.9992270469665527]], "normalized_score": [0.026559388265013695], "tokens": [["\u2581Not", "\u2581Av", "ailable"]], "entropy": [[3.016800880432129, 3.5042903423309326, 0.007566904649138451]], "avg_entropy": [2.1762193758040667], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "AD 95\u2013110", "score_of_biggest_cluster": 0.026559388265013695, "score_of_first_answer": 0.026559388265013695, "max_score_of_first_answer": 0.9992270469665527, "min_score_of_first_answer": 0.0020424651447683573, "avg_score": 0.026559388265013695, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4684040859942885, "std_of_first_answer": 0.4684040859942885, "avg_entropy_of_first_answer": 2.1762193758040667, "lex_sim": 1.0}
{"question": "where does the last name andersen originate from", "answer": ["Danish"], "scores": [[0.01602952554821968, 0.9475829601287842]], "normalized_score": [0.12324491888284683], "tokens": [["\u2581Dan", "ish"]], "entropy": [[2.2372872829437256, 0.28363367915153503]], "avg_entropy": [1.2604604810476303], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Danish", "score_of_biggest_cluster": 0.12324491888284683, "score_of_first_answer": 0.12324491888284683, "max_score_of_first_answer": 0.9475829601287842, "min_score_of_first_answer": 0.01602952554821968, "avg_score": 0.12324491888284683, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.46577671729028225, "std_of_first_answer": 0.46577671729028225, "avg_entropy_of_first_answer": 1.2604604810476303, "lex_sim": 1.0}
{"question": "where was the original flight of the phoenix filmed", "answer": ["Yucat\u00e1n Peninsula, Mexico"], "scores": [[0.029614873230457306, 0.018009904772043228, 0.6762263178825378, 0.959080159664154, 0.6024082899093628, 0.9987342953681946, 0.9993680119514465, 0.2112940400838852, 0.869269847869873]], "normalized_score": [0.32293835282325745], "tokens": [["\u2581Y", "uc", "at", "\u00e1n", "\u2581Pen", "ins", "ula", ",", "\u2581Mexico"]], "entropy": [[5.792898178100586, 1.3722202777862549, 0.7030230164527893, 0.30546867847442627, 1.2521567344665527, 0.011907465755939484, 0.006442688871175051, 0.9148619771003723, 0.8931806683540344]], "avg_entropy": [1.2502399650402367], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "20th Century-Fox Studios", "score_of_biggest_cluster": 0.32293835282325745, "score_of_first_answer": 0.32293835282325745, "max_score_of_first_answer": 0.9993680119514465, "min_score_of_first_answer": 0.018009904772043228, "avg_score": 0.32293835282325745, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3859684707455884, "std_of_first_answer": 0.3859684707455884, "avg_entropy_of_first_answer": 1.2502399650402367, "lex_sim": 1.0}
{"question": "who is rose in the fall season 2", "answer": ["Eliza Taylor"], "scores": [[0.009375828318297863, 0.5307325720787048, 0.7258131504058838]], "normalized_score": [0.15342754125595093], "tokens": [["\u2581El", "iza", "\u2581Taylor"]], "entropy": [[5.199142932891846, 1.9234552383422852, 1.3201196193695068]], "avg_entropy": [2.814239263534546], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Valene Kane", "score_of_biggest_cluster": 0.15342754125595093, "score_of_first_answer": 0.15342754125595093, "max_score_of_first_answer": 0.7258131504058838, "min_score_of_first_answer": 0.009375828318297863, "avg_score": 0.15342754125595093, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30242569753852167, "std_of_first_answer": 0.30242569753852167, "avg_entropy_of_first_answer": 2.814239263534546, "lex_sim": 1.0}
{"question": "the era of the great mughals began with the accession of", "answer": ["Babur"], "scores": [[0.7392602562904358, 0.9163089990615845]], "normalized_score": [0.8230375647544861], "tokens": [["\u2581Bab", "ur"]], "entropy": [[1.281512975692749, 0.3209018111228943]], "avg_entropy": [0.8012073934078217], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Akbar the Great", "score_of_biggest_cluster": 0.8230375647544861, "score_of_first_answer": 0.8230375647544861, "max_score_of_first_answer": 0.9163089990615845, "min_score_of_first_answer": 0.7392602562904358, "avg_score": 0.8230375647544861, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08852437138557434, "std_of_first_answer": 0.08852437138557434, "avg_entropy_of_first_answer": 0.8012073934078217, "lex_sim": 1.0}
{"question": "what emperor took over france after the reign of terror", "answer": ["Napoleon Bonaparte"], "scores": [[0.8123888373374939, 0.6639543771743774, 0.999030590057373, 0.9951828122138977]], "normalized_score": [0.8557481169700623], "tokens": [["\u2581Napoleon", "\u2581Bon", "ap", "arte"]], "entropy": [[1.1614338159561157, 1.007016897201538, 0.011247656308114529, 0.037926480174064636]], "avg_entropy": [0.5544062124099582], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Napoleon", "score_of_biggest_cluster": 0.8557481169700623, "score_of_first_answer": 0.8557481169700623, "max_score_of_first_answer": 0.999030590057373, "min_score_of_first_answer": 0.6639543771743774, "avg_score": 0.8557481169700623, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1397061022482672, "std_of_first_answer": 0.1397061022482672, "avg_entropy_of_first_answer": 0.5544062124099582, "lex_sim": 1.0}
{"question": "how many terms can a mayor serve in texas", "answer": ["2"], "scores": [[0.5592544674873352, 0.31207898259162903]], "normalized_score": [0.41776973009109497], "tokens": [["\u2581", "2"]], "entropy": [[2.4440364837646484, 1.6544286012649536]], "avg_entropy": [2.049232542514801], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "two four-year terms", "score_of_biggest_cluster": 0.41776973009109497, "score_of_first_answer": 0.41776973009109497, "max_score_of_first_answer": 0.5592544674873352, "min_score_of_first_answer": 0.31207898259162903, "avg_score": 0.41776973009109497, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12358774244785309, "std_of_first_answer": 0.12358774244785309, "avg_entropy_of_first_answer": 2.049232542514801, "lex_sim": 1.0}
{"question": "what stations in seattle are owned by sinclair", "answer": ["Komo-tv"], "scores": [[0.0008321920176967978, 0.9939781427383423, 0.32430824637413025, 0.5063724517822266]], "normalized_score": [0.10795857757329941], "tokens": [["\u2581Kom", "o", "-", "tv"]], "entropy": [[1.4892898797988892, 0.06439852714538574, 2.423832893371582, 1.2321934700012207]], "avg_entropy": [1.3024286925792694], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "KOMO-TV", "score_of_biggest_cluster": 0.10795857757329941, "score_of_first_answer": 0.10795857757329941, "max_score_of_first_answer": 0.9939781427383423, "min_score_of_first_answer": 0.0008321920176967978, "avg_score": 0.10795857757329941, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3593319362248623, "std_of_first_answer": 0.3593319362248623, "avg_entropy_of_first_answer": 1.3024286925792694, "lex_sim": 1.0}
{"question": "who gets the golden boot if its a tie", "answer": ["2 or more players share the Golden Boot"], "scores": [[0.014460026286542416, 0.8581777215003967, 0.2216520607471466, 0.9673847556114197, 0.7781785130500793, 0.046343084424734116, 0.7494656443595886, 0.24827222526073456, 0.9688137769699097]], "normalized_score": [0.29572567343711853], "tokens": [["\u2581", "2", "\u2581or", "\u2581more", "\u2581players", "\u2581share", "\u2581the", "\u2581Golden", "\u2581Boot"]], "entropy": [[4.510769844055176, 0.6523709893226624, 2.344420909881592, 0.1740027368068695, 1.3995814323425293, 2.7246763706207275, 1.2142982482910156, 1.9385792016983032, 0.17667433619499207]], "avg_entropy": [1.681708229912652], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "shared", "score_of_biggest_cluster": 0.29572567343711853, "score_of_first_answer": 0.29572567343711853, "max_score_of_first_answer": 0.9688137769699097, "min_score_of_first_answer": 0.014460026286542416, "avg_score": 0.29572567343711853, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.37634425475597943, "std_of_first_answer": 0.37634425475597943, "avg_entropy_of_first_answer": 1.681708229912652, "lex_sim": 1.0}
{"question": "who is the robot in lost in space 2018", "answer": ["ARIEL-1"], "scores": [[0.009552163071930408, 0.05463692173361778, 0.3266923725605011, 0.013165316544473171, 0.4226250946521759]], "normalized_score": [0.06243421509861946], "tokens": [["\u2581A", "RI", "EL", "-", "1"]], "entropy": [[4.144770622253418, 4.521325588226318, 2.4240546226501465, 1.9316885471343994, 2.814509391784668]], "avg_entropy": [3.16726975440979], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brian Steele", "score_of_biggest_cluster": 0.06243421509861946, "score_of_first_answer": 0.06243421509861946, "max_score_of_first_answer": 0.4226250946521759, "min_score_of_first_answer": 0.009552163071930408, "avg_score": 0.06243421509861946, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17430568772129934, "std_of_first_answer": 0.17430568772129934, "avg_entropy_of_first_answer": 3.16726975440979, "lex_sim": 1.0}
{"question": "who plays young voldemort in the half blood prince", "answer": ["Helena Bonham Carter"], "scores": [[0.005398999899625778, 0.995999813079834, 0.8403300046920776, 0.9973887801170349, 0.9894802570343018]], "normalized_score": [0.3387340307235718], "tokens": [["\u2581Hel", "ena", "\u2581Bon", "ham", "\u2581Carter"]], "entropy": [[1.53572678565979, 0.037642814218997955, 0.5046941041946411, 0.02536689303815365, 0.07484400272369385]], "avg_entropy": [0.4356549199670553], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hero Beauregard Fiennes-Tiffin", "score_of_biggest_cluster": 0.3387340307235718, "score_of_first_answer": 0.3387340307235718, "max_score_of_first_answer": 0.9973887801170349, "min_score_of_first_answer": 0.005398999899625778, "avg_score": 0.3387340307235718, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3848175008010732, "std_of_first_answer": 0.3848175008010732, "avg_entropy_of_first_answer": 0.4356549199670553, "lex_sim": 1.0}
{"question": "how do you say evil eye in greek", "answer": ["\u03bc\u03b1\u03c4\u03b1k\u03b9\u03b1 (mataki/mattakia)"], "scores": [[0.253165602684021, 0.5939576625823975, 0.9024938344955444, 0.9455260634422302, 0.6934335231781006, 0.009368130005896091, 0.8288920521736145, 0.6341637372970581, 0.08243884146213531, 0.29303547739982605, 0.8329601287841797, 0.4697888195514679, 0.0006986751686781645, 0.328134685754776, 0.030660836026072502, 0.16109628975391388, 0.9801003336906433, 0.8171001672744751]], "normalized_score": [0.23188051581382751], "tokens": [["\u2581", "\u03bc", "\u03b1", "\u03c4", "\u03b1", "k", "\u03b9", "\u03b1", "\u2581(", "m", "ata", "ki", "/", "m", "att", "ak", "ia", ")"]], "entropy": [[3.0300798416137695, 1.5081254243850708, 0.3862612843513489, 0.30533459782600403, 1.4065446853637695, 1.6416206359863281, 0.90907883644104, 1.7165210247039795, 1.6233954429626465, 2.2669272422790527, 1.0614393949508667, 1.0133211612701416, 2.296660900115967, 2.2077078819274902, 2.103997230529785, 2.707812786102295, 0.17103883624076843, 1.0058435201644897]], "avg_entropy": [1.5200950404008229], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "matiasma", "score_of_biggest_cluster": 0.23188051581382751, "score_of_first_answer": 0.23188051581382751, "max_score_of_first_answer": 0.9801003336906433, "min_score_of_first_answer": 0.0006986751686781645, "avg_score": 0.23188051581382751, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3421671433382694, "std_of_first_answer": 0.3421671433382694, "avg_entropy_of_first_answer": 1.5200950404008229, "lex_sim": 1.0}
{"question": "who is the song diamonds and rust about", "answer": ["John G. Brennan"], "scores": [[0.014144509099423885, 0.01152261346578598, 0.1218322217464447, 0.01811080612242222, 0.11012289673089981, 0.9964207410812378]], "normalized_score": [0.05834807828068733], "tokens": [["\u2581John", "\u2581G", ".", "\u2581B", "ren", "nan"]], "entropy": [[4.485980033874512, 3.5837552547454834, 2.889561176300049, 3.1898856163024902, 4.31761360168457, 0.03229023143649101]], "avg_entropy": [3.0831809857239327], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bob Dylan", "score_of_biggest_cluster": 0.05834807828068733, "score_of_first_answer": 0.05834807828068733, "max_score_of_first_answer": 0.9964207410812378, "min_score_of_first_answer": 0.01152261346578598, "avg_score": 0.05834807828068733, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35373155188549243, "std_of_first_answer": 0.35373155188549243, "avg_entropy_of_first_answer": 3.0831809857239327, "lex_sim": 1.0}
{"question": "who does dwyane wade play for right now", "answer": ["Miami Heat"], "scores": [[0.6240764856338501, 0.9675208330154419, 0.9969526529312134]], "normalized_score": [0.8443533778190613], "tokens": [["\u2581Miami", "\u2581He", "at"]], "entropy": [[1.816869854927063, 0.2062166929244995, 0.023004528135061264]], "avg_entropy": [0.682030358662208], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "the Miami Heat", "score_of_biggest_cluster": 0.8443533778190613, "score_of_first_answer": 0.8443533778190613, "max_score_of_first_answer": 0.9969526529312134, "min_score_of_first_answer": 0.6240764856338501, "avg_score": 0.8443533778190613, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.16926536953365834, "std_of_first_answer": 0.16926536953365834, "avg_entropy_of_first_answer": 0.682030358662208, "lex_sim": 1.0}
{"question": "how many wins does tiger woods have on the pga tour", "answer": ["82"], "scores": [[0.9793230295181274, 0.3718167543411255, 0.14003247022628784]], "normalized_score": [0.3708183467388153], "tokens": [["\u2581", "8", "2"]], "entropy": [[0.2097352296113968, 0.9122695922851562, 1.043999195098877]], "avg_entropy": [0.7220013389984766], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "79", "score_of_biggest_cluster": 0.3708183467388153, "score_of_first_answer": 0.3708183467388153, "max_score_of_first_answer": 0.9793230295181274, "min_score_of_first_answer": 0.14003247022628784, "avg_score": 0.3708183467388153, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3538983645151484, "std_of_first_answer": 0.3538983645151484, "avg_entropy_of_first_answer": 0.7220013389984766, "lex_sim": 1.0}
{"question": "who are the judges on do you think you can dance", "answer": ["Nigel Lythgoe"], "scores": [[0.2599222660064697, 0.9952343106269836, 0.9997683167457581, 0.9869258403778076, 0.9993805885314941, 0.9980449676513672, 0.9987432360649109]], "normalized_score": [0.8223204016685486], "tokens": [["\u2581N", "ig", "el", "\u2581L", "yth", "go", "e"]], "entropy": [[3.498255729675293, 0.04799759015440941, 0.0028892650734633207, 0.12293136119842529, 0.007586624938994646, 0.020457787439227104, 0.012718340381979942]], "avg_entropy": [0.5304052426945418], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Nigel Lythgoe", "score_of_biggest_cluster": 0.8223204016685486, "score_of_first_answer": 0.8223204016685486, "max_score_of_first_answer": 0.9997683167457581, "min_score_of_first_answer": 0.2599222660064697, "avg_score": 0.8223204016685486, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2577289952453857, "std_of_first_answer": 0.2577289952453857, "avg_entropy_of_first_answer": 0.5304052426945418, "lex_sim": 1.0}
{"question": "when does the day of the dead end", "answer": ["November 2"], "scores": [[0.7831329703330994, 0.960666835308075, 0.7563782334327698]], "normalized_score": [0.8286715745925903], "tokens": [["\u2581November", "\u2581", "2"]], "entropy": [[1.4857866764068604, 0.22366958856582642, 0.9150018095970154]], "avg_entropy": [0.8748193581899008], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "November 2", "score_of_biggest_cluster": 0.8286715745925903, "score_of_first_answer": 0.8286715745925903, "max_score_of_first_answer": 0.960666835308075, "min_score_of_first_answer": 0.7563782334327698, "avg_score": 0.8286715745925903, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.09065681442532213, "std_of_first_answer": 0.09065681442532213, "avg_entropy_of_first_answer": 0.8748193581899008, "lex_sim": 1.0}
{"question": "how many episodes of sabrina the teenage witch are there", "answer": ["162 (not counting holiday episodes)"], "scores": [[0.9458064436912537, 0.5207090377807617, 0.73244708776474, 0.15328368544578552, 0.10361094772815704, 0.009633016772568226, 0.2847806513309479, 0.008155044168233871, 0.9903246164321899, 0.35543155670166016, 0.6638243198394775]], "normalized_score": [0.20702975988388062], "tokens": [["\u2581", "1", "6", "2", "\u2581(", "not", "\u2581counting", "\u2581hol", "iday", "\u2581episodes", ")"]], "entropy": [[0.4212436079978943, 1.2508127689361572, 1.0679168701171875, 1.2425001859664917, 2.5221714973449707, 3.6213502883911133, 1.2983171939849854, 3.2357168197631836, 0.0574125275015831, 1.8584808111190796, 1.3416467905044556]], "avg_entropy": [1.6288699419661001], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "163", "score_of_biggest_cluster": 0.20702975988388062, "score_of_first_answer": 0.20702975988388062, "max_score_of_first_answer": 0.9903246164321899, "min_score_of_first_answer": 0.008155044168233871, "avg_score": 0.20702975988388062, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3436008837706532, "std_of_first_answer": 0.3436008837706532, "avg_entropy_of_first_answer": 1.6288699419661001, "lex_sim": 1.0}
{"question": "when does the new adventure time come out", "answer": ["in September or October of 2014"], "scores": [[0.0014909565215930343, 0.021038059145212173, 0.005806257016956806, 0.7978882789611816, 0.12262987345457077, 0.8778758645057678, 0.9967067837715149, 0.9997842907905579, 0.9477570652961731, 0.03526630252599716]], "normalized_score": [0.11794698238372803], "tokens": [["\u2581in", "\u2581September", "\u2581or", "\u2581October", "\u2581of", "\u2581", "2", "0", "1", "4"]], "entropy": [[3.6260037422180176, 3.1290082931518555, 1.4965577125549316, 1.2954981327056885, 1.722726821899414, 0.4714593291282654, 0.024108748883008957, 0.0026669339276850224, 0.2438642978668213, 1.930138111114502]], "avg_entropy": [1.394203212345019], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "April 21, 2017", "score_of_biggest_cluster": 0.11794698238372803, "score_of_first_answer": 0.11794698238372803, "max_score_of_first_answer": 0.9997842907905579, "min_score_of_first_answer": 0.0014909565215930343, "avg_score": 0.11794698238372803, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44780472164381085, "std_of_first_answer": 0.44780472164381085, "avg_entropy_of_first_answer": 1.394203212345019, "lex_sim": 1.0}
{"question": "the first political society started under british rule in india was", "answer": ["Bharat Gaurav"], "scores": [[0.004298597574234009, 0.9057489037513733, 0.8495899438858032, 0.007623505778610706, 0.2975255846977234, 0.9578863978385925]], "normalized_score": [0.1389172375202179], "tokens": [["\u2581B", "har", "at", "\u2581G", "aur", "av"]], "entropy": [[3.8513193130493164, 0.7261521816253662, 0.6131533980369568, 4.162764549255371, 2.8292603492736816, 0.3395578861236572]], "avg_entropy": [2.0870346128940582], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The British Indian Association", "score_of_biggest_cluster": 0.1389172375202179, "score_of_first_answer": 0.1389172375202179, "max_score_of_first_answer": 0.9578863978385925, "min_score_of_first_answer": 0.004298597574234009, "avg_score": 0.1389172375202179, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41343479118678367, "std_of_first_answer": 0.41343479118678367, "avg_entropy_of_first_answer": 2.0870346128940582, "lex_sim": 1.0}
{"question": "when did marathon change its name to snickers", "answer": ["1970\u20131973"], "scores": [[0.5798512101173401, 0.8463636636734009, 0.9925422668457031, 0.014221200719475746, 0.13386818766593933, 0.0031660650856792927, 0.7245913147926331, 0.993545413017273, 0.41592147946357727, 0.10823068022727966]], "normalized_score": [0.19853629171848297], "tokens": [["\u2581", "1", "9", "7", "0", "\u2013", "1", "9", "7", "3"]], "entropy": [[2.1666250228881836, 0.5027807950973511, 0.06442691385746002, 1.1144212484359741, 2.1130590438842773, 1.536417841911316, 1.0017735958099365, 0.05040637403726578, 1.0224316120147705, 2.0561790466308594]], "avg_entropy": [1.1628521494567394], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "19 July 1990", "score_of_biggest_cluster": 0.19853629171848297, "score_of_first_answer": 0.19853629171848297, "max_score_of_first_answer": 0.993545413017273, "min_score_of_first_answer": 0.0031660650856792927, "avg_score": 0.19853629171848297, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3790616111078345, "std_of_first_answer": 0.3790616111078345, "avg_entropy_of_first_answer": 1.1628521494567394, "lex_sim": 1.0}
{"question": "when did this season of american idol start", "answer": ["March 3, 2010"], "scores": [[0.69028240442276, 0.9897164702415466, 0.1544620394706726, 0.8087999820709229, 0.9977152347564697, 0.9980483055114746, 0.9999195337295532, 0.7412851452827454, 0.15493157505989075]], "normalized_score": [0.5978682041168213], "tokens": [["\u2581March", "\u2581", "3", ",", "\u2581", "2", "0", "1", "0"]], "entropy": [[1.4047727584838867, 0.07306697964668274, 0.868316650390625, 0.6750826239585876, 0.020265309140086174, 0.015723321586847305, 0.0010829585371538997, 0.6583350300788879, 1.5976383686065674]], "avg_entropy": [0.5904760000477027], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "March 11, 2018", "score_of_biggest_cluster": 0.5978682041168213, "score_of_first_answer": 0.5978682041168213, "max_score_of_first_answer": 0.9999195337295532, "min_score_of_first_answer": 0.1544620394706726, "avg_score": 0.5978682041168213, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32550091191154257, "std_of_first_answer": 0.32550091191154257, "avg_entropy_of_first_answer": 0.5904760000477027, "lex_sim": 1.0}
{"question": "the length and direction of a straight line between two locations or positions", "answer": ["distance"], "scores": [[0.7585577964782715]], "normalized_score": [0.7585577964782715], "tokens": [["\u2581distance"]], "entropy": [[1.6012258529663086]], "avg_entropy": [1.6012258529663086], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "displacement", "score_of_biggest_cluster": 0.7585577964782715, "score_of_first_answer": 0.7585577964782715, "max_score_of_first_answer": 0.7585577964782715, "min_score_of_first_answer": 0.7585577964782715, "avg_score": 0.7585577964782715, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.6012258529663086, "lex_sim": 1.0}
{"question": "when did the last state became a state", "answer": ["Hawaii"], "scores": [[0.33447152376174927, 0.9809767603874207]], "normalized_score": [0.572807788848877], "tokens": [["\u2581Hawai", "i"]], "entropy": [[2.7432925701141357, 0.13336196541786194]], "avg_entropy": [1.4383272677659988], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Hawaii", "score_of_biggest_cluster": 0.572807788848877, "score_of_first_answer": 0.572807788848877, "max_score_of_first_answer": 0.9809767603874207, "min_score_of_first_answer": 0.33447152376174927, "avg_score": 0.572807788848877, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3232526183128357, "std_of_first_answer": 0.3232526183128357, "avg_entropy_of_first_answer": 1.4383272677659988, "lex_sim": 1.0}
{"question": "who has made the most premier league appearances", "answer": ["Gareth Barry"], "scores": [[0.3211183249950409, 0.9794268608093262, 0.9994080066680908, 0.9588262438774109]], "normalized_score": [0.7409347891807556], "tokens": [["\u2581G", "are", "th", "\u2581Barry"]], "entropy": [[2.0890250205993652, 0.16252535581588745, 0.007184608373790979, 0.20992296934127808]], "avg_entropy": [0.6171644885325804], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Gareth Barry", "score_of_biggest_cluster": 0.7409347891807556, "score_of_first_answer": 0.7409347891807556, "max_score_of_first_answer": 0.9994080066680908, "min_score_of_first_answer": 0.3211183249950409, "avg_score": 0.7409347891807556, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.28532754439792113, "std_of_first_answer": 0.28532754439792113, "avg_entropy_of_first_answer": 0.6171644885325804, "lex_sim": 1.0}
{"question": "when did the granite mountain hotshots get certified", "answer": ["2008"], "scores": [[0.391032874584198, 0.66551274061203, 0.9815322160720825, 0.8226012587547302, 0.6144798398017883]], "normalized_score": [0.6640400290489197], "tokens": [["\u2581", "2", "0", "0", "8"]], "entropy": [[2.715916633605957, 0.7546585202217102, 0.13962340354919434, 0.48972654342651367, 1.3891061544418335]], "avg_entropy": [1.0978062510490418], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "2008", "score_of_biggest_cluster": 0.6640400290489197, "score_of_first_answer": 0.6640400290489197, "max_score_of_first_answer": 0.9815322160720825, "min_score_of_first_answer": 0.391032874584198, "avg_score": 0.6640400290489197, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.19906370589805533, "std_of_first_answer": 0.19906370589805533, "avg_entropy_of_first_answer": 1.0978062510490418, "lex_sim": 1.0}
{"question": "who sings ive loved you for a thousand years", "answer": ["Christina Perri"], "scores": [[0.6032909750938416, 0.978347659111023, 0.9764209389686584, 0.9969034790992737]], "normalized_score": [0.8706178069114685], "tokens": [["\u2581Christ", "ina", "\u2581Per", "ri"]], "entropy": [[1.787362813949585, 0.17216846346855164, 0.15908917784690857, 0.02921259216964245]], "avg_entropy": [0.5369582618586719], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Christina Perri", "score_of_biggest_cluster": 0.8706178069114685, "score_of_first_answer": 0.8706178069114685, "max_score_of_first_answer": 0.9969034790992737, "min_score_of_first_answer": 0.6032909750938416, "avg_score": 0.8706178069114685, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.16499845730959842, "std_of_first_answer": 0.16499845730959842, "avg_entropy_of_first_answer": 0.5369582618586719, "lex_sim": 1.0}
{"question": "colby chees is named after a town in what state", "answer": ["Wisconsin"], "scores": [[0.39588192105293274]], "normalized_score": [0.39588192105293274], "tokens": [["\u2581Wisconsin"]], "entropy": [[2.3193836212158203]], "avg_entropy": [2.3193836212158203], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Wisconsin", "score_of_biggest_cluster": 0.39588192105293274, "score_of_first_answer": 0.39588192105293274, "max_score_of_first_answer": 0.39588192105293274, "min_score_of_first_answer": 0.39588192105293274, "avg_score": 0.39588192105293274, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.3193836212158203, "lex_sim": 1.0}
{"question": "who were the members of the traveling wilburys group", "answer": ["George Harrison, Roy Orbison, Bob Dylan, Tom Petty and Jeff Lynne"], "scores": [[0.26523762941360474, 0.9675021171569824, 0.9081153273582458, 0.19378313422203064, 0.9868373870849609, 0.9971928000450134, 0.9996644258499146, 0.9943314790725708, 0.44763922691345215, 0.9990058541297913, 0.9996013045310974, 0.9982457160949707, 0.9767314195632935, 0.5816125869750977, 0.9972789883613586, 0.9999562501907349, 0.23010224103927612, 0.9917940497398376, 0.9798791408538818, 0.9832919239997864]], "normalized_score": [0.740467369556427], "tokens": [["\u2581George", "\u2581Harrison", ",", "\u2581Roy", "\u2581Or", "b", "ison", ",", "\u2581Bob", "\u2581D", "yl", "an", ",", "\u2581Tom", "\u2581Pet", "ty", "\u2581and", "\u2581Jeff", "\u2581Lyn", "ne"]], "entropy": [[2.291066884994507, 0.2966350317001343, 0.5495975017547607, 1.4176301956176758, 0.11782969534397125, 0.023306570947170258, 0.0038442902732640505, 0.05103232339024544, 1.1734354496002197, 0.011846736073493958, 0.004121950827538967, 0.015172642655670643, 0.1496170461177826, 0.8135488033294678, 0.0268733911216259, 0.0005950733902864158, 0.945056140422821, 0.07951728999614716, 0.1413695067167282, 0.09111301600933075]], "avg_entropy": [0.4101604770141421], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Roy Orbison", "score_of_biggest_cluster": 0.740467369556427, "score_of_first_answer": 0.740467369556427, "max_score_of_first_answer": 0.9999562501907349, "min_score_of_first_answer": 0.19378313422203064, "avg_score": 0.740467369556427, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2881407541923031, "std_of_first_answer": 0.2881407541923031, "avg_entropy_of_first_answer": 0.4101604770141421, "lex_sim": 1.0}
{"question": "when did the government change the retirement age", "answer": ["July 1, 2001"], "scores": [[0.037773240357637405, 0.9896296858787537, 0.7901092171669006, 0.8308678269386292, 0.9987986087799072, 0.9058995246887207, 0.9998888969421387, 0.4282276928424835, 0.039025936275720596]], "normalized_score": [0.41574186086654663], "tokens": [["\u2581July", "\u2581", "1", ",", "\u2581", "2", "0", "0", "1"]], "entropy": [[3.245603084564209, 0.06923440843820572, 0.7730145454406738, 0.8832439184188843, 0.012089163064956665, 0.31291651725769043, 0.0013893225695937872, 0.8759342432022095, 2.096280574798584]], "avg_entropy": [0.9188561975283341], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2015", "score_of_biggest_cluster": 0.41574186086654663, "score_of_first_answer": 0.41574186086654663, "max_score_of_first_answer": 0.9998888969421387, "min_score_of_first_answer": 0.037773240357637405, "avg_score": 0.41574186086654663, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37584479362789835, "std_of_first_answer": 0.37584479362789835, "avg_entropy_of_first_answer": 0.9188561975283341, "lex_sim": 1.0}
{"question": "what is the salary of us secretary of state", "answer": ["not disclosed"], "scores": [[0.0011074536014348269, 0.05684986710548401, 0.9923599362373352]], "normalized_score": [0.03968028351664543], "tokens": [["\u2581not", "\u2581dis", "closed"]], "entropy": [[1.6768052577972412, 3.074420928955078, 0.059301119297742844]], "avg_entropy": [1.6035091020166874], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "$205,700", "score_of_biggest_cluster": 0.03968028351664543, "score_of_first_answer": 0.03968028351664543, "max_score_of_first_answer": 0.9923599362373352, "min_score_of_first_answer": 0.0011074536014348269, "avg_score": 0.03968028351664543, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45471209333699075, "std_of_first_answer": 0.45471209333699075, "avg_entropy_of_first_answer": 1.6035091020166874, "lex_sim": 1.0}
{"question": "what caused the breakup of the democratic republican party", "answer": ["the tariff of abominations"], "scores": [[0.10503340512514114, 0.054906561970710754, 0.9088019132614136, 0.8217035531997681, 0.5785361528396606, 0.9029880166053772, 0.9996376037597656]], "normalized_score": [0.41851693391799927], "tokens": [["\u2581the", "\u2581tar", "iff", "\u2581of", "\u2581ab", "omin", "ations"]], "entropy": [[4.796606063842773, 4.861909866333008, 0.3396006226539612, 0.9115486145019531, 0.7250687479972839, 0.35510942339897156, 0.004461118020117283]], "avg_entropy": [1.713472065249724], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the disputed 1824 presidential election", "score_of_biggest_cluster": 0.41851693391799927, "score_of_first_answer": 0.41851693391799927, "max_score_of_first_answer": 0.9996376037597656, "min_score_of_first_answer": 0.054906561970710754, "avg_score": 0.41851693391799927, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.365356092026653, "std_of_first_answer": 0.365356092026653, "avg_entropy_of_first_answer": 1.713472065249724, "lex_sim": 1.0}
{"question": "who performed the first c section in 1794", "answer": ["James Marion Sims"], "scores": [[0.16322696208953857, 0.6574525237083435, 0.998364269733429, 0.9090036153793335, 0.9919392466545105]], "normalized_score": [0.6266127228736877], "tokens": [["\u2581James", "\u2581Mar", "ion", "\u2581Sim", "s"]], "entropy": [[3.366398811340332, 2.0371127128601074, 0.01752290688455105, 0.4981989860534668, 0.05893019214272499]], "avg_entropy": [1.1956327218562364], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dr. Jesse Bennett", "score_of_biggest_cluster": 0.6266127228736877, "score_of_first_answer": 0.6266127228736877, "max_score_of_first_answer": 0.998364269733429, "min_score_of_first_answer": 0.16322696208953857, "avg_score": 0.6266127228736877, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31565456816503096, "std_of_first_answer": 0.31565456816503096, "avg_entropy_of_first_answer": 1.1956327218562364, "lex_sim": 1.0}
{"question": "who played scotty baldwins father on general hospital", "answer": ["John Ingle"], "scores": [[0.1893969029188156, 0.5703690648078918, 0.9952284097671509]], "normalized_score": [0.4755000174045563], "tokens": [["\u2581John", "\u2581In", "gle"]], "entropy": [[4.705005645751953, 2.6677913665771484, 0.0388619527220726]], "avg_entropy": [2.4705529883503914], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ross Elliott", "score_of_biggest_cluster": 0.4755000174045563, "score_of_first_answer": 0.4755000174045563, "max_score_of_first_answer": 0.9952284097671509, "min_score_of_first_answer": 0.1893969029188156, "avg_score": 0.4755000174045563, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32914192625389255, "std_of_first_answer": 0.32914192625389255, "avg_entropy_of_first_answer": 2.4705529883503914, "lex_sim": 1.0}
{"question": "who sings the theme song for the proud family", "answer": ["Wanda Sykes"], "scores": [[0.015394887886941433, 0.9449144005775452, 0.7002511620521545, 0.9994818568229675]], "normalized_score": [0.3176504075527191], "tokens": [["\u2581W", "anda", "\u2581Sy", "kes"]], "entropy": [[2.6284961700439453, 0.3926629424095154, 1.192699670791626, 0.004926085472106934]], "avg_entropy": [1.0546962171792984], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Destiny's Child", "score_of_biggest_cluster": 0.3176504075527191, "score_of_first_answer": 0.3176504075527191, "max_score_of_first_answer": 0.9994818568229675, "min_score_of_first_answer": 0.015394887886941433, "avg_score": 0.3176504075527191, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39161831860128665, "std_of_first_answer": 0.39161831860128665, "avg_entropy_of_first_answer": 1.0546962171792984, "lex_sim": 1.0}
{"question": "the golden age of india took place during the rule of the", "answer": ["Maurya Dynasty"], "scores": [[0.09819459170103073, 0.9997548460960388, 0.6027809977531433, 0.12785522639751434, 0.9939140677452087, 0.9995244741439819]], "normalized_score": [0.4425886571407318], "tokens": [["\u2581Ma", "ury", "a", "\u2581D", "ynast", "y"]], "entropy": [[1.340491533279419, 0.002518727909773588, 0.8769364953041077, 1.3628517389297485, 0.04168480262160301, 0.005333429202437401]], "avg_entropy": [0.6049694545411816], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chandragupta II", "score_of_biggest_cluster": 0.4425886571407318, "score_of_first_answer": 0.4425886571407318, "max_score_of_first_answer": 0.9997548460960388, "min_score_of_first_answer": 0.09819459170103073, "avg_score": 0.4425886571407318, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39604573241610225, "std_of_first_answer": 0.39604573241610225, "avg_entropy_of_first_answer": 0.6049694545411816, "lex_sim": 1.0}
{"question": "how many gold medals did australia win in the 2000 olympics", "answer": ["51"], "scores": [[0.9431248307228088, 0.7979559898376465, 0.00964419823139906]], "normalized_score": [0.19361458718776703], "tokens": [["\u2581", "5", "1"]], "entropy": [[0.5100668668746948, 0.7627303004264832, 0.21097177267074585]], "avg_entropy": [0.4945896466573079], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "16", "score_of_biggest_cluster": 0.19361458718776703, "score_of_first_answer": 0.19361458718776703, "max_score_of_first_answer": 0.9431248307228088, "min_score_of_first_answer": 0.00964419823139906, "avg_score": 0.19361458718776703, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41013487813779775, "std_of_first_answer": 0.41013487813779775, "avg_entropy_of_first_answer": 0.4945896466573079, "lex_sim": 1.0}
{"question": "when was the drinking age set to 21", "answer": ["1984"], "scores": [[0.3174057602882385, 0.9525027871131897, 0.9797523021697998, 0.9401800632476807, 0.8866325616836548]], "normalized_score": [0.7559801936149597], "tokens": [["\u2581", "1", "9", "8", "4"]], "entropy": [[2.467963218688965, 0.26493749022483826, 0.1551038771867752, 0.32023853063583374, 0.5745558738708496]], "avg_entropy": [0.7565597981214524], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "By mid-1988", "score_of_biggest_cluster": 0.7559801936149597, "score_of_first_answer": 0.7559801936149597, "max_score_of_first_answer": 0.9797523021697998, "min_score_of_first_answer": 0.3174057602882385, "avg_score": 0.7559801936149597, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25077926661951305, "std_of_first_answer": 0.25077926661951305, "avg_entropy_of_first_answer": 0.7565597981214524, "lex_sim": 1.0}
{"question": "who was the winner of the first indianapolis 500", "answer": ["Speedway, Indiana"], "scores": [[0.001106155221350491, 0.9679516553878784, 0.24235251545906067, 0.25982898473739624]], "normalized_score": [0.09061522036790848], "tokens": [["\u2581Speed", "way", ",", "\u2581Indiana"]], "entropy": [[0.5864008069038391, 0.31060105562210083, 3.503106117248535, 3.6621885299682617]], "avg_entropy": [2.015574127435684], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ray Harroun", "score_of_biggest_cluster": 0.09061522036790848, "score_of_first_answer": 0.09061522036790848, "max_score_of_first_answer": 0.9679516553878784, "min_score_of_first_answer": 0.001106155221350491, "avg_score": 0.09061522036790848, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3612620919746185, "std_of_first_answer": 0.3612620919746185, "avg_entropy_of_first_answer": 2.015574127435684, "lex_sim": 1.0}
{"question": "how many kilometers of great wall of china", "answer": ["8,851 km"], "scores": [[0.9011279344558716, 0.2489769160747528, 0.6723142862319946, 0.9676698446273804, 0.936320424079895, 0.7430463433265686, 0.26779165863990784]], "normalized_score": [0.597523033618927], "tokens": [["\u2581", "8", ",", "8", "5", "1", "\u2581km"]], "entropy": [[0.7551629543304443, 1.4160194396972656, 0.8401976823806763, 0.20840317010879517, 0.3033829927444458, 0.7079524993896484, 1.4854207038879395]], "avg_entropy": [0.8166484917913165], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "8,850\u00a0km", "score_of_biggest_cluster": 0.597523033618927, "score_of_first_answer": 0.597523033618927, "max_score_of_first_answer": 0.9676698446273804, "min_score_of_first_answer": 0.2489769160747528, "avg_score": 0.597523033618927, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2820861660452552, "std_of_first_answer": 0.2820861660452552, "avg_entropy_of_first_answer": 0.8166484917913165, "lex_sim": 1.0}
{"question": "who wrote the song always be humble and kind", "answer": ["Tim McGraw"], "scores": [[0.5296223163604736, 0.8743187189102173, 0.9973649382591248]], "normalized_score": [0.7729713320732117], "tokens": [["\u2581Tim", "\u2581McG", "raw"]], "entropy": [[1.9712696075439453, 0.6591478586196899, 0.027730541303753853]], "avg_entropy": [0.886049335822463], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lori McKenna", "score_of_biggest_cluster": 0.7729713320732117, "score_of_first_answer": 0.7729713320732117, "max_score_of_first_answer": 0.9973649382591248, "min_score_of_first_answer": 0.5296223163604736, "avg_score": 0.7729713320732117, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19797282147008036, "std_of_first_answer": 0.19797282147008036, "avg_entropy_of_first_answer": 0.886049335822463, "lex_sim": 1.0}
{"question": "who were the twins that played for kentucky", "answer": ["Aaron & Andrew Harrison"], "scores": [[0.14979377388954163, 0.9902335405349731, 0.02462225966155529, 0.8398820757865906, 0.9996832609176636]], "normalized_score": [0.3142881393432617], "tokens": [["\u2581A", "aron", "\u2581&", "\u2581Andrew", "\u2581Harrison"]], "entropy": [[4.597324848175049, 0.08530120551586151, 1.085864782333374, 0.8466135263442993, 0.0038212002255022526]], "avg_entropy": [1.3237851125188171], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Aaron Harrison", "score_of_biggest_cluster": 0.3142881393432617, "score_of_first_answer": 0.3142881393432617, "max_score_of_first_answer": 0.9996832609176636, "min_score_of_first_answer": 0.02462225966155529, "avg_score": 0.3142881393432617, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42504446179450994, "std_of_first_answer": 0.42504446179450994, "avg_entropy_of_first_answer": 1.3237851125188171, "lex_sim": 1.0}
{"question": "who was the top scorer in 2014 world cup", "answer": ["James Rodr\u00edguez"], "scores": [[0.9651062488555908, 0.4634512662887573]], "normalized_score": [0.6687897443771362], "tokens": [["\u2581James", "\u2581Rodr\u00edguez"]], "entropy": [[0.25286346673965454, 1.300361156463623]], "avg_entropy": [0.7766123116016388], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "James Rodr\u00edguez", "score_of_biggest_cluster": 0.6687897443771362, "score_of_first_answer": 0.6687897443771362, "max_score_of_first_answer": 0.9651062488555908, "min_score_of_first_answer": 0.4634512662887573, "avg_score": 0.6687897443771362, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.25082749128341675, "std_of_first_answer": 0.25082749128341675, "avg_entropy_of_first_answer": 0.7766123116016388, "lex_sim": 1.0}
{"question": "which is produced in plants of narora kakrapar tarapur", "answer": ["nuclear electricity"], "scores": [[0.07016869634389877, 0.04351048544049263, 0.9605406522750854]], "normalized_score": [0.14313672482967377], "tokens": [["\u2581nuclear", "\u2581electric", "ity"]], "entropy": [[4.734671592712402, 1.373558759689331, 0.21272538602352142]], "avg_entropy": [2.1069852461417518], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Atomic Power", "score_of_biggest_cluster": 0.14313672482967377, "score_of_first_answer": 0.14313672482967377, "max_score_of_first_answer": 0.9605406522750854, "min_score_of_first_answer": 0.04351048544049263, "avg_score": 0.14313672482967377, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4261477582823928, "std_of_first_answer": 0.4261477582823928, "avg_entropy_of_first_answer": 2.1069852461417518, "lex_sim": 1.0}
{"question": "who plays dorian tyrell when he puts on the mask", "answer": ["Billy Drago"], "scores": [[0.005377189721912146, 0.17492462694644928, 0.9971882700920105]], "normalized_score": [0.09788763523101807], "tokens": [["\u2581Billy", "\u2581Drag", "o"]], "entropy": [[5.432928085327148, 2.5790019035339355, 0.03001217730343342]], "avg_entropy": [2.6806473887215057], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Peter Greene", "score_of_biggest_cluster": 0.09788763523101807, "score_of_first_answer": 0.09788763523101807, "max_score_of_first_answer": 0.9971882700920105, "min_score_of_first_answer": 0.005377189721912146, "avg_score": 0.09788763523101807, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43314778738559395, "std_of_first_answer": 0.43314778738559395, "avg_entropy_of_first_answer": 2.6806473887215057, "lex_sim": 1.0}
{"question": "who is the ceo of crate and barrel", "answer": ["Lawrence Known"], "scores": [[0.0019422188634052873, 0.06860080361366272, 0.005858366843312979]], "normalized_score": [0.009207350201904774], "tokens": [["\u2581Lawrence", "\u2581Kn", "own"]], "entropy": [[3.788541078567505, 4.549637794494629, 3.130711078643799]], "avg_entropy": [3.822963317235311], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Neela Montgomery", "score_of_biggest_cluster": 0.009207350201904774, "score_of_first_answer": 0.009207350201904774, "max_score_of_first_answer": 0.06860080361366272, "min_score_of_first_answer": 0.0019422188634052873, "avg_score": 0.009207350201904774, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.03054198659730409, "std_of_first_answer": 0.03054198659730409, "avg_entropy_of_first_answer": 3.822963317235311, "lex_sim": 1.0}
{"question": "authorities involved in formulating accounting standard in india", "answer": ["The Institute of Chartered Accountants of India (ICAI) sets the Accounting Standards."], "scores": [[0.13066691160202026, 0.21562911570072174, 0.9871410131454468, 0.943020224571228, 0.9997164607048035, 0.9918714761734009, 0.9861310720443726, 0.973063051700592, 0.997769832611084, 0.3549073040485382, 0.9847548007965088, 0.998386025428772, 0.848893940448761, 0.0030126329511404037, 0.39288556575775146, 0.17282019555568695, 0.9826934337615967, 0.8242786526679993, 0.9995453953742981, 0.11668557673692703]], "normalized_score": [0.4546699523925781], "tokens": [["\u2581The", "\u2581Institute", "\u2581of", "\u2581Char", "tered", "\u2581Account", "ants", "\u2581of", "\u2581India", "\u2581(", "IC", "AI", ")", "\u2581sets", "\u2581the", "\u2581Account", "ing", "\u2581Stand", "ards", "."]], "entropy": [[4.077726364135742, 3.3902034759521484, 0.09411104023456573, 0.3272075057029724, 0.002901339204981923, 0.06527599692344666, 0.08425433933734894, 0.17967988550662994, 0.02291026897728443, 1.3473466634750366, 0.1425844430923462, 0.016226310282945633, 0.5804241895675659, 0.9645503759384155, 1.6024754047393799, 1.5387308597564697, 0.1012210100889206, 0.805340051651001, 0.005410232115536928, 1.757298469543457]], "avg_entropy": [0.8552939113113098], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "government department", "score_of_biggest_cluster": 0.4546699523925781, "score_of_first_answer": 0.4546699523925781, "max_score_of_first_answer": 0.9997164607048035, "min_score_of_first_answer": 0.0030126329511404037, "avg_score": 0.4546699523925781, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37514486004282976, "std_of_first_answer": 0.37514486004282976, "avg_entropy_of_first_answer": 0.8552939113113098, "lex_sim": 1.0}
{"question": "when did the book thief movie come out", "answer": ["12 February 2014"], "scores": [[0.11367936432361603, 0.1815323829650879, 0.06111645698547363, 0.3871646523475647, 0.9883253574371338, 0.9968773126602173, 0.9939788579940796, 0.9741186499595642, 0.9114370942115784]], "normalized_score": [0.4220145642757416], "tokens": [["\u2581", "1", "2", "\u2581February", "\u2581", "2", "0", "1", "4"]], "entropy": [[1.6146546602249146, 0.9227155447006226, 2.384303092956543, 2.1962106227874756, 0.07530450820922852, 0.02361471951007843, 0.04924628511071205, 0.13973505795001984, 0.41637754440307617]], "avg_entropy": [0.8691291150947412], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "November 27, 2013", "score_of_biggest_cluster": 0.4220145642757416, "score_of_first_answer": 0.4220145642757416, "max_score_of_first_answer": 0.9968773126602173, "min_score_of_first_answer": 0.06111645698547363, "avg_score": 0.4220145642757416, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40041434987158886, "std_of_first_answer": 0.40041434987158886, "avg_entropy_of_first_answer": 0.8691291150947412, "lex_sim": 1.0}
{"question": "who is the longest railway station in india", "answer": ["Kharagpur"], "scores": [[0.5901418328285217, 0.7553409934043884, 0.9884344339370728, 0.9966170191764832]], "normalized_score": [0.8140365481376648], "tokens": [["\u2581K", "har", "ag", "pur"]], "entropy": [[1.9891570806503296, 1.369564175605774, 0.10339560359716415, 0.036434806883335114]], "avg_entropy": [0.8746379166841507], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gorakhpur Junction", "score_of_biggest_cluster": 0.8140365481376648, "score_of_first_answer": 0.8140365481376648, "max_score_of_first_answer": 0.9966170191764832, "min_score_of_first_answer": 0.5901418328285217, "avg_score": 0.8140365481376648, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17025045212905035, "std_of_first_answer": 0.17025045212905035, "avg_entropy_of_first_answer": 0.8746379166841507, "lex_sim": 1.0}
{"question": "who developed the first printing press in 1430s", "answer": ["Johannes Gutenberg"], "scores": [[0.7144403457641602, 0.9827644228935242, 0.9826646447181702, 0.9957849383354187]], "normalized_score": [0.9104300141334534], "tokens": [["\u2581Johannes", "\u2581G", "uten", "berg"]], "entropy": [[1.4742616415023804, 0.13471844792366028, 0.09496309608221054, 0.03261658176779747]], "avg_entropy": [0.43413994181901217], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Johannes Gutenberg", "score_of_biggest_cluster": 0.9104300141334534, "score_of_first_answer": 0.9104300141334534, "max_score_of_first_answer": 0.9957849383354187, "min_score_of_first_answer": 0.7144403457641602, "avg_score": 0.9104300141334534, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.11817321782251526, "std_of_first_answer": 0.11817321782251526, "avg_entropy_of_first_answer": 0.43413994181901217, "lex_sim": 1.0}
{"question": "cls bank deals with transactions arising out of", "answer": ["Credit and loan syndication"], "scores": [[0.03413783386349678, 0.9695405960083008, 0.05651290342211723, 0.10682128369808197, 0.1569208800792694, 0.9567248821258545, 0.8924769759178162]], "normalized_score": [0.22223395109176636], "tokens": [["\u2581Cred", "it", "\u2581and", "\u2581loan", "\u2581synd", "ic", "ation"]], "entropy": [[5.556897163391113, 0.14887768030166626, 3.310417652130127, 3.1904969215393066, 3.3043465614318848, 0.18847543001174927, 0.390303373336792]], "avg_entropy": [2.2985449688775197], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "foreign exchange market", "score_of_biggest_cluster": 0.22223395109176636, "score_of_first_answer": 0.22223395109176636, "max_score_of_first_answer": 0.9695405960083008, "min_score_of_first_answer": 0.03413783386349678, "avg_score": 0.22223395109176636, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42322611079562733, "std_of_first_answer": 0.42322611079562733, "avg_entropy_of_first_answer": 2.2985449688775197, "lex_sim": 1.0}
{"question": "whats the movie called with justin timberlake and time", "answer": ["In Time"], "scores": [[0.6066992282867432, 0.9035549759864807]], "normalized_score": [0.7403959035873413], "tokens": [["\u2581In", "\u2581Time"]], "entropy": [[2.4764244556427, 0.4184814691543579]], "avg_entropy": [1.447452962398529], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "In Time", "score_of_biggest_cluster": 0.7403959035873413, "score_of_first_answer": 0.7403959035873413, "max_score_of_first_answer": 0.9035549759864807, "min_score_of_first_answer": 0.6066992282867432, "avg_score": 0.7403959035873413, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.14842787384986877, "std_of_first_answer": 0.14842787384986877, "avg_entropy_of_first_answer": 1.447452962398529, "lex_sim": 1.0}
{"question": "the atomic number of indium which belongs to 5th period is", "answer": ["49"], "scores": [[0.97657710313797, 0.9911218881607056, 0.9903268218040466]], "normalized_score": [0.9859859347343445], "tokens": [["\u2581", "4", "9"]], "entropy": [[0.21514782309532166, 0.06747592985630035, 0.07404237985610962]], "avg_entropy": [0.11888871093591054], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "49", "score_of_biggest_cluster": 0.9859859347343445, "score_of_first_answer": 0.9859859347343445, "max_score_of_first_answer": 0.9911218881607056, "min_score_of_first_answer": 0.97657710313797, "avg_score": 0.9859859347343445, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.006676972577992227, "std_of_first_answer": 0.006676972577992227, "avg_entropy_of_first_answer": 0.11888871093591054, "lex_sim": 1.0}
{"question": "derek and meredith get back together season 3", "answer": ["February 12, 2009 (3.13, I stood watching and wishing)"], "scores": [[0.006830282974988222, 0.9778429865837097, 0.39382678270339966, 0.1813487708568573, 0.9250072836875916, 0.9978111386299133, 0.9983408451080322, 0.999909520149231, 0.9636743664741516, 0.5512897372245789, 0.008464274927973747, 0.03004078008234501, 0.4803725779056549, 0.9042274951934814, 0.2152089625597, 0.04857534542679787, 0.4509258568286896, 0.0015373077476397157, 0.004087791778147221, 0.6298902034759521, 0.007357462774962187, 0.9933577179908752, 0.3386140763759613]], "normalized_score": [0.1662224680185318], "tokens": [["\u2581February", "\u2581", "1", "2", ",", "\u2581", "2", "0", "0", "9", "\u2581(", "3", ".", "1", "3", ",", "\u2581I", "\u2581stood", "\u2581watching", "\u2581and", "\u2581wish", "ing", ")"]], "entropy": [[3.779684543609619, 0.15375104546546936, 1.577469825744629, 2.2910375595092773, 0.38173574209213257, 0.020466230809688568, 0.014582729898393154, 0.0012250544968992472, 0.1642223596572876, 1.0195860862731934, 0.5305847525596619, 4.6022233963012695, 1.7758164405822754, 0.455593466758728, 2.0037648677825928, 2.4805550575256348, 2.5284790992736816, 3.4262027740478516, 3.0043559074401855, 1.7763407230377197, 2.7770941257476807, 0.053854215890169144, 1.8100135326385498]], "avg_entropy": [1.5925495450931562], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Staring at the Sun", "score_of_biggest_cluster": 0.1662224680185318, "score_of_first_answer": 0.1662224680185318, "max_score_of_first_answer": 0.999909520149231, "min_score_of_first_answer": 0.0015373077476397157, "avg_score": 0.1662224680185318, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3988562878872395, "std_of_first_answer": 0.3988562878872395, "avg_entropy_of_first_answer": 1.5925495450931562, "lex_sim": 1.0}
{"question": "when did american idol end the first time", "answer": ["June 10, 2002"], "scores": [[0.05251462012529373, 0.9936262369155884, 0.5014188289642334, 0.35491642355918884, 0.9551054239273071, 0.9985328912734985, 0.9973528385162354, 0.999948263168335, 0.8226543664932251, 0.9478819370269775]], "normalized_score": [0.607860803604126], "tokens": [["\u2581June", "\u2581", "1", "0", ",", "\u2581", "2", "0", "0", "2"]], "entropy": [[2.7585549354553223, 0.05084312707185745, 1.459431529045105, 1.715935468673706, 0.24114912748336792, 0.013359861448407173, 0.01928241178393364, 0.0007434461731463671, 0.4939665198326111, 0.2955127954483032]], "avg_entropy": [0.704877922241576], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "April 7, 2016", "score_of_biggest_cluster": 0.607860803604126, "score_of_first_answer": 0.607860803604126, "max_score_of_first_answer": 0.999948263168335, "min_score_of_first_answer": 0.05251462012529373, "avg_score": 0.607860803604126, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3215769170398044, "std_of_first_answer": 0.3215769170398044, "avg_entropy_of_first_answer": 0.704877922241576, "lex_sim": 1.0}
{"question": "what do mobile numbers start with in the uk", "answer": ["7"], "scores": [[0.9046066999435425, 0.07138749957084656]], "normalized_score": [0.25412124395370483], "tokens": [["\u2581", "7"]], "entropy": [[0.7774339914321899, 0.36988499760627747]], "avg_entropy": [0.5736594945192337], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "0", "score_of_biggest_cluster": 0.25412124395370483, "score_of_first_answer": 0.25412124395370483, "max_score_of_first_answer": 0.9046066999435425, "min_score_of_first_answer": 0.07138749957084656, "avg_score": 0.25412124395370483, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41660960018634796, "std_of_first_answer": 0.41660960018634796, "avg_entropy_of_first_answer": 0.5736594945192337, "lex_sim": 1.0}
{"question": "who sings sugar sugar you are my candy girl", "answer": ["?"], "scores": [[0.004575406201183796, 0.22448532283306122, 0.8721815347671509, 0.9948561191558838, 0.9992876648902893, 0.9851456880569458]], "normalized_score": [0.3094063997268677], "tokens": [["\u2581?", "\u2581and", "\u2581the", "\u2581Myst", "er", "ians"]], "entropy": [[3.331793785095215, 3.002281665802002, 0.45465904474258423, 0.04164058715105057, 0.006908722687512636, 0.0871644914150238]], "avg_entropy": [1.154074716148898], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Archies", "score_of_biggest_cluster": 0.3094063997268677, "score_of_first_answer": 0.3094063997268677, "max_score_of_first_answer": 0.9992876648902893, "min_score_of_first_answer": 0.004575406201183796, "avg_score": 0.3094063997268677, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40718926350145684, "std_of_first_answer": 0.40718926350145684, "avg_entropy_of_first_answer": 1.154074716148898, "lex_sim": 0.0}
{"question": "where did the idea of a unicorn come from", "answer": ["Ancient tales from China and Greece"], "scores": [[0.07459766417741776, 0.9534314274787903, 0.00156412401702255, 0.08370862901210785, 0.07293280959129333, 0.2786877155303955, 0.05970863997936249]], "normalized_score": [0.07323747873306274], "tokens": [["\u2581An", "cient", "\u2581tales", "\u2581from", "\u2581China", "\u2581and", "\u2581Greece"]], "entropy": [[4.498867034912109, 0.3798859715461731, 2.686032295227051, 2.801788330078125, 2.8857102394104004, 2.5968198776245117, 2.5091938972473145]], "avg_entropy": [2.622613949435098], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Greek writers of natural history", "score_of_biggest_cluster": 0.07323747873306274, "score_of_first_answer": 0.07323747873306274, "max_score_of_first_answer": 0.9534314274787903, "min_score_of_first_answer": 0.00156412401702255, "avg_score": 0.07323747873306274, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3107791247471632, "std_of_first_answer": 0.3107791247471632, "avg_entropy_of_first_answer": 2.622613949435098, "lex_sim": 1.0}
{"question": "when does season 18 of law and order svu start", "answer": ["Wednesday, September 26, 2017 at 9:00pm"], "scores": [[0.07669366896152496, 0.9677502512931824, 0.998675525188446, 0.9032087326049805, 0.7904167175292969, 0.9989701509475708, 0.9219129085540771, 0.49407726526260376, 0.7140954732894897, 0.9917382597923279, 0.9921532273292542, 0.9997552037239075, 0.9902322292327881, 0.1769789308309555, 0.09315194934606552, 0.9933121800422668, 0.788151204586029, 0.7480111718177795, 0.9970948696136475, 0.9990987777709961, 0.1362495869398117]], "normalized_score": [0.6000139713287354], "tokens": [["\u2581Wed", "nes", "day", ",", "\u2581September", "\u2581", "2", "6", ",", "\u2581", "2", "0", "1", "7", "\u2581at", "\u2581", "9", ":", "0", "0", "pm"]], "entropy": [[2.0919337272644043, 0.19610220193862915, 0.012193797156214714, 0.4759753346443176, 0.9426162242889404, 0.010396061465144157, 0.30626630783081055, 1.6154966354370117, 0.9402968883514404, 0.061517756432294846, 0.05464758351445198, 0.0029914844781160355, 0.05633894354104996, 0.728703498840332, 1.103858470916748, 0.06844750791788101, 0.8096441626548767, 1.0383312702178955, 0.0250752754509449, 0.009165894240140915, 1.4910321235656738]], "avg_entropy": [0.5733824357213009], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "September 21, 2016", "score_of_biggest_cluster": 0.6000139713287354, "score_of_first_answer": 0.6000139713287354, "max_score_of_first_answer": 0.9997552037239075, "min_score_of_first_answer": 0.07669366896152496, "avg_score": 0.6000139713287354, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33119042223747047, "std_of_first_answer": 0.33119042223747047, "avg_entropy_of_first_answer": 0.5733824357213009, "lex_sim": 1.0}
{"question": "when was the last time mount ruapehu erupted", "answer": ["September 25, 2007"], "scores": [[0.20024602115154266, 0.9847633242607117, 0.7380994558334351, 0.40290167927742004, 0.9387726783752441, 0.9972074627876282, 0.8886656165122986, 0.9990330934524536, 0.953874409198761, 0.9978659749031067]], "normalized_score": [0.7356034517288208], "tokens": [["\u2581September", "\u2581", "2", "5", ",", "\u2581", "2", "0", "0", "7"]], "entropy": [[1.790977954864502, 0.10412704199552536, 0.8021999001502991, 1.609398365020752, 0.3693932890892029, 0.026202082633972168, 0.3503517806529999, 0.010200639255344868, 0.18785376846790314, 0.019441623240709305]], "avg_entropy": [0.5270146445371211], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "25 September 2007", "score_of_biggest_cluster": 0.7356034517288208, "score_of_first_answer": 0.7356034517288208, "max_score_of_first_answer": 0.9990330934524536, "min_score_of_first_answer": 0.20024602115154266, "avg_score": 0.7356034517288208, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2688466026222808, "std_of_first_answer": 0.2688466026222808, "avg_entropy_of_first_answer": 0.5270146445371211, "lex_sim": 1.0}
{"question": "who plays the short cop on will and grace", "answer": ["Leslie Jordan"], "scores": [[0.021494880318641663, 0.8426096439361572, 0.9820811152458191]], "normalized_score": [0.26103755831718445], "tokens": [["\u2581Les", "lie", "\u2581Jordan"]], "entropy": [[3.666814088821411, 0.6070038080215454, 0.1646217554807663]], "avg_entropy": [1.4794798841079075], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ryan Pinkston", "score_of_biggest_cluster": 0.26103755831718445, "score_of_first_answer": 0.26103755831718445, "max_score_of_first_answer": 0.9820811152458191, "min_score_of_first_answer": 0.021494880318641663, "avg_score": 0.26103755831718445, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4237934063080344, "std_of_first_answer": 0.4237934063080344, "avg_entropy_of_first_answer": 1.4794798841079075, "lex_sim": 1.0}
{"question": "who is the richest club in the championship", "answer": ["Derby County"], "scores": [[0.08159609884023666, 0.8846042156219482]], "normalized_score": [0.26866385340690613], "tokens": [["\u2581Derby", "\u2581County"]], "entropy": [[2.84861421585083, 0.4698996841907501]], "avg_entropy": [1.65925695002079], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Manchester City", "score_of_biggest_cluster": 0.26866385340690613, "score_of_first_answer": 0.26866385340690613, "max_score_of_first_answer": 0.8846042156219482, "min_score_of_first_answer": 0.08159609884023666, "avg_score": 0.26866385340690613, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4015040583908558, "std_of_first_answer": 0.4015040583908558, "avg_entropy_of_first_answer": 1.65925695002079, "lex_sim": 1.0}
{"question": "who developed a set of postulates to prove that specific microorganisms cause disease", "answer": ["Louis Pasteur"], "scores": [[0.4800346791744232, 0.9829711318016052, 0.9911319613456726, 0.9986951947212219]], "normalized_score": [0.8266933560371399], "tokens": [["\u2581Louis", "\u2581P", "aste", "ur"]], "entropy": [[2.716116428375244, 0.126585453748703, 0.05758506804704666, 0.013555850833654404]], "avg_entropy": [0.728460700251162], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Robert Koch", "score_of_biggest_cluster": 0.8266933560371399, "score_of_first_answer": 0.8266933560371399, "max_score_of_first_answer": 0.9986951947212219, "min_score_of_first_answer": 0.4800346791744232, "avg_score": 0.8266933560371399, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22129523342753607, "std_of_first_answer": 0.22129523342753607, "avg_entropy_of_first_answer": 0.728460700251162, "lex_sim": 1.0}
{"question": "who has the most number one single hits", "answer": ["Elvis Presley"], "scores": [[0.544269323348999, 0.9818447828292847, 0.9720519185066223, 0.9969584941864014]], "normalized_score": [0.8483123779296875], "tokens": [["\u2581El", "vis", "\u2581Pres", "ley"]], "entropy": [[2.206211566925049, 0.09715728461742401, 0.17862118780612946, 0.024666495621204376]], "avg_entropy": [0.6266641337424517], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Beatles", "score_of_biggest_cluster": 0.8483123779296875, "score_of_first_answer": 0.8483123779296875, "max_score_of_first_answer": 0.9969584941864014, "min_score_of_first_answer": 0.544269323348999, "avg_score": 0.8483123779296875, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19045051503398294, "std_of_first_answer": 0.19045051503398294, "avg_entropy_of_first_answer": 0.6266641337424517, "lex_sim": 1.0}
{"question": "state the position of india in terms of population size", "answer": ["2nd"], "scores": [[0.5611344575881958, 0.8996119499206543, 0.9498522877693176]], "normalized_score": [0.7826953530311584], "tokens": [["\u2581", "2", "nd"]], "entropy": [[1.844595193862915, 0.4277436435222626, 0.29893559217453003]], "avg_entropy": [0.8570914765199026], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "second", "score_of_biggest_cluster": 0.7826953530311584, "score_of_first_answer": 0.7826953530311584, "max_score_of_first_answer": 0.9498522877693176, "min_score_of_first_answer": 0.5611344575881958, "avg_score": 0.7826953530311584, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.17262440147568195, "std_of_first_answer": 0.17262440147568195, "avg_entropy_of_first_answer": 0.8570914765199026, "lex_sim": 1.0}
{"question": "first jnanpith award was an autor of which language", "answer": ["Kumudini Hajabar"], "scores": [[0.12380591779947281, 0.3447934687137604, 0.5223276615142822, 0.6870535612106323, 0.5278990864753723, 0.809015154838562, 0.0018462896114215255, 0.8705198764801025]], "normalized_score": [0.23863138258457184], "tokens": [["\u2581K", "um", "ud", "ini", "\u2581H", "aj", "ab", "ar"]], "entropy": [[3.277930736541748, 2.6405017375946045, 0.9874016642570496, 1.7972991466522217, 2.5448083877563477, 1.1175198554992676, 1.8102517127990723, 0.9125583171844482]], "avg_entropy": [1.886033944785595], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Malayalam", "score_of_biggest_cluster": 0.23863138258457184, "score_of_first_answer": 0.23863138258457184, "max_score_of_first_answer": 0.8705198764801025, "min_score_of_first_answer": 0.0018462896114215255, "avg_score": 0.23863138258457184, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29180915793923634, "std_of_first_answer": 0.29180915793923634, "avg_entropy_of_first_answer": 1.886033944785595, "lex_sim": 1.0}
{"question": "who did the steelers lose to in the playoffs last year", "answer": ["Baltimore Ravens"], "scores": [[0.32842814922332764, 0.9782177805900574, 0.9998784065246582]], "normalized_score": [0.6848692893981934], "tokens": [["\u2581Baltimore", "\u2581Rav", "ens"]], "entropy": [[2.7031197547912598, 0.1395011842250824, 0.001711815595626831]], "avg_entropy": [0.9481109182039896], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "New England Patriots", "score_of_biggest_cluster": 0.6848692893981934, "score_of_first_answer": 0.6848692893981934, "max_score_of_first_answer": 0.9998784065246582, "min_score_of_first_answer": 0.32842814922332764, "avg_score": 0.6848692893981934, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.311544752586493, "std_of_first_answer": 0.311544752586493, "avg_entropy_of_first_answer": 0.9481109182039896, "lex_sim": 1.0}
{"question": "when did macbook pro 13 inch come out", "answer": ["April 14, 2015"], "scores": [[0.020004000514745712, 0.9726540446281433, 0.6474983096122742, 0.09788531810045242, 0.979150116443634, 0.9986757636070251, 0.9961355924606323, 0.9999591112136841, 0.8609403967857361, 0.5317033529281616]], "normalized_score": [0.4720900058746338], "tokens": [["\u2581April", "\u2581", "1", "4", ",", "\u2581", "2", "0", "1", "5"]], "entropy": [[2.666447401046753, 0.14779987931251526, 0.8395389914512634, 1.839369535446167, 0.1318759173154831, 0.011948813684284687, 0.025589102879166603, 0.0005653195548802614, 0.408415287733078, 1.2501068115234375]], "avg_entropy": [0.7321657059947029], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "October 2008", "score_of_biggest_cluster": 0.4720900058746338, "score_of_first_answer": 0.4720900058746338, "max_score_of_first_answer": 0.9999591112136841, "min_score_of_first_answer": 0.020004000514745712, "avg_score": 0.4720900058746338, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36055074440324336, "std_of_first_answer": 0.36055074440324336, "avg_entropy_of_first_answer": 0.7321657059947029, "lex_sim": 1.0}
{"question": "who wrote i can feel at home in this world anymore", "answer": ["Maharshi Rishi Padmini"], "scores": [[0.005386635195463896, 0.019200218841433525, 0.2012932300567627, 0.995050847530365, 0.007693643681704998, 0.35007786750793457, 0.1383458375930786, 0.005235985852777958, 0.9909075498580933]], "normalized_score": [0.06994109600782394], "tokens": [["\u2581Mah", "ar", "sh", "i", "\u2581R", "ishi", "\u2581P", "admin", "i"]], "entropy": [[4.93513298034668, 2.8616414070129395, 2.034576654434204, 0.052965957671403885, 3.1696624755859375, 2.8481016159057617, 2.9575464725494385, 3.410616636276245, 0.0887058824300766]], "avg_entropy": [2.484327786912521], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Macon Blair", "score_of_biggest_cluster": 0.06994109600782394, "score_of_first_answer": 0.06994109600782394, "max_score_of_first_answer": 0.995050847530365, "min_score_of_first_answer": 0.005235985852777958, "avg_score": 0.06994109600782394, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38539833021215697, "std_of_first_answer": 0.38539833021215697, "avg_entropy_of_first_answer": 2.484327786912521, "lex_sim": 1.0}
{"question": "what's the population of prince edward island", "answer": ["150,250"], "scores": [[0.9291940927505493, 0.9865330457687378, 0.2153068482875824, 0.21728454530239105, 0.8063704967498779, 0.13715487718582153, 0.056192267686128616, 0.19017666578292847]], "normalized_score": [0.2904762029647827], "tokens": [["\u2581", "1", "5", "0", ",", "2", "5", "0"]], "entropy": [[0.6017543077468872, 0.10237718373537064, 0.716217577457428, 1.8834519386291504, 0.9855911135673523, 2.0707197189331055, 2.1347265243530273, 1.6547598838806152]], "avg_entropy": [1.268699781037867], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "142,907 residents", "score_of_biggest_cluster": 0.2904762029647827, "score_of_first_answer": 0.2904762029647827, "max_score_of_first_answer": 0.9865330457687378, "min_score_of_first_answer": 0.056192267686128616, "avg_score": 0.2904762029647827, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36635357515842043, "std_of_first_answer": 0.36635357515842043, "avg_entropy_of_first_answer": 1.268699781037867, "lex_sim": 1.0}
{"question": "when did the last volcano erupt in iceland", "answer": ["2010"], "scores": [[0.24938668310642242, 0.6986373662948608, 0.7459213137626648, 0.8960368037223816, 0.312349796295166]], "normalized_score": [0.5154149532318115], "tokens": [["\u2581", "2", "0", "1", "0"]], "entropy": [[3.1763503551483154, 0.9994040727615356, 1.0968196392059326, 0.48396041989326477, 1.6889748573303223]], "avg_entropy": [1.489101868867874], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "2010", "score_of_biggest_cluster": 0.5154149532318115, "score_of_first_answer": 0.5154149532318115, "max_score_of_first_answer": 0.8960368037223816, "min_score_of_first_answer": 0.24938668310642242, "avg_score": 0.5154149532318115, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2539388420818387, "std_of_first_answer": 0.2539388420818387, "avg_entropy_of_first_answer": 1.489101868867874, "lex_sim": 1.0}
{"question": "when did frank sinatra first sing new york new york", "answer": ["October 1972"], "scores": [[0.23889465630054474, 0.9923601746559143, 0.5354328751564026, 0.109538234770298, 0.2677750587463379, 0.20145870745182037]], "normalized_score": [0.3014281392097473], "tokens": [["\u2581October", "\u2581", "1", "9", "7", "2"]], "entropy": [[3.091428279876709, 0.05222275108098984, 1.2210301160812378, 2.346156120300293, 1.3988624811172485, 1.5277371406555176]], "avg_entropy": [1.606239481518666], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1980", "score_of_biggest_cluster": 0.3014281392097473, "score_of_first_answer": 0.3014281392097473, "max_score_of_first_answer": 0.9923601746559143, "min_score_of_first_answer": 0.109538234770298, "avg_score": 0.3014281392097473, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29887620487178, "std_of_first_answer": 0.29887620487178, "avg_entropy_of_first_answer": 1.606239481518666, "lex_sim": 1.0}
{"question": "what kind of beast is the beast from beauty and the beast", "answer": ["a creature"], "scores": [[0.23008787631988525, 0.006898222025483847]], "normalized_score": [0.039839643985033035], "tokens": [["\u2581a", "\u2581creature"]], "entropy": [[3.9946112632751465, 3.3465256690979004]], "avg_entropy": [3.6705684661865234], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "a chimera", "score_of_biggest_cluster": 0.039839643985033035, "score_of_first_answer": 0.039839643985033035, "max_score_of_first_answer": 0.23008787631988525, "min_score_of_first_answer": 0.006898222025483847, "avg_score": 0.039839643985033035, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1115948271472007, "std_of_first_answer": 0.1115948271472007, "avg_entropy_of_first_answer": 3.6705684661865234, "lex_sim": 1.0}
{"question": "who scored the most points in a game nba history", "answer": ["Wilt Chamberlain"], "scores": [[0.7193193435668945, 0.9864029288291931, 0.9922772645950317, 0.9747658967971802]], "normalized_score": [0.9101801514625549], "tokens": [["\u2581W", "ilt", "\u2581Chamber", "lain"]], "entropy": [[1.2747479677200317, 0.07776062935590744, 0.06412392854690552, 0.14416386187076569]], "avg_entropy": [0.3901990968734026], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Wilt Chamberlain", "score_of_biggest_cluster": 0.9101801514625549, "score_of_first_answer": 0.9101801514625549, "max_score_of_first_answer": 0.9922772645950317, "min_score_of_first_answer": 0.7193193435668945, "avg_score": 0.9101801514625549, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.114991626342735, "std_of_first_answer": 0.114991626342735, "avg_entropy_of_first_answer": 0.3901990968734026, "lex_sim": 1.0}
{"question": "when does the second half of vikings season 5 air", "answer": ["November"], "scores": [[0.5806172490119934]], "normalized_score": [0.5806172490119934], "tokens": [["\u2581November"]], "entropy": [[2.197101354598999]], "avg_entropy": [2.197101354598999], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2018", "score_of_biggest_cluster": 0.5806172490119934, "score_of_first_answer": 0.5806172490119934, "max_score_of_first_answer": 0.5806172490119934, "min_score_of_first_answer": 0.5806172490119934, "avg_score": 0.5806172490119934, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.197101354598999, "lex_sim": 1.0}
{"question": "where did the french immigrants settle in texas", "answer": ["AUSTIN, TEXAS"], "scores": [[0.0024527115747332573, 0.022502388805150986, 0.9954620003700256, 0.1344725787639618, 0.5294644236564636, 0.6256452798843384, 0.9904835224151611]], "normalized_score": [0.15768569707870483], "tokens": [["\u2581A", "UST", "IN", ",", "\u2581T", "EX", "AS"]], "entropy": [[4.486517429351807, 5.33661413192749, 0.04609554633498192, 1.4857573509216309, 2.2942419052124023, 0.7535244226455688, 0.07352058589458466]], "avg_entropy": [2.0680387674697807], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1685", "score_of_biggest_cluster": 0.15768569707870483, "score_of_first_answer": 0.15768569707870483, "max_score_of_first_answer": 0.9954620003700256, "min_score_of_first_answer": 0.0024527115747332573, "avg_score": 0.15768569707870483, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3975267129122444, "std_of_first_answer": 0.3975267129122444, "avg_entropy_of_first_answer": 2.0680387674697807, "lex_sim": 1.0}
{"question": "who sings song at the end of beauty and the beast", "answer": ["Celine Dion"], "scores": [[0.5293691754341125, 0.8073734641075134, 0.9701757431030273, 0.9971513152122498]], "normalized_score": [0.8018832802772522], "tokens": [["\u2581C", "eline", "\u2581D", "ion"]], "entropy": [[1.940660834312439, 0.6646321415901184, 0.1681487113237381, 0.023755833506584167]], "avg_entropy": [0.6992993801832199], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Josh Groban", "score_of_biggest_cluster": 0.8018832802772522, "score_of_first_answer": 0.8018832802772522, "max_score_of_first_answer": 0.9971513152122498, "min_score_of_first_answer": 0.5293691754341125, "avg_score": 0.8018832802772522, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18602164545136782, "std_of_first_answer": 0.18602164545136782, "avg_entropy_of_first_answer": 0.6992993801832199, "lex_sim": 1.0}
{"question": "who is the no. 1 ranked tennis player in the world", "answer": ["Stefan Edberg"], "scores": [[0.0009065340855158865, 0.14875508844852448, 0.9943727254867554]], "normalized_score": [0.05118410289287567], "tokens": [["\u2581Stefan", "\u2581Ed", "berg"]], "entropy": [[2.2236294746398926, 0.790549099445343, 0.05375444516539574]], "avg_entropy": [1.0226443397502105], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rafael Nadal", "score_of_biggest_cluster": 0.05118410289287567, "score_of_first_answer": 0.05118410289287567, "max_score_of_first_answer": 0.9943727254867554, "min_score_of_first_answer": 0.0009065340855158865, "avg_score": 0.05118410289287567, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43765834677984927, "std_of_first_answer": 0.43765834677984927, "avg_entropy_of_first_answer": 1.0226443397502105, "lex_sim": 1.0}
{"question": "when does the men's worlds curling start", "answer": ["30 March 2019"], "scores": [[0.044933270663022995, 0.5488077402114868, 0.5022271275520325, 0.6881881952285767, 0.7995734214782715, 0.987966001033783, 0.9990134239196777, 0.9943354725837708, 0.9197375774383545]], "normalized_score": [0.5679751634597778], "tokens": [["\u2581", "3", "0", "\u2581March", "\u2581", "2", "0", "1", "9"]], "entropy": [[1.9723429679870605, 1.3526800870895386, 1.165307641029358, 1.2785327434539795, 0.8040652871131897, 0.07874565571546555, 0.010349635034799576, 0.03908313810825348, 0.3804996907711029]], "avg_entropy": [0.7868452051447498], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "March 31", "score_of_biggest_cluster": 0.5679751634597778, "score_of_first_answer": 0.5679751634597778, "max_score_of_first_answer": 0.9990134239196777, "min_score_of_first_answer": 0.044933270663022995, "avg_score": 0.5679751634597778, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2989978821715449, "std_of_first_answer": 0.2989978821715449, "avg_entropy_of_first_answer": 0.7868452051447498, "lex_sim": 1.0}
{"question": "who was the youngest president to be elected", "answer": ["John F. Kennedy"], "scores": [[0.282370001077652, 0.7026623487472534, 0.8637692332267761, 0.9953238368034363]], "normalized_score": [0.6426608562469482], "tokens": [["\u2581John", "\u2581F", ".", "\u2581Kennedy"]], "entropy": [[1.3365144729614258, 0.8117424249649048, 0.4486299157142639, 0.03988076001405716]], "avg_entropy": [0.6591918934136629], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Theodore Roosevelt", "score_of_biggest_cluster": 0.6426608562469482, "score_of_first_answer": 0.6426608562469482, "max_score_of_first_answer": 0.9953238368034363, "min_score_of_first_answer": 0.282370001077652, "avg_score": 0.6426608562469482, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26831496010629635, "std_of_first_answer": 0.26831496010629635, "avg_entropy_of_first_answer": 0.6591918934136629, "lex_sim": 1.0}
{"question": "what is the cross on a letter t called", "answer": ["a crossbar or kite"], "scores": [[0.03670000657439232, 0.7139701843261719, 0.35409820079803467, 0.04019023850560188, 0.002915559569373727, 0.336076021194458]], "normalized_score": [0.08455219119787216], "tokens": [["\u2581a", "\u2581cross", "bar", "\u2581or", "\u2581k", "ite"]], "entropy": [[3.3864848613739014, 1.8750799894332886, 2.6494147777557373, 0.7248997688293457, 2.8464651107788086, 3.5045673847198486]], "avg_entropy": [2.497818648815155], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "arm", "score_of_biggest_cluster": 0.08455219119787216, "score_of_first_answer": 0.08455219119787216, "max_score_of_first_answer": 0.7139701843261719, "min_score_of_first_answer": 0.002915559569373727, "avg_score": 0.08455219119787216, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25299494727266997, "std_of_first_answer": 0.25299494727266997, "avg_entropy_of_first_answer": 2.497818648815155, "lex_sim": 1.0}
{"question": "who is the number one ranked golfer in the world right now", "answer": ["Justin Thomas"], "scores": [[0.15536744892597198, 0.6266564130783081]], "normalized_score": [0.3120288550853729], "tokens": [["\u2581Justin", "\u2581Thomas"]], "entropy": [[2.0815441608428955, 1.4045844078063965]], "avg_entropy": [1.743064284324646], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dustin Johnson", "score_of_biggest_cluster": 0.3120288550853729, "score_of_first_answer": 0.3120288550853729, "max_score_of_first_answer": 0.6266564130783081, "min_score_of_first_answer": 0.15536744892597198, "avg_score": 0.3120288550853729, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23564448207616806, "std_of_first_answer": 0.23564448207616806, "avg_entropy_of_first_answer": 1.743064284324646, "lex_sim": 1.0}
{"question": "what is the purpose of the bromophenol blue dye in the samples", "answer": ["The purpose of the dye is to color the protein mixture or it can also be used as a tracking dye"], "scores": [[0.05088602751493454, 0.18875260651111603, 0.88625168800354, 0.5986975431442261, 0.1594507247209549, 0.9923326969146729, 0.8551374077796936, 0.8789023756980896, 0.03468530252575874, 0.6131212115287781, 0.03581459820270538, 0.014005562290549278, 0.0022016321308910847, 0.002561353612691164, 0.2859043776988983, 0.1638982743024826, 0.6824467778205872, 0.893288254737854, 0.42868322134017944, 0.7449009418487549, 0.1468656361103058, 0.9054635167121887, 0.9996199607849121]], "normalized_score": [0.19450676441192627], "tokens": [["\u2581The", "\u2581purpose", "\u2581of", "\u2581the", "\u2581d", "ye", "\u2581is", "\u2581to", "\u2581color", "\u2581the", "\u2581protein", "\u2581mixture", "\u2581or", "\u2581it", "\u2581can", "\u2581also", "\u2581be", "\u2581used", "\u2581as", "\u2581a", "\u2581tracking", "\u2581d", "ye"]], "entropy": [[4.506117820739746, 2.806539297103882, 0.46686941385269165, 1.6285141706466675, 1.677579402923584, 0.04829346016049385, 0.8311818242073059, 0.8343290090560913, 4.048759937286377, 1.9423611164093018, 4.4375834465026855, 3.625901699066162, 2.9730241298675537, 3.9502687454223633, 2.957871913909912, 1.7727950811386108, 1.7503570318222046, 0.7645763754844666, 1.2705401182174683, 1.155931830406189, 3.9069085121154785, 0.6139811277389526, 0.003878192277625203]], "avg_entropy": [2.0858332024502526], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "as a pH indicator", "score_of_biggest_cluster": 0.19450676441192627, "score_of_first_answer": 0.19450676441192627, "max_score_of_first_answer": 0.9996199607849121, "min_score_of_first_answer": 0.0022016321308910847, "avg_score": 0.19450676441192627, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37149729027777784, "std_of_first_answer": 0.37149729027777784, "avg_entropy_of_first_answer": 2.0858332024502526, "lex_sim": 1.0}
{"question": "known as the punk poet who used poetry in their music", "answer": ["Patti Smith"], "scores": [[0.39865046739578247, 0.9183034300804138, 0.9904773235321045]], "normalized_score": [0.7130845189094543], "tokens": [["\u2581P", "atti", "\u2581Smith"]], "entropy": [[3.3132662773132324, 0.36001652479171753, 0.0631275475025177]], "avg_entropy": [1.2454701165358226], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "John Cooper Clarke", "score_of_biggest_cluster": 0.7130845189094543, "score_of_first_answer": 0.7130845189094543, "max_score_of_first_answer": 0.9904773235321045, "min_score_of_first_answer": 0.39865046739578247, "avg_score": 0.7130845189094543, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.26363006491991214, "std_of_first_answer": 0.26363006491991214, "avg_entropy_of_first_answer": 1.2454701165358226, "lex_sim": 1.0}
{"question": "when did the black death end in england", "answer": ["July 1, 1350"], "scores": [[0.009144160896539688, 0.9689508676528931, 0.549494206905365, 0.15969136357307434, 0.9979552030563354, 0.9742684364318848, 0.8346650004386902, 0.37963783740997314, 0.8871967792510986]], "normalized_score": [0.3907791078090668], "tokens": [["\u2581July", "\u2581", "1", ",", "\u2581", "1", "3", "5", "0"]], "entropy": [[2.4204211235046387, 0.18252715468406677, 1.206214427947998, 2.1305863857269287, 0.01892700418829918, 0.13325311243534088, 0.7134150266647339, 1.0578186511993408, 0.5459569692611694]], "avg_entropy": [0.9343466506236129], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "December 1349", "score_of_biggest_cluster": 0.3907791078090668, "score_of_first_answer": 0.3907791078090668, "max_score_of_first_answer": 0.9979552030563354, "min_score_of_first_answer": 0.009144160896539688, "avg_score": 0.3907791078090668, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3577292297980452, "std_of_first_answer": 0.3577292297980452, "avg_entropy_of_first_answer": 0.9343466506236129, "lex_sim": 1.0}
{"question": "who is known as the father of indian constitution", "answer": ["Dr. Bhim Rao Ambedkar"], "scores": [[0.26985877752304077, 0.833509087562561, 0.16841500997543335, 0.9983218312263489, 0.31211426854133606, 0.9996100068092346, 0.9674235582351685, 0.9911255240440369, 0.9806466102600098]], "normalized_score": [0.6064414978027344], "tokens": [["\u2581Dr", ".", "\u2581Bh", "im", "\u2581R", "ao", "\u2581Am", "bed", "kar"]], "entropy": [[1.7652947902679443, 0.6880343556404114, 1.3044955730438232, 0.01866394281387329, 0.7469220161437988, 0.004049007315188646, 0.2223946452140808, 0.07126137614250183, 0.12744756042957306]], "avg_entropy": [0.549840363001244], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Bhimrao Ramji Ambedkar", "score_of_biggest_cluster": 0.6064414978027344, "score_of_first_answer": 0.6064414978027344, "max_score_of_first_answer": 0.9996100068092346, "min_score_of_first_answer": 0.16841500997543335, "avg_score": 0.6064414978027344, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.34062852390327253, "std_of_first_answer": 0.34062852390327253, "avg_entropy_of_first_answer": 0.549840363001244, "lex_sim": 1.0}
{"question": "where does sound travel fastest solid liquid or gas", "answer": ["Solid"], "scores": [[0.30567240715026855, 0.9943782687187195]], "normalized_score": [0.5513201951980591], "tokens": [["\u2581Sol", "id"]], "entropy": [[2.731473922729492, 0.03818114846944809]], "avg_entropy": [1.3848275355994701], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "solids", "score_of_biggest_cluster": 0.5513201951980591, "score_of_first_answer": 0.5513201951980591, "max_score_of_first_answer": 0.9943782687187195, "min_score_of_first_answer": 0.30567240715026855, "avg_score": 0.5513201951980591, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.34435293078422546, "std_of_first_answer": 0.34435293078422546, "avg_entropy_of_first_answer": 1.3848275355994701, "lex_sim": 1.0}
{"question": "when does star trek discovery season 1 end", "answer": ["October 4"], "scores": [[0.0856618657708168, 0.9846033453941345, 0.03323018178343773]], "normalized_score": [0.14099179208278656], "tokens": [["\u2581October", "\u2581", "4"]], "entropy": [[3.8661601543426514, 0.10328524559736252, 1.3329821825027466]], "avg_entropy": [1.7674758608142536], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "February\u00a011,\u00a02018", "score_of_biggest_cluster": 0.14099179208278656, "score_of_first_answer": 0.14099179208278656, "max_score_of_first_answer": 0.9846033453941345, "min_score_of_first_answer": 0.03323018178343773, "avg_score": 0.14099179208278656, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4366483153537916, "std_of_first_answer": 0.4366483153537916, "avg_entropy_of_first_answer": 1.7674758608142536, "lex_sim": 1.0}
{"question": "who produced the movie i can only imagine", "answer": ["Aaron L. Willis"], "scores": [[0.002534938743337989, 0.849430501461029, 0.8344967365264893, 0.7526845335960388, 0.44214722514152527, 0.7182413935661316]], "normalized_score": [0.27468085289001465], "tokens": [["\u2581A", "aron", "\u2581L", ".", "\u2581Will", "is"]], "entropy": [[4.179582595825195, 1.2502778768539429, 1.355370283126831, 1.5603922605514526, 1.6541450023651123, 1.494997501373291]], "avg_entropy": [1.9157942533493042], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pete Shilaimon", "score_of_biggest_cluster": 0.27468085289001465, "score_of_first_answer": 0.27468085289001465, "max_score_of_first_answer": 0.849430501461029, "min_score_of_first_answer": 0.002534938743337989, "avg_score": 0.27468085289001465, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.298992711845123, "std_of_first_answer": 0.298992711845123, "avg_entropy_of_first_answer": 1.9157942533493042, "lex_sim": 1.0}
{"question": "who represents the insured in the investigation of a claim", "answer": ["the claimants\u2019 representative\u301c\u30a2\u30fc\u30ab\u30ea\u30af\u30de\u30b9 Carol\u301c"], "scores": [[0.09640929847955704, 0.11713031679391861, 0.0075215185061097145, 0.042853571474552155, 0.06441247463226318, 2.819665612108224e-10, 0.0021505935583263636, 0.009434481151401997, 0.2362034022808075, 0.004541589878499508, 0.003993560094386339, 0.03812514618039131, 0.03779487684369087, 4.710671873908723e-06, 0.011264636181294918]], "normalized_score": [0.003768671303987503], "tokens": [["\u2581the", "\u2581claim", "ants", "\u2019", "\u2581representative", "\u301c", "\u30a2", "\u30fc", "\u30ab", "\u30ea", "\u30af", "\u30de", "\u30b9", "\u2581Carol", "\u301c"]], "entropy": [[3.5584027767181396, 2.058000326156616, 0.17606288194656372, 2.649996280670166, 2.3854141235351562, 1.0154500007629395, 4.689913749694824, 3.697068691253662, 3.2834672927856445, 2.1114184856414795, 2.1881227493286133, 4.243106365203857, 1.793194055557251, 4.587099075317383, 5.168581008911133]], "avg_entropy": [2.907019857565562], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Public adjusters", "score_of_biggest_cluster": 0.003768671303987503, "score_of_first_answer": 0.003768671303987503, "max_score_of_first_answer": 0.2362034022808075, "min_score_of_first_answer": 2.819665612108224e-10, "avg_score": 0.003768671303987503, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06212679450618136, "std_of_first_answer": 0.06212679450618136, "avg_entropy_of_first_answer": 2.907019857565562, "lex_sim": 1.0}
{"question": "where do rory and paris go to college", "answer": ["Chilton is a private school for exceptional girls in Connecticut."], "scores": [[0.22350946068763733, 0.9587615132331848, 0.9973030090332031, 0.0008107323083095253, 0.6134345531463623, 0.19502389430999756, 0.546949565410614, 0.32597875595092773, 0.002877958817407489, 0.5580437779426575, 0.6255959272384644, 0.19922736287117004, 0.13062728941440582, 0.5687016248703003]], "normalized_score": [0.18498821556568146], "tokens": [["\u2581Ch", "il", "ton", "\u2581is", "\u2581a", "\u2581private", "\u2581school", "\u2581for", "\u2581exception", "al", "\u2581girls", "\u2581in", "\u2581Connecticut", "."]], "entropy": [[2.8298370838165283, 0.36954590678215027, 0.02598339132964611, 1.679704189300537, 2.069692611694336, 3.242769718170166, 1.8725612163543701, 2.1939189434051514, 0.999791145324707, 0.692681074142456, 1.4371763467788696, 2.2035114765167236, 3.566344976425171, 1.542285442352295]], "avg_entropy": [1.7661288230280792], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yale University", "score_of_biggest_cluster": 0.18498821556568146, "score_of_first_answer": 0.18498821556568146, "max_score_of_first_answer": 0.9973030090332031, "min_score_of_first_answer": 0.0008107323083095253, "avg_score": 0.18498821556568146, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30942861694918816, "std_of_first_answer": 0.30942861694918816, "avg_entropy_of_first_answer": 1.7661288230280792, "lex_sim": 1.0}
{"question": "what is the maximum data rate for the 802.11a standard select one", "answer": ["54 Mbps"], "scores": [[0.9609777331352234, 0.9634137153625488, 0.9762144684791565, 0.6039866805076599, 0.7358593940734863, 0.9577038884162903]], "normalized_score": [0.8528136610984802], "tokens": [["\u2581", "5", "4", "\u2581M", "b", "ps"]], "entropy": [[0.38944900035858154, 0.2270093560218811, 0.16639824211597443, 1.4672954082489014, 0.6934783458709717, 0.19079241156578064]], "avg_entropy": [0.5224037940303484], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "54\u00a0Mbit/s", "score_of_biggest_cluster": 0.8528136610984802, "score_of_first_answer": 0.8528136610984802, "max_score_of_first_answer": 0.9762144684791565, "min_score_of_first_answer": 0.6039866805076599, "avg_score": 0.8528136610984802, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.14413755941863104, "std_of_first_answer": 0.14413755941863104, "avg_entropy_of_first_answer": 0.5224037940303484, "lex_sim": 1.0}
{"question": "when did canada gain independence from great britain", "answer": ["Canada's independence from GB"], "scores": [[0.1841408610343933, 0.027671916410326958, 0.9990718364715576, 0.5432523488998413, 0.4625939726829529, 0.000361429963959381]], "normalized_score": [0.0879363864660263], "tokens": [["\u2581Canada", "'", "s", "\u2581independence", "\u2581from", "\u2581GB"]], "entropy": [[3.3094193935394287, 2.9112467765808105, 0.011762704700231552, 2.8071775436401367, 2.1447253227233887, 0.9828874468803406]], "avg_entropy": [2.0278698646773896], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "April 17, 1982", "score_of_biggest_cluster": 0.0879363864660263, "score_of_first_answer": 0.0879363864660263, "max_score_of_first_answer": 0.9990718364715576, "min_score_of_first_answer": 0.000361429963959381, "avg_score": 0.0879363864660263, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3470554219360582, "std_of_first_answer": 0.3470554219360582, "avg_entropy_of_first_answer": 2.0278698646773896, "lex_sim": 1.0}
{"question": "who sang the original song send in the clowns", "answer": ["Judy Collins"], "scores": [[0.4720954895019531, 0.9882131218910217, 0.9906156659126282]], "normalized_score": [0.7731466293334961], "tokens": [["\u2581Jud", "y", "\u2581Collins"]], "entropy": [[2.2203915119171143, 0.07621775567531586, 0.07538041472434998]], "avg_entropy": [0.7906632274389267], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Glynis Johns", "score_of_biggest_cluster": 0.7731466293334961, "score_of_first_answer": 0.7731466293334961, "max_score_of_first_answer": 0.9906156659126282, "min_score_of_first_answer": 0.4720954895019531, "avg_score": 0.7731466293334961, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2438684426820765, "std_of_first_answer": 0.2438684426820765, "avg_entropy_of_first_answer": 0.7906632274389267, "lex_sim": 1.0}
{"question": "a good that can be used in place of another good", "answer": ["substitute good"], "scores": [[0.8542832732200623, 0.44204187393188477]], "normalized_score": [0.6145152449607849], "tokens": [["\u2581substitute", "\u2581good"]], "entropy": [[0.9117377400398254, 1.063224196434021]], "avg_entropy": [0.9874809682369232], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "A substitute good", "score_of_biggest_cluster": 0.6145152449607849, "score_of_first_answer": 0.6145152449607849, "max_score_of_first_answer": 0.8542832732200623, "min_score_of_first_answer": 0.44204187393188477, "avg_score": 0.6145152449607849, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.20612069964408875, "std_of_first_answer": 0.20612069964408875, "avg_entropy_of_first_answer": 0.9874809682369232, "lex_sim": 1.0}
{"question": "what is the name of the gap between two front teeth", "answer": ["Diastema"], "scores": [[0.17193038761615753, 0.9993212223052979, 0.9939889907836914]], "normalized_score": [0.5548127293586731], "tokens": [["\u2581Di", "ast", "ema"]], "entropy": [[1.394992709159851, 0.008941125124692917, 0.052276164293289185]], "avg_entropy": [0.4854033328592777], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "diastema (plural diastemata)", "score_of_biggest_cluster": 0.5548127293586731, "score_of_first_answer": 0.5548127293586731, "max_score_of_first_answer": 0.9993212223052979, "min_score_of_first_answer": 0.17193038761615753, "avg_score": 0.5548127293586731, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3887850553058205, "std_of_first_answer": 0.3887850553058205, "avg_entropy_of_first_answer": 0.4854033328592777, "lex_sim": 1.0}
{"question": "what does rt mean on a dodge car", "answer": ["Rear Traction control"], "scores": [[0.3063167631626129, 0.7538580894470215, 0.251424103975296, 0.683349072933197, 0.01473582349717617]], "normalized_score": [0.2256196141242981], "tokens": [["\u2581R", "ear", "\u2581Tra", "ction", "\u2581control"]], "entropy": [[4.02703857421875, 1.0762544870376587, 2.7288718223571777, 1.042374610900879, 1.4105536937713623]], "avg_entropy": [2.0570186376571655], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Road/Track", "score_of_biggest_cluster": 0.2256196141242981, "score_of_first_answer": 0.2256196141242981, "max_score_of_first_answer": 0.7538580894470215, "min_score_of_first_answer": 0.01473582349717617, "avg_score": 0.2256196141242981, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27740343207596646, "std_of_first_answer": 0.27740343207596646, "avg_entropy_of_first_answer": 2.0570186376571655, "lex_sim": 1.0}
{"question": "when were 7 books removed from the bible", "answer": ["Third Council of Carthage"], "scores": [[0.005336982663720846, 0.257780522108078, 0.9902825355529785, 0.8836385011672974, 0.9998946189880371, 0.997891366481781]], "normalized_score": [0.3260389566421509], "tokens": [["\u2581Third", "\u2581Council", "\u2581of", "\u2581Car", "th", "age"]], "entropy": [[4.072343349456787, 1.8132473230361938, 0.07892905175685883, 0.4562614858150482, 0.0014833533205091953, 0.02143239974975586]], "avg_entropy": [1.073949493855859], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1546", "score_of_biggest_cluster": 0.3260389566421509, "score_of_first_answer": 0.3260389566421509, "max_score_of_first_answer": 0.9998946189880371, "min_score_of_first_answer": 0.005336982663720846, "avg_score": 0.3260389566421509, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40292056678867816, "std_of_first_answer": 0.40292056678867816, "avg_entropy_of_first_answer": 1.073949493855859, "lex_sim": 1.0}
{"question": "when was the first super bowl the eagles ever won", "answer": ["February 6, 1981"], "scores": [[0.6103883981704712, 0.9976261258125305, 0.4170816242694855, 0.9879968762397766, 0.998938262462616, 0.6297459006309509, 0.9999457597732544, 0.28523334860801697, 0.06323394179344177]], "normalized_score": [0.5213789939880371], "tokens": [["\u2581February", "\u2581", "6", ",", "\u2581", "1", "9", "8", "1"]], "entropy": [[1.8321547508239746, 0.021728141233325005, 1.2641444206237793, 0.08433836698532104, 0.010208602994680405, 0.6596459746360779, 0.0007081517251208425, 1.1232391595840454, 1.110872507095337]], "avg_entropy": [0.6785600084112957], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2017", "score_of_biggest_cluster": 0.5213789939880371, "score_of_first_answer": 0.5213789939880371, "max_score_of_first_answer": 0.9999457597732544, "min_score_of_first_answer": 0.06323394179344177, "avg_score": 0.5213789939880371, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3351121622360394, "std_of_first_answer": 0.3351121622360394, "avg_entropy_of_first_answer": 0.6785600084112957, "lex_sim": 1.0}
{"question": "who ran the fastest 40 yard dash in the nfl", "answer": ["Chris Johnson"], "scores": [[0.7799115180969238, 0.9983799457550049]], "normalized_score": [0.8824103474617004], "tokens": [["\u2581Chris", "\u2581Johnson"]], "entropy": [[1.477567195892334, 0.018247976899147034]], "avg_entropy": [0.7479075863957405], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jakeem Grant", "score_of_biggest_cluster": 0.8824103474617004, "score_of_first_answer": 0.8824103474617004, "max_score_of_first_answer": 0.9983799457550049, "min_score_of_first_answer": 0.7799115180969238, "avg_score": 0.8824103474617004, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10923421382904053, "std_of_first_answer": 0.10923421382904053, "avg_entropy_of_first_answer": 0.7479075863957405, "lex_sim": 1.0}
{"question": "what genre is the magic tree house books", "answer": ["children's fiction"], "scores": [[0.15835286676883698, 0.6681339740753174, 0.9996150732040405, 0.09097962081432343]], "normalized_score": [0.3131963014602661], "tokens": [["\u2581children", "'", "s", "\u2581fiction"]], "entropy": [[2.7725672721862793, 1.0938276052474976, 0.004980749450623989, 2.138345241546631]], "avg_entropy": [1.502430217107758], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Children's historical fantasy", "score_of_biggest_cluster": 0.3131963014602661, "score_of_first_answer": 0.3131963014602661, "max_score_of_first_answer": 0.9996150732040405, "min_score_of_first_answer": 0.09097962081432343, "avg_score": 0.3131963014602661, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37422781192747956, "std_of_first_answer": 0.37422781192747956, "avg_entropy_of_first_answer": 1.502430217107758, "lex_sim": 1.0}
{"question": "who opens the church of the holy sepulchre", "answer": ["the guardianship of the holy sepulchre belongs to"], "scores": [[0.07172952592372894, 0.014605994336307049, 0.27159589529037476, 0.0477416105568409, 0.9311842918395996, 0.9530738592147827, 0.030573764815926552, 0.9157478213310242, 0.997275173664093, 0.9086872339248657, 0.9930558800697327, 0.02650291472673416, 0.8858457803726196]], "normalized_score": [0.23622311651706696], "tokens": [["\u2581the", "\u2581guard", "ians", "hip", "\u2581of", "\u2581the", "\u2581holy", "\u2581sep", "ul", "ch", "re", "\u2581belongs", "\u2581to"]], "entropy": [[4.089836120605469, 4.715608596801758, 1.3200758695602417, 0.7310221195220947, 0.47650641202926636, 0.40637654066085815, 1.8947240114212036, 0.4089590609073639, 0.025335021317005157, 0.3280757963657379, 0.056166090071201324, 2.507493019104004, 0.6072719097137451]], "avg_entropy": [1.3513423513907652], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Sunni Muslim family", "score_of_biggest_cluster": 0.23622311651706696, "score_of_first_answer": 0.23622311651706696, "max_score_of_first_answer": 0.997275173664093, "min_score_of_first_answer": 0.014605994336307049, "avg_score": 0.23622311651706696, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.435673857562213, "std_of_first_answer": 0.435673857562213, "avg_entropy_of_first_answer": 1.3513423513907652, "lex_sim": 1.0}
{"question": "in what state was the united states center of population gravity located during the period 1790", "answer": ["South Carolina"], "scores": [[0.017787931486964226, 0.9478374719619751]], "normalized_score": [0.12984633445739746], "tokens": [["\u2581South", "\u2581Carolina"]], "entropy": [[2.7606101036071777, 0.3014626204967499]], "avg_entropy": [1.5310363620519638], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Maryland", "score_of_biggest_cluster": 0.12984633445739746, "score_of_first_answer": 0.12984633445739746, "max_score_of_first_answer": 0.9478374719619751, "min_score_of_first_answer": 0.017787931486964226, "avg_score": 0.12984633445739746, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46502477023750544, "std_of_first_answer": 0.46502477023750544, "avg_entropy_of_first_answer": 1.5310363620519638, "lex_sim": 1.0}
{"question": "who died in the plane crash greys anatomy", "answer": ["Mark Sloan"], "scores": [[0.12115351855754852, 0.7055058479309082, 0.9673545956611633]], "normalized_score": [0.43565306067466736], "tokens": [["\u2581Mark", "\u2581Slo", "an"]], "entropy": [[2.369126796722412, 1.7021160125732422, 0.15295714139938354]], "avg_entropy": [1.4080666502316792], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dr. Lexie Grey", "score_of_biggest_cluster": 0.43565306067466736, "score_of_first_answer": 0.43565306067466736, "max_score_of_first_answer": 0.9673545956611633, "min_score_of_first_answer": 0.12115351855754852, "avg_score": 0.43565306067466736, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35372441790864567, "std_of_first_answer": 0.35372441790864567, "avg_entropy_of_first_answer": 1.4080666502316792, "lex_sim": 1.0}
{"question": "bacterial cell walls are made rigid by the presence of", "answer": ["peptidoglycans"], "scores": [[0.866117537021637, 0.997672975063324, 0.9975094795227051, 0.9907670021057129, 0.9967135190963745, 0.06733161956071854, 0.9860105514526367]], "normalized_score": [0.6633337736129761], "tokens": [["\u2581pe", "pt", "id", "og", "ly", "c", "ans"]], "entropy": [[0.8740091323852539, 0.02537619136273861, 0.02655213698744774, 0.08930465579032898, 0.029329977929592133, 0.281680703163147, 0.10935501009225845]], "avg_entropy": [0.20508682967296668], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Peptidoglycan", "score_of_biggest_cluster": 0.6633337736129761, "score_of_first_answer": 0.6633337736129761, "max_score_of_first_answer": 0.997672975063324, "min_score_of_first_answer": 0.06733161956071854, "avg_score": 0.6633337736129761, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.31980072724830144, "std_of_first_answer": 0.31980072724830144, "avg_entropy_of_first_answer": 0.20508682967296668, "lex_sim": 1.0}
{"question": "what is the 180 degree line of longitude called", "answer": ["the meridian"], "scores": [[0.1885419636964798, 0.02183082513511181, 0.9956644177436829, 0.9651504755020142]], "normalized_score": [0.2507821023464203], "tokens": [["\u2581the", "\u2581mer", "id", "ian"]], "entropy": [[2.648648977279663, 2.1689796447753906, 0.03935055807232857, 0.20912742614746094]], "avg_entropy": [1.2665266515687108], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "180th meridian or antimeridian", "score_of_biggest_cluster": 0.2507821023464203, "score_of_first_answer": 0.2507821023464203, "max_score_of_first_answer": 0.9956644177436829, "min_score_of_first_answer": 0.02183082513511181, "avg_score": 0.2507821023464203, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4416938256366615, "std_of_first_answer": 0.4416938256366615, "avg_entropy_of_first_answer": 1.2665266515687108, "lex_sim": 1.0}
{"question": "who wrote the song if i were a boy", "answer": ["Christopher \u201cTricky\u201d Stewart"], "scores": [[0.29470688104629517, 0.02835983783006668, 0.9753949046134949, 0.9999727010726929, 0.9953476786613464, 0.9987449645996094]], "normalized_score": [0.4481760859489441], "tokens": [["\u2581Christopher", "\u2581\u201c", "Tr", "icky", "\u201d", "\u2581Stewart"]], "entropy": [[2.8165926933288574, 2.9286391735076904, 0.17001166939735413, 0.000382303143851459, 0.039739154279232025, 0.013007089495658875]], "avg_entropy": [0.9947286805254407], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "BC Jean and Toby Gad", "score_of_biggest_cluster": 0.4481760859489441, "score_of_first_answer": 0.4481760859489441, "max_score_of_first_answer": 0.9999727010726929, "min_score_of_first_answer": 0.02835983783006668, "avg_score": 0.4481760859489441, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3992160915375904, "std_of_first_answer": 0.3992160915375904, "avg_entropy_of_first_answer": 0.9947286805254407, "lex_sim": 1.0}
{"question": "who was the original host of jokers wild", "answer": ["Bill Cullen"], "scores": [[0.07104882597923279, 0.9886066317558289, 0.9986214637756348, 0.9954198598861694]], "normalized_score": [0.514039933681488], "tokens": [["\u2581Bill", "\u2581C", "ull", "en"]], "entropy": [[2.573803424835205, 0.11484655737876892, 0.014344019815325737, 0.04240952804684639]], "avg_entropy": [0.6863508825190365], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jack Barry", "score_of_biggest_cluster": 0.514039933681488, "score_of_first_answer": 0.514039933681488, "max_score_of_first_answer": 0.9986214637756348, "min_score_of_first_answer": 0.07104882597923279, "avg_score": 0.514039933681488, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3997594668095524, "std_of_first_answer": 0.3997594668095524, "avg_entropy_of_first_answer": 0.6863508825190365, "lex_sim": 1.0}
{"question": "what has been the origin of most classical dances of india", "answer": ["temple"], "scores": [[0.17309987545013428]], "normalized_score": [0.17309986054897308], "tokens": [["\u2581temple"]], "entropy": [[4.097890853881836]], "avg_entropy": [4.097890853881836], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Sanskrit text Natya Shastra", "score_of_biggest_cluster": 0.17309986054897308, "score_of_first_answer": 0.17309986054897308, "max_score_of_first_answer": 0.17309987545013428, "min_score_of_first_answer": 0.17309987545013428, "avg_score": 0.17309986054897308, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.097890853881836, "lex_sim": 1.0}
{"question": "who is directly elected according to the constitution", "answer": ["U.S. President"], "scores": [[0.006647446658462286, 0.9208982586860657, 0.9703585505485535, 0.9575881361961365, 0.6071085333824158]], "normalized_score": [0.32184648513793945], "tokens": [["\u2581U", ".", "S", ".", "\u2581President"]], "entropy": [[2.968252182006836, 0.46174079179763794, 0.15109331905841827, 0.23922841250896454, 1.539211630821228]], "avg_entropy": [1.071905267238617], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "senators", "score_of_biggest_cluster": 0.32184648513793945, "score_of_first_answer": 0.32184648513793945, "max_score_of_first_answer": 0.9703585505485535, "min_score_of_first_answer": 0.006647446658462286, "avg_score": 0.32184648513793945, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36805659458695483, "std_of_first_answer": 0.36805659458695483, "avg_entropy_of_first_answer": 1.071905267238617, "lex_sim": 1.0}
{"question": "who plays colin on young and the restless", "answer": ["Scott Grainger"], "scores": [[0.49766963720321655, 0.1299149990081787, 0.9645217657089233]], "normalized_score": [0.39655566215515137], "tokens": [["\u2581Scott", "\u2581Gra", "inger"]], "entropy": [[2.8474109172821045, 1.2307734489440918, 0.20676349103450775]], "avg_entropy": [1.4283159524202347], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tristan Rogers", "score_of_biggest_cluster": 0.39655566215515137, "score_of_first_answer": 0.39655566215515137, "max_score_of_first_answer": 0.9645217657089233, "min_score_of_first_answer": 0.1299149990081787, "avg_score": 0.39655566215515137, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34152644908588836, "std_of_first_answer": 0.34152644908588836, "avg_entropy_of_first_answer": 1.4283159524202347, "lex_sim": 1.0}
{"question": "when was the first underwater cable laid beneath the english channel", "answer": ["1851"], "scores": [[0.5688123106956482, 0.9159629940986633, 0.840634286403656, 0.8884044289588928, 0.6771852970123291]], "normalized_score": [0.7658689618110657], "tokens": [["\u2581", "1", "8", "5", "1"]], "entropy": [[1.8390531539916992, 0.3940448462963104, 0.5540904402732849, 0.5652937889099121, 0.7717502117156982]], "avg_entropy": [0.8248464882373809], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1850", "score_of_biggest_cluster": 0.7658689618110657, "score_of_first_answer": 0.7658689618110657, "max_score_of_first_answer": 0.9159629940986633, "min_score_of_first_answer": 0.5688123106956482, "avg_score": 0.7658689618110657, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13346825351050912, "std_of_first_answer": 0.13346825351050912, "avg_entropy_of_first_answer": 0.8248464882373809, "lex_sim": 1.0}
{"question": "the tenure of the office of the lokayukt of up", "answer": ["Lokayukta is the head of Lokayukta institutions."], "scores": [[0.002124364022165537, 0.8682816624641418, 0.8822061419487, 0.9965914487838745, 0.13258908689022064, 0.12156099826097488, 0.3263415992259979, 0.9747036099433899, 0.30108213424682617, 0.8313933610916138, 0.8540711402893066, 0.9982442855834961, 0.005456048995256424, 0.2978804111480713, 0.007782576605677605, 0.05917245149612427, 0.03023805283010006, 0.9992369413375854, 0.9862713813781738, 0.9289103150367737, 0.534221351146698, 0.08617190271615982, 0.5345408916473389, 0.40076586604118347, 0.6741947531700134, 0.9987142086029053, 0.9978177547454834, 0.951012372970581, 0.8093007802963257, 0.04875098168849945, 0.9514661431312561, 0.0205747801810503, 0.012102986685931683, 0.40161460638046265, 0.4463546872138977, 0.9995237588882446, 0.14009392261505127, 0.9999613761901855, 0.050407011061906815, 0.45926588773727417, 0.9888764023780823, 0.9994804263114929, 0.02333049476146698, 0.062451690435409546, 0.18420088291168213, 0.10984240472316742, 0.11687471717596054, 0.918883740901947, 0.9988378882408142, 0.998164713382721]], "normalized_score": [0.2581535577774048], "tokens": [["\u2581Lok", "ay", "uk", "ta", "\u2581is", "\u2581the", "\u2581head", "\u2581of", "\u2581Lok", "ay", "uk", "ta", "\u2581institutions", ".", "\u2581As", "\u2581the", "\u2581om", "bud", "s", "man", ",", "\u2581they", "\u2581address", "\u2581the", "\u2581gr", "iev", "ances", "\u2581of", "\u2581the", "\u2581general", "\u2581public", "\u2581by", "\u2581direct", "\u2581and", "\u2581speed", "y", "\u2581red", "ress", "\u2581of", "\u2581their", "\u2581gr", "iev", "ance", ",", "\u2581through", "\u2581a", "\u2581quasi", "-", "jud", "icial"]], "entropy": [[2.2908270359039307, 0.6302103996276855, 0.4659607410430908, 0.035600725561380386, 3.1134450435638428, 2.2714786529541016, 3.4816505908966064, 0.21310776472091675, 1.6099324226379395, 0.6562985181808472, 0.49907755851745605, 0.016075195744633675, 2.6179611682891846, 2.3623597621917725, 3.1797232627868652, 1.9786648750305176, 3.805330276489258, 0.009160426445305347, 0.07758587598800659, 0.2625153362751007, 2.068474054336548, 2.3421342372894287, 1.9585306644439697, 1.8019472360610962, 1.2990795373916626, 0.012258227914571762, 0.015865692868828773, 0.3600844144821167, 0.7434056997299194, 0.5709500312805176, 0.2895256280899048, 1.9775879383087158, 2.6134862899780273, 1.4533426761627197, 2.064728260040283, 0.004687121137976646, 3.68406343460083, 0.00048243545461446047, 0.7589239478111267, 1.9327703714370728, 0.08589288592338562, 0.005291765555739403, 0.1109171137213707, 1.576019525527954, 3.1385531425476074, 3.2763671875, 4.094579219818115, 0.3198573887348175, 0.010382233187556267, 0.014129976741969585]], "avg_entropy": [1.3632256794185378], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "eight years", "score_of_biggest_cluster": 0.2581535577774048, "score_of_first_answer": 0.2581535577774048, "max_score_of_first_answer": 0.9999613761901855, "min_score_of_first_answer": 0.002124364022165537, "avg_score": 0.2581535577774048, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4026769836979993, "std_of_first_answer": 0.4026769836979993, "avg_entropy_of_first_answer": 1.3632256794185378, "lex_sim": 1.0}
{"question": "how long did the democrats control the house and senate", "answer": ["March 2007 \u2013 January 1995"], "scores": [[0.0026459340006113052, 0.9908199906349182, 0.20116758346557617, 0.4402729272842407, 0.4932694733142853, 0.4277121424674988, 0.4494401514530182, 0.6961043477058411, 0.9990426898002625, 0.013333907350897789, 0.7439775466918945, 0.8543547987937927, 0.9443695545196533]], "normalized_score": [0.29398971796035767], "tokens": [["\u2581March", "\u2581", "2", "0", "0", "7", "\u2581\u2013", "\u2581January", "\u2581", "1", "9", "9", "5"]], "entropy": [[2.3641815185546875, 0.07169593870639801, 1.6396958827972412, 1.9487371444702148, 1.1164886951446533, 1.4252175092697144, 1.6708810329437256, 1.3806071281433105, 0.010010110214352608, 0.3488348126411438, 1.0847212076187134, 0.579743504524231, 0.30123937129974365]], "avg_entropy": [1.07246568125601], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "57 yrs", "score_of_biggest_cluster": 0.29398971796035767, "score_of_first_answer": 0.29398971796035767, "max_score_of_first_answer": 0.9990426898002625, "min_score_of_first_answer": 0.0026459340006113052, "avg_score": 0.29398971796035767, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33418533636224423, "std_of_first_answer": 0.33418533636224423, "avg_entropy_of_first_answer": 1.07246568125601, "lex_sim": 1.0}
{"question": "who won big brother head of household canada", "answer": ["Ivette Rodriguez,"], "scores": [[0.004003923386335373, 0.09332098066806793, 0.8893083333969116, 0.05025731772184372, 0.961371898651123, 0.9912102222442627, 0.0024971547536551952]], "normalized_score": [0.08764872699975967], "tokens": [["\u2581I", "v", "ette", "\u2581Rod", "rig", "uez", ","]], "entropy": [[5.3177595138549805, 3.4402737617492676, 0.43486106395721436, 3.929997205734253, 0.2353808432817459, 0.06398817896842957, 0.5475720763206482]], "avg_entropy": [1.99569037769522], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kaela", "score_of_biggest_cluster": 0.08764872699975967, "score_of_first_answer": 0.08764872699975967, "max_score_of_first_answer": 0.9912102222442627, "min_score_of_first_answer": 0.0024971547536551952, "avg_score": 0.08764872699975967, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45198282507530796, "std_of_first_answer": 0.45198282507530796, "avg_entropy_of_first_answer": 1.99569037769522, "lex_sim": 1.0}
{"question": "real name of daya in taarak mehta ka ooltah chashmah", "answer": ["Malavika"], "scores": [[0.003870433196425438, 0.5738807916641235, 0.5199399590492249]], "normalized_score": [0.10491674393415451], "tokens": [["\u2581Mal", "av", "ika"]], "entropy": [[2.959557056427002, 1.5145399570465088, 2.230691909790039]], "avg_entropy": [2.23492964108785], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Disha Vakani", "score_of_biggest_cluster": 0.10491674393415451, "score_of_first_answer": 0.10491674393415451, "max_score_of_first_answer": 0.5738807916641235, "min_score_of_first_answer": 0.003870433196425438, "avg_score": 0.10491674393415451, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2569369091814814, "std_of_first_answer": 0.2569369091814814, "avg_entropy_of_first_answer": 2.23492964108785, "lex_sim": 1.0}
{"question": "who wrote the text for jeanie with the light brown hair", "answer": ["Stephen Foster"], "scores": [[0.9258139133453369, 0.9202779531478882, 0.9998816251754761]], "normalized_score": [0.9479754567146301], "tokens": [["\u2581Stephen", "\u2581F", "oster"]], "entropy": [[0.5127514600753784, 0.43877458572387695, 0.0014015857595950365]], "avg_entropy": [0.31764254385295015], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Stephen Foster", "score_of_biggest_cluster": 0.9479754567146301, "score_of_first_answer": 0.9479754567146301, "max_score_of_first_answer": 0.9998816251754761, "min_score_of_first_answer": 0.9202779531478882, "avg_score": 0.9479754567146301, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03629113358404663, "std_of_first_answer": 0.03629113358404663, "avg_entropy_of_first_answer": 0.31764254385295015, "lex_sim": 1.0}
{"question": "what level is a city and guilds qualification", "answer": ["7-10 year experience"], "scores": [[0.20749181509017944, 0.1784912645816803, 0.009696436114609241, 0.1312960386276245, 0.4355255961418152, 0.00046761578414589167, 0.09502717107534409]], "normalized_score": [0.05112163722515106], "tokens": [["\u2581", "7", "-", "1", "0", "\u2581year", "\u2581experience"]], "entropy": [[2.056413173675537, 1.9869858026504517, 0.9200499057769775, 2.1224865913391113, 1.8091458082199097, 0.7750948667526245, 2.84476900100708]], "avg_entropy": [1.7878493070602417], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "entry level to level 7", "score_of_biggest_cluster": 0.05112163722515106, "score_of_first_answer": 0.05112163722515106, "max_score_of_first_answer": 0.4355255961418152, "min_score_of_first_answer": 0.00046761578414589167, "avg_score": 0.05112163722515106, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13682609791660527, "std_of_first_answer": 0.13682609791660527, "avg_entropy_of_first_answer": 1.7878493070602417, "lex_sim": 1.0}
{"question": "who wrote the music for christmas story live", "answer": ["Alan Silvestri"], "scores": [[0.0934537798166275, 0.2896302044391632, 0.9995088577270508, 0.9963447451591492]], "normalized_score": [0.40519049763679504], "tokens": [["\u2581Alan", "\u2581Sil", "vest", "ri"]], "entropy": [[4.1813249588012695, 0.728773832321167, 0.005667449906468391, 0.031305521726608276]], "avg_entropy": [1.2367679406888783], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pasek and Paul", "score_of_biggest_cluster": 0.40519049763679504, "score_of_first_answer": 0.40519049763679504, "max_score_of_first_answer": 0.9995088577270508, "min_score_of_first_answer": 0.0934537798166275, "avg_score": 0.40519049763679504, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4091161391607922, "std_of_first_answer": 0.4091161391607922, "avg_entropy_of_first_answer": 1.2367679406888783, "lex_sim": 1.0}
{"question": "who controlled the house and the senate in 2012", "answer": ["Democrats"], "scores": [[0.2292861044406891, 0.998724639415741]], "normalized_score": [0.4785328507423401], "tokens": [["\u2581Democr", "ats"]], "entropy": [[2.593574047088623, 0.013073338195681572]], "avg_entropy": [1.3033236926421523], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Republican", "score_of_biggest_cluster": 0.4785328507423401, "score_of_first_answer": 0.4785328507423401, "max_score_of_first_answer": 0.998724639415741, "min_score_of_first_answer": 0.2292861044406891, "avg_score": 0.4785328507423401, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38471926748752594, "std_of_first_answer": 0.38471926748752594, "avg_entropy_of_first_answer": 1.3033236926421523, "lex_sim": 1.0}
{"question": "which approach to psychology focuses on the body especially the brain and nervous system", "answer": ["neuroscience"], "scores": [[0.05161681026220322, 0.9971020817756653, 0.9750652313232422]], "normalized_score": [0.368854284286499], "tokens": [["\u2581neu", "ros", "cience"]], "entropy": [[2.485830068588257, 0.02689543180167675, 0.15078401565551758]], "avg_entropy": [0.8878365053484837], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "neuropsychology", "score_of_biggest_cluster": 0.368854284286499, "score_of_first_answer": 0.368854284286499, "max_score_of_first_answer": 0.9971020817756653, "min_score_of_first_answer": 0.05161681026220322, "avg_score": 0.368854284286499, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44060375342463304, "std_of_first_answer": 0.44060375342463304, "avg_entropy_of_first_answer": 0.8878365053484837, "lex_sim": 1.0}
{"question": "what was the first light beer in the us", "answer": ["Heineken"], "scores": [[0.009724719449877739, 0.9128264784812927, 0.9947994947433472]], "normalized_score": [0.20669671893119812], "tokens": [["\u2581He", "ine", "ken"]], "entropy": [[3.2746009826660156, 0.6343922019004822, 0.04380681365728378]], "avg_entropy": [1.3175999994079273], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Miller Lite", "score_of_biggest_cluster": 0.20669671893119812, "score_of_first_answer": 0.20669671893119812, "max_score_of_first_answer": 0.9947994947433472, "min_score_of_first_answer": 0.009724719449877739, "avg_score": 0.20669671893119812, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44630391681506476, "std_of_first_answer": 0.44630391681506476, "avg_entropy_of_first_answer": 1.3175999994079273, "lex_sim": 1.0}
{"question": "who has been chosen as the brand ambassador of the campaign 'beti bachao-beti padhao", "answer": ["Madhuri Dixit"], "scores": [[0.16826090216636658, 0.9982932209968567, 0.9800381064414978, 0.9903469681739807, 0.997911274433136, 0.9984984397888184]], "normalized_score": [0.7386723160743713], "tokens": [["\u2581Mad", "h", "uri", "\u2581D", "ix", "it"]], "entropy": [[3.3354530334472656, 0.016683652997016907, 0.13209591805934906, 0.07478824257850647, 0.019300783053040504, 0.015568722039461136]], "avg_entropy": [0.59898172536244], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sakshi Malik", "score_of_biggest_cluster": 0.7386723160743713, "score_of_first_answer": 0.7386723160743713, "max_score_of_first_answer": 0.9984984397888184, "min_score_of_first_answer": 0.16826090216636658, "avg_score": 0.7386723160743713, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.307438457503657, "std_of_first_answer": 0.307438457503657, "avg_entropy_of_first_answer": 0.59898172536244, "lex_sim": 1.0}
{"question": "who was the declaration of independence written for", "answer": ["united states"], "scores": [[0.001072272309102118, 0.9805682897567749, 0.9398931860923767]], "normalized_score": [0.09960640221834183], "tokens": [["\u2581un", "ited", "\u2581states"]], "entropy": [[2.883565902709961, 0.1604648381471634, 0.3697693347930908]], "avg_entropy": [1.1379333585500717], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Second Continental Congress", "score_of_biggest_cluster": 0.09960640221834183, "score_of_first_answer": 0.09960640221834183, "max_score_of_first_answer": 0.9805682897567749, "min_score_of_first_answer": 0.001072272309102118, "avg_score": 0.09960640221834183, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4524564583607066, "std_of_first_answer": 0.4524564583607066, "avg_entropy_of_first_answer": 1.1379333585500717, "lex_sim": 1.0}
{"question": "who was the first president of the constituent assembly", "answer": ["Dr. Rajendra Prasad"], "scores": [[0.28703388571739197, 0.8348278999328613, 0.8526835441589355, 0.9944844841957092, 0.9948896169662476, 0.9971689581871033, 0.9985542893409729]], "normalized_score": [0.7953299283981323], "tokens": [["\u2581Dr", ".", "\u2581Raj", "endra", "\u2581Pr", "as", "ad"]], "entropy": [[2.6054019927978516, 0.6112660765647888, 0.8854432106018066, 0.04434850811958313, 0.04545480012893677, 0.023474346846342087, 0.015227429568767548]], "avg_entropy": [0.6043737663754395], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Dr. Rajendra Prasad", "score_of_biggest_cluster": 0.7953299283981323, "score_of_first_answer": 0.7953299283981323, "max_score_of_first_answer": 0.9985542893409729, "min_score_of_first_answer": 0.28703388571739197, "avg_score": 0.7953299283981323, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23986627323157844, "std_of_first_answer": 0.23986627323157844, "avg_entropy_of_first_answer": 0.6043737663754395, "lex_sim": 1.0}
{"question": "when did the revolution begin in the united states", "answer": ["1775, Boston, MA"], "scores": [[0.27863046526908875, 0.9622669219970703, 0.9096828699111938, 0.7874844670295715, 0.6422775387763977, 0.010332314297556877, 0.035463254898786545, 0.3441315293312073, 0.06179242953658104]], "normalized_score": [0.21449829638004303], "tokens": [["\u2581", "1", "7", "7", "5", ",", "\u2581Boston", ",", "\u2581MA"]], "entropy": [[1.7015931606292725, 0.21328943967819214, 0.4114995002746582, 0.6581528186798096, 0.8724135160446167, 0.7351264357566833, 3.2645649909973145, 1.763533115386963, 0.74773770570755]], "avg_entropy": [1.1519900759061177], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1775", "score_of_biggest_cluster": 0.21449829638004303, "score_of_first_answer": 0.21449829638004303, "max_score_of_first_answer": 0.9622669219970703, "min_score_of_first_answer": 0.010332314297556877, "avg_score": 0.21449829638004303, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36224386500088873, "std_of_first_answer": 0.36224386500088873, "avg_entropy_of_first_answer": 1.1519900759061177, "lex_sim": 1.0}
{"question": "when is the new season of chicago pd", "answer": ["September 4"], "scores": [[0.07969723641872406, 0.9741083383560181, 0.0034703691489994526]], "normalized_score": [0.06458655744791031], "tokens": [["\u2581September", "\u2581", "4"]], "entropy": [[3.2785253524780273, 0.17027567327022552, 0.6669794917106628]], "avg_entropy": [1.371926839152972], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "September 27, 2017", "score_of_biggest_cluster": 0.06458655744791031, "score_of_first_answer": 0.06458655744791031, "max_score_of_first_answer": 0.9741083383560181, "min_score_of_first_answer": 0.0034703691489994526, "avg_score": 0.06458655744791031, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4406963961011362, "std_of_first_answer": 0.4406963961011362, "avg_entropy_of_first_answer": 1.371926839152972, "lex_sim": 1.0}
{"question": "when was the last year the eagles went to the superbowl", "answer": ["They first won their Superbowl in 1960, beating the Green Bay Packers 13-18, despite being huge underdogs"], "scores": [[0.012008131481707096, 0.001208895817399025, 0.04932248964905739, 0.022376833483576775, 0.2686038315296173, 0.018235523253679276, 0.989709198474884, 0.5717307925224304, 0.8494572043418884, 0.7861427664756775, 0.9985252022743225, 0.25581681728363037, 0.9278812408447266, 0.19328075647354126, 0.0436251163482666, 0.9988917708396912, 0.6963199973106384, 0.7758517861366272, 0.9479209780693054, 0.983997106552124, 0.9974392652511597, 0.4271797835826874, 0.9674599170684814, 0.0013659674441441894, 0.6497243046760559, 0.6440805196762085, 0.005645842757076025, 0.06146132946014404, 0.002181778661906719, 0.3462393879890442, 0.01168864220380783, 0.9391406178474426, 0.9884883761405945, 0.9998227953910828]], "normalized_score": [0.17544516921043396], "tokens": [["\u2581They", "\u2581first", "\u2581won", "\u2581their", "\u2581Super", "bow", "l", "\u2581in", "\u2581", "1", "9", "6", "0", ",", "\u2581be", "ating", "\u2581the", "\u2581Green", "\u2581Bay", "\u2581Pack", "ers", "\u2581", "1", "3", "-", "1", "8", ",", "\u2581despite", "\u2581being", "\u2581huge", "\u2581under", "d", "ogs"]], "entropy": [[1.7204227447509766, 2.35825252532959, 2.0289618968963623, 1.1852105855941772, 2.99613094329834, 0.1590513288974762, 0.06346415728330612, 2.1160566806793213, 0.6903775930404663, 0.5542016625404358, 0.015285029076039791, 1.296095371246338, 0.344751238822937, 2.1425352096557617, 3.544161796569824, 0.012145793996751308, 1.1633753776550293, 1.1832853555679321, 0.2763289511203766, 0.1211126446723938, 0.022142348811030388, 1.8482098579406738, 0.19677825272083282, 0.3801993429660797, 1.3353826999664307, 1.4171593189239502, 1.5117099285125732, 1.5979363918304443, 3.4851608276367188, 3.2919070720672607, 3.445239543914795, 0.36777186393737793, 0.07893923670053482, 0.002368427347391844]], "avg_entropy": [1.2632974117638214], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "following the 2017 season", "score_of_biggest_cluster": 0.17544516921043396, "score_of_first_answer": 0.17544516921043396, "max_score_of_first_answer": 0.9998227953910828, "min_score_of_first_answer": 0.001208895817399025, "avg_score": 0.17544516921043396, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40693405333317345, "std_of_first_answer": 0.40693405333317345, "avg_entropy_of_first_answer": 1.2632974117638214, "lex_sim": 1.0}
{"question": "what does the air gap do on a dishwasher", "answer": ["Air gap is a physical device, designed to remove minerals at the last step of water filtration, that prevents hard water scale/lime build-up in plumbing fixtures and appliances by reducing the potential for water"], "scores": [[0.06644267588853836, 0.4243839383125305, 0.298082172870636, 0.44293463230133057, 0.029387066140770912, 0.05286582186818123, 0.0722544938325882, 0.02607950009405613, 0.8617316484451294, 0.006144682876765728, 0.0081288181245327, 0.6896575689315796, 0.9984787106513977, 0.00037184948450885713, 0.7269952893257141, 0.009871994145214558, 0.08833982795476913, 0.6707400679588318, 0.2929506301879883, 0.18211756646633148, 0.9996874332427979, 0.9984099864959717, 0.1215445026755333, 0.045266687870025635, 0.10472260415554047, 0.05471771955490112, 0.7939476370811462, 0.08002525568008423, 0.001974209677428007, 0.12169810384511948, 0.2732386589050293, 0.6473991870880127, 0.9887210726737976, 0.3347734212875366, 0.07231319695711136, 0.9950555562973022, 0.9990724325180054, 0.12983940541744232, 0.9962376356124878, 0.9998089671134949, 0.37034013867378235, 0.39301401376724243, 0.9830545783042908, 0.9999412298202515, 0.052228283137083054, 0.03786369413137436, 0.5267706513404846, 0.008526083081960678, 0.7483622431755066, 0.15018697082996368]], "normalized_score": [0.15854518115520477], "tokens": [["\u2581Air", "\u2581gap", "\u2581is", "\u2581a", "\u2581physical", "\u2581device", ",", "\u2581designed", "\u2581to", "\u2581remove", "\u2581min", "er", "als", "\u2581at", "\u2581the", "\u2581last", "\u2581step", "\u2581of", "\u2581water", "\u2581fil", "tr", "ation", ",", "\u2581that", "\u2581prevents", "\u2581hard", "\u2581water", "\u2581scale", "/", "lime", "\u2581build", "-", "up", "\u2581in", "\u2581pl", "umb", "ing", "\u2581fi", "xt", "ures", "\u2581and", "\u2581app", "li", "ances", "\u2581by", "\u2581reducing", "\u2581the", "\u2581potential", "\u2581for", "\u2581water"]], "entropy": [[4.632625579833984, 1.638140320777893, 3.834792137145996, 2.4925270080566406, 4.355082988739014, 3.059816360473633, 3.0033063888549805, 3.6925017833709717, 0.7796173095703125, 3.035402774810791, 4.088865280151367, 0.6497417688369751, 0.0177994966506958, 1.8619379997253418, 1.6629490852355957, 3.3194003105163574, 2.4437179565429688, 1.278792381286621, 2.110194206237793, 1.8635766506195068, 0.0038823105860501528, 0.01591104082763195, 2.2097344398498535, 3.975341796875, 4.377780437469482, 4.158238410949707, 1.053060531616211, 2.940856456756592, 2.204281806945801, 3.1661429405212402, 2.123647928237915, 0.7388076186180115, 0.07031133770942688, 2.377589702606201, 3.0250606536865234, 0.03951365873217583, 0.00960567593574524, 2.6396493911743164, 0.026122841984033585, 0.0025292483624070883, 1.991539478302002, 3.144697666168213, 0.08847730606794357, 0.0008049030438996851, 2.019819736480713, 4.410726070404053, 2.314542770385742, 3.4244585037231445, 1.1962993144989014, 3.7769484519958496]], "avg_entropy": [2.1469434043590443], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "backflow prevention", "score_of_biggest_cluster": 0.15854518115520477, "score_of_first_answer": 0.15854518115520477, "max_score_of_first_answer": 0.9999412298202515, "min_score_of_first_answer": 0.00037184948450885713, "avg_score": 0.15854518115520477, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3796048867307761, "std_of_first_answer": 0.3796048867307761, "avg_entropy_of_first_answer": 2.1469434043590443, "lex_sim": 1.0}
{"question": "who played ashley on the young and the restless", "answer": ["Eileen Davidson"], "scores": [[0.6224305629730225, 0.9897531270980835, 0.9999809265136719, 0.9643093347549438, 0.9986411929130554]], "normalized_score": [0.9008387923240662], "tokens": [["\u2581E", "ile", "en", "\u2581David", "son"]], "entropy": [[1.9588117599487305, 0.07432015985250473, 0.0003161299682687968, 0.25749698281288147, 0.013650073669850826]], "avg_entropy": [0.46091902125044726], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brenda Epperson", "score_of_biggest_cluster": 0.9008387923240662, "score_of_first_answer": 0.9008387923240662, "max_score_of_first_answer": 0.9999809265136719, "min_score_of_first_answer": 0.6224305629730225, "avg_score": 0.9008387923240662, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1468563382075157, "std_of_first_answer": 0.1468563382075157, "avg_entropy_of_first_answer": 0.46091902125044726, "lex_sim": 1.0}
{"question": "baseball teams who have not won a world series", "answer": ["Seattle Mariners, San Diego Padres, Texas Rangers, Tampa Bay Rays, Washington Nationals, and five other teams"], "scores": [[0.11918831616640091, 0.9884037375450134, 0.9986518025398254, 0.9998795986175537, 0.25623250007629395, 0.09254658222198486, 0.9915916919708252, 0.996260404586792, 0.9998199343681335, 0.8842753171920776, 0.41884109377861023, 0.9974990487098694, 0.9988365769386292, 0.8556714057922363, 0.11011693626642227, 0.998422384262085, 0.9933532476425171, 0.9224857091903687, 0.9989067316055298, 0.859133243560791, 0.40751898288726807, 0.9897582530975342, 0.9993300437927246, 0.70078045129776, 0.032763976603746414, 0.0014224780024960637, 0.2037971168756485, 0.7420058250427246]], "normalized_score": [0.4457784593105316], "tokens": [["\u2581Seattle", "\u2581Mar", "in", "ers", ",", "\u2581San", "\u2581Diego", "\u2581Pad", "res", ",", "\u2581Texas", "\u2581R", "angers", ",", "\u2581T", "ampa", "\u2581Bay", "\u2581R", "ays", ",", "\u2581Washington", "\u2581National", "s", ",", "\u2581and", "\u2581five", "\u2581other", "\u2581teams"]], "entropy": [[3.5220258235931396, 0.09911999106407166, 0.012667208909988403, 0.0016522440128028393, 1.1697328090667725, 2.1611714363098145, 0.056997109204530716, 0.03467581421136856, 0.002323923399671912, 0.4706381559371948, 2.120185613632202, 0.024832412600517273, 0.010304262861609459, 0.562633752822876, 2.1493418216705322, 0.01601438596844673, 0.05310136824846268, 0.3981296718120575, 0.01187165454030037, 0.5530463457107544, 1.953275442123413, 0.08241705596446991, 0.006955109536647797, 1.0234010219573975, 2.5087730884552, 2.568830728530884, 1.9406991004943848, 1.474871277809143]], "avg_entropy": [0.8924888796588805], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Washington Nationals", "score_of_biggest_cluster": 0.4457784593105316, "score_of_first_answer": 0.4457784593105316, "max_score_of_first_answer": 0.9998795986175537, "min_score_of_first_answer": 0.0014224780024960637, "avg_score": 0.4457784593105316, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3718095456205607, "std_of_first_answer": 0.3718095456205607, "avg_entropy_of_first_answer": 0.8924888796588805, "lex_sim": 1.0}
{"question": "who dies in akame ga kill night raid", "answer": ["Wave"], "scores": [[0.01893986389040947, 0.5185640454292297]], "normalized_score": [0.0991036519408226], "tokens": [["\u2581W", "ave"]], "entropy": [[5.119224548339844, 2.7360432147979736]], "avg_entropy": [3.9276338815689087], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tatsumi", "score_of_biggest_cluster": 0.0991036519408226, "score_of_first_answer": 0.0991036519408226, "max_score_of_first_answer": 0.5185640454292297, "min_score_of_first_answer": 0.01893986389040947, "avg_score": 0.0991036519408226, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24981209076941013, "std_of_first_answer": 0.24981209076941013, "avg_entropy_of_first_answer": 3.9276338815689087, "lex_sim": 1.0}
{"question": "where are trigger points located in the body", "answer": ["in muscles"], "scores": [[0.12574420869350433, 0.22562885284423828, 0.9123010039329529]], "normalized_score": [0.295805960893631], "tokens": [["\u2581in", "\u2581mus", "cles"]], "entropy": [[4.2771759033203125, 2.5287463665008545, 0.3114703595638275]], "avg_entropy": [2.372464209794998], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "muscles", "score_of_biggest_cluster": 0.295805960893631, "score_of_first_answer": 0.295805960893631, "max_score_of_first_answer": 0.9123010039329529, "min_score_of_first_answer": 0.12574420869350433, "avg_score": 0.295805960893631, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.34962951460641767, "std_of_first_answer": 0.34962951460641767, "avg_entropy_of_first_answer": 2.372464209794998, "lex_sim": 1.0}
{"question": "who started the state of the union address", "answer": ["The State of the Union address"], "scores": [[0.026761291548609734, 0.06761674582958221, 0.9724316596984863, 0.9814506769180298, 0.9936255216598511, 0.4910832345485687]], "normalized_score": [0.30733436346054077], "tokens": [["\u2581The", "\u2581State", "\u2581of", "\u2581the", "\u2581Union", "\u2581address"]], "entropy": [[2.031308174133301, 3.6198911666870117, 0.1747129112482071, 0.11398512125015259, 0.053318433463573456, 1.383596658706665]], "avg_entropy": [1.2294687442481518], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Woodrow Wilson", "score_of_biggest_cluster": 0.30733436346054077, "score_of_first_answer": 0.30733436346054077, "max_score_of_first_answer": 0.9936255216598511, "min_score_of_first_answer": 0.026761291548609734, "avg_score": 0.30733436346054077, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42077301147264107, "std_of_first_answer": 0.42077301147264107, "avg_entropy_of_first_answer": 1.2294687442481518, "lex_sim": 1.0}
{"question": "when did the second it movie come out", "answer": ["September 6, 2019"], "scores": [[0.7681978940963745, 0.9948451519012451, 0.5532892346382141, 0.9536411166191101, 0.9987987279891968, 0.9987282156944275, 0.999930739402771, 0.9997981190681458, 0.9678667783737183]], "normalized_score": [0.9004620909690857], "tokens": [["\u2581September", "\u2581", "6", ",", "\u2581", "2", "0", "1", "9"]], "entropy": [[1.2343919277191162, 0.04163268953561783, 1.2331604957580566, 0.2374878227710724, 0.011196528561413288, 0.009929495863616467, 0.0009140463080257177, 0.002227198099717498, 0.15686018764972687]], "avg_entropy": [0.32531115469626254], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "September 6, 2019", "score_of_biggest_cluster": 0.9004620909690857, "score_of_first_answer": 0.9004620909690857, "max_score_of_first_answer": 0.999930739402771, "min_score_of_first_answer": 0.5532892346382141, "avg_score": 0.9004620909690857, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.14586348560407025, "std_of_first_answer": 0.14586348560407025, "avg_entropy_of_first_answer": 0.32531115469626254, "lex_sim": 1.0}
{"question": "when was the last time vancouver had an earthquake", "answer": ["1828"], "scores": [[0.24972441792488098, 0.5136682987213135, 0.0935971811413765, 0.024387910962104797, 0.5892635583877563]], "normalized_score": [0.1767576038837433], "tokens": [["\u2581", "1", "8", "2", "8"]], "entropy": [[2.855553150177002, 1.2953767776489258, 1.2574940919876099, 2.847529888153076, 1.8217027187347412]], "avg_entropy": [2.015531325340271], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1946", "score_of_biggest_cluster": 0.1767576038837433, "score_of_first_answer": 0.1767576038837433, "max_score_of_first_answer": 0.5892635583877563, "min_score_of_first_answer": 0.024387910962104797, "avg_score": 0.1767576038837433, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2237172070425098, "std_of_first_answer": 0.2237172070425098, "avg_entropy_of_first_answer": 2.015531325340271, "lex_sim": 1.0}
{"question": "where did the jungle book story come from", "answer": ["Rudyard Kipling"], "scores": [[0.47590625286102295, 0.9984325766563416, 0.9885785579681396, 0.997428834438324]], "normalized_score": [0.8273386359214783], "tokens": [["\u2581Rud", "yard", "\u2581Ki", "pling"]], "entropy": [[2.4920434951782227, 0.017804749310016632, 0.0945109874010086, 0.020242581143975258]], "avg_entropy": [0.6561504532583058], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "English author Rudyard Kipling", "score_of_biggest_cluster": 0.8273386359214783, "score_of_first_answer": 0.8273386359214783, "max_score_of_first_answer": 0.9984325766563416, "min_score_of_first_answer": 0.47590625286102295, "avg_score": 0.8273386359214783, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.22472606812994206, "std_of_first_answer": 0.22472606812994206, "avg_entropy_of_first_answer": 0.6561504532583058, "lex_sim": 1.0}
{"question": "who won the national championship in volleyball 2017", "answer": ["Minnesota Golden Gophers"], "scores": [[0.017741607502102852, 0.5077353715896606, 0.9931508302688599, 0.9963208436965942, 0.9998470544815063]], "normalized_score": [0.3890412747859955], "tokens": [["\u2581Minnesota", "\u2581Golden", "\u2581G", "oph", "ers"]], "entropy": [[3.1949706077575684, 1.3751020431518555, 0.06124436855316162, 0.02490345388650894, 0.0020472262986004353]], "avg_entropy": [0.9316535399295389], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ohio State", "score_of_biggest_cluster": 0.3890412747859955, "score_of_first_answer": 0.3890412747859955, "max_score_of_first_answer": 0.9998470544815063, "min_score_of_first_answer": 0.017741607502102852, "avg_score": 0.3890412747859955, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3914205223686885, "std_of_first_answer": 0.3914205223686885, "avg_entropy_of_first_answer": 0.9316535399295389, "lex_sim": 1.0}
{"question": "who conducted the opening concert at carnegie hall", "answer": ["Walter Damrosch"], "scores": [[0.36627867817878723, 0.9895987510681152, 0.9975237250328064, 0.9981021285057068]], "normalized_score": [0.7750723361968994], "tokens": [["\u2581Walter", "\u2581Dam", "ro", "sch"]], "entropy": [[3.543423652648926, 0.10274167358875275, 0.02713792398571968, 0.017757996916770935]], "avg_entropy": [0.9227653117850423], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pyotr Ilyich Tchaikovsky", "score_of_biggest_cluster": 0.7750723361968994, "score_of_first_answer": 0.7750723361968994, "max_score_of_first_answer": 0.9981021285057068, "min_score_of_first_answer": 0.36627867817878723, "avg_score": 0.7750723361968994, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27229746395466436, "std_of_first_answer": 0.27229746395466436, "avg_entropy_of_first_answer": 0.9227653117850423, "lex_sim": 1.0}
{"question": "where was a hologram for the king filmed", "answer": ["Morocco"], "scores": [[0.15102723240852356, 0.9946921467781067, 0.9972924590110779]], "normalized_score": [0.5311153531074524], "tokens": [["\u2581Mor", "oc", "co"]], "entropy": [[4.1865997314453125, 0.039490956813097, 0.023555820807814598]], "avg_entropy": [1.416548836355408], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Morocco", "score_of_biggest_cluster": 0.5311153531074524, "score_of_first_answer": 0.5311153531074524, "max_score_of_first_answer": 0.9972924590110779, "min_score_of_first_answer": 0.15102723240852356, "avg_score": 0.5311153531074524, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3983217687468565, "std_of_first_answer": 0.3983217687468565, "avg_entropy_of_first_answer": 1.416548836355408, "lex_sim": 1.0}
{"question": "what size engine does a 2005 honda civic have", "answer": ["1.7 L, 1.7 L, and 1.7 L."], "scores": [[0.8905349969863892, 0.8441734313964844, 0.8151593208312988, 0.8717243671417236, 0.29749053716659546, 0.04369642585515976, 0.8547000288963318, 0.7927441596984863, 0.6381928324699402, 0.27693694829940796, 0.8890940546989441, 0.2817912697792053, 0.06577437371015549, 0.9812606573104858, 0.39777159690856934, 0.9778209924697876, 0.6768532395362854, 0.9699786305427551, 0.01742692105472088]], "normalized_score": [0.4167173504829407], "tokens": [["\u2581", "1", ".", "7", "\u2581L", ",", "\u2581", "1", ".", "7", "\u2581L", ",", "\u2581and", "\u2581", "1", ".", "7", "\u2581L", "."]], "entropy": [[0.8543438911437988, 0.5551971197128296, 0.8392768502235413, 0.5081074833869934, 2.144705295562744, 2.61320161819458, 0.9403603076934814, 0.6441275477409363, 1.2230902910232544, 0.6883546113967896, 0.5826774835586548, 3.0265533924102783, 0.721002459526062, 0.16797621548175812, 0.7107741236686707, 0.15498456358909607, 0.7587090730667114, 0.1981637328863144, 0.6872982978820801]], "avg_entropy": [0.9483633872709776], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1169\u00a0cc", "score_of_biggest_cluster": 0.4167173504829407, "score_of_first_answer": 0.4167173504829407, "max_score_of_first_answer": 0.9812606573104858, "min_score_of_first_answer": 0.01742692105472088, "avg_score": 0.4167173504829407, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3365964569789813, "std_of_first_answer": 0.3365964569789813, "avg_entropy_of_first_answer": 0.9483633872709776, "lex_sim": 1.0}
{"question": "the first permanent european settlement on future united states soil was located where", "answer": ["St. Augustine, Florida"], "scores": [[0.5716382265090942, 0.926013708114624, 0.9783957004547119, 0.999128520488739, 0.7908074855804443, 0.9544128775596619]], "normalized_score": [0.8549622297286987], "tokens": [["\u2581St", ".", "\u2581August", "ine", ",", "\u2581Florida"]], "entropy": [[2.176022529602051, 0.3205442428588867, 0.1540517956018448, 0.00930667296051979, 0.7265332341194153, 0.25181978940963745]], "avg_entropy": [0.6063797107587258], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jamestown", "score_of_biggest_cluster": 0.8549622297286987, "score_of_first_answer": 0.8549622297286987, "max_score_of_first_answer": 0.999128520488739, "min_score_of_first_answer": 0.5716382265090942, "avg_score": 0.8549622297286987, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14943117038263637, "std_of_first_answer": 0.14943117038263637, "avg_entropy_of_first_answer": 0.6063797107587258, "lex_sim": 1.0}
{"question": "where is gall bladder situated in human body", "answer": ["under the liver"], "scores": [[0.08919689059257507, 0.8482210636138916, 0.9119131565093994, 0.9993770718574524]], "normalized_score": [0.5124309659004211], "tokens": [["\u2581under", "\u2581the", "\u2581li", "ver"]], "entropy": [[4.170430660247803, 0.7320579886436462, 0.49162614345550537, 0.007674518972635269]], "avg_entropy": [1.3504473278298974], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "beneath the liver", "score_of_biggest_cluster": 0.5124309659004211, "score_of_first_answer": 0.5124309659004211, "max_score_of_first_answer": 0.9993770718574524, "min_score_of_first_answer": 0.08919689059257507, "avg_score": 0.5124309659004211, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3636587058816042, "std_of_first_answer": 0.3636587058816042, "avg_entropy_of_first_answer": 1.3504473278298974, "lex_sim": 1.0}
{"question": "when was the cat and mouse act introduced", "answer": ["1911"], "scores": [[0.39314937591552734, 0.8703689575195312, 0.8219289183616638, 0.5058260560035706, 0.036570530384778976]], "normalized_score": [0.34933772683143616], "tokens": [["\u2581", "1", "9", "1", "1"]], "entropy": [[2.4859261512756348, 0.5572521686553955, 0.9463033676147461, 1.5637381076812744, 1.7180482149124146]], "avg_entropy": [1.454253602027893], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1913", "score_of_biggest_cluster": 0.34933772683143616, "score_of_first_answer": 0.34933772683143616, "max_score_of_first_answer": 0.8703689575195312, "min_score_of_first_answer": 0.036570530384778976, "avg_score": 0.34933772683143616, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30455227987420486, "std_of_first_answer": 0.30455227987420486, "avg_entropy_of_first_answer": 1.454253602027893, "lex_sim": 1.0}
{"question": "who invented the first computer game in 1962 and what was the name of the game", "answer": ["Spacewar!"], "scores": [[0.32240980863571167, 0.9027044773101807, 0.7159741520881653]], "normalized_score": [0.5928576588630676], "tokens": [["\u2581Space", "war", "!"]], "entropy": [[3.6715915203094482, 0.47573238611221313, 1.0079846382141113]], "avg_entropy": [1.7184361815452576], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Spacewar!", "score_of_biggest_cluster": 0.5928576588630676, "score_of_first_answer": 0.5928576588630676, "max_score_of_first_answer": 0.9027044773101807, "min_score_of_first_answer": 0.32240980863571167, "avg_score": 0.5928576588630676, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2418684236629247, "std_of_first_answer": 0.2418684236629247, "avg_entropy_of_first_answer": 1.7184361815452576, "lex_sim": 1.0}
{"question": "where do the signals for apoptosis come from", "answer": ["DNA damage"], "scores": [[0.03252734988927841, 0.4059021770954132]], "normalized_score": [0.11490396410226822], "tokens": [["\u2581DNA", "\u2581damage"]], "entropy": [[4.7508440017700195, 1.6520586013793945]], "avg_entropy": [3.201451301574707], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "from other cells", "score_of_biggest_cluster": 0.11490396410226822, "score_of_first_answer": 0.11490396410226822, "max_score_of_first_answer": 0.4059021770954132, "min_score_of_first_answer": 0.03252734988927841, "avg_score": 0.11490396410226822, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1866874136030674, "std_of_first_answer": 0.1866874136030674, "avg_entropy_of_first_answer": 3.201451301574707, "lex_sim": 1.0}
{"question": "who won college basketball player of the year", "answer": ["Zion Williamson"], "scores": [[0.18964996933937073, 0.9919539093971252, 0.9770650863647461, 0.998822033405304]], "normalized_score": [0.6545818448066711], "tokens": [["\u2581Z", "ion", "\u2581William", "son"]], "entropy": [[3.9616456031799316, 0.07420317828655243, 0.20085382461547852, 0.012272235006093979]], "avg_entropy": [1.0622437102720141], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A'ja Wilson", "score_of_biggest_cluster": 0.6545818448066711, "score_of_first_answer": 0.6545818448066711, "max_score_of_first_answer": 0.998822033405304, "min_score_of_first_answer": 0.18964996933937073, "avg_score": 0.6545818448066711, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3463394127279817, "std_of_first_answer": 0.3463394127279817, "avg_entropy_of_first_answer": 1.0622437102720141, "lex_sim": 1.0}
{"question": "who has won india's next super star", "answer": ["Akshay Chaudhary"], "scores": [[0.005900947842746973, 0.2899750769138336, 0.22729310393333435, 0.051617302000522614, 0.05813160911202431, 0.9923533797264099, 0.894772469997406]], "normalized_score": [0.13965773582458496], "tokens": [["\u2581Ak", "sh", "ay", "\u2581Ch", "aud", "h", "ary"]], "entropy": [[5.201535224914551, 2.2731165885925293, 2.670578718185425, 3.6575069427490234, 1.9395296573638916, 0.06561924517154694, 0.44018587470054626]], "avg_entropy": [2.3211531788110733], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Natasha Bharadwaj", "score_of_biggest_cluster": 0.13965773582458496, "score_of_first_answer": 0.13965773582458496, "max_score_of_first_answer": 0.9923533797264099, "min_score_of_first_answer": 0.005900947842746973, "avg_score": 0.13965773582458496, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38172120603199117, "std_of_first_answer": 0.38172120603199117, "avg_entropy_of_first_answer": 2.3211531788110733, "lex_sim": 1.0}
{"question": "where does what in the sam hill come from", "answer": ["It is American slang, originating around 1830."], "scores": [[0.04570457339286804, 0.4568908214569092, 0.008382081985473633, 0.6237964630126953, 0.9981397390365601, 0.1566910594701767, 0.043275780975818634, 0.554600179195404, 0.028462113812565804, 0.517606258392334, 0.9986032843589783, 0.6259046792984009, 0.49748772382736206, 0.2112533301115036, 0.26564106345176697, 0.009260854683816433, 0.004484829492866993, 0.6444443464279175, 0.16252464056015015, 0.36850693821907043, 0.15620547533035278, 0.419063001871109, 0.14725589752197266, 0.5663422346115112, 0.237253800034523, 0.9977795481681824, 0.18769508600234985, 0.2739296555519104]], "normalized_score": [0.19386667013168335], "tokens": [["\u2581It", "\u2581is", "\u2581American", "\u2581sl", "ang", ",", "\u2581origin", "ating", "\u2581around", "\u2581", "1", "8", "3", "0", ".", "\u2581In", "\u2581general", ",", "\u2581the", "\u2581phrase", "\u2581means", "\u2581\"", "What", "\u2581the", "\u2581he", "ck", "?", "!\""]], "entropy": [[4.9520063400268555, 2.227267026901245, 2.8023858070373535, 1.9120687246322632, 0.01755647361278534, 2.860081672668457, 4.19862699508667, 1.497502326965332, 1.5463050603866577, 0.9062035083770752, 0.012517398223280907, 0.7015926241874695, 1.6491119861602783, 1.8929407596588135, 2.3586416244506836, 2.5327823162078857, 3.5420358180999756, 1.5553652048110962, 2.5618977546691895, 2.870654582977295, 2.790656805038452, 2.452069044113159, 3.406890392303467, 1.393237829208374, 2.0845649242401123, 0.022633321583271027, 2.7756197452545166, 2.1169488430023193]], "avg_entropy": [2.1300058896387264], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "simple bowdlerization", "score_of_biggest_cluster": 0.19386667013168335, "score_of_first_answer": 0.19386667013168335, "max_score_of_first_answer": 0.9986032843589783, "min_score_of_first_answer": 0.004484829492866993, "avg_score": 0.19386667013168335, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2995195633728394, "std_of_first_answer": 0.2995195633728394, "avg_entropy_of_first_answer": 2.1300058896387264, "lex_sim": 1.0}
{"question": "when did cristiano ronaldo go to manchester united", "answer": ["2003"], "scores": [[0.12054234743118286, 0.7195760607719421, 0.94622403383255, 0.9954765439033508, 0.9438958168029785]], "normalized_score": [0.5990086197853088], "tokens": [["\u2581", "2", "0", "0", "3"]], "entropy": [[1.138472080230713, 0.732414722442627, 0.2981273829936981, 0.036569125950336456, 0.3080374598503113]], "avg_entropy": [0.5027241542935371], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "2003", "score_of_biggest_cluster": 0.5990086197853088, "score_of_first_answer": 0.5990086197853088, "max_score_of_first_answer": 0.9954765439033508, "min_score_of_first_answer": 0.12054234743118286, "avg_score": 0.5990086197853088, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3266137753152405, "std_of_first_answer": 0.3266137753152405, "avg_entropy_of_first_answer": 0.5027241542935371, "lex_sim": 1.0}
{"question": "which term means the study of the nature and causes of disease", "answer": ["pathology"], "scores": [[0.726391077041626, 0.9699464440345764]], "normalized_score": [0.8393809795379639], "tokens": [["\u2581path", "ology"]], "entropy": [[1.1181695461273193, 0.17420192062854767]], "avg_entropy": [0.6461857333779335], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "pathology", "score_of_biggest_cluster": 0.8393809795379639, "score_of_first_answer": 0.8393809795379639, "max_score_of_first_answer": 0.9699464440345764, "min_score_of_first_answer": 0.726391077041626, "avg_score": 0.8393809795379639, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.12177768349647522, "std_of_first_answer": 0.12177768349647522, "avg_entropy_of_first_answer": 0.6461857333779335, "lex_sim": 1.0}
{"question": "who discovered that plants were made of cells", "answer": ["Robert Hooke"], "scores": [[0.6519885659217834, 0.6839917302131653, 0.999347984790802, 0.9994568228721619]], "normalized_score": [0.8169451355934143], "tokens": [["\u2581Robert", "\u2581H", "oo", "ke"]], "entropy": [[2.14268159866333, 1.2147327661514282, 0.007195658981800079, 0.006501705385744572]], "avg_entropy": [0.8427779322955757], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Parashara", "score_of_biggest_cluster": 0.8169451355934143, "score_of_first_answer": 0.8169451355934143, "max_score_of_first_answer": 0.9994568228721619, "min_score_of_first_answer": 0.6519885659217834, "avg_score": 0.8169451355934143, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1660919853994937, "std_of_first_answer": 0.1660919853994937, "avg_entropy_of_first_answer": 0.8427779322955757, "lex_sim": 1.0}
{"question": "when did they start adding zinc to pennies", "answer": ["1983"], "scores": [[0.6716607213020325, 0.9483873248100281, 0.9962356686592102, 0.9654657244682312, 0.1019311398267746]], "normalized_score": [0.5742596387863159], "tokens": [["\u2581", "1", "9", "8", "3"]], "entropy": [[1.7602295875549316, 0.24432028830051422, 0.035085394978523254, 0.18388190865516663, 0.4396396577358246]], "avg_entropy": [0.5326313674449921], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1864", "score_of_biggest_cluster": 0.5742596387863159, "score_of_first_answer": 0.5742596387863159, "max_score_of_first_answer": 0.9962356686592102, "min_score_of_first_answer": 0.1019311398267746, "avg_score": 0.5742596387863159, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3381318235778739, "std_of_first_answer": 0.3381318235778739, "avg_entropy_of_first_answer": 0.5326313674449921, "lex_sim": 1.0}
{"question": "when did martial law in the philippines end", "answer": ["July 1987"], "scores": [[0.13266603648662567, 0.9964872598648071, 0.2817027270793915, 0.07615457475185394, 0.6530539393424988, 0.040412887930870056]], "normalized_score": [0.20528846979141235], "tokens": [["\u2581July", "\u2581", "1", "9", "8", "7"]], "entropy": [[2.084463596343994, 0.027611760422587395, 0.8491281867027283, 1.7830190658569336, 0.8718580603599548, 1.2837107181549072]], "avg_entropy": [1.149965231306851], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1981", "score_of_biggest_cluster": 0.20528846979141235, "score_of_first_answer": 0.20528846979141235, "max_score_of_first_answer": 0.9964872598648071, "min_score_of_first_answer": 0.040412887930870056, "avg_score": 0.20528846979141235, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3491518526827168, "std_of_first_answer": 0.3491518526827168, "avg_entropy_of_first_answer": 1.149965231306851, "lex_sim": 1.0}
{"question": "who sings the song only in my dreams", "answer": ["Enya"], "scores": [[0.005674524232745171, 0.13344065845012665]], "normalized_score": [0.02751748450100422], "tokens": [["\u2581En", "ya"]], "entropy": [[5.328826904296875, 1.1991996765136719]], "avg_entropy": [3.2640132904052734], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Debbie Gibson", "score_of_biggest_cluster": 0.02751748450100422, "score_of_first_answer": 0.02751748450100422, "max_score_of_first_answer": 0.13344065845012665, "min_score_of_first_answer": 0.005674524232745171, "avg_score": 0.02751748450100422, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06388306710869074, "std_of_first_answer": 0.06388306710869074, "avg_entropy_of_first_answer": 3.2640132904052734, "lex_sim": 1.0}
{"question": "fictional character whose favorite song was blueberry hill", "answer": ["Blueberry Hill"], "scores": [[0.006067649926990271, 0.8951966166496277, 0.33630162477493286]], "normalized_score": [0.12224259227514267], "tokens": [["\u2581Blue", "berry", "\u2581Hill"]], "entropy": [[5.665476322174072, 0.8183615207672119, 2.6282095909118652]], "avg_entropy": [3.0373491446177163], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Richie Cunningham", "score_of_biggest_cluster": 0.12224259227514267, "score_of_first_answer": 0.12224259227514267, "max_score_of_first_answer": 0.8951966166496277, "min_score_of_first_answer": 0.006067649926990271, "avg_score": 0.12224259227514267, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3669647893420584, "std_of_first_answer": 0.3669647893420584, "avg_entropy_of_first_answer": 3.0373491446177163, "lex_sim": 1.0}
{"question": "who wrote ain't living long like this", "answer": ["Waylon Jennings"], "scores": [[0.1481240689754486, 0.9950118064880371, 0.9814328551292419, 0.999568521976471]], "normalized_score": [0.6166400909423828], "tokens": [["\u2581Way", "lon", "\u2581Jenn", "ings"]], "entropy": [[4.009950637817383, 0.04479343444108963, 0.14174553751945496, 0.00487858522683382]], "avg_entropy": [1.0503420487511903], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rodney Crowell", "score_of_biggest_cluster": 0.6166400909423828, "score_of_first_answer": 0.6166400909423828, "max_score_of_first_answer": 0.999568521976471, "min_score_of_first_answer": 0.1481240689754486, "avg_score": 0.6166400909423828, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36547179128187773, "std_of_first_answer": 0.36547179128187773, "avg_entropy_of_first_answer": 1.0503420487511903, "lex_sim": 1.0}
{"question": "who become the ceo of it wipro company in 2016", "answer": ["Abid Ali Neemuchwala"], "scores": [[0.8598206639289856, 0.9947261214256287, 0.06908641755580902, 0.9700883626937866, 0.9983388185501099, 0.996961772441864, 0.9956436157226562, 0.9935372471809387]], "normalized_score": [0.6981439590454102], "tokens": [["\u2581Ab", "id", "\u2581Ali", "\u2581Ne", "em", "uch", "w", "ala"]], "entropy": [[0.9347579479217529, 0.05137956514954567, 0.4664546549320221, 0.1997305154800415, 0.01769336871802807, 0.03268589824438095, 0.037093885242938995, 0.04269060865044594]], "avg_entropy": [0.22281080554239452], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Abid Ali Neemuchwala", "score_of_biggest_cluster": 0.6981439590454102, "score_of_first_answer": 0.6981439590454102, "max_score_of_first_answer": 0.9983388185501099, "min_score_of_first_answer": 0.06908641755580902, "avg_score": 0.6981439590454102, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3020641574045143, "std_of_first_answer": 0.3020641574045143, "avg_entropy_of_first_answer": 0.22281080554239452, "lex_sim": 1.0}
{"question": "the most important battles in the revolutionary war", "answer": ["Trenton"], "scores": [[0.018340708687901497, 0.9981032609939575, 0.9956329464912415]], "normalized_score": [0.26316630840301514], "tokens": [["\u2581Tr", "ent", "on"]], "entropy": [[3.549283266067505, 0.020424917340278625, 0.047879766672849655]], "avg_entropy": [1.2058626500268776], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Philadelphia campaign", "score_of_biggest_cluster": 0.26316630840301514, "score_of_first_answer": 0.26316630840301514, "max_score_of_first_answer": 0.9981032609939575, "min_score_of_first_answer": 0.018340708687901497, "avg_score": 0.26316630840301514, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46128334018928824, "std_of_first_answer": 0.46128334018928824, "avg_entropy_of_first_answer": 1.2058626500268776, "lex_sim": 1.0}
{"question": "when did fender start making amps in mexico", "answer": ["1998"], "scores": [[0.718268871307373, 0.8781636357307434, 0.9966448545455933, 0.41082555055618286, 0.2734505832195282]], "normalized_score": [0.5885561108589172], "tokens": [["\u2581", "1", "9", "9", "8"]], "entropy": [[1.9175910949707031, 0.3925469219684601, 0.032678890973329544, 1.2300529479980469, 2.088815689086914]], "avg_entropy": [1.1323371089994907], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1996", "score_of_biggest_cluster": 0.5885561108589172, "score_of_first_answer": 0.5885561108589172, "max_score_of_first_answer": 0.9966448545455933, "min_score_of_first_answer": 0.2734505832195282, "avg_score": 0.5885561108589172, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27412631265355775, "std_of_first_answer": 0.27412631265355775, "avg_entropy_of_first_answer": 1.1323371089994907, "lex_sim": 1.0}
{"question": "what us president is the only president to become an eagle scout", "answer": ["Gerald Rudolph Ford, Jr."], "scores": [[0.8128311038017273, 0.9978341460227966, 0.670818030834198, 0.9723389148712158, 0.9284143447875977, 0.37502148747444153, 0.9941285848617554, 0.8943618535995483]], "normalized_score": [0.7975893616676331], "tokens": [["\u2581Ger", "ald", "\u2581Rud", "olph", "\u2581Ford", ",", "\u2581Jr", "."]], "entropy": [[1.16554594039917, 0.02086097002029419, 0.8430889844894409, 0.19832125306129456, 0.3301621377468109, 1.0931804180145264, 0.049457889050245285, 0.39733096957206726]], "avg_entropy": [0.5122435702942312], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Ford", "score_of_biggest_cluster": 0.7975893616676331, "score_of_first_answer": 0.7975893616676331, "max_score_of_first_answer": 0.9978341460227966, "min_score_of_first_answer": 0.37502148747444153, "avg_score": 0.7975893616676331, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.20057095927672547, "std_of_first_answer": 0.20057095927672547, "avg_entropy_of_first_answer": 0.5122435702942312, "lex_sim": 1.0}
{"question": "where does the red river start and end in louisiana", "answer": ["Arkansas"], "scores": [[0.037628259509801865, 0.993802547454834]], "normalized_score": [0.1933780163526535], "tokens": [["\u2581Ark", "ansas"]], "entropy": [[4.6553239822387695, 0.060132838785648346]], "avg_entropy": [2.357728410512209], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Texas Panhandle", "score_of_biggest_cluster": 0.1933780163526535, "score_of_first_answer": 0.1933780163526535, "max_score_of_first_answer": 0.993802547454834, "min_score_of_first_answer": 0.037628259509801865, "avg_score": 0.1933780163526535, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.47808714397251606, "std_of_first_answer": 0.47808714397251606, "avg_entropy_of_first_answer": 2.357728410512209, "lex_sim": 1.0}
{"question": "when did the first ice age come out", "answer": ["March 15, 2002ibilee April 13, 2010 at 5:18 pm\tReply"], "scores": [[0.9059228301048279, 0.9991421699523926, 0.8608169555664062, 0.9478442072868347, 0.9933331608772278, 0.9988784193992615, 0.999359667301178, 0.9999781847000122, 0.9992479681968689, 0.9986661672592163, 1.818656475416347e-10, 0.001086143427528441, 0.0002112040965585038, 0.9587264060974121, 0.3202569782733917, 0.06954146921634674, 0.9457435607910156, 0.9885243773460388, 0.949277400970459, 0.9967139959335327, 0.3682251572608948, 0.05435401201248169, 0.0913054421544075, 0.9938302636146545, 0.06718327105045319, 0.9966021776199341, 0.15210428833961487, 0.10165621340274811, 0.09664759039878845, 0.009476231411099434, 0.17592290043830872, 0.9997594952583313]], "normalized_score": [0.13569007813930511], "tokens": [["\u2581March", "\u2581", "1", "5", ",", "\u2581", "2", "0", "0", "2", "ibile", "e", "\u2581April", "\u2581", "1", "3", ",", "\u2581", "2", "0", "1", "0", "\u2581at", "\u2581", "5", ":", "1", "8", "\u2581pm", "<0x09>", "Rep", "ly"]], "entropy": [[0.5534473657608032, 0.008278817869722843, 0.5857185125350952, 0.31602081656455994, 0.05038328096270561, 0.009544871747493744, 0.005544852465391159, 0.00032481655944138765, 0.006477471441030502, 0.013146182522177696, 0.387554794549942, 2.479675769805908, 2.5720643997192383, 0.3337137699127197, 1.8585435152053833, 2.398043155670166, 0.370843768119812, 0.12104056030511856, 0.20905868709087372, 0.0324244387447834, 0.7847808003425598, 2.1462013721466064, 2.4339261054992676, 0.06638441234827042, 1.9750293493270874, 0.03537961468100548, 1.799063801765442, 2.308704376220703, 1.4632660150527954, 2.2975900173187256, 2.9637820720672607, 0.0028555484022945166]], "avg_entropy": [0.955900416647637], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2002", "score_of_biggest_cluster": 0.13569007813930511, "score_of_first_answer": 0.13569007813930511, "max_score_of_first_answer": 0.9999781847000122, "min_score_of_first_answer": 1.818656475416347e-10, "avg_score": 0.13569007813930511, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4368417042223559, "std_of_first_answer": 0.4368417042223559, "avg_entropy_of_first_answer": 0.955900416647637, "lex_sim": 1.0}
{"question": "who supported states rights during the civil war", "answer": ["south (confederacy)"], "scores": [[0.028695425018668175, 0.0049942368641495705, 0.27538448572158813, 0.9970510005950928, 0.46489420533180237, 0.9691928029060364]], "normalized_score": [0.1614793986082077], "tokens": [["\u2581south", "\u2581(", "conf", "eder", "acy", ")"]], "entropy": [[3.484158515930176, 0.6226906776428223, 3.227433204650879, 0.02539769560098648, 1.003337025642395, 0.24060392379760742]], "avg_entropy": [1.4339368405441444], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "the Confederacy", "score_of_biggest_cluster": 0.1614793986082077, "score_of_first_answer": 0.1614793986082077, "max_score_of_first_answer": 0.9970510005950928, "min_score_of_first_answer": 0.0049942368641495705, "avg_score": 0.1614793986082077, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.40307082715575143, "std_of_first_answer": 0.40307082715575143, "avg_entropy_of_first_answer": 1.4339368405441444, "lex_sim": 1.0}
{"question": "how many gallons of water in the gulf of mexico", "answer": ["(42,400,000)"], "scores": [[0.001371239311993122, 0.0616205632686615, 0.04862603172659874, 0.27221593260765076, 0.0639541894197464, 0.21340182423591614, 0.8041256070137024, 0.7737471461296082, 0.9654134511947632, 0.9954368472099304, 0.9963648915290833, 0.07018494606018066]], "normalized_score": [0.1712971031665802], "tokens": [["\u2581(", "4", "2", ",", "4", "0", "0", ",", "0", "0", "0", ")"]], "entropy": [[1.0347459316253662, 3.909719705581665, 2.6552977561950684, 2.7984859943389893, 2.14573335647583, 2.2531561851501465, 0.9365402460098267, 1.305127739906311, 0.23207777738571167, 0.03985686972737312, 0.036970093846321106, 1.792405605316162]], "avg_entropy": [1.5950097717965643], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "660 quadrillion US gallons", "score_of_biggest_cluster": 0.1712971031665802, "score_of_first_answer": 0.1712971031665802, "max_score_of_first_answer": 0.9963648915290833, "min_score_of_first_answer": 0.001371239311993122, "avg_score": 0.1712971031665802, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40666143220089834, "std_of_first_answer": 0.40666143220089834, "avg_entropy_of_first_answer": 1.5950097717965643, "lex_sim": 1.0}
{"question": "how does the cash cab guy read the questions", "answer": ["off of a plasma screen television"], "scores": [[0.01747872121632099, 0.29024913907051086, 0.6892688274383545, 0.0008402992971241474, 0.954960823059082, 0.7664878368377686, 0.013241397216916084]], "normalized_score": [0.0835748016834259], "tokens": [["\u2581off", "\u2581of", "\u2581a", "\u2581pl", "asma", "\u2581screen", "\u2581television"]], "entropy": [[5.015293121337891, 1.514585256576538, 1.4504424333572388, 2.7493937015533447, 0.23175781965255737, 1.131378412246704, 2.400714159011841]], "avg_entropy": [2.070509271962302], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "walkie-talkie and earpiece", "score_of_biggest_cluster": 0.0835748016834259, "score_of_first_answer": 0.0835748016834259, "max_score_of_first_answer": 0.954960823059082, "min_score_of_first_answer": 0.0008402992971241474, "avg_score": 0.0835748016834259, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.376563293200275, "std_of_first_answer": 0.376563293200275, "avg_entropy_of_first_answer": 2.070509271962302, "lex_sim": 1.0}
{"question": "when was the last time nebraska voted democrat", "answer": ["1964"], "scores": [[0.6513605713844299, 0.8370388150215149, 0.9954628348350525, 0.9661985039710999, 0.9959675073623657]], "normalized_score": [0.8781743049621582], "tokens": [["\u2581", "1", "9", "6", "4"]], "entropy": [[1.4197747707366943, 0.4926585555076599, 0.03903649002313614, 0.20082218945026398, 0.0327238030731678]], "avg_entropy": [0.43700316175818443], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "1964", "score_of_biggest_cluster": 0.8781743049621582, "score_of_first_answer": 0.8781743049621582, "max_score_of_first_answer": 0.9959675073623657, "min_score_of_first_answer": 0.6513605713844299, "avg_score": 0.8781743049621582, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1325958092644461, "std_of_first_answer": 0.1325958092644461, "avg_entropy_of_first_answer": 0.43700316175818443, "lex_sim": 1.0}
{"question": "which organelle is the site of atp production and storage", "answer": ["mitochondria"], "scores": [[0.8320697546005249, 0.9824187159538269, 0.993895947933197, 0.7508166432380676]], "normalized_score": [0.8837568759918213], "tokens": [["\u2581mit", "och", "ond", "ria"]], "entropy": [[0.8015747666358948, 0.11249974370002747, 0.05174221098423004, 0.6229866743087769]], "avg_entropy": [0.3972008489072323], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "mitochondrion", "score_of_biggest_cluster": 0.8837568759918213, "score_of_first_answer": 0.8837568759918213, "max_score_of_first_answer": 0.993895947933197, "min_score_of_first_answer": 0.7508166432380676, "avg_score": 0.8837568759918213, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.10254675471640108, "std_of_first_answer": 0.10254675471640108, "avg_entropy_of_first_answer": 0.3972008489072323, "lex_sim": 1.0}
{"question": "who are the australia's got talent judges", "answer": ["David Walliams"], "scores": [[0.0341668576002121, 0.23294876515865326, 0.9708878397941589, 0.9988640546798706]], "normalized_score": [0.29640477895736694], "tokens": [["\u2581David", "\u2581Wal", "li", "ams"]], "entropy": [[4.382864475250244, 2.7522194385528564, 0.1628054827451706, 0.010020720772445202]], "avg_entropy": [1.826977529330179], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kelly Osbourne", "score_of_biggest_cluster": 0.29640477895736694, "score_of_first_answer": 0.29640477895736694, "max_score_of_first_answer": 0.9988640546798706, "min_score_of_first_answer": 0.0341668576002121, "avg_score": 0.29640477895736694, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43153534795135284, "std_of_first_answer": 0.43153534795135284, "avg_entropy_of_first_answer": 1.826977529330179, "lex_sim": 1.0}
{"question": "blood alcohol concentration means the parts of alcohol in the blood in relation to what", "answer": ["1,000,000"], "scores": [[0.08310777693986893, 0.9146238565444946, 0.11530802398920059, 0.9845888018608093, 0.9980258941650391, 0.9992309808731079, 0.08861078321933746, 0.9903817772865295, 0.9990398287773132, 0.9988070726394653]], "normalized_score": [0.4872097969055176], "tokens": [["\u2581", "1", ",", "0", "0", "0", ",", "0", "0", "0"]], "entropy": [[3.7606163024902344, 0.4959699809551239, 1.649836778640747, 0.12375436723232269, 0.01955755054950714, 0.008681989274919033, 2.2268319129943848, 0.08805729448795319, 0.011724216863512993, 0.01255868747830391]], "avg_entropy": [0.839758908096701], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "volume of blood", "score_of_biggest_cluster": 0.4872097969055176, "score_of_first_answer": 0.4872097969055176, "max_score_of_first_answer": 0.9992309808731079, "min_score_of_first_answer": 0.08310777693986893, "avg_score": 0.4872097969055176, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40764119239807767, "std_of_first_answer": 0.40764119239807767, "avg_entropy_of_first_answer": 0.839758908096701, "lex_sim": 1.0}
{"question": "georgia out of angus thongs and perfect snogging", "answer": ["Alicia Silverstone"], "scores": [[0.20495593547821045, 0.6006056666374207, 0.9993847608566284, 0.2882230877876282, 0.9952319264411926]], "normalized_score": [0.5123032927513123], "tokens": [["\u2581A", "lic", "ia", "\u2581Silver", "stone"]], "entropy": [[3.5951640605926514, 1.8227949142456055, 0.0071915071457624435, 4.058071136474609, 0.04391902685165405]], "avg_entropy": [1.9054281290620565], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Georgia Groome as Georgia Nicolson", "score_of_biggest_cluster": 0.5123032927513123, "score_of_first_answer": 0.5123032927513123, "max_score_of_first_answer": 0.9993847608566284, "min_score_of_first_answer": 0.20495593547821045, "avg_score": 0.5123032927513123, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3368734257608829, "std_of_first_answer": 0.3368734257608829, "avg_entropy_of_first_answer": 1.9054281290620565, "lex_sim": 1.0}
{"question": "when did the soviet union start and end", "answer": ["It started March 13, 1923 and ended as the country on December 26, 1991."], "scores": [[0.01519517507404089, 0.175296813249588, 0.0013476089807227254, 0.9900887608528137, 0.6268003582954407, 0.01327355857938528, 0.9141702651977539, 0.9916941523551941, 0.9810308218002319, 0.9791796803474426, 0.3218577802181244, 0.018336471170186996, 0.5144869089126587, 0.7471618056297302, 0.003276681527495384, 0.3529292643070221, 0.006780566181987524, 0.09848636388778687, 0.8469158411026001, 0.9966936111450195, 0.679221510887146, 0.5558212995529175, 0.9850971698760986, 0.9956158399581909, 0.9979391694068909, 0.9998472929000854, 0.9990867376327515, 0.9981653094291687, 0.4735284149646759]], "normalized_score": [0.255664199590683], "tokens": [["\u2581It", "\u2581started", "\u2581March", "\u2581", "1", "3", ",", "\u2581", "1", "9", "2", "3", "\u2581and", "\u2581ended", "\u2581as", "\u2581the", "\u2581country", "\u2581on", "\u2581December", "\u2581", "2", "6", ",", "\u2581", "1", "9", "9", "1", "."]], "entropy": [[1.8671616315841675, 2.221496820449829, 1.684502363204956, 0.0724555030465126, 1.2812721729278564, 1.4499156475067139, 0.4680929481983185, 0.062244512140750885, 0.09521269798278809, 0.10910329967737198, 1.9068553447723389, 1.3122886419296265, 1.697223424911499, 1.3877716064453125, 1.7229779958724976, 2.008699417114258, 3.001732110977173, 2.6745247840881348, 0.7845144271850586, 0.028741635382175446, 0.839867353439331, 0.9738942384719849, 0.10260936617851257, 0.033862531185150146, 0.01563386805355549, 0.0018197493627667427, 0.008511221967637539, 0.016949579119682312, 1.1030516624450684]], "avg_entropy": [0.9976891915731388], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in Eurasia", "score_of_biggest_cluster": 0.255664199590683, "score_of_first_answer": 0.255664199590683, "max_score_of_first_answer": 0.9998472929000854, "min_score_of_first_answer": 0.0013476089807227254, "avg_score": 0.255664199590683, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3968451477279072, "std_of_first_answer": 0.3968451477279072, "avg_entropy_of_first_answer": 0.9976891915731388, "lex_sim": 1.0}
{"question": "what type of dog was laika the spacedog", "answer": ["The Russian Space Dogs"], "scores": [[0.008351083844900131, 0.07094097882509232, 0.33540937304496765, 0.46433025598526, 0.999846339225769]], "normalized_score": [0.15595349669456482], "tokens": [["\u2581The", "\u2581Russian", "\u2581Space", "\u2581D", "ogs"]], "entropy": [[3.5503554344177246, 3.713977575302124, 1.751479983329773, 1.4709227085113525, 0.0020429142750799656]], "avg_entropy": [2.097755723167211], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "mongrel female", "score_of_biggest_cluster": 0.15595349669456482, "score_of_first_answer": 0.15595349669456482, "max_score_of_first_answer": 0.999846339225769, "min_score_of_first_answer": 0.008351083844900131, "avg_score": 0.15595349669456482, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35407924770359234, "std_of_first_answer": 0.35407924770359234, "avg_entropy_of_first_answer": 2.097755723167211, "lex_sim": 1.0}
{"question": "who plays jack skellington in nightmare before christmas", "answer": ["Chris Sarandon"], "scores": [[0.9553624987602234, 0.9824016690254211, 0.9914073348045349]], "normalized_score": [0.9762696623802185], "tokens": [["\u2581Chris", "\u2581Sar", "andon"]], "entropy": [[0.2881934940814972, 0.15934839844703674, 0.062243133783340454]], "avg_entropy": [0.16992834210395813], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Danny Elfman", "score_of_biggest_cluster": 0.9762696623802185, "score_of_first_answer": 0.9762696623802185, "max_score_of_first_answer": 0.9914073348045349, "min_score_of_first_answer": 0.9553624987602234, "avg_score": 0.9762696623802185, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.015316835146758377, "std_of_first_answer": 0.015316835146758377, "avg_entropy_of_first_answer": 0.16992834210395813, "lex_sim": 1.0}
{"question": "who sings oh what a night late december back in 63", "answer": ["Frankie Valli"], "scores": [[0.7833231687545776, 0.9582481980323792, 0.991754412651062, 0.9989616870880127]], "normalized_score": [0.9286305904388428], "tokens": [["\u2581Frank", "ie", "\u2581Val", "li"]], "entropy": [[1.352815866470337, 0.22235772013664246, 0.06279552727937698, 0.01195619534701109]], "avg_entropy": [0.41248132730834186], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "the Four Seasons", "score_of_biggest_cluster": 0.9286305904388428, "score_of_first_answer": 0.9286305904388428, "max_score_of_first_answer": 0.9989616870880127, "min_score_of_first_answer": 0.7833231687545776, "avg_score": 0.9286305904388428, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08781177288064818, "std_of_first_answer": 0.08781177288064818, "avg_entropy_of_first_answer": 0.41248132730834186, "lex_sim": 1.0}
{"question": "who is responsible for introducing the principle of uniformitarianism", "answer": ["James Hutton"], "scores": [[0.4520200490951538, 0.9880719184875488, 0.997528612613678]], "normalized_score": [0.763760507106781], "tokens": [["\u2581James", "\u2581H", "utton"]], "entropy": [[1.875861406326294, 0.10382995754480362, 0.024421200156211853]], "avg_entropy": [0.6680375213424364], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "James Hutton", "score_of_biggest_cluster": 0.763760507106781, "score_of_first_answer": 0.763760507106781, "max_score_of_first_answer": 0.997528612613678, "min_score_of_first_answer": 0.4520200490951538, "avg_score": 0.763760507106781, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.25495547075582525, "std_of_first_answer": 0.25495547075582525, "avg_entropy_of_first_answer": 0.6680375213424364, "lex_sim": 1.0}
{"question": "when do the walking dead comics come out", "answer": ["21st"], "scores": [[0.047958310693502426, 0.4621421694755554, 0.0850318968296051, 0.3829166293144226]], "normalized_score": [0.1639009267091751], "tokens": [["\u2581", "2", "1", "st"]], "entropy": [[4.1293134689331055, 1.5392568111419678, 2.300168991088867, 2.463305950164795]], "avg_entropy": [2.608011305332184], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "October\u00a02003\u00a0\u2013 present", "score_of_biggest_cluster": 0.1639009267091751, "score_of_first_answer": 0.1639009267091751, "max_score_of_first_answer": 0.4621421694755554, "min_score_of_first_answer": 0.047958310693502426, "avg_score": 0.1639009267091751, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1806834166218662, "std_of_first_answer": 0.1806834166218662, "avg_entropy_of_first_answer": 2.608011305332184, "lex_sim": 1.0}
{"question": "what is the function of a political action committee (pac)", "answer": ["political action committee"], "scores": [[0.032289378345012665, 0.8394263982772827, 0.7289589047431946]], "normalized_score": [0.27034297585487366], "tokens": [["\u2581political", "\u2581action", "\u2581committee"]], "entropy": [[3.570197105407715, 0.9316725730895996, 0.6671048402786255]], "avg_entropy": [1.7229915062586467], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "pools campaign contributions from members", "score_of_biggest_cluster": 0.27034297585487366, "score_of_first_answer": 0.27034297585487366, "max_score_of_first_answer": 0.8394263982772827, "min_score_of_first_answer": 0.032289378345012665, "avg_score": 0.27034297585487366, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.357308093804635, "std_of_first_answer": 0.357308093804635, "avg_entropy_of_first_answer": 1.7229915062586467, "lex_sim": 1.0}
{"question": "how long did it take to build the great pyramid of egypt", "answer": ["21 years"], "scores": [[0.6467260122299194, 0.5882387161254883, 0.019160250201821327, 0.811138391494751]], "normalized_score": [0.2772952914237976], "tokens": [["\u2581", "2", "1", "\u2581years"]], "entropy": [[1.9919013977050781, 1.1019980907440186, 0.9741697907447815, 1.1088159084320068]], "avg_entropy": [1.2942212969064713], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "20-year period", "score_of_biggest_cluster": 0.2772952914237976, "score_of_first_answer": 0.2772952914237976, "max_score_of_first_answer": 0.811138391494751, "min_score_of_first_answer": 0.019160250201821327, "avg_score": 0.2772952914237976, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2984391606628768, "std_of_first_answer": 0.2984391606628768, "avg_entropy_of_first_answer": 1.2942212969064713, "lex_sim": 1.0}
{"question": "who sang the song you got a friend in me", "answer": ["Lyle Lovett"], "scores": [[0.04680160805583, 0.9231550097465515, 0.8777417540550232, 0.9899499416351318]], "normalized_score": [0.4401785433292389], "tokens": [["\u2581L", "yle", "\u2581Lov", "ett"]], "entropy": [[1.2375730276107788, 0.5380712151527405, 0.8925421833992004, 0.0721476823091507]], "avg_entropy": [0.6850835271179676], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Lyle Lovett", "score_of_biggest_cluster": 0.4401785433292389, "score_of_first_answer": 0.4401785433292389, "max_score_of_first_answer": 0.9899499416351318, "min_score_of_first_answer": 0.04680160805583, "avg_score": 0.4401785433292389, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3846345825991852, "std_of_first_answer": 0.3846345825991852, "avg_entropy_of_first_answer": 0.6850835271179676, "lex_sim": 1.0}
{"question": "which state is located in the centre of india", "answer": ["Madhya Pradesh"], "scores": [[0.9172684550285339, 0.9988777041435242, 0.9998389482498169, 0.9905931353569031, 0.9989452958106995]], "normalized_score": [0.9805622696876526], "tokens": [["\u2581Mad", "h", "ya", "\u2581Pr", "adesh"]], "entropy": [[0.5474300384521484, 0.01021009124815464, 0.0019484508084133267, 0.0628475546836853, 0.009825168177485466]], "avg_entropy": [0.12645226067397744], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chhattisgarh", "score_of_biggest_cluster": 0.9805622696876526, "score_of_first_answer": 0.9805622696876526, "max_score_of_first_answer": 0.9998389482498169, "min_score_of_first_answer": 0.9172684550285339, "avg_score": 0.9805622696876526, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.03209434582673745, "std_of_first_answer": 0.03209434582673745, "avg_entropy_of_first_answer": 0.12645226067397744, "lex_sim": 1.0}
{"question": "who played the judge in good will hunting", "answer": ["George W. Malone"], "scores": [[0.013632495887577534, 0.0195839311927557, 0.8588447570800781, 0.00654224818572402, 0.046224262565374374]], "normalized_score": [0.036999620497226715], "tokens": [["\u2581George", "\u2581W", ".", "\u2581Mal", "one"]], "entropy": [[3.3177714347839355, 2.580670118331909, 0.8300036191940308, 3.9071972370147705, 2.2238495349884033]], "avg_entropy": [2.5718983888626097], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "James P. Flynn", "score_of_biggest_cluster": 0.036999620497226715, "score_of_first_answer": 0.036999620497226715, "max_score_of_first_answer": 0.8588447570800781, "min_score_of_first_answer": 0.00654224818572402, "avg_score": 0.036999620497226715, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.335208383223243, "std_of_first_answer": 0.335208383223243, "avg_entropy_of_first_answer": 2.5718983888626097, "lex_sim": 1.0}
{"question": "what written material is included in the talmud", "answer": ["Gemara"], "scores": [[0.06622832268476486, 0.9561711549758911]], "normalized_score": [0.2516458034515381], "tokens": [["\u2581Gem", "ara"]], "entropy": [[3.9700939655303955, 0.23693573474884033]], "avg_entropy": [2.103514850139618], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Mishnah", "score_of_biggest_cluster": 0.2516458034515381, "score_of_first_answer": 0.2516458034515381, "max_score_of_first_answer": 0.9561711549758911, "min_score_of_first_answer": 0.06622832268476486, "avg_score": 0.2516458034515381, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4449714161455631, "std_of_first_answer": 0.4449714161455631, "avg_entropy_of_first_answer": 2.103514850139618, "lex_sim": 1.0}
{"question": "when did the first ninja turtles come out", "answer": ["December 25, 1965"], "scores": [[0.35352998971939087, 0.9960042834281921, 0.4921005070209503, 0.25419914722442627, 0.9845259189605713, 0.9985849857330322, 0.8308498859405518, 0.9995482563972473, 0.02067732810974121, 0.08547826111316681]], "normalized_score": [0.38047003746032715], "tokens": [["\u2581December", "\u2581", "2", "5", ",", "\u2581", "1", "9", "6", "5"]], "entropy": [[2.2908735275268555, 0.03052925318479538, 1.1984840631484985, 2.158505439758301, 0.10454416275024414, 0.012947635725140572, 0.45776990056037903, 0.004463459365069866, 0.8111700415611267, 2.2827794551849365]], "avg_entropy": [0.9352066938765347], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1984", "score_of_biggest_cluster": 0.38047003746032715, "score_of_first_answer": 0.38047003746032715, "max_score_of_first_answer": 0.9995482563972473, "min_score_of_first_answer": 0.02067732810974121, "avg_score": 0.38047003746032715, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3832455346726763, "std_of_first_answer": 0.3832455346726763, "avg_entropy_of_first_answer": 0.9352066938765347, "lex_sim": 1.0}
{"question": "when is dancing on ice on the tv", "answer": ["Sunday, January 6, 2019 - Sunday, March 17, 2019"], "scores": [[0.21477800607681274, 0.16621828079223633, 0.3683983087539673, 0.9963395595550537, 0.601054847240448, 0.7057260870933533, 0.9880785346031189, 0.9807266592979431, 0.9992865920066833, 0.9737018346786499, 0.7534195780754089, 0.029677867889404297, 0.36571604013442993, 0.9881211519241333, 0.7471066117286682, 0.999060332775116, 0.5156015753746033, 0.5440507531166077, 0.9969357252120972, 0.9992823004722595, 0.9999010562896729, 0.9996626377105713, 0.9998584985733032, 0.9992390871047974]], "normalized_score": [0.6157392859458923], "tokens": [["\u2581Sunday", ",", "\u2581January", "\u2581", "6", ",", "\u2581", "2", "0", "1", "9", "\u2581-", "\u2581Sunday", ",", "\u2581March", "\u2581", "1", "7", ",", "\u2581", "2", "0", "1", "9"]], "entropy": [[3.0638232231140137, 2.1074776649475098, 1.8840190172195435, 0.032680317759513855, 1.2753655910491943, 1.0886740684509277, 0.08992677927017212, 0.1303705871105194, 0.007717928849160671, 0.125965878367424, 0.7132928967475891, 1.6179043054580688, 1.7764012813568115, 0.09342287480831146, 0.7878459692001343, 0.009151014499366283, 1.0509891510009766, 0.7257013320922852, 0.026179220527410507, 0.00792434811592102, 0.0011276915902271867, 0.0038508521392941475, 0.0016792999813333154, 0.007314537651836872]], "avg_entropy": [0.6928669096378144], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2019", "score_of_biggest_cluster": 0.6157392859458923, "score_of_first_answer": 0.6157392859458923, "max_score_of_first_answer": 0.9999010562896729, "min_score_of_first_answer": 0.029677867889404297, "avg_score": 0.6157392859458923, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3118974172224848, "std_of_first_answer": 0.3118974172224848, "avg_entropy_of_first_answer": 0.6928669096378144, "lex_sim": 1.0}
{"question": "when was half life 2 episode 2 released", "answer": ["October 2, 2007"], "scores": [[0.8731235861778259, 0.9962472319602966, 0.43479087948799133, 0.3654494881629944, 0.9984322190284729, 0.9998205304145813, 0.9999462366104126, 0.9892411828041077, 0.9926679730415344]], "normalized_score": [0.8008322715759277], "tokens": [["\u2581October", "\u2581", "2", ",", "\u2581", "2", "0", "0", "7"]], "entropy": [[0.7973633408546448, 0.02992943301796913, 0.9824110865592957, 2.0502519607543945, 0.014408950693905354, 0.00200758152641356, 0.0007989672012627125, 0.06365198642015457, 0.052563127130270004]], "avg_entropy": [0.44370960379536784], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in 2007", "score_of_biggest_cluster": 0.8008322715759277, "score_of_first_answer": 0.8008322715759277, "max_score_of_first_answer": 0.9999462366104126, "min_score_of_first_answer": 0.3654494881629944, "avg_score": 0.8008322715759277, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2439979970300653, "std_of_first_answer": 0.2439979970300653, "avg_entropy_of_first_answer": 0.44370960379536784, "lex_sim": 1.0}
{"question": "where are most nutrients absorbed in the human digestive tract", "answer": ["small intestines"], "scores": [[0.5786503553390503, 0.9936063289642334, 0.9983536005020142, 0.0565953403711319]], "normalized_score": [0.42454519867897034], "tokens": [["\u2581small", "\u2581int", "est", "ines"]], "entropy": [[1.9109680652618408, 0.05617364123463631, 0.01525021716952324, 0.2278403490781784]], "avg_entropy": [0.5525580681860447], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "The small intestine", "score_of_biggest_cluster": 0.42454519867897034, "score_of_first_answer": 0.42454519867897034, "max_score_of_first_answer": 0.9983536005020142, "min_score_of_first_answer": 0.0565953403711319, "avg_score": 0.42454519867897034, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.38615099560896526, "std_of_first_answer": 0.38615099560896526, "avg_entropy_of_first_answer": 0.5525580681860447, "lex_sim": 1.0}
{"question": "who has won more national championships michigan or ohio state", "answer": ["Ohio State University and the University of Michigan"], "scores": [[0.42859363555908203, 0.9779924750328064, 0.23760110139846802, 0.004760621581226587, 0.15323500335216522, 0.8721427321434021, 0.9986798167228699, 0.9844195246696472]], "normalized_score": [0.2980613708496094], "tokens": [["\u2581Ohio", "\u2581State", "\u2581University", "\u2581and", "\u2581the", "\u2581University", "\u2581of", "\u2581Michigan"]], "entropy": [[1.4142591953277588, 0.14519883692264557, 1.3830814361572266, 0.8518640398979187, 2.0888915061950684, 0.6074159145355225, 0.012977026402950287, 0.13165190815925598]], "avg_entropy": [0.8294174829497933], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Michigan", "score_of_biggest_cluster": 0.2980613708496094, "score_of_first_answer": 0.2980613708496094, "max_score_of_first_answer": 0.9986798167228699, "min_score_of_first_answer": 0.004760621581226587, "avg_score": 0.2980613708496094, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3930243267531613, "std_of_first_answer": 0.3930243267531613, "avg_entropy_of_first_answer": 0.8294174829497933, "lex_sim": 1.0}
{"question": "who was selected for the 2018 football hall of fame", "answer": ["Brian Urlacher"], "scores": [[0.05121713876724243, 0.5190305709838867, 0.9898641705513, 0.9997738003730774]], "normalized_score": [0.4027368724346161], "tokens": [["\u2581Brian", "\u2581Ur", "la", "cher"]], "entropy": [[3.376335859298706, 1.1838765144348145, 0.09795893728733063, 0.002760724863037467]], "avg_entropy": [1.1652330089709722], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ray Lewis", "score_of_biggest_cluster": 0.4027368724346161, "score_of_first_answer": 0.4027368724346161, "max_score_of_first_answer": 0.9997738003730774, "min_score_of_first_answer": 0.05121713876724243, "avg_score": 0.4027368724346161, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3915165968516297, "std_of_first_answer": 0.3915165968516297, "avg_entropy_of_first_answer": 1.1652330089709722, "lex_sim": 1.0}
{"question": "when was fingerprinting first used by the police", "answer": ["1892"], "scores": [[0.7051541209220886, 0.9758179187774658, 0.7726722359657288, 0.32174569368362427, 0.6542932391166687]], "normalized_score": [0.6453371047973633], "tokens": [["\u2581", "1", "8", "9", "2"]], "entropy": [[2.006049156188965, 0.1544680893421173, 0.745421290397644, 1.289534568786621, 1.3260365724563599]], "avg_entropy": [1.1043019354343415], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the early 20th century", "score_of_biggest_cluster": 0.6453371047973633, "score_of_first_answer": 0.6453371047973633, "max_score_of_first_answer": 0.9758179187774658, "min_score_of_first_answer": 0.32174569368362427, "avg_score": 0.6453371047973633, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2123958008833792, "std_of_first_answer": 0.2123958008833792, "avg_entropy_of_first_answer": 1.1043019354343415, "lex_sim": 1.0}
{"question": "what is final season of game of thrones", "answer": ["Season 8"], "scores": [[0.3976980447769165, 0.8583936095237732, 0.9766209125518799]], "normalized_score": [0.6934076547622681], "tokens": [["\u2581Season", "\u2581", "8"]], "entropy": [[2.9402575492858887, 0.632102370262146, 0.15166762471199036]], "avg_entropy": [1.2413425147533417], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "The eighth", "score_of_biggest_cluster": 0.6934076547622681, "score_of_first_answer": 0.6934076547622681, "max_score_of_first_answer": 0.9766209125518799, "min_score_of_first_answer": 0.3976980447769165, "avg_score": 0.6934076547622681, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.24974871490972073, "std_of_first_answer": 0.24974871490972073, "avg_entropy_of_first_answer": 1.2413425147533417, "lex_sim": 1.0}
{"question": "what is the meaning of the dragon boat festival", "answer": ["Dragon Boat Festival, also often known as the Tuen Ng Festival or the Duanwu Festival, is a traditional holiday that is marked by the custom of eating rice dumplings, also known as Zongzi in Chinese"], "scores": [[0.1637067198753357, 0.54526686668396, 0.990327775478363, 0.9475610256195068, 0.27937746047973633, 0.5603206753730774, 0.2446921318769455, 0.9598056077957153, 0.9661295413970947, 0.6925793290138245, 0.25555989146232605, 0.9675575494766235, 0.9962021708488464, 0.5980714559555054, 0.8258426189422607, 0.9692153334617615, 0.846645712852478, 0.9991750121116638, 0.9644699096679688, 0.9981676340103149, 0.9941732287406921, 0.9355313181877136, 0.9890885949134827, 0.9898410439491272, 0.9967957139015198, 0.5636835694313049, 0.9979115128517151, 0.41854581236839294, 0.009118864312767982, 0.3654269576072693, 0.9413531422615051, 0.7182740569114685, 0.20894378423690796, 0.8824880123138428, 0.9978321194648743, 0.9999996423721313, 0.27169331908226013, 0.9981141090393066, 0.999910831451416, 0.997706413269043, 0.9999024868011475, 0.07187121361494064, 0.11442437767982483, 0.9461480379104614, 0.9927011728286743, 0.05386402830481529, 0.9860144853591919, 0.9865375757217407, 0.03687126561999321, 0.13505633175373077]], "normalized_score": [0.5243322253227234], "tokens": [["\u2581Dragon", "\u2581Bo", "at", "\u2581Festival", ",", "\u2581also", "\u2581often", "\u2581known", "\u2581as", "\u2581the", "\u2581T", "uen", "\u2581Ng", "\u2581Festival", "\u2581or", "\u2581the", "\u2581Du", "an", "w", "u", "\u2581Festival", ",", "\u2581is", "\u2581a", "\u2581traditional", "\u2581hol", "iday", "\u2581that", "\u2581is", "\u2581marked", "\u2581by", "\u2581the", "\u2581custom", "\u2581of", "\u2581e", "ating", "\u2581rice", "\u2581d", "um", "pl", "ings", ",", "\u2581also", "\u2581known", "\u2581as", "\u2581Z", "ong", "zi", "\u2581in", "\u2581Chinese"]], "entropy": [[3.770392417907715, 1.0146195888519287, 0.058900050818920135, 0.3053765296936035, 2.045456886291504, 2.4491424560546875, 1.081346035003662, 0.18277305364608765, 0.22908851504325867, 1.5161383152008057, 1.972890853881836, 0.20076599717140198, 0.03206133097410202, 1.1706936359405518, 0.7401514649391174, 0.20570796728134155, 0.7833521366119385, 0.008716283366084099, 0.16052818298339844, 0.017546271905303, 0.03709762543439865, 0.40865403413772583, 0.10335327684879303, 0.08317961543798447, 0.028075605630874634, 1.0907667875289917, 0.02044016309082508, 1.1233683824539185, 1.089228868484497, 2.562699317932129, 0.36754053831100464, 1.0636436939239502, 1.7623114585876465, 0.39881446957588196, 0.019352033734321594, 7.821961844456382e-06, 1.092589020729065, 0.019222494214773178, 0.001082717441022396, 0.01629204861819744, 0.0011836044723168015, 1.7186238765716553, 2.8912293910980225, 0.34943151473999023, 0.05915456637740135, 1.2216064929962158, 0.1141105443239212, 0.10170090198516846, 1.6146783828735352, 1.6763155460357666]], "avg_entropy": [0.7796280553823817], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "commemorating fealty and filial piety", "score_of_biggest_cluster": 0.5243322253227234, "score_of_first_answer": 0.5243322253227234, "max_score_of_first_answer": 0.9999996423721313, "min_score_of_first_answer": 0.009118864312767982, "avg_score": 0.5243322253227234, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3513394620425717, "std_of_first_answer": 0.3513394620425717, "avg_entropy_of_first_answer": 0.7796280553823817, "lex_sim": 1.0}
{"question": "what is the term for circular movement around a central point", "answer": ["orbit"], "scores": [[0.3654704988002777]], "normalized_score": [0.3654704988002777], "tokens": [["\u2581orbit"]], "entropy": [[2.7814602851867676]], "avg_entropy": [2.7814602851867676], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "angular rotation", "score_of_biggest_cluster": 0.3654704988002777, "score_of_first_answer": 0.3654704988002777, "max_score_of_first_answer": 0.3654704988002777, "min_score_of_first_answer": 0.3654704988002777, "avg_score": 0.3654704988002777, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.7814602851867676, "lex_sim": 1.0}
{"question": "when does planet of the apes come out 2017", "answer": ["July 14, 2017"], "scores": [[0.8147088289260864, 0.9917639493942261, 0.9895426034927368, 0.9874505996704102, 0.7009186148643494, 0.998090922832489, 0.9998220801353455, 0.9998966455459595, 0.9997181296348572, 0.9993294477462769]], "normalized_score": [0.9422501921653748], "tokens": [["\u2581July", "\u2581", "1", "4", ",", "\u2581", "2", "0", "1", "7"]], "entropy": [[1.2107967138290405, 0.05805685743689537, 0.07297513633966446, 0.09180302917957306, 0.7190208435058594, 0.016785716637969017, 0.0019412020919844508, 0.0012889800127595663, 0.0029225903563201427, 0.006803420837968588]], "avg_entropy": [0.21823944902280346], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "July 14, 2017", "score_of_biggest_cluster": 0.9422501921653748, "score_of_first_answer": 0.9422501921653748, "max_score_of_first_answer": 0.9998966455459595, "min_score_of_first_answer": 0.7009186148643494, "avg_score": 0.9422501921653748, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.09859532516199843, "std_of_first_answer": 0.09859532516199843, "avg_entropy_of_first_answer": 0.21823944902280346, "lex_sim": 1.0}
{"question": "who has played in the most premier league derbies", "answer": ["Ryan Giggs"], "scores": [[0.3460874557495117, 0.9088149070739746, 0.9936137795448303, 0.9996114373207092]], "normalized_score": [0.7476142048835754], "tokens": [["\u2581Ryan", "\u2581G", "ig", "gs"]], "entropy": [[3.1656203269958496, 0.47322073578834534, 0.05025330185890198, 0.004988847766071558]], "avg_entropy": [0.9235208031022921], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Neville Southall", "score_of_biggest_cluster": 0.7476142048835754, "score_of_first_answer": 0.7476142048835754, "max_score_of_first_answer": 0.9996114373207092, "min_score_of_first_answer": 0.3460874557495117, "avg_score": 0.7476142048835754, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27139880129924204, "std_of_first_answer": 0.27139880129924204, "avg_entropy_of_first_answer": 0.9235208031022921, "lex_sim": 1.0}
{"question": "who sang picking up pebbles and throwing them into the sea", "answer": ["The Byrds"], "scores": [[0.08617240190505981, 0.009696765802800655, 0.982384204864502, 0.9986397624015808]], "normalized_score": [0.16920816898345947], "tokens": [["\u2581The", "\u2581By", "rd", "s"]], "entropy": [[6.222874641418457, 5.914058685302734, 0.12892693281173706, 0.016034334897994995]], "avg_entropy": [3.070473648607731], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Matt Flinders", "score_of_biggest_cluster": 0.16920816898345947, "score_of_first_answer": 0.16920816898345947, "max_score_of_first_answer": 0.9986397624015808, "min_score_of_first_answer": 0.009696765802800655, "avg_score": 0.16920816898345947, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.47209864901532855, "std_of_first_answer": 0.47209864901532855, "avg_entropy_of_first_answer": 3.070473648607731, "lex_sim": 1.0}
{"question": "how long have we been celebrating pi day", "answer": ["34 years"], "scores": [[0.5299984216690063, 0.5636698603630066, 0.02422781102359295, 0.8617181181907654]], "normalized_score": [0.2810248136520386], "tokens": [["\u2581", "3", "4", "\u2581years"]], "entropy": [[2.2625014781951904, 1.0803776979446411, 1.2559435367584229, 0.7257549166679382]], "avg_entropy": [1.3311444073915482], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1988", "score_of_biggest_cluster": 0.2810248136520386, "score_of_first_answer": 0.2810248136520386, "max_score_of_first_answer": 0.8617181181907654, "min_score_of_first_answer": 0.02422781102359295, "avg_score": 0.2810248136520386, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30085255282129897, "std_of_first_answer": 0.30085255282129897, "avg_entropy_of_first_answer": 1.3311444073915482, "lex_sim": 1.0}
{"question": "what is the system of government in japan", "answer": ["Constitutional monarchy"], "scores": [[0.43368294835090637, 0.9980905652046204, 0.7253590226173401, 0.9991737008094788]], "normalized_score": [0.7484005093574524], "tokens": [["\u2581Constitution", "al", "\u2581mon", "archy"]], "entropy": [[1.7005162239074707, 0.018271533772349358, 0.8084940314292908, 0.009410781785845757]], "avg_entropy": [0.6341731427237391], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "constitutional monarchy", "score_of_biggest_cluster": 0.7484005093574524, "score_of_first_answer": 0.7484005093574524, "max_score_of_first_answer": 0.9991737008094788, "min_score_of_first_answer": 0.43368294835090637, "avg_score": 0.7484005093574524, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2335552433146666, "std_of_first_answer": 0.2335552433146666, "avg_entropy_of_first_answer": 0.6341731427237391, "lex_sim": 1.0}
{"question": "the vascular layer of the eye is the", "answer": ["Blood Vessel"], "scores": [[0.001732856035232544, 0.35306888818740845, 0.1506032794713974]], "normalized_score": [0.0451667420566082], "tokens": [["\u2581Blood", "\u2581V", "essel"]], "entropy": [[2.5041146278381348, 2.323458194732666, 0.5522406101226807]], "avg_entropy": [1.7932711442311604], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "middle layer", "score_of_biggest_cluster": 0.0451667420566082, "score_of_first_answer": 0.0451667420566082, "max_score_of_first_answer": 0.35306888818740845, "min_score_of_first_answer": 0.001732856035232544, "avg_score": 0.0451667420566082, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14398755084120707, "std_of_first_answer": 0.14398755084120707, "avg_entropy_of_first_answer": 1.7932711442311604, "lex_sim": 1.0}
{"question": "who played violet in a series of unfortunate events netflix", "answer": ["Malina Weissman"], "scores": [[0.5185403823852539, 0.9982866644859314, 0.9938356280326843, 0.9805012941360474, 0.9951387643814087]], "normalized_score": [0.8712380528450012], "tokens": [["\u2581Mal", "ina", "\u2581We", "iss", "man"]], "entropy": [[2.2600221633911133, 0.014997491613030434, 0.06577178835868835, 0.1156967431306839, 0.03737463802099228]], "avg_entropy": [0.49877256490290167], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Malina Weissman", "score_of_biggest_cluster": 0.8712380528450012, "score_of_first_answer": 0.8712380528450012, "max_score_of_first_answer": 0.9982866644859314, "min_score_of_first_answer": 0.5185403823852539, "avg_score": 0.8712380528450012, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1894577278434959, "std_of_first_answer": 0.1894577278434959, "avg_entropy_of_first_answer": 0.49877256490290167, "lex_sim": 1.0}
{"question": "who made possible the first reproduction of the human voice on the phonograph", "answer": ["Thomas Alva Edison"], "scores": [[0.8067627549171448, 0.11321135610342026, 0.9978323578834534, 0.9947787523269653, 0.9992020726203918]], "normalized_score": [0.6186067461967468], "tokens": [["\u2581Thomas", "\u2581Al", "va", "\u2581Ed", "ison"]], "entropy": [[1.2020549774169922, 0.5995129346847534, 0.020930590108036995, 0.04845745116472244, 0.008153246715664864]], "avg_entropy": [0.375821840018034], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Thomas Alva Edison", "score_of_biggest_cluster": 0.6186067461967468, "score_of_first_answer": 0.6186067461967468, "max_score_of_first_answer": 0.9992020726203918, "min_score_of_first_answer": 0.11321135610342026, "avg_score": 0.6186067461967468, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3426152214523446, "std_of_first_answer": 0.3426152214523446, "avg_entropy_of_first_answer": 0.375821840018034, "lex_sim": 1.0}
{"question": "who discovered the atom and in what year", "answer": ["Democritus estimated the universe consists of a finite number of tiny invisible indivisible bits"], "scores": [[0.08307766914367676, 0.9936416745185852, 0.9976491332054138, 0.9881166219711304, 0.0004111805174034089, 0.14193062484264374, 0.023111913353204727, 0.009507441893219948, 0.934423565864563, 0.12456630915403366, 0.7403897643089294, 0.7421795725822449, 0.9730286002159119, 0.041841112077236176, 0.04908503219485283, 0.026565780863165855, 0.7254648804664612, 0.9983314871788025, 0.9571817517280579, 0.004027984105050564]], "normalized_score": [0.1411788910627365], "tokens": [["\u2581Dem", "oc", "rit", "us", "\u2581estimated", "\u2581the", "\u2581universe", "\u2581consists", "\u2581of", "\u2581a", "\u2581finite", "\u2581number", "\u2581of", "\u2581tiny", "\u2581invisible", "\u2581ind", "iv", "is", "ible", "\u2581bits"]], "entropy": [[2.2598013877868652, 0.05693928524851799, 0.023279227316379547, 0.086921826004982, 2.421504497528076, 2.5497329235076904, 2.9185738563537598, 2.475356101989746, 0.486045241355896, 3.2695846557617188, 1.7247314453125, 1.3547170162200928, 0.207250714302063, 1.7826716899871826, 2.881702423095703, 1.9820640087127686, 0.6174229383468628, 0.019182113930583, 0.22457458078861237, 2.9437050819396973]], "avg_entropy": [1.514288050774485], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "early 1800s", "score_of_biggest_cluster": 0.1411788910627365, "score_of_first_answer": 0.1411788910627365, "max_score_of_first_answer": 0.9983314871788025, "min_score_of_first_answer": 0.0004111805174034089, "avg_score": 0.1411788910627365, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4359311329644905, "std_of_first_answer": 0.4359311329644905, "avg_entropy_of_first_answer": 1.514288050774485, "lex_sim": 1.0}
{"question": "when did the first movie of harry potter come out", "answer": ["November 16, 2001"], "scores": [[0.591969907283783, 0.9943509101867676, 0.8917496800422668, 0.9133490920066833, 0.9775229096412659, 0.9984768033027649, 0.9998505115509033, 0.999975323677063, 0.9989370703697205, 0.998935878276825]], "normalized_score": [0.9266557097434998], "tokens": [["\u2581November", "\u2581", "1", "6", ",", "\u2581", "2", "0", "0", "1"]], "entropy": [[1.6072643995285034, 0.04443558305501938, 0.4588982164859772, 0.4630149304866791, 0.13689246773719788, 0.013214977458119392, 0.0015191303100436926, 0.0003703309630509466, 0.008546637371182442, 0.010880360379815102]], "avg_entropy": [0.27450370337755886], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "16 November 2001", "score_of_biggest_cluster": 0.9266557097434998, "score_of_first_answer": 0.9266557097434998, "max_score_of_first_answer": 0.999975323677063, "min_score_of_first_answer": 0.591969907283783, "avg_score": 0.9266557097434998, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.12080947844625634, "std_of_first_answer": 0.12080947844625634, "avg_entropy_of_first_answer": 0.27450370337755886, "lex_sim": 1.0}
{"question": "locations for the film an englishman who went up a hill", "answer": ["County Antrim"], "scores": [[0.055880822241306305, 0.11552773416042328, 0.9997212290763855]], "normalized_score": [0.1861841082572937], "tokens": [["\u2581County", "\u2581An", "trim"]], "entropy": [[5.151141166687012, 2.785994529724121, 0.003234244417399168]], "avg_entropy": [2.646789980276177], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Llanrhaeadr-ym-Mochnant", "score_of_biggest_cluster": 0.1861841082572937, "score_of_first_answer": 0.1861841082572937, "max_score_of_first_answer": 0.9997212290763855, "min_score_of_first_answer": 0.055880822241306305, "avg_score": 0.1861841082572937, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4315592664497477, "std_of_first_answer": 0.4315592664497477, "avg_entropy_of_first_answer": 2.646789980276177, "lex_sim": 1.0}
{"question": "how many seasons of the glades were made", "answer": ["4"], "scores": [[0.7986696362495422, 0.7766475081443787]], "normalized_score": [0.7875816226005554], "tokens": [["\u2581", "4"]], "entropy": [[1.162806749343872, 0.7959820032119751]], "avg_entropy": [0.9793943762779236], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "four", "score_of_biggest_cluster": 0.7875816226005554, "score_of_first_answer": 0.7875816226005554, "max_score_of_first_answer": 0.7986696362495422, "min_score_of_first_answer": 0.7766475081443787, "avg_score": 0.7875816226005554, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.011011064052581787, "std_of_first_answer": 0.011011064052581787, "avg_entropy_of_first_answer": 0.9793943762779236, "lex_sim": 1.0}
{"question": "how many pieces in a terry's chocolate orange", "answer": ["15"], "scores": [[0.9605250358581543, 0.848203182220459, 0.05122809112071991]], "normalized_score": [0.3468744158744812], "tokens": [["\u2581", "1", "5"]], "entropy": [[0.3693667948246002, 0.5573790669441223, 1.8519628047943115]], "avg_entropy": [0.926236222187678], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "six", "score_of_biggest_cluster": 0.3468744158744812, "score_of_first_answer": 0.3468744158744812, "max_score_of_first_answer": 0.9605250358581543, "min_score_of_first_answer": 0.05122809112071991, "avg_score": 0.3468744158744812, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40477791281789166, "std_of_first_answer": 0.40477791281789166, "avg_entropy_of_first_answer": 0.926236222187678, "lex_sim": 1.0}
{"question": "when was the last easter fell on april 1", "answer": ["1940"], "scores": [[0.6043948531150818, 0.6222528219223022, 0.8939908742904663, 0.19360177218914032, 0.11380385607481003]], "normalized_score": [0.37491896748542786], "tokens": [["\u2581", "1", "9", "4", "0"]], "entropy": [[1.2947947978973389, 0.7080408930778503, 0.46705859899520874, 1.7307329177856445, 0.7822146415710449]], "avg_entropy": [0.9965683698654175], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2018", "score_of_biggest_cluster": 0.37491896748542786, "score_of_first_answer": 0.37491896748542786, "max_score_of_first_answer": 0.8939908742904663, "min_score_of_first_answer": 0.11380385607481003, "avg_score": 0.37491896748542786, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29088301922308396, "std_of_first_answer": 0.29088301922308396, "avg_entropy_of_first_answer": 0.9965683698654175, "lex_sim": 1.0}
{"question": "when did rachel have her baby on friends", "answer": ["May 6, 2002"], "scores": [[0.20605696737766266, 0.9825811982154846, 0.31024160981178284, 0.8258622884750366, 0.9930457472801208, 0.5849306583404541, 0.9997004270553589, 0.9937695860862732, 0.9489547610282898]], "normalized_score": [0.6732242703437805], "tokens": [["\u2581May", "\u2581", "6", ",", "\u2581", "2", "0", "0", "2"]], "entropy": [[3.8265390396118164, 0.11771447956562042, 1.6173124313354492, 0.6484075784683228, 0.05375978350639343, 0.6816360354423523, 0.003511629533022642, 0.03883258253335953, 0.27028992772102356]], "avg_entropy": [0.8064448319685956], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "May\u00a016,\u00a02002", "score_of_biggest_cluster": 0.6732242703437805, "score_of_first_answer": 0.6732242703437805, "max_score_of_first_answer": 0.9997004270553589, "min_score_of_first_answer": 0.20605696737766266, "avg_score": 0.6732242703437805, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29735515523428463, "std_of_first_answer": 0.29735515523428463, "avg_entropy_of_first_answer": 0.8064448319685956, "lex_sim": 1.0}
{"question": "who is the oldest wrestler that is still wrestling", "answer": ["The Rock"], "scores": [[0.21255265176296234, 0.005061817355453968]], "normalized_score": [0.032800957560539246], "tokens": [["\u2581The", "\u2581Rock"]], "entropy": [[4.029120445251465, 0.32630977034568787]], "avg_entropy": [2.1777151077985764], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dory Funk Jr.", "score_of_biggest_cluster": 0.032800957560539246, "score_of_first_answer": 0.032800957560539246, "max_score_of_first_answer": 0.21255265176296234, "min_score_of_first_answer": 0.005061817355453968, "avg_score": 0.032800957560539246, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10374541720375419, "std_of_first_answer": 0.10374541720375419, "avg_entropy_of_first_answer": 2.1777151077985764, "lex_sim": 1.0}
{"question": "who is the most played artist on spotify", "answer": ["Drake"], "scores": [[0.139256551861763, 0.9993067979812622]], "normalized_score": [0.373041570186615], "tokens": [["\u2581Dra", "ke"]], "entropy": [[1.5423108339309692, 0.008814346045255661]], "avg_entropy": [0.7755625899881124], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Justin Bieber", "score_of_biggest_cluster": 0.373041570186615, "score_of_first_answer": 0.373041570186615, "max_score_of_first_answer": 0.9993067979812622, "min_score_of_first_answer": 0.139256551861763, "avg_score": 0.373041570186615, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4300251230597496, "std_of_first_answer": 0.4300251230597496, "avg_entropy_of_first_answer": 0.7755625899881124, "lex_sim": 1.0}
{"question": "who was the girl that played cindy lou who", "answer": ["Whoopi Goldberg"], "scores": [[0.017693286761641502, 0.9574023485183716, 0.9735894203186035, 0.9925335645675659, 0.9977405071258545]], "normalized_score": [0.43914535641670227], "tokens": [["\u2581Who", "op", "i", "\u2581Gold", "berg"]], "entropy": [[4.1815056800842285, 0.3268283009529114, 0.12984760105609894, 0.06914274394512177, 0.022760853171348572]], "avg_entropy": [0.9460170358419419], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Taylor Michel Momsen", "score_of_biggest_cluster": 0.43914535641670227, "score_of_first_answer": 0.43914535641670227, "max_score_of_first_answer": 0.9977405071258545, "min_score_of_first_answer": 0.017693286761641502, "avg_score": 0.43914535641670227, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3853149007652687, "std_of_first_answer": 0.3853149007652687, "avg_entropy_of_first_answer": 0.9460170358419419, "lex_sim": 1.0}
{"question": "where does a brisket come from on a cow", "answer": ["Neck/Shoulder Clod"], "scores": [[0.007409641053527594, 0.9989690780639648, 0.016713863238692284, 0.2501182556152344, 0.9803493022918701, 0.0005507260211743414, 0.8219743967056274]], "normalized_score": [0.07530447095632553], "tokens": [["\u2581Ne", "ck", "/", "Should", "er", "\u2581Cl", "od"]], "entropy": [[4.032853126525879, 0.011376832611858845, 1.529293179512024, 2.850957155227661, 0.1066487729549408, 1.4376212358474731, 0.806259274482727]], "avg_entropy": [1.5392870824517948], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the breast or lower chest", "score_of_biggest_cluster": 0.07530447095632553, "score_of_first_answer": 0.07530447095632553, "max_score_of_first_answer": 0.9989690780639648, "min_score_of_first_answer": 0.0005507260211743414, "avg_score": 0.07530447095632553, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43847175550461276, "std_of_first_answer": 0.43847175550461276, "avg_entropy_of_first_answer": 1.5392870824517948, "lex_sim": 1.0}
{"question": "who sang how can you mend a broken heart first", "answer": ["Andy Gibb"], "scores": [[0.05745123699307442, 0.5182496309280396, 0.9977509379386902]], "normalized_score": [0.30970877408981323], "tokens": [["\u2581Andy", "\u2581Gib", "b"]], "entropy": [[2.5102314949035645, 0.919405460357666, 0.0176647137850523]], "avg_entropy": [1.1491005563487608], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bee Gees", "score_of_biggest_cluster": 0.30970877408981323, "score_of_first_answer": 0.30970877408981323, "max_score_of_first_answer": 0.9977509379386902, "min_score_of_first_answer": 0.05745123699307442, "avg_score": 0.30970877408981323, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38390105653473616, "std_of_first_answer": 0.38390105653473616, "avg_entropy_of_first_answer": 1.1491005563487608, "lex_sim": 1.0}
{"question": "who sang the song with reba does he love you", "answer": ["Trisha Yearwood"], "scores": [[0.16375364363193512, 0.9653217196464539, 0.9975479245185852, 0.9808269739151001, 0.9983204007148743]], "normalized_score": [0.6882271766662598], "tokens": [["\u2581Tr", "ish", "a", "\u2581Year", "wood"]], "entropy": [[3.2847681045532227, 0.17141327261924744, 0.02717394381761551, 0.15195536613464355, 0.01747705787420273]], "avg_entropy": [0.7305575489997864], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Linda Davis", "score_of_biggest_cluster": 0.6882271766662598, "score_of_first_answer": 0.6882271766662598, "max_score_of_first_answer": 0.9983204007148743, "min_score_of_first_answer": 0.16375364363193512, "avg_score": 0.6882271766662598, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.328924845120904, "std_of_first_answer": 0.328924845120904, "avg_entropy_of_first_answer": 0.7305575489997864, "lex_sim": 1.0}
{"question": "who wrote the french declaration of the rights of man and of the citizen", "answer": ["National Constituent Assembly"], "scores": [[0.030245058238506317, 0.449512243270874, 0.9971514344215393, 0.9968382120132446, 0.9927243590354919]], "normalized_score": [0.42220377922058105], "tokens": [["\u2581National", "\u2581Const", "itu", "ent", "\u2581Assembly"]], "entropy": [[3.878828525543213, 0.805544376373291, 0.024898651987314224, 0.026225386187434196, 0.06491847336292267]], "avg_entropy": [0.960083082690835], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "General Lafayette", "score_of_biggest_cluster": 0.42220377922058105, "score_of_first_answer": 0.42220377922058105, "max_score_of_first_answer": 0.9971514344215393, "min_score_of_first_answer": 0.030245058238506317, "avg_score": 0.42220377922058105, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39324048954723345, "std_of_first_answer": 0.39324048954723345, "avg_entropy_of_first_answer": 0.960083082690835, "lex_sim": 1.0}
{"question": "who are the artists in the traveling wilburys", "answer": ["Tom Petty, George Harrison, Jeff Lynne, Bob Dylan, Roy Orbison"], "scores": [[0.20472289621829987, 0.995826005935669, 0.9999498128890991, 0.7912611961364746, 0.16560661792755127, 0.9923725128173828, 0.9790164828300476, 0.35448408126831055, 0.9935164451599121, 0.9923460483551025, 0.9809617400169373, 0.5197567343711853, 0.9991433620452881, 0.999792754650116, 0.9984360337257385, 0.8182474374771118, 0.8956999778747559, 0.9833027720451355, 0.9966769218444824, 0.9985694885253906]], "normalized_score": [0.7515885233879089], "tokens": [["\u2581Tom", "\u2581Pet", "ty", ",", "\u2581George", "\u2581Harrison", ",", "\u2581Jeff", "\u2581Lyn", "ne", ",", "\u2581Bob", "\u2581D", "yl", "an", ",", "\u2581Roy", "\u2581Or", "b", "ison"]], "entropy": [[2.2830541133880615, 0.04196994751691818, 0.0007106605917215347, 0.8519966006278992, 1.7371068000793457, 0.07848678529262543, 0.1435559242963791, 1.2568820714950562, 0.0537027008831501, 0.04680235683917999, 0.13077211380004883, 0.9499198198318481, 0.010522298514842987, 0.0023190940264612436, 0.01419618260115385, 0.6788854598999023, 0.5482733845710754, 0.13233467936515808, 0.028770320117473602, 0.014879943802952766]], "avg_entropy": [0.4502570628770627], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Roy Orbison", "score_of_biggest_cluster": 0.7515885233879089, "score_of_first_answer": 0.7515885233879089, "max_score_of_first_answer": 0.9999498128890991, "min_score_of_first_answer": 0.16560661792755127, "avg_score": 0.7515885233879089, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.274488349886166, "std_of_first_answer": 0.274488349886166, "avg_entropy_of_first_answer": 0.4502570628770627, "lex_sim": 1.0}
{"question": "name of the boundary line between india and bangladesh", "answer": ["border"], "scores": [[0.029951589182019234]], "normalized_score": [0.029951585456728935], "tokens": [["\u2581border"]], "entropy": [[3.4853219985961914]], "avg_entropy": [3.4853219985961914], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "International Border (IB)", "score_of_biggest_cluster": 0.029951585456728935, "score_of_first_answer": 0.029951585456728935, "max_score_of_first_answer": 0.029951589182019234, "min_score_of_first_answer": 0.029951589182019234, "avg_score": 0.029951585456728935, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.4853219985961914, "lex_sim": 1.0}
{"question": "who drove the number 18 car in nascar", "answer": ["Daniel Suarez"], "scores": [[0.01953849196434021, 0.3118903338909149, 0.7041232585906982, 0.9994626641273499]], "normalized_score": [0.2559039890766144], "tokens": [["\u2581Daniel", "\u2581Su", "are", "z"]], "entropy": [[3.7219438552856445, 1.6894545555114746, 0.7290809750556946, 0.00639873743057251]], "avg_entropy": [1.5367195308208466], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kyle Busch", "score_of_biggest_cluster": 0.2559039890766144, "score_of_first_answer": 0.2559039890766144, "max_score_of_first_answer": 0.9994626641273499, "min_score_of_first_answer": 0.01953849196434021, "avg_score": 0.2559039890766144, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3731793324221057, "std_of_first_answer": 0.3731793324221057, "avg_entropy_of_first_answer": 1.5367195308208466, "lex_sim": 1.0}
{"question": "joined mexico and the united states to form nafta", "answer": ["Canada"], "scores": [[0.7794092297554016]], "normalized_score": [0.7794092297554016], "tokens": [["\u2581Canada"]], "entropy": [[1.5292770862579346]], "avg_entropy": [1.5292770862579346], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Canada", "score_of_biggest_cluster": 0.7794092297554016, "score_of_first_answer": 0.7794092297554016, "max_score_of_first_answer": 0.7794092297554016, "min_score_of_first_answer": 0.7794092297554016, "avg_score": 0.7794092297554016, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.5292770862579346, "lex_sim": 1.0}
{"question": "how many goals did mikel score for chelsea", "answer": ["115 goals"], "scores": [[0.9468106627464294, 0.3688885569572449, 0.30912020802497864, 0.0403280109167099, 0.7042720913887024]], "normalized_score": [0.31428709626197815], "tokens": [["\u2581", "1", "1", "5", "\u2581goals"]], "entropy": [[0.4411796033382416, 1.9897403717041016, 2.1288692951202393, 1.4315770864486694, 1.703556776046753]], "avg_entropy": [1.538984626531601], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "6", "score_of_biggest_cluster": 0.31428709626197815, "score_of_first_answer": 0.31428709626197815, "max_score_of_first_answer": 0.9468106627464294, "min_score_of_first_answer": 0.0403280109167099, "avg_score": 0.31428709626197815, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3171372721286107, "std_of_first_answer": 0.3171372721286107, "avg_entropy_of_first_answer": 1.538984626531601, "lex_sim": 1.0}
{"question": "who plays timon in lion king on broadway", "answer": ["Max Casella"], "scores": [[0.021510636433959007, 0.8328039050102234, 0.9990733861923218]], "normalized_score": [0.26157593727111816], "tokens": [["\u2581Max", "\u2581Cas", "ella"]], "entropy": [[3.1869263648986816, 1.245593547821045, 0.01021855603903532]], "avg_entropy": [1.4809128229195874], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Max Casella", "score_of_biggest_cluster": 0.26157593727111816, "score_of_first_answer": 0.26157593727111816, "max_score_of_first_answer": 0.9990733861923218, "min_score_of_first_answer": 0.021510636433959007, "avg_score": 0.26157593727111816, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4270663801805749, "std_of_first_answer": 0.4270663801805749, "avg_entropy_of_first_answer": 1.4809128229195874, "lex_sim": 1.0}
{"question": "who is the present rajya sabha speaker of india", "answer": ["Ram Nath Kovind"], "scores": [[0.008952880278229713, 0.8947875499725342, 0.9951063394546509, 0.9827147126197815, 0.9990390539169312, 0.9993001222610474]], "normalized_score": [0.44552943110466003], "tokens": [["\u2581Ram", "\u2581N", "ath", "\u2581K", "ov", "ind"]], "entropy": [[2.8409805297851562, 0.7367904782295227, 0.036900412291288376, 0.1437256932258606, 0.011062819510698318, 0.007896259427070618]], "avg_entropy": [0.6295593654115995], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Venkaiah Naidu", "score_of_biggest_cluster": 0.44552943110466003, "score_of_first_answer": 0.44552943110466003, "max_score_of_first_answer": 0.9993001222610474, "min_score_of_first_answer": 0.008952880278229713, "avg_score": 0.44552943110466003, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36158549926344513, "std_of_first_answer": 0.36158549926344513, "avg_entropy_of_first_answer": 0.6295593654115995, "lex_sim": 1.0}
{"question": "the sport psychology sub-field of performance enhancement deals primarily with", "answer": ["motivation, goal setting, goal attainment, arousal, focus and attention, and concentration"], "scores": [[0.01641346700489521, 0.9087775945663452, 0.1024310290813446, 0.04806803539395332, 0.6515238285064697, 0.854637086391449, 0.0021740507800132036, 0.15039722621440887, 0.996299684047699, 0.9964829683303833, 0.7747064232826233, 0.05437728390097618, 0.9995458722114563, 0.9989011287689209, 0.5295933485031128, 0.01207081787288189, 0.09274736791849136, 0.2776152789592743, 0.6459106206893921, 0.2125929594039917, 0.10245310515165329]], "normalized_score": [0.19544634222984314], "tokens": [["\u2581motiv", "ation", ",", "\u2581goal", "\u2581setting", ",", "\u2581goal", "\u2581att", "ain", "ment", ",", "\u2581ar", "ous", "al", ",", "\u2581focus", "\u2581and", "\u2581attention", ",", "\u2581and", "\u2581concentration"]], "entropy": [[5.365398406982422, 0.3927258849143982, 1.196241021156311, 3.637507438659668, 0.7698105573654175, 0.5822943449020386, 3.064056873321533, 3.37900447845459, 0.03218796104192734, 0.028672656044363976, 0.9809107184410095, 3.584845781326294, 0.0056051346473395824, 0.011073603294789791, 1.8783910274505615, 2.8800864219665527, 1.0644928216934204, 2.6591317653656006, 1.2797423601150513, 3.575827121734619, 4.073592662811279]], "avg_entropy": [1.925790430556628], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Arousal regulation", "score_of_biggest_cluster": 0.19544634222984314, "score_of_first_answer": 0.19544634222984314, "max_score_of_first_answer": 0.9995458722114563, "min_score_of_first_answer": 0.0021740507800132036, "avg_score": 0.19544634222984314, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39089540417633944, "std_of_first_answer": 0.39089540417633944, "avg_entropy_of_first_answer": 1.925790430556628, "lex_sim": 1.0}
{"question": "who got the most passing yards in the nfl", "answer": ["Patrick Mahomes"], "scores": [[0.10662780702114105, 0.8843441009521484, 0.9781070351600647, 0.9997525811195374]], "normalized_score": [0.5510520935058594], "tokens": [["\u2581Patrick", "\u2581Mah", "om", "es"]], "entropy": [[2.9255995750427246, 0.8853618502616882, 0.16357988119125366, 0.00312480004504323]], "avg_entropy": [0.9944165266351774], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Peyton Manning", "score_of_biggest_cluster": 0.5510520935058594, "score_of_first_answer": 0.5510520935058594, "max_score_of_first_answer": 0.9997525811195374, "min_score_of_first_answer": 0.10662780702114105, "avg_score": 0.5510520935058594, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.369507245853973, "std_of_first_answer": 0.369507245853973, "avg_entropy_of_first_answer": 0.9944165266351774, "lex_sim": 1.0}
{"question": "what year did bennie and the jets come out", "answer": ["1973"], "scores": [[0.8084760904312134, 0.9680810570716858, 0.990487277507782, 0.993752658367157, 0.4301753640174866]], "normalized_score": [0.8018078804016113], "tokens": [["\u2581", "1", "9", "7", "3"]], "entropy": [[1.2088623046875, 0.17839141190052032, 0.08060425519943237, 0.05178745836019516, 1.0628654956817627]], "avg_entropy": [0.5165021851658821], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "in 1973", "score_of_biggest_cluster": 0.8018078804016113, "score_of_first_answer": 0.8018078804016113, "max_score_of_first_answer": 0.993752658367157, "min_score_of_first_answer": 0.4301753640174866, "avg_score": 0.8018078804016113, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.21523231524042433, "std_of_first_answer": 0.21523231524042433, "avg_entropy_of_first_answer": 0.5165021851658821, "lex_sim": 1.0}
{"question": "how many students does monarch high school have", "answer": ["1000"], "scores": [[0.7761783003807068, 0.3597186803817749, 0.12279115617275238, 0.3783385455608368, 0.6352795362472534]], "normalized_score": [0.3829899728298187], "tokens": [["\u2581", "1", "0", "0", "0"]], "entropy": [[1.6066241264343262, 1.8621375560760498, 2.1472833156585693, 2.222221851348877, 1.6956077814102173]], "avg_entropy": [1.906774926185608], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2,406", "score_of_biggest_cluster": 0.3829899728298187, "score_of_first_answer": 0.3829899728298187, "max_score_of_first_answer": 0.7761783003807068, "min_score_of_first_answer": 0.12279115617275238, "avg_score": 0.3829899728298187, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22846136868863734, "std_of_first_answer": 0.22846136868863734, "avg_entropy_of_first_answer": 1.906774926185608, "lex_sim": 1.0}
{"question": "what does it mean when your baby comes out feet first", "answer": ["Breech birth"], "scores": [[0.22104480862617493, 0.9892428517341614, 0.9984122514724731, 0.5000917911529541]], "normalized_score": [0.5748251080513], "tokens": [["\u2581B", "ree", "ch", "\u2581birth"]], "entropy": [[3.516718864440918, 0.07078620046377182, 0.014026079326868057, 1.678955078125]], "avg_entropy": [1.3201215555891395], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A footling breech", "score_of_biggest_cluster": 0.5748251080513, "score_of_first_answer": 0.5748251080513, "max_score_of_first_answer": 0.9984122514724731, "min_score_of_first_answer": 0.22104480862617493, "avg_score": 0.5748251080513, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33165981368545877, "std_of_first_answer": 0.33165981368545877, "avg_entropy_of_first_answer": 1.3201215555891395, "lex_sim": 1.0}
{"question": "who is the queen of hearts in alice in wonderland", "answer": ["Anne Hathaway"], "scores": [[0.025051400065422058, 0.8569447994232178, 0.998354434967041, 0.9908015131950378]], "normalized_score": [0.3817368149757385], "tokens": [["\u2581Anne", "\u2581H", "ath", "away"]], "entropy": [[2.834179639816284, 0.9532675743103027, 0.016239391639828682, 0.056190963834524155]], "avg_entropy": [0.9649693924002349], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the primary antagonist", "score_of_biggest_cluster": 0.3817368149757385, "score_of_first_answer": 0.3817368149757385, "max_score_of_first_answer": 0.998354434967041, "min_score_of_first_answer": 0.025051400065422058, "avg_score": 0.3817368149757385, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4038881387624052, "std_of_first_answer": 0.4038881387624052, "avg_entropy_of_first_answer": 0.9649693924002349, "lex_sim": 1.0}
{"question": "when did lionel messi play his first game for barcelona", "answer": ["October 16, 2004"], "scores": [[0.8516495227813721, 0.9967648983001709, 0.9766904711723328, 0.9698314070701599, 0.9794257283210754, 0.9984238147735596, 0.9997013211250305, 0.9999721050262451, 0.9975183010101318, 0.9960500597953796]], "normalized_score": [0.9755855202674866], "tokens": [["\u2581October", "\u2581", "1", "6", ",", "\u2581", "2", "0", "0", "4"]], "entropy": [[0.8827692866325378, 0.026810819283127785, 0.14214320480823517, 0.19043099880218506, 0.12926514446735382, 0.014446951448917389, 0.003049582475796342, 0.00040464772609993815, 0.01876487024128437, 0.03338334709405899]], "avg_entropy": [0.14414688529795966], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2002", "score_of_biggest_cluster": 0.9755855202674866, "score_of_first_answer": 0.9755855202674866, "max_score_of_first_answer": 0.9999721050262451, "min_score_of_first_answer": 0.8516495227813721, "avg_score": 0.9755855202674866, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0429478019759485, "std_of_first_answer": 0.0429478019759485, "avg_entropy_of_first_answer": 0.14414688529795966, "lex_sim": 1.0}
{"question": "who plays the queen in victoria and abdul", "answer": ["Judi Dench"], "scores": [[0.8803563714027405, 0.9968440532684326, 0.9964776635169983, 0.9995187520980835]], "normalized_score": [0.9669100046157837], "tokens": [["\u2581J", "udi", "\u2581Den", "ch"]], "entropy": [[0.8283802270889282, 0.029990535229444504, 0.03297325223684311, 0.005762955639511347]], "avg_entropy": [0.2242767425486818], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Judi Dench", "score_of_biggest_cluster": 0.9669100046157837, "score_of_first_answer": 0.9669100046157837, "max_score_of_first_answer": 0.9995187520980835, "min_score_of_first_answer": 0.8803563714027405, "avg_score": 0.9669100046157837, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.05078739014063103, "std_of_first_answer": 0.05078739014063103, "avg_entropy_of_first_answer": 0.2242767425486818, "lex_sim": 1.0}
{"question": "who owns the majority of the us debt", "answer": ["China, Japan"], "scores": [[0.3250424265861511, 0.09148199111223221, 0.42055007815361023]], "normalized_score": [0.2321120798587799], "tokens": [["\u2581China", ",", "\u2581Japan"]], "entropy": [[3.077808380126953, 1.4510931968688965, 3.211358070373535]], "avg_entropy": [2.5800865491231284], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "China", "score_of_biggest_cluster": 0.2321120798587799, "score_of_first_answer": 0.2321120798587799, "max_score_of_first_answer": 0.42055007815361023, "min_score_of_first_answer": 0.09148199111223221, "avg_score": 0.2321120798587799, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13822606058987805, "std_of_first_answer": 0.13822606058987805, "avg_entropy_of_first_answer": 2.5800865491231284, "lex_sim": 1.0}
{"question": "who devised the fingerprint classification still used today in europe and the us", "answer": ["Henry Faulds"], "scores": [[0.5927007794380188, 0.7287781238555908, 0.9932076334953308, 0.9992585778236389]], "normalized_score": [0.8091651797294617], "tokens": [["\u2581Henry", "\u2581Fa", "ul", "ds"]], "entropy": [[2.027344226837158, 1.5807533264160156, 0.053569868206977844, 0.008631167002022266]], "avg_entropy": [0.9175746471155435], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hem Chandra Bose", "score_of_biggest_cluster": 0.8091651797294617, "score_of_first_answer": 0.8091651797294617, "max_score_of_first_answer": 0.9992585778236389, "min_score_of_first_answer": 0.5927007794380188, "avg_score": 0.8091651797294617, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1745227926935915, "std_of_first_answer": 0.1745227926935915, "avg_entropy_of_first_answer": 0.9175746471155435, "lex_sim": 1.0}
{"question": "who wrote i'm in the mood for dancing", "answer": ["Nick Van Eede"], "scores": [[0.0597928911447525, 0.03565976396203041, 0.5417263507843018, 0.9965214729309082]], "normalized_score": [0.1841932237148285], "tokens": [["\u2581Nick", "\u2581Van", "\u2581E", "ede"]], "entropy": [[4.431300640106201, 3.323420524597168, 2.7351837158203125, 0.029202532023191452]], "avg_entropy": [2.6297768531367183], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ben Findon", "score_of_biggest_cluster": 0.1841932237148285, "score_of_first_answer": 0.1841932237148285, "max_score_of_first_answer": 0.9965214729309082, "min_score_of_first_answer": 0.03565976396203041, "avg_score": 0.1841932237148285, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3950079050215125, "std_of_first_answer": 0.3950079050215125, "avg_entropy_of_first_answer": 2.6297768531367183, "lex_sim": 1.0}
{"question": "when do you pray the chaplet of divine mercy", "answer": ["The Chaplet of Divine Mercy can be prayed anytime after the celebration of Mass."], "scores": [[0.07514890283346176, 0.5749293565750122, 0.9987215399742126, 0.7897714376449585, 0.6786296963691711, 0.9997649788856506, 0.9962765574455261, 0.9990154504776001, 0.10251487791538239, 0.9861683249473572, 0.752192497253418, 0.9987825751304626, 0.2509755790233612, 0.6795469522476196, 0.0026031380984932184, 0.4769672453403473, 0.048037681728601456, 0.9974958300590515, 0.9929295182228088, 0.3583192527294159, 0.303867906332016]], "normalized_score": [0.3844950795173645], "tokens": [["\u2581The", "\u2581Chap", "let", "\u2581of", "\u2581Div", "ine", "\u2581Mer", "cy", "\u2581can", "\u2581be", "\u2581pray", "ed", "\u2581any", "time", "\u2581after", "\u2581the", "\u2581celebr", "ation", "\u2581of", "\u2581Mass", "."]], "entropy": [[4.417914867401123, 2.0817809104919434, 0.011081523261964321, 0.8573964238166809, 0.9982123374938965, 0.0029408601112663746, 0.03470347821712494, 0.011340627446770668, 1.3956202268600464, 0.12428148835897446, 0.8297754526138306, 0.013439087197184563, 1.9734128713607788, 0.7945786118507385, 1.9066495895385742, 2.0436229705810547, 3.864508867263794, 0.02206461876630783, 0.06401275843381882, 1.6877694129943848, 2.5288405418395996]], "avg_entropy": [1.2220927393285645], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Good Friday", "score_of_biggest_cluster": 0.3844950795173645, "score_of_first_answer": 0.3844950795173645, "max_score_of_first_answer": 0.9997649788856506, "min_score_of_first_answer": 0.0026031380984932184, "avg_score": 0.3844950795173645, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36418699081699935, "std_of_first_answer": 0.36418699081699935, "avg_entropy_of_first_answer": 1.2220927393285645, "lex_sim": 1.0}
{"question": "where can carbon be found in the biosphere", "answer": ["oceans, plants, rocks, soil, air"], "scores": [[0.01373220607638359, 0.9853982925415039, 0.9996927976608276, 0.37339460849761963, 0.03430112078785896, 0.786457359790802, 0.01826743222773075, 0.5422190427780151, 0.2538248896598816, 0.6311218738555908, 0.12517818808555603]], "normalized_score": [0.2051333338022232], "tokens": [["\u2581o", "ce", "ans", ",", "\u2581plants", ",", "\u2581rocks", ",", "\u2581soil", ",", "\u2581air"]], "entropy": [[4.243199348449707, 0.09814973175525665, 0.003923970740288496, 1.286426305770874, 3.345950126647949, 0.7829341292381287, 2.743851661682129, 1.0804264545440674, 2.9334468841552734, 0.9818181991577148, 2.9157891273498535]], "avg_entropy": [1.8559923581355675], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "other living organisms", "score_of_biggest_cluster": 0.2051333338022232, "score_of_first_answer": 0.2051333338022232, "max_score_of_first_answer": 0.9996927976608276, "min_score_of_first_answer": 0.01373220607638359, "avg_score": 0.2051333338022232, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3617580712597532, "std_of_first_answer": 0.3617580712597532, "avg_entropy_of_first_answer": 1.8559923581355675, "lex_sim": 1.0}
{"question": "when do new episodes of ncis los angeles return", "answer": ["February 27"], "scores": [[0.11480738967657089, 0.9846770763397217, 0.183192640542984, 0.28327351808547974]], "normalized_score": [0.27675431966781616], "tokens": [["\u2581February", "\u2581", "2", "7"]], "entropy": [[2.9847054481506348, 0.1044701337814331, 1.2852668762207031, 2.1244301795959473]], "avg_entropy": [1.6247181594371796], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "May\u00a013,\u00a02018", "score_of_biggest_cluster": 0.27675431966781616, "score_of_first_answer": 0.27675431966781616, "max_score_of_first_answer": 0.9846770763397217, "min_score_of_first_answer": 0.11480738967657089, "avg_score": 0.27675431966781616, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34767900831248066, "std_of_first_answer": 0.34767900831248066, "avg_entropy_of_first_answer": 1.6247181594371796, "lex_sim": 1.0}
{"question": "who is doing 2018 super bowl half time show", "answer": ["Justin Timberlake"], "scores": [[0.8902369737625122, 0.9940628409385681, 0.9996713399887085, 0.9993059635162354, 0.9998201727867126]], "normalized_score": [0.9756171107292175], "tokens": [["\u2581Justin", "\u2581Tim", "ber", "la", "ke"]], "entropy": [[0.7475067377090454, 0.05660777539014816, 0.004148711450397968, 0.0083710802718997, 0.002360955346375704]], "avg_entropy": [0.1637990520335734], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Justin Timberlake", "score_of_biggest_cluster": 0.9756171107292175, "score_of_first_answer": 0.9756171107292175, "max_score_of_first_answer": 0.9998201727867126, "min_score_of_first_answer": 0.8902369737625122, "avg_score": 0.9756171107292175, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.043244757263176455, "std_of_first_answer": 0.043244757263176455, "avg_entropy_of_first_answer": 0.1637990520335734, "lex_sim": 1.0}
{"question": "what color is the golden gate bridge in san francisco", "answer": ["International Orange"], "scores": [[0.3819837272167206, 0.7301839590072632]], "normalized_score": [0.5281271934509277], "tokens": [["\u2581International", "\u2581Orange"]], "entropy": [[2.566667079925537, 0.5959910154342651]], "avg_entropy": [1.5813290476799011], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "orange vermilion called international orange", "score_of_biggest_cluster": 0.5281271934509277, "score_of_first_answer": 0.5281271934509277, "max_score_of_first_answer": 0.7301839590072632, "min_score_of_first_answer": 0.3819837272167206, "avg_score": 0.5281271934509277, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1741001158952713, "std_of_first_answer": 0.1741001158952713, "avg_entropy_of_first_answer": 1.5813290476799011, "lex_sim": 1.0}
{"question": "the names of the beaches on d day", "answer": ["Omaha, Utah, Gold, Sword, Juno & Ower"], "scores": [[0.218742236495018, 0.996264636516571, 0.9996466636657715, 0.2253977507352829, 0.6577491164207458, 0.9997621178627014, 0.906765878200531, 0.6484460234642029, 0.9222701787948608, 0.12009324133396149, 0.999717652797699, 0.72211754322052, 0.9335831999778748, 0.9993413090705872, 0.013693882152438164, 0.04166676849126816, 0.015499807894229889]], "normalized_score": [0.34289783239364624], "tokens": [["\u2581O", "ma", "ha", ",", "\u2581Ut", "ah", ",", "\u2581Gold", ",", "\u2581S", "word", ",", "\u2581Jun", "o", "\u2581&", "\u2581O", "wer"]], "entropy": [[2.660205841064453, 0.027691565454006195, 0.004063393920660019, 1.0497695207595825, 1.030585765838623, 0.003090315032750368, 0.4355340898036957, 0.9451581835746765, 0.41304898262023926, 0.49666234850883484, 0.0031685614958405495, 0.9529837369918823, 0.29925084114074707, 0.007937525399029255, 0.9076679944992065, 4.478044509887695, 2.996835708618164]], "avg_entropy": [0.9830411108594168], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Juno Beach", "score_of_biggest_cluster": 0.34289783239364624, "score_of_first_answer": 0.34289783239364624, "max_score_of_first_answer": 0.9997621178627014, "min_score_of_first_answer": 0.013693882152438164, "avg_score": 0.34289783239364624, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39376730930765674, "std_of_first_answer": 0.39376730930765674, "avg_entropy_of_first_answer": 0.9830411108594168, "lex_sim": 1.0}
{"question": "when was the last time an american won the new york marathon", "answer": ["2017"], "scores": [[0.5279338955879211, 0.6888834834098816, 0.9907857179641724, 0.5767412185668945, 0.08231725543737411]], "normalized_score": [0.443236380815506], "tokens": [["\u2581", "2", "0", "1", "7"]], "entropy": [[1.9639334678649902, 0.6879439353942871, 0.07797162234783173, 0.6957716345787048, 1.7204608917236328]], "avg_entropy": [1.0292163103818894], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2009", "score_of_biggest_cluster": 0.443236380815506, "score_of_first_answer": 0.443236380815506, "max_score_of_first_answer": 0.9907857179641724, "min_score_of_first_answer": 0.08231725543737411, "avg_score": 0.443236380815506, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2935260949194088, "std_of_first_answer": 0.2935260949194088, "avg_entropy_of_first_answer": 1.0292163103818894, "lex_sim": 1.0}
{"question": "who was the first black person to register to vote", "answer": ["Thomas Mundy Peterson"], "scores": [[0.24284406006336212, 0.5630239248275757, 0.651327908039093, 0.9736486077308655, 0.9898961782455444, 0.9863590598106384]], "normalized_score": [0.6626438498497009], "tokens": [["\u2581Thomas", "\u2581M", "und", "y", "\u2581Pet", "erson"]], "entropy": [[4.6335601806640625, 2.636845588684082, 1.5075771808624268, 0.201343834400177, 0.08435212075710297, 0.08453995734453201]], "avg_entropy": [1.5247031437853973], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Thomas Mundy Peterson", "score_of_biggest_cluster": 0.6626438498497009, "score_of_first_answer": 0.6626438498497009, "max_score_of_first_answer": 0.9898961782455444, "min_score_of_first_answer": 0.24284406006336212, "avg_score": 0.6626438498497009, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.27805858294774344, "std_of_first_answer": 0.27805858294774344, "avg_entropy_of_first_answer": 1.5247031437853973, "lex_sim": 1.0}
{"question": "who has the most rushing yards in a super bowl", "answer": ["Emmitt Smith"], "scores": [[0.15513303875923157, 0.9825981855392456, 0.9959690570831299]], "normalized_score": [0.5334684252738953], "tokens": [["\u2581Em", "mitt", "\u2581Smith"]], "entropy": [[3.1666476726531982, 0.11351430416107178, 0.03426387906074524]], "avg_entropy": [1.104808618625005], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Timmy Smith", "score_of_biggest_cluster": 0.5334684252738953, "score_of_first_answer": 0.5334684252738953, "max_score_of_first_answer": 0.9959690570831299, "min_score_of_first_answer": 0.15513303875923157, "avg_score": 0.5334684252738953, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.393260241662538, "std_of_first_answer": 0.393260241662538, "avg_entropy_of_first_answer": 1.104808618625005, "lex_sim": 1.0}
{"question": "when is the strictly come dancing results show recorded", "answer": ["On Sunday"], "scores": [[0.0076609281823039055, 0.37757936120033264]], "normalized_score": [0.05378298461437225], "tokens": [["\u2581On", "\u2581Sunday"]], "entropy": [[2.5837478637695312, 2.385321617126465]], "avg_entropy": [2.484534740447998], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Saturday", "score_of_biggest_cluster": 0.05378298461437225, "score_of_first_answer": 0.05378298461437225, "max_score_of_first_answer": 0.37757936120033264, "min_score_of_first_answer": 0.0076609281823039055, "avg_score": 0.05378298461437225, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18495921650901437, "std_of_first_answer": 0.18495921650901437, "avg_entropy_of_first_answer": 2.484534740447998, "lex_sim": 1.0}
{"question": "where does the saskatchewan river start and end", "answer": ["Saskatchewan River Forks"], "scores": [[0.23537307977676392, 0.9782671928405762, 0.9785566329956055, 0.9995799660682678, 0.9995266199111938, 0.6003162860870361, 0.1363058239221573, 0.6827293634414673, 0.995818555355072]], "normalized_score": [0.614663302898407], "tokens": [["\u2581S", "ask", "atch", "ew", "an", "\u2581River", "\u2581F", "ork", "s"]], "entropy": [[4.132500648498535, 0.19586181640625, 0.1473923623561859, 0.004877359140664339, 0.005845456384122372, 1.5391032695770264, 2.091498613357544, 0.7407605648040771, 0.03924097120761871]], "avg_entropy": [0.9885645624146693], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "central Saskatchewan", "score_of_biggest_cluster": 0.614663302898407, "score_of_first_answer": 0.614663302898407, "max_score_of_first_answer": 0.9995799660682678, "min_score_of_first_answer": 0.1363058239221573, "avg_score": 0.614663302898407, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3258199214188924, "std_of_first_answer": 0.3258199214188924, "avg_entropy_of_first_answer": 0.9885645624146693, "lex_sim": 1.0}
{"question": "where does the formation of atp take place", "answer": ["cell"], "scores": [[0.054340511560440063]], "normalized_score": [0.054340507835149765], "tokens": [["\u2581cell"]], "entropy": [[3.3504478931427]], "avg_entropy": [3.3504478931427], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "plasma membrane in bacteria", "score_of_biggest_cluster": 0.054340507835149765, "score_of_first_answer": 0.054340507835149765, "max_score_of_first_answer": 0.054340511560440063, "min_score_of_first_answer": 0.054340511560440063, "avg_score": 0.054340507835149765, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.3504478931427, "lex_sim": 1.0}
{"question": "phase change from gas to solid is called", "answer": ["freezing"], "scores": [[0.1574242115020752, 0.9890725016593933]], "normalized_score": [0.39459341764450073], "tokens": [["\u2581free", "zing"]], "entropy": [[2.1346945762634277, 0.06765386462211609]], "avg_entropy": [1.101174220442772], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Deposition", "score_of_biggest_cluster": 0.39459341764450073, "score_of_first_answer": 0.39459341764450073, "max_score_of_first_answer": 0.9890725016593933, "min_score_of_first_answer": 0.1574242115020752, "avg_score": 0.39459341764450073, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41582414507865906, "std_of_first_answer": 0.41582414507865906, "avg_entropy_of_first_answer": 1.101174220442772, "lex_sim": 1.0}
{"question": "where are more than half your bones located", "answer": ["legs & pelvis"], "scores": [[0.04903180152177811, 0.006963702384382486, 0.05212913453578949, 0.9069478511810303]], "normalized_score": [0.06338626891374588], "tokens": [["\u2581legs", "\u2581&", "\u2581pel", "vis"]], "entropy": [[3.4331934452056885, 1.1284496784210205, 2.443063735961914, 0.32428231835365295]], "avg_entropy": [1.832247294485569], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "the appendicular skeleton", "score_of_biggest_cluster": 0.06338626891374588, "score_of_first_answer": 0.06338626891374588, "max_score_of_first_answer": 0.9069478511810303, "min_score_of_first_answer": 0.006963702384382486, "avg_score": 0.06338626891374588, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3775352375419277, "std_of_first_answer": 0.3775352375419277, "avg_entropy_of_first_answer": 1.832247294485569, "lex_sim": 1.0}
{"question": "what started the war between us and japan", "answer": ["japan's attack on pearl harbor"], "scores": [[0.00687286164611578, 0.9583706855773926, 0.11998934298753738, 0.9991681575775146, 0.571675717830658, 0.9748402833938599, 0.6446152925491333, 0.9998646974563599, 0.9939425587654114, 0.9947367310523987, 0.9208852648735046]], "normalized_score": [0.47187063097953796], "tokens": [["\u2581jap", "an", "'", "s", "\u2581attack", "\u2581on", "\u2581p", "ear", "l", "\u2581har", "bor"]], "entropy": [[3.5848605632781982, 0.21659687161445618, 2.421896457672119, 0.010763045400381088, 2.248175859451294, 0.18059755861759186, 1.3752989768981934, 0.001994375605136156, 0.04114712402224541, 0.04705740138888359, 0.28594985604286194]], "avg_entropy": [0.9467580081810328], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "attack on Pearl Harbor", "score_of_biggest_cluster": 0.47187063097953796, "score_of_first_answer": 0.47187063097953796, "max_score_of_first_answer": 0.9998646974563599, "min_score_of_first_answer": 0.00687286164611578, "avg_score": 0.47187063097953796, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.35146016968583976, "std_of_first_answer": 0.35146016968583976, "avg_entropy_of_first_answer": 0.9467580081810328, "lex_sim": 1.0}
{"question": "who caused to build qutub minar in delhi", "answer": ["Prithviraj Chauhan"], "scores": [[0.005991168785840273, 0.9896835088729858, 0.9858128428459167, 0.4159174859523773, 0.9921796321868896, 0.9982609152793884, 0.996516227722168]], "normalized_score": [0.42240843176841736], "tokens": [["\u2581Pr", "ith", "vi", "raj", "\u2581Ch", "au", "han"]], "entropy": [[1.4056602716445923, 0.07065436244010925, 0.1056516170501709, 0.7959233522415161, 0.06176944822072983, 0.016443390399217606, 0.031194785609841347]], "avg_entropy": [0.35532817537231104], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Iltutmish", "score_of_biggest_cluster": 0.42240843176841736, "score_of_first_answer": 0.42240843176841736, "max_score_of_first_answer": 0.9982609152793884, "min_score_of_first_answer": 0.005991168785840273, "avg_score": 0.42240843176841736, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36968941454798654, "std_of_first_answer": 0.36968941454798654, "avg_entropy_of_first_answer": 0.35532817537231104, "lex_sim": 1.0}
{"question": "what was the lowest barometric pressure for hurricane irma", "answer": ["The lowest barometric pressure was recorded in Naples, Florida at 934 mb"], "scores": [[0.0037110024131834507, 0.7404933571815491, 0.2808510363101959, 0.9958562254905701, 0.9871314167976379, 0.15721051394939423, 0.052351489663124084, 0.2830711603164673, 0.028275594115257263, 0.9977814555168152, 0.7846499085426331, 0.689638078212738, 0.21270804107189178, 0.9406585693359375, 0.814458966255188, 0.2161519080400467, 0.03683915361762047, 0.31733790040016174, 0.5067170858383179]], "normalized_score": [0.25581374764442444], "tokens": [["\u2581The", "\u2581lowest", "\u2581bar", "ometric", "\u2581pressure", "\u2581was", "\u2581recorded", "\u2581in", "\u2581Nap", "les", ",", "\u2581Florida", "\u2581at", "\u2581", "9", "3", "4", "\u2581m", "b"]], "entropy": [[0.5765736103057861, 1.7546782493591309, 2.7449378967285156, 0.034752532839775085, 0.09901448339223862, 2.822547674179077, 1.2142117023468018, 1.9872510433197021, 3.6171298027038574, 0.018249735236167908, 1.1903347969055176, 1.154563069343567, 2.367342710494995, 0.42549994587898254, 0.7162100076675415, 0.9112146496772766, 1.9667835235595703, 1.8266377449035645, 0.867713212966919]], "avg_entropy": [1.3839813890425783], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "914 mbar (hPa)", "score_of_biggest_cluster": 0.25581374764442444, "score_of_first_answer": 0.25581374764442444, "max_score_of_first_answer": 0.9977814555168152, "min_score_of_first_answer": 0.0037110024131834507, "avg_score": 0.25581374764442444, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36134492466941015, "std_of_first_answer": 0.36134492466941015, "avg_entropy_of_first_answer": 1.3839813890425783, "lex_sim": 1.0}
{"question": "who played mary in christmas with the kranks", "answer": ["Jamie"], "scores": [[0.1016463190317154, 0.9998682737350464]], "normalized_score": [0.3187991976737976], "tokens": [["\u2581Jam", "ie"]], "entropy": [[3.2809829711914062, 0.001771801384165883]], "avg_entropy": [1.641377386287786], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Felicity Huffman", "score_of_biggest_cluster": 0.3187991976737976, "score_of_first_answer": 0.3187991976737976, "max_score_of_first_answer": 0.9998682737350464, "min_score_of_first_answer": 0.1016463190317154, "avg_score": 0.3187991976737976, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4491109773516655, "std_of_first_answer": 0.4491109773516655, "avg_entropy_of_first_answer": 1.641377386287786, "lex_sim": 1.0}
{"question": "what is the name of the compound p4010", "answer": ["phosphorus"], "scores": [[0.10992579162120819, 0.9505288600921631, 0.6963056921958923]], "normalized_score": [0.4174664616584778], "tokens": [["\u2581ph", "osph", "orus"]], "entropy": [[4.642033100128174, 0.2844736576080322, 1.236583948135376]], "avg_entropy": [2.054363568623861], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Phosphorus pentoxide", "score_of_biggest_cluster": 0.4174664616584778, "score_of_first_answer": 0.4174664616584778, "max_score_of_first_answer": 0.9505288600921631, "min_score_of_first_answer": 0.10992579162120819, "avg_score": 0.4174664616584778, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35199184422621954, "std_of_first_answer": 0.35199184422621954, "avg_entropy_of_first_answer": 2.054363568623861, "lex_sim": 1.0}
{"question": "who is command sergeant major of the army", "answer": ["Sergeant Major of the Army Daniel A. Dailey"], "scores": [[0.05358213931322098, 0.9977013468742371, 0.9654410481452942, 0.8042061924934387, 0.9937169551849365, 0.9955125451087952, 0.3169180452823639, 0.9686819911003113, 0.9925899505615234, 0.9992437362670898, 0.9925858378410339]], "normalized_score": [0.6709737181663513], "tokens": [["\u2581Serge", "ant", "\u2581Major", "\u2581of", "\u2581the", "\u2581Army", "\u2581Daniel", "\u2581A", ".", "\u2581Da", "iley"]], "entropy": [[2.9637343883514404, 0.023146577179431915, 0.19904640316963196, 0.8657452464103699, 0.04130422696471214, 0.035041581839323044, 2.3121371269226074, 0.16181674599647522, 0.061329662799835205, 0.007367245852947235, 0.060409508645534515]], "avg_entropy": [0.6119162467393008], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Daniel A. Dailey", "score_of_biggest_cluster": 0.6709737181663513, "score_of_first_answer": 0.6709737181663513, "max_score_of_first_answer": 0.9992437362670898, "min_score_of_first_answer": 0.05358213931322098, "avg_score": 0.6709737181663513, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.31158377143072374, "std_of_first_answer": 0.31158377143072374, "avg_entropy_of_first_answer": 0.6119162467393008, "lex_sim": 1.0}
{"question": "who plays the robot maid in richie rich", "answer": ["Gloria Gory"], "scores": [[0.005050030071288347, 0.9325107932090759, 0.18730323016643524, 0.0023258975706994534]], "normalized_score": [0.03784608095884323], "tokens": [["\u2581Gl", "oria", "\u2581G", "ory"]], "entropy": [[5.1867289543151855, 0.34592029452323914, 3.679950714111328, 2.7361085414886475]], "avg_entropy": [2.9871771261096], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brooke Wexler", "score_of_biggest_cluster": 0.03784608095884323, "score_of_first_answer": 0.03784608095884323, "max_score_of_first_answer": 0.9325107932090759, "min_score_of_first_answer": 0.0023258975706994534, "avg_score": 0.03784608095884323, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38309610271664307, "std_of_first_answer": 0.38309610271664307, "avg_entropy_of_first_answer": 2.9871771261096, "lex_sim": 1.0}
{"question": "who was the viceroy when the simon commission visited india", "answer": ["Freeman Freeman-Thomas"], "scores": [[0.0413035973906517, 0.9958227872848511, 0.9939225316047668, 0.9945821166038513, 0.9637195467948914, 0.9937118291854858, 0.9884640574455261]], "normalized_score": [0.6279210448265076], "tokens": [["\u2581Fre", "eman", "\u2581Fre", "eman", "-", "Th", "omas"]], "entropy": [[3.0302035808563232, 0.04520546644926071, 0.059283848851919174, 0.056126587092876434, 0.25613218545913696, 0.05329030752182007, 0.07087382674217224]], "avg_entropy": [0.510159400424787], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Lord Irwin", "score_of_biggest_cluster": 0.6279210448265076, "score_of_first_answer": 0.6279210448265076, "max_score_of_first_answer": 0.9958227872848511, "min_score_of_first_answer": 0.0413035973906517, "avg_score": 0.6279210448265076, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3315683860537966, "std_of_first_answer": 0.3315683860537966, "avg_entropy_of_first_answer": 0.510159400424787, "lex_sim": 1.0}
{"question": "which government had more power under the articles of confederation", "answer": ["None"], "scores": [[0.022192027419805527]], "normalized_score": [0.022192025557160378], "tokens": [["\u2581None"]], "entropy": [[3.797607898712158]], "avg_entropy": [3.797607898712158], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the states", "score_of_biggest_cluster": 0.022192025557160378, "score_of_first_answer": 0.022192025557160378, "max_score_of_first_answer": 0.022192027419805527, "min_score_of_first_answer": 0.022192027419805527, "avg_score": 0.022192025557160378, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.797607898712158, "lex_sim": 1.0}
{"question": "when does dragon ball super episode 113 start", "answer": ["November 6, 2016"], "scores": [[0.05886869505047798, 0.9713850021362305, 0.03853946551680565, 0.8426704406738281, 0.9932214021682739, 0.9693980813026428, 0.9978300929069519, 0.9804989695549011, 0.5612041354179382]], "normalized_score": [0.4632112681865692], "tokens": [["\u2581November", "\u2581", "6", ",", "\u2581", "2", "0", "1", "6"]], "entropy": [[3.3080201148986816, 0.16942012310028076, 1.7650275230407715, 0.8272362947463989, 0.06291808933019638, 0.19019320607185364, 0.021432949230074883, 0.12058480083942413, 1.1947323083877563]], "avg_entropy": [0.8510628232939376], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "October 29, 2017", "score_of_biggest_cluster": 0.4632112681865692, "score_of_first_answer": 0.4632112681865692, "max_score_of_first_answer": 0.9978300929069519, "min_score_of_first_answer": 0.03853946551680565, "avg_score": 0.4632112681865692, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3780579926715057, "std_of_first_answer": 0.3780579926715057, "avg_entropy_of_first_answer": 0.8510628232939376, "lex_sim": 1.0}
{"question": "how many scholarships do division 2 football teams get", "answer": ["30"], "scores": [[0.8538143038749695, 0.8987312912940979, 0.22943927347660065]], "normalized_score": [0.5604716539382935], "tokens": [["\u2581", "3", "0"]], "entropy": [[1.127974271774292, 0.5195374488830566, 0.6632513403892517]], "avg_entropy": [0.7702543536822001], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "36.0", "score_of_biggest_cluster": 0.5604716539382935, "score_of_first_answer": 0.5604716539382935, "max_score_of_first_answer": 0.8987312912940979, "min_score_of_first_answer": 0.22943927347660065, "avg_score": 0.5604716539382935, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3054711332893996, "std_of_first_answer": 0.3054711332893996, "avg_entropy_of_first_answer": 0.7702543536822001, "lex_sim": 1.0}
{"question": "when is the next episode of flash airing", "answer": ["4/10/2019"], "scores": [[0.0329161062836647, 0.03716299310326576, 0.1392597258090973, 0.4874541461467743, 0.2644486129283905, 0.9067825078964233, 0.719839334487915, 0.9989370703697205, 0.9959314465522766, 0.868356466293335]], "normalized_score": [0.32305940985679626], "tokens": [["\u2581", "4", "/", "1", "0", "/", "2", "0", "1", "9"]], "entropy": [[2.6020116806030273, 1.77908194065094, 3.2101645469665527, 1.5100165605545044, 2.160546064376831, 0.44870713353157043, 0.6139958500862122, 0.010787907987833023, 0.03328970447182655, 0.518570601940155]], "avg_entropy": [1.2887171991169453], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "May\u00a08,\u00a02018", "score_of_biggest_cluster": 0.32305940985679626, "score_of_first_answer": 0.32305940985679626, "max_score_of_first_answer": 0.9989370703697205, "min_score_of_first_answer": 0.0329161062836647, "avg_score": 0.32305940985679626, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3797613475646528, "std_of_first_answer": 0.3797613475646528, "avg_entropy_of_first_answer": 1.2887171991169453, "lex_sim": 1.0}
{"question": "who was the first to say i'm going to disney world", "answer": ["Charles Gibson"], "scores": [[0.012963710352778435, 0.05188150331377983, 0.9677959084510803]], "normalized_score": [0.08666462451219559], "tokens": [["\u2581Charles", "\u2581Gib", "son"]], "entropy": [[4.558788776397705, 2.967968702316284, 0.18583530187606812]], "avg_entropy": [2.570864260196686], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jeana Yeager", "score_of_biggest_cluster": 0.08666462451219559, "score_of_first_answer": 0.08666462451219559, "max_score_of_first_answer": 0.9677959084510803, "min_score_of_first_answer": 0.012963710352778435, "avg_score": 0.08666462451219559, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44122535419647024, "std_of_first_answer": 0.44122535419647024, "avg_entropy_of_first_answer": 2.570864260196686, "lex_sim": 1.0}
{"question": "how long do former presidents get secret service protection", "answer": ["464"], "scores": [[0.25724583864212036, 0.012797757983207703, 0.011320691555738449, 0.008007955737411976]], "normalized_score": [0.023373259231448174], "tokens": [["\u2581", "4", "6", "4"]], "entropy": [[3.87555193901062, 0.6325833797454834, 2.6726531982421875, 2.5545008182525635]], "avg_entropy": [2.4338223338127136], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "lifetime protection", "score_of_biggest_cluster": 0.023373259231448174, "score_of_first_answer": 0.023373259231448174, "max_score_of_first_answer": 0.25724583864212036, "min_score_of_first_answer": 0.008007955737411976, "avg_score": 0.023373259231448174, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10676775687546457, "std_of_first_answer": 0.10676775687546457, "avg_entropy_of_first_answer": 2.4338223338127136, "lex_sim": 1.0}
{"question": "who is the actress that plays penny on the big bang theory", "answer": ["Kaley Cuoco"], "scores": [[0.9346811175346375, 0.9633996486663818, 0.9993849992752075, 0.8486918807029724, 0.9951454997062683]], "normalized_score": [0.946603000164032], "tokens": [["\u2581K", "ale", "y", "\u2581Cu", "oco"]], "entropy": [[0.5209878087043762, 0.20847243070602417, 0.007326094433665276, 0.543716549873352, 0.04204931482672691]], "avg_entropy": [0.2645104397088289], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Kaley Christine Cuoco", "score_of_biggest_cluster": 0.946603000164032, "score_of_first_answer": 0.946603000164032, "max_score_of_first_answer": 0.9993849992752075, "min_score_of_first_answer": 0.8486918807029724, "avg_score": 0.946603000164032, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.05502621379436143, "std_of_first_answer": 0.05502621379436143, "avg_entropy_of_first_answer": 0.2645104397088289, "lex_sim": 1.0}
{"question": "which apostle spoke at the council of jerusalem", "answer": ["Peter"], "scores": [[0.3980630040168762]], "normalized_score": [0.3980630040168762], "tokens": [["\u2581Peter"]], "entropy": [[1.9884190559387207]], "avg_entropy": [1.9884190559387207], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paul", "score_of_biggest_cluster": 0.3980630040168762, "score_of_first_answer": 0.3980630040168762, "max_score_of_first_answer": 0.3980630040168762, "min_score_of_first_answer": 0.3980630040168762, "avg_score": 0.3980630040168762, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.9884190559387207, "lex_sim": 1.0}
{"question": "where was the tv show in the heat of the night filmed", "answer": ["Covington, Georgia, USA"], "scores": [[0.20122723281383514, 0.9989913105964661, 0.7919665575027466, 0.9254589676856995, 0.27265670895576477, 0.8074387311935425]], "normalized_score": [0.5647284984588623], "tokens": [["\u2581Cov", "ington", ",", "\u2581Georgia", ",", "\u2581USA"]], "entropy": [[4.1991167068481445, 0.01095553021878004, 0.8346891403198242, 0.4095415771007538, 0.9293239116668701, 0.742792546749115]], "avg_entropy": [1.1877365688172479], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Decatur in Dekalb County", "score_of_biggest_cluster": 0.5647284984588623, "score_of_first_answer": 0.5647284984588623, "max_score_of_first_answer": 0.9989913105964661, "min_score_of_first_answer": 0.20122723281383514, "avg_score": 0.5647284984588623, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3121983404152702, "std_of_first_answer": 0.3121983404152702, "avg_entropy_of_first_answer": 1.1877365688172479, "lex_sim": 1.0}
{"question": "what is the word for clarified butter in the balkans and middle east", "answer": ["ghee"], "scores": [[0.6748252511024475, 0.9973024129867554, 0.99892657995224]], "normalized_score": [0.8760265111923218], "tokens": [["\u2581g", "he", "e"]], "entropy": [[1.7040590047836304, 0.03168441355228424, 0.011524369940161705]], "avg_entropy": [0.5824225960920254], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "smen", "score_of_biggest_cluster": 0.8760265111923218, "score_of_first_answer": 0.8760265111923218, "max_score_of_first_answer": 0.99892657995224, "min_score_of_first_answer": 0.6748252511024475, "avg_score": 0.8760265111923218, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.15240145421349466, "std_of_first_answer": 0.15240145421349466, "avg_entropy_of_first_answer": 0.5824225960920254, "lex_sim": 1.0}
{"question": "where did the book small steps take place", "answer": ["moon"], "scores": [[0.07575425505638123]], "normalized_score": [0.07575424760580063], "tokens": [["\u2581moon"]], "entropy": [[5.42817497253418]], "avg_entropy": [5.42817497253418], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Austin, Texas", "score_of_biggest_cluster": 0.07575424760580063, "score_of_first_answer": 0.07575424760580063, "max_score_of_first_answer": 0.07575425505638123, "min_score_of_first_answer": 0.07575425505638123, "avg_score": 0.07575424760580063, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.42817497253418, "lex_sim": 1.0}
{"question": "who was the author of the art of war", "answer": ["Sun Tzu"], "scores": [[0.9450992941856384, 0.9490627646446228, 0.9892330169677734]], "normalized_score": [0.9609268307685852], "tokens": [["\u2581Sun", "\u2581T", "zu"]], "entropy": [[0.5209324955940247, 0.3236271142959595, 0.074643075466156]], "avg_entropy": [0.3064008951187134], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Sun Tzu", "score_of_biggest_cluster": 0.9609268307685852, "score_of_first_answer": 0.9609268307685852, "max_score_of_first_answer": 0.9892330169677734, "min_score_of_first_answer": 0.9450992941856384, "avg_score": 0.9609268307685852, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.01993640934013047, "std_of_first_answer": 0.01993640934013047, "avg_entropy_of_first_answer": 0.3064008951187134, "lex_sim": 1.0}
{"question": "how much money did the film titanic make", "answer": ["$659.41 million"], "scores": [[0.6686267256736755, 0.21808645129203796, 0.6503013372421265, 0.7354410290718079, 0.3124920725822449, 0.8047764897346497, 0.01850438304245472, 0.574809730052948]], "normalized_score": [0.3418601453304291], "tokens": [["\u2581$", "6", "5", "9", ".", "4", "1", "\u2581million"]], "entropy": [[1.5159177780151367, 1.298160195350647, 0.7589282989501953, 0.6836327314376831, 0.8912817239761353, 0.636175274848938, 1.4457764625549316, 1.8967759609222412]], "avg_entropy": [1.1408310532569885], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "$2.18 billion", "score_of_biggest_cluster": 0.3418601453304291, "score_of_first_answer": 0.3418601453304291, "max_score_of_first_answer": 0.8047764897346497, "min_score_of_first_answer": 0.01850438304245472, "avg_score": 0.3418601453304291, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2625308756484562, "std_of_first_answer": 0.2625308756484562, "avg_entropy_of_first_answer": 1.1408310532569885, "lex_sim": 1.0}
{"question": "when did australia 2 win the america's cup", "answer": ["January 26, 1987"], "scores": [[0.06494665145874023, 0.9923728108406067, 0.5307644009590149, 0.5486028790473938, 0.9693869948387146, 0.9983951449394226, 0.7903486490249634, 0.9966022968292236, 0.9115037322044373, 0.937644362449646]], "normalized_score": [0.6437731385231018], "tokens": [["\u2581January", "\u2581", "2", "6", ",", "\u2581", "1", "9", "8", "7"]], "entropy": [[1.7563848495483398, 0.056271571666002274, 1.3058358430862427, 1.6045341491699219, 0.19574475288391113, 0.016184506937861443, 0.5157627463340759, 0.025601960718631744, 0.32555705308914185, 0.3346524238586426]], "avg_entropy": [0.6136529857292772], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1983", "score_of_biggest_cluster": 0.6437731385231018, "score_of_first_answer": 0.6437731385231018, "max_score_of_first_answer": 0.9983951449394226, "min_score_of_first_answer": 0.06494665145874023, "avg_score": 0.6437731385231018, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2905934953758808, "std_of_first_answer": 0.2905934953758808, "avg_entropy_of_first_answer": 0.6136529857292772, "lex_sim": 1.0}
{"question": "two main types of research methods used in psychology", "answer": ["experiment and correlation"], "scores": [[0.10566070675849915, 0.4064500033855438, 0.06031154468655586]], "normalized_score": [0.13733263313770294], "tokens": [["\u2581experiment", "\u2581and", "\u2581correlation"]], "entropy": [[3.506723165512085, 1.2217029333114624, 1.7426254749298096]], "avg_entropy": [2.157017191251119], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "quantitative data", "score_of_biggest_cluster": 0.13733263313770294, "score_of_first_answer": 0.13733263313770294, "max_score_of_first_answer": 0.4064500033855438, "min_score_of_first_answer": 0.06031154468655586, "avg_score": 0.13733263313770294, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.15360214844292114, "std_of_first_answer": 0.15360214844292114, "avg_entropy_of_first_answer": 2.157017191251119, "lex_sim": 1.0}
{"question": "who owns the crown plaza hotel in chicago illinois", "answer": ["Rocky Wirtz"], "scores": [[0.002312619239091873, 0.043258052319288254, 0.06122281774878502, 0.9854212403297424, 0.998214840888977]], "normalized_score": [0.090362049639225], "tokens": [["\u2581Rock", "y", "\u2581W", "irt", "z"]], "entropy": [[4.346713066101074, 3.774148941040039, 2.6209921836853027, 0.1285892128944397, 0.018476903438568115]], "avg_entropy": [2.177784061431885], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "InterContinental Hotels Group", "score_of_biggest_cluster": 0.090362049639225, "score_of_first_answer": 0.090362049639225, "max_score_of_first_answer": 0.998214840888977, "min_score_of_first_answer": 0.002312619239091873, "avg_score": 0.090362049639225, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4688568165889265, "std_of_first_answer": 0.4688568165889265, "avg_entropy_of_first_answer": 2.177784061431885, "lex_sim": 1.0}
{"question": "when was the first episode of scooby doo", "answer": ["September 13, 1997 (Ultimate Sewer Shark)"], "scores": [[0.5418403148651123, 0.9941471815109253, 0.9900553226470947, 0.9795839190483093, 0.9713682532310486, 0.9937847852706909, 0.9775865077972412, 0.9997796416282654, 0.03133421018719673, 0.8022067546844482, 0.1292773187160492, 0.015785930678248405, 0.002376331714913249, 0.8750086426734924, 0.0011211773380637169, 0.05097698047757149, 0.004659354221075773, 0.8523165583610535, 0.38818106055259705]], "normalized_score": [0.1760222613811493], "tokens": [["\u2581September", "\u2581", "1", "3", ",", "\u2581", "1", "9", "9", "7", "\u2581(", "U", "lt", "imate", "\u2581Se", "wer", "\u2581Sh", "ark", ")"]], "entropy": [[1.7401793003082275, 0.04621466249227524, 0.07311324030160904, 0.14198024570941925, 0.18143892288208008, 0.04501693695783615, 0.10832571238279343, 0.0026067255530506372, 0.2635820508003235, 0.8270398378372192, 2.3713274002075195, 4.708433151245117, 0.21500961482524872, 0.5455154180526733, 3.059539794921875, 3.29000186920166, 4.314848899841309, 0.9619234800338745, 3.2792065143585205]], "avg_entropy": [1.3776475672585595], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "September\u00a013,\u00a01969", "score_of_biggest_cluster": 0.1760222613811493, "score_of_first_answer": 0.1760222613811493, "max_score_of_first_answer": 0.9997796416282654, "min_score_of_first_answer": 0.0011211773380637169, "avg_score": 0.1760222613811493, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42918234230020896, "std_of_first_answer": 0.42918234230020896, "avg_entropy_of_first_answer": 1.3776475672585595, "lex_sim": 1.0}
{"question": "the resting stage of the cell cycle is", "answer": ["interphase"], "scores": [[0.6519657373428345, 0.973003089427948]], "normalized_score": [0.7964701056480408], "tokens": [["\u2581inter", "phase"]], "entropy": [[1.1959905624389648, 0.19059793651103973]], "avg_entropy": [0.6932942494750023], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Gap 0", "score_of_biggest_cluster": 0.7964701056480408, "score_of_first_answer": 0.7964701056480408, "max_score_of_first_answer": 0.973003089427948, "min_score_of_first_answer": 0.6519657373428345, "avg_score": 0.7964701056480408, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.16051867604255676, "std_of_first_answer": 0.16051867604255676, "avg_entropy_of_first_answer": 0.6932942494750023, "lex_sim": 1.0}
{"question": "who is the owner of the crowne plaza", "answer": ["Quite a few!"], "scores": [[0.0031378010753542185, 0.0021322777029126883, 0.33761879801750183, 0.6997596025466919, 0.02779579721391201, 0.002528644632548094, 0.009886582382023335, 0.037965744733810425, 0.9948031306266785, 0.15341848134994507, 0.9991531372070312, 0.0021733399480581284, 0.06989925354719162, 0.07524535059928894, 0.03563085198402405, 0.009558041580021381, 0.0851883739233017, 0.02426043152809143, 0.07646774500608444, 0.135619655251503]], "normalized_score": [0.04327504336833954], "tokens": [["\u2581Qu", "ite", "\u2581a", "\u2581few", "!", "\u2581[", "S", "ever", "al", "\u2581hot", "els", "\u2581appear", "\u2581on", "\u2581screen", ".", "\u2581Search", "\u2581for", "\u2581hotel", "\u2581chain", "]"]], "entropy": [[3.0088696479797363, 2.717475652694702, 4.189914703369141, 1.5892789363861084, 3.373293876647949, 2.027925491333008, 5.928487777709961, 3.742166519165039, 0.048835135996341705, 4.549721717834473, 0.010685274377465248, 3.8194656372070312, 2.5066118240356445, 2.1609296798706055, 1.7984504699707031, 4.7626953125, 4.074881553649902, 3.6737570762634277, 4.124410629272461, 3.972838878631592]], "avg_entropy": [3.1040347897447647], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "InterContinental Hotels Group", "score_of_biggest_cluster": 0.04327504336833954, "score_of_first_answer": 0.04327504336833954, "max_score_of_first_answer": 0.9991531372070312, "min_score_of_first_answer": 0.0021322777029126883, "avg_score": 0.04327504336833954, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.311944778689576, "std_of_first_answer": 0.311944778689576, "avg_entropy_of_first_answer": 3.1040347897447647, "lex_sim": 1.0}
{"question": "when did sierra nevada brewery open in asheville", "answer": ["March 2"], "scores": [[0.08678834140300751, 0.9756365418434143, 0.4111812114715576]], "normalized_score": [0.32653337717056274], "tokens": [["\u2581March", "\u2581", "2"]], "entropy": [[2.9821724891662598, 0.13196855783462524, 1.4888312816619873]], "avg_entropy": [1.5343241095542908], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "early 2014", "score_of_biggest_cluster": 0.32653337717056274, "score_of_first_answer": 0.32653337717056274, "max_score_of_first_answer": 0.9756365418434143, "min_score_of_first_answer": 0.08678834140300751, "avg_score": 0.32653337717056274, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36725584142735984, "std_of_first_answer": 0.36725584142735984, "avg_entropy_of_first_answer": 1.5343241095542908, "lex_sim": 1.0}
{"question": "how did early humans make use of stones during the prehistoric period", "answer": ["Stone Age"], "scores": [[0.05275639519095421, 0.412936270236969]], "normalized_score": [0.1475975215435028], "tokens": [["\u2581Stone", "\u2581Age"]], "entropy": [[4.44803524017334, 1.5288951396942139]], "avg_entropy": [2.988465189933777], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "hunt", "score_of_biggest_cluster": 0.1475975215435028, "score_of_first_answer": 0.1475975215435028, "max_score_of_first_answer": 0.412936270236969, "min_score_of_first_answer": 0.05275639519095421, "avg_score": 0.1475975215435028, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1800899375230074, "std_of_first_answer": 0.1800899375230074, "avg_entropy_of_first_answer": 2.988465189933777, "lex_sim": 1.0}
{"question": "bosnia and herzegovina croatia macedonia and slovenia all used to be parts of", "answer": ["Yugoslavia"], "scores": [[0.9125673174858093, 0.9986411929130554, 0.9958569407463074, 0.9991219639778137]], "normalized_score": [0.9758261442184448], "tokens": [["\u2581Y", "ug", "oslav", "ia"]], "entropy": [[0.607597827911377, 0.013655190356075764, 0.034860994666814804, 0.009542894549667835]], "avg_entropy": [0.16641422687098384], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Yugoslavia", "score_of_biggest_cluster": 0.9758261442184448, "score_of_first_answer": 0.9758261442184448, "max_score_of_first_answer": 0.9991219639778137, "min_score_of_first_answer": 0.9125673174858093, "avg_score": 0.9758261442184448, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.036959626436274734, "std_of_first_answer": 0.036959626436274734, "avg_entropy_of_first_answer": 0.16641422687098384, "lex_sim": 1.0}
{"question": "how long is the bridge between new brunswick and prince edward island", "answer": [".04 kilometers"], "scores": [[0.001203823951072991, 0.05072762817144394, 0.19248397648334503, 0.06547266989946365, 0.9973401427268982]], "normalized_score": [0.059844061732292175], "tokens": [["\u2581.", "0", "4", "\u2581kilom", "eters"]], "entropy": [[0.7052152752876282, 2.4947752952575684, 2.340606451034546, 2.68233585357666, 0.02840445376932621]], "avg_entropy": [1.6502674657851457], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "12.9-kilometre", "score_of_biggest_cluster": 0.059844061732292175, "score_of_first_answer": 0.059844061732292175, "max_score_of_first_answer": 0.9973401427268982, "min_score_of_first_answer": 0.001203823951072991, "avg_score": 0.059844061732292175, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3733174837904331, "std_of_first_answer": 0.3733174837904331, "avg_entropy_of_first_answer": 1.6502674657851457, "lex_sim": 1.0}
{"question": "what order do the captain america movies go in", "answer": ["Captain America: The First Avenger, Captain America: The Winter Soldier, Avengers: Infinity War, Avengers: Endgame, Captain America: Civil War"], "scores": [[0.6472057700157166, 0.9962467551231384, 0.9279500246047974, 0.9413853883743286, 0.8549128174781799, 0.999397873878479, 0.9992194175720215, 0.6386475563049316, 0.6844543218612671, 0.998704195022583, 0.99662846326828, 0.6798416376113892, 0.9968414306640625, 0.9966024160385132, 0.9994264841079712, 0.9690607786178589, 0.5301284193992615, 0.9992700219154358, 0.9952724575996399, 0.6990377902984619, 0.9995476603507996, 0.999958872795105, 0.9996334314346313, 0.8014021515846252, 0.7183128595352173, 0.9993922710418701, 0.9937407374382019, 0.9911511540412903, 0.9980200529098511, 0.5638225078582764, 0.6896153688430786, 0.9912883043289185, 0.9924401044845581, 0.973313570022583, 0.999423623085022]], "normalized_score": [0.8781842589378357], "tokens": [["\u2581Captain", "\u2581America", ":", "\u2581The", "\u2581First", "\u2581Aven", "ger", ",", "\u2581Captain", "\u2581America", ":", "\u2581The", "\u2581Winter", "\u2581Sold", "ier", ",", "\u2581Aven", "gers", ":", "\u2581In", "fin", "ity", "\u2581War", ",", "\u2581Aven", "gers", ":", "\u2581End", "game", ",", "\u2581Captain", "\u2581America", ":", "\u2581Civil", "\u2581War"]], "entropy": [[1.6323041915893555, 0.033597834408283234, 0.4286084771156311, 0.2916032373905182, 0.4438769817352295, 0.0066950879991054535, 0.006901346147060394, 1.431464672088623, 0.8853634595870972, 0.01294796634465456, 0.032361797988414764, 0.6790426969528198, 0.030148539692163467, 0.027520611882209778, 0.005885571241378784, 0.18896621465682983, 0.8386774063110352, 0.006318630650639534, 0.04076660796999931, 0.7399177551269531, 0.004631633870303631, 0.0005997453117743134, 0.004186991602182388, 0.6573178172111511, 0.7210087776184082, 0.005757750011980534, 0.0484897643327713, 0.0608639158308506, 0.01757056638598442, 0.8693028688430786, 0.8669400215148926, 0.05627605691552162, 0.06448741257190704, 0.12925641238689423, 0.006382829509675503]], "avg_entropy": [0.32217261859415364], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Captain America: Civil War", "score_of_biggest_cluster": 0.8781842589378357, "score_of_first_answer": 0.8781842589378357, "max_score_of_first_answer": 0.999958872795105, "min_score_of_first_answer": 0.5301284193992615, "avg_score": 0.8781842589378357, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1517659650044162, "std_of_first_answer": 0.1517659650044162, "avg_entropy_of_first_answer": 0.32217261859415364, "lex_sim": 1.0}
{"question": "who were the first settlers in san antonio", "answer": ["Spaniards and Franciscan missionaries"], "scores": [[0.05502048507332802, 0.994196891784668, 0.8670699596405029, 0.038883864879608154, 0.11656396836042404, 0.4752812683582306, 0.26651710271835327, 0.9846891164779663]], "normalized_score": [0.2682532072067261], "tokens": [["\u2581Sp", "ani", "ards", "\u2581and", "\u2581Francis", "can", "\u2581mission", "aries"]], "entropy": [[4.284537315368652, 0.057321518659591675, 0.40809935331344604, 0.9724752306938171, 3.3930816650390625, 0.7209522724151611, 1.7272248268127441, 0.11628443002700806]], "avg_entropy": [1.4599970765411854], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Spanish", "score_of_biggest_cluster": 0.2682532072067261, "score_of_first_answer": 0.2682532072067261, "max_score_of_first_answer": 0.994196891784668, "min_score_of_first_answer": 0.038883864879608154, "avg_score": 0.2682532072067261, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39076663894481595, "std_of_first_answer": 0.39076663894481595, "avg_entropy_of_first_answer": 1.4599970765411854, "lex_sim": 1.0}
{"question": "when did goku first go super saiyan 4", "answer": ["Dragon Ball Z: Dragon Fist Explosion!"], "scores": [[0.3267890512943268, 0.9737808704376221, 0.30637261271476746, 0.7985033988952637, 0.007589088287204504, 0.6743693947792053, 0.9545613527297974, 0.893255889415741, 0.9934293627738953, 0.9967897534370422, 0.9962050318717957, 0.17996470630168915, 0.007182725239545107, 0.0016944773960858583, 0.9817366600036621, 0.06600792706012726, 0.035382721573114395, 0.1972886621952057, 0.13608567416667938]], "normalized_score": [0.19844159483909607], "tokens": [["\u2581Dragon", "\u2581Ball", "\u2581Z", ":", "\u2581Dragon", "\u2581F", "ist", "\u2581Ex", "pl", "os", "ion", "!", "\u2581A", "\u2581Veg", "eta", "-", "B", "last", "!!"]], "entropy": [[3.961378574371338, 0.15198495984077454, 1.3317859172821045, 1.1802235841751099, 2.1710195541381836, 1.5718541145324707, 0.2774343490600586, 0.6190974116325378, 0.05504528060555458, 0.025290178135037422, 0.028048688545823097, 0.9779797792434692, 1.5497915744781494, 3.373199224472046, 0.13657814264297485, 4.077211856842041, 3.7684268951416016, 3.1444740295410156, 2.873236656188965]], "avg_entropy": [1.6460031984668029], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dragon Ball GT", "score_of_biggest_cluster": 0.19844159483909607, "score_of_first_answer": 0.19844159483909607, "max_score_of_first_answer": 0.9967897534370422, "min_score_of_first_answer": 0.0016944773960858583, "avg_score": 0.19844159483909607, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4107519084067333, "std_of_first_answer": 0.4107519084067333, "avg_entropy_of_first_answer": 1.6460031984668029, "lex_sim": 1.0}
{"question": "when did houston go to the american league", "answer": ["It never did."], "scores": [[0.004344903863966465, 0.08013815432786942, 0.5125035047531128, 0.608144223690033]], "normalized_score": [0.10206591337919235], "tokens": [["\u2581It", "\u2581never", "\u2581did", "."]], "entropy": [[1.8361047506332397, 3.048945426940918, 1.7535041570663452, 1.4799957275390625]], "avg_entropy": [2.0296375155448914], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2013", "score_of_biggest_cluster": 0.10206591337919235, "score_of_first_answer": 0.10206591337919235, "max_score_of_first_answer": 0.608144223690033, "min_score_of_first_answer": 0.004344903863966465, "avg_score": 0.10206591337919235, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2626095904268249, "std_of_first_answer": 0.2626095904268249, "avg_entropy_of_first_answer": 2.0296375155448914, "lex_sim": 1.0}
{"question": "type of disappearing lake found in limestone areas in the west of ireland", "answer": ["limestone landform"], "scores": [[0.0024754228070378304, 0.9477130174636841, 0.004255290143191814, 0.5712698101997375]], "normalized_score": [0.04886794462800026], "tokens": [["\u2581lim", "estone", "\u2581land", "form"]], "entropy": [[1.9104557037353516, 0.3944850564002991, 3.097334384918213, 1.752311110496521]], "avg_entropy": [1.7886465638875961], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "turlough", "score_of_biggest_cluster": 0.04886794462800026, "score_of_first_answer": 0.04886794462800026, "max_score_of_first_answer": 0.9477130174636841, "min_score_of_first_answer": 0.0024754228070378304, "avg_score": 0.04886794462800026, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40080635683740123, "std_of_first_answer": 0.40080635683740123, "avg_entropy_of_first_answer": 1.7886465638875961, "lex_sim": 1.0}
{"question": "who did the chiefs play in the playoffs", "answer": ["Houston Texans"], "scores": [[0.0799487829208374, 0.8418596982955933, 0.9983616471290588]], "normalized_score": [0.40654924511909485], "tokens": [["\u2581Houston", "\u2581Tex", "ans"]], "entropy": [[2.972301959991455, 0.6072463989257812, 0.014529183506965637]], "avg_entropy": [1.198025847474734], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Tennessee Titans", "score_of_biggest_cluster": 0.40654924511909485, "score_of_first_answer": 0.40654924511909485, "max_score_of_first_answer": 0.9983616471290588, "min_score_of_first_answer": 0.0799487829208374, "avg_score": 0.40654924511909485, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4011765048092949, "std_of_first_answer": 0.4011765048092949, "avg_entropy_of_first_answer": 1.198025847474734, "lex_sim": 1.0}
{"question": "who has won the most masters green jackets", "answer": ["Jack Nicklaus"], "scores": [[0.6250537037849426, 0.9897575378417969, 0.9899803996086121]], "normalized_score": [0.8492279052734375], "tokens": [["\u2581Jack", "\u2581Nick", "laus"]], "entropy": [[1.1872972249984741, 0.09666252136230469, 0.08070565015077591]], "avg_entropy": [0.4548884655038516], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Jack Nicklaus", "score_of_biggest_cluster": 0.8492279052734375, "score_of_first_answer": 0.8492279052734375, "max_score_of_first_answer": 0.9899803996086121, "min_score_of_first_answer": 0.6250537037849426, "avg_score": 0.8492279052734375, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1719755892134816, "std_of_first_answer": 0.1719755892134816, "avg_entropy_of_first_answer": 0.4548884655038516, "lex_sim": 1.0}
{"question": "where do the maple leafs play hockey in toronto", "answer": ["Scotiabank Arena, 55 Bay Street, Toronto, Ontario M5J 2X2, Canada"], "scores": [[0.6738596558570862, 0.9794530868530273, 0.999705970287323, 0.9996042847633362, 0.9585626721382141, 0.027581077069044113, 0.03494907543063164, 0.023416059091687202, 0.26813626289367676, 0.7416046261787415, 0.5212092399597168, 0.8710998296737671, 0.9533680081367493, 0.7630439400672913, 0.319130539894104, 0.4664098620414734, 0.9944708347320557, 0.9568732380867004, 0.9628633260726929, 0.9846369028091431, 0.89021235704422, 0.9594236016273499, 0.6644366979598999, 0.9939199686050415]], "normalized_score": [0.5036545395851135], "tokens": [["\u2581Scot", "i", "ab", "ank", "\u2581Arena", ",", "\u2581", "5", "5", "\u2581Bay", "\u2581Street", ",", "\u2581Toronto", ",", "\u2581Ontario", "\u2581M", "5", "J", "\u2581", "2", "X", "2", ",", "\u2581Canada"]], "entropy": [[1.2863808870315552, 0.10652771592140198, 0.003641693387180567, 0.004260595887899399, 0.2512235939502716, 0.7364198565483093, 2.2801928520202637, 0.49158477783203125, 1.6286568641662598, 1.5263798236846924, 0.873333215713501, 0.6687314510345459, 0.36443793773651123, 0.7046332359313965, 0.8993415832519531, 1.2101259231567383, 0.05142880976200104, 0.28146350383758545, 0.223972350358963, 0.10033213347196579, 0.6118775606155396, 0.2525586485862732, 0.8391855955123901, 0.05886063724756241]], "avg_entropy": [0.6439813019436164], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Air Canada Centre", "score_of_biggest_cluster": 0.5036545395851135, "score_of_first_answer": 0.5036545395851135, "max_score_of_first_answer": 0.999705970287323, "min_score_of_first_answer": 0.023416059091687202, "avg_score": 0.5036545395851135, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3345920136243943, "std_of_first_answer": 0.3345920136243943, "avg_entropy_of_first_answer": 0.6439813019436164, "lex_sim": 1.0}
{"question": "which president of the united states was a boy scout", "answer": ["Gerald Ford"], "scores": [[0.6413512825965881, 0.9979180693626404, 0.3932148218154907]], "normalized_score": [0.6313549280166626], "tokens": [["\u2581Ger", "ald", "\u2581Ford"]], "entropy": [[1.8219308853149414, 0.020177112892270088, 0.9878054857254028]], "avg_entropy": [0.9433044946442047], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Gerald Ford", "score_of_biggest_cluster": 0.6313549280166626, "score_of_first_answer": 0.6313549280166626, "max_score_of_first_answer": 0.9979180693626404, "min_score_of_first_answer": 0.3932148218154907, "avg_score": 0.6313549280166626, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.24818845755856708, "std_of_first_answer": 0.24818845755856708, "avg_entropy_of_first_answer": 0.9433044946442047, "lex_sim": 1.0}
{"question": "where did the british empire control an entire continent", "answer": ["Asia"], "scores": [[0.11432013660669327]], "normalized_score": [0.11432012170553207], "tokens": [["\u2581Asia"]], "entropy": [[2.509857416152954]], "avg_entropy": [2.509857416152954], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Australia", "score_of_biggest_cluster": 0.11432012170553207, "score_of_first_answer": 0.11432012170553207, "max_score_of_first_answer": 0.11432013660669327, "min_score_of_first_answer": 0.11432013660669327, "avg_score": 0.11432012170553207, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.509857416152954, "lex_sim": 1.0}
{"question": "who formed and first came to the colony of maryland", "answer": ["George Calvert, 1st Baron Baltimore"], "scores": [[0.27410411834716797, 0.9842474460601807, 0.9856531620025635, 0.353503555059433, 0.7868971228599548, 0.9985439777374268, 0.9971178770065308, 0.9712262153625488, 0.985996425151825]], "normalized_score": [0.7448015809059143], "tokens": [["\u2581George", "\u2581Cal", "vert", ",", "\u2581", "1", "st", "\u2581Baron", "\u2581Baltimore"]], "entropy": [[2.578502655029297, 0.149959996342659, 0.08403722196817398, 1.240495204925537, 1.1781575679779053, 0.013543719425797462, 0.03001401573419571, 0.13919082283973694, 0.1016266793012619]], "avg_entropy": [0.6128364315049516], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the English", "score_of_biggest_cluster": 0.7448015809059143, "score_of_first_answer": 0.7448015809059143, "max_score_of_first_answer": 0.9985439777374268, "min_score_of_first_answer": 0.27410411834716797, "avg_score": 0.7448015809059143, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2758029556138995, "std_of_first_answer": 0.2758029556138995, "avg_entropy_of_first_answer": 0.6128364315049516, "lex_sim": 1.0}
{"question": "who played nicky on the drew carey show", "answer": ["Christa Miller"], "scores": [[0.27394968271255493, 0.8966257572174072, 0.9998326301574707]], "normalized_score": [0.626233696937561], "tokens": [["\u2581Christ", "a", "\u2581Miller"]], "entropy": [[2.891118049621582, 0.36477428674697876, 0.0021697254851460457]], "avg_entropy": [1.086020687284569], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kate Walsh", "score_of_biggest_cluster": 0.626233696937561, "score_of_first_answer": 0.626233696937561, "max_score_of_first_answer": 0.9998326301574707, "min_score_of_first_answer": 0.27394968271255493, "avg_score": 0.626233696937561, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3206388078290235, "std_of_first_answer": 0.3206388078290235, "avg_entropy_of_first_answer": 1.086020687284569, "lex_sim": 1.0}
{"question": "what song is played while raising the american flag", "answer": ["\"The Star-Spangled Banner\""], "scores": [[0.13881999254226685, 0.5960654616355896, 0.9367914795875549, 0.858648419380188, 0.994317352771759, 0.9997690320014954, 0.9999285936355591, 0.9979689717292786, 0.9999582767486572, 0.9376587867736816]], "normalized_score": [0.7571364641189575], "tokens": [["\u2581\"", "The", "\u2581Star", "-", "Sp", "ang", "led", "\u2581B", "anner", "\""]], "entropy": [[2.3337461948394775, 2.0550222396850586, 0.4116874039173126, 0.43499526381492615, 0.04278338700532913, 0.0026747179217636585, 0.0009577102027833462, 0.0198514461517334, 0.0005634811241179705, 0.3723064064979553]], "avg_entropy": [0.5674588251160457], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Reveille", "score_of_biggest_cluster": 0.7571364641189575, "score_of_first_answer": 0.7571364641189575, "max_score_of_first_answer": 0.9999582767486572, "min_score_of_first_answer": 0.13881999254226685, "avg_score": 0.7571364641189575, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2637252221473885, "std_of_first_answer": 0.2637252221473885, "avg_entropy_of_first_answer": 0.5674588251160457, "lex_sim": 1.0}
{"question": "who played mr. willoughby in sense and sensibility", "answer": ["Hugh Grant"], "scores": [[0.20812317728996277, 0.544983983039856]], "normalized_score": [0.3367844820022583], "tokens": [["\u2581Hugh", "\u2581Grant"]], "entropy": [[1.385581374168396, 0.7752247452735901]], "avg_entropy": [1.080403059720993], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Matthew Gregory Wise", "score_of_biggest_cluster": 0.3367844820022583, "score_of_first_answer": 0.3367844820022583, "max_score_of_first_answer": 0.544983983039856, "min_score_of_first_answer": 0.20812317728996277, "avg_score": 0.3367844820022583, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1684304028749466, "std_of_first_answer": 0.1684304028749466, "avg_entropy_of_first_answer": 1.080403059720993, "lex_sim": 1.0}
{"question": "what was icq as related to instant messaging", "answer": ["ICQ, formerly known as I Seek You, is an instant messaging computer program, first developed in November 1996 by the Israeli company Mirabilis, then owned by AOL."], "scores": [[0.17623214423656464, 0.9986609220504761, 0.05172858014702797, 0.08620195090770721, 0.8874471187591553, 0.9985424280166626, 0.5509669184684753, 0.6598231792449951, 0.999946117401123, 0.9890996813774109, 0.856207013130188, 0.8984889388084412, 0.7528948783874512, 0.9841984510421753, 0.9986029267311096, 0.9963778853416443, 0.8469406962394714, 0.997836172580719, 0.47784116864204407, 0.054521720856428146, 0.6645793318748474, 0.5859388113021851, 0.8714995980262756, 0.9977827668190002, 0.9997302889823914, 0.9996670484542847, 0.9998466968536377, 0.999043881893158, 0.7484449148178101, 0.9980524778366089, 0.3571610748767853, 0.9998522996902466, 0.9908271431922913, 0.7397083640098572, 0.9665663242340088, 0.999958872795105, 0.9376532435417175, 0.8317984342575073, 0.16219203174114227, 0.9986985921859741, 0.9045800566673279, 0.9975622892379761, 0.6024630069732666]], "normalized_score": [0.656345784664154], "tokens": [["\u2581IC", "Q", ",", "\u2581formerly", "\u2581known", "\u2581as", "\u2581I", "\u2581Se", "ek", "\u2581You", ",", "\u2581is", "\u2581an", "\u2581instant", "\u2581mess", "aging", "\u2581computer", "\u2581program", ",", "\u2581first", "\u2581developed", "\u2581in", "\u2581November", "\u2581", "1", "9", "9", "6", "\u2581by", "\u2581the", "\u2581Isra", "eli", "\u2581company", "\u2581Mira", "bil", "is", ",", "\u2581then", "\u2581owned", "\u2581by", "\u2581A", "OL", "."]], "entropy": [[3.46138334274292, 0.01595146209001541, 2.5073819160461426, 3.546759605407715, 0.5289412140846252, 0.016874361783266068, 1.4671847820281982, 1.7287572622299194, 0.0007054249290376902, 0.07715298235416412, 0.7130751609802246, 0.4614395797252655, 0.6116644740104675, 0.13149823248386383, 0.01346546784043312, 0.024553807452321053, 0.8086780309677124, 0.01903369091451168, 1.477807641029358, 1.0987112522125244, 1.1554811000823975, 0.9628288745880127, 0.4547366499900818, 0.020571548491716385, 0.002898290753364563, 0.003776059951633215, 0.001785141765139997, 0.008686051703989506, 0.7727705836296082, 0.019806072115898132, 1.1076531410217285, 0.0019145999103784561, 0.08743011951446533, 1.2589654922485352, 0.16131536662578583, 0.0005840620142407715, 0.4171115458011627, 0.9048227071762085, 1.242692232131958, 0.01400366798043251, 0.5696940422058105, 0.02316472679376602, 1.3716747760772705]], "avg_entropy": [0.680822942881076], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "an instant messaging client", "score_of_biggest_cluster": 0.656345784664154, "score_of_first_answer": 0.656345784664154, "max_score_of_first_answer": 0.999958872795105, "min_score_of_first_answer": 0.05172858014702797, "avg_score": 0.656345784664154, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29431344910970264, "std_of_first_answer": 0.29431344910970264, "avg_entropy_of_first_answer": 0.680822942881076, "lex_sim": 1.0}
{"question": "how many episodes are there in dragon ball z", "answer": ["291"], "scores": [[0.9328206777572632, 0.528428852558136, 0.8955613970756531, 0.9233365058898926]], "normalized_score": [0.7990243434906006], "tokens": [["\u2581", "2", "9", "1"]], "entropy": [[0.5853681564331055, 1.2147866487503052, 0.49253085255622864, 0.3288225829601288]], "avg_entropy": [0.655377060174942], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "291 episodes", "score_of_biggest_cluster": 0.7990243434906006, "score_of_first_answer": 0.7990243434906006, "max_score_of_first_answer": 0.9328206777572632, "min_score_of_first_answer": 0.528428852558136, "avg_score": 0.7990243434906006, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1689158015799489, "std_of_first_answer": 0.1689158015799489, "avg_entropy_of_first_answer": 0.655377060174942, "lex_sim": 1.0}
{"question": "who plays the judge in drop dead diva", "answer": ["Mark Harelik"], "scores": [[0.03154018148779869, 0.859293520450592, 0.554739236831665, 0.9842516183853149]], "normalized_score": [0.3487788736820221], "tokens": [["\u2581Mark", "\u2581H", "are", "lik"]], "entropy": [[5.0795087814331055, 0.9814175963401794, 1.2494685649871826, 0.09562504291534424]], "avg_entropy": [1.851504996418953], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lex Medlin", "score_of_biggest_cluster": 0.3487788736820221, "score_of_first_answer": 0.3487788736820221, "max_score_of_first_answer": 0.9842516183853149, "min_score_of_first_answer": 0.03154018148779869, "avg_score": 0.3487788736820221, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36737418318130416, "std_of_first_answer": 0.36737418318130416, "avg_entropy_of_first_answer": 1.851504996418953, "lex_sim": 1.0}
{"question": "time of the state of the nation address", "answer": ["12:00 p.m."], "scores": [[0.6607277393341064, 0.18705657124519348, 0.213454931974411, 0.7177719473838806, 0.7838556170463562, 0.9626249074935913, 0.05624556541442871, 0.9956477284431458, 0.9871100187301636, 0.8992950320243835]], "normalized_score": [0.4843332767486572], "tokens": [["\u2581", "1", "2", ":", "0", "0", "\u2581p", ".", "m", "."]], "entropy": [[2.1538584232330322, 2.107969284057617, 2.326706886291504, 1.4956715106964111, 0.8669007420539856, 0.2340114414691925, 2.4919979572296143, 0.043798960745334625, 0.07419430464506149, 0.522057056427002]], "avg_entropy": [1.2317166566848754], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "after 9pm\u00a0ET", "score_of_biggest_cluster": 0.4843332767486572, "score_of_first_answer": 0.4843332767486572, "max_score_of_first_answer": 0.9956477284431458, "min_score_of_first_answer": 0.05624556541442871, "avg_score": 0.4843332767486572, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3424798880802402, "std_of_first_answer": 0.3424798880802402, "avg_entropy_of_first_answer": 1.2317166566848754, "lex_sim": 1.0}
{"question": "where was the witch of blackbird pond published", "answer": ["A: the United States B: the United States C: the United States D: the United States"], "scores": [[0.0017776130698621273, 0.03981606289744377, 0.009308547712862492, 0.2884305715560913, 0.9616716504096985, 0.09263355284929276, 0.9869645237922668, 0.22778794169425964, 0.69743812084198, 0.41807055473327637, 0.5115565657615662, 0.9975734353065491, 0.447223424911499, 0.8797746300697327, 0.8250981569290161, 0.6042267680168152, 0.99698406457901, 0.7147586941719055, 0.9697399139404297, 0.9610658884048462]], "normalized_score": [0.3114546835422516], "tokens": [["\u2581A", ":", "\u2581the", "\u2581United", "\u2581States", "\u2581B", ":", "\u2581the", "\u2581United", "\u2581States", "\u2581C", ":", "\u2581the", "\u2581United", "\u2581States", "\u2581D", ":", "\u2581the", "\u2581United", "\u2581States"]], "entropy": [[4.283763408660889, 5.328048229217529, 4.810173511505127, 3.212022066116333, 0.2039753496646881, 1.5284478664398193, 0.09800520539283752, 4.289304733276367, 1.9313116073608398, 0.8239477276802063, 1.6474895477294922, 0.025461403653025627, 3.286234140396118, 0.8443914651870728, 0.5467143058776855, 1.2527906894683838, 0.03223147988319397, 1.9646966457366943, 0.26312729716300964, 0.19417327642440796]], "avg_entropy": [1.828315497841686], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "United States", "score_of_biggest_cluster": 0.3114546835422516, "score_of_first_answer": 0.3114546835422516, "max_score_of_first_answer": 0.9975734353065491, "min_score_of_first_answer": 0.0017776130698621273, "avg_score": 0.3114546835422516, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.35958494202626284, "std_of_first_answer": 0.35958494202626284, "avg_entropy_of_first_answer": 1.828315497841686, "lex_sim": 1.0}
{"question": "who missed the plane the day the music died", "answer": ["Buddy Holly"], "scores": [[0.21813079714775085, 0.999919056892395, 0.9891068339347839, 0.9778011441230774]], "normalized_score": [0.6777099967002869], "tokens": [["\u2581Bud", "dy", "\u2581Hol", "ly"]], "entropy": [[3.8805456161499023, 0.0010893589351326227, 0.10374037176370621, 0.11496694386005402]], "avg_entropy": [1.0250855726771988], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tommy Allsup", "score_of_biggest_cluster": 0.6777099967002869, "score_of_first_answer": 0.6777099967002869, "max_score_of_first_answer": 0.999919056892395, "min_score_of_first_answer": 0.21813079714775085, "avg_score": 0.6777099967002869, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33386279866330265, "std_of_first_answer": 0.33386279866330265, "avg_entropy_of_first_answer": 1.0250855726771988, "lex_sim": 1.0}
{"question": "what does ctrl+shift+t do", "answer": ["Pull up the Tabs/Windows Pane"], "scores": [[0.005123358219861984, 0.028513435274362564, 0.47085434198379517, 0.5003879070281982, 0.02212117612361908, 0.10744734108448029, 0.00474964827299118, 0.5577690005302429, 0.02664893865585327, 0.36585649847984314]], "normalized_score": [0.06799715757369995], "tokens": [["\u2581P", "ull", "\u2581up", "\u2581the", "\u2581T", "abs", "/", "Windows", "\u2581P", "ane"]], "entropy": [[5.386604309082031, 1.6824647188186646, 1.7507705688476562, 2.6753787994384766, 5.0727219581604, 4.776548385620117, 4.433465957641602, 2.5324416160583496, 4.324152946472168, 2.274250030517578]], "avg_entropy": [3.4908799290657044], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "New tab", "score_of_biggest_cluster": 0.06799715757369995, "score_of_first_answer": 0.06799715757369995, "max_score_of_first_answer": 0.5577690005302429, "min_score_of_first_answer": 0.00474964827299118, "avg_score": 0.06799715757369995, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22227599868600603, "std_of_first_answer": 0.22227599868600603, "avg_entropy_of_first_answer": 3.4908799290657044, "lex_sim": 1.0}
{"question": "where does stevia in the raw come from", "answer": ["Stevia rebaudiana"], "scores": [[0.21075576543807983, 0.9928004741668701, 0.39430877566337585, 0.9966374635696411, 0.9930429458618164, 0.9846322536468506]], "normalized_score": [0.656965970993042], "tokens": [["\u2581Ste", "via", "\u2581re", "ba", "ud", "iana"]], "entropy": [[2.6468443870544434, 0.05683446675539017, 2.461921453475952, 0.036173295229673386, 0.05750974267721176, 0.11041442304849625]], "avg_entropy": [0.8949496280401945], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "South America", "score_of_biggest_cluster": 0.656965970993042, "score_of_first_answer": 0.656965970993042, "max_score_of_first_answer": 0.9966374635696411, "min_score_of_first_answer": 0.21075576543807983, "avg_score": 0.656965970993042, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3292255049639868, "std_of_first_answer": 0.3292255049639868, "avg_entropy_of_first_answer": 0.8949496280401945, "lex_sim": 1.0}
{"question": "where is the snooker world open currently held", "answer": ["World Snooker Championship"], "scores": [[0.004634794779121876, 0.8991585373878479, 0.9999958276748657, 0.9999675750732422, 0.4406563639640808]], "normalized_score": [0.2836548388004303], "tokens": [["\u2581World", "\u2581S", "no", "oker", "\u2581Championship"]], "entropy": [[3.7110116481781006, 0.5994071364402771, 6.498034053947777e-05, 0.0004659246769733727, 1.8666256666183472]], "avg_entropy": [1.2355150712508476], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hainan International Exhibition Center", "score_of_biggest_cluster": 0.2836548388004303, "score_of_first_answer": 0.2836548388004303, "max_score_of_first_answer": 0.9999958276748657, "min_score_of_first_answer": 0.004634794779121876, "avg_score": 0.2836548388004303, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39130361591895463, "std_of_first_answer": 0.39130361591895463, "avg_entropy_of_first_answer": 1.2355150712508476, "lex_sim": 1.0}
{"question": "what age do you need to be to buy a bb gun", "answer": ["18"], "scores": [[0.6476606130599976, 0.8848675489425659, 0.7699421048164368]], "normalized_score": [0.7613095641136169], "tokens": [["\u2581", "1", "8"]], "entropy": [[2.4559240341186523, 0.5224503874778748, 0.9686856269836426]], "avg_entropy": [1.3156866828600566], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "18", "score_of_biggest_cluster": 0.7613095641136169, "score_of_first_answer": 0.7613095641136169, "max_score_of_first_answer": 0.8848675489425659, "min_score_of_first_answer": 0.6476606130599976, "avg_score": 0.7613095641136169, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.09685484635643922, "std_of_first_answer": 0.09685484635643922, "avg_entropy_of_first_answer": 1.3156866828600566, "lex_sim": 1.0}
{"question": "who plays the dwarf king in the hobbit", "answer": ["Peter Hambleton"], "scores": [[0.14313773810863495, 0.2019830346107483, 0.846718430519104, 0.9969307780265808]], "normalized_score": [0.3952467143535614], "tokens": [["\u2581Peter", "\u2581Ham", "ble", "ton"]], "entropy": [[3.0102550983428955, 1.4519920349121094, 0.9098545908927917, 0.025321682915091515]], "avg_entropy": [1.349355851765722], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Richard Crispin Armitage", "score_of_biggest_cluster": 0.3952467143535614, "score_of_first_answer": 0.3952467143535614, "max_score_of_first_answer": 0.9969307780265808, "min_score_of_first_answer": 0.14313773810863495, "avg_score": 0.3952467143535614, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37894924705594474, "std_of_first_answer": 0.37894924705594474, "avg_entropy_of_first_answer": 1.349355851765722, "lex_sim": 1.0}
{"question": "how many episodes curse of oak island season 5", "answer": ["27"], "scores": [[0.899917721748352, 0.3133404552936554, 0.02486420050263405]], "normalized_score": [0.19139529764652252], "tokens": [["\u2581", "2", "7"]], "entropy": [[0.8179497718811035, 1.7566418647766113, 1.831889033317566]], "avg_entropy": [1.4688268899917603], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "18", "score_of_biggest_cluster": 0.19139529764652252, "score_of_first_answer": 0.19139529764652252, "max_score_of_first_answer": 0.899917721748352, "min_score_of_first_answer": 0.02486420050263405, "avg_score": 0.19139529764652252, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3640833396878383, "std_of_first_answer": 0.3640833396878383, "avg_entropy_of_first_answer": 1.4688268899917603, "lex_sim": 1.0}
{"question": "where does the story the great gatsby take place", "answer": ["Long Island, New York State"], "scores": [[0.3448394536972046, 0.9863005876541138, 0.25806429982185364, 0.7998708486557007, 0.9980461597442627, 0.010570501908659935]], "normalized_score": [0.3007946312427521], "tokens": [["\u2581Long", "\u2581Island", ",", "\u2581New", "\u2581York", "\u2581State"]], "entropy": [[2.051063299179077, 0.08428188413381577, 1.025606632232666, 1.1067142486572266, 0.0194292850792408, 0.944013237953186]], "avg_entropy": [0.8718514312058687], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Long Island of 1922", "score_of_biggest_cluster": 0.3007946312427521, "score_of_first_answer": 0.3007946312427521, "max_score_of_first_answer": 0.9980461597442627, "min_score_of_first_answer": 0.010570501908659935, "avg_score": 0.3007946312427521, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3808433350162965, "std_of_first_answer": 0.3808433350162965, "avg_entropy_of_first_answer": 0.8718514312058687, "lex_sim": 1.0}
{"question": "what is the name of the first earthquake early warning system", "answer": ["ShakeAlert"], "scores": [[0.6260843276977539, 0.9922838807106018, 0.9571599364280701]], "normalized_score": [0.8409130573272705], "tokens": [["\u2581Sh", "ake", "Alert"]], "entropy": [[2.2812819480895996, 0.06902191042900085, 0.23753097653388977]], "avg_entropy": [0.8626116116841634], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1991", "score_of_biggest_cluster": 0.8409130573272705, "score_of_first_answer": 0.8409130573272705, "max_score_of_first_answer": 0.9922838807106018, "min_score_of_first_answer": 0.6260843276977539, "avg_score": 0.8409130573272705, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.16497368907560225, "std_of_first_answer": 0.16497368907560225, "avg_entropy_of_first_answer": 0.8626116116841634, "lex_sim": 1.0}
{"question": "who sings the whiskey ain't workin anymore", "answer": ["Hank Williams Jr."], "scores": [[0.014045171439647675, 0.9794225096702576, 0.9836206436157227, 0.4400174915790558, 0.843915581703186]], "normalized_score": [0.3469115197658539], "tokens": [["\u2581H", "ank", "\u2581Williams", "\u2581Jr", "."]], "entropy": [[3.613231658935547, 0.1627350151538849, 0.1380714476108551, 1.3619012832641602, 0.5355879664421082]], "avg_entropy": [1.162305474281311], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dennis Locorriere", "score_of_biggest_cluster": 0.3469115197658539, "score_of_first_answer": 0.3469115197658539, "max_score_of_first_answer": 0.9836206436157227, "min_score_of_first_answer": 0.014045171439647675, "avg_score": 0.3469115197658539, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3757478090742445, "std_of_first_answer": 0.3757478090742445, "avg_entropy_of_first_answer": 1.162305474281311, "lex_sim": 1.0}
{"question": "where was the movie i am number 4 filmed", "answer": ["British Columbia, Canada"], "scores": [[0.06620857119560242, 0.9873988628387451, 0.33072277903556824, 0.9013533592224121]], "normalized_score": [0.3736298084259033], "tokens": [["\u2581British", "\u2581Columbia", ",", "\u2581Canada"]], "entropy": [[4.122648239135742, 0.09447439014911652, 1.1934247016906738, 0.7870573401451111]], "avg_entropy": [1.549401167780161], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Pittsburgh metropolitan area", "score_of_biggest_cluster": 0.3736298084259033, "score_of_first_answer": 0.3736298084259033, "max_score_of_first_answer": 0.9873988628387451, "min_score_of_first_answer": 0.06620857119560242, "avg_score": 0.3736298084259033, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38570331079334536, "std_of_first_answer": 0.38570331079334536, "avg_entropy_of_first_answer": 1.549401167780161, "lex_sim": 1.0}
{"question": "when did the first train run in england", "answer": ["August 27, 1825"], "scores": [[0.08250745385885239, 0.9905659556388855, 0.357526570558548, 0.7886078953742981, 0.9663456678390503, 0.9951300621032715, 0.9985945820808411, 0.9927428960800171, 0.3785648047924042, 0.9591469168663025]], "normalized_score": [0.6168520450592041], "tokens": [["\u2581August", "\u2581", "2", "7", ",", "\u2581", "1", "8", "2", "5"]], "entropy": [[2.216096878051758, 0.06590035557746887, 1.438018560409546, 0.8854614496231079, 0.19380663335323334, 0.0338982418179512, 0.010875859297811985, 0.04865384101867676, 0.7665784358978271, 0.22219234704971313]], "avg_entropy": [0.5881482602097095], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1560s", "score_of_biggest_cluster": 0.6168520450592041, "score_of_first_answer": 0.6168520450592041, "max_score_of_first_answer": 0.9985945820808411, "min_score_of_first_answer": 0.08250745385885239, "avg_score": 0.6168520450592041, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32684229837489565, "std_of_first_answer": 0.32684229837489565, "avg_entropy_of_first_answer": 0.5881482602097095, "lex_sim": 1.0}
{"question": "the xix of the social security act of 1965 established", "answer": ["Medicare"], "scores": [[0.6155579090118408, 0.9993244409561157]], "normalized_score": [0.7843099236488342], "tokens": [["\u2581Medic", "are"]], "entropy": [[1.9928617477416992, 0.007642787881195545]], "avg_entropy": [1.0002522678114474], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Medicaid", "score_of_biggest_cluster": 0.7843099236488342, "score_of_first_answer": 0.7843099236488342, "max_score_of_first_answer": 0.9993244409561157, "min_score_of_first_answer": 0.6155579090118408, "avg_score": 0.7843099236488342, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19188326597213745, "std_of_first_answer": 0.19188326597213745, "avg_entropy_of_first_answer": 1.0002522678114474, "lex_sim": 1.0}
{"question": "a synonym for the content component of communication is", "answer": ["information"], "scores": [[0.11039727926254272]], "normalized_score": [0.11039727926254272], "tokens": [["\u2581information"]], "entropy": [[2.643310546875]], "avg_entropy": [2.643310546875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "information", "score_of_biggest_cluster": 0.11039727926254272, "score_of_first_answer": 0.11039727926254272, "max_score_of_first_answer": 0.11039727926254272, "min_score_of_first_answer": 0.11039727926254272, "avg_score": 0.11039727926254272, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.643310546875, "lex_sim": 1.0}
{"question": "when was national service abolished in the uk", "answer": ["December 31, 1960"], "scores": [[0.320801705121994, 0.991306722164154, 0.7312705516815186, 0.9669895172119141, 0.9855052828788757, 0.9970357418060303, 0.9973748922348022, 0.9999433755874634, 0.986125648021698, 0.9877967238426208]], "normalized_score": [0.8573828339576721], "tokens": [["\u2581December", "\u2581", "3", "1", ",", "\u2581", "1", "9", "6", "0"]], "entropy": [[2.08695650100708, 0.056408174335956573, 0.9288178086280823, 0.16709348559379578, 0.09218066930770874, 0.024014916270971298, 0.018449120223522186, 0.0006562931812368333, 0.08141641318798065, 0.08247101306915283]], "avg_entropy": [0.3538464394805487], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "1960", "score_of_biggest_cluster": 0.8573828339576721, "score_of_first_answer": 0.8573828339576721, "max_score_of_first_answer": 0.9999433755874634, "min_score_of_first_answer": 0.320801705121994, "avg_score": 0.8573828339576721, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.20687189559326294, "std_of_first_answer": 0.20687189559326294, "avg_entropy_of_first_answer": 0.3538464394805487, "lex_sim": 1.0}
{"question": "what is the highest peak in the ozarks", "answer": ["Black Mountain"], "scores": [[0.011663274839520454, 0.3176202178001404]], "normalized_score": [0.06086454540491104], "tokens": [["\u2581Black", "\u2581Mountain"]], "entropy": [[3.289191246032715, 3.6118786334991455]], "avg_entropy": [3.45053493976593], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lookout", "score_of_biggest_cluster": 0.06086454540491104, "score_of_first_answer": 0.06086454540491104, "max_score_of_first_answer": 0.3176202178001404, "min_score_of_first_answer": 0.011663274839520454, "avg_score": 0.06086454540491104, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.15297847148030996, "std_of_first_answer": 0.15297847148030996, "avg_entropy_of_first_answer": 3.45053493976593, "lex_sim": 1.0}
{"question": "when was the last time the jets won a playoff game", "answer": ["January 10, 2005"], "scores": [[0.7337504625320435, 0.9970121383666992, 0.6093709468841553, 0.0584200955927372, 0.9889135956764221, 0.9981715679168701, 0.4804409444332123, 0.9998007416725159, 0.2998940646648407, 0.5200607776641846]], "normalized_score": [0.5351347923278809], "tokens": [["\u2581January", "\u2581", "1", "0", ",", "\u2581", "2", "0", "0", "5"]], "entropy": [[1.313349962234497, 0.025167841464281082, 1.4653408527374268, 2.2778244018554688, 0.08145062625408173, 0.01685497537255287, 0.6932783126831055, 0.0023051255848258734, 0.6296319365501404, 1.2958121299743652]], "avg_entropy": [0.7801016164710746], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2010", "score_of_biggest_cluster": 0.5351347923278809, "score_of_first_answer": 0.5351347923278809, "max_score_of_first_answer": 0.9998007416725159, "min_score_of_first_answer": 0.0584200955927372, "avg_score": 0.5351347923278809, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3165834259579608, "std_of_first_answer": 0.3165834259579608, "avg_entropy_of_first_answer": 0.7801016164710746, "lex_sim": 1.0}
{"question": "who played hyde in league of extraordinary gentlemen", "answer": ["Sean Connery"], "scores": [[0.3092253804206848, 0.9956616759300232, 0.9311268925666809, 0.9991797804832458, 0.9982567429542542]], "normalized_score": [0.7784959077835083], "tokens": [["\u2581Se", "an", "\u2581Con", "n", "ery"]], "entropy": [[2.5484793186187744, 0.03062364086508751, 0.5169784426689148, 0.00924399122595787, 0.015261567197740078]], "avg_entropy": [0.6241173921152949], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jason Flemyng", "score_of_biggest_cluster": 0.7784959077835083, "score_of_first_answer": 0.7784959077835083, "max_score_of_first_answer": 0.9991797804832458, "min_score_of_first_answer": 0.3092253804206848, "avg_score": 0.7784959077835083, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26996887860521995, "std_of_first_answer": 0.26996887860521995, "avg_entropy_of_first_answer": 0.6241173921152949, "lex_sim": 1.0}
{"question": "list of books written by abul kalam azad", "answer": ["The Calling, India Wins Freedom"], "scores": [[0.028087511658668518, 0.0044473824091255665, 0.42991945147514343, 0.34115102887153625, 0.5077794194221497, 0.3549429178237915, 0.9999446868896484, 0.919955849647522, 0.9995189905166626]], "normalized_score": [0.24373863637447357], "tokens": [["\u2581The", "\u2581Call", "ing", ",", "\u2581India", "\u2581W", "ins", "\u2581Fre", "edom"]], "entropy": [[3.896885395050049, 5.666341304779053, 1.5476096868515015, 2.4502978324890137, 3.020305871963501, 2.4094467163085938, 0.0008195320842787623, 0.3709013760089874, 0.004920960869640112]], "avg_entropy": [2.151947630711624], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ghubar-e-Khatir", "score_of_biggest_cluster": 0.24373863637447357, "score_of_first_answer": 0.24373863637447357, "max_score_of_first_answer": 0.9999446868896484, "min_score_of_first_answer": 0.0044473824091255665, "avg_score": 0.24373863637447357, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3643151904573079, "std_of_first_answer": 0.3643151904573079, "avg_entropy_of_first_answer": 2.151947630711624, "lex_sim": 1.0}
{"question": "perth is the capital of which australian state", "answer": ["Western Australia"], "scores": [[0.8956335783004761, 0.9901716113090515]], "normalized_score": [0.9417170286178589], "tokens": [["\u2581Western", "\u2581Australia"]], "entropy": [[0.6697520017623901, 0.07608926296234131]], "avg_entropy": [0.3729206323623657], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Western Australia", "score_of_biggest_cluster": 0.9417170286178589, "score_of_first_answer": 0.9417170286178589, "max_score_of_first_answer": 0.9901716113090515, "min_score_of_first_answer": 0.8956335783004761, "avg_score": 0.9417170286178589, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.04726901650428772, "std_of_first_answer": 0.04726901650428772, "avg_entropy_of_first_answer": 0.3729206323623657, "lex_sim": 1.0}
{"question": "when does kc undercover season 3 episode 10 air", "answer": ["January 19, 2017"], "scores": [[0.13945885002613068, 0.9851309657096863, 0.304200679063797, 0.0395219661295414, 0.7253163456916809, 0.9958587288856506, 0.9972224235534668, 0.9997283816337585, 0.975009024143219, 0.41128984093666077]], "normalized_score": [0.46542471647262573], "tokens": [["\u2581January", "\u2581", "1", "9", ",", "\u2581", "2", "0", "1", "7"]], "entropy": [[3.665538787841797, 0.10254865884780884, 1.648302435874939, 2.166017770767212, 0.7801802754402161, 0.035644445568323135, 0.023424947634339333, 0.0032711769454181194, 0.1242598369717598, 1.142371416091919]], "avg_entropy": [0.9691559751983732], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "November\u00a010,\u00a02017", "score_of_biggest_cluster": 0.46542471647262573, "score_of_first_answer": 0.46542471647262573, "max_score_of_first_answer": 0.9997283816337585, "min_score_of_first_answer": 0.0395219661295414, "avg_score": 0.46542471647262573, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37356045434573004, "std_of_first_answer": 0.37356045434573004, "avg_entropy_of_first_answer": 0.9691559751983732, "lex_sim": 1.0}
{"question": "who played mr. kincaid on the partridge family", "answer": ["Dabney Coleman"], "scores": [[0.02207055315375328, 0.06643098592758179, 0.9846204519271851, 0.9815358519554138, 0.9972948431968689]], "normalized_score": [0.26917609572410583], "tokens": [["\u2581D", "ab", "ney", "\u2581Cole", "man"]], "entropy": [[2.648777723312378, 1.3470135927200317, 0.11930743604898453, 0.14947950839996338, 0.02291327714920044]], "avg_entropy": [0.8574983075261116], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "David Joseph Madden", "score_of_biggest_cluster": 0.26917609572410583, "score_of_first_answer": 0.26917609572410583, "max_score_of_first_answer": 0.9972948431968689, "min_score_of_first_answer": 0.02207055315375328, "avg_score": 0.26917609572410583, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46249415407963734, "std_of_first_answer": 0.46249415407963734, "avg_entropy_of_first_answer": 0.8574983075261116, "lex_sim": 1.0}
{"question": "who came up with the idea of the transcontinental railroad", "answer": ["Asa Whitney"], "scores": [[0.2420581579208374, 0.992003321647644, 0.9940710663795471, 0.9944517016410828]], "normalized_score": [0.6980049014091492], "tokens": [["\u2581As", "a", "\u2581Whit", "ney"]], "entropy": [[2.401001453399658, 0.0729897990822792, 0.06460100412368774, 0.04334593191742897]], "avg_entropy": [0.6454845471307635], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dr. Hartwell Carver", "score_of_biggest_cluster": 0.6980049014091492, "score_of_first_answer": 0.6980049014091492, "max_score_of_first_answer": 0.9944517016410828, "min_score_of_first_answer": 0.2420581579208374, "avg_score": 0.6980049014091492, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3253889617378844, "std_of_first_answer": 0.3253889617378844, "avg_entropy_of_first_answer": 0.6454845471307635, "lex_sim": 1.0}
{"question": "where do kidneys for kidney transplant come from", "answer": ["living donors"], "scores": [[0.055036239326000214, 0.5584834814071655, 0.47152355313301086]], "normalized_score": [0.2438115030527115], "tokens": [["\u2581living", "\u2581don", "ors"]], "entropy": [[3.841878890991211, 1.314199686050415, 0.7885378003120422]], "avg_entropy": [1.9815387924512227], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "living-donor", "score_of_biggest_cluster": 0.2438115030527115, "score_of_first_answer": 0.2438115030527115, "max_score_of_first_answer": 0.5584834814071655, "min_score_of_first_answer": 0.055036239326000214, "avg_score": 0.2438115030527115, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2197177070815987, "std_of_first_answer": 0.2197177070815987, "avg_entropy_of_first_answer": 1.9815387924512227, "lex_sim": 1.0}
{"question": "when was the first documented case of tool mark identification", "answer": ["1897"], "scores": [[0.7394643425941467, 0.954217255115509, 0.4723457396030426, 0.44174924492836, 0.047507867217063904]], "normalized_score": [0.37064072489738464], "tokens": [["\u2581", "1", "8", "9", "7"]], "entropy": [[1.9294044971466064, 0.2741847634315491, 1.2372151613235474, 1.778720498085022, 1.578537106513977]], "avg_entropy": [1.3596124053001404], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1835", "score_of_biggest_cluster": 0.37064072489738464, "score_of_first_answer": 0.37064072489738464, "max_score_of_first_answer": 0.954217255115509, "min_score_of_first_answer": 0.047507867217063904, "avg_score": 0.37064072489738464, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3058564607736711, "std_of_first_answer": 0.3058564607736711, "avg_entropy_of_first_answer": 1.3596124053001404, "lex_sim": 1.0}
{"question": "which layer of the meninges is connected to the brain", "answer": ["pia mater"], "scores": [[0.6676691174507141, 0.8381989002227783]], "normalized_score": [0.7480905652046204], "tokens": [["\u2581pia", "\u2581mater"]], "entropy": [[1.401026725769043, 0.713092565536499]], "avg_entropy": [1.057059645652771], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "pia mater", "score_of_biggest_cluster": 0.7480905652046204, "score_of_first_answer": 0.7480905652046204, "max_score_of_first_answer": 0.8381989002227783, "min_score_of_first_answer": 0.6676691174507141, "avg_score": 0.7480905652046204, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0852648913860321, "std_of_first_answer": 0.0852648913860321, "avg_entropy_of_first_answer": 1.057059645652771, "lex_sim": 1.0}
{"question": "who scored fastest 10000 runs in test cricket", "answer": ["Mohammad Yousuf"], "scores": [[0.0031897772569209337, 0.8573746085166931, 0.9978392720222473, 0.6909791827201843, 0.9411205053329468, 0.9667258858680725]], "normalized_score": [0.34599319100379944], "tokens": [["\u2581Moh", "amm", "ad", "\u2581Y", "ous", "uf"]], "entropy": [[3.70977520942688, 0.6152533292770386, 0.02225005067884922, 1.3162870407104492, 0.2288770079612732, 0.168737530708313]], "avg_entropy": [1.0101966947938006], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alastair Cook", "score_of_biggest_cluster": 0.34599319100379944, "score_of_first_answer": 0.34599319100379944, "max_score_of_first_answer": 0.9978392720222473, "min_score_of_first_answer": 0.0031897772569209337, "avg_score": 0.34599319100379944, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34577325045244084, "std_of_first_answer": 0.34577325045244084, "avg_entropy_of_first_answer": 1.0101966947938006, "lex_sim": 1.0}
{"question": "christmas though a christian holiday began as a celebration of", "answer": ["the winter solstice"], "scores": [[0.2722385823726654, 0.7166285514831543, 0.9766223430633545, 0.9804450869560242, 0.9989815354347229]], "normalized_score": [0.7148093581199646], "tokens": [["\u2581the", "\u2581winter", "\u2581sol", "st", "ice"]], "entropy": [[3.0628671646118164, 1.5555164813995361, 0.1860949695110321, 0.12861928343772888, 0.010328585281968117]], "avg_entropy": [0.9886852968484163], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "the winter solstice", "score_of_biggest_cluster": 0.7148093581199646, "score_of_first_answer": 0.7148093581199646, "max_score_of_first_answer": 0.9989815354347229, "min_score_of_first_answer": 0.2722385823726654, "avg_score": 0.7148093581199646, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2786487387592057, "std_of_first_answer": 0.2786487387592057, "avg_entropy_of_first_answer": 0.9886852968484163, "lex_sim": 1.0}
{"question": "when did the nba add the three point line", "answer": ["1979, just two seasons after the ABA-NBA merger brought the up-tempo game to a league that once prided itself on defense."], "scores": [[0.715024471282959, 0.9663853049278259, 0.996921956539154, 0.5593630075454712, 0.9856599569320679, 0.005879268515855074, 0.0008267399971373379, 0.011033749207854271, 0.21347704529762268, 0.8326402306556702, 0.6376633644104004, 0.559859573841095, 0.9992515444755554, 0.3280658423900604, 0.9843226075172424, 0.9997753500938416, 0.9740254282951355, 0.9993506073951721, 0.006681280676275492, 0.5833486318588257, 0.0060106064192950726, 0.7441373467445374, 0.981115460395813, 0.9996192455291748, 0.04825353994965553, 0.3262757956981659, 0.010867146775126457, 0.5736284852027893, 0.6040460467338562, 0.017621053382754326, 0.11077357083559036, 0.999554455280304, 0.9953676462173462, 0.9525320529937744, 0.19024138152599335, 0.4703296720981598, 0.0012709731236100197, 0.359211266040802, 0.6562816500663757, 0.16895192861557007, 0.8594387769699097, 0.018975703045725822]], "normalized_score": [0.21250905096530914], "tokens": [["\u2581", "1", "9", "7", "9", ",", "\u2581just", "\u2581two", "\u2581seasons", "\u2581after", "\u2581the", "\u2581A", "BA", "-", "N", "BA", "\u2581mer", "ger", "\u2581brought", "\u2581the", "\u2581up", "-", "tem", "po", "\u2581game", "\u2581to", "\u2581a", "\u2581league", "\u2581that", "\u2581once", "\u2581pr", "ided", "\u2581itself", "\u2581on", "\u2581defense", ".", "\u2581", "1", "9", "8", "0", "\u2581season"]], "entropy": [[1.4971330165863037, 0.1735292673110962, 0.03044210746884346, 0.7642217874526978, 0.09253323078155518, 1.4620416164398193, 1.7878938913345337, 2.5089077949523926, 1.1767759323120117, 0.7861534357070923, 2.030198097229004, 2.2571589946746826, 0.007923545315861702, 3.0125226974487305, 0.11294180154800415, 0.002559106796979904, 0.174433171749115, 0.006435937248170376, 1.6210684776306152, 2.2333168983459473, 2.3590970039367676, 0.7114052772521973, 0.11691313236951828, 0.004275110550224781, 2.1719777584075928, 1.8285150527954102, 0.3223140239715576, 2.37689208984375, 2.2327919006347656, 2.498767614364624, 4.426604270935059, 0.004954762756824493, 0.047758907079696655, 0.3019411265850067, 3.4474973678588867, 1.755549430847168, 1.283237338066101, 2.174485921859741, 1.7528623342514038, 1.1158907413482666, 0.6807215213775635, 2.487293243408203]], "avg_entropy": [1.3295223033055663], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "1979\u201380", "score_of_biggest_cluster": 0.21250905096530914, "score_of_first_answer": 0.21250905096530914, "max_score_of_first_answer": 0.9997753500938416, "min_score_of_first_answer": 0.0008267399971373379, "avg_score": 0.21250905096530914, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.38893368872042117, "std_of_first_answer": 0.38893368872042117, "avg_entropy_of_first_answer": 1.3295223033055663, "lex_sim": 1.0}
{"question": "who beat university of virginia in basketball this year", "answer": ["Virginia Commonwealth"], "scores": [[0.07179274410009384, 0.6054821610450745]], "normalized_score": [0.20849274098873138], "tokens": [["\u2581Virginia", "\u2581Commonwealth"]], "entropy": [[4.865829944610596, 1.3299157619476318]], "avg_entropy": [3.0978728532791138], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Virginia Tech", "score_of_biggest_cluster": 0.20849274098873138, "score_of_first_answer": 0.20849274098873138, "max_score_of_first_answer": 0.6054821610450745, "min_score_of_first_answer": 0.07179274410009384, "avg_score": 0.20849274098873138, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2668447084724903, "std_of_first_answer": 0.2668447084724903, "avg_entropy_of_first_answer": 3.0978728532791138, "lex_sim": 1.0}
{"question": "when did the eagles play in the superbowl", "answer": ["February 6-7, 1981"], "scores": [[0.4491690695285797, 0.9963569045066833, 0.45736104249954224, 0.00011439972877269611, 0.23970302939414978, 0.8593383431434631, 0.99152010679245, 0.32376861572265625, 0.9972497820854187, 0.9130672216415405, 0.009429738856852055]], "normalized_score": [0.19229666888713837], "tokens": [["\u2581February", "\u2581", "6", "-", "7", ",", "\u2581", "1", "9", "8", "1"]], "entropy": [[2.0837159156799316, 0.030952531844377518, 1.4583176374435425, 0.09242741763591766, 2.569202184677124, 0.727144718170166, 0.0734691321849823, 0.6432188153266907, 0.026649147272109985, 0.3882070481777191, 0.6986351013183594]], "avg_entropy": [0.7992672408846292], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "February 6, 2005", "score_of_biggest_cluster": 0.19229666888713837, "score_of_first_answer": 0.19229666888713837, "max_score_of_first_answer": 0.9972497820854187, "min_score_of_first_answer": 0.00011439972877269611, "avg_score": 0.19229666888713837, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37891439403331084, "std_of_first_answer": 0.37891439403331084, "avg_entropy_of_first_answer": 0.7992672408846292, "lex_sim": 1.0}
{"question": "what is the meaning of the harp in ireland", "answer": ["it is used on the flag of Ireland and is a symbol of Ireland, a sign of peace and harmony."], "scores": [[0.027408402413129807, 0.4525953531265259, 0.06098093092441559, 0.1560993194580078, 0.619094967842102, 0.2705079913139343, 0.7760329246520996, 0.44608795642852783, 0.20535241067409515, 0.3362143933773041, 0.26596158742904663, 0.532737135887146, 0.8646968007087708, 0.32070308923721313, 0.058196667581796646, 0.010196460410952568, 0.04374239221215248, 0.9044170379638672, 0.032819394022226334, 0.449093759059906, 0.2138766646385193, 0.9989198446273804, 0.36980140209198]], "normalized_score": [0.21628467738628387], "tokens": [["\u2581it", "\u2581is", "\u2581used", "\u2581on", "\u2581the", "\u2581flag", "\u2581of", "\u2581Ireland", "\u2581and", "\u2581is", "\u2581a", "\u2581symbol", "\u2581of", "\u2581Ireland", ",", "\u2581a", "\u2581sign", "\u2581of", "\u2581peace", "\u2581and", "\u2581harm", "ony", "."]], "entropy": [[4.30959415435791, 2.203185558319092, 2.561008930206299, 1.7541126012802124, 2.167771339416504, 3.2932937145233154, 1.0532221794128418, 1.4900598526000977, 2.0948522090911865, 2.9371695518493652, 2.8940749168395996, 2.5224359035491943, 0.6028854846954346, 2.1663732528686523, 1.8337862491607666, 4.578517913818359, 3.970564842224121, 0.6489204168319702, 3.87074875831604, 1.565585732460022, 3.8164870738983154, 0.009616397321224213, 1.813457727432251]], "avg_entropy": [2.354683685237947], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the arms of Ireland", "score_of_biggest_cluster": 0.21628467738628387, "score_of_first_answer": 0.21628467738628387, "max_score_of_first_answer": 0.9989198446273804, "min_score_of_first_answer": 0.010196460410952568, "avg_score": 0.21628467738628387, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29332782916599714, "std_of_first_answer": 0.29332782916599714, "avg_entropy_of_first_answer": 2.354683685237947, "lex_sim": 1.0}
{"question": "who sang the songs in the movie beyond the sea", "answer": ["Kevin Spacey"], "scores": [[0.6745073795318604, 0.9756546020507812, 0.9985755681991577]], "normalized_score": [0.8694032430648804], "tokens": [["\u2581Kevin", "\u2581Space", "y"]], "entropy": [[2.0012569427490234, 0.1994834840297699, 0.013369359076023102]], "avg_entropy": [0.7380365952849388], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Kevin Spacey", "score_of_biggest_cluster": 0.8694032430648804, "score_of_first_answer": 0.8694032430648804, "max_score_of_first_answer": 0.9985755681991577, "min_score_of_first_answer": 0.6745073795318604, "avg_score": 0.8694032430648804, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.14766147892547954, "std_of_first_answer": 0.14766147892547954, "avg_entropy_of_first_answer": 0.7380365952849388, "lex_sim": 1.0}
{"question": "who dies in season 2 of the originals", "answer": ["Rebekah Mikaelson"], "scores": [[0.02987963892519474, 0.9339610934257507, 0.9946478009223938, 0.4786469638347626, 0.9994640946388245, 0.9979103207588196, 0.9997749924659729]], "normalized_score": [0.5391823649406433], "tokens": [["\u2581Reb", "ek", "ah", "\u2581M", "ika", "el", "son"]], "entropy": [[4.7576003074646, 0.25558245182037354, 0.03922493755817413, 1.3890368938446045, 0.004815298598259687, 0.017643559724092484, 0.002636614255607128]], "avg_entropy": [0.9237914376093873], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kol", "score_of_biggest_cluster": 0.5391823649406433, "score_of_first_answer": 0.5391823649406433, "max_score_of_first_answer": 0.9997749924659729, "min_score_of_first_answer": 0.02987963892519474, "avg_score": 0.5391823649406433, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3519593894368803, "std_of_first_answer": 0.3519593894368803, "avg_entropy_of_first_answer": 0.9237914376093873, "lex_sim": 1.0}
{"question": "when did they stop using aluminum wiring in ontario", "answer": ["1978"], "scores": [[0.45748162269592285, 0.7181136012077332, 0.9721892476081848, 0.7302886843681335, 0.04978606477379799]], "normalized_score": [0.4101890027523041], "tokens": [["\u2581", "1", "9", "7", "8"]], "entropy": [[2.9972829818725586, 0.6781578660011292, 0.20818449556827545, 0.894782304763794, 1.877223014831543]], "avg_entropy": [1.33112613260746], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "mid-1970s", "score_of_biggest_cluster": 0.4101890027523041, "score_of_first_answer": 0.4101890027523041, "max_score_of_first_answer": 0.9721892476081848, "min_score_of_first_answer": 0.04978606477379799, "avg_score": 0.4101890027523041, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31351453046943123, "std_of_first_answer": 0.31351453046943123, "avg_entropy_of_first_answer": 1.33112613260746, "lex_sim": 1.0}
{"question": "how many seasons are in star wars the clone wars", "answer": ["5"], "scores": [[0.8048014640808105, 0.3021959960460663]], "normalized_score": [0.49316102266311646], "tokens": [["\u2581", "5"]], "entropy": [[1.0109834671020508, 1.3961718082427979]], "avg_entropy": [1.2035776376724243], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "6", "score_of_biggest_cluster": 0.49316102266311646, "score_of_first_answer": 0.49316102266311646, "max_score_of_first_answer": 0.8048014640808105, "min_score_of_first_answer": 0.3021959960460663, "avg_score": 0.49316102266311646, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25130273401737213, "std_of_first_answer": 0.25130273401737213, "avg_entropy_of_first_answer": 1.2035776376724243, "lex_sim": 1.0}
{"question": "different ways to say bless you in french", "answer": ["\"salute\" or \"touchez-vous\", or less idiomatically, perhaps, \"prenez garde!\""], "scores": [[0.05908164754509926, 0.00980663113296032, 0.012015404179692268, 0.6292892694473267, 0.1657673716545105, 0.9545013308525085, 0.012692221440374851, 0.15263333916664124, 0.34596970677375793, 0.2865964472293854, 0.49944016337394714, 0.014350526966154575, 0.19926100969314575, 0.003587499260902405, 0.0011484869755804539, 0.9965565204620361, 0.7458991408348083, 0.39741620421409607, 0.0008141634752973914, 0.24315206706523895, 0.8704773187637329, 0.0028184433467686176, 0.6484866142272949, 0.42923080921173096, 0.9990849494934082, 0.08035258948802948]], "normalized_score": [0.09482508897781372], "tokens": [["\u2581\"", "sal", "ute", "\"", "\u2581or", "\u2581\"", "t", "ou", "chez", "-", "vous", "\",", "\u2581or", "\u2581less", "\u2581id", "iom", "atically", ",", "\u2581perhaps", ",", "\u2581\"", "pr", "enez", "\u2581gar", "de", "!\""]], "entropy": [[4.278459548950195, 3.315868854522705, 1.4409044981002808, 1.9040635824203491, 2.070253610610962, 0.42151734232902527, 3.9176297187805176, 3.6133430004119873, 1.032103419303894, 2.259963035583496, 1.7738393545150757, 0.6944588422775269, 2.9106101989746094, 2.174257516860962, 2.5847973823547363, 0.029823362827301025, 0.6039089560508728, 1.6489108800888062, 0.8239778280258179, 2.1741483211517334, 0.9624985456466675, 4.027613639831543, 1.137990951538086, 2.7518110275268555, 0.011625146493315697, 2.0525665283203125]], "avg_entropy": [1.9468055805191398], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u00e0 tes / vos souhaits", "score_of_biggest_cluster": 0.09482508897781372, "score_of_first_answer": 0.09482508897781372, "max_score_of_first_answer": 0.9990849494934082, "min_score_of_first_answer": 0.0008141634752973914, "avg_score": 0.09482508897781372, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34165331643347985, "std_of_first_answer": 0.34165331643347985, "avg_entropy_of_first_answer": 1.9468055805191398, "lex_sim": 1.0}
{"question": "sequel to a christmas story it runs in the family", "answer": ["A Christmas Story 2"], "scores": [[0.47997915744781494, 0.9787201881408691, 0.9149280786514282, 0.9720884561538696, 0.9967441558837891]], "normalized_score": [0.8392888903617859], "tokens": [["\u2581A", "\u2581Christmas", "\u2581Story", "\u2581", "2"]], "entropy": [[2.260190963745117, 0.21278533339500427, 0.5093256235122681, 0.19293493032455444, 0.024325404316186905]], "avg_entropy": [0.6399124510586262], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "My Summer Story", "score_of_biggest_cluster": 0.8392888903617859, "score_of_first_answer": 0.8392888903617859, "max_score_of_first_answer": 0.9967441558837891, "min_score_of_first_answer": 0.47997915744781494, "avg_score": 0.8392888903617859, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1961783030336431, "std_of_first_answer": 0.1961783030336431, "avg_entropy_of_first_answer": 0.6399124510586262, "lex_sim": 1.0}
{"question": "when was the first orca put in captivity", "answer": ["December 17, 1964"], "scores": [[0.039019159972667694, 0.9842392206192017, 0.7018173336982727, 0.034931618720293045, 0.9767205119132996, 0.9943773150444031, 0.9630466103553772, 0.999396562576294, 0.7001481652259827, 0.33873340487480164]], "normalized_score": [0.42850667238235474], "tokens": [["\u2581December", "\u2581", "1", "7", ",", "\u2581", "1", "9", "6", "4"]], "entropy": [[2.676928758621216, 0.09282010048627853, 1.0928235054016113, 1.958509922027588, 0.1519184559583664, 0.03898609057068825, 0.15973840653896332, 0.005801537074148655, 0.8763235807418823, 1.5663739442825317]], "avg_entropy": [0.8620224301703274], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in the 1960s", "score_of_biggest_cluster": 0.42850667238235474, "score_of_first_answer": 0.42850667238235474, "max_score_of_first_answer": 0.999396562576294, "min_score_of_first_answer": 0.034931618720293045, "avg_score": 0.42850667238235474, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37489000055415655, "std_of_first_answer": 0.37489000055415655, "avg_entropy_of_first_answer": 0.8620224301703274, "lex_sim": 1.0}
{"question": "the victim was asking for it is an example of a", "answer": ["rape myth"], "scores": [[0.04237190634012222, 0.9916574358940125, 0.9083859324455261]], "normalized_score": [0.33669501543045044], "tokens": [["\u2581ra", "pe", "\u2581myth"]], "entropy": [[3.98524808883667, 0.06141664460301399, 0.4962174892425537]], "avg_entropy": [1.5142940742274125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Victim blaming", "score_of_biggest_cluster": 0.33669501543045044, "score_of_first_answer": 0.33669501543045044, "max_score_of_first_answer": 0.9916574358940125, "min_score_of_first_answer": 0.04237190634012222, "avg_score": 0.33669501543045044, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4292185992291719, "std_of_first_answer": 0.4292185992291719, "avg_entropy_of_first_answer": 1.5142940742274125, "lex_sim": 1.0}
{"question": "who was the actor who played crocodile dundee", "answer": ["Paul Hogan"], "scores": [[0.9834989905357361, 0.9885311126708984, 0.9996981620788574]], "normalized_score": [0.9905530214309692], "tokens": [["\u2581Paul", "\u2581Hog", "an"]], "entropy": [[0.15667590498924255, 0.12118604779243469, 0.004000613000243902]], "avg_entropy": [0.09395418859397371], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Paul Hogan", "score_of_biggest_cluster": 0.9905530214309692, "score_of_first_answer": 0.9905530214309692, "max_score_of_first_answer": 0.9996981620788574, "min_score_of_first_answer": 0.9834989905357361, "avg_score": 0.9905530214309692, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.006769526524937243, "std_of_first_answer": 0.006769526524937243, "avg_entropy_of_first_answer": 0.09395418859397371, "lex_sim": 1.0}
{"question": "who sings don't take your guns to town", "answer": ["Johnny Cash"], "scores": [[0.9697609543800354, 0.9831734299659729, 0.9996850490570068]], "normalized_score": [0.9841304421424866], "tokens": [["\u2581Johnny", "\u2581C", "ash"]], "entropy": [[0.25312840938568115, 0.11912927776575089, 0.004070127382874489]], "avg_entropy": [0.12544260484476885], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "U2", "score_of_biggest_cluster": 0.9841304421424866, "score_of_first_answer": 0.9841304421424866, "max_score_of_first_answer": 0.9996850490570068, "min_score_of_first_answer": 0.9697609543800354, "avg_score": 0.9841304421424866, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.012238280146904793, "std_of_first_answer": 0.012238280146904793, "avg_entropy_of_first_answer": 0.12544260484476885, "lex_sim": 1.0}
{"question": "who sang the original scooby doo theme song", "answer": ["David Benoit"], "scores": [[0.20901645720005035, 0.015762776136398315, 0.9704777598381042]], "normalized_score": [0.14732153713703156], "tokens": [["\u2581David", "\u2581Ben", "oit"]], "entropy": [[4.2948808670043945, 4.4308552742004395, 0.2368665635585785]], "avg_entropy": [2.9875342349211373], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Larry Marks", "score_of_biggest_cluster": 0.14732153713703156, "score_of_first_answer": 0.14732153713703156, "max_score_of_first_answer": 0.9704777598381042, "min_score_of_first_answer": 0.015762776136398315, "avg_score": 0.14732153713703156, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41212875560442175, "std_of_first_answer": 0.41212875560442175, "avg_entropy_of_first_answer": 2.9875342349211373, "lex_sim": 1.0}
{"question": "name the process of fusion of an egg with a sperm", "answer": ["fertilization"], "scores": [[0.6261677742004395, 0.9955937266349792, 0.782781183719635]], "normalized_score": [0.7872954607009888], "tokens": [["\u2581fert", "il", "ization"]], "entropy": [[1.5132631063461304, 0.0400550402700901, 0.5481768846511841]], "avg_entropy": [0.7004983437558016], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "fertilization", "score_of_biggest_cluster": 0.7872954607009888, "score_of_first_answer": 0.7872954607009888, "max_score_of_first_answer": 0.9955937266349792, "min_score_of_first_answer": 0.6261677742004395, "avg_score": 0.7872954607009888, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.15139810390545014, "std_of_first_answer": 0.15139810390545014, "avg_entropy_of_first_answer": 0.7004983437558016, "lex_sim": 1.0}
{"question": "first day collection of mission china assamese film", "answer": ["53.05 lakhs"], "scores": [[0.5687254071235657, 0.07468120753765106, 0.042432159185409546, 0.14686794579029083, 0.07281821221113205, 0.10943114012479782, 0.30895915627479553, 0.9967453479766846]], "normalized_score": [0.1684904545545578], "tokens": [["\u2581", "5", "3", ".", "0", "5", "\u2581lak", "hs"]], "entropy": [[2.0838801860809326, 2.148613452911377, 2.644151210784912, 2.6798276901245117, 2.307063102722168, 2.3384485244750977, 2.2026047706604004, 0.03076050616800785]], "avg_entropy": [2.054418680490926], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u20b9 39.97 lakh", "score_of_biggest_cluster": 0.1684904545545578, "score_of_first_answer": 0.1684904545545578, "max_score_of_first_answer": 0.9967453479766846, "min_score_of_first_answer": 0.042432159185409546, "avg_score": 0.1684904545545578, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31333849104513734, "std_of_first_answer": 0.31333849104513734, "avg_entropy_of_first_answer": 2.054418680490926, "lex_sim": 1.0}
{"question": "who played young monica in love and basketball", "answer": ["Sanaa Lathan"], "scores": [[0.5519468784332275, 0.9973785877227783, 0.9872786998748779, 0.9907960891723633, 0.9956342577934265]], "normalized_score": [0.8827876448631287], "tokens": [["\u2581S", "ana", "a", "\u2581L", "athan"]], "entropy": [[3.2018680572509766, 0.025514857843518257, 0.08732277899980545, 0.08252262324094772, 0.03722451627254486]], "avg_entropy": [0.6868905667215586], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kyla Pratt", "score_of_biggest_cluster": 0.8827876448631287, "score_of_first_answer": 0.8827876448631287, "max_score_of_first_answer": 0.9973785877227783, "min_score_of_first_answer": 0.5519468784332275, "avg_score": 0.8827876448631287, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17636601687740347, "std_of_first_answer": 0.17636601687740347, "avg_entropy_of_first_answer": 0.6868905667215586, "lex_sim": 1.0}
{"question": "do you need a permit or license to own a gun in florida", "answer": ["Yes"], "scores": [[0.33284831047058105]], "normalized_score": [0.33284831047058105], "tokens": [["\u2581Yes"]], "entropy": [[2.9871625900268555]], "avg_entropy": [2.9871625900268555], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "No", "score_of_biggest_cluster": 0.33284831047058105, "score_of_first_answer": 0.33284831047058105, "max_score_of_first_answer": 0.33284831047058105, "min_score_of_first_answer": 0.33284831047058105, "avg_score": 0.33284831047058105, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.9871625900268555, "lex_sim": 1.0}
{"question": "when was the last time.michigan beat ohio state", "answer": ["Nov. 22, 2016"], "scores": [[0.026231402531266212, 0.224430114030838, 0.9643961787223816, 0.764298677444458, 0.17352637648582458, 0.9277270436286926, 0.9939731955528259, 0.9965848922729492, 0.9997979998588562, 0.9738726615905762, 0.07383046299219131]], "normalized_score": [0.40625160932540894], "tokens": [["\u2581Nov", ".", "\u2581", "2", "2", ",", "\u2581", "2", "0", "1", "6"]], "entropy": [[1.6414527893066406, 0.6684876084327698, 0.21882520616054535, 0.6841972470283508, 1.9736160039901733, 0.3800954818725586, 0.0452430434525013, 0.023669933900237083, 0.002524255309253931, 0.12260483205318451, 1.9155197143554688]], "avg_entropy": [0.6978396468965168], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2011", "score_of_biggest_cluster": 0.40625160932540894, "score_of_first_answer": 0.40625160932540894, "max_score_of_first_answer": 0.9997979998588562, "min_score_of_first_answer": 0.026231402531266212, "avg_score": 0.40625160932540894, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4026998030536519, "std_of_first_answer": 0.4026998030536519, "avg_entropy_of_first_answer": 0.6978396468965168, "lex_sim": 1.0}
{"question": "where did the dewey decimal system come from", "answer": ["Dewey Decimal Classification"], "scores": [[0.14810746908187866, 0.9957999587059021, 0.9987769722938538, 0.757016122341156, 0.9995406866073608, 0.9483258724212646]], "normalized_score": [0.6876172423362732], "tokens": [["\u2581De", "we", "y", "\u2581Dec", "imal", "\u2581Classification"]], "entropy": [[3.710754871368408, 0.046818073838949203, 0.012811942026019096, 1.0792922973632812, 0.004473187029361725, 0.30772337317466736]], "avg_entropy": [0.8603122908001145], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Melvil Dewey", "score_of_biggest_cluster": 0.6876172423362732, "score_of_first_answer": 0.6876172423362732, "max_score_of_first_answer": 0.9995406866073608, "min_score_of_first_answer": 0.14810746908187866, "avg_score": 0.6876172423362732, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30716481451347927, "std_of_first_answer": 0.30716481451347927, "avg_entropy_of_first_answer": 0.8603122908001145, "lex_sim": 1.0}
{"question": "when did the tradition of self government began in america", "answer": ["1620"], "scores": [[0.5942199230194092, 0.9736794829368591, 0.788987398147583, 0.4273633658885956, 0.9463321566581726]], "normalized_score": [0.71327143907547], "tokens": [["\u2581", "1", "6", "2", "0"]], "entropy": [[2.2817420959472656, 0.17255175113677979, 0.8835129737854004, 1.710207223892212, 0.2961372137069702]], "avg_entropy": [1.0688302516937256], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1783", "score_of_biggest_cluster": 0.71327143907547, "score_of_first_answer": 0.71327143907547, "max_score_of_first_answer": 0.9736794829368591, "min_score_of_first_answer": 0.4273633658885956, "avg_score": 0.71327143907547, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20899055714860537, "std_of_first_answer": 0.20899055714860537, "avg_entropy_of_first_answer": 1.0688302516937256, "lex_sim": 1.0}
{"question": "who was an early advocate for using scientific methods based on inductive reasoning", "answer": ["Sir Francis Bacon"], "scores": [[0.061317697167396545, 0.8807460069656372, 0.9900335669517517, 0.9997536540031433]], "normalized_score": [0.48083364963531494], "tokens": [["\u2581Sir", "\u2581Francis", "\u2581Ba", "con"]], "entropy": [[2.8985047340393066, 0.611889123916626, 0.0833103358745575, 0.002826143056154251]], "avg_entropy": [0.8991325842216611], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Aristotle", "score_of_biggest_cluster": 0.48083364963531494, "score_of_first_answer": 0.48083364963531494, "max_score_of_first_answer": 0.9997536540031433, "min_score_of_first_answer": 0.061317697167396545, "avg_score": 0.48083364963531494, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3905796203787828, "std_of_first_answer": 0.3905796203787828, "avg_entropy_of_first_answer": 0.8991325842216611, "lex_sim": 1.0}
{"question": "what was the immediate catalyst to the civil war", "answer": ["Confederate Army opens fire on Fort Sumter"], "scores": [[0.00671853544190526, 0.8837047219276428, 0.0057216426357626915, 0.0025921729393303394, 0.9796282052993774, 0.8888176083564758, 0.759965181350708, 0.9842883348464966, 0.9952837824821472]], "normalized_score": [0.156735360622406], "tokens": [["\u2581Confeder", "ate", "\u2581Army", "\u2581opens", "\u2581fire", "\u2581on", "\u2581Fort", "\u2581Sum", "ter"]], "entropy": [[3.8709325790405273, 0.48145464062690735, 3.123340606689453, 3.790619373321533, 0.17451176047325134, 0.6019442677497864, 1.1589744091033936, 0.11080341786146164, 0.03888566046953201]], "avg_entropy": [1.4834963017039828], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "slavery", "score_of_biggest_cluster": 0.156735360622406, "score_of_first_answer": 0.156735360622406, "max_score_of_first_answer": 0.9952837824821472, "min_score_of_first_answer": 0.0025921729393303394, "avg_score": 0.156735360622406, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43439837249686114, "std_of_first_answer": 0.43439837249686114, "avg_entropy_of_first_answer": 1.4834963017039828, "lex_sim": 1.0}
{"question": "where does the sun hit the us first", "answer": ["Hawaii"], "scores": [[0.23249375820159912, 0.9807358980178833]], "normalized_score": [0.47750914096832275], "tokens": [["\u2581Hawai", "i"]], "entropy": [[4.364709854125977, 0.13115888833999634]], "avg_entropy": [2.2479343712329865], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the summit of Cadillac Mountain", "score_of_biggest_cluster": 0.47750914096832275, "score_of_first_answer": 0.47750914096832275, "max_score_of_first_answer": 0.9807358980178833, "min_score_of_first_answer": 0.23249375820159912, "avg_score": 0.47750914096832275, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3741210699081421, "std_of_first_answer": 0.3741210699081421, "avg_entropy_of_first_answer": 2.2479343712329865, "lex_sim": 1.0}
{"question": "what information is displayed in clear text from the ftp header", "answer": ["ftp-ip port21-type-of-terminal-device"], "scores": [[0.01696714013814926, 0.994450569152832, 0.030963774770498276, 0.04773519188165665, 0.002561285626143217, 0.003020026721060276, 0.09026361256837845, 0.08350946754217148, 0.0688304454088211, 0.1862417757511139, 0.03799629211425781, 0.9252049922943115, 0.002149031264707446, 0.5839420557022095, 0.609316349029541, 0.06010851263999939]], "normalized_score": [0.060245804488658905], "tokens": [["\u2581f", "tp", "-", "ip", "\u2581port", "2", "1", "-", "type", "-", "of", "-", "term", "inal", "-", "device"]], "entropy": [[5.287896633148193, 0.0608295314013958, 4.501403331756592, 4.610650539398193, 2.1685774326324463, 4.464044570922852, 3.2436158657073975, 4.222871780395508, 5.067513942718506, 3.993680477142334, 4.148282051086426, 0.5700274705886841, 3.512754440307617, 2.008165121078491, 1.7283260822296143, 4.474841117858887]], "avg_entropy": [3.378967524273321], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "all transmissions", "score_of_biggest_cluster": 0.060245804488658905, "score_of_first_answer": 0.060245804488658905, "max_score_of_first_answer": 0.994450569152832, "min_score_of_first_answer": 0.002149031264707446, "avg_score": 0.060245804488658905, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33020716126467703, "std_of_first_answer": 0.33020716126467703, "avg_entropy_of_first_answer": 3.378967524273321, "lex_sim": 1.0}
{"question": "when did taylor swift's first album release", "answer": ["October 24, 2006"], "scores": [[0.8879692554473877, 0.9958016276359558, 0.9418833255767822, 0.9696094393730164, 0.9887701272964478, 0.998505711555481, 0.9999185800552368, 0.9999723434448242, 0.9997519850730896, 0.9974621534347534]], "normalized_score": [0.9773164391517639], "tokens": [["\u2581October", "\u2581", "2", "4", ",", "\u2581", "2", "0", "0", "6"]], "entropy": [[0.7508189678192139, 0.03184666112065315, 0.2776575982570648, 0.18174701929092407, 0.0776575356721878, 0.012791058048605919, 0.000947603490203619, 0.000408693915233016, 0.002461964264512062, 0.02274436503648758]], "avg_entropy": [0.13590814669150858], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "October 24, 2006", "score_of_biggest_cluster": 0.9773164391517639, "score_of_first_answer": 0.9773164391517639, "max_score_of_first_answer": 0.9999723434448242, "min_score_of_first_answer": 0.8879692554473877, "avg_score": 0.9773164391517639, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03485519775166541, "std_of_first_answer": 0.03485519775166541, "avg_entropy_of_first_answer": 0.13590814669150858, "lex_sim": 1.0}
{"question": "who wrote you must have been a beautiful baby", "answer": ["Sammy Cahn"], "scores": [[0.00814326386898756, 0.92275470495224, 0.783968985080719, 0.9875713586807251]], "normalized_score": [0.2761773467063904], "tokens": [["\u2581Sam", "my", "\u2581C", "ahn"]], "entropy": [[4.927596092224121, 0.42977291345596313, 0.9012929201126099, 0.08555398136377335]], "avg_entropy": [1.5860539767891169], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Johnny Mercer", "score_of_biggest_cluster": 0.2761773467063904, "score_of_first_answer": 0.2761773467063904, "max_score_of_first_answer": 0.9875713586807251, "min_score_of_first_answer": 0.00814326386898756, "avg_score": 0.2761773467063904, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3923180814034232, "std_of_first_answer": 0.3923180814034232, "avg_entropy_of_first_answer": 1.5860539767891169, "lex_sim": 1.0}
{"question": "which greek god flew too close to the sun", "answer": ["Apollo"], "scores": [[0.01506060641258955, 0.999288022518158]], "normalized_score": [0.12267794460058212], "tokens": [["\u2581Apol", "lo"]], "entropy": [[0.7615525722503662, 0.00836282130330801]], "avg_entropy": [0.3849576967768371], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Icarus", "score_of_biggest_cluster": 0.12267794460058212, "score_of_first_answer": 0.12267794460058212, "max_score_of_first_answer": 0.999288022518158, "min_score_of_first_answer": 0.01506060641258955, "avg_score": 0.12267794460058212, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4921137080527842, "std_of_first_answer": 0.4921137080527842, "avg_entropy_of_first_answer": 0.3849576967768371, "lex_sim": 1.0}
{"question": "who played zoe hart on hart of dixie", "answer": ["Rachel Bilson"], "scores": [[0.9820479154586792, 0.99186110496521, 0.9897770285606384, 0.9984394907951355]], "normalized_score": [0.9905140995979309], "tokens": [["\u2581Rach", "el", "\u2581Bil", "son"]], "entropy": [[0.1620485931634903, 0.05238262191414833, 0.07727047055959702, 0.0174062829464674]], "avg_entropy": [0.07727699214592576], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Rachel Sarah Bilson", "score_of_biggest_cluster": 0.9905140995979309, "score_of_first_answer": 0.9905140995979309, "max_score_of_first_answer": 0.9984394907951355, "min_score_of_first_answer": 0.9820479154586792, "avg_score": 0.9905140995979309, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.00584902986718861, "std_of_first_answer": 0.00584902986718861, "avg_entropy_of_first_answer": 0.07727699214592576, "lex_sim": 1.0}
{"question": "when was the last wolf pack killed in yellowstone", "answer": ["1923"], "scores": [[0.37061068415641785, 0.6395279169082642, 0.9511147737503052, 0.744554340839386, 0.7608538866043091]], "normalized_score": [0.6625850200653076], "tokens": [["\u2581", "1", "9", "2", "3"]], "entropy": [[2.696882724761963, 0.8727208375930786, 0.27681899070739746, 1.075056791305542, 0.8536736369132996]], "avg_entropy": [1.1550305962562561], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1926", "score_of_biggest_cluster": 0.6625850200653076, "score_of_first_answer": 0.6625850200653076, "max_score_of_first_answer": 0.9511147737503052, "min_score_of_first_answer": 0.37061068415641785, "avg_score": 0.6625850200653076, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19009409189699428, "std_of_first_answer": 0.19009409189699428, "avg_entropy_of_first_answer": 1.1550305962562561, "lex_sim": 1.0}
{"question": "who invented the first home video security system", "answer": ["James P. Sullivan"], "scores": [[0.013483142480254173, 0.06673114746809006, 0.618737518787384, 0.2496044635772705, 0.455331414937973, 0.9771052598953247]], "normalized_score": [0.19884955883026123], "tokens": [["\u2581James", "\u2581P", ".", "\u2581S", "ull", "ivan"]], "entropy": [[5.503994941711426, 4.026420593261719, 2.793269634246826, 4.286208152770996, 1.2852234840393066, 0.1635318398475647]], "avg_entropy": [3.009774774312973], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "her husband Albert Brown", "score_of_biggest_cluster": 0.19884955883026123, "score_of_first_answer": 0.19884955883026123, "max_score_of_first_answer": 0.9771052598953247, "min_score_of_first_answer": 0.013483142480254173, "avg_score": 0.19884955883026123, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3334116984177557, "std_of_first_answer": 0.3334116984177557, "avg_entropy_of_first_answer": 3.009774774312973, "lex_sim": 1.0}
{"question": "chemicals produced in one part of a plant that affect the growth and response of other parts", "answer": ["plant hormones"], "scores": [[0.1985468566417694, 0.867062509059906, 0.9998669624328613, 0.8232266306877136]], "normalized_score": [0.6135412454605103], "tokens": [["\u2581plant", "\u2581h", "orm", "ones"]], "entropy": [[2.151689052581787, 0.4437188506126404, 0.001963510410860181, 0.47915777564048767]], "avg_entropy": [0.7691322973114438], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Plant hormones", "score_of_biggest_cluster": 0.6135412454605103, "score_of_first_answer": 0.6135412454605103, "max_score_of_first_answer": 0.9998669624328613, "min_score_of_first_answer": 0.1985468566417694, "avg_score": 0.6135412454605103, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.30923416728291975, "std_of_first_answer": 0.30923416728291975, "avg_entropy_of_first_answer": 0.7691322973114438, "lex_sim": 1.0}
{"question": "when is season 7 race to the edge coming out", "answer": ["December 3, 2016"], "scores": [[0.03601875901222229, 0.9702605605125427, 0.23815611004829407, 0.2603573799133301, 0.9980633854866028, 0.9989927411079407, 0.9998383522033691, 0.9940007328987122, 0.7409846782684326]], "normalized_score": [0.4887393116950989], "tokens": [["\u2581December", "\u2581", "3", ",", "\u2581", "2", "0", "1", "6"]], "entropy": [[3.7282800674438477, 0.17605039477348328, 1.4616808891296387, 1.2578072547912598, 0.019642598927021027, 0.00932325329631567, 0.0021078651770949364, 0.04241485893726349, 1.0318760871887207]], "avg_entropy": [0.8587981410738494], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "February 16, 2018", "score_of_biggest_cluster": 0.4887393116950989, "score_of_first_answer": 0.4887393116950989, "max_score_of_first_answer": 0.9998383522033691, "min_score_of_first_answer": 0.03601875901222229, "avg_score": 0.4887393116950989, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37657953714643994, "std_of_first_answer": 0.37657953714643994, "avg_entropy_of_first_answer": 0.8587981410738494, "lex_sim": 1.0}
{"question": "when does closure of the ductus arteriosus occur", "answer": ["39 weeks in gestation"], "scores": [[0.4850449562072754, 0.16802385449409485, 0.04700532555580139, 0.3984058201313019, 0.0071386001072824, 0.09370332211256027, 0.867630660533905]], "normalized_score": [0.13656289875507355], "tokens": [["\u2581", "3", "9", "\u2581weeks", "\u2581in", "\u2581gest", "ation"]], "entropy": [[3.0790038108825684, 1.6718635559082031, 2.4960665702819824, 2.079150438308716, 1.709374189376831, 2.6389877796173096, 0.41774246096611023]], "avg_entropy": [2.0131698293345317], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "at birth", "score_of_biggest_cluster": 0.13656289875507355, "score_of_first_answer": 0.13656289875507355, "max_score_of_first_answer": 0.867630660533905, "min_score_of_first_answer": 0.0071386001072824, "avg_score": 0.13656289875507355, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28678613632745636, "std_of_first_answer": 0.28678613632745636, "avg_entropy_of_first_answer": 2.0131698293345317, "lex_sim": 1.0}
{"question": "when is the next scandal episode coming out", "answer": ["October 3, 2019"], "scores": [[0.20587702095508575, 0.99210524559021, 0.16453640162944794, 0.6139324307441711, 0.9961657524108887, 0.9970272183418274, 0.9998070597648621, 0.9936915636062622, 0.31853431463241577]], "normalized_score": [0.5713194012641907], "tokens": [["\u2581October", "\u2581", "3", ",", "\u2581", "2", "0", "1", "9"]], "entropy": [[3.286191463470459, 0.0621294304728508, 1.8626856803894043, 1.2249398231506348, 0.036270733922719955, 0.025499239563941956, 0.0024532487150281668, 0.0390443280339241, 1.5299819707870483]], "avg_entropy": [0.8965773242784457], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "February\u00a01,\u00a02018", "score_of_biggest_cluster": 0.5713194012641907, "score_of_first_answer": 0.5713194012641907, "max_score_of_first_answer": 0.9998070597648621, "min_score_of_first_answer": 0.16453640162944794, "avg_score": 0.5713194012641907, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35295089036523664, "std_of_first_answer": 0.35295089036523664, "avg_entropy_of_first_answer": 0.8965773242784457, "lex_sim": 1.0}
{"question": "what is the full form of cfc's", "answer": ["chlorofluorocarbons"], "scores": [[0.5519641637802124, 0.9743968844413757, 0.9631714820861816, 0.9218599796295166, 0.9830471873283386, 0.9980676770210266, 0.9987257122993469, 0.8759127855300903, 0.9988083839416504]], "normalized_score": [0.9055330753326416], "tokens": [["\u2581ch", "lor", "of", "lu", "or", "oc", "ar", "b", "ons"]], "entropy": [[1.3979753255844116, 0.20274971425533295, 0.18707354366779327, 0.3804532289505005, 0.09710336476564407, 0.01768317073583603, 0.01480191946029663, 0.3909909725189209, 0.011267133988440037]], "avg_entropy": [0.3000109304363529], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Chlorofluorocarbons", "score_of_biggest_cluster": 0.9055330753326416, "score_of_first_answer": 0.9055330753326416, "max_score_of_first_answer": 0.9988083839416504, "min_score_of_first_answer": 0.5519641637802124, "avg_score": 0.9055330753326416, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1352678024864522, "std_of_first_answer": 0.1352678024864522, "avg_entropy_of_first_answer": 0.3000109304363529, "lex_sim": 1.0}
{"question": "when is the last time the vikings were in the nfc championship", "answer": ["2000"], "scores": [[0.3336881697177887, 0.8481420874595642, 0.9890477061271667, 0.7541083693504333, 0.11294243484735489]], "normalized_score": [0.4736565351486206], "tokens": [["\u2581", "2", "0", "0", "0"]], "entropy": [[1.5976414680480957, 0.5850464701652527, 0.08742834627628326, 0.5678445100784302, 0.6698691248893738]], "avg_entropy": [0.7015659838914872], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1976", "score_of_biggest_cluster": 0.4736565351486206, "score_of_first_answer": 0.4736565351486206, "max_score_of_first_answer": 0.9890477061271667, "min_score_of_first_answer": 0.11294243484735489, "avg_score": 0.4736565351486206, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3300125292031457, "std_of_first_answer": 0.3300125292031457, "avg_entropy_of_first_answer": 0.7015659838914872, "lex_sim": 1.0}
{"question": "who has won the 2018 formula one australia grand prix tournament", "answer": ["Lewis Hamilton"], "scores": [[0.6004185676574707, 0.7790939211845398]], "normalized_score": [0.6839462518692017], "tokens": [["\u2581Lewis", "\u2581Hamilton"]], "entropy": [[1.8450300693511963, 0.9401674270629883]], "avg_entropy": [1.3925987482070923], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ferrari driver Sebastian Vettel", "score_of_biggest_cluster": 0.6839462518692017, "score_of_first_answer": 0.6839462518692017, "max_score_of_first_answer": 0.7790939211845398, "min_score_of_first_answer": 0.6004185676574707, "avg_score": 0.6839462518692017, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08933767676353455, "std_of_first_answer": 0.08933767676353455, "avg_entropy_of_first_answer": 1.3925987482070923, "lex_sim": 1.0}
{"question": "when did the first fleet arive in australia", "answer": ["January 1803"], "scores": [[0.39243194460868835, 0.9886355400085449, 0.3797668218612671, 0.8123717904090881, 0.021148979663848877, 0.05769360810518265]], "normalized_score": [0.22948181629180908], "tokens": [["\u2581January", "\u2581", "1", "8", "0", "3"]], "entropy": [[1.3984553813934326, 0.0817035436630249, 0.6940398216247559, 0.783605694770813, 0.5009913444519043, 1.917489767074585]], "avg_entropy": [0.8960475921630859], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "18 January 1788", "score_of_biggest_cluster": 0.22948181629180908, "score_of_first_answer": 0.22948181629180908, "max_score_of_first_answer": 0.9886355400085449, "min_score_of_first_answer": 0.021148979663848877, "avg_score": 0.22948181629180908, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.357566455613953, "std_of_first_answer": 0.357566455613953, "avg_entropy_of_first_answer": 0.8960475921630859, "lex_sim": 1.0}
{"question": "batman and robin episode only fools and horses", "answer": ["S7E02: The Legend of Badly Injured Ploverman"], "scores": [[0.009701620787382126, 0.006898655090481043, 0.38725730776786804, 0.14728792011737823, 0.1012614443898201, 0.07215356081724167, 0.10909135639667511, 0.008580340072512627, 0.3331436216831207, 0.8406140804290771, 0.0026634903624653816, 0.7299249172210693, 0.02433972805738449, 0.9119563102722168, 0.9214175939559937, 0.005196373909711838, 0.027865350246429443, 0.6048908829689026, 0.004038472659885883]], "normalized_score": [0.06962159276008606], "tokens": [["\u2581S", "7", "E", "0", "2", ":", "\u2581The", "\u2581Leg", "end", "\u2581of", "\u2581Bad", "ly", "\u2581In", "j", "ured", "\u2581P", "lo", "ver", "man"]], "entropy": [[4.8319807052612305, 2.296313762664795, 1.9152631759643555, 2.296558141708374, 2.292757272720337, 2.4861319065093994, 4.821887969970703, 4.789142608642578, 1.1781448125839233, 0.7774266004562378, 3.5905704498291016, 1.4397869110107422, 4.593486309051514, 0.6195392608642578, 0.4040653109550476, 4.116361618041992, 4.074089527130127, 1.3000881671905518, 3.262606620788574]], "avg_entropy": [2.6887474279654655], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "\"Heroes and Villains\"", "score_of_biggest_cluster": 0.06962159276008606, "score_of_first_answer": 0.06962159276008606, "max_score_of_first_answer": 0.9214175939559937, "min_score_of_first_answer": 0.0026634903624653816, "avg_score": 0.06962159276008606, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.33622698340766305, "std_of_first_answer": 0.33622698340766305, "avg_entropy_of_first_answer": 2.6887474279654655, "lex_sim": 1.0}
{"question": "who played the wicked witch in wicked on broadway", "answer": ["Kristin Chenoweth"], "scores": [[0.06445468962192535, 0.9013851881027222, 0.9943577647209167, 0.9967290163040161, 0.9644999504089355]], "normalized_score": [0.5609412789344788], "tokens": [["\u2581Krist", "in", "\u2581Chen", "ow", "eth"]], "entropy": [[3.2256693840026855, 0.376639187335968, 0.05812321975827217, 0.03350168466567993, 0.17079661786556244]], "avg_entropy": [0.7729460187256336], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rachel Kelly Tucker", "score_of_biggest_cluster": 0.5609412789344788, "score_of_first_answer": 0.5609412789344788, "max_score_of_first_answer": 0.9967290163040161, "min_score_of_first_answer": 0.06445468962192535, "avg_score": 0.5609412789344788, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36155458362021525, "std_of_first_answer": 0.36155458362021525, "avg_entropy_of_first_answer": 0.7729460187256336, "lex_sim": 1.0}
{"question": "what is the common name for gravitational force", "answer": ["weight"], "scores": [[0.07496380060911179]], "normalized_score": [0.07496380060911179], "tokens": [["\u2581weight"]], "entropy": [[2.015073776245117]], "avg_entropy": [2.015073776245117], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gravity", "score_of_biggest_cluster": 0.07496380060911179, "score_of_first_answer": 0.07496380060911179, "max_score_of_first_answer": 0.07496380060911179, "min_score_of_first_answer": 0.07496380060911179, "avg_score": 0.07496380060911179, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.015073776245117, "lex_sim": 1.0}
{"question": "what was one reason south carolina gave for its decision to secede from the union", "answer": ["slavery"], "scores": [[0.09280908107757568, 0.9979508519172668]], "normalized_score": [0.30433350801467896], "tokens": [["\u2581sla", "very"]], "entropy": [[4.493686676025391, 0.017534850165247917]], "avg_entropy": [2.2556107630953193], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "to preserve slavery", "score_of_biggest_cluster": 0.30433350801467896, "score_of_first_answer": 0.30433350801467896, "max_score_of_first_answer": 0.9979508519172668, "min_score_of_first_answer": 0.09280908107757568, "avg_score": 0.30433350801467896, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4525708854198456, "std_of_first_answer": 0.4525708854198456, "avg_entropy_of_first_answer": 2.2556107630953193, "lex_sim": 1.0}
{"question": "who won the 30 man royal rumble match", "answer": ["The Miz"], "scores": [[0.0223371759057045, 0.30181753635406494, 0.9833657145500183]], "normalized_score": [0.18785782158374786], "tokens": [["\u2581The", "\u2581M", "iz"]], "entropy": [[3.9210100173950195, 2.1940536499023438, 0.11400982737541199]], "avg_entropy": [2.076357831557592], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Shinsuke Nakamura", "score_of_biggest_cluster": 0.18785782158374786, "score_of_first_answer": 0.18785782158374786, "max_score_of_first_answer": 0.9833657145500183, "min_score_of_first_answer": 0.0223371759057045, "avg_score": 0.18785782158374786, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40362152832785125, "std_of_first_answer": 0.40362152832785125, "avg_entropy_of_first_answer": 2.076357831557592, "lex_sim": 1.0}
{"question": "what do the 3 dots mean in math", "answer": ["Dot Product"], "scores": [[0.007429519668221474, 0.25981128215789795, 0.17862574756145477]], "normalized_score": [0.07012200355529785], "tokens": [["\u2581D", "ot", "\u2581Product"]], "entropy": [[5.144216537475586, 1.9487841129302979, 2.6440391540527344]], "avg_entropy": [3.2456799348195395], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "therefore sign", "score_of_biggest_cluster": 0.07012200355529785, "score_of_first_answer": 0.07012200355529785, "max_score_of_first_answer": 0.25981128215789795, "min_score_of_first_answer": 0.007429519668221474, "avg_score": 0.07012200355529785, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10519600410132696, "std_of_first_answer": 0.10519600410132696, "avg_entropy_of_first_answer": 3.2456799348195395, "lex_sim": 1.0}
{"question": "who won three stanley cups in a row", "answer": ["Montreal Canadiens"], "scores": [[0.29105430841445923, 0.9770406484603882, 0.9369014501571655]], "normalized_score": [0.6434678435325623], "tokens": [["\u2581Montreal", "\u2581Canad", "iens"]], "entropy": [[3.148174524307251, 0.18149910867214203, 0.25576460361480713]], "avg_entropy": [1.1951460788647335], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Toronto Maple Leafs", "score_of_biggest_cluster": 0.6434678435325623, "score_of_first_answer": 0.6434678435325623, "max_score_of_first_answer": 0.9770406484603882, "min_score_of_first_answer": 0.29105430841445923, "avg_score": 0.6434678435325623, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3143435743236756, "std_of_first_answer": 0.3143435743236756, "avg_entropy_of_first_answer": 1.1951460788647335, "lex_sim": 1.0}
{"question": "when was the canadian pacific railway started and finished", "answer": ["1881-1885"], "scores": [[0.4241451025009155, 0.8794286847114563, 0.8726639747619629, 0.6424538493156433, 0.7583898901939392, 0.19484557211399078, 0.9050601720809937, 0.9388659596443176, 0.9695107340812683, 0.9557980895042419]], "normalized_score": [0.6896340847015381], "tokens": [["\u2581", "1", "8", "8", "1", "-", "1", "8", "8", "5"]], "entropy": [[2.9333748817443848, 0.5205074548721313, 0.6278252601623535, 1.0304861068725586, 0.8733400702476501, 2.3434195518493652, 0.5896960496902466, 0.2539467215538025, 0.14191071689128876, 0.24242019653320312]], "avg_entropy": [0.9556927010416985], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "between 1881 and 1885", "score_of_biggest_cluster": 0.6896340847015381, "score_of_first_answer": 0.6896340847015381, "max_score_of_first_answer": 0.9695107340812683, "min_score_of_first_answer": 0.19484557211399078, "avg_score": 0.6896340847015381, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2465552768400774, "std_of_first_answer": 0.2465552768400774, "avg_entropy_of_first_answer": 0.9556927010416985, "lex_sim": 1.0}
{"question": "all the gases in the earth's atmosphere", "answer": ["water vapor, carbon dioxide, nitrogen, oxygen, and small amounts of exotic gases"], "scores": [[0.031388912349939346, 0.8709843754768372, 0.8428449034690857, 0.851648211479187, 0.6019719243049622, 0.9894866943359375, 0.9996622800827026, 0.999083399772644, 0.9737926125526428, 0.16764338314533234, 0.933867335319519, 0.9987393021583557, 0.8708721399307251, 0.6176287531852722, 0.9112060070037842, 0.7324787378311157, 0.195079043507576, 0.015654608607292175, 0.9119816422462463, 0.9869157075881958, 0.0003238267090637237, 0.8945938348770142, 0.8973676562309265, 0.975229799747467]], "normalized_score": [0.4076952338218689], "tokens": [["\u2581water", "\u2581v", "apor", ",", "\u2581carbon", "\u2581dio", "x", "ide", ",", "\u2581nit", "ro", "gen", ",", "\u2581o", "xygen", ",", "\u2581and", "\u2581small", "\u2581amounts", "\u2581of", "\u2581ex", "otic", "\u2581g", "ases"]], "entropy": [[4.122954845428467, 0.5776433944702148, 0.4468161463737488, 0.7488570213317871, 1.3980088233947754, 0.08789950609207153, 0.0038059456273913383, 0.01076988223940134, 0.1734198033809662, 1.7018592357635498, 0.2843487560749054, 0.01504815649241209, 0.6591137647628784, 1.5940732955932617, 0.3439885973930359, 0.8523203134536743, 1.9865829944610596, 1.6976191997528076, 0.5320665836334229, 0.11075551807880402, 1.8873929977416992, 0.5684342980384827, 0.7439673542976379, 0.12376502901315689]], "avg_entropy": [0.8613129776204005], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Water vapor", "score_of_biggest_cluster": 0.4076952338218689, "score_of_first_answer": 0.4076952338218689, "max_score_of_first_answer": 0.9996622800827026, "min_score_of_first_answer": 0.0003238267090637237, "avg_score": 0.4076952338218689, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34518816625881765, "std_of_first_answer": 0.34518816625881765, "avg_entropy_of_first_answer": 0.8613129776204005, "lex_sim": 1.0}
{"question": "the winner of the best actor award in the 64th national film festival 2017", "answer": ["Akshay Kumar"], "scores": [[0.1810607761144638, 0.9496757984161377, 0.9904541969299316, 0.9424686431884766, 0.9989259839057922]], "normalized_score": [0.6934367418289185], "tokens": [["\u2581Ak", "sh", "ay", "\u2581K", "umar"]], "entropy": [[2.4818806648254395, 0.2840956449508667, 0.06269841641187668, 0.33609941601753235, 0.010711923241615295]], "avg_entropy": [0.6350972130894661], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Akshay Kumar", "score_of_biggest_cluster": 0.6934367418289185, "score_of_first_answer": 0.6934367418289185, "max_score_of_first_answer": 0.9989259839057922, "min_score_of_first_answer": 0.1810607761144638, "avg_score": 0.6934367418289185, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3164954608098396, "std_of_first_answer": 0.3164954608098396, "avg_entropy_of_first_answer": 0.6350972130894661, "lex_sim": 1.0}
{"question": "who took part in the american civil war", "answer": ["all of the above"], "scores": [[0.005708630662411451, 0.397676020860672, 0.7385740876197815, 0.24563266336917877]], "normalized_score": [0.142457515001297], "tokens": [["\u2581all", "\u2581of", "\u2581the", "\u2581above"]], "entropy": [[4.115227699279785, 2.853548526763916, 1.2985823154449463, 3.1690759658813477]], "avg_entropy": [2.859108626842499], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "nationalists of the Union", "score_of_biggest_cluster": 0.142457515001297, "score_of_first_answer": 0.142457515001297, "max_score_of_first_answer": 0.7385740876197815, "min_score_of_first_answer": 0.005708630662411451, "avg_score": 0.142457515001297, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2658257978298529, "std_of_first_answer": 0.2658257978298529, "avg_entropy_of_first_answer": 2.859108626842499, "lex_sim": 1.0}
{"question": "where are alkali metals located on the periodic table", "answer": ["group 1 columns of the periodic table"], "scores": [[0.2134704738855362, 0.7730337381362915, 0.9959226846694946, 0.001238117110915482, 0.05968744307756424, 0.7587841153144836, 0.9386407732963562, 0.9960378408432007]], "normalized_score": [0.23276226222515106], "tokens": [["\u2581group", "\u2581", "1", "\u2581columns", "\u2581of", "\u2581the", "\u2581periodic", "\u2581table"]], "entropy": [[2.8310556411743164, 0.8413293957710266, 0.0327555388212204, 1.2794203758239746, 1.7917752265930176, 1.3587548732757568, 0.42078346014022827, 0.03730197250843048]], "avg_entropy": [1.0741470605134964], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "in the s-block", "score_of_biggest_cluster": 0.23276226222515106, "score_of_first_answer": 0.23276226222515106, "max_score_of_first_answer": 0.9960378408432007, "min_score_of_first_answer": 0.001238117110915482, "avg_score": 0.23276226222515106, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4004358364342659, "std_of_first_answer": 0.4004358364342659, "avg_entropy_of_first_answer": 1.0741470605134964, "lex_sim": 1.0}
{"question": "who is the secretary of state for northern ireland", "answer": ["Julian Smith"], "scores": [[0.22094713151454926, 0.7187337875366211]], "normalized_score": [0.39849990606307983], "tokens": [["\u2581Julian", "\u2581Smith"]], "entropy": [[1.5612587928771973, 1.6005535125732422]], "avg_entropy": [1.5809061527252197], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Karen Bradley", "score_of_biggest_cluster": 0.39849990606307983, "score_of_first_answer": 0.39849990606307983, "max_score_of_first_answer": 0.7187337875366211, "min_score_of_first_answer": 0.22094713151454926, "avg_score": 0.39849990606307983, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24889332801103592, "std_of_first_answer": 0.24889332801103592, "avg_entropy_of_first_answer": 1.5809061527252197, "lex_sim": 1.0}
{"question": "who sang rip it up and start again", "answer": ["Orange Juice"], "scores": [[0.7138654589653015, 0.9625324606895447, 0.9995831847190857]], "normalized_score": [0.8823012709617615], "tokens": [["\u2581Orange", "\u2581Ju", "ice"]], "entropy": [[1.6102466583251953, 0.19892819225788116, 0.004541163798421621]], "avg_entropy": [0.6045720047938327], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Scottish post-punk band Orange Juice", "score_of_biggest_cluster": 0.8823012709617615, "score_of_first_answer": 0.8823012709617615, "max_score_of_first_answer": 0.9995831847190857, "min_score_of_first_answer": 0.7138654589653015, "avg_score": 0.8823012709617615, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.12686066403190926, "std_of_first_answer": 0.12686066403190926, "avg_entropy_of_first_answer": 0.6045720047938327, "lex_sim": 1.0}
{"question": "how many nuclear power plants in washington state", "answer": ["3"], "scores": [[0.6246668696403503, 0.10374747216701508]], "normalized_score": [0.25457340478897095], "tokens": [["\u2581", "3"]], "entropy": [[1.8840583562850952, 1.3911032676696777]], "avg_entropy": [1.6375808119773865], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "3", "score_of_biggest_cluster": 0.25457340478897095, "score_of_first_answer": 0.25457340478897095, "max_score_of_first_answer": 0.6246668696403503, "min_score_of_first_answer": 0.10374747216701508, "avg_score": 0.25457340478897095, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.26045969873666763, "std_of_first_answer": 0.26045969873666763, "avg_entropy_of_first_answer": 1.6375808119773865, "lex_sim": 1.0}
{"question": "who is the book of acts written to", "answer": ["Theophilus"], "scores": [[0.7601158618927002, 0.994282603263855, 0.9851973652839661]], "normalized_score": [0.9063674211502075], "tokens": [["\u2581The", "oph", "ilus"]], "entropy": [[1.5245583057403564, 0.0647469013929367, 0.10435245931148529]], "avg_entropy": [0.5645525554815928], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jewish audiences", "score_of_biggest_cluster": 0.9063674211502075, "score_of_first_answer": 0.9063674211502075, "max_score_of_first_answer": 0.994282603263855, "min_score_of_first_answer": 0.7601158618927002, "avg_score": 0.9063674211502075, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10830937554698933, "std_of_first_answer": 0.10830937554698933, "avg_entropy_of_first_answer": 0.5645525554815928, "lex_sim": 1.0}
{"question": "who plays whitey bulger's girlfriend in black mass", "answer": ["Julianne Nicholson"], "scores": [[0.7170318961143494, 0.9758224487304688, 0.9996457099914551, 0.9963573813438416, 0.999971866607666]], "normalized_score": [0.9303185343742371], "tokens": [["\u2581Julian", "ne", "\u2581Nich", "ol", "son"]], "entropy": [[1.281303882598877, 0.1185520812869072, 0.004037339240312576, 0.028883155435323715, 0.0003886626218445599]], "avg_entropy": [0.286633024236653], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "actress Dakota Johnson", "score_of_biggest_cluster": 0.9303185343742371, "score_of_first_answer": 0.9303185343742371, "max_score_of_first_answer": 0.999971866607666, "min_score_of_first_answer": 0.7170318961143494, "avg_score": 0.9303185343742371, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11072800569403891, "std_of_first_answer": 0.11072800569403891, "avg_entropy_of_first_answer": 0.286633024236653, "lex_sim": 1.0}
{"question": "when is the world consumer right day celebrated", "answer": ["March 15"], "scores": [[0.7704811096191406, 0.9890701770782471, 0.9967418313026428, 0.9902524948120117]], "normalized_score": [0.9312782287597656], "tokens": [["\u2581March", "\u2581", "1", "5"]], "entropy": [[1.141956090927124, 0.07762698829174042, 0.02609497308731079, 0.07499229907989502]], "avg_entropy": [0.33016758784651756], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "15 March every year", "score_of_biggest_cluster": 0.9312782287597656, "score_of_first_answer": 0.9312782287597656, "max_score_of_first_answer": 0.9967418313026428, "min_score_of_first_answer": 0.7704811096191406, "avg_score": 0.9312782287597656, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.09597425435397547, "std_of_first_answer": 0.09597425435397547, "avg_entropy_of_first_answer": 0.33016758784651756, "lex_sim": 1.0}
{"question": "what was the final episode of quantum leap", "answer": ["Mirror Image"], "scores": [[0.8767629861831665, 0.9999306201934814, 0.9850625991821289]], "normalized_score": [0.9522960186004639], "tokens": [["\u2581Mir", "ror", "\u2581Image"]], "entropy": [[0.9321155548095703, 0.0010086549445986748, 0.09515934437513351]], "avg_entropy": [0.3427611847097675], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "\"Mirror Image\"", "score_of_biggest_cluster": 0.9522960186004639, "score_of_first_answer": 0.9522960186004639, "max_score_of_first_answer": 0.9999306201934814, "min_score_of_first_answer": 0.8767629861831665, "avg_score": 0.9522960186004639, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.054893968796639554, "std_of_first_answer": 0.054893968796639554, "avg_entropy_of_first_answer": 0.3427611847097675, "lex_sim": 1.0}
{"question": "who said if a tree falls in the woods and nobody hears it", "answer": ["George Berkeley"], "scores": [[0.4441698491573334, 0.8874639272689819, 0.926178514957428]], "normalized_score": [0.7147126793861389], "tokens": [["\u2581George", "\u2581Ber", "keley"]], "entropy": [[3.7912185192108154, 0.8157891035079956, 0.3241814076900482]], "avg_entropy": [1.643729676802953], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Philosopher George Berkeley", "score_of_biggest_cluster": 0.7147126793861389, "score_of_first_answer": 0.7147126793861389, "max_score_of_first_answer": 0.926178514957428, "min_score_of_first_answer": 0.4441698491573334, "avg_score": 0.7147126793861389, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.21866788945164461, "std_of_first_answer": 0.21866788945164461, "avg_entropy_of_first_answer": 1.643729676802953, "lex_sim": 1.0}
{"question": "how many players in football hall of fame", "answer": ["263"], "scores": [[0.9129825830459595, 0.35261568427085876, 0.10899510234594345, 0.06961879134178162]], "normalized_score": [0.2223178595304489], "tokens": [["\u2581", "2", "6", "3"]], "entropy": [[0.7373576164245605, 1.645072340965271, 2.3227341175079346, 2.158388376235962]], "avg_entropy": [1.715888112783432], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "318", "score_of_biggest_cluster": 0.2223178595304489, "score_of_first_answer": 0.2223178595304489, "max_score_of_first_answer": 0.9129825830459595, "min_score_of_first_answer": 0.06961879134178162, "avg_score": 0.2223178595304489, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33658748293869245, "std_of_first_answer": 0.33658748293869245, "avg_entropy_of_first_answer": 1.715888112783432, "lex_sim": 1.0}
{"question": "when did the first battle of ypres end", "answer": ["November 22, 1914"], "scores": [[0.8066182732582092, 0.9897435307502747, 0.7778836488723755, 0.9768726825714111, 0.8849607110023499, 0.9973320960998535, 0.997295081615448, 0.9997714161872864, 0.9996089339256287, 0.9985495209693909]], "normalized_score": [0.9389946460723877], "tokens": [["\u2581November", "\u2581", "2", "2", ",", "\u2581", "1", "9", "1", "4"]], "entropy": [[1.0058658123016357, 0.07950276136398315, 0.6871753334999084, 0.16090303659439087, 0.46757641434669495, 0.022595159709453583, 0.01980889029800892, 0.0026754667051136494, 0.004291635937988758, 0.01344806794077158]], "avg_entropy": [0.24638425786979495], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "22 November 1914", "score_of_biggest_cluster": 0.9389946460723877, "score_of_first_answer": 0.9389946460723877, "max_score_of_first_answer": 0.9997714161872864, "min_score_of_first_answer": 0.7778836488723755, "avg_score": 0.9389946460723877, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08244836020795218, "std_of_first_answer": 0.08244836020795218, "avg_entropy_of_first_answer": 0.24638425786979495, "lex_sim": 1.0}
{"question": "what is the biggest town in west virginia", "answer": ["Charleston"], "scores": [[0.5511155724525452, 0.9994706511497498, 0.9965195655822754]], "normalized_score": [0.8187780380249023], "tokens": [["\u2581Char", "l", "eston"]], "entropy": [[2.093015193939209, 0.006303333677351475, 0.028796929866075516]], "avg_entropy": [0.7093718191608787], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Charleston", "score_of_biggest_cluster": 0.8187780380249023, "score_of_first_answer": 0.8187780380249023, "max_score_of_first_answer": 0.9994706511497498, "min_score_of_first_answer": 0.5511155724525452, "avg_score": 0.8187780380249023, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.21066447852120782, "std_of_first_answer": 0.21066447852120782, "avg_entropy_of_first_answer": 0.7093718191608787, "lex_sim": 1.0}
{"question": "what is money as a medium of exchange", "answer": ["Medium of exchange."], "scores": [[0.04790569841861725, 0.9870173335075378, 0.9607646465301514, 0.9110062122344971, 0.02251770719885826]], "normalized_score": [0.24767087399959564], "tokens": [["\u2581Med", "ium", "\u2581of", "\u2581exchange", "."]], "entropy": [[4.294088363647461, 0.09053147584199905, 0.27938419580459595, 0.39390942454338074, 1.331998586654663]], "avg_entropy": [1.27798240929842], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "fiat money", "score_of_biggest_cluster": 0.24767087399959564, "score_of_first_answer": 0.24767087399959564, "max_score_of_first_answer": 0.9870173335075378, "min_score_of_first_answer": 0.02251770719885826, "avg_score": 0.24767087399959564, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45032213523933956, "std_of_first_answer": 0.45032213523933956, "avg_entropy_of_first_answer": 1.27798240929842, "lex_sim": 1.0}
{"question": "who sings i feel love with the blue man group", "answer": ["David Hasselhoff"], "scores": [[0.0052550132386386395, 0.05594726651906967, 0.9740151166915894, 0.9820401072502136]], "normalized_score": [0.129497691988945], "tokens": [["\u2581David", "\u2581Hass", "el", "hoff"]], "entropy": [[4.546589374542236, 4.191970348358154, 0.159991055727005, 0.1145758330821991]], "avg_entropy": [2.2532816529273987], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Annette Strean", "score_of_biggest_cluster": 0.129497691988945, "score_of_first_answer": 0.129497691988945, "max_score_of_first_answer": 0.9820401072502136, "min_score_of_first_answer": 0.0052550132386386395, "avg_score": 0.129497691988945, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.47406064288465233, "std_of_first_answer": 0.47406064288465233, "avg_entropy_of_first_answer": 2.2532816529273987, "lex_sim": 1.0}
{"question": "who sings the song i'll never forget you", "answer": ["Aaliyah"], "scores": [[0.020146634429693222, 0.007546292617917061, 0.997651994228363, 0.9989705085754395]], "normalized_score": [0.1109473779797554], "tokens": [["\u2581A", "ali", "y", "ah"]], "entropy": [[5.591311931610107, 3.93776798248291, 0.0205729641020298, 0.010316587053239346]], "avg_entropy": [2.3899923663120717], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mariah Carey", "score_of_biggest_cluster": 0.1109473779797554, "score_of_first_answer": 0.1109473779797554, "max_score_of_first_answer": 0.9989705085754395, "min_score_of_first_answer": 0.007546292617917061, "avg_score": 0.1109473779797554, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.49225277351143315, "std_of_first_answer": 0.49225277351143315, "avg_entropy_of_first_answer": 2.3899923663120717, "lex_sim": 1.0}
{"question": "where is wake forest located in north carolina", "answer": ["Wake Forest, North Carolina"], "scores": [[0.884567379951477, 0.1957564800977707, 0.8328919410705566, 0.4678347408771515, 0.7516222596168518, 0.9988059997558594]], "normalized_score": [0.6082771420478821], "tokens": [["\u2581W", "ake", "\u2581Forest", ",", "\u2581North", "\u2581Carolina"]], "entropy": [[0.9205117225646973, 0.5500916838645935, 0.5657368302345276, 1.2190145254135132, 0.842544674873352, 0.013103043660521507]], "avg_entropy": [0.6851670801018676], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "78.51889\u00b0W", "score_of_biggest_cluster": 0.6082771420478821, "score_of_first_answer": 0.6082771420478821, "max_score_of_first_answer": 0.9988059997558594, "min_score_of_first_answer": 0.1957564800977707, "avg_score": 0.6082771420478821, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27418211042167895, "std_of_first_answer": 0.27418211042167895, "avg_entropy_of_first_answer": 0.6851670801018676, "lex_sim": 1.0}
{"question": "actress who plays penelope garcia on criminal minds", "answer": ["Kirsten Vangsness"], "scores": [[0.9421209096908569, 0.9921956062316895, 0.9981264472007751, 0.984078049659729, 0.9777147769927979, 0.9938015341758728]], "normalized_score": [0.9811569452285767], "tokens": [["\u2581K", "irst", "en", "\u2581V", "angs", "ness"]], "entropy": [[0.4723584055900574, 0.07208304852247238, 0.017579570412635803, 0.15375259518623352, 0.1334429830312729, 0.040152907371520996]], "avg_entropy": [0.14822825168569884], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Kirsten Simone Vangsness", "score_of_biggest_cluster": 0.9811569452285767, "score_of_first_answer": 0.9811569452285767, "max_score_of_first_answer": 0.9981264472007751, "min_score_of_first_answer": 0.9421209096908569, "avg_score": 0.9811569452285767, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.018769190319832763, "std_of_first_answer": 0.018769190319832763, "avg_entropy_of_first_answer": 0.14822825168569884, "lex_sim": 1.0}
{"question": "when does the call of the wild take place", "answer": ["1903"], "scores": [[0.6263367533683777, 0.9874995350837708, 0.302944540977478, 0.7028074264526367, 0.24735566973686218]], "normalized_score": [0.5041656494140625], "tokens": [["\u2581", "1", "9", "0", "3"]], "entropy": [[2.261173725128174, 0.08184068650007248, 0.646399736404419, 1.1964915990829468, 1.6294949054718018]], "avg_entropy": [1.1630801305174827], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1890s", "score_of_biggest_cluster": 0.5041656494140625, "score_of_first_answer": 0.5041656494140625, "max_score_of_first_answer": 0.9874995350837708, "min_score_of_first_answer": 0.24735566973686218, "avg_score": 0.5041656494140625, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2722050943012742, "std_of_first_answer": 0.2722050943012742, "avg_entropy_of_first_answer": 1.1630801305174827, "lex_sim": 1.0}
{"question": "french film pioneer who introduced newsreel in 1910", "answer": ["L\u00e9on Gaumont"], "scores": [[0.02812492661178112, 0.7778265476226807, 0.9193426370620728, 0.9999502897262573, 0.9986621141433716]], "normalized_score": [0.45768818259239197], "tokens": [["\u2581L\u00e9", "on", "\u2581Ga", "um", "ont"]], "entropy": [[3.386484146118164, 0.6221430897712708, 0.6368647813796997, 0.0006414406816475093, 0.01541988831013441]], "avg_entropy": [0.9323106692521833], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charles Path\u00e9", "score_of_biggest_cluster": 0.45768818259239197, "score_of_first_answer": 0.45768818259239197, "max_score_of_first_answer": 0.9999502897262573, "min_score_of_first_answer": 0.02812492661178112, "avg_score": 0.45768818259239197, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3673490676218784, "std_of_first_answer": 0.3673490676218784, "avg_entropy_of_first_answer": 0.9323106692521833, "lex_sim": 1.0}
{"question": "who sings angel of the morning in deadpool", "answer": ["The Jim Coughlin's Juice Orchestra"], "scores": [[0.3301693797111511, 0.009071682579815388, 0.0027738723438233137, 0.002568908967077732, 0.3541586399078369, 0.004225998185575008, 0.9958353042602539, 0.055298980325460434, 0.052993420511484146, 0.4061698913574219]], "normalized_score": [0.045480191707611084], "tokens": [["\u2581The", "\u2581Jim", "\u2581C", "ough", "lin", "'", "s", "\u2581Ju", "ice", "\u2581Orchestra"]], "entropy": [[3.535118818283081, 1.6059445142745972, 1.550482988357544, 2.8479435443878174, 1.067854404449463, 3.4807677268981934, 0.05265393108129501, 5.321816921234131, 0.640065610408783, 1.8485052585601807]], "avg_entropy": [2.1951153717935084], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Juice Newton", "score_of_biggest_cluster": 0.045480191707611084, "score_of_first_answer": 0.045480191707611084, "max_score_of_first_answer": 0.9958353042602539, "min_score_of_first_answer": 0.002568908967077732, "avg_score": 0.045480191707611084, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3012022825439443, "std_of_first_answer": 0.3012022825439443, "avg_entropy_of_first_answer": 2.1951153717935084, "lex_sim": 1.0}
{"question": "where are the spanish steps located in italy", "answer": ["Rome"], "scores": [[0.7074528336524963]], "normalized_score": [0.7074528336524963], "tokens": [["\u2581Rome"]], "entropy": [[1.5892326831817627]], "avg_entropy": [1.5892326831817627], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Rome", "score_of_biggest_cluster": 0.7074528336524963, "score_of_first_answer": 0.7074528336524963, "max_score_of_first_answer": 0.7074528336524963, "min_score_of_first_answer": 0.7074528336524963, "avg_score": 0.7074528336524963, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.5892326831817627, "lex_sim": 1.0}
{"question": "when does the turn of the screw take place", "answer": ["1845-1852"], "scores": [[0.5115382075309753, 0.9860636591911316, 0.8972696661949158, 0.046038467437028885, 0.06917507946491241, 0.15293718874454498, 0.7682203054428101, 0.9736243486404419, 0.17350716888904572, 0.04125203564763069]], "normalized_score": [0.25538355112075806], "tokens": [["\u2581", "1", "8", "4", "5", "-", "1", "8", "5", "2"]], "entropy": [[2.771878480911255, 0.08701593428850174, 0.36950910091400146, 0.7172176837921143, 1.3846718072891235, 1.4919294118881226, 0.9226664304733276, 0.1297038048505783, 1.3046847581863403, 1.456417441368103]], "avg_entropy": [1.0635694853961468], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1898", "score_of_biggest_cluster": 0.25538355112075806, "score_of_first_answer": 0.25538355112075806, "max_score_of_first_answer": 0.9860636591911316, "min_score_of_first_answer": 0.04125203564763069, "avg_score": 0.25538355112075806, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38796893818764727, "std_of_first_answer": 0.38796893818764727, "avg_entropy_of_first_answer": 1.0635694853961468, "lex_sim": 1.0}
{"question": "cat in the hat knows a lot about space movie", "answer": ["Puss In Boots"], "scores": [[0.004556659143418074, 0.25160396099090576, 0.07832035422325134, 0.9615201950073242, 0.998783528804779]], "normalized_score": [0.1538628190755844], "tokens": [["\u2581P", "uss", "\u2581In", "\u2581Bo", "ots"]], "entropy": [[2.2138705253601074, 1.8912508487701416, 0.8131187558174133, 0.23966164886951447, 0.010378742590546608]], "avg_entropy": [1.0336561042815446], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Space is the Place", "score_of_biggest_cluster": 0.1538628190755844, "score_of_first_answer": 0.1538628190755844, "max_score_of_first_answer": 0.998783528804779, "min_score_of_first_answer": 0.004556659143418074, "avg_score": 0.1538628190755844, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4332070301466557, "std_of_first_answer": 0.4332070301466557, "avg_entropy_of_first_answer": 1.0336561042815446, "lex_sim": 1.0}
{"question": "when was the reaper invented by cyrus mccormick", "answer": ["1831"], "scores": [[0.818949282169342, 0.9937001466751099, 0.9791749119758606, 0.8318148255348206, 0.8536633253097534]], "normalized_score": [0.8923540115356445], "tokens": [["\u2581", "1", "8", "3", "1"]], "entropy": [[1.2655620574951172, 0.05013483762741089, 0.13051028549671173, 0.765823483467102, 0.6482038497924805]], "avg_entropy": [0.5720469027757644], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1837", "score_of_biggest_cluster": 0.8923540115356445, "score_of_first_answer": 0.8923540115356445, "max_score_of_first_answer": 0.9937001466751099, "min_score_of_first_answer": 0.818949282169342, "avg_score": 0.8923540115356445, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07524742267408166, "std_of_first_answer": 0.07524742267408166, "avg_entropy_of_first_answer": 0.5720469027757644, "lex_sim": 1.0}
{"question": "who made the poppies at tower of london", "answer": ["Tom Piper"], "scores": [[0.18952882289886475, 0.9910843372344971, 0.9613273739814758]], "normalized_score": [0.5652220249176025], "tokens": [["\u2581Tom", "\u2581Pi", "per"]], "entropy": [[1.5914373397827148, 0.08792605996131897, 0.26518121361732483]], "avg_entropy": [0.6481815377871195], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paul Cummins", "score_of_biggest_cluster": 0.5652220249176025, "score_of_first_answer": 0.5652220249176025, "max_score_of_first_answer": 0.9910843372344971, "min_score_of_first_answer": 0.18952882289886475, "avg_score": 0.5652220249176025, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37104203458744545, "std_of_first_answer": 0.37104203458744545, "avg_entropy_of_first_answer": 0.6481815377871195, "lex_sim": 1.0}
{"question": "when did they stop saying mass in latin", "answer": ["November 29, 1964"], "scores": [[0.14968329668045044, 0.9763859510421753, 0.7258911728858948, 0.9552820920944214, 0.9759092926979065, 0.9969459176063538, 0.997044026851654, 0.9997405409812927, 0.9957667589187622, 0.9918633699417114]], "normalized_score": [0.7919705510139465], "tokens": [["\u2581November", "\u2581", "2", "9", ",", "\u2581", "1", "9", "6", "4"]], "entropy": [[3.0539069175720215, 0.1423904001712799, 0.7791885137557983, 0.26585257053375244, 0.15078464150428772, 0.025221195071935654, 0.021083898842334747, 0.0029742144979536533, 0.03363665193319321, 0.059599511325359344]], "avg_entropy": [0.45346385152079166], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "from 1967 onwards", "score_of_biggest_cluster": 0.7919705510139465, "score_of_first_answer": 0.7919705510139465, "max_score_of_first_answer": 0.9997405409812927, "min_score_of_first_answer": 0.14968329668045044, "avg_score": 0.7919705510139465, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25470985911560357, "std_of_first_answer": 0.25470985911560357, "avg_entropy_of_first_answer": 0.45346385152079166, "lex_sim": 1.0}
{"question": "who sponsored the phoenix open before waste management", "answer": ["FBR"], "scores": [[0.4279603660106659, 0.9839019179344177]], "normalized_score": [0.6488998532295227], "tokens": [["\u2581F", "BR"]], "entropy": [[3.1633009910583496, 0.10895901918411255]], "avg_entropy": [1.636130005121231], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Friedman Billings Ramsey", "score_of_biggest_cluster": 0.6488998532295227, "score_of_first_answer": 0.6488998532295227, "max_score_of_first_answer": 0.9839019179344177, "min_score_of_first_answer": 0.4279603660106659, "avg_score": 0.6488998532295227, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2779707759618759, "std_of_first_answer": 0.2779707759618759, "avg_entropy_of_first_answer": 1.636130005121231, "lex_sim": 1.0}
{"question": "where does the river irk start and finish", "answer": ["Manchester"], "scores": [[0.007395213004201651]], "normalized_score": [0.007395213469862938], "tokens": [["\u2581Manchester"]], "entropy": [[4.7417826652526855]], "avg_entropy": [4.7417826652526855], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "east of Royton", "score_of_biggest_cluster": 0.007395213469862938, "score_of_first_answer": 0.007395213469862938, "max_score_of_first_answer": 0.007395213004201651, "min_score_of_first_answer": 0.007395213004201651, "avg_score": 0.007395213469862938, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.7417826652526855, "lex_sim": 1.0}
{"question": "5 cities with the highest population in europe", "answer": ["Moscow, London, St Petersburg, Berlin, Paris"], "scores": [[0.11067896336317062, 0.8032726049423218, 0.3804060220718384, 0.9902425408363342, 0.07725890725851059, 0.2109651416540146, 0.9859117865562439, 0.06865503638982773, 0.8807534575462341, 0.546354353427887]], "normalized_score": [0.33495891094207764], "tokens": [["\u2581Moscow", ",", "\u2581London", ",", "\u2581St", "\u2581Petersburg", ",", "\u2581Berlin", ",", "\u2581Paris"]], "entropy": [[1.9703575372695923, 0.9227513074874878, 1.7100715637207031, 0.08671201020479202, 1.7864753007888794, 0.8478447198867798, 0.11150509864091873, 1.6876270771026611, 0.4841011166572571, 1.8737748861312866]], "avg_entropy": [1.1481220617890358], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "London", "score_of_biggest_cluster": 0.33495891094207764, "score_of_first_answer": 0.33495891094207764, "max_score_of_first_answer": 0.9902425408363342, "min_score_of_first_answer": 0.06865503638982773, "avg_score": 0.33495891094207764, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3646135547532758, "std_of_first_answer": 0.3646135547532758, "avg_entropy_of_first_answer": 1.1481220617890358, "lex_sim": 1.0}
{"question": "when does the new episodes of supernatural start", "answer": ["October 2 (2012)"], "scores": [[0.8289562463760376, 0.9881891012191772, 0.03381822258234024, 0.0004715306858997792, 0.3925657272338867, 0.9802122116088867, 0.9026710987091064, 0.1269768625497818, 0.825147271156311]], "normalized_score": [0.1983581781387329], "tokens": [["\u2581October", "\u2581", "2", "\u2581(", "2", "0", "1", "2", ")"]], "entropy": [[1.0432652235031128, 0.08170339465141296, 1.54154634475708, 2.2374651432037354, 3.587470054626465, 0.15314120054244995, 0.3445036709308624, 1.7863370180130005, 0.832343339920044]], "avg_entropy": [1.2897528211275737], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "October\u00a012,\u00a02017", "score_of_biggest_cluster": 0.1983581781387329, "score_of_first_answer": 0.1983581781387329, "max_score_of_first_answer": 0.9881891012191772, "min_score_of_first_answer": 0.0004715306858997792, "avg_score": 0.1983581781387329, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3979533475406053, "std_of_first_answer": 0.3979533475406053, "avg_entropy_of_first_answer": 1.2897528211275737, "lex_sim": 1.0}
{"question": "who is the actress who plays baker on blue bloods", "answer": ["Amy Carlson"], "scores": [[0.6450600028038025, 0.9980250597000122, 0.9996863603591919]], "normalized_score": [0.8633795976638794], "tokens": [["\u2581Amy", "\u2581Carl", "son"]], "entropy": [[1.818074345588684, 0.022812647745013237, 0.00377913867123425]], "avg_entropy": [0.6148887106683105], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Abigail Hawk", "score_of_biggest_cluster": 0.8633795976638794, "score_of_first_answer": 0.8633795976638794, "max_score_of_first_answer": 0.9996863603591919, "min_score_of_first_answer": 0.6450600028038025, "avg_score": 0.8633795976638794, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.16678227483179903, "std_of_first_answer": 0.16678227483179903, "avg_entropy_of_first_answer": 0.6148887106683105, "lex_sim": 1.0}
{"question": "when did the great fire of london end", "answer": ["September 5, 1666"], "scores": [[0.7747390866279602, 0.9751850366592407, 0.7798863649368286, 0.890276312828064, 0.9972088932991028, 0.9989151954650879, 0.996732234954834, 0.9996438026428223, 0.9993809461593628]], "normalized_score": [0.9299813508987427], "tokens": [["\u2581September", "\u2581", "5", ",", "\u2581", "1", "6", "6", "6"]], "entropy": [[1.28941810131073, 0.15770617127418518, 0.9135310053825378, 0.49460577964782715, 0.025974754244089127, 0.009861178696155548, 0.02463882975280285, 0.003953552804887295, 0.006534370593726635]], "avg_entropy": [0.3251359715229935], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "5 September 1666", "score_of_biggest_cluster": 0.9299813508987427, "score_of_first_answer": 0.9299813508987427, "max_score_of_first_answer": 0.9996438026428223, "min_score_of_first_answer": 0.7747390866279602, "avg_score": 0.9299813508987427, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.09033734238857138, "std_of_first_answer": 0.09033734238857138, "avg_entropy_of_first_answer": 0.3251359715229935, "lex_sim": 1.0}
{"question": "who were the the continent of the americas named after logically", "answer": ["Christopher Columbus"], "scores": [[0.040803030133247375, 0.9786332249641418, 0.9997475743293762]], "normalized_score": [0.3417702317237854], "tokens": [["\u2581Christopher", "\u2581Columb", "us"]], "entropy": [[2.431694984436035, 0.13710199296474457, 0.0030204798094928265]], "avg_entropy": [0.8572724857367575], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Amerigo Vespucci", "score_of_biggest_cluster": 0.3417702317237854, "score_of_first_answer": 0.3417702317237854, "max_score_of_first_answer": 0.9997475743293762, "min_score_of_first_answer": 0.040803030133247375, "avg_score": 0.3417702317237854, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4471571845098758, "std_of_first_answer": 0.4471571845098758, "avg_entropy_of_first_answer": 0.8572724857367575, "lex_sim": 1.0}
{"question": "who's flag is red white and green", "answer": ["Italy"], "scores": [[0.35496240854263306]], "normalized_score": [0.35496240854263306], "tokens": [["\u2581Italy"]], "entropy": [[3.516446590423584]], "avg_entropy": [3.516446590423584], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hungary", "score_of_biggest_cluster": 0.35496240854263306, "score_of_first_answer": 0.35496240854263306, "max_score_of_first_answer": 0.35496240854263306, "min_score_of_first_answer": 0.35496240854263306, "avg_score": 0.35496240854263306, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.516446590423584, "lex_sim": 1.0}
{"question": "who wins the next iron chef super chefs", "answer": ["Christopher Birmingham"], "scores": [[0.004376336932182312, 0.006466577295213938, 0.0034767372999340296]], "normalized_score": [0.004616567865014076], "tokens": [["\u2581Christopher", "\u2581B", "irmingham"]], "entropy": [[5.289984703063965, 4.125359058380127, 4.440812110900879]], "avg_entropy": [4.61871862411499], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Zakarian", "score_of_biggest_cluster": 0.004616567865014076, "score_of_first_answer": 0.004616567865014076, "max_score_of_first_answer": 0.006466577295213938, "min_score_of_first_answer": 0.0034767372999340296, "avg_score": 0.004616567865014076, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0012524432775340984, "std_of_first_answer": 0.0012524432775340984, "avg_entropy_of_first_answer": 4.61871862411499, "lex_sim": 1.0}
{"question": "when did krakow become the capital of poland", "answer": ["30 December 1996"], "scores": [[0.5733128190040588, 0.006023271009325981, 0.22071844339370728, 0.18582068383693695, 0.9782969951629639, 0.9857224225997925, 0.4968009293079376, 0.7930318713188171, 0.012954703532159328]], "normalized_score": [0.20697616040706635], "tokens": [["\u2581", "3", "0", "\u2581December", "\u2581", "1", "9", "9", "6"]], "entropy": [[2.4486730098724365, 0.34900152683258057, 2.2431788444519043, 2.657470226287842, 0.12071040272712708, 0.0890718623995781, 1.4296773672103882, 0.8304353356361389, 1.289560079574585]], "avg_entropy": [1.2730865172213979], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1038", "score_of_biggest_cluster": 0.20697616040706635, "score_of_first_answer": 0.20697616040706635, "max_score_of_first_answer": 0.9857224225997925, "min_score_of_first_answer": 0.006023271009325981, "avg_score": 0.20697616040706635, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36622453534823707, "std_of_first_answer": 0.36622453534823707, "avg_entropy_of_first_answer": 1.2730865172213979, "lex_sim": 1.0}
{"question": "most runs ever scored in a mlb inning", "answer": ["10 runs in 1st inning in 2 games: Baltimore Orioles VS Oakland Athletics, 1973, Baltimore in 8 innings & Oakland Athletics in 9 innings on June 7 & September"], "scores": [[0.8425251841545105, 0.8436309695243835, 0.4389958381652832, 0.1445162147283554, 0.06827568262815475, 0.19357812404632568, 0.6133173108100891, 0.20729802548885345, 0.8635885119438171, 0.9942405223846436, 0.034008678048849106, 0.559273362159729, 0.3523750901222229, 0.006093283649533987, 0.043085504323244095, 0.004790530540049076, 0.40043094754219055, 0.9948272109031677, 0.00171462947037071, 0.030599256977438927, 0.9977777600288391, 0.7541762590408325, 0.1384865790605545, 0.25615423917770386, 0.4405279755592346, 0.6601619720458984, 0.4837633967399597, 0.06771793216466904, 0.09668668359518051, 0.1338348090648651, 0.00039016222581267357, 0.568595290184021, 0.019206887111067772, 0.017196960747241974, 0.9431084394454956, 0.042862046509981155, 0.5278698205947876, 0.9900554418563843, 0.024677392095327377, 0.8179934620857239, 0.9534921050071716, 0.3561384975910187, 0.920085072517395, 0.9983023405075073, 0.014007030986249447, 0.10094801336526871, 0.9904102087020874, 0.028118450194597244, 0.04222775623202324, 0.017291482537984848]], "normalized_score": [0.15152421593666077], "tokens": [["\u2581", "1", "0", "\u2581runs", "\u2581in", "\u2581", "1", "st", "\u2581in", "ning", "\u2581in", "\u2581", "2", "\u2581games", ":", "\u2581Baltimore", "\u2581Ori", "oles", "\u2581VS", "\u2581Oak", "land", "\u2581Athletics", ",", "\u2581", "1", "9", "7", "3", ",", "\u2581Baltimore", "\u2581in", "\u2581", "8", "\u2581inn", "ings", "\u2581&", "\u2581Oak", "land", "\u2581Athletics", "\u2581in", "\u2581", "9", "\u2581inn", "ings", "\u2581on", "\u2581June", "\u2581", "7", "\u2581&", "\u2581September"]], "entropy": [[1.2502813339233398, 0.745488703250885, 1.8554195165634155, 1.4295094013214111, 1.3778737783432007, 1.9440357685089111, 1.3853925466537476, 2.146486759185791, 0.7833447456359863, 0.05620589852333069, 3.0170791149139404, 2.565178632736206, 1.0784926414489746, 0.36434072256088257, 2.9994802474975586, 4.0260844230651855, 2.4185357093811035, 0.03943168371915817, 2.7164933681488037, 3.265571117401123, 0.02442208118736744, 0.9428036212921143, 2.7480483055114746, 3.0413618087768555, 1.8021308183670044, 1.5418553352355957, 1.7000343799591064, 2.153609275817871, 3.132981300354004, 3.665299415588379, 0.7251100540161133, 2.2084641456604004, 1.2841601371765137, 1.8657809495925903, 0.38095787167549133, 3.017860174179077, 2.531986713409424, 0.08190704882144928, 0.6521328687667847, 1.0479834079742432, 0.34117865562438965, 1.9197666645050049, 0.47084611654281616, 0.017171496525406837, 1.6863899230957031, 2.622203826904297, 0.07587870955467224, 1.7066781520843506, 1.3932468891143799, 3.0305097103118896]], "avg_entropy": [1.6655497194081545], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "18", "score_of_biggest_cluster": 0.15152421593666077, "score_of_first_answer": 0.15152421593666077, "max_score_of_first_answer": 0.9983023405075073, "min_score_of_first_answer": 0.00039016222581267357, "avg_score": 0.15152421593666077, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3728856869048132, "std_of_first_answer": 0.3728856869048132, "avg_entropy_of_first_answer": 1.6655497194081545, "lex_sim": 1.0}
{"question": "yo la tengo theres a riot going on release date", "answer": ["January 15, 2018"], "scores": [[0.2850303053855896, 0.9953666925430298, 0.9665848612785339, 0.2377634197473526, 0.9675452709197998, 0.998806357383728, 0.9723115563392639, 0.9999364614486694, 0.7158528566360474, 0.989672064781189]], "normalized_score": [0.7307191491127014], "tokens": [["\u2581January", "\u2581", "1", "5", ",", "\u2581", "2", "0", "1", "8"]], "entropy": [[2.461886405944824, 0.035763271152973175, 0.1895112246274948, 1.3868461847305298, 0.18778206408023834, 0.011827612295746803, 0.12752395868301392, 0.0008789714192971587, 0.6909884214401245, 0.0723918005824089]], "avg_entropy": [0.5165399914956652], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "March 16, 2018", "score_of_biggest_cluster": 0.7307191491127014, "score_of_first_answer": 0.7307191491127014, "max_score_of_first_answer": 0.9999364614486694, "min_score_of_first_answer": 0.2377634197473526, "avg_score": 0.7307191491127014, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28738113297122003, "std_of_first_answer": 0.28738113297122003, "avg_entropy_of_first_answer": 0.5165399914956652, "lex_sim": 1.0}
{"question": "how many books are in the one piece series", "answer": ["91 so far"], "scores": [[0.8644964098930359, 0.33416885137557983, 0.22404484450817108, 0.001472173142246902, 0.9965060949325562]], "normalized_score": [0.15685579180717468], "tokens": [["\u2581", "9", "1", "\u2581so", "\u2581far"]], "entropy": [[1.0843009948730469, 1.5372872352600098, 1.9617418050765991, 1.5011262893676758, 0.039741117507219315]], "avg_entropy": [1.2248394884169103], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "83", "score_of_biggest_cluster": 0.15685579180717468, "score_of_first_answer": 0.15685579180717468, "max_score_of_first_answer": 0.9965060949325562, "min_score_of_first_answer": 0.001472173142246902, "avg_score": 0.15685579180717468, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3821777283332338, "std_of_first_answer": 0.3821777283332338, "avg_entropy_of_first_answer": 1.2248394884169103, "lex_sim": 1.0}
{"question": "who was the last nba player to get drafted out of high school", "answer": ["Kevin Garnett"], "scores": [[0.12582887709140778, 0.8270595669746399, 0.9896953105926514]], "normalized_score": [0.46874815225601196], "tokens": [["\u2581Kevin", "\u2581Gar", "nett"]], "entropy": [[3.6968424320220947, 0.6294749975204468, 0.06320410221815109]], "avg_entropy": [1.4631738439202309], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Thon Maker", "score_of_biggest_cluster": 0.46874815225601196, "score_of_first_answer": 0.46874815225601196, "max_score_of_first_answer": 0.9896953105926514, "min_score_of_first_answer": 0.12582887709140778, "avg_score": 0.46874815225601196, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37482441525873234, "std_of_first_answer": 0.37482441525873234, "avg_entropy_of_first_answer": 1.4631738439202309, "lex_sim": 1.0}
{"question": "what is the third book in the lunar chronicles", "answer": ["Cress"], "scores": [[0.9023982286453247, 0.9192101359367371]], "normalized_score": [0.9107654094696045], "tokens": [["\u2581C", "ress"]], "entropy": [[0.7378751635551453, 0.3201347589492798]], "avg_entropy": [0.5290049612522125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Cress", "score_of_biggest_cluster": 0.9107654094696045, "score_of_first_answer": 0.9107654094696045, "max_score_of_first_answer": 0.9192101359367371, "min_score_of_first_answer": 0.9023982286453247, "avg_score": 0.9107654094696045, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.008405953645706177, "std_of_first_answer": 0.008405953645706177, "avg_entropy_of_first_answer": 0.5290049612522125, "lex_sim": 1.0}
{"question": "where are the winter olympics and when do they start", "answer": ["in Salt Lake City, Utah, and 26 February 2002"], "scores": [[0.019107133150100708, 0.006033973302692175, 0.9635865688323975, 0.9373044967651367, 0.45564863085746765, 0.680238664150238, 0.9990718364715576, 0.4318596124649048, 0.18123316764831543, 0.015736915171146393, 0.6586355566978455, 0.023036576807498932, 0.5656660199165344, 0.7272453904151917, 0.9827165603637695, 0.9980452060699463, 0.9786196351051331, 0.9962213039398193]], "normalized_score": [0.29192227125167847], "tokens": [["\u2581in", "\u2581Salt", "\u2581Lake", "\u2581City", ",", "\u2581Ut", "ah", ",", "\u2581and", "\u2581", "2", "6", "\u2581February", "\u2581", "2", "0", "0", "2"]], "entropy": [[2.570863962173462, 2.7235097885131836, 0.23681189119815826, 0.41173988580703735, 2.1275129318237305, 1.714913010597229, 0.010151367634534836, 2.196535110473633, 2.710331439971924, 2.4884963035583496, 1.165285348892212, 0.953095018863678, 1.4771307706832886, 1.1888093948364258, 0.09676762670278549, 0.01848018355667591, 0.1199227049946785, 0.03332502394914627]], "avg_entropy": [1.2357600980127852], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pyeongchang County, South Korea", "score_of_biggest_cluster": 0.29192227125167847, "score_of_first_answer": 0.29192227125167847, "max_score_of_first_answer": 0.9990718364715576, "min_score_of_first_answer": 0.006033973302692175, "avg_score": 0.29192227125167847, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3818730993501954, "std_of_first_answer": 0.3818730993501954, "avg_entropy_of_first_answer": 1.2357600980127852, "lex_sim": 1.0}
{"question": "prior to 1948 north and south vietnam were a part of", "answer": ["French Indochina"], "scores": [[0.5899764895439148, 0.9893283247947693, 0.9805492758750916, 0.9975827932357788]], "normalized_score": [0.869257390499115], "tokens": [["\u2581French", "\u2581Ind", "och", "ina"]], "entropy": [[1.7353209257125854, 0.09139054268598557, 0.09902486205101013, 0.021628061309456825]], "avg_entropy": [0.4868410979397595], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "France", "score_of_biggest_cluster": 0.869257390499115, "score_of_first_answer": 0.869257390499115, "max_score_of_first_answer": 0.9975827932357788, "min_score_of_first_answer": 0.5899764895439148, "avg_score": 0.869257390499115, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.17295361321986802, "std_of_first_answer": 0.17295361321986802, "avg_entropy_of_first_answer": 0.4868410979397595, "lex_sim": 1.0}
{"question": "gestational sac and yolk sac but no baby at 7 weeks", "answer": ["an ectopic pregnancy"], "scores": [[0.009169191122055054, 0.3324158489704132, 0.9977699518203735, 0.9993804693222046, 0.9995507597923279, 0.96058189868927, 0.9977055191993713, 0.9973322153091431]], "normalized_score": [0.48180297017097473], "tokens": [["\u2581an", "\u2581e", "ct", "op", "ic", "\u2581pre", "gn", "ancy"]], "entropy": [[4.492250919342041, 2.0705459117889404, 0.02401716634631157, 0.006542532239109278, 0.005856573581695557, 0.25026410818099976, 0.017343571409583092, 0.02127418853342533]], "avg_entropy": [0.8610118714277633], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "an anembryonic gestation", "score_of_biggest_cluster": 0.48180297017097473, "score_of_first_answer": 0.48180297017097473, "max_score_of_first_answer": 0.9995507597923279, "min_score_of_first_answer": 0.009169191122055054, "avg_score": 0.48180297017097473, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3648872023353366, "std_of_first_answer": 0.3648872023353366, "avg_entropy_of_first_answer": 0.8610118714277633, "lex_sim": 1.0}
{"question": "largest state in the us by land mass", "answer": ["Alaska"], "scores": [[0.9455052018165588, 0.9963933825492859]], "normalized_score": [0.9706158638000488], "tokens": [["\u2581Al", "aska"]], "entropy": [[0.42507287859916687, 0.02816656418144703]], "avg_entropy": [0.22661972139030695], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Alaska", "score_of_biggest_cluster": 0.9706158638000488, "score_of_first_answer": 0.9706158638000488, "max_score_of_first_answer": 0.9963933825492859, "min_score_of_first_answer": 0.9455052018165588, "avg_score": 0.9706158638000488, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.025444090366363525, "std_of_first_answer": 0.025444090366363525, "avg_entropy_of_first_answer": 0.22661972139030695, "lex_sim": 1.0}
{"question": "what is the corporate tax rate in great britain", "answer": ["20%"], "scores": [[0.8845747113227844, 0.39859381318092346, 0.8510384559631348, 0.8799393773078918]], "normalized_score": [0.7168309688568115], "tokens": [["\u2581", "2", "0", "%"]], "entropy": [[0.814694344997406, 0.8152836561203003, 0.6835537552833557, 0.6096274256706238]], "avg_entropy": [0.7307897955179214], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "20%", "score_of_biggest_cluster": 0.7168309688568115, "score_of_first_answer": 0.7168309688568115, "max_score_of_first_answer": 0.8845747113227844, "min_score_of_first_answer": 0.39859381318092346, "avg_score": 0.7168309688568115, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.205328786682079, "std_of_first_answer": 0.205328786682079, "avg_entropy_of_first_answer": 0.7307897955179214, "lex_sim": 1.0}
{"question": "when was last time bayern munich won champions league", "answer": ["May 24, 2013"], "scores": [[0.21800105273723602, 0.9963477253913879, 0.8257777690887451, 0.7919434309005737, 0.9830899238586426, 0.9986370205879211, 0.9975287318229675, 0.9999750852584839, 0.9979908466339111, 0.9817360639572144]], "normalized_score": [0.8193064332008362], "tokens": [["\u2581May", "\u2581", "2", "4", ",", "\u2581", "2", "0", "1", "3"]], "entropy": [[1.380679726600647, 0.0305795855820179, 0.5922455191612244, 0.9284272193908691, 0.10555484890937805, 0.011645907536149025, 0.01787126623094082, 0.0003473993274383247, 0.014663265086710453, 0.10733117908239365]], "avg_entropy": [0.31893459169077687], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "2012\u201313", "score_of_biggest_cluster": 0.8193064332008362, "score_of_first_answer": 0.8193064332008362, "max_score_of_first_answer": 0.9999750852584839, "min_score_of_first_answer": 0.21800105273723602, "avg_score": 0.8193064332008362, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23230297550871684, "std_of_first_answer": 0.23230297550871684, "avg_entropy_of_first_answer": 0.31893459169077687, "lex_sim": 1.0}
{"question": "what album is help by the beatles on", "answer": ["Help!"], "scores": [[0.8937953114509583, 0.9255095720291138]], "normalized_score": [0.9095141887664795], "tokens": [["\u2581Help", "!"]], "entropy": [[0.7498538494110107, 0.37804535031318665]], "avg_entropy": [0.5639495998620987], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Help!", "score_of_biggest_cluster": 0.9095141887664795, "score_of_first_answer": 0.9095141887664795, "max_score_of_first_answer": 0.9255095720291138, "min_score_of_first_answer": 0.8937953114509583, "avg_score": 0.9095141887664795, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.01585713028907776, "std_of_first_answer": 0.01585713028907776, "avg_entropy_of_first_answer": 0.5639495998620987, "lex_sim": 1.0}
{"question": "who is the original singer of fool if you think it's over", "answer": ["Chris Rea"], "scores": [[0.5332028269767761, 0.9301728010177612, 0.9991217255592346]], "normalized_score": [0.7913309931755066], "tokens": [["\u2581Chris", "\u2581Re", "a"]], "entropy": [[1.7813146114349365, 0.35412833094596863, 0.009756585583090782]], "avg_entropy": [0.715066509321332], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Chris Rea", "score_of_biggest_cluster": 0.7913309931755066, "score_of_first_answer": 0.7913309931755066, "max_score_of_first_answer": 0.9991217255592346, "min_score_of_first_answer": 0.5332028269767761, "avg_score": 0.7913309931755066, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2053234669008926, "std_of_first_answer": 0.2053234669008926, "avg_entropy_of_first_answer": 0.715066509321332, "lex_sim": 1.0}
{"question": "when did mcdonald's sell 1 million burgers", "answer": ["1954"], "scores": [[0.5057591199874878, 0.9481553435325623, 0.9581810832023621, 0.41092559695243835, 0.18222777545452118]], "normalized_score": [0.5097177028656006], "tokens": [["\u2581", "1", "9", "5", "4"]], "entropy": [[2.3220696449279785, 0.28801485896110535, 0.29181671142578125, 1.5805208683013916, 2.1044933795928955]], "avg_entropy": [1.3173830926418304], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "By 1965", "score_of_biggest_cluster": 0.5097177028656006, "score_of_first_answer": 0.5097177028656006, "max_score_of_first_answer": 0.9581810832023621, "min_score_of_first_answer": 0.18222777545452118, "avg_score": 0.5097177028656006, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3061582683250253, "std_of_first_answer": 0.3061582683250253, "avg_entropy_of_first_answer": 1.3173830926418304, "lex_sim": 1.0}
{"question": "where is arachidonic acid found in the body", "answer": ["Human body"], "scores": [[0.004585971590131521, 0.4433760344982147]], "normalized_score": [0.04509224370121956], "tokens": [["\u2581Human", "\u2581body"]], "entropy": [[4.789627552032471, 2.843690872192383]], "avg_entropy": [3.8166592121124268], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "brain", "score_of_biggest_cluster": 0.04509224370121956, "score_of_first_answer": 0.04509224370121956, "max_score_of_first_answer": 0.4433760344982147, "min_score_of_first_answer": 0.004585971590131521, "avg_score": 0.04509224370121956, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2193950314540416, "std_of_first_answer": 0.2193950314540416, "avg_entropy_of_first_answer": 3.8166592121124268, "lex_sim": 1.0}
{"question": "where's the tv show the crossing filmed", "answer": ["Victoria, British Columbia"], "scores": [[0.0967695564031601, 0.6651431322097778, 0.646095871925354, 0.9986851811408997]], "normalized_score": [0.4514346420764923], "tokens": [["\u2581Victoria", ",", "\u2581British", "\u2581Columbia"]], "entropy": [[4.934131622314453, 1.0232244729995728, 1.0540813207626343, 0.011862738057971]], "avg_entropy": [1.7558250385336578], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "British Columbia, Canada", "score_of_biggest_cluster": 0.4514346420764923, "score_of_first_answer": 0.4514346420764923, "max_score_of_first_answer": 0.9986851811408997, "min_score_of_first_answer": 0.0967695564031601, "avg_score": 0.4514346420764923, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32347643212538163, "std_of_first_answer": 0.32347643212538163, "avg_entropy_of_first_answer": 1.7558250385336578, "lex_sim": 1.0}
{"question": "who did the 49ers beat in the super bowls", "answer": ["Miami Dolphins, Cincinnati Bengals, Denver Broncos, San Diego Chargers"], "scores": [[0.02058909274637699, 0.874966025352478, 0.9984679818153381, 0.998874843120575, 0.18581469357013702, 0.3011195957660675, 0.9998892545700073, 0.9967356324195862, 0.9959346055984497, 0.9968811273574829, 0.9985692501068115, 0.6335653066635132, 0.10055794566869736, 0.9994226694107056, 0.9980213642120361, 0.9980162382125854, 0.4567644000053406, 0.7281261682510376, 0.8227994441986084, 0.9801371693611145, 0.9993135929107666, 0.9973547458648682]], "normalized_score": [0.6067581176757812], "tokens": [["\u2581Miami", "\u2581Dol", "ph", "ins", ",", "\u2581C", "inc", "inn", "ati", "\u2581Beng", "als", ",", "\u2581Den", "ver", "\u2581Bron", "cos", ",", "\u2581San", "\u2581Diego", "\u2581Ch", "arg", "ers"]], "entropy": [[2.6697263717651367, 0.6368100047111511, 0.01508675329387188, 0.010871924459934235, 1.328057050704956, 1.950939655303955, 0.001325800083577633, 0.023950301110744476, 0.029497656971216202, 0.029807884246110916, 0.013046352192759514, 0.9587856531143188, 1.3464254140853882, 0.007118681911379099, 0.020236674696207047, 0.016935452818870544, 1.1123931407928467, 1.2380023002624512, 0.544985830783844, 0.14865811169147491, 0.008372481912374496, 0.019829770550131798]], "avg_entropy": [0.5514028757937591], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "San Diego Chargers", "score_of_biggest_cluster": 0.6067581176757812, "score_of_first_answer": 0.6067581176757812, "max_score_of_first_answer": 0.9998892545700073, "min_score_of_first_answer": 0.02058909274637699, "avg_score": 0.6067581176757812, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.328282199007378, "std_of_first_answer": 0.328282199007378, "avg_entropy_of_first_answer": 0.5514028757937591, "lex_sim": 1.0}
{"question": "when did the ship hector arrived in pictou", "answer": ["September 15, 1773"], "scores": [[0.33863532543182373, 0.9817602038383484, 0.9636596441268921, 0.8949470520019531, 0.9430339932441711, 0.9927502870559692, 0.9990666508674622, 0.9941555261611938, 0.9887927174568176, 0.9918395280838013]], "normalized_score": [0.8744644522666931], "tokens": [["\u2581September", "\u2581", "1", "5", ",", "\u2581", "1", "7", "7", "3"]], "entropy": [[2.093214511871338, 0.11867447197437286, 0.21532197296619415, 0.5408737659454346, 0.31551292538642883, 0.05314569175243378, 0.007994377054274082, 0.04187220707535744, 0.07987749576568604, 0.05629640072584152]], "avg_entropy": [0.3522783820517361], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1773", "score_of_biggest_cluster": 0.8744644522666931, "score_of_first_answer": 0.8744644522666931, "max_score_of_first_answer": 0.9990666508674622, "min_score_of_first_answer": 0.33863532543182373, "avg_score": 0.8744644522666931, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19249936111022287, "std_of_first_answer": 0.19249936111022287, "avg_entropy_of_first_answer": 0.3522783820517361, "lex_sim": 1.0}
{"question": "when did the ottoman empire surrender in ww1", "answer": ["October 30, 1918 at 9:05 am (an exact moment is not known)"], "scores": [[0.771969199180603, 0.9953045845031738, 0.9102850556373596, 0.9813843965530396, 0.9790173172950745, 0.9969313144683838, 0.9991117119789124, 0.9999034404754639, 0.9996447563171387, 0.9985060691833496, 0.0003145352238789201, 0.695472776889801, 0.49285367131233215, 0.7581024169921875, 0.4642988443374634, 0.4849529266357422, 0.13945560157299042, 0.015803564339876175, 0.0042496053501963615, 0.0316067636013031, 0.047669943422079086, 0.0728602409362793, 0.08401094377040863, 0.22297154366970062, 0.6841392517089844]], "normalized_score": [0.23354953527450562], "tokens": [["\u2581October", "\u2581", "3", "0", ",", "\u2581", "1", "9", "1", "8", "\u2581at", "\u2581", "9", ":", "0", "5", "\u2581am", "\u2581(", "an", "\u2581exact", "\u2581moment", "\u2581is", "\u2581not", "\u2581known", ")"]], "entropy": [[1.1032867431640625, 0.036940909922122955, 0.3927062451839447, 0.0996202826499939, 0.13372203707695007, 0.023269932717084885, 0.007881184108555317, 0.001216780743561685, 0.003717550542205572, 0.013904260471463203, 0.1856066882610321, 1.385809302330017, 1.6685864925384521, 1.0475656986236572, 1.2290754318237305, 0.9050580859184265, 1.9455515146255493, 0.884514331817627, 5.278388977050781, 2.5004310607910156, 1.859787940979004, 3.0678367614746094, 4.575070381164551, 3.725937843322754, 1.5693359375]], "avg_entropy": [1.3457928949920461], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "1918", "score_of_biggest_cluster": 0.23354953527450562, "score_of_first_answer": 0.23354953527450562, "max_score_of_first_answer": 0.9999034404754639, "min_score_of_first_answer": 0.0003145352238789201, "avg_score": 0.23354953527450562, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3989233391887226, "std_of_first_answer": 0.3989233391887226, "avg_entropy_of_first_answer": 1.3457928949920461, "lex_sim": 1.0}
{"question": "who was the first baseball player to be featured on a postage stamp", "answer": ["Babe Ruth"], "scores": [[0.1476610004901886, 0.9970653653144836, 0.9951356053352356]], "normalized_score": [0.5271779298782349], "tokens": [["\u2581B", "abe", "\u2581Ruth"]], "entropy": [[1.939354419708252, 0.027775181457400322, 0.04551120847463608]], "avg_entropy": [0.6708802698800961], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jackie Robinson", "score_of_biggest_cluster": 0.5271779298782349, "score_of_first_answer": 0.5271779298782349, "max_score_of_first_answer": 0.9970653653144836, "min_score_of_first_answer": 0.1476610004901886, "avg_score": 0.5271779298782349, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3999589846768194, "std_of_first_answer": 0.3999589846768194, "avg_entropy_of_first_answer": 0.6708802698800961, "lex_sim": 1.0}
{"question": "who was the first imperial power that was fighting in vietnam", "answer": ["France"], "scores": [[0.7112707495689392]], "normalized_score": [0.7112707495689392], "tokens": [["\u2581France"]], "entropy": [[1.6142245531082153]], "avg_entropy": [1.6142245531082153], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "France", "score_of_biggest_cluster": 0.7112707495689392, "score_of_first_answer": 0.7112707495689392, "max_score_of_first_answer": 0.7112707495689392, "min_score_of_first_answer": 0.7112707495689392, "avg_score": 0.7112707495689392, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.6142245531082153, "lex_sim": 1.0}
{"question": "abbreviated name of the highest peak in tasmania", "answer": ["Mt."], "scores": [[0.6154283881187439, 0.9781709313392639, 0.45453760027885437, 0.8343064785003662, 0.9305919408798218, 0.9938405752182007]], "normalized_score": [0.7716622948646545], "tokens": [["\u2581M", "t", ".", "\u2581O", "ss", "a"]], "entropy": [[1.7768044471740723, 0.14871689677238464, 1.5780646800994873, 1.053263545036316, 0.4935848116874695, 0.06338907778263092]], "avg_entropy": [0.8523039097587267], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mount Ossa", "score_of_biggest_cluster": 0.7716622948646545, "score_of_first_answer": 0.7716622948646545, "max_score_of_first_answer": 0.9938405752182007, "min_score_of_first_answer": 0.45453760027885437, "avg_score": 0.7716622948646545, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2003968182959993, "std_of_first_answer": 0.2003968182959993, "avg_entropy_of_first_answer": 0.8523039097587267, "lex_sim": 1.0}
{"question": "what is a coherent set of values and beliefs about public policy called", "answer": ["public policy"], "scores": [[0.08473528176546097, 0.8047434687614441]], "normalized_score": [0.26113247871398926], "tokens": [["\u2581public", "\u2581policy"]], "entropy": [[2.23721981048584, 0.7983385920524597]], "avg_entropy": [1.5177792012691498], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "a political ideology", "score_of_biggest_cluster": 0.26113247871398926, "score_of_first_answer": 0.26113247871398926, "max_score_of_first_answer": 0.8047434687614441, "min_score_of_first_answer": 0.08473528176546097, "avg_score": 0.26113247871398926, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36000409349799156, "std_of_first_answer": 0.36000409349799156, "avg_entropy_of_first_answer": 1.5177792012691498, "lex_sim": 1.0}
{"question": "why does kerosene oil rise up in the wick of lantern", "answer": ["it has a high boiling point of 200-300 degrees C"], "scores": [[0.02582787349820137, 0.2095586210489273, 0.7658433318138123, 0.1545480191707611, 0.27666884660720825, 0.9982736110687256, 0.9500455856323242, 0.022202175110578537, 0.7283275127410889, 0.7275550365447998, 0.4313572943210602, 0.39494800567626953, 0.17183522880077362, 0.19017399847507477, 0.8772103190422058, 0.9964189529418945, 0.5142816305160522, 0.12804628908634186]], "normalized_score": [0.30831465125083923], "tokens": [["\u2581it", "\u2581has", "\u2581a", "\u2581high", "\u2581bo", "iling", "\u2581point", "\u2581of", "\u2581", "2", "0", "0", "-", "3", "0", "0", "\u2581degrees", "\u2581C"]], "entropy": [[3.466203451156616, 2.4599967002868652, 1.404184103012085, 2.344242572784424, 2.6551170349121094, 0.01705259084701538, 0.27473700046539307, 1.842605471611023, 1.3044285774230957, 0.8878783583641052, 1.8476691246032715, 1.5926140546798706, 2.489025354385376, 0.7697727084159851, 0.6276698112487793, 0.03394559398293495, 2.1252665519714355, 2.094864845275879]], "avg_entropy": [1.568737439190348], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "capillary action", "score_of_biggest_cluster": 0.30831465125083923, "score_of_first_answer": 0.30831465125083923, "max_score_of_first_answer": 0.9982736110687256, "min_score_of_first_answer": 0.022202175110578537, "avg_score": 0.30831465125083923, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33896956376612836, "std_of_first_answer": 0.33896956376612836, "avg_entropy_of_first_answer": 1.568737439190348, "lex_sim": 1.0}
{"question": "how many plane surfaces does a cylinder have", "answer": ["3"], "scores": [[0.6202870011329651, 0.23417796194553375]], "normalized_score": [0.38112667202949524], "tokens": [["\u2581", "3"]], "entropy": [[1.9424734115600586, 1.412255048751831]], "avg_entropy": [1.6773642301559448], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "two parallel planes", "score_of_biggest_cluster": 0.38112667202949524, "score_of_first_answer": 0.38112667202949524, "max_score_of_first_answer": 0.6202870011329651, "min_score_of_first_answer": 0.23417796194553375, "avg_score": 0.38112667202949524, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19305451959371567, "std_of_first_answer": 0.19305451959371567, "avg_entropy_of_first_answer": 1.6773642301559448, "lex_sim": 1.0}
{"question": "who commissioned the first christmas card in 1943", "answer": ["Sir Henry Cole"], "scores": [[0.7776730060577393, 0.9918981790542603, 0.9984166622161865]], "normalized_score": [0.9166256189346313], "tokens": [["\u2581Sir", "\u2581Henry", "\u2581Cole"]], "entropy": [[1.3917796611785889, 0.07658582180738449, 0.018349654972553253]], "avg_entropy": [0.4955717126528422], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Sir Henry Cole", "score_of_biggest_cluster": 0.9166256189346313, "score_of_first_answer": 0.9166256189346313, "max_score_of_first_answer": 0.9984166622161865, "min_score_of_first_answer": 0.7776730060577393, "avg_score": 0.9166256189346313, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.10255766784395579, "std_of_first_answer": 0.10255766784395579, "avg_entropy_of_first_answer": 0.4955717126528422, "lex_sim": 1.0}
{"question": "is a network connection device that can build tables that identify addresses on each network", "answer": ["router"], "scores": [[0.687186598777771]], "normalized_score": [0.687186598777771], "tokens": [["\u2581router"]], "entropy": [[1.7400023937225342]], "avg_entropy": [1.7400023937225342], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "routing table", "score_of_biggest_cluster": 0.687186598777771, "score_of_first_answer": 0.687186598777771, "max_score_of_first_answer": 0.687186598777771, "min_score_of_first_answer": 0.687186598777771, "avg_score": 0.687186598777771, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.7400023937225342, "lex_sim": 1.0}
{"question": "how many seasons of the rugrats are there", "answer": ["9 seasons"], "scores": [[0.8952335715293884, 0.8002042770385742, 0.20794783532619476]], "normalized_score": [0.5301073789596558], "tokens": [["\u2581", "9", "\u2581seasons"]], "entropy": [[0.744018018245697, 0.8068581223487854, 1.147496223449707]], "avg_entropy": [0.8994574546813965], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "9 seasons", "score_of_biggest_cluster": 0.5301073789596558, "score_of_first_answer": 0.5301073789596558, "max_score_of_first_answer": 0.8952335715293884, "min_score_of_first_answer": 0.20794783532619476, "avg_score": 0.5301073789596558, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.30407600342852825, "std_of_first_answer": 0.30407600342852825, "avg_entropy_of_first_answer": 0.8994574546813965, "lex_sim": 1.0}
{"question": "what is the current population of bora bora", "answer": ["unknown, but small"], "scores": [[0.002240374218672514, 0.013443192467093468, 0.37505343556404114, 0.007950082421302795]], "normalized_score": [0.017310991883277893], "tokens": [["\u2581unknown", ",", "\u2581but", "\u2581small"]], "entropy": [[0.9064795970916748, 0.5645588636398315, 3.772881269454956, 4.263637065887451]], "avg_entropy": [2.3768891990184784], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "10,605", "score_of_biggest_cluster": 0.017310991883277893, "score_of_first_answer": 0.017310991883277893, "max_score_of_first_answer": 0.37505343556404114, "min_score_of_first_answer": 0.002240374218672514, "avg_score": 0.017310991883277893, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1590410121386585, "std_of_first_answer": 0.1590410121386585, "avg_entropy_of_first_answer": 2.3768891990184784, "lex_sim": 1.0}
{"question": "who is singing in something just like this", "answer": ["Chainsmokers"], "scores": [[0.27234962582588196, 0.9180390238761902, 0.9992988109588623, 0.9822602868080139, 0.9998596906661987]], "normalized_score": [0.7550396919250488], "tokens": [["\u2581Ch", "ains", "m", "ok", "ers"]], "entropy": [[3.0237936973571777, 0.42628347873687744, 0.008121790364384651, 0.09879597276449203, 0.0019566304981708527]], "avg_entropy": [0.7117903139442205], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Will Champion", "score_of_biggest_cluster": 0.7550396919250488, "score_of_first_answer": 0.7550396919250488, "max_score_of_first_answer": 0.9998596906661987, "min_score_of_first_answer": 0.27234962582588196, "avg_score": 0.7550396919250488, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2826047747346681, "std_of_first_answer": 0.2826047747346681, "avg_entropy_of_first_answer": 0.7117903139442205, "lex_sim": 1.0}
{"question": "when was the last time lebron scored 50 points", "answer": ["October 30, 2017"], "scores": [[0.026400912553071976, 0.9951913356781006, 0.17288337647914886, 0.745334804058075, 0.9599666595458984, 0.9973739385604858, 0.9988981485366821, 0.999870777130127, 0.8308368921279907, 0.14904716610908508]], "normalized_score": [0.45740920305252075], "tokens": [["\u2581October", "\u2581", "3", "0", ",", "\u2581", "2", "0", "1", "7"]], "entropy": [[2.395271062850952, 0.038147665560245514, 0.9277070760726929, 0.6592714786529541, 0.229375422000885, 0.02206617034971714, 0.009342323057353497, 0.0016776605043560266, 0.4578229784965515, 1.5826148986816406]], "avg_entropy": [0.6323296736227348], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "March 5, 2008", "score_of_biggest_cluster": 0.45740920305252075, "score_of_first_answer": 0.45740920305252075, "max_score_of_first_answer": 0.999870777130127, "min_score_of_first_answer": 0.026400912553071976, "avg_score": 0.45740920305252075, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38408100672198675, "std_of_first_answer": 0.38408100672198675, "avg_entropy_of_first_answer": 0.6323296736227348, "lex_sim": 1.0}
{"question": "where does half life 2 episode 2 take place", "answer": ["City Thirteen"], "scores": [[0.5999194383621216, 0.0007007390377111733, 0.8997437357902527, 0.9996806383132935]], "normalized_score": [0.1394464075565338], "tokens": [["\u2581City", "\u2581Th", "ir", "teen"]], "entropy": [[2.3934836387634277, 0.16466999053955078, 0.6481530070304871, 0.003360054222866893]], "avg_entropy": [0.8024166726390831], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "outside City 17", "score_of_biggest_cluster": 0.1394464075565338, "score_of_first_answer": 0.1394464075565338, "max_score_of_first_answer": 0.9996806383132935, "min_score_of_first_answer": 0.0007007390377111733, "avg_score": 0.1394464075565338, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38930974249200456, "std_of_first_answer": 0.38930974249200456, "avg_entropy_of_first_answer": 0.8024166726390831, "lex_sim": 1.0}
{"question": "who played alex cross in along came a spider", "answer": ["Morgan Freeman"], "scores": [[0.9475372433662415, 0.9968841671943665, 0.9984297156333923]], "normalized_score": [0.9806623458862305], "tokens": [["\u2581Morgan", "\u2581Fre", "eman"]], "entropy": [[0.3801472783088684, 0.02650354988873005, 0.015430977568030357]], "avg_entropy": [0.1406939352552096], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Morgan Freeman", "score_of_biggest_cluster": 0.9806623458862305, "score_of_first_answer": 0.9806623458862305, "max_score_of_first_answer": 0.9984297156333923, "min_score_of_first_answer": 0.9475372433662415, "avg_score": 0.9806623458862305, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.02363507596945896, "std_of_first_answer": 0.02363507596945896, "avg_entropy_of_first_answer": 0.1406939352552096, "lex_sim": 1.0}
{"question": "who proclaimed 5th october as world\u2019s teachers day", "answer": ["UNESCO"], "scores": [[0.517630934715271, 0.9863627552986145, 0.996976375579834]], "normalized_score": [0.7984491586685181], "tokens": [["\u2581UN", "ES", "CO"]], "entropy": [[2.3445887565612793, 0.1057448759675026, 0.02363503724336624]], "avg_entropy": [0.8246562232573827], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "UNESCO/ILO", "score_of_biggest_cluster": 0.7984491586685181, "score_of_first_answer": 0.7984491586685181, "max_score_of_first_answer": 0.996976375579834, "min_score_of_first_answer": 0.517630934715271, "avg_score": 0.7984491586685181, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22350595822047933, "std_of_first_answer": 0.22350595822047933, "avg_entropy_of_first_answer": 0.8246562232573827, "lex_sim": 1.0}
{"question": "who plays the dragon queen from game of thrones", "answer": ["Emilia Clarke"], "scores": [[0.07231878489255905, 0.9755767583847046, 0.9855989217758179, 0.9997153878211975]], "normalized_score": [0.5134785175323486], "tokens": [["\u2581Em", "ilia", "\u2581Clar", "ke"]], "entropy": [[2.6702194213867188, 0.13022381067276, 0.1073780357837677, 0.0033884451258927584]], "avg_entropy": [0.7278024282422848], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Emilia Clarke", "score_of_biggest_cluster": 0.5134785175323486, "score_of_first_answer": 0.5134785175323486, "max_score_of_first_answer": 0.9997153878211975, "min_score_of_first_answer": 0.07231878489255905, "avg_score": 0.5134785175323486, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3961456823276904, "std_of_first_answer": 0.3961456823276904, "avg_entropy_of_first_answer": 0.7278024282422848, "lex_sim": 1.0}
{"question": "who has the most yards per carry in nfl history", "answer": ["O.J."], "scores": [[0.15897858142852783, 0.7796591520309448, 0.5345683693885803, 0.9773167967796326, 0.9842345118522644, 0.9987165927886963]], "normalized_score": [0.6318836808204651], "tokens": [["\u2581O", ".", "J", ".", "\u2581Sim", "pson"]], "entropy": [[3.7948360443115234, 0.7862125039100647, 0.7015544772148132, 0.1217387467622757, 0.11706861108541489, 0.012401280924677849]], "avg_entropy": [0.9223019440347949], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Emmitt Smith", "score_of_biggest_cluster": 0.6318836808204651, "score_of_first_answer": 0.6318836808204651, "max_score_of_first_answer": 0.9987165927886963, "min_score_of_first_answer": 0.15897858142852783, "avg_score": 0.6318836808204651, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30666426488060106, "std_of_first_answer": 0.30666426488060106, "avg_entropy_of_first_answer": 0.9223019440347949, "lex_sim": 1.0}
{"question": "where is the tibia and fibula bone located", "answer": ["lower leg bone"], "scores": [[0.25027284026145935, 0.6745620965957642, 0.013899710029363632, 0.5994033813476562]], "normalized_score": [0.19366008043289185], "tokens": [["\u2581lower", "\u2581leg", "\u2581b", "one"]], "entropy": [[3.4227371215820312, 1.0471513271331787, 0.4772682785987854, 0.6870842576026917]], "avg_entropy": [1.4085602462291718], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "leg", "score_of_biggest_cluster": 0.19366008043289185, "score_of_first_answer": 0.19366008043289185, "max_score_of_first_answer": 0.6745620965957642, "min_score_of_first_answer": 0.013899710029363632, "avg_score": 0.19366008043289185, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2672456659007933, "std_of_first_answer": 0.2672456659007933, "avg_entropy_of_first_answer": 1.4085602462291718, "lex_sim": 1.0}
{"question": "what is the snow on mars made of", "answer": ["Carbon dioxide"], "scores": [[0.06910276412963867, 0.9948833584785461, 0.8521068692207336, 0.9992827773094177, 0.9991014003753662]], "normalized_score": [0.5667766332626343], "tokens": [["\u2581Car", "bon", "\u2581dio", "x", "ide"]], "entropy": [[2.948012113571167, 0.034994009882211685, 0.5457133650779724, 0.007621478755027056, 0.01038885023444891]], "avg_entropy": [0.7093459635041655], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "water ice", "score_of_biggest_cluster": 0.5667766332626343, "score_of_first_answer": 0.5667766332626343, "max_score_of_first_answer": 0.9992827773094177, "min_score_of_first_answer": 0.06910276412963867, "avg_score": 0.5667766332626343, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3613302037448594, "std_of_first_answer": 0.3613302037448594, "avg_entropy_of_first_answer": 0.7093459635041655, "lex_sim": 1.0}
{"question": "where does the sweetness of fruit come from", "answer": ["Sugar"], "scores": [[0.06191281974315643, 0.616677463054657]], "normalized_score": [0.19539764523506165], "tokens": [["\u2581Sug", "ar"]], "entropy": [[3.405583381652832, 0.694485068321228]], "avg_entropy": [2.05003422498703], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Fructose, or fruit sugar", "score_of_biggest_cluster": 0.19539764523506165, "score_of_first_answer": 0.19539764523506165, "max_score_of_first_answer": 0.616677463054657, "min_score_of_first_answer": 0.06191281974315643, "avg_score": 0.19539764523506165, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2773823216557503, "std_of_first_answer": 0.2773823216557503, "avg_entropy_of_first_answer": 2.05003422498703, "lex_sim": 1.0}
{"question": "when was the movie the wizard of oz made", "answer": ["1939"], "scores": [[0.2466498464345932, 0.9519081711769104, 0.9822920560836792, 0.953066349029541, 0.966397225856781]], "normalized_score": [0.7335658073425293], "tokens": [["\u2581", "1", "9", "3", "9"]], "entropy": [[1.1773464679718018, 0.271042138338089, 0.13145577907562256, 0.2341872900724411, 0.18991079926490784]], "avg_entropy": [0.40078849494457247], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "1939", "score_of_biggest_cluster": 0.7335658073425293, "score_of_first_answer": 0.7335658073425293, "max_score_of_first_answer": 0.9822920560836792, "min_score_of_first_answer": 0.2466498464345932, "avg_score": 0.7335658073425293, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2869172901698701, "std_of_first_answer": 0.2869172901698701, "avg_entropy_of_first_answer": 0.40078849494457247, "lex_sim": 1.0}
{"question": "who carried the usa flag in opening ceremony", "answer": ["Chad Hedrick"], "scores": [[0.010224423371255398, 0.10517727583646774, 0.9419898390769958, 0.9979974627494812]], "normalized_score": [0.1783134490251541], "tokens": [["\u2581Ch", "ad", "\u2581Hed", "rick"]], "entropy": [[4.267738342285156, 2.6297316551208496, 0.4314805865287781, 0.01995597779750824]], "avg_entropy": [1.837226640433073], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Erin Hamlin", "score_of_biggest_cluster": 0.1783134490251541, "score_of_first_answer": 0.1783134490251541, "max_score_of_first_answer": 0.9979974627494812, "min_score_of_first_answer": 0.010224423371255398, "avg_score": 0.1783134490251541, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45780853103979774, "std_of_first_answer": 0.45780853103979774, "avg_entropy_of_first_answer": 1.837226640433073, "lex_sim": 1.0}
{"question": "who wrote antigone and what are the dates of his birth and death", "answer": ["Sophocles and?????????????"], "scores": [[0.8614640831947327, 0.9980378746986389, 0.9990704655647278, 0.07109756022691727, 0.00013094497262500226, 0.3930090069770813, 0.5296327471733093, 0.7041636109352112, 0.7540660500526428, 0.061252497136592865, 0.013071422465145588, 0.32550516724586487, 0.0343853160738945]], "normalized_score": [0.1399776041507721], "tokens": [["\u2581Soph", "oc", "les", "\u2581and", "??", "??", "??", "??", "??", "???", "\u2581and", "??", "???"]], "entropy": [[1.0040414333343506, 0.019265133887529373, 0.009224348701536655, 2.4358744621276855, 2.7150251865386963, 2.845802068710327, 1.837630271911621, 1.3552460670471191, 1.1723787784576416, 0.9231834411621094, 1.7382549047470093, 2.724087715148926, 0.20808009803295135]], "avg_entropy": [1.460622608446731], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "winter 406/5 BC", "score_of_biggest_cluster": 0.1399776041507721, "score_of_first_answer": 0.1399776041507721, "max_score_of_first_answer": 0.9990704655647278, "min_score_of_first_answer": 0.00013094497262500226, "avg_score": 0.1399776041507721, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37345953148318695, "std_of_first_answer": 0.37345953148318695, "avg_entropy_of_first_answer": 1.460622608446731, "lex_sim": 1.0}
{"question": "who sings the wizards of waverly place theme song", "answer": ["Steve Rushton"], "scores": [[0.11533786356449127, 0.9779125452041626, 0.9938801527023315, 0.9995100498199463]], "normalized_score": [0.5785598158836365], "tokens": [["\u2581Steve", "\u2581Rus", "ht", "on"]], "entropy": [[2.7828102111816406, 0.18390300869941711, 0.05364874005317688, 0.00588987534865737]], "avg_entropy": [0.756562958820723], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Selena Gomez", "score_of_biggest_cluster": 0.5785598158836365, "score_of_first_answer": 0.5785598158836365, "max_score_of_first_answer": 0.9995100498199463, "min_score_of_first_answer": 0.11533786356449127, "avg_score": 0.5785598158836365, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37901065309842275, "std_of_first_answer": 0.37901065309842275, "avg_entropy_of_first_answer": 0.756562958820723, "lex_sim": 1.0}
{"question": "who starred in the movie the day the earth stood still", "answer": ["Michael Rennie"], "scores": [[0.762400209903717, 0.989970862865448, 0.9990019202232361, 0.9995033740997314]], "normalized_score": [0.9317277073860168], "tokens": [["\u2581Michael", "\u2581R", "enn", "ie"]], "entropy": [[1.2224615812301636, 0.09864616394042969, 0.008654547855257988, 0.005345188081264496]], "avg_entropy": [0.33377687027677894], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jennifer Connelly", "score_of_biggest_cluster": 0.9317277073860168, "score_of_first_answer": 0.9317277073860168, "max_score_of_first_answer": 0.9995033740997314, "min_score_of_first_answer": 0.762400209903717, "avg_score": 0.9317277073860168, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10129146129818088, "std_of_first_answer": 0.10129146129818088, "avg_entropy_of_first_answer": 0.33377687027677894, "lex_sim": 1.0}
{"question": "when's the last time army won the army navy game", "answer": ["December 8, 2018"], "scores": [[0.5175081491470337, 0.9944164752960205, 0.3236396014690399, 0.9740031361579895, 0.9982307553291321, 0.9481006860733032, 0.999903678894043, 0.9831125140190125, 0.8226348161697388]], "normalized_score": [0.793101966381073], "tokens": [["\u2581December", "\u2581", "8", ",", "\u2581", "2", "0", "1", "8"]], "entropy": [[1.6079000234603882, 0.04330376535654068, 1.4158090353012085, 0.16124367713928223, 0.01577729545533657, 0.20462077856063843, 0.0012218718184158206, 0.08741915225982666, 0.5343252420425415]], "avg_entropy": [0.4524023157104643], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2017", "score_of_biggest_cluster": 0.793101966381073, "score_of_first_answer": 0.793101966381073, "max_score_of_first_answer": 0.999903678894043, "min_score_of_first_answer": 0.3236396014690399, "avg_score": 0.793101966381073, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23464032393915857, "std_of_first_answer": 0.23464032393915857, "avg_entropy_of_first_answer": 0.4524023157104643, "lex_sim": 1.0}
{"question": "who is the coach for the ottawa senators", "answer": ["Guy Boucher"], "scores": [[0.6971561312675476, 0.9847032427787781, 0.9999234676361084, 0.9942508935928345]], "normalized_score": [0.9089176654815674], "tokens": [["\u2581Guy", "\u2581B", "ouch", "er"]], "entropy": [[1.3627820014953613, 0.15342292189598083, 0.0011036503128707409, 0.05073847994208336]], "avg_entropy": [0.39201176341157407], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Guy Boucher", "score_of_biggest_cluster": 0.9089176654815674, "score_of_first_answer": 0.9089176654815674, "max_score_of_first_answer": 0.9999234676361084, "min_score_of_first_answer": 0.6971561312675476, "avg_score": 0.9089176654815674, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.12820191352129245, "std_of_first_answer": 0.12820191352129245, "avg_entropy_of_first_answer": 0.39201176341157407, "lex_sim": 1.0}
{"question": "who played lionel in as time goes by", "answer": ["Geoffrey Whitehead"], "scores": [[0.8997318148612976, 0.9978558421134949, 0.05297018587589264, 0.9936110377311707]], "normalized_score": [0.46623751521110535], "tokens": [["\u2581Geoff", "rey", "\u2581White", "head"]], "entropy": [[0.7032445073127747, 0.02306591346859932, 2.243891477584839, 0.05379442125558853]], "avg_entropy": [0.7559990799054503], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Geoffrey Dyson Palmer, OBE", "score_of_biggest_cluster": 0.46623751521110535, "score_of_first_answer": 0.46623751521110535, "max_score_of_first_answer": 0.9978558421134949, "min_score_of_first_answer": 0.05297018587589264, "avg_score": 0.46623751521110535, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3963173462832287, "std_of_first_answer": 0.3963173462832287, "avg_entropy_of_first_answer": 0.7559990799054503, "lex_sim": 1.0}
{"question": "where did the british army launch a successful invasion in 1776", "answer": ["New York Harbor"], "scores": [[0.5482662916183472, 0.984455943107605, 0.04039813205599785, 0.970937192440033]], "normalized_score": [0.3814477324485779], "tokens": [["\u2581New", "\u2581York", "\u2581Har", "bor"]], "entropy": [[2.5033977031707764, 0.10825826972723007, 1.3915205001831055, 0.13678786158561707]], "avg_entropy": [1.0349910836666822], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "New York", "score_of_biggest_cluster": 0.3814477324485779, "score_of_first_answer": 0.3814477324485779, "max_score_of_first_answer": 0.984455943107605, "min_score_of_first_answer": 0.04039813205599785, "avg_score": 0.3814477324485779, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3860191883258971, "std_of_first_answer": 0.3860191883258971, "avg_entropy_of_first_answer": 1.0349910836666822, "lex_sim": 1.0}
{"question": "who carried the us flag in the 2014 olympics", "answer": ["John Shuster"], "scores": [[0.01247777696698904, 0.020437825471162796, 0.9910175800323486]], "normalized_score": [0.06322436779737473], "tokens": [["\u2581John", "\u2581Sh", "uster"]], "entropy": [[4.338288307189941, 3.4104208946228027, 0.06847850978374481]], "avg_entropy": [2.6057292371988297], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Todd Lodwick", "score_of_biggest_cluster": 0.06322436779737473, "score_of_first_answer": 0.06322436779737473, "max_score_of_first_answer": 0.9910175800323486, "min_score_of_first_answer": 0.01247777696698904, "avg_score": 0.06322436779737473, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45942337875493855, "std_of_first_answer": 0.45942337875493855, "avg_entropy_of_first_answer": 2.6057292371988297, "lex_sim": 1.0}
{"question": "who is the quarterback for the green bay packers", "answer": ["Aaron Rodgers"], "scores": [[0.958173394203186, 0.9997352957725525, 0.9285921454429626, 0.9989135265350342]], "normalized_score": [0.9708910584449768], "tokens": [["\u2581A", "aron", "\u2581Rod", "gers"]], "entropy": [[0.32782214879989624, 0.0035715322010219097, 0.5134612917900085, 0.009939398616552353]], "avg_entropy": [0.21369859285186976], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Aaron Rodgers", "score_of_biggest_cluster": 0.9708910584449768, "score_of_first_answer": 0.9708910584449768, "max_score_of_first_answer": 0.9997352957725525, "min_score_of_first_answer": 0.9285921454429626, "avg_score": 0.9708910584449768, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.02986356486683138, "std_of_first_answer": 0.02986356486683138, "avg_entropy_of_first_answer": 0.21369859285186976, "lex_sim": 1.0}
{"question": "when does madea's family funeral come out", "answer": ["February 8, 2019"], "scores": [[0.19169881939888, 0.9900723695755005, 0.04417765885591507, 0.8472757935523987, 0.9984293580055237, 0.9985974431037903, 0.9999109506607056, 0.9576590657234192, 0.29259490966796875]], "normalized_score": [0.500887393951416], "tokens": [["\u2581February", "\u2581", "8", ",", "\u2581", "2", "0", "1", "9"]], "entropy": [[2.8464646339416504, 0.07097098231315613, 1.0171297788619995, 0.5369551181793213, 0.014932061545550823, 0.01116976048797369, 0.0011855494230985641, 0.20605817437171936, 1.5005626678466797]], "avg_entropy": [0.6894920807745721], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2018", "score_of_biggest_cluster": 0.500887393951416, "score_of_first_answer": 0.500887393951416, "max_score_of_first_answer": 0.9999109506607056, "min_score_of_first_answer": 0.04417765885591507, "avg_score": 0.500887393951416, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37929657890478047, "std_of_first_answer": 0.37929657890478047, "avg_entropy_of_first_answer": 0.6894920807745721, "lex_sim": 1.0}
{"question": "when did canada sign the un declaration of indigenous rights", "answer": ["September 13, 2018"], "scores": [[0.182149738073349, 0.9788203835487366, 0.5868759751319885, 0.8973402976989746, 0.8908448219299316, 0.9929879307746887, 0.9991255402565002, 0.9993626475334167, 0.9973599314689636, 0.7924640774726868]], "normalized_score": [0.7614748477935791], "tokens": [["\u2581September", "\u2581", "1", "3", ",", "\u2581", "2", "0", "1", "8"]], "entropy": [[3.012847423553467, 0.13176751136779785, 0.8850338459014893, 0.5564441680908203, 0.5119351148605347, 0.051808468997478485, 0.008126510307192802, 0.007112997118383646, 0.02011089026927948, 0.769737720489502]], "avg_entropy": [0.5954924650955945], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "May 2016", "score_of_biggest_cluster": 0.7614748477935791, "score_of_first_answer": 0.7614748477935791, "max_score_of_first_answer": 0.9993626475334167, "min_score_of_first_answer": 0.182149738073349, "avg_score": 0.7614748477935791, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24948828487002844, "std_of_first_answer": 0.24948828487002844, "avg_entropy_of_first_answer": 0.5954924650955945, "lex_sim": 1.0}
{"question": "who won the 2017 sports personality of the year", "answer": ["Sir Mo Farah"], "scores": [[0.16373421251773834, 0.9828835129737854, 0.9895957112312317, 0.9830483198165894]], "normalized_score": [0.6290258765220642], "tokens": [["\u2581Sir", "\u2581Mo", "\u2581Far", "ah"]], "entropy": [[1.2521650791168213, 0.14716915786266327, 0.09474178403615952, 0.0940425843000412]], "avg_entropy": [0.3970296513289213], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Mo Farah", "score_of_biggest_cluster": 0.6290258765220642, "score_of_first_answer": 0.6290258765220642, "max_score_of_first_answer": 0.9895957112312317, "min_score_of_first_answer": 0.16373421251773834, "avg_score": 0.6290258765220642, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3557049643854195, "std_of_first_answer": 0.3557049643854195, "avg_entropy_of_first_answer": 0.3970296513289213, "lex_sim": 1.0}
{"question": "what are the zords in the power rangers movie", "answer": ["The Zords are the five Power Rangers vehicles, who are Dacre Montgomery, Naomi Scott, Ludi Lin, Becky G and RJ Cyler."], "scores": [[0.023103035986423492, 0.16346034407615662, 0.9694176912307739, 0.2568027973175049, 0.22856390476226807, 0.051255036145448685, 0.08390843868255615, 0.42052406072616577, 0.8022854328155518, 0.03495584800839424, 0.09359323233366013, 0.0025865244679152966, 0.2181280255317688, 0.0072511970065534115, 0.26202327013015747, 0.9901584982872009, 0.9995580315589905, 0.9999034404754639, 0.2784002423286438, 0.6485081315040588, 0.9983054399490356, 0.9951801300048828, 0.9855368733406067, 0.2812577784061432, 0.9994240999221802, 0.9914451837539673, 0.976499617099762, 0.34225890040397644, 0.9696698188781738, 0.9831581115722656, 0.29523733258247375, 0.989276647567749, 0.9026806950569153, 0.9911460280418396, 0.9870443940162659, 0.5529242753982544, 0.0540936253964901, 0.023476263508200645, 0.7124438285827637, 0.038595493882894516, 0.9117769598960876, 0.9996151924133301, 0.9869697093963623, 0.055435679852962494, 0.0004987830761820078]], "normalized_score": [0.2553752660751343], "tokens": [["\u2581The", "\u2581Z", "ords", "\u2581are", "\u2581the", "\u2581five", "\u2581Power", "\u2581R", "angers", "\u2581vehicles", ",", "\u2581who", "\u2581are", "\u2581D", "acre", "\u2581Mont", "gom", "ery", ",", "\u2581Na", "omi", "\u2581Scott", ",", "\u2581L", "udi", "\u2581Lin", ",", "\u2581Beck", "y", "\u2581G", "\u2581and", "\u2581R", "J", "\u2581Cy", "ler", ".", "\u2581They", "\u2581form", "\u2581the", "\u2581mighty", "\u2581Meg", "az", "ord", "!", ":"]], "entropy": [[4.702175140380859, 4.128380298614502, 0.17896109819412231, 2.475616455078125, 4.640251159667969, 4.888233661651611, 4.480363845825195, 1.6629388332366943, 0.5270473957061768, 2.0396459102630615, 2.9441137313842773, 4.154363632202148, 3.8549861907958984, 4.580792427062988, 2.4905929565429688, 0.09040692448616028, 0.004714997485280037, 0.0012337377993389964, 1.7448899745941162, 1.1545666456222534, 0.017531489953398705, 0.04456532001495361, 0.10789956897497177, 0.8976994156837463, 0.007039956748485565, 0.07911412417888641, 0.14929704368114471, 0.8869902491569519, 0.22909507155418396, 0.1389763057231903, 1.4164526462554932, 0.10944592207670212, 0.3592214584350586, 0.08213405311107635, 0.09453039616346359, 1.9196102619171143, 1.2068065404891968, 3.6172168254852295, 1.5403659343719482, 2.578979253768921, 0.6380985975265503, 0.004788271617144346, 0.07344861328601837, 2.433391571044922, 0.9305059909820557]], "avg_entropy": [1.5623884421954346], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "animal-themed Ninja Zords", "score_of_biggest_cluster": 0.2553752660751343, "score_of_first_answer": 0.2553752660751343, "max_score_of_first_answer": 0.9999034404754639, "min_score_of_first_answer": 0.0004987830761820078, "avg_score": 0.2553752660751343, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.410328865205826, "std_of_first_answer": 0.410328865205826, "avg_entropy_of_first_answer": 1.5623884421954346, "lex_sim": 1.0}
{"question": "where is nuclear energy used most in the world", "answer": ["United States"], "scores": [[0.07484360039234161, 0.9724327921867371]], "normalized_score": [0.26977837085723877], "tokens": [["\u2581United", "\u2581States"]], "entropy": [[3.570178985595703, 0.16804184019565582]], "avg_entropy": [1.8691104128956795], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "United States", "score_of_biggest_cluster": 0.26977837085723877, "score_of_first_answer": 0.26977837085723877, "max_score_of_first_answer": 0.9724327921867371, "min_score_of_first_answer": 0.07484360039234161, "avg_score": 0.26977837085723877, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4487945958971977, "std_of_first_answer": 0.4487945958971977, "avg_entropy_of_first_answer": 1.8691104128956795, "lex_sim": 1.0}
{"question": "when did the romanticism period start and end", "answer": ["Around 1850"], "scores": [[0.012282541953027248, 0.984836995601654, 0.8676489591598511, 0.9965524673461914, 0.41937029361724854, 0.03153075650334358, 0.9354895949363708]], "normalized_score": [0.2783252000808716], "tokens": [["\u2581Ar", "ound", "\u2581", "1", "8", "5", "0"]], "entropy": [[2.933124303817749, 0.12130491435527802, 0.5544423460960388, 0.029904838651418686, 0.7664502859115601, 1.0093320608139038, 0.34965965151786804]], "avg_entropy": [0.823459771594831], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1780\u20131830", "score_of_biggest_cluster": 0.2783252000808716, "score_of_first_answer": 0.2783252000808716, "max_score_of_first_answer": 0.9965524673461914, "min_score_of_first_answer": 0.012282541953027248, "avg_score": 0.2783252000808716, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4123822063244662, "std_of_first_answer": 0.4123822063244662, "avg_entropy_of_first_answer": 0.823459771594831, "lex_sim": 1.0}
{"question": "in what part of the digestive tube do you expect the initial digestion of starch", "answer": ["stomach"], "scores": [[0.09118296205997467, 0.9864184260368347, 0.9964975714683533]], "normalized_score": [0.4475247263908386], "tokens": [["\u2581st", "om", "ach"]], "entropy": [[2.49418306350708, 0.08655010163784027, 0.03309758007526398]], "avg_entropy": [0.8712769150733948], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the mouth", "score_of_biggest_cluster": 0.4475247263908386, "score_of_first_answer": 0.4475247263908386, "max_score_of_first_answer": 0.9964975714683533, "min_score_of_first_answer": 0.09118296205997467, "avg_score": 0.4475247263908386, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4244136696854377, "std_of_first_answer": 0.4244136696854377, "avg_entropy_of_first_answer": 0.8712769150733948, "lex_sim": 1.0}
{"question": "who were the first kings to issue coins bearing their names", "answer": ["Lydians"], "scores": [[0.21678994596004486, 0.9392731189727783, 0.12087178975343704]], "normalized_score": [0.29088324308395386], "tokens": [["\u2581L", "yd", "ians"]], "entropy": [[4.132033348083496, 0.3556157648563385, 1.0359008312225342]], "avg_entropy": [1.8411833147207897], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Indo-Greek kings", "score_of_biggest_cluster": 0.29088324308395386, "score_of_first_answer": 0.29088324308395386, "max_score_of_first_answer": 0.9392731189727783, "min_score_of_first_answer": 0.12087178975343704, "avg_score": 0.29088324308395386, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36529485254222405, "std_of_first_answer": 0.36529485254222405, "avg_entropy_of_first_answer": 1.8411833147207897, "lex_sim": 1.0}
{"question": "what are the main sources of economic growth in the us currently", "answer": ["Consumer spending, government spending, business spending"], "scores": [[0.024006040766835213, 0.6931412220001221, 0.74122154712677, 0.9998682737350464, 0.35769930481910706, 0.08192358911037445, 0.8944766521453857, 0.9998273849487305, 0.809116005897522, 0.20227521657943726, 0.21768954396247864, 0.9998226761817932]], "normalized_score": [0.38764190673828125], "tokens": [["\u2581Cons", "umer", "\u2581sp", "ending", ",", "\u2581government", "\u2581sp", "ending", ",", "\u2581business", "\u2581sp", "ending"]], "entropy": [[5.332840919494629, 0.7909624576568604, 1.2502875328063965, 0.001872724387794733, 1.840573787689209, 3.9841010570526123, 0.6270430088043213, 0.0024388786405324936, 0.7641052603721619, 2.5615181922912598, 1.2544493675231934, 0.0025209877640008926]], "avg_entropy": [1.5343928478735809], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "pharmaceuticals", "score_of_biggest_cluster": 0.38764190673828125, "score_of_first_answer": 0.38764190673828125, "max_score_of_first_answer": 0.9998682737350464, "min_score_of_first_answer": 0.024006040766835213, "avg_score": 0.38764190673828125, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3650615945811583, "std_of_first_answer": 0.3650615945811583, "avg_entropy_of_first_answer": 1.5343928478735809, "lex_sim": 1.0}
{"question": "who is mostly responsible for writing the declaration of independence", "answer": ["Jefferson, Thomas"], "scores": [[0.008612911216914654, 0.07326896488666534, 0.7888250946998596]], "normalized_score": [0.07925322651863098], "tokens": [["\u2581Jefferson", ",", "\u2581Thomas"]], "entropy": [[0.536941409111023, 0.8984937071800232, 1.5198192596435547]], "avg_entropy": [0.9850847919782003], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Thomas Jefferson", "score_of_biggest_cluster": 0.07925322651863098, "score_of_first_answer": 0.07925322651863098, "max_score_of_first_answer": 0.7888250946998596, "min_score_of_first_answer": 0.008612911216914654, "avg_score": 0.07925322651863098, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3535427102038588, "std_of_first_answer": 0.3535427102038588, "avg_entropy_of_first_answer": 0.9850847919782003, "lex_sim": 1.0}
{"question": "when does the implantation of the embryo occur", "answer": ["5-7 days after conception"], "scores": [[0.3837246298789978, 0.15142251551151276, 0.35216301679611206, 0.233117014169693, 0.9414516091346741, 0.7381286025047302, 0.12354432046413422]], "normalized_score": [0.32812556624412537], "tokens": [["\u2581", "5", "-", "7", "\u2581days", "\u2581after", "\u2581conception"]], "entropy": [[3.5309624671936035, 1.864643931388855, 1.9797685146331787, 1.6602249145507812, 0.36306801438331604, 0.9505078196525574, 1.528465747833252]], "avg_entropy": [1.6968059156622206], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "around 9 days after ovulation", "score_of_biggest_cluster": 0.32812556624412537, "score_of_first_answer": 0.32812556624412537, "max_score_of_first_answer": 0.9414516091346741, "min_score_of_first_answer": 0.12354432046413422, "avg_score": 0.32812556624412537, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2863418993325997, "std_of_first_answer": 0.2863418993325997, "avg_entropy_of_first_answer": 1.6968059156622206, "lex_sim": 1.0}
{"question": "when did the angel of the north get built", "answer": ["1998"], "scores": [[0.6707791090011597, 0.9872235059738159, 0.9665789604187012, 0.9857585430145264, 0.9767230153083801]], "normalized_score": [0.9077261686325073], "tokens": [["\u2581", "1", "9", "9", "8"]], "entropy": [[1.2114711999893188, 0.08470761775970459, 0.2082158327102661, 0.09387712180614471, 0.15481778979301453]], "avg_entropy": [0.35061791241168977], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "1998", "score_of_biggest_cluster": 0.9077261686325073, "score_of_first_answer": 0.9077261686325073, "max_score_of_first_answer": 0.9872235059738159, "min_score_of_first_answer": 0.6707791090011597, "avg_score": 0.9077261686325073, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.12353773979037781, "std_of_first_answer": 0.12353773979037781, "avg_entropy_of_first_answer": 0.35061791241168977, "lex_sim": 1.0}
{"question": "oklahoma's 10 geographic regions are defined by surface features called", "answer": ["terrain"], "scores": [[0.0053439452312886715]], "normalized_score": [0.005343945696949959], "tokens": [["\u2581terrain"]], "entropy": [[3.122000217437744]], "avg_entropy": [3.122000217437744], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ecological regions", "score_of_biggest_cluster": 0.005343945696949959, "score_of_first_answer": 0.005343945696949959, "max_score_of_first_answer": 0.0053439452312886715, "min_score_of_first_answer": 0.0053439452312886715, "avg_score": 0.005343945696949959, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.122000217437744, "lex_sim": 1.0}
{"question": "what is the poorest county in the state of florida", "answer": ["Hardee County"], "scores": [[0.06131541728973389, 0.9987566471099854, 0.9993929862976074, 0.8072892427444458]], "normalized_score": [0.4714641869068146], "tokens": [["\u2581Har", "de", "e", "\u2581County"]], "entropy": [[4.288841247558594, 0.012875120155513287, 0.00726533867418766, 0.6778152585029602]], "avg_entropy": [1.2466992412228137], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Union", "score_of_biggest_cluster": 0.4714641869068146, "score_of_first_answer": 0.4714641869068146, "max_score_of_first_answer": 0.9993929862976074, "min_score_of_first_answer": 0.06131541728973389, "avg_score": 0.4714641869068146, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3863957292445905, "std_of_first_answer": 0.3863957292445905, "avg_entropy_of_first_answer": 1.2466992412228137, "lex_sim": 1.0}
{"question": "which mirror is used in vehicles for rear view", "answer": ["Rear-view Mirror"], "scores": [[0.1197458803653717, 0.9953216910362244, 0.6340621709823608, 0.9219446778297424, 0.09859851002693176, 0.9985527396202087]], "normalized_score": [0.4358968436717987], "tokens": [["\u2581R", "ear", "-", "view", "\u2581Mir", "ror"]], "entropy": [[2.831758737564087, 0.04312591627240181, 1.163364291191101, 0.3227028250694275, 0.3913838267326355, 0.014841891825199127]], "avg_entropy": [0.7945295814424753], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "rear-view mirror", "score_of_biggest_cluster": 0.4358968436717987, "score_of_first_answer": 0.4358968436717987, "max_score_of_first_answer": 0.9985527396202087, "min_score_of_first_answer": 0.09859851002693176, "avg_score": 0.4358968436717987, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3867089690173273, "std_of_first_answer": 0.3867089690173273, "avg_entropy_of_first_answer": 0.7945295814424753, "lex_sim": 1.0}
{"question": "a request to the supreme court that it review a case that was already decided is made via a(n)", "answer": ["Petition for a Writ of Certiorari (Cert Petition)"], "scores": [[0.0729612335562706, 0.9860390424728394, 0.8838901519775391, 0.36242032051086426, 0.5530626177787781, 0.9848567247390747, 0.9704763293266296, 0.9966683983802795, 0.98521488904953, 0.011261901818215847, 0.27790766954421997, 0.3171297013759613, 0.9876365065574646, 0.9096450805664062]], "normalized_score": [0.4414273202419281], "tokens": [["\u2581Pet", "ition", "\u2581for", "\u2581a", "\u2581Writ", "\u2581of", "\u2581Cert", "ior", "ari", "\u2581(", "Cert", "\u2581Pet", "ition", ")"]], "entropy": [[1.6791446208953857, 0.09972694516181946, 0.5510314702987671, 1.904259204864502, 0.8452235460281372, 0.10582828521728516, 0.21064937114715576, 0.031778354197740555, 0.10467015951871872, 0.4231797456741333, 3.392064332962036, 1.7788236141204834, 0.08872143924236298, 0.5220755338668823]], "avg_entropy": [0.8383697587996721], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "a writ of certiorari", "score_of_biggest_cluster": 0.4414273202419281, "score_of_first_answer": 0.4414273202419281, "max_score_of_first_answer": 0.9966683983802795, "min_score_of_first_answer": 0.011261901818215847, "avg_score": 0.4414273202419281, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.36605071780261617, "std_of_first_answer": 0.36605071780261617, "avg_entropy_of_first_answer": 0.8383697587996721, "lex_sim": 1.0}
{"question": "who starred in the movie natural born killers", "answer": ["Woody Harrelson and Juliette Lewis"], "scores": [[0.7510008811950684, 0.9974839687347412, 0.9936059713363647, 0.9988228678703308, 0.9996558427810669, 0.10053671151399612, 0.9979286193847656, 0.9998594522476196, 0.9978936314582825, 0.9991174340248108]], "normalized_score": [0.7711099982261658], "tokens": [["\u2581Wo", "ody", "\u2581Har", "rel", "son", "\u2581and", "\u2581Jul", "iet", "te", "\u2581Lewis"]], "entropy": [[1.3822612762451172, 0.028967395424842834, 0.05238553509116173, 0.011858338490128517, 0.004080041311681271, 0.9277202486991882, 0.02132849581539631, 0.0014547738246619701, 0.016172997653484344, 0.009170090779662132]], "avg_entropy": [0.24553991933353245], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Robert Downey Jr.", "score_of_biggest_cluster": 0.7711099982261658, "score_of_first_answer": 0.7711099982261658, "max_score_of_first_answer": 0.9998594522476196, "min_score_of_first_answer": 0.10053671151399612, "avg_score": 0.7711099982261658, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2712160364687782, "std_of_first_answer": 0.2712160364687782, "avg_entropy_of_first_answer": 0.24553991933353245, "lex_sim": 1.0}
{"question": "where did chocolate originate the americas spain asia", "answer": ["The Americas"], "scores": [[0.15725967288017273, 0.7338601350784302, 0.9995531439781189]], "normalized_score": [0.4867943525314331], "tokens": [["\u2581The", "\u2581Amer", "icas"]], "entropy": [[2.7622923851013184, 1.207382082939148, 0.0050840796902775764]], "avg_entropy": [1.324919515910248], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Mesoamerica", "score_of_biggest_cluster": 0.4867943525314331, "score_of_first_answer": 0.4867943525314331, "max_score_of_first_answer": 0.9995531439781189, "min_score_of_first_answer": 0.15725967288017273, "avg_score": 0.4867943525314331, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3515867459642611, "std_of_first_answer": 0.3515867459642611, "avg_entropy_of_first_answer": 1.324919515910248, "lex_sim": 1.0}
{"question": "when was beowulf most likely composed when did it's events probably take place", "answer": ["Beowulf"], "scores": [[0.044704828411340714, 0.9985852241516113, 0.9987210631370544]], "normalized_score": [0.35459116101264954], "tokens": [["\u2581Be", "ow", "ulf"]], "entropy": [[3.642021417617798, 0.01503852941095829, 0.013563724234700203]], "avg_entropy": [1.2235412237544854], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in the 7th century", "score_of_biggest_cluster": 0.35459116101264954, "score_of_first_answer": 0.35459116101264954, "max_score_of_first_answer": 0.9987210631370544, "min_score_of_first_answer": 0.044704828411340714, "avg_score": 0.35459116101264954, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44969555182121373, "std_of_first_answer": 0.44969555182121373, "avg_entropy_of_first_answer": 1.2235412237544854, "lex_sim": 1.0}
{"question": "where did they film ghosts of girlfriends past", "answer": ["Boston"], "scores": [[0.4800557792186737]], "normalized_score": [0.4800557792186737], "tokens": [["\u2581Boston"]], "entropy": [[3.361877918243408]], "avg_entropy": [3.361877918243408], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Massachusetts", "score_of_biggest_cluster": 0.4800557792186737, "score_of_first_answer": 0.4800557792186737, "max_score_of_first_answer": 0.4800557792186737, "min_score_of_first_answer": 0.4800557792186737, "avg_score": 0.4800557792186737, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.361877918243408, "lex_sim": 1.0}
{"question": "when did dragon ball z air in australia", "answer": ["July 1997"], "scores": [[0.04012617841362953, 0.9905858635902405, 0.38351204991340637, 0.1736370027065277, 0.7873404622077942, 0.19867055118083954]], "normalized_score": [0.273006409406662], "tokens": [["\u2581July", "\u2581", "1", "9", "9", "7"]], "entropy": [[3.0421254634857178, 0.07035285234451294, 1.6594414710998535, 2.348616123199463, 0.6557825803756714, 2.0316715240478516]], "avg_entropy": [1.6346650024255116], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fall 1998", "score_of_biggest_cluster": 0.273006409406662, "score_of_first_answer": 0.273006409406662, "max_score_of_first_answer": 0.9905858635902405, "min_score_of_first_answer": 0.04012617841362953, "avg_score": 0.273006409406662, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3452874368821684, "std_of_first_answer": 0.3452874368821684, "avg_entropy_of_first_answer": 1.6346650024255116, "lex_sim": 1.0}
{"question": "jonny cash one piece at a time car", "answer": ["1982 Chevrolet Corvette"], "scores": [[0.20638541877269745, 0.8997842669487, 0.9907225370407104, 0.23340927064418793, 0.013914481736719608, 0.03954176604747772, 0.9036756753921509, 0.9969209432601929, 0.9998039603233337, 0.8078503608703613, 0.9942008256912231, 0.9991890788078308]], "normalized_score": [0.4005938172340393], "tokens": [["\u2581", "1", "9", "8", "2", "\u2581Che", "v", "ro", "let", "\u2581Cor", "v", "ette"]], "entropy": [[3.3616065979003906, 0.5330196022987366, 0.0819614827632904, 1.569962501525879, 1.4042634963989258, 3.1365468502044678, 0.35203316807746887, 0.030083514750003815, 0.0022550763096660376, 1.1196961402893066, 0.059779949486255646, 0.007887085899710655]], "avg_entropy": [0.9715912888253418], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cadillac", "score_of_biggest_cluster": 0.4005938172340393, "score_of_first_answer": 0.4005938172340393, "max_score_of_first_answer": 0.9998039603233337, "min_score_of_first_answer": 0.013914481736719608, "avg_score": 0.4005938172340393, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3970405482048795, "std_of_first_answer": 0.3970405482048795, "avg_entropy_of_first_answer": 0.9715912888253418, "lex_sim": 1.0}
{"question": "what is the meaning of the name sinead", "answer": ["God's Journey"], "scores": [[0.06564629822969437, 0.10116639733314514, 0.9994087219238281, 0.033913906663656235, 0.9082164764404297, 0.9997865557670593]], "normalized_score": [0.24270424246788025], "tokens": [["\u2581God", "'", "s", "\u2581J", "our", "ney"]], "entropy": [[5.245126724243164, 1.2518935203552246, 0.007294612471014261, 4.917416572570801, 0.63111412525177, 0.0021886304020881653]], "avg_entropy": [2.009172364215677], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "God forgave/God gratified", "score_of_biggest_cluster": 0.24270424246788025, "score_of_first_answer": 0.24270424246788025, "max_score_of_first_answer": 0.9997865557670593, "min_score_of_first_answer": 0.033913906663656235, "avg_score": 0.24270424246788025, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4525584691021632, "std_of_first_answer": 0.4525584691021632, "avg_entropy_of_first_answer": 2.009172364215677, "lex_sim": 1.0}
{"question": "who sang my bucket got a hole in it", "answer": ["Hank Williams"], "scores": [[0.5362651348114014, 0.972976803779602, 0.9592360258102417]], "normalized_score": [0.7939671277999878], "tokens": [["\u2581H", "ank", "\u2581Williams"]], "entropy": [[2.957987070083618, 0.16188500821590424, 0.21849289536476135]], "avg_entropy": [1.1127883245547612], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Hank Williams", "score_of_biggest_cluster": 0.7939671277999878, "score_of_first_answer": 0.7939671277999878, "max_score_of_first_answer": 0.972976803779602, "min_score_of_first_answer": 0.5362651348114014, "avg_score": 0.7939671277999878, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.20270675739011854, "std_of_first_answer": 0.20270675739011854, "avg_entropy_of_first_answer": 1.1127883245547612, "lex_sim": 1.0}
{"question": "who does the voice of the beast in the new movie", "answer": ["Ian McKellen"], "scores": [[0.0821831226348877, 0.529072642326355, 0.9520501494407654]], "normalized_score": [0.3459281623363495], "tokens": [["\u2581Ian", "\u2581McK", "ellen"]], "entropy": [[4.9866743087768555, 1.2664363384246826, 0.23860761523246765]], "avg_entropy": [2.163906087478002], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dan Stevens", "score_of_biggest_cluster": 0.3459281623363495, "score_of_first_answer": 0.3459281623363495, "max_score_of_first_answer": 0.9520501494407654, "min_score_of_first_answer": 0.0821831226348877, "avg_score": 0.3459281623363495, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.355166449031513, "std_of_first_answer": 0.355166449031513, "avg_entropy_of_first_answer": 2.163906087478002, "lex_sim": 1.0}
{"question": "what are the roles of international monetary fund", "answer": ["The International Monetary Fund is an international organization headquartered in Washington, D.C., of \"188 countries working to foster global monetary cooperation\", secure financial stability,[5] facilitate international trade,[6"], "scores": [[0.15411847829818726, 0.4710410535335541, 0.9957848191261292, 0.9997168183326721, 0.99985671043396, 0.99811851978302, 0.10711270570755005, 0.8027148842811584, 0.6403026580810547, 0.8998976945877075, 0.49489110708236694, 0.996414303779602, 0.9995877146720886, 0.9990524649620056, 0.9922938346862793, 0.9628255367279053, 0.8951466083526611, 0.861724853515625, 0.9979248046875, 0.9723551273345947, 0.8526155948638916, 0.8273811340332031, 0.9101043343544006, 0.9913823008537292, 0.9864308834075928, 0.4128186106681824, 0.9995551705360413, 0.49102678894996643, 0.9594791531562805, 0.9386534690856934, 0.9998801946640015, 0.9972057938575745, 0.9948675632476807, 0.9999732971191406, 0.9999771118164062, 0.9940390586853027, 0.998346209526062, 0.010797294788062572, 0.8816617727279663, 0.991405725479126, 0.9904429316520691, 0.0036227090749889612, 0.217108815908432, 0.944290041923523, 0.9964438080787659, 0.9999854564666748, 0.9976828098297119, 0.999916672706604, 0.8100659251213074, 0.8689577579498291]], "normalized_score": [0.6557965278625488], "tokens": [["\u2581The", "\u2581International", "\u2581Mon", "et", "ary", "\u2581Fund", "\u2581is", "\u2581an", "\u2581international", "\u2581organization", "\u2581head", "qu", "arter", "ed", "\u2581in", "\u2581Washington", ",", "\u2581D", ".", "C", ".,", "\u2581of", "\u2581\"", "1", "8", "8", "\u2581countries", "\u2581working", "\u2581to", "\u2581fost", "er", "\u2581global", "\u2581mon", "et", "ary", "\u2581co", "operation", "\",", "\u2581secure", "\u2581financial", "\u2581stability", ",[", "5", "]", "\u2581facil", "itate", "\u2581international", "\u2581trade", ",[", "6"]], "entropy": [[4.685144424438477, 1.7115566730499268, 0.04420710727572441, 0.0034736422821879387, 0.0018204045481979847, 0.018030907958745956, 1.6272833347320557, 1.1066291332244873, 0.9650870561599731, 0.5095454454421997, 1.5666358470916748, 0.028506925329566002, 0.00416073901578784, 0.009648213163018227, 0.06799127161502838, 0.27406489849090576, 0.49220800399780273, 0.5513336062431335, 0.020968183875083923, 0.14849747717380524, 0.5558720231056213, 0.8129030466079712, 0.47534888982772827, 0.07666151225566864, 0.07460376620292664, 0.808260440826416, 0.004773302935063839, 0.7706144452095032, 0.18434856832027435, 0.4102494716644287, 0.0014616622356697917, 0.02735241688787937, 0.044915467500686646, 0.00039429718162864447, 0.00032282600295729935, 0.055914416909217834, 0.01394981611520052, 0.5945245027542114, 0.7526678442955017, 0.07314413785934448, 0.0787917897105217, 0.3236905336380005, 2.3240790367126465, 0.23761482536792755, 0.031125083565711975, 0.00020574041991494596, 0.019247684627771378, 0.001092840451747179, 0.6032423973083496, 0.47348183393478394]], "avg_entropy": [0.47335295831086116], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "foster global monetary cooperation", "score_of_biggest_cluster": 0.6557965278625488, "score_of_first_answer": 0.6557965278625488, "max_score_of_first_answer": 0.9999854564666748, "min_score_of_first_answer": 0.0036227090749889612, "avg_score": 0.6557965278625488, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2853395475396119, "std_of_first_answer": 0.2853395475396119, "avg_entropy_of_first_answer": 0.47335295831086116, "lex_sim": 1.0}
{"question": "the probability of making a type i error when retaining ho at the .05 level of significance is", "answer": ["5%"], "scores": [[0.8184248208999634, 0.33142194151878357, 0.8431860208511353]], "normalized_score": [0.611544132232666], "tokens": [["\u2581", "5", "%"]], "entropy": [[0.9164111614227295, 1.2909064292907715, 0.7117795944213867]], "avg_entropy": [0.9730323950449625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "5%", "score_of_biggest_cluster": 0.611544132232666, "score_of_first_answer": 0.611544132232666, "max_score_of_first_answer": 0.8431860208511353, "min_score_of_first_answer": 0.33142194151878357, "avg_score": 0.611544132232666, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23562856703895255, "std_of_first_answer": 0.23562856703895255, "avg_entropy_of_first_answer": 0.9730323950449625, "lex_sim": 1.0}
{"question": "who came up with popcorn at the movies", "answer": ["George Herman \"Babe\" Ruth, Jr."], "scores": [[0.009079884737730026, 0.11450108885765076, 0.24635644257068634, 0.29500076174736023, 0.940688967704773, 0.9464716911315918, 0.9952535629272461, 0.7233430743217468, 0.6260348558425903, 0.9280638694763184, 0.8194793462753296]], "normalized_score": [0.3788248300552368], "tokens": [["\u2581George", "\u2581H", "erman", "\u2581\"", "B", "abe", "\"", "\u2581Ruth", ",", "\u2581Jr", "."]], "entropy": [[3.858654022216797, 4.571122169494629, 1.9451640844345093, 3.082366466522217, 0.39633268117904663, 0.3320941925048828, 0.04506782442331314, 2.067091941833496, 1.1150200366973877, 0.3861900568008423, 0.7424249053001404]], "avg_entropy": [1.685593489218842], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Glen W. Dickson", "score_of_biggest_cluster": 0.3788248300552368, "score_of_first_answer": 0.3788248300552368, "max_score_of_first_answer": 0.9952535629272461, "min_score_of_first_answer": 0.009079884737730026, "avg_score": 0.3788248300552368, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3525016156375396, "std_of_first_answer": 0.3525016156375396, "avg_entropy_of_first_answer": 1.685593489218842, "lex_sim": 1.0}
{"question": "love is not love that alters when it alteration finds meaning", "answer": ["Sonnet 116"], "scores": [[0.029388651251792908, 0.99941086769104, 0.8033334016799927, 0.9980644583702087, 0.9978960752487183, 0.998892605304718]], "normalized_score": [0.5350943803787231], "tokens": [["\u2581Son", "net", "\u2581", "1", "1", "6"]], "entropy": [[4.848931312561035, 0.006398837082087994, 0.7630749940872192, 0.018169663846492767, 0.020424745976924896, 0.011398370377719402]], "avg_entropy": [0.9447329873219132], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Sonnet 116", "score_of_biggest_cluster": 0.5350943803787231, "score_of_first_answer": 0.5350943803787231, "max_score_of_first_answer": 0.99941086769104, "min_score_of_first_answer": 0.029388651251792908, "avg_score": 0.5350943803787231, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.35389427250816635, "std_of_first_answer": 0.35389427250816635, "avg_entropy_of_first_answer": 0.9447329873219132, "lex_sim": 1.0}
{"question": "the mughal garden of rashtrapati bhavan is modelled on which garden", "answer": ["the Taj Mahal garden or Charbagh"], "scores": [[0.06531062722206116, 0.054681334644556046, 0.993826687335968, 0.8352360129356384, 0.9989136457443237, 0.12087753415107727, 0.0009957703296095133, 0.28121665120124817, 0.7976096868515015, 0.9986149072647095]], "normalized_score": [0.19508349895477295], "tokens": [["\u2581the", "\u2581T", "aj", "\u2581Mah", "al", "\u2581garden", "\u2581or", "\u2581Char", "b", "agh"]], "entropy": [[3.154001235961914, 3.8299126625061035, 0.05947422981262207, 0.8854745626449585, 0.011940531432628632, 1.6346491575241089, 0.862790584564209, 2.8251357078552246, 0.7688530683517456, 0.01448617409914732]], "avg_entropy": [1.4046717914752662], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "the Persian gardens", "score_of_biggest_cluster": 0.19508349895477295, "score_of_first_answer": 0.19508349895477295, "max_score_of_first_answer": 0.9989136457443237, "min_score_of_first_answer": 0.0009957703296095133, "avg_score": 0.19508349895477295, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.42048195186164794, "std_of_first_answer": 0.42048195186164794, "avg_entropy_of_first_answer": 1.4046717914752662, "lex_sim": 1.0}
{"question": "who played the girl in silence of the lambs", "answer": ["Jodie Foster"], "scores": [[0.9751768708229065, 0.9877930283546448, 0.996938943862915, 0.9847502708435059, 0.9995884299278259]], "normalized_score": [0.9888104796409607], "tokens": [["\u2581J", "od", "ie", "\u2581F", "oster"]], "entropy": [[0.23251201212406158, 0.08042456209659576, 0.024350907653570175, 0.12557078897953033, 0.004113652743399143]], "avg_entropy": [0.0933943847194314], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Masha Skorobogatov", "score_of_biggest_cluster": 0.9888104796409607, "score_of_first_answer": 0.9888104796409607, "max_score_of_first_answer": 0.9995884299278259, "min_score_of_first_answer": 0.9751768708229065, "avg_score": 0.9888104796409607, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.008782076276539905, "std_of_first_answer": 0.008782076276539905, "avg_entropy_of_first_answer": 0.0933943847194314, "lex_sim": 1.0}
{"question": "who was the first lady nominated member of the rajya sabha", "answer": ["Ela Bhatt"], "scores": [[0.010776729322969913, 0.6242564916610718, 0.9657818078994751]], "normalized_score": [0.18659918010234833], "tokens": [["\u2581Ela", "\u2581Bh", "att"]], "entropy": [[3.588561534881592, 2.150165557861328, 0.20357850193977356]], "avg_entropy": [1.9807685315608978], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mary Kom", "score_of_biggest_cluster": 0.18659918010234833, "score_of_first_answer": 0.18659918010234833, "max_score_of_first_answer": 0.9657818078994751, "min_score_of_first_answer": 0.010776729322969913, "avg_score": 0.18659918010234833, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39511343807641314, "std_of_first_answer": 0.39511343807641314, "avg_entropy_of_first_answer": 1.9807685315608978, "lex_sim": 1.0}
{"question": "who was the buccaneers qb when they won the superbowl", "answer": ["Doug Flutie"], "scores": [[0.8641501665115356, 0.005851254798471928, 0.9916674494743347, 0.9975612163543701]], "normalized_score": [0.26594141125679016], "tokens": [["\u2581Doug", "\u2581Fl", "ut", "ie"]], "entropy": [[0.9303147792816162, 0.13092920184135437, 0.06830953061580658, 0.026463791728019714]], "avg_entropy": [0.2890043258666992], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brad Johnson", "score_of_biggest_cluster": 0.26594141125679016, "score_of_first_answer": 0.26594141125679016, "max_score_of_first_answer": 0.9975612163543701, "min_score_of_first_answer": 0.005851254798471928, "avg_score": 0.26594141125679016, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41277211793741647, "std_of_first_answer": 0.41277211793741647, "avg_entropy_of_first_answer": 0.2890043258666992, "lex_sim": 1.0}
{"question": "which indian actor has won most national awards", "answer": ["Shahid Kapoor"], "scores": [[0.04519382491707802, 0.00497782789170742, 0.9573219418525696, 0.9794883131980896]], "normalized_score": [0.12051583081483841], "tokens": [["\u2581Shah", "id", "\u2581Kap", "oor"]], "entropy": [[3.10849666595459, 0.8713515996932983, 0.2688474953174591, 0.10682623833417892]], "avg_entropy": [1.0888804998248816], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Amitabh Bachchan", "score_of_biggest_cluster": 0.12051583081483841, "score_of_first_answer": 0.12051583081483841, "max_score_of_first_answer": 0.9794883131980896, "min_score_of_first_answer": 0.00497782789170742, "avg_score": 0.12051583081483841, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.47193898998837197, "std_of_first_answer": 0.47193898998837197, "avg_entropy_of_first_answer": 1.0888804998248816, "lex_sim": 1.0}
{"question": "who had the most governmental power under the articles of confederation", "answer": ["Congress"], "scores": [[0.29073360562324524]], "normalized_score": [0.29073360562324524], "tokens": [["\u2581Congress"]], "entropy": [[3.264646053314209]], "avg_entropy": [3.264646053314209], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the states", "score_of_biggest_cluster": 0.29073360562324524, "score_of_first_answer": 0.29073360562324524, "max_score_of_first_answer": 0.29073360562324524, "min_score_of_first_answer": 0.29073360562324524, "avg_score": 0.29073360562324524, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.264646053314209, "lex_sim": 1.0}
{"question": "who won latest america's next top model", "answer": ["Anniesa hibbert"], "scores": [[0.08190546929836273, 9.460704291086586e-07, 6.55579351587221e-05, 0.031964100897312164, 0.17593280971050262]], "normalized_score": [0.001955123618245125], "tokens": [["\u2581Ann", "iesa", "\u2581h", "ib", "bert"]], "entropy": [[4.450892448425293, 2.018998146057129, 0.5230880379676819, 4.4105329513549805, 4.349806785583496]], "avg_entropy": [3.150663673877716], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kyla Coleman", "score_of_biggest_cluster": 0.001955123618245125, "score_of_first_answer": 0.001955123618245125, "max_score_of_first_answer": 0.17593280971050262, "min_score_of_first_answer": 9.460704291086586e-07, "avg_score": 0.001955123618245125, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06614797579136936, "std_of_first_answer": 0.06614797579136936, "avg_entropy_of_first_answer": 3.150663673877716, "lex_sim": 1.0}
{"question": "who was appointed to manage the national recovery administration", "answer": ["John Nance Garner"], "scores": [[0.050980985164642334, 0.23647239804267883, 0.8720106482505798, 0.9795736074447632, 0.9995813965797424]], "normalized_score": [0.40041759610176086], "tokens": [["\u2581John", "\u2581N", "ance", "\u2581Gar", "ner"]], "entropy": [[2.887824058532715, 2.5175671577453613, 0.5063547492027283, 0.13425661623477936, 0.0045126439072191715]], "avg_entropy": [1.2101030451245607], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hugh S. Johnson", "score_of_biggest_cluster": 0.40041759610176086, "score_of_first_answer": 0.40041759610176086, "max_score_of_first_answer": 0.9995813965797424, "min_score_of_first_answer": 0.050980985164642334, "avg_score": 0.40041759610176086, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40186125898348596, "std_of_first_answer": 0.40186125898348596, "avg_entropy_of_first_answer": 1.2101030451245607, "lex_sim": 1.0}
{"question": "who starred in the movie summer of 42", "answer": ["Richad Thomas"], "scores": [[0.042845599353313446, 0.06889026612043381, 0.5947334170341492]], "normalized_score": [0.12063191086053848], "tokens": [["\u2581Rich", "ad", "\u2581Thomas"]], "entropy": [[3.166443109512329, 2.1469340324401855, 0.9314051866531372]], "avg_entropy": [2.0815941095352173], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Oliver Conant", "score_of_biggest_cluster": 0.12063191086053848, "score_of_first_answer": 0.12063191086053848, "max_score_of_first_answer": 0.5947334170341492, "min_score_of_first_answer": 0.042845599353313446, "avg_score": 0.12063191086053848, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2542460547786097, "std_of_first_answer": 0.2542460547786097, "avg_entropy_of_first_answer": 2.0815941095352173, "lex_sim": 1.0}
{"question": "who is the longest serving manager in manchester united history", "answer": ["Sir Alex Ferguson"], "scores": [[0.7448146343231201, 0.8689023852348328, 0.988440752029419, 0.9994527697563171, 0.999092698097229, 0.9964832067489624]], "normalized_score": [0.9274730682373047], "tokens": [["\u2581Sir", "\u2581Alex", "\u2581Fer", "g", "us", "on"]], "entropy": [[1.0237693786621094, 0.5023579597473145, 0.0935530737042427, 0.00646635890007019, 0.009594290517270565, 0.032572537660598755]], "avg_entropy": [0.2780522665319343], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Sir Alex Ferguson", "score_of_biggest_cluster": 0.9274730682373047, "score_of_first_answer": 0.9274730682373047, "max_score_of_first_answer": 0.9994527697563171, "min_score_of_first_answer": 0.7448146343231201, "avg_score": 0.9274730682373047, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.09609902500083592, "std_of_first_answer": 0.09609902500083592, "avg_entropy_of_first_answer": 0.2780522665319343, "lex_sim": 1.0}
{"question": "what ethnic group celebrates its contribution to america in october", "answer": ["Hispanic or Latino American"], "scores": [[0.4758268892765045, 0.999523401260376, 0.966681957244873, 0.2321273684501648, 0.9278823137283325, 0.9907997250556946, 0.1406661421060562]], "normalized_score": [0.5423433780670166], "tokens": [["\u2581His", "pan", "ic", "\u2581or", "\u2581Lat", "ino", "\u2581American"]], "entropy": [[2.503173828125, 0.004621143918484449, 0.15239715576171875, 2.0250320434570312, 0.35494327545166016, 0.06259748339653015, 1.5441148281097412]], "avg_entropy": [0.9495542511743095], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Filipino Americans", "score_of_biggest_cluster": 0.5423433780670166, "score_of_first_answer": 0.5423433780670166, "max_score_of_first_answer": 0.999523401260376, "min_score_of_first_answer": 0.1406661421060562, "avg_score": 0.5423433780670166, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3536303882020445, "std_of_first_answer": 0.3536303882020445, "avg_entropy_of_first_answer": 0.9495542511743095, "lex_sim": 1.0}
{"question": "when did the united kingdom entered world war 2", "answer": ["3 September 1939"], "scores": [[0.12833735346794128, 0.7383347153663635, 0.8835283517837524, 0.9699276089668274, 0.9962562322616577, 0.9998289346694946, 0.9994596838951111, 0.9997199177742004]], "normalized_score": [0.730194091796875], "tokens": [["\u2581", "3", "\u2581September", "\u2581", "1", "9", "3", "9"]], "entropy": [[0.9544639587402344, 0.8639824390411377, 0.5661017894744873, 0.16309133172035217, 0.02636028826236725, 0.0020672446116805077, 0.0057824961841106415, 0.003246914828196168]], "avg_entropy": [0.32313705785782076], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "1939", "score_of_biggest_cluster": 0.730194091796875, "score_of_first_answer": 0.730194091796875, "max_score_of_first_answer": 0.9998289346694946, "min_score_of_first_answer": 0.12833735346794128, "avg_score": 0.730194091796875, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.28206203698407595, "std_of_first_answer": 0.28206203698407595, "avg_entropy_of_first_answer": 0.32313705785782076, "lex_sim": 1.0}
{"question": "when was the first dark tower book published", "answer": ["February 18, 1987"], "scores": [[0.07605244964361191, 0.9810319542884827, 0.33487263321876526, 0.04688212648034096, 0.9560304880142212, 0.9872788190841675, 0.6756142973899841, 0.9994768500328064, 0.976534366607666, 0.6618430018424988]], "normalized_score": [0.4659624397754669], "tokens": [["\u2581February", "\u2581", "1", "8", ",", "\u2581", "1", "9", "8", "7"]], "entropy": [[2.029855728149414, 0.10293811559677124, 1.2405774593353271, 1.9531728029251099, 0.2553444504737854, 0.07496589422225952, 0.6323906183242798, 0.005146395415067673, 0.11734096705913544, 1.0866048336029053]], "avg_entropy": [0.7498337265104056], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1982", "score_of_biggest_cluster": 0.4659624397754669, "score_of_first_answer": 0.4659624397754669, "max_score_of_first_answer": 0.9994768500328064, "min_score_of_first_answer": 0.04688212648034096, "avg_score": 0.4659624397754669, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36540313942739105, "std_of_first_answer": 0.36540313942739105, "avg_entropy_of_first_answer": 0.7498337265104056, "lex_sim": 1.0}
{"question": "who was the main character in their eyes were watching god", "answer": ["Janie Crawford"], "scores": [[0.968942403793335, 0.9897443652153015, 0.596480667591095, 0.9999520778656006, 0.9987441301345825]], "normalized_score": [0.8940672278404236], "tokens": [["\u2581Jan", "ie", "\u2581C", "raw", "ford"]], "entropy": [[0.28084176778793335, 0.0906570702791214, 1.1436102390289307, 0.0006474386900663376, 0.014681621454656124]], "avg_entropy": [0.30608762744814155], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Janie Crawford", "score_of_biggest_cluster": 0.8940672278404236, "score_of_first_answer": 0.8940672278404236, "max_score_of_first_answer": 0.9999520778656006, "min_score_of_first_answer": 0.596480667591095, "avg_score": 0.8940672278404236, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.15753834703869235, "std_of_first_answer": 0.15753834703869235, "avg_entropy_of_first_answer": 0.30608762744814155, "lex_sim": 1.0}
{"question": "when did john hart play the lone ranger", "answer": ["February 28, 2006"], "scores": [[0.017364852130413055, 0.9908079504966736, 0.23028072714805603, 0.08586007356643677, 0.9401673078536987, 0.9968158602714539, 0.06753108650445938, 0.9959264993667603, 0.4312203824520111, 0.03161830082535744]], "normalized_score": [0.22212111949920654], "tokens": [["\u2581February", "\u2581", "2", "8", ",", "\u2581", "2", "0", "0", "6"]], "entropy": [[2.5634140968322754, 0.07489520311355591, 1.6941721439361572, 2.403085231781006, 0.3437103033065796, 0.028670893982052803, 0.24985244870185852, 0.034615546464920044, 0.8067116141319275, 1.1228673458099365]], "avg_entropy": [0.9321994828060269], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "from 1952 until 1953", "score_of_biggest_cluster": 0.22212111949920654, "score_of_first_answer": 0.22212111949920654, "max_score_of_first_answer": 0.9968158602714539, "min_score_of_first_answer": 0.017364852130413055, "avg_score": 0.22212111949920654, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4255861354826906, "std_of_first_answer": 0.4255861354826906, "avg_entropy_of_first_answer": 0.9321994828060269, "lex_sim": 1.0}
{"question": "who is the captain of richmond football club", "answer": ["Trent Cotchin"], "scores": [[0.608424723148346, 0.9973316192626953, 0.9269202351570129, 0.9997933506965637]], "normalized_score": [0.865963876247406], "tokens": [["\u2581Tr", "ent", "\u2581Cot", "chin"]], "entropy": [[1.5546150207519531, 0.024410169571638107, 0.5402023792266846, 0.002132842317223549]], "avg_entropy": [0.5303401029668748], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Trent Cotchin", "score_of_biggest_cluster": 0.865963876247406, "score_of_first_answer": 0.865963876247406, "max_score_of_first_answer": 0.9997933506965637, "min_score_of_first_answer": 0.608424723148346, "avg_score": 0.865963876247406, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.16127067343167725, "std_of_first_answer": 0.16127067343167725, "avg_entropy_of_first_answer": 0.5303401029668748, "lex_sim": 1.0}
{"question": "wolf of wall street number of f words", "answer": ["506 in 181 mins, 0.27 per min"], "scores": [[0.927033543586731, 0.9007654786109924, 0.7734026312828064, 0.8898909687995911, 0.0028600916266441345, 0.7460297346115112, 0.8525146842002869, 0.5344247817993164, 0.5489316582679749, 0.01433312427252531, 0.9838146567344666, 0.028306644409894943, 0.3938850164413452, 0.02845495566725731, 0.8529220819473267, 0.7255274653434753, 0.5294263362884521, 0.14862263202667236, 0.3561629354953766]], "normalized_score": [0.27597296237945557], "tokens": [["\u2581", "5", "0", "6", "\u2581in", "\u2581", "1", "8", "1", "\u2581m", "ins", ",", "\u2581", "0", ".", "2", "7", "\u2581per", "\u2581min"]], "entropy": [[0.6675571203231812, 0.4899815618991852, 0.8537567853927612, 0.5211341381072998, 0.7418134212493896, 1.293027639389038, 0.6170441508293152, 1.6200170516967773, 0.8776789903640747, 0.5304727554321289, 0.12237126380205154, 1.547965407371521, 2.942774772644043, 1.3031034469604492, 0.9160177707672119, 1.103326678276062, 0.977385401725769, 2.8097777366638184, 1.1971101760864258]], "avg_entropy": [1.1122271720516055], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "569", "score_of_biggest_cluster": 0.27597296237945557, "score_of_first_answer": 0.27597296237945557, "max_score_of_first_answer": 0.9838146567344666, "min_score_of_first_answer": 0.0028600916266441345, "avg_score": 0.27597296237945557, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.342063690604678, "std_of_first_answer": 0.342063690604678, "avg_entropy_of_first_answer": 1.1122271720516055, "lex_sim": 1.0}
{"question": "what do you call a person who officiates a wedding", "answer": ["officiant"], "scores": [[0.5148280262947083, 0.864290177822113]], "normalized_score": [0.6670538187026978], "tokens": [["\u2581offici", "ant"]], "entropy": [[2.160451889038086, 0.5415093898773193]], "avg_entropy": [1.3509806394577026], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "vow master", "score_of_biggest_cluster": 0.6670538187026978, "score_of_first_answer": 0.6670538187026978, "max_score_of_first_answer": 0.864290177822113, "min_score_of_first_answer": 0.5148280262947083, "avg_score": 0.6670538187026978, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1747310757637024, "std_of_first_answer": 0.1747310757637024, "avg_entropy_of_first_answer": 1.3509806394577026, "lex_sim": 1.0}
{"question": "who developed the concept of total quality management", "answer": ["W. Edwards Deming"], "scores": [[0.5904921889305115, 0.8927093148231506, 0.9439235329627991, 0.9872035384178162, 0.9944756627082825]], "normalized_score": [0.8665075898170471], "tokens": [["\u2581W", ".", "\u2581Edwards", "\u2581Dem", "ing"]], "entropy": [[1.925370693206787, 0.4502301514148712, 0.2547629475593567, 0.09242268651723862, 0.04066256806254387]], "avg_entropy": [0.5526898093521595], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "W. Edwards Deming", "score_of_biggest_cluster": 0.8665075898170471, "score_of_first_answer": 0.8665075898170471, "max_score_of_first_answer": 0.9944756627082825, "min_score_of_first_answer": 0.5904921889305115, "avg_score": 0.8665075898170471, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.15009616284300517, "std_of_first_answer": 0.15009616284300517, "avg_entropy_of_first_answer": 0.5526898093521595, "lex_sim": 1.0}
{"question": "what are the 4 types of nitrogenous bases", "answer": ["pyrimidine, cytosine, adenine, and guanine"], "scores": [[0.00298280268907547, 0.9972680807113647, 0.9971798658370972, 0.9944354295730591, 0.5793940424919128, 0.6398499011993408, 0.028940817341208458, 0.9969738721847534, 0.9962316155433655, 0.9983423948287964, 0.9497421383857727, 0.21602840721607208, 0.998094379901886, 0.9741818904876709, 0.8402182459831238, 0.10646893829107285, 0.6089925169944763, 0.9950339794158936, 0.9948475360870361]], "normalized_score": [0.4564472734928131], "tokens": [["\u2581p", "yr", "im", "id", "ine", ",", "\u2581c", "yt", "os", "ine", ",", "\u2581ad", "en", "ine", ",", "\u2581and", "\u2581gu", "an", "ine"]], "entropy": [[1.7607622146606445, 0.03129040449857712, 0.029024474322795868, 0.044792938977479935, 0.7117127776145935, 1.6604797840118408, 1.1292877197265625, 0.029208600521087646, 0.02912428416311741, 0.016370147466659546, 0.3111281991004944, 1.9065595865249634, 0.020910155028104782, 0.13139745593070984, 0.5885517001152039, 1.2724223136901855, 0.9126613140106201, 0.044761545956134796, 0.04328946769237518]], "avg_entropy": [0.5617755307374817], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "cytosine (C)", "score_of_biggest_cluster": 0.4564472734928131, "score_of_first_answer": 0.4564472734928131, "max_score_of_first_answer": 0.9983423948287964, "min_score_of_first_answer": 0.00298280268907547, "avg_score": 0.4564472734928131, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36110698705905697, "std_of_first_answer": 0.36110698705905697, "avg_entropy_of_first_answer": 0.5617755307374817, "lex_sim": 1.0}
{"question": "when boy meets girl waiting for a star to fall", "answer": ["November 11, 2018"], "scores": [[0.03445044904947281, 0.9445340633392334, 0.4364779591560364, 0.07267710566520691, 0.7993469834327698, 0.996579110622406, 0.5881013870239258, 0.999180018901825, 0.6381721496582031, 0.23687642812728882]], "normalized_score": [0.38579580187797546], "tokens": [["\u2581November", "\u2581", "1", "1", ",", "\u2581", "2", "0", "1", "8"]], "entropy": [[3.7660372257232666, 0.3062291443347931, 1.6066755056381226, 2.2791318893432617, 0.6507371664047241, 0.03446144610643387, 0.6892756223678589, 0.00881134532392025, 0.7340488433837891, 2.180941343307495]], "avg_entropy": [1.2256349531933666], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1988", "score_of_biggest_cluster": 0.38579580187797546, "score_of_first_answer": 0.38579580187797546, "max_score_of_first_answer": 0.999180018901825, "min_score_of_first_answer": 0.03445044904947281, "avg_score": 0.38579580187797546, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35024901262636576, "std_of_first_answer": 0.35024901262636576, "avg_entropy_of_first_answer": 1.2256349531933666, "lex_sim": 1.0}
{"question": "which material is the heaviest in term of density", "answer": ["gold"], "scores": [[0.05539947375655174]], "normalized_score": [0.05539947748184204], "tokens": [["\u2581gold"]], "entropy": [[2.529203414916992]], "avg_entropy": [2.529203414916992], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Osmium", "score_of_biggest_cluster": 0.05539947748184204, "score_of_first_answer": 0.05539947748184204, "max_score_of_first_answer": 0.05539947375655174, "min_score_of_first_answer": 0.05539947375655174, "avg_score": 0.05539947748184204, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.529203414916992, "lex_sim": 1.0}
{"question": "what is the scientific name for the calf muscle", "answer": ["gastrocnemius"], "scores": [[0.4718734323978424, 0.955623984336853, 0.999061644077301, 0.9977986216545105, 0.9901536107063293]], "normalized_score": [0.8505308628082275], "tokens": [["\u2581g", "ast", "roc", "nem", "ius"]], "entropy": [[1.7637434005737305, 0.28043419122695923, 0.01149599440395832, 0.024583805352449417, 0.08237016201019287]], "avg_entropy": [0.4325255107134581], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "gastrocnemius", "score_of_biggest_cluster": 0.8505308628082275, "score_of_first_answer": 0.8505308628082275, "max_score_of_first_answer": 0.999061644077301, "min_score_of_first_answer": 0.4718734323978424, "avg_score": 0.8505308628082275, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2061214083832747, "std_of_first_answer": 0.2061214083832747, "avg_entropy_of_first_answer": 0.4325255107134581, "lex_sim": 1.0}
{"question": "season 2 this is us number of episodes", "answer": ["18"], "scores": [[0.9293126463890076, 0.9325686693191528, 0.9727391004562378]], "normalized_score": [0.9446690082550049], "tokens": [["\u2581", "1", "8"]], "entropy": [[0.6342450976371765, 0.38027292490005493, 0.19375747442245483]], "avg_entropy": [0.4027584989865621], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "15", "score_of_biggest_cluster": 0.9446690082550049, "score_of_first_answer": 0.9446690082550049, "max_score_of_first_answer": 0.9727391004562378, "min_score_of_first_answer": 0.9293126463890076, "avg_score": 0.9446690082550049, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.019748761243610455, "std_of_first_answer": 0.019748761243610455, "avg_entropy_of_first_answer": 0.4027584989865621, "lex_sim": 1.0}
{"question": "in which year was led zepplin refused entry into singapore because the band members had long hair", "answer": ["1677 - year of the Great Fire of London"], "scores": [[0.7852106690406799, 0.9889261722564697, 0.0008360963547602296, 0.016644690185785294, 0.08128311485052109, 0.003584926715120673, 0.0028333126101642847, 0.2744830548763275, 0.2405712604522705, 0.21553952991962433, 0.7010549306869507, 0.8668501973152161, 0.9199132323265076]], "normalized_score": [0.09739861637353897], "tokens": [["\u2581", "1", "6", "7", "7", "\u2581-", "\u2581year", "\u2581of", "\u2581the", "\u2581Great", "\u2581Fire", "\u2581of", "\u2581London"]], "entropy": [[1.3477003574371338, 0.08076665550470352, 0.07434172928333282, 3.098191261291504, 2.54567813873291, 1.2781776189804077, 4.401310443878174, 3.31951642036438, 3.386579990386963, 4.6405930519104, 1.9851657152175903, 0.6807595491409302, 0.6422979831695557]], "avg_entropy": [2.113929147330614], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1972", "score_of_biggest_cluster": 0.09739861637353897, "score_of_first_answer": 0.09739861637353897, "max_score_of_first_answer": 0.9889261722564697, "min_score_of_first_answer": 0.0008360963547602296, "avg_score": 0.09739861637353897, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37939904498788296, "std_of_first_answer": 0.37939904498788296, "avg_entropy_of_first_answer": 2.113929147330614, "lex_sim": 1.0}
{"question": "when did beds become popular in france and germany", "answer": ["18th Century"], "scores": [[0.5690197944641113, 0.9283764362335205, 0.06958886235952377, 0.6172637939453125, 0.08021679520606995]], "normalized_score": [0.28315576910972595], "tokens": [["\u2581", "1", "8", "th", "\u2581Century"]], "entropy": [[2.6038124561309814, 0.41082334518432617, 2.106684684753418, 1.468018651008606, 0.758842945098877]], "avg_entropy": [1.4696364164352418], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the 12th century", "score_of_biggest_cluster": 0.28315576910972595, "score_of_first_answer": 0.28315576910972595, "max_score_of_first_answer": 0.9283764362335205, "min_score_of_first_answer": 0.06958886235952377, "avg_score": 0.28315576910972595, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33238450745453885, "std_of_first_answer": 0.33238450745453885, "avg_entropy_of_first_answer": 1.4696364164352418, "lex_sim": 1.0}
{"question": "who was originally cast as phoebe on charmed", "answer": ["Kathleen Robertson"], "scores": [[0.0033794192131608725, 0.6632079482078552, 0.9997578263282776, 0.8175797462463379, 0.9995521903038025]], "normalized_score": [0.28349432349205017], "tokens": [["\u2581Kath", "le", "en", "\u2581Robert", "son"]], "entropy": [[2.6501717567443848, 0.9302463531494141, 0.003102736547589302, 1.3018889427185059, 0.005362475290894508]], "avg_entropy": [0.9781544528901577], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lori Rom", "score_of_biggest_cluster": 0.28349432349205017, "score_of_first_answer": 0.28349432349205017, "max_score_of_first_answer": 0.9997578263282776, "min_score_of_first_answer": 0.0033794192131608725, "avg_score": 0.28349432349205017, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3687792167393529, "std_of_first_answer": 0.3687792167393529, "avg_entropy_of_first_answer": 0.9781544528901577, "lex_sim": 1.0}
{"question": "who do you meet at the gates of heaven", "answer": ["Jesus"], "scores": [[0.02065969631075859]], "normalized_score": [0.02065969631075859], "tokens": [["\u2581Jesus"]], "entropy": [[3.055809497833252]], "avg_entropy": [3.055809497833252], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Saint Peter", "score_of_biggest_cluster": 0.02065969631075859, "score_of_first_answer": 0.02065969631075859, "max_score_of_first_answer": 0.02065969631075859, "min_score_of_first_answer": 0.02065969631075859, "avg_score": 0.02065969631075859, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.055809497833252, "lex_sim": 1.0}
{"question": "who did america declare war on in ww1", "answer": ["Germany"], "scores": [[0.7064985036849976]], "normalized_score": [0.7064985036849976], "tokens": [["\u2581Germany"]], "entropy": [[1.5130866765975952]], "avg_entropy": [1.5130866765975952], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Austria-Hungary", "score_of_biggest_cluster": 0.7064985036849976, "score_of_first_answer": 0.7064985036849976, "max_score_of_first_answer": 0.7064985036849976, "min_score_of_first_answer": 0.7064985036849976, "avg_score": 0.7064985036849976, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.5130866765975952, "lex_sim": 1.0}
{"question": "who dies in transformers revenge of the fallen", "answer": ["Optimus Prime"], "scores": [[0.564346432685852, 0.9997524619102478, 0.9987190961837769, 0.9566055536270142]], "normalized_score": [0.8568476438522339], "tokens": [["\u2581Opt", "im", "us", "\u2581Prime"]], "entropy": [[2.2800207138061523, 0.002982241567224264, 0.013158094137907028, 0.27842721343040466]], "avg_entropy": [0.6436470657354221], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ravage and the Decepticon Rampage", "score_of_biggest_cluster": 0.8568476438522339, "score_of_first_answer": 0.8568476438522339, "max_score_of_first_answer": 0.9997524619102478, "min_score_of_first_answer": 0.564346432685852, "avg_score": 0.8568476438522339, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18298932910257648, "std_of_first_answer": 0.18298932910257648, "avg_entropy_of_first_answer": 0.6436470657354221, "lex_sim": 1.0}
{"question": "when did the astros change from the national league to the american league", "answer": ["November 17, 2017"], "scores": [[0.44255495071411133, 0.9843809604644775, 0.29844292998313904, 0.8346298336982727, 0.9825767874717712, 0.9987553358078003, 0.974809467792511, 0.9997887015342712, 0.9988362193107605, 0.24051018059253693]], "normalized_score": [0.6913161277770996], "tokens": [["\u2581November", "\u2581", "1", "7", ",", "\u2581", "2", "0", "1", "7"]], "entropy": [[1.6982178688049316, 0.09931717813014984, 0.9565346837043762, 0.790184497833252, 0.11489361524581909, 0.013107849285006523, 0.11773261427879333, 0.0025731062050908804, 0.009601281024515629, 1.1145732402801514]], "avg_entropy": [0.49167359347920864], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2013", "score_of_biggest_cluster": 0.6913161277770996, "score_of_first_answer": 0.6913161277770996, "max_score_of_first_answer": 0.9997887015342712, "min_score_of_first_answer": 0.24051018059253693, "avg_score": 0.6913161277770996, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30073410827855057, "std_of_first_answer": 0.30073410827855057, "avg_entropy_of_first_answer": 0.49167359347920864, "lex_sim": 1.0}
{"question": "when was the time the eagles won the superbowl", "answer": ["January 25, 2018"], "scores": [[0.17894095182418823, 0.9962812066078186, 0.7443305850028992, 0.4085107147693634, 0.9847720265388489, 0.9977472424507141, 0.44163376092910767, 0.9998053908348083, 0.6714890003204346, 0.3225163221359253]], "normalized_score": [0.5897915959358215], "tokens": [["\u2581January", "\u2581", "2", "5", ",", "\u2581", "2", "0", "1", "8"]], "entropy": [[2.0818347930908203, 0.030156154185533524, 0.7114322185516357, 1.444564938545227, 0.10036821663379669, 0.018042653799057007, 0.6873928904533386, 0.0021519740112125874, 0.6430731415748596, 1.238903284072876]], "avg_entropy": [0.6957920264918357], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2017", "score_of_biggest_cluster": 0.5897915959358215, "score_of_first_answer": 0.5897915959358215, "max_score_of_first_answer": 0.9998053908348083, "min_score_of_first_answer": 0.17894095182418823, "avg_score": 0.5897915959358215, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3015721785098044, "std_of_first_answer": 0.3015721785098044, "avg_entropy_of_first_answer": 0.6957920264918357, "lex_sim": 1.0}
{"question": "who was running as vice president in 1984", "answer": ["Geraldine Ferraro"], "scores": [[0.4542309045791626, 0.9960070252418518, 0.9971488118171692, 0.8032630085945129, 0.9788732528686523]], "normalized_score": [0.8127868175506592], "tokens": [["\u2581Ger", "ald", "ine", "\u2581Ferr", "aro"]], "entropy": [[1.5132038593292236, 0.03837186098098755, 0.026484983041882515, 0.8153334856033325, 0.11636028438806534]], "avg_entropy": [0.5019508946686984], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George H. W. Bush", "score_of_biggest_cluster": 0.8127868175506592, "score_of_first_answer": 0.8127868175506592, "max_score_of_first_answer": 0.9971488118171692, "min_score_of_first_answer": 0.4542309045791626, "avg_score": 0.8127868175506592, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2089558811422134, "std_of_first_answer": 0.2089558811422134, "avg_entropy_of_first_answer": 0.5019508946686984, "lex_sim": 1.0}
{"question": "when did the song rapper's delight come out", "answer": ["September 13, 1979"], "scores": [[0.2158852219581604, 0.9755867719650269, 0.6464247703552246, 0.08792584389448166, 0.9663869738578796, 0.9869194626808167, 0.9954498410224915, 0.999713122844696, 0.8234711289405823, 0.9988865256309509]], "normalized_score": [0.6267026662826538], "tokens": [["\u2581September", "\u2581", "1", "3", ",", "\u2581", "1", "9", "7", "9"]], "entropy": [[2.5856595039367676, 0.13646499812602997, 1.2096143960952759, 2.1592817306518555, 0.1980883777141571, 0.08140994608402252, 0.030439674854278564, 0.0032847863622009754, 0.4746206998825073, 0.010729185305535793]], "avg_entropy": [0.6889593299012631], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "1979", "score_of_biggest_cluster": 0.6267026662826538, "score_of_first_answer": 0.6267026662826538, "max_score_of_first_answer": 0.999713122844696, "min_score_of_first_answer": 0.08792584389448166, "avg_score": 0.6267026662826538, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.32776055696879963, "std_of_first_answer": 0.32776055696879963, "avg_entropy_of_first_answer": 0.6889593299012631, "lex_sim": 1.0}
{"question": "who proved that cells come from other cells", "answer": ["Rudolph Virchow"], "scores": [[0.016995908692479134, 0.9281601905822754, 0.8895756006240845, 0.9874136447906494, 0.9982637763023376]], "normalized_score": [0.4247945249080658], "tokens": [["\u2581Rud", "olph", "\u2581Vir", "ch", "ow"]], "entropy": [[2.722546100616455, 0.5028817057609558, 0.7957470417022705, 0.10491029918193817, 0.01676832139492035]], "avg_entropy": [0.828570693731308], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Matthias Schleiden", "score_of_biggest_cluster": 0.4247945249080658, "score_of_first_answer": 0.4247945249080658, "max_score_of_first_answer": 0.9982637763023376, "min_score_of_first_answer": 0.016995908692479134, "avg_score": 0.4247945249080658, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37563973489617963, "std_of_first_answer": 0.37563973489617963, "avg_entropy_of_first_answer": 0.828570693731308, "lex_sim": 1.0}
{"question": "when does the second part of vikings season 4 start", "answer": ["12 October 14"], "scores": [[0.10115467756986618, 0.4450567662715912, 0.043641943484544754, 0.03525569662451744, 0.7936140298843384, 0.023083947598934174, 0.029397351667284966]], "normalized_score": [0.08686068654060364], "tokens": [["\u2581", "1", "2", "\u2581October", "\u2581", "1", "4"]], "entropy": [[2.5031402111053467, 1.3431702852249146, 2.447418212890625, 3.0431013107299805, 0.7314764261245728, 0.18710634112358093, 2.1681571006774902]], "avg_entropy": [1.774795698268073], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "November 30, 2016", "score_of_biggest_cluster": 0.08686068654060364, "score_of_first_answer": 0.08686068654060364, "max_score_of_first_answer": 0.7936140298843384, "min_score_of_first_answer": 0.023083947598934174, "avg_score": 0.08686068654060364, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2760614884903242, "std_of_first_answer": 0.2760614884903242, "avg_entropy_of_first_answer": 1.774795698268073, "lex_sim": 1.0}
{"question": "who has the most votes for the all star game", "answer": ["Aaron Judge"], "scores": [[0.014260850846767426, 0.9930467009544373, 0.8808590769767761]], "normalized_score": [0.2319212406873703], "tokens": [["\u2581A", "aron", "\u2581Judge"]], "entropy": [[4.045348644256592, 0.067293681204319, 0.5840291380882263]], "avg_entropy": [1.5655571545163791], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "fan voting", "score_of_biggest_cluster": 0.2319212406873703, "score_of_first_answer": 0.2319212406873703, "max_score_of_first_answer": 0.9930467009544373, "min_score_of_first_answer": 0.014260850846767426, "avg_score": 0.2319212406873703, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4373658891434829, "std_of_first_answer": 0.4373658891434829, "avg_entropy_of_first_answer": 1.5655571545163791, "lex_sim": 1.0}
{"question": "when was the last time the eu was audited", "answer": ["1994"], "scores": [[0.46707385778427124, 0.5526147484779358, 0.9424316883087158, 0.988289475440979, 0.42246246337890625]], "normalized_score": [0.6329159736633301], "tokens": [["\u2581", "1", "9", "9", "4"]], "entropy": [[3.1168713569641113, 0.7968552112579346, 0.35903167724609375, 0.08615057170391083, 1.7554268836975098]], "avg_entropy": [1.222867140173912], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "1994", "score_of_biggest_cluster": 0.6329159736633301, "score_of_first_answer": 0.6329159736633301, "max_score_of_first_answer": 0.988289475440979, "min_score_of_first_answer": 0.42246246337890625, "avg_score": 0.6329159736633301, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.24151841986277978, "std_of_first_answer": 0.24151841986277978, "avg_entropy_of_first_answer": 1.222867140173912, "lex_sim": 1.0}
{"question": "when were birth certificates first issued in the united states", "answer": ["1862"], "scores": [[0.5419378876686096, 0.9773039221763611, 0.4333212375640869, 0.28146272897720337, 0.04804345965385437]], "normalized_score": [0.31504228711128235], "tokens": [["\u2581", "1", "8", "6", "2"]], "entropy": [[2.7133102416992188, 0.14018310606479645, 1.1591030359268188, 2.150697708129883, 1.7198556661605835]], "avg_entropy": [1.57662995159626], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1902", "score_of_biggest_cluster": 0.31504228711128235, "score_of_first_answer": 0.31504228711128235, "max_score_of_first_answer": 0.9773039221763611, "min_score_of_first_answer": 0.04804345965385437, "avg_score": 0.31504228711128235, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30872264818725187, "std_of_first_answer": 0.30872264818725187, "avg_entropy_of_first_answer": 1.57662995159626, "lex_sim": 1.0}
{"question": "who said that religion is the opiate of the masses", "answer": ["Karl Marx"], "scores": [[0.9463140964508057, 0.9842340350151062]], "normalized_score": [0.9650878310203552], "tokens": [["\u2581Karl", "\u2581Marx"]], "entropy": [[0.4897119998931885, 0.12440071254968643]], "avg_entropy": [0.30705635622143745], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Karl Marx.", "score_of_biggest_cluster": 0.9650878310203552, "score_of_first_answer": 0.9650878310203552, "max_score_of_first_answer": 0.9842340350151062, "min_score_of_first_answer": 0.9463140964508057, "avg_score": 0.9650878310203552, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.01895996928215027, "std_of_first_answer": 0.01895996928215027, "avg_entropy_of_first_answer": 0.30705635622143745, "lex_sim": 1.0}
{"question": "on which river did the exploration of the louisiana purchase begin", "answer": ["Missouri River"], "scores": [[0.5594328045845032, 0.7115048766136169]], "normalized_score": [0.630903422832489], "tokens": [["\u2581Missouri", "\u2581River"]], "entropy": [[1.9613075256347656, 0.8350245952606201]], "avg_entropy": [1.3981660604476929], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Missouri River", "score_of_biggest_cluster": 0.630903422832489, "score_of_first_answer": 0.630903422832489, "max_score_of_first_answer": 0.7115048766136169, "min_score_of_first_answer": 0.5594328045845032, "avg_score": 0.630903422832489, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.07603603601455688, "std_of_first_answer": 0.07603603601455688, "avg_entropy_of_first_answer": 1.3981660604476929, "lex_sim": 1.0}
{"question": "when was to god be the glory written", "answer": ["It was written by William M. Runyan (1902-1987) of the Reformed Church in America (RCA) who died in 1987."], "scores": [[0.01995214633643627, 0.7953841686248779, 0.60589599609375, 0.3691335916519165, 0.40926703810691833, 0.08801540732383728, 0.9310278296470642, 0.9208700656890869, 0.9920003414154053, 0.05359336733818054, 0.7722967863082886, 0.2765728235244751, 0.37616240978240967, 0.7603589296340942, 0.6333847045898438, 0.8969365358352661, 0.9983000159263611, 0.903224766254425, 0.9759519100189209, 0.6219061613082886, 0.011958172544836998, 0.09189263731241226, 0.042467646300792694, 0.9955238103866577, 0.9567010402679443, 0.8852785229682922, 0.6399579644203186, 0.05821164697408676, 0.8009069561958313, 0.9910076260566711, 0.4648059606552124, 0.018485942855477333, 0.03865019604563713, 0.3716227412223816, 0.38510558009147644, 0.9948008060455322, 0.9969990253448486, 0.989844799041748, 0.9872636198997498, 0.645241379737854]], "normalized_score": [0.36692216992378235], "tokens": [["\u2581It", "\u2581was", "\u2581written", "\u2581by", "\u2581William", "\u2581M", ".", "\u2581Run", "yan", "\u2581(", "1", "9", "0", "2", "-", "1", "9", "8", "7", ")", "\u2581of", "\u2581the", "\u2581Re", "formed", "\u2581Church", "\u2581in", "\u2581America", "\u2581(", "R", "CA", ")", "\u2581who", "\u2581died", "\u2581in", "\u2581", "1", "9", "8", "7", "."]], "entropy": [[2.6111414432525635, 1.0210150480270386, 1.791740894317627, 1.5056188106536865, 3.224841594696045, 2.8589601516723633, 0.49585023522377014, 0.6557033061981201, 0.06848859786987305, 1.6466479301452637, 1.5442335605621338, 0.6419642567634583, 1.7613074779510498, 0.9555142521858215, 1.2136809825897217, 0.5792992115020752, 0.01570049114525318, 0.3631363809108734, 0.1599864512681961, 1.0228934288024902, 1.708089828491211, 4.160540580749512, 2.7175464630126953, 0.037978578358888626, 0.27352091670036316, 0.5116835832595825, 1.179572582244873, 1.9625190496444702, 1.2287952899932861, 0.05693898722529411, 1.1044667959213257, 2.137012481689453, 3.293379783630371, 2.054198980331421, 3.7324705123901367, 0.04096074402332306, 0.027306044474244118, 0.07327420264482498, 0.09310348331928253, 1.460872769355774]], "avg_entropy": [1.2997989040799438], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "around 1872", "score_of_biggest_cluster": 0.36692216992378235, "score_of_first_answer": 0.36692216992378235, "max_score_of_first_answer": 0.9983000159263611, "min_score_of_first_answer": 0.011958172544836998, "avg_score": 0.36692216992378235, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3618455512113732, "std_of_first_answer": 0.3618455512113732, "avg_entropy_of_first_answer": 1.2997989040799438, "lex_sim": 1.0}
{"question": "where do the royalties for winnie the pooh go", "answer": ["the royalties go to A."], "scores": [[0.04869106784462929, 0.1526116281747818, 0.9982445240020752, 0.9947975873947144, 0.27376729249954224, 0.9410475492477417, 0.03752034157514572, 0.9374801516532898, 0.46220582723617554, 0.9934169054031372, 0.9948857426643372, 0.9771694540977478, 0.6630843877792358, 0.999168872833252, 0.03206558898091316, 0.990159809589386, 0.0035984544083476067]], "normalized_score": [0.3100360929965973], "tokens": [["\u2581the", "\u2581roy", "alt", "ies", "\u2581go", "\u2581to", "\u2581A", ".", "\u2581A", ".", "\u2581Mil", "ne", "'", "s", "\u2581steps", "on", "..."]], "entropy": [[4.307218551635742, 4.705782890319824, 0.016858646646142006, 0.04450735077261925, 2.347311496734619, 0.4273079037666321, 2.633577346801758, 0.4019533395767212, 0.744688868522644, 0.054934415966272354, 0.04919501021504402, 0.12674586474895477, 1.0487638711929321, 0.01007077470421791, 2.910029411315918, 0.07656767964363098, 1.8242595195770264]], "avg_entropy": [1.2782219377729822], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Slesinger family", "score_of_biggest_cluster": 0.3100360929965973, "score_of_first_answer": 0.3100360929965973, "max_score_of_first_answer": 0.999168872833252, "min_score_of_first_answer": 0.0035984544083476067, "avg_score": 0.3100360929965973, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4152977728032008, "std_of_first_answer": 0.4152977728032008, "avg_entropy_of_first_answer": 1.2782219377729822, "lex_sim": 1.0}
{"question": "when does buffy tell her friends she was in heaven", "answer": ["Season 3, Episode 22, \"The Gift\""], "scores": [[0.22940492630004883, 0.981862485408783, 0.15365202724933624, 0.5080955624580383, 0.8167290091514587, 0.9989122152328491, 0.991752564907074, 0.1575039178133011, 0.630488395690918, 0.18735092878341675, 0.2922654449939728, 0.41209161281585693, 0.7940180897712708, 0.9998076558113098, 0.9253411293029785]], "normalized_score": [0.49125388264656067], "tokens": [["\u2581Season", "\u2581", "3", ",", "\u2581E", "pisode", "\u2581", "2", "2", ",", "\u2581\"", "The", "\u2581G", "ift", "\""]], "entropy": [[3.8653171062469482, 0.1549203097820282, 1.4250802993774414, 1.5823752880096436, 0.7262987494468689, 0.011420411989092827, 0.07527296245098114, 1.5428566932678223, 1.5950891971588135, 2.0376510620117188, 2.125891923904419, 1.345770001411438, 0.7099502086639404, 0.0020596026442945004, 0.4491742253303528]], "avg_entropy": [1.1766085361130536], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Something to Sing About", "score_of_biggest_cluster": 0.49125388264656067, "score_of_first_answer": 0.49125388264656067, "max_score_of_first_answer": 0.9998076558113098, "min_score_of_first_answer": 0.15365202724933624, "avg_score": 0.49125388264656067, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3316970352362277, "std_of_first_answer": 0.3316970352362277, "avg_entropy_of_first_answer": 1.1766085361130536, "lex_sim": 1.0}
{"question": "who sings for the beast in the new movie", "answer": ["Idinalo Menzel"], "scores": [[0.009321432560682297, 1.2248738130438142e-05, 0.19198545813560486, 0.7111945748329163, 0.9010816812515259, 0.9980454444885254]], "normalized_score": [0.04910482466220856], "tokens": [["\u2581Id", "inal", "o", "\u2581M", "enz", "el"]], "entropy": [[3.251920700073242, 0.04345244914293289, 3.858788013458252, 1.7639236450195312, 0.5687816143035889, 0.020027412101626396]], "avg_entropy": [1.5844823056831956], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dan Stevens", "score_of_biggest_cluster": 0.04910482466220856, "score_of_first_answer": 0.04910482466220856, "max_score_of_first_answer": 0.9980454444885254, "min_score_of_first_answer": 1.2248738130438142e-05, "avg_score": 0.04910482466220856, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41497628419802385, "std_of_first_answer": 0.41497628419802385, "avg_entropy_of_first_answer": 1.5844823056831956, "lex_sim": 1.0}
{"question": "whens the last time the patriots played the eagles", "answer": ["November 27, 2011"], "scores": [[0.20736461877822876, 0.9933990240097046, 0.5252019762992859, 0.6209604740142822, 0.9281315803527832, 0.9964821338653564, 0.9279318451881409, 0.999711811542511, 0.8117826581001282, 0.6153321862220764]], "normalized_score": [0.7013370394706726], "tokens": [["\u2581November", "\u2581", "2", "7", ",", "\u2581", "2", "0", "1", "1"]], "entropy": [[2.7035255432128906, 0.05186591297388077, 1.263108491897583, 1.2908741235733032, 0.34974992275238037, 0.030402036383748055, 0.2630002796649933, 0.003413025289773941, 0.493275910615921, 1.3301715850830078]], "avg_entropy": [0.7779386831447482], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Super Bowl LII", "score_of_biggest_cluster": 0.7013370394706726, "score_of_first_answer": 0.7013370394706726, "max_score_of_first_answer": 0.999711811542511, "min_score_of_first_answer": 0.20736461877822876, "avg_score": 0.7013370394706726, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25054720429462096, "std_of_first_answer": 0.25054720429462096, "avg_entropy_of_first_answer": 0.7779386831447482, "lex_sim": 1.0}
{"question": "the secret of crickley hall how many episodes", "answer": ["2012"], "scores": [[0.7958916425704956, 0.04400685429573059, 0.20696204900741577, 0.4134148061275482, 0.8766340613365173]], "normalized_score": [0.30471503734588623], "tokens": [["\u2581", "2", "0", "1", "2"]], "entropy": [[1.0184838771820068, 0.7366775870323181, 2.5449905395507812, 1.497981071472168, 0.5797865986824036]], "avg_entropy": [1.2755839347839355], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "3", "score_of_biggest_cluster": 0.30471503734588623, "score_of_first_answer": 0.30471503734588623, "max_score_of_first_answer": 0.8766340613365173, "min_score_of_first_answer": 0.04400685429573059, "avg_score": 0.30471503734588623, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3241553201351083, "std_of_first_answer": 0.3241553201351083, "avg_entropy_of_first_answer": 1.2755839347839355, "lex_sim": 1.0}
{"question": "who are the characters in yo gabba gabba", "answer": ["Brobee, Foofa, Muno and Toodee"], "scores": [[0.38254740834236145, 0.9715437293052673, 0.9994263648986816, 0.8374589681625366, 0.40747690200805664, 0.9961483478546143, 0.9988420605659485, 0.9911088943481445, 0.8491512537002563, 0.9995929598808289, 0.001515091280452907, 0.7099152207374573, 0.998353123664856, 0.9994226694107056]], "normalized_score": [0.5227738618850708], "tokens": [["\u2581Bro", "be", "e", ",", "\u2581Fo", "of", "a", ",", "\u2581M", "uno", "\u2581and", "\u2581To", "ode", "e"]], "entropy": [[2.698765277862549, 0.1706523597240448, 0.0057733370922505856, 0.9051288366317749, 1.5692758560180664, 0.03299052268266678, 0.011918693780899048, 0.07852023839950562, 0.621381402015686, 0.005169904790818691, 0.06941888481378555, 1.0758224725723267, 0.016870032995939255, 0.006707284599542618]], "avg_entropy": [0.519171078855704], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "DJ Lance Rock", "score_of_biggest_cluster": 0.5227738618850708, "score_of_first_answer": 0.5227738618850708, "max_score_of_first_answer": 0.9995929598808289, "min_score_of_first_answer": 0.001515091280452907, "avg_score": 0.5227738618850708, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3024071447727989, "std_of_first_answer": 0.3024071447727989, "avg_entropy_of_first_answer": 0.519171078855704, "lex_sim": 1.0}
{"question": "who discovered cells divide to make new cells", "answer": ["Robert Hooke"], "scores": [[0.5787703990936279, 0.8922805786132812, 0.9998419284820557, 0.9993526339530945]], "normalized_score": [0.8475483059883118], "tokens": [["\u2581Robert", "\u2581H", "oo", "ke"]], "entropy": [[2.551647424697876, 0.6030670404434204, 0.0019536877516657114, 0.00756820198148489]], "avg_entropy": [0.7910590887186117], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hugo von Mohl", "score_of_biggest_cluster": 0.8475483059883118, "score_of_first_answer": 0.8475483059883118, "max_score_of_first_answer": 0.9998419284820557, "min_score_of_first_answer": 0.5787703990936279, "avg_score": 0.8475483059883118, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17239369736027896, "std_of_first_answer": 0.17239369736027896, "avg_entropy_of_first_answer": 0.7910590887186117, "lex_sim": 1.0}
{"question": "what culture region is germany a part of", "answer": ["Western culture"], "scores": [[0.5970878601074219, 0.28785133361816406]], "normalized_score": [0.41457512974739075], "tokens": [["\u2581Western", "\u2581culture"]], "entropy": [[1.9225013256072998, 1.8961819410324097]], "avg_entropy": [1.9093416333198547], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Central Germany", "score_of_biggest_cluster": 0.41457512974739075, "score_of_first_answer": 0.41457512974739075, "max_score_of_first_answer": 0.5970878601074219, "min_score_of_first_answer": 0.28785133361816406, "avg_score": 0.41457512974739075, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1546182632446289, "std_of_first_answer": 0.1546182632446289, "avg_entropy_of_first_answer": 1.9093416333198547, "lex_sim": 1.0}
{"question": "who wrote shes always a woman to me", "answer": ["Billy Joel"], "scores": [[0.8142901659011841, 0.9853958487510681, 0.9993834495544434]], "normalized_score": [0.9290534257888794], "tokens": [["\u2581Billy", "\u2581Jo", "el"]], "entropy": [[1.3626726865768433, 0.09634889662265778, 0.006842330098152161]], "avg_entropy": [0.4886213044325511], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Billy Joel", "score_of_biggest_cluster": 0.9290534257888794, "score_of_first_answer": 0.9290534257888794, "max_score_of_first_answer": 0.9993834495544434, "min_score_of_first_answer": 0.8142901659011841, "avg_score": 0.9290534257888794, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08415087728684155, "std_of_first_answer": 0.08415087728684155, "avg_entropy_of_first_answer": 0.4886213044325511, "lex_sim": 1.0}
{"question": "who plays david in alvin and the chipmunks", "answer": ["Jason Lee"], "scores": [[0.9410161972045898, 0.9866845011711121]], "normalized_score": [0.9635798335075378], "tokens": [["\u2581Jason", "\u2581Lee"]], "entropy": [[0.38457241654396057, 0.09120230376720428]], "avg_entropy": [0.23788736015558243], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Jason Lee", "score_of_biggest_cluster": 0.9635798335075378, "score_of_first_answer": 0.9635798335075378, "max_score_of_first_answer": 0.9866845011711121, "min_score_of_first_answer": 0.9410161972045898, "avg_score": 0.9635798335075378, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.02283415198326111, "std_of_first_answer": 0.02283415198326111, "avg_entropy_of_first_answer": 0.23788736015558243, "lex_sim": 1.0}
{"question": "who sings god is great beer is good", "answer": ["Kevin Smith"], "scores": [[0.006931904703378677, 0.0034906526561826468]], "normalized_score": [0.004919030237942934], "tokens": [["\u2581Kevin", "\u2581Smith"]], "entropy": [[5.972475528717041, 4.319034099578857]], "avg_entropy": [5.145754814147949], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Currington", "score_of_biggest_cluster": 0.004919030237942934, "score_of_first_answer": 0.004919030237942934, "max_score_of_first_answer": 0.006931904703378677, "min_score_of_first_answer": 0.0034906526561826468, "avg_score": 0.004919030237942934, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0017206260235980153, "std_of_first_answer": 0.0017206260235980153, "avg_entropy_of_first_answer": 5.145754814147949, "lex_sim": 1.0}
{"question": "when did athens emerges as wealthiest greek city state", "answer": ["700 BC"], "scores": [[0.6191298961639404, 0.20102505385875702, 0.36347347497940063, 0.9962356686592102, 0.46590301394462585]], "normalized_score": [0.4617771506309509], "tokens": [["\u2581", "7", "0", "0", "\u2581BC"]], "entropy": [[2.1767778396606445, 1.9345687627792358, 1.6338697671890259, 0.035788360983133316, 1.654198408126831]], "avg_entropy": [1.4870406277477741], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the late 6th century BCE", "score_of_biggest_cluster": 0.4617771506309509, "score_of_first_answer": 0.4617771506309509, "max_score_of_first_answer": 0.9962356686592102, "min_score_of_first_answer": 0.20102505385875702, "avg_score": 0.4617771506309509, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2703258315404374, "std_of_first_answer": 0.2703258315404374, "avg_entropy_of_first_answer": 1.4870406277477741, "lex_sim": 1.0}
{"question": "who was the oldest man elected president of usa", "answer": ["Ronald Wilson Reagan"], "scores": [[0.7738236784934998, 0.9988595247268677, 0.6463398933410645, 0.9923946857452393, 0.9980515241622925]], "normalized_score": [0.8687382936477661], "tokens": [["\u2581Ron", "ald", "\u2581Wilson", "\u2581Re", "agan"]], "entropy": [[1.2620360851287842, 0.013045962899923325, 0.8304996490478516, 0.0624702088534832, 0.0174698606133461]], "avg_entropy": [0.4371043533086777], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Ronald Reagan", "score_of_biggest_cluster": 0.8687382936477661, "score_of_first_answer": 0.8687382936477661, "max_score_of_first_answer": 0.9988595247268677, "min_score_of_first_answer": 0.6463398933410645, "avg_score": 0.8687382936477661, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1459786542605627, "std_of_first_answer": 0.1459786542605627, "avg_entropy_of_first_answer": 0.4371043533086777, "lex_sim": 1.0}
{"question": "where do the florida marlins have spring training", "answer": ["Fort Lauderdale"], "scores": [[0.00972591433674097, 0.539158821105957, 0.9978208541870117, 0.9984754920005798, 0.9993347525596619]], "normalized_score": [0.3495824337005615], "tokens": [["\u2581Fort", "\u2581La", "ud", "erd", "ale"]], "entropy": [[2.1269195079803467, 1.484999656677246, 0.018392879515886307, 0.01607157476246357, 0.007989085279405117]], "avg_entropy": [0.7308745408430696], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Roger Dean Stadium", "score_of_biggest_cluster": 0.3495824337005615, "score_of_first_answer": 0.3495824337005615, "max_score_of_first_answer": 0.9993347525596619, "min_score_of_first_answer": 0.00972591433674097, "avg_score": 0.3495824337005615, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3922596183323417, "std_of_first_answer": 0.3922596183323417, "avg_entropy_of_first_answer": 0.7308745408430696, "lex_sim": 1.0}
{"question": "who was the dj on in living color", "answer": ["Dwayne Wayans"], "scores": [[0.04203873127698898, 0.4886493980884552, 0.9987906813621521, 0.3063807785511017, 0.9649693369865417]], "normalized_score": [0.3602295219898224], "tokens": [["\u2581D", "way", "ne", "\u2581Way", "ans"]], "entropy": [[4.627923488616943, 2.0691025257110596, 0.01392448041588068, 3.6890525817871094, 0.2674289643764496]], "avg_entropy": [2.1334864081814886], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "DJ Twist", "score_of_biggest_cluster": 0.3602295219898224, "score_of_first_answer": 0.3602295219898224, "max_score_of_first_answer": 0.9987906813621521, "min_score_of_first_answer": 0.04203873127698898, "avg_score": 0.3602295219898224, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3726217337473665, "std_of_first_answer": 0.3726217337473665, "avg_entropy_of_first_answer": 2.1334864081814886, "lex_sim": 1.0}
{"question": "when does it's a wonderful life comes on", "answer": ["Nov 28, 2012"], "scores": [[0.018717065453529358, 0.7505970597267151, 0.677108645439148, 0.10720105469226837, 0.4285507798194885, 0.9346681237220764, 0.7228043079376221, 0.9496063590049744, 0.9635439515113831, 0.03272471949458122]], "normalized_score": [0.3123542368412018], "tokens": [["\u2581Nov", "\u2581", "2", "8", ",", "\u2581", "2", "0", "1", "2"]], "entropy": [[3.135105848312378, 0.6849383115768433, 0.9224622249603271, 2.2460713386535645, 1.7826242446899414, 0.4715498089790344, 1.090396761894226, 0.2964867353439331, 0.17960822582244873, 1.1334319114685059]], "avg_entropy": [1.1942675411701202], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Christmas season", "score_of_biggest_cluster": 0.3123542368412018, "score_of_first_answer": 0.3123542368412018, "max_score_of_first_answer": 0.9635439515113831, "min_score_of_first_answer": 0.018717065453529358, "avg_score": 0.3123542368412018, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3639856059481283, "std_of_first_answer": 0.3639856059481283, "avg_entropy_of_first_answer": 1.1942675411701202, "lex_sim": 1.0}
{"question": "where does arsenic and old lace take place", "answer": ["Brooklyn"], "scores": [[0.38056254386901855, 0.8513132929801941]], "normalized_score": [0.5691906213760376], "tokens": [["\u2581Brook", "lyn"]], "entropy": [[2.9716386795043945, 0.4658900499343872]], "avg_entropy": [1.7187643647193909], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Brooklyn, New York", "score_of_biggest_cluster": 0.5691906213760376, "score_of_first_answer": 0.5691906213760376, "max_score_of_first_answer": 0.8513132929801941, "min_score_of_first_answer": 0.38056254386901855, "avg_score": 0.5691906213760376, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23537537455558777, "std_of_first_answer": 0.23537537455558777, "avg_entropy_of_first_answer": 1.7187643647193909, "lex_sim": 1.0}
{"question": "avatar the last airbender season 3 episode 12", "answer": ["The Western Air Temple"], "scores": [[0.6700919270515442, 0.742314338684082, 0.965286910533905, 0.9960202574729919]], "normalized_score": [0.831594705581665], "tokens": [["\u2581The", "\u2581Western", "\u2581Air", "\u2581Temple"]], "entropy": [[2.3226871490478516, 1.0744580030441284, 0.24375000596046448, 0.029433270916342735]], "avg_entropy": [0.9175821072421968], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "The Western Air Temple", "score_of_biggest_cluster": 0.831594705581665, "score_of_first_answer": 0.831594705581665, "max_score_of_first_answer": 0.9960202574729919, "min_score_of_first_answer": 0.6700919270515442, "avg_score": 0.831594705581665, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1400029980095094, "std_of_first_answer": 0.1400029980095094, "avg_entropy_of_first_answer": 0.9175821072421968, "lex_sim": 1.0}
{"question": "today gestalt psychology ideas are part of which branch of psychology", "answer": ["cognitive psychology"], "scores": [[0.6969749927520752, 0.9821332097053528, 0.7289883494377136, 0.9984651803970337]], "normalized_score": [0.8401566743850708], "tokens": [["\u2581cogn", "itive", "\u2581psych", "ology"]], "entropy": [[1.8630695343017578, 0.10272373259067535, 0.8282163739204407, 0.015012035146355629]], "avg_entropy": [0.7022554189898074], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Berlin School of experimental psychology", "score_of_biggest_cluster": 0.8401566743850708, "score_of_first_answer": 0.8401566743850708, "max_score_of_first_answer": 0.9984651803970337, "min_score_of_first_answer": 0.6969749927520752, "avg_score": 0.8401566743850708, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1392397242435957, "std_of_first_answer": 0.1392397242435957, "avg_entropy_of_first_answer": 0.7022554189898074, "lex_sim": 1.0}
{"question": "the plane of earth's orbit is called the", "answer": ["ecliptic"], "scores": [[0.7715551257133484, 0.9992473125457764, 0.9957914352416992]], "normalized_score": [0.9156639575958252], "tokens": [["\u2581e", "cli", "ptic"]], "entropy": [[1.1384142637252808, 0.007899136282503605, 0.02778293564915657]], "avg_entropy": [0.3913654452189803], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "ecliptic", "score_of_biggest_cluster": 0.9156639575958252, "score_of_first_answer": 0.9156639575958252, "max_score_of_first_answer": 0.9992473125457764, "min_score_of_first_answer": 0.7715551257133484, "avg_score": 0.9156639575958252, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.10652991105442247, "std_of_first_answer": 0.10652991105442247, "avg_entropy_of_first_answer": 0.3913654452189803, "lex_sim": 1.0}
{"question": "when did the apple iphone se come out", "answer": ["March 31"], "scores": [[0.5347882509231567, 0.9828503131866455, 0.23048366606235504, 0.9857285618782043]], "normalized_score": [0.5878502130508423], "tokens": [["\u2581March", "\u2581", "3", "1"]], "entropy": [[1.6843547821044922, 0.10228150337934494, 0.6204270720481873, 0.08107126504182816]], "avg_entropy": [0.6220336556434631], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "March 31, 2016", "score_of_biggest_cluster": 0.5878502130508423, "score_of_first_answer": 0.5878502130508423, "max_score_of_first_answer": 0.9857285618782043, "min_score_of_first_answer": 0.23048366606235504, "avg_score": 0.5878502130508423, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3194885331204999, "std_of_first_answer": 0.3194885331204999, "avg_entropy_of_first_answer": 0.6220336556434631, "lex_sim": 1.0}
{"question": "where does the last name waller come from", "answer": ["It's a locational name from south Germany"], "scores": [[0.02319992519915104, 0.10286407917737961, 0.9990496039390564, 0.23722274601459503, 0.07373858988285065, 0.9195601940155029, 0.38464853167533875, 0.6725459694862366, 0.0010075311874970794, 0.033346544951200485]], "normalized_score": [0.11279408633708954], "tokens": [["\u2581It", "'", "s", "\u2581a", "\u2581loc", "ational", "\u2581name", "\u2581from", "\u2581south", "\u2581Germany"]], "entropy": [[4.40294075012207, 2.1849474906921387, 0.011537715792655945, 3.1956124305725098, 4.372448921203613, 0.30703413486480713, 0.9339134693145752, 1.7051607370376587, 2.958263397216797, 3.1530942916870117]], "avg_entropy": [2.3224953338503838], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Anglo-Saxon origin", "score_of_biggest_cluster": 0.11279408633708954, "score_of_first_answer": 0.11279408633708954, "max_score_of_first_answer": 0.9990496039390564, "min_score_of_first_answer": 0.0010075311874970794, "avg_score": 0.11279408633708954, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3646368538749532, "std_of_first_answer": 0.3646368538749532, "avg_entropy_of_first_answer": 2.3224953338503838, "lex_sim": 1.0}
{"question": "when did the age of new media began", "answer": ["?"], "scores": [[0.0020622529555112123]], "normalized_score": [0.002062253188341856], "tokens": [["\u2581?"]], "entropy": [[3.122281074523926]], "avg_entropy": [3.122281074523926], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the 1980s", "score_of_biggest_cluster": 0.002062253188341856, "score_of_first_answer": 0.002062253188341856, "max_score_of_first_answer": 0.0020622529555112123, "min_score_of_first_answer": 0.0020622529555112123, "avg_score": 0.002062253188341856, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.122281074523926, "lex_sim": 0.0}
{"question": "when do willow and tara get back together", "answer": ["Season 6"], "scores": [[0.1894153505563736, 0.9172007441520691, 0.521483838558197]], "normalized_score": [0.44913145899772644], "tokens": [["\u2581Season", "\u2581", "6"]], "entropy": [[4.449310302734375, 0.5367637872695923, 1.3512482643127441]], "avg_entropy": [2.1124407847722373], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Entropy", "score_of_biggest_cluster": 0.44913145899772644, "score_of_first_answer": 0.44913145899772644, "max_score_of_first_answer": 0.9172007441520691, "min_score_of_first_answer": 0.1894153505563736, "avg_score": 0.44913145899772644, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29749564501534176, "std_of_first_answer": 0.29749564501534176, "avg_entropy_of_first_answer": 2.1124407847722373, "lex_sim": 1.0}
{"question": "when did sweet caroline start at red sox games", "answer": ["The song first began to be played at Red Sox home games in April 2002."], "scores": [[0.011652960442006588, 0.40037062764167786, 0.047408562153577805, 0.0893280878663063, 0.36927586793899536, 0.8308605551719666, 0.7516910433769226, 0.47905075550079346, 0.35222598910331726, 0.9973649382591248, 0.9999401569366455, 0.15720947086811066, 0.978049099445343, 0.4702571630477905, 0.03872634470462799, 0.8542152047157288, 0.9006928205490112, 0.9979300498962402, 0.9815038442611694, 0.9659807682037354, 0.3203139901161194]], "normalized_score": [0.3606796860694885], "tokens": [["\u2581The", "\u2581song", "\u2581first", "\u2581began", "\u2581to", "\u2581be", "\u2581played", "\u2581at", "\u2581Red", "\u2581So", "x", "\u2581home", "\u2581games", "\u2581in", "\u2581April", "\u2581", "2", "0", "0", "2", "."]], "entropy": [[2.5654520988464355, 2.790271520614624, 2.546335220336914, 2.3952064514160156, 1.8353796005249023, 0.7874286770820618, 0.9764330387115479, 1.9525095224380493, 1.984580159187317, 0.02546865865588188, 0.0008208545041270554, 0.5899927616119385, 0.16583065688610077, 1.920522928237915, 1.3000597953796387, 0.5622768402099609, 0.341602623462677, 0.020170878618955612, 0.09376056492328644, 0.20367592573165894, 1.9120914936065674]], "avg_entropy": [1.1890414414755512], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1997", "score_of_biggest_cluster": 0.3606796860694885, "score_of_first_answer": 0.3606796860694885, "max_score_of_first_answer": 0.9999401569366455, "min_score_of_first_answer": 0.011652960442006588, "avg_score": 0.3606796860694885, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36514979390855035, "std_of_first_answer": 0.36514979390855035, "avg_entropy_of_first_answer": 1.1890414414755512, "lex_sim": 1.0}
{"question": "who won the battle of stirling bridge 1297", "answer": ["William Wallace over King Edward I"], "scores": [[0.8040633797645569, 0.9881177544593811, 0.9992559552192688, 8.133194933179766e-05, 0.053750179708004, 0.9387542009353638, 0.8354818820953369]], "normalized_score": [0.16031956672668457], "tokens": [["\u2581William", "\u2581Wal", "lace", "\u2581over", "\u2581King", "\u2581Edward", "\u2581I"]], "entropy": [[1.0632381439208984, 0.1128193587064743, 0.007713494822382927, 0.4223434329032898, 2.591254711151123, 0.354995459318161, 0.8748318552970886]], "avg_entropy": [0.7753137794456312], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Scottish victory", "score_of_biggest_cluster": 0.16031956672668457, "score_of_first_answer": 0.16031956672668457, "max_score_of_first_answer": 0.9992559552192688, "min_score_of_first_answer": 8.133194933179766e-05, "avg_score": 0.16031956672668457, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.40620884403624097, "std_of_first_answer": 0.40620884403624097, "avg_entropy_of_first_answer": 0.7753137794456312, "lex_sim": 1.0}
{"question": "an open-source clone of unix created in the early 1990s", "answer": ["Linux"], "scores": [[0.5948286652565002]], "normalized_score": [0.5948286652565002], "tokens": [["\u2581Linux"]], "entropy": [[2.1144819259643555]], "avg_entropy": [2.1144819259643555], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "GNU/Linux", "score_of_biggest_cluster": 0.5948286652565002, "score_of_first_answer": 0.5948286652565002, "max_score_of_first_answer": 0.5948286652565002, "min_score_of_first_answer": 0.5948286652565002, "avg_score": 0.5948286652565002, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.1144819259643555, "lex_sim": 1.0}
{"question": "when was the last time the carolina hurricanes made the playoffs", "answer": ["2009"], "scores": [[0.8114729523658752, 0.9867472648620605, 0.9990869760513306, 0.7952120304107666, 0.6561772227287292]], "normalized_score": [0.8396871089935303], "tokens": [["\u2581", "2", "0", "0", "9"]], "entropy": [[1.1348278522491455, 0.09291893988847733, 0.010237745940685272, 0.5132719874382019, 0.6976712346076965]], "avg_entropy": [0.4897855520248413], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "2008\u201309", "score_of_biggest_cluster": 0.8396871089935303, "score_of_first_answer": 0.8396871089935303, "max_score_of_first_answer": 0.9990869760513306, "min_score_of_first_answer": 0.6561772227287292, "avg_score": 0.8396871089935303, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.12882524987381802, "std_of_first_answer": 0.12882524987381802, "avg_entropy_of_first_answer": 0.4897855520248413, "lex_sim": 1.0}
{"question": "who drives the number 95 car in nascar", "answer": ["Matt Kenseth"], "scores": [[0.05450078845024109, 0.45352447032928467, 0.9993574023246765, 0.9990290403366089]], "normalized_score": [0.3963470458984375], "tokens": [["\u2581Matt", "\u2581K", "ens", "eth"]], "entropy": [[2.6132116317749023, 0.9716081619262695, 0.007976646535098553, 0.01049060933291912]], "avg_entropy": [0.9008217623922974], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Michael Christopher McDowell", "score_of_biggest_cluster": 0.3963470458984375, "score_of_first_answer": 0.3963470458984375, "max_score_of_first_answer": 0.9993574023246765, "min_score_of_first_answer": 0.05450078845024109, "avg_score": 0.3963470458984375, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.398404354226429, "std_of_first_answer": 0.398404354226429, "avg_entropy_of_first_answer": 0.9008217623922974, "lex_sim": 1.0}
{"question": "the boiling point of water is 100 degrees celsius express this in si units", "answer": ["100 degrees Celsius = 100 degrees centigrade = 212 degrees Fahrenheit = 212 \u00b0F = 373.1 +/- 0.5 kelvin"], "scores": [[0.9390857815742493, 0.3810403347015381, 0.8231126070022583, 0.9948752522468567, 0.37768977880477905, 0.571618378162384, 0.9828853607177734, 0.9999055862426758, 0.45361328125, 0.9772846698760986, 0.17685560882091522, 0.8557199239730835, 0.9933752417564392, 0.3403257429599762, 0.00644907308742404, 0.9687464237213135, 0.9990948438644409, 0.6501072645187378, 0.9782329797744751, 0.21849338710308075, 0.9777474999427795, 0.990542471408844, 0.8596497774124146, 0.8716516494750977, 0.9714266061782837, 0.9973018169403076, 0.4587146043777466, 0.9773038029670715, 0.12801270186901093, 0.8530868887901306, 0.9398319125175476, 0.025985121726989746, 0.9295963048934937, 0.5551368594169617, 0.9670006632804871, 0.46819570660591125, 0.7367259860038757, 0.7997547388076782, 0.5875653028488159, 0.9543029069900513, 5.461412365548313e-05, 0.4454180598258972, 0.7080895900726318, 0.7857611179351807, 0.9905816316604614, 0.6089341044425964, 0.1703883558511734, 0.9908947944641113, 0.7700698971748352]], "normalized_score": [0.46983909606933594], "tokens": [["\u2581", "1", "0", "0", "\u2581degrees", "\u2581Cel", "si", "us", "\u2581=", "\u2581", "1", "0", "0", "\u2581degrees", "\u2581cent", "ig", "rade", "\u2581=", "\u2581", "2", "1", "2", "\u2581degrees", "\u2581F", "ahren", "heit", "\u2581=", "\u2581", "2", "1", "2", "\u2581\u00b0", "F", "\u2581=", "\u2581", "3", "7", "3", ".", "1", "\u2581+", "/-", "\u2581", "0", ".", "5", "\u2581k", "el", "vin"]], "entropy": [[0.5218733549118042, 1.2739293575286865, 0.9273990392684937, 0.045247066766023636, 2.195021629333496, 1.2142455577850342, 0.09338682144880295, 0.0012185481609776616, 1.943688154220581, 0.18042941391468048, 1.0277974605560303, 0.8162925243377686, 0.058813974261283875, 2.692286491394043, 1.696317195892334, 0.20588508248329163, 0.009313292801380157, 1.1635119915008545, 0.17514993250370026, 1.2130030393600464, 0.1506083607673645, 0.06497153639793396, 0.6516363620758057, 0.5748021602630615, 0.1828921139240265, 0.025679808109998703, 1.046161413192749, 0.18531814217567444, 0.8925252556800842, 0.7309301495552063, 0.2875182628631592, 1.2664645910263062, 0.3882630467414856, 1.1833336353302002, 0.25131988525390625, 1.313429832458496, 1.0969841480255127, 0.8281129002571106, 1.422399878501892, 0.2400979995727539, 0.428735613822937, 1.958756685256958, 1.3752667903900146, 0.7531666159629822, 0.08226536214351654, 1.301893711090088, 1.9001717567443848, 0.07770448923110962, 0.5619127750396729]], "avg_entropy": [0.7893496573527287], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "100\u00a0\u00b0C", "score_of_biggest_cluster": 0.46983909606933594, "score_of_first_answer": 0.46983909606933594, "max_score_of_first_answer": 0.9999055862426758, "min_score_of_first_answer": 5.461412365548313e-05, "avg_score": 0.46983909606933594, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3144453060694075, "std_of_first_answer": 0.3144453060694075, "avg_entropy_of_first_answer": 0.7893496573527287, "lex_sim": 1.0}
{"question": "when did god save the queen became the national anthem", "answer": ["1977"], "scores": [[0.5104148387908936, 0.9217572808265686, 0.890595018863678, 0.3178237974643707, 0.6498797535896301]], "normalized_score": [0.6129822731018066], "tokens": [["\u2581", "1", "9", "7", "7"]], "entropy": [[2.755734920501709, 0.34088027477264404, 0.5661343336105347, 1.7036173343658447, 1.1990549564361572]], "avg_entropy": [1.313084363937378], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in the 1780s and 1790s", "score_of_biggest_cluster": 0.6129822731018066, "score_of_first_answer": 0.6129822731018066, "max_score_of_first_answer": 0.9217572808265686, "min_score_of_first_answer": 0.3178237974643707, "avg_score": 0.6129822731018066, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2285762317748444, "std_of_first_answer": 0.2285762317748444, "avg_entropy_of_first_answer": 1.313084363937378, "lex_sim": 1.0}
{"question": "who does the voice of the elephant in sing", "answer": ["Tori Kelly"], "scores": [[0.5205770134925842, 0.8920817971229553, 0.9934043288230896]], "normalized_score": [0.7726898789405823], "tokens": [["\u2581T", "ori", "\u2581Kelly"]], "entropy": [[2.775362014770508, 0.5186401605606079, 0.06273499876260757]], "avg_entropy": [1.1189123913645744], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Tori Kelly", "score_of_biggest_cluster": 0.7726898789405823, "score_of_first_answer": 0.7726898789405823, "max_score_of_first_answer": 0.9934043288230896, "min_score_of_first_answer": 0.5205770134925842, "avg_score": 0.7726898789405823, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2032643953664036, "std_of_first_answer": 0.2032643953664036, "avg_entropy_of_first_answer": 1.1189123913645744, "lex_sim": 1.0}
{"question": "what does sip stand for in the ngn / ims context", "answer": ["Session Initiation Protocol"], "scores": [[0.6293815970420837, 0.9704135656356812, 0.998336136341095, 0.9898285865783691, 0.9957316517829895, 0.9968929290771484]], "normalized_score": [0.9181559085845947], "tokens": [["\u2581Session", "\u2581In", "iti", "ation", "\u2581Pro", "tocol"]], "entropy": [[1.0774109363555908, 0.2022988200187683, 0.01773880422115326, 0.06643127650022507, 0.03769570216536522, 0.029050316661596298]], "avg_entropy": [0.23843764265378317], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Session Initiation Protocol", "score_of_biggest_cluster": 0.9181559085845947, "score_of_first_answer": 0.9181559085845947, "max_score_of_first_answer": 0.998336136341095, "min_score_of_first_answer": 0.6293815970420837, "avg_score": 0.9181559085845947, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.13481420848677, "std_of_first_answer": 0.13481420848677, "avg_entropy_of_first_answer": 0.23843764265378317, "lex_sim": 1.0}
{"question": "who played joey's agent estelle on friends", "answer": ["Maggie Wheeler"], "scores": [[0.09520730376243591, 0.9934197068214417, 0.9986314177513123, 0.9957245588302612]], "normalized_score": [0.5537794828414917], "tokens": [["\u2581Mag", "gie", "\u2581Whe", "eler"]], "entropy": [[3.1807522773742676, 0.04542431980371475, 0.01315198466181755, 0.04150902479887009]], "avg_entropy": [0.8202094016596675], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "June Gable", "score_of_biggest_cluster": 0.5537794828414917, "score_of_first_answer": 0.5537794828414917, "max_score_of_first_answer": 0.9986314177513123, "min_score_of_first_answer": 0.09520730376243591, "avg_score": 0.5537794828414917, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3900266738893393, "std_of_first_answer": 0.3900266738893393, "avg_entropy_of_first_answer": 0.8202094016596675, "lex_sim": 1.0}
{"question": "how many stores does lowes have in canada", "answer": ["600"], "scores": [[0.8926728963851929, 0.15167978405952454, 0.41563841700553894, 0.10071362555027008]], "normalized_score": [0.27438223361968994], "tokens": [["\u2581", "6", "0", "0"]], "entropy": [[0.8417520523071289, 1.9493448734283447, 2.0844566822052, 1.7200639247894287]], "avg_entropy": [1.6489043831825256], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "62", "score_of_biggest_cluster": 0.27438223361968994, "score_of_first_answer": 0.27438223361968994, "max_score_of_first_answer": 0.8926728963851929, "min_score_of_first_answer": 0.10071362555027008, "avg_score": 0.27438223361968994, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3137755474843387, "std_of_first_answer": 0.3137755474843387, "avg_entropy_of_first_answer": 1.6489043831825256, "lex_sim": 1.0}
{"question": "when did all night long come out lionel richie", "answer": ["October 28, 1983"], "scores": [[0.42287471890449524, 0.9849154949188232, 0.09566762298345566, 0.06445576995611191, 0.9849193096160889, 0.9967350363731384, 0.8151034116744995, 0.9997058510780334, 0.9706923961639404, 0.9335941672325134]], "normalized_score": [0.5333278775215149], "tokens": [["\u2581October", "\u2581", "2", "8", ",", "\u2581", "1", "9", "8", "3"]], "entropy": [[2.1412529945373535, 0.09096190333366394, 0.8725154399871826, 2.3779211044311523, 0.10284880548715591, 0.025814007967710495, 0.4803534150123596, 0.003207787638530135, 0.16808916628360748, 0.32353466749191284]], "avg_entropy": [0.6586499292170629], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "1983", "score_of_biggest_cluster": 0.5333278775215149, "score_of_first_answer": 0.5333278775215149, "max_score_of_first_answer": 0.9997058510780334, "min_score_of_first_answer": 0.06445576995611191, "avg_score": 0.5333278775215149, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3633228624748689, "std_of_first_answer": 0.3633228624748689, "avg_entropy_of_first_answer": 0.6586499292170629, "lex_sim": 1.0}
{"question": "when was the day that the pubs ran dry in the poem 9-3", "answer": ["16th or 18th century"], "scores": [[0.2246481031179428, 0.5221930146217346, 0.11156807094812393, 0.25645318627357483, 0.000836408871691674, 0.8610394597053528, 0.8882033228874207, 0.07782978564500809, 0.9658668041229248, 0.22022011876106262]], "normalized_score": [0.17991843819618225], "tokens": [["\u2581", "1", "6", "th", "\u2581or", "\u2581", "1", "8", "th", "\u2581century"]], "entropy": [[3.5764660835266113, 1.5708210468292236, 1.9403153657913208, 2.4504032135009766, 2.037015914916992, 0.756859302520752, 0.48980197310447693, 0.734217643737793, 0.27240368723869324, 2.524486541748047]], "avg_entropy": [1.6352790772914887], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "31 October 1972", "score_of_biggest_cluster": 0.17991843819618225, "score_of_first_answer": 0.17991843819618225, "max_score_of_first_answer": 0.9658668041229248, "min_score_of_first_answer": 0.000836408871691674, "avg_score": 0.17991843819618225, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34847794386129166, "std_of_first_answer": 0.34847794386129166, "avg_entropy_of_first_answer": 1.6352790772914887, "lex_sim": 1.0}
{"question": "when did india win their first cricket match", "answer": ["June 25, 1975"], "scores": [[0.5170260667800903, 0.9954082369804382, 0.9619724154472351, 0.9600737690925598, 0.9750939011573792, 0.9970989227294922, 0.9944796562194824, 0.9899033308029175, 0.760101854801178, 0.165152907371521]], "normalized_score": [0.7510541081428528], "tokens": [["\u2581June", "\u2581", "2", "5", ",", "\u2581", "1", "9", "7", "5"]], "entropy": [[2.0913197994232178, 0.036417726427316666, 0.20833458006381989, 0.25179022550582886, 0.15895560383796692, 0.021301524713635445, 0.034594375640153885, 0.057043418288230896, 0.6913051009178162, 0.7539712190628052]], "avg_entropy": [0.43050335738807916], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1952", "score_of_biggest_cluster": 0.7510541081428528, "score_of_first_answer": 0.7510541081428528, "max_score_of_first_answer": 0.9970989227294922, "min_score_of_first_answer": 0.165152907371521, "avg_score": 0.7510541081428528, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2661064066176997, "std_of_first_answer": 0.2661064066176997, "avg_entropy_of_first_answer": 0.43050335738807916, "lex_sim": 1.0}
{"question": "what was the purpose of the mall of america", "answer": ["The mall was built to be the biggest shopping mall in the world in order to attract more consumers into the Twin Cities area, and to compete with other entertainment facilities in nearby downtown area."], "scores": [[0.091654472053051, 0.11744779348373413, 0.9965490102767944, 0.307087779045105, 0.48143792152404785, 0.4904458820819855, 0.2997893989086151, 0.8167024254798889, 0.07242512702941895, 0.2580552101135254, 0.999843955039978, 0.6582939624786377, 0.9996905326843262, 0.9052345752716064, 0.900946855545044, 0.387050598859787, 0.04194445163011551, 0.56967693567276, 0.9787476062774658, 0.056448664516210556, 0.8549573421478271, 0.019041938707232475, 0.9995328187942505, 0.05906106159090996, 0.5242409706115723, 0.23636583983898163, 0.9998819828033447, 0.9726381301879883, 0.9964001178741455, 0.5853550434112549, 0.35239046812057495, 0.39268919825553894, 0.7940394878387451, 0.6917515397071838, 0.9987737536430359, 0.8439001441001892, 0.3206288814544678, 0.08050127327442169, 0.9772467613220215, 0.03978554159402847, 0.427998811006546, 0.2449360340833664, 0.021232856437563896, 0.999995231628418, 0.999923586845398, 0.002932751551270485, 0.40096908807754517, 0.0022801903542131186, 0.5170072913169861, 0.22664128243923187]], "normalized_score": [0.2947390675544739], "tokens": [["\u2581The", "\u2581m", "all", "\u2581was", "\u2581built", "\u2581to", "\u2581be", "\u2581the", "\u2581biggest", "\u2581sho", "pping", "\u2581m", "all", "\u2581in", "\u2581the", "\u2581world", "\u2581in", "\u2581order", "\u2581to", "\u2581attract", "\u2581more", "\u2581consum", "ers", "\u2581into", "\u2581the", "\u2581T", "win", "\u2581C", "ities", "\u2581area", ",", "\u2581and", "\u2581to", "\u2581comp", "ete", "\u2581with", "\u2581other", "\u2581entertain", "ment", "\u2581facilities", "\u2581in", "\u2581nearby", "\u2581dow", "nt", "own", "\u2581area", ".", "\u2581Also", ",", "\u2581it"]], "entropy": [[3.9035019874572754, 2.107388973236084, 0.03169036656618118, 2.554629325866699, 2.072359085083008, 1.982314109802246, 2.689828872680664, 0.7884762287139893, 2.2102861404418945, 1.5867037773132324, 0.0017338052857667208, 1.1810507774353027, 0.0032508699223399162, 0.5917748212814331, 0.5605989694595337, 1.8262290954589844, 1.808595061302185, 1.4936155080795288, 0.13051211833953857, 1.4861279726028442, 0.7247880101203918, 1.6679067611694336, 0.005889059975743294, 1.860487937927246, 1.8132938146591187, 2.6428632736206055, 0.001595005625858903, 0.1701570302248001, 0.02499176375567913, 1.7594678401947021, 1.6893242597579956, 2.864734172821045, 0.8579753637313843, 1.6770726442337036, 0.011751835234463215, 0.5596247315406799, 2.385678768157959, 2.3947505950927734, 0.1539154201745987, 1.81773042678833, 1.9144450426101685, 1.8258576393127441, 0.8626264333724976, 7.512667798437178e-05, 0.001004643738269806, 1.082169532775879, 2.0436184406280518, 1.1019328832626343, 2.4727706909179688, 2.5076792240142822]], "avg_entropy": [1.3581369247689146], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "shopping", "score_of_biggest_cluster": 0.2947390675544739, "score_of_first_answer": 0.2947390675544739, "max_score_of_first_answer": 0.999995231628418, "min_score_of_first_answer": 0.0022801903542131186, "avg_score": 0.2947390675544739, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36359575626826984, "std_of_first_answer": 0.36359575626826984, "avg_entropy_of_first_answer": 1.3581369247689146, "lex_sim": 1.0}
{"question": "what was tom hanks character name in castaway", "answer": ["Chuck Noland"], "scores": [[0.9214395880699158, 0.9927549958229065, 0.9621998071670532, 0.9774012565612793, 0.9989721775054932]], "normalized_score": [0.970152735710144], "tokens": [["\u2581Ch", "uck", "\u2581N", "ol", "and"]], "entropy": [[0.6352963447570801, 0.07651390135288239, 0.20423226058483124, 0.1165817528963089, 0.012423996813595295]], "avg_entropy": [0.20900965128093957], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Chuck Noland", "score_of_biggest_cluster": 0.970152735710144, "score_of_first_answer": 0.970152735710144, "max_score_of_first_answer": 0.9989721775054932, "min_score_of_first_answer": 0.9214395880699158, "avg_score": 0.970152735710144, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.027674464572455437, "std_of_first_answer": 0.027674464572455437, "avg_entropy_of_first_answer": 0.20900965128093957, "lex_sim": 1.0}
{"question": "who was elected the president of constituent assembly", "answer": ["Dr. Rajendra Prasad"], "scores": [[0.40065521001815796, 0.844736635684967, 0.8602392077445984, 0.9946025013923645, 0.9937206506729126, 0.9969213008880615, 0.998600423336029]], "normalized_score": [0.8364494442939758], "tokens": [["\u2581Dr", ".", "\u2581Raj", "endra", "\u2581Pr", "as", "ad"]], "entropy": [[2.6267194747924805, 0.6000844836235046, 0.8502470254898071, 0.04389939084649086, 0.05334248021245003, 0.02557092159986496, 0.01457537803798914]], "avg_entropy": [0.6020627363717982], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Rajendra Prasad", "score_of_biggest_cluster": 0.8364494442939758, "score_of_first_answer": 0.8364494442939758, "max_score_of_first_answer": 0.998600423336029, "min_score_of_first_answer": 0.40065521001815796, "avg_score": 0.8364494442939758, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.20159991197095953, "std_of_first_answer": 0.20159991197095953, "avg_entropy_of_first_answer": 0.6020627363717982, "lex_sim": 1.0}
{"question": "who named all the animals in the world", "answer": ["Yahweh"], "scores": [[0.0041223871521651745, 0.7269716858863831, 0.9346715211868286, 0.9896089434623718]], "normalized_score": [0.22945483028888702], "tokens": [["\u2581Y", "ah", "we", "h"]], "entropy": [[2.3120129108428955, 1.0006670951843262, 0.4415687620639801, 0.08107812702655792]], "avg_entropy": [0.9588317237794399], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Adam", "score_of_biggest_cluster": 0.22945483028888702, "score_of_first_answer": 0.22945483028888702, "max_score_of_first_answer": 0.9896089434623718, "min_score_of_first_answer": 0.0041223871521651745, "avg_score": 0.22945483028888702, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39328363041341685, "std_of_first_answer": 0.39328363041341685, "avg_entropy_of_first_answer": 0.9588317237794399, "lex_sim": 1.0}
{"question": "where is the extensor pollicis longus tendon located", "answer": ["The muscle originates on the lateral epicondyle of the distal humerus."], "scores": [[0.051474954932928085, 0.03826120123267174, 0.9248967170715332, 0.28152111172676086, 0.9786990880966187, 0.33603423833847046, 0.9706022143363953, 0.24461296200752258, 0.999778687953949, 0.9835917949676514, 0.9998227953910828, 0.9995438456535339, 0.9962480664253235, 0.9283812642097473, 0.9884684681892395, 0.007060487754642963, 0.9996588230133057, 0.9029579758644104, 0.9884939789772034, 0.9990524649620056, 0.2884059250354767]], "normalized_score": [0.45483511686325073], "tokens": [["\u2581The", "\u2581mus", "cle", "\u2581origin", "ates", "\u2581on", "\u2581the", "\u2581later", "al", "\u2581ep", "ic", "ond", "yle", "\u2581of", "\u2581the", "\u2581dist", "al", "\u2581hum", "er", "us", "."]], "entropy": [[4.711800575256348, 2.8395771980285645, 0.34573841094970703, 2.9271275997161865, 0.14880946278572083, 1.617369532585144, 0.22514215111732483, 3.1127262115478516, 0.0029449942521750927, 0.12874239683151245, 0.002254629973322153, 0.005958292633295059, 0.02617376297712326, 0.40156835317611694, 0.0798482745885849, 0.37104445695877075, 0.00442368071526289, 0.5228850841522217, 0.08233441412448883, 0.010305922478437424, 1.934340238571167]], "avg_entropy": [0.9286245544485393], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "dorsally on the forearm", "score_of_biggest_cluster": 0.45483511686325073, "score_of_first_answer": 0.45483511686325073, "max_score_of_first_answer": 0.9998227953910828, "min_score_of_first_answer": 0.007060487754642963, "avg_score": 0.45483511686325073, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3841596737121663, "std_of_first_answer": 0.3841596737121663, "avg_entropy_of_first_answer": 0.9286245544485393, "lex_sim": 1.0}
{"question": "who has majority in the house and senate", "answer": ["Republicans"], "scores": [[0.1390160769224167, 0.9957966804504395]], "normalized_score": [0.37206417322158813], "tokens": [["\u2581Republic", "ans"]], "entropy": [[2.8441431522369385, 0.040886685252189636]], "avg_entropy": [1.442514918744564], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Republican", "score_of_biggest_cluster": 0.37206417322158813, "score_of_first_answer": 0.37206417322158813, "max_score_of_first_answer": 0.9957966804504395, "min_score_of_first_answer": 0.1390160769224167, "avg_score": 0.37206417322158813, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4283903017640114, "std_of_first_answer": 0.4283903017640114, "avg_entropy_of_first_answer": 1.442514918744564, "lex_sim": 1.0}
{"question": "the gulf stream the world's fastest ocean current flows along the western side of this water body", "answer": ["Atlantic Ocean"], "scores": [[0.26312872767448425, 0.8525750041007996]], "normalized_score": [0.4736422598361969], "tokens": [["\u2581Atlantic", "\u2581Ocean"]], "entropy": [[2.756159543991089, 0.6055971384048462]], "avg_entropy": [1.6808783411979675], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Atlantic ocean", "score_of_biggest_cluster": 0.4736422598361969, "score_of_first_answer": 0.4736422598361969, "max_score_of_first_answer": 0.8525750041007996, "min_score_of_first_answer": 0.26312872767448425, "avg_score": 0.4736422598361969, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.29472313821315765, "std_of_first_answer": 0.29472313821315765, "avg_entropy_of_first_answer": 1.6808783411979675, "lex_sim": 1.0}
{"question": "what is the written dialogue portion of a musical called", "answer": ["libretto"], "scores": [[0.7117106318473816, 0.9936768412590027]], "normalized_score": [0.8409579992294312], "tokens": [["\u2581libre", "tto"]], "entropy": [[1.4235548973083496, 0.0402706116437912]], "avg_entropy": [0.7319127544760704], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "script", "score_of_biggest_cluster": 0.8409579992294312, "score_of_first_answer": 0.8409579992294312, "max_score_of_first_answer": 0.9936768412590027, "min_score_of_first_answer": 0.7117106318473816, "avg_score": 0.8409579992294312, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.14098310470581055, "std_of_first_answer": 0.14098310470581055, "avg_entropy_of_first_answer": 0.7319127544760704, "lex_sim": 1.0}
{"question": "natural resources can be divided into what two categories", "answer": ["renewable or nonrenewable"], "scores": [[0.7814503908157349, 0.9940126538276672, 0.02254474349319935, 0.9605415463447571, 0.569718062877655, 0.9985499978065491, 0.9985365867614746]], "normalized_score": [0.514593243598938], "tokens": [["\u2581renew", "able", "\u2581or", "\u2581non", "ren", "ew", "able"]], "entropy": [[1.4165923595428467, 0.04576161503791809, 1.0350089073181152, 0.3074209690093994, 0.8959532380104065, 0.014568573795258999, 0.014644606038928032]], "avg_entropy": [0.5328500383932676], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Biotic", "score_of_biggest_cluster": 0.514593243598938, "score_of_first_answer": 0.514593243598938, "max_score_of_first_answer": 0.9985499978065491, "min_score_of_first_answer": 0.02254474349319935, "avg_score": 0.514593243598938, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3357907796386501, "std_of_first_answer": 0.3357907796386501, "avg_entropy_of_first_answer": 0.5328500383932676, "lex_sim": 1.0}
{"question": "is jaffa cakes a biscuit or a cake", "answer": ["cake"], "scores": [[0.25492972135543823, 0.9623426198959351]], "normalized_score": [0.49530768394470215], "tokens": [["\u2581c", "ake"]], "entropy": [[3.1604819297790527, 0.1688699722290039]], "avg_entropy": [1.6646759510040283], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Cake", "score_of_biggest_cluster": 0.49530768394470215, "score_of_first_answer": 0.49530768394470215, "max_score_of_first_answer": 0.9623426198959351, "min_score_of_first_answer": 0.25492972135543823, "avg_score": 0.49530768394470215, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3537064492702484, "std_of_first_answer": 0.3537064492702484, "avg_entropy_of_first_answer": 1.6646759510040283, "lex_sim": 1.0}
{"question": "new movie of ajay devgan and sonakshi sinha", "answer": ["Singham"], "scores": [[0.041068002581596375, 0.9907304644584656]], "normalized_score": [0.20171098411083221], "tokens": [["\u2581Sing", "ham"]], "entropy": [[4.610734939575195, 0.07750605046749115]], "avg_entropy": [2.3441204950213432], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Action Jackson", "score_of_biggest_cluster": 0.20171098411083221, "score_of_first_answer": 0.20171098411083221, "max_score_of_first_answer": 0.9907304644584656, "min_score_of_first_answer": 0.041068002581596375, "avg_score": 0.20171098411083221, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4748312309384346, "std_of_first_answer": 0.4748312309384346, "avg_entropy_of_first_answer": 2.3441204950213432, "lex_sim": 1.0}
{"question": "how many grams of alcohol in one beer", "answer": ["A 12 oz can of Coors has 3.5 grams"], "scores": [[0.0035008324775844812, 0.16002234816551208, 0.3976386487483978, 0.9114905595779419, 0.23202826082706451, 0.9997500777244568, 0.07580415159463882, 0.8240305781364441, 0.02652919664978981, 0.9891246557235718, 0.09043154120445251, 0.7565826177597046, 0.028543520718812943, 0.3768765330314636, 0.15337097644805908, 0.21819251775741577, 0.995537519454956]], "normalized_score": [0.20437191426753998], "tokens": [["\u2581A", "\u2581", "1", "2", "\u2581o", "z", "\u2581can", "\u2581of", "\u2581Co", "ors", "\u2581has", "\u2581", "3", ".", "5", "\u2581gr", "ams"]], "entropy": [[1.1343482732772827, 3.2664432525634766, 1.312395691871643, 0.5198746919631958, 1.6384856700897217, 0.003191789612174034, 1.903380274772644, 0.9439010620117188, 1.6103925704956055, 0.07442167401313782, 2.332561731338501, 1.1207327842712402, 0.8931048512458801, 2.314819812774658, 2.1495361328125, 2.6211018562316895, 0.036988984793424606]], "avg_entropy": [1.4044518296552055], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "14", "score_of_biggest_cluster": 0.20437191426753998, "score_of_first_answer": 0.20437191426753998, "max_score_of_first_answer": 0.9997500777244568, "min_score_of_first_answer": 0.0035008324775844812, "avg_score": 0.20437191426753998, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37842397896185925, "std_of_first_answer": 0.37842397896185925, "avg_entropy_of_first_answer": 1.4044518296552055, "lex_sim": 1.0}
{"question": "who were the bands in pitch perfect 3", "answer": ["Caltech Maidens, CU Deep Tones and Bellas"], "scores": [[0.0020376390311867, 0.01645139791071415, 0.05170315504074097, 0.39743608236312866, 0.9980682730674744, 0.811955451965332, 0.0150319067761302, 0.0708300992846489, 0.00577558483928442, 0.20162168145179749, 0.9903544187545776, 0.039133746176958084, 0.21562112867832184, 0.9983525276184082]], "normalized_score": [0.09615316987037659], "tokens": [["\u2581Cal", "tech", "\u2581Ma", "id", "ens", ",", "\u2581C", "U", "\u2581Deep", "\u2581T", "ones", "\u2581and", "\u2581Bell", "as"]], "entropy": [[3.217409133911133, 3.7664101123809814, 4.4272003173828125, 2.673764705657959, 0.02321816422045231, 1.0098339319229126, 3.3556296825408936, 3.9767565727233887, 4.952507495880127, 2.7085862159729004, 0.08757565915584564, 0.5770175457000732, 3.3038127422332764, 0.02056461200118065]], "avg_entropy": [2.435734777977424], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Evermoist", "score_of_biggest_cluster": 0.09615316987037659, "score_of_first_answer": 0.09615316987037659, "max_score_of_first_answer": 0.9983525276184082, "min_score_of_first_answer": 0.0020376390311867, "avg_score": 0.09615316987037659, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3993022127327399, "std_of_first_answer": 0.3993022127327399, "avg_entropy_of_first_answer": 2.435734777977424, "lex_sim": 1.0}
{"question": "who are the permanent member of un security council", "answer": ["permanent five (P5): China"], "scores": [[0.00652589974924922, 0.1643826961517334, 0.07731617242097855, 0.35306522250175476, 0.9366632699966431, 0.24024434387683868, 0.5845564603805542]], "normalized_score": [0.16847150027751923], "tokens": [["\u2581permanent", "\u2581five", "\u2581(", "P", "5", "):", "\u2581China"]], "entropy": [[2.12673282623291, 1.180451512336731, 2.1740784645080566, 2.768768548965454, 0.2978694438934326, 0.9214262962341309, 1.760378360748291]], "avg_entropy": [1.6042436361312866], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "China", "score_of_biggest_cluster": 0.16847150027751923, "score_of_first_answer": 0.16847150027751923, "max_score_of_first_answer": 0.9366632699966431, "min_score_of_first_answer": 0.00652589974924922, "avg_score": 0.16847150027751923, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3016486907383622, "std_of_first_answer": 0.3016486907383622, "avg_entropy_of_first_answer": 1.6042436361312866, "lex_sim": 1.0}
{"question": "where does the movie road to perdition take place", "answer": ["1947 New York City"], "scores": [[0.032935671508312225, 0.8723588585853577, 0.979176938533783, 0.0728338286280632, 0.02601655200123787, 0.04235402122139931, 0.27618372440338135, 0.3569754362106323]], "normalized_score": [0.14738120138645172], "tokens": [["\u2581", "1", "9", "4", "7", "\u2581New", "\u2581York", "\u2581City"]], "entropy": [[3.18625545501709, 0.6056137681007385, 0.17441897094249725, 1.4011240005493164, 1.2833994626998901, 2.809401750564575, 0.9515292644500732, 1.8687922954559326]], "avg_entropy": [1.5350668709725142], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rock Island, Illinois", "score_of_biggest_cluster": 0.14738120138645172, "score_of_first_answer": 0.14738120138645172, "max_score_of_first_answer": 0.979176938533783, "min_score_of_first_answer": 0.02601655200123787, "avg_score": 0.14738120138645172, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3620585502742365, "std_of_first_answer": 0.3620585502742365, "avg_entropy_of_first_answer": 1.5350668709725142, "lex_sim": 1.0}
{"question": "when did the phantom of the opera open", "answer": ["June 23, 1988"], "scores": [[0.06400370597839355, 0.9936188459396362, 0.45356786251068115, 0.09693031013011932, 0.9700128436088562, 0.9974996447563171, 0.8789147138595581, 0.9983609318733215, 0.9639920592308044, 0.7510914206504822]], "normalized_score": [0.5290805101394653], "tokens": [["\u2581June", "\u2581", "2", "3", ",", "\u2581", "1", "9", "8", "8"]], "entropy": [[2.4483182430267334, 0.04916966333985329, 1.3222880363464355, 2.1488049030303955, 0.18074838817119598, 0.02158299833536148, 0.3714695870876312, 0.013309377245604992, 0.20688968896865845, 0.8243380784988403]], "avg_entropy": [0.7586918964050711], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "October 1986:", "score_of_biggest_cluster": 0.5290805101394653, "score_of_first_answer": 0.5290805101394653, "max_score_of_first_answer": 0.9983609318733215, "min_score_of_first_answer": 0.06400370597839355, "avg_score": 0.5290805101394653, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3559561566815979, "std_of_first_answer": 0.3559561566815979, "avg_entropy_of_first_answer": 0.7586918964050711, "lex_sim": 1.0}
{"question": "what was the most important new religious figure of the new kingdom of ancient egypt", "answer": ["Amun"], "scores": [[0.5410803556442261, 0.9437292218208313]], "normalized_score": [0.7145861387252808], "tokens": [["\u2581Am", "un"]], "entropy": [[1.7884509563446045, 0.2617233991622925]], "avg_entropy": [1.0250871777534485], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Aten", "score_of_biggest_cluster": 0.7145861387252808, "score_of_first_answer": 0.7145861387252808, "max_score_of_first_answer": 0.9437292218208313, "min_score_of_first_answer": 0.5410803556442261, "avg_score": 0.7145861387252808, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2013244330883026, "std_of_first_answer": 0.2013244330883026, "avg_entropy_of_first_answer": 1.0250871777534485, "lex_sim": 1.0}
{"question": "when was i can only imagine the song released", "answer": ["September 8, 2003"], "scores": [[0.1921718269586563, 0.988243818283081, 0.028140604496002197, 0.9774672389030457, 0.9978540539741516, 0.8884637355804443, 0.999646782875061, 0.35526496171951294, 0.45147302746772766]], "normalized_score": [0.44905856251716614], "tokens": [["\u2581September", "\u2581", "8", ",", "\u2581", "2", "0", "0", "3"]], "entropy": [[2.781899929046631, 0.07809120416641235, 1.4763180017471313, 0.14234700798988342, 0.02029314823448658, 0.35259371995925903, 0.003951674327254295, 0.6634780764579773, 1.6785260438919067]], "avg_entropy": [0.7997220895356603], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1999", "score_of_biggest_cluster": 0.44905856251716614, "score_of_first_answer": 0.44905856251716614, "max_score_of_first_answer": 0.999646782875061, "min_score_of_first_answer": 0.028140604496002197, "avg_score": 0.44905856251716614, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37182279233233745, "std_of_first_answer": 0.37182279233233745, "avg_entropy_of_first_answer": 0.7997220895356603, "lex_sim": 1.0}
{"question": "who sang the most number of songs in the world", "answer": ["Rolf Harris"], "scores": [[0.009359275922179222, 0.03971195966005325, 0.8384623527526855]], "normalized_score": [0.06779783219099045], "tokens": [["\u2581R", "olf", "\u2581Harris"]], "entropy": [[4.246331214904785, 3.703650951385498, 1.1553115844726562]], "avg_entropy": [3.03509791692098], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Asha Bhosle", "score_of_biggest_cluster": 0.06779783219099045, "score_of_first_answer": 0.06779783219099045, "max_score_of_first_answer": 0.8384623527526855, "min_score_of_first_answer": 0.009359275922179222, "avg_score": 0.06779783219099045, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3838887842252606, "std_of_first_answer": 0.3838887842252606, "avg_entropy_of_first_answer": 3.03509791692098, "lex_sim": 1.0}
{"question": "when was the young and the restless first aired", "answer": ["March 26, 1973"], "scores": [[0.9586555361747742, 0.9983837604522705, 0.9892221689224243, 0.9524707198143005, 0.9888330698013306, 0.9977864027023315, 0.9991474151611328, 0.9999039173126221, 0.9973300695419312, 0.9993008375167847]], "normalized_score": [0.9879589676856995], "tokens": [["\u2581March", "\u2581", "2", "6", ",", "\u2581", "1", "9", "7", "3"]], "entropy": [[0.3076641261577606, 0.01496809720993042, 0.06934472918510437, 0.23828056454658508, 0.0761314332485199, 0.016843823716044426, 0.007761313579976559, 0.0012646589893847704, 0.020774226635694504, 0.007417858112603426]], "avg_entropy": [0.07604508313816041], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "March 26, 1973", "score_of_biggest_cluster": 0.9879589676856995, "score_of_first_answer": 0.9879589676856995, "max_score_of_first_answer": 0.9999039173126221, "min_score_of_first_answer": 0.9524707198143005, "avg_score": 0.9879589676856995, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.01676255205173179, "std_of_first_answer": 0.01676255205173179, "avg_entropy_of_first_answer": 0.07604508313816041, "lex_sim": 1.0}
{"question": "what does istj mean in a personality test", "answer": ["Introverted Sensing Thinking Judging type."], "scores": [[0.33429986238479614, 0.9984725117683411, 0.7624820470809937, 0.8506261706352234, 0.7117621302604675, 0.9978790283203125, 0.9984731078147888, 0.9588306546211243, 0.9998921155929565, 0.7100372910499573, 0.8802008628845215, 0.001443434040993452, 0.08510348945856094]], "normalized_score": [0.41635963320732117], "tokens": [["\u2581Int", "ro", "vert", "ed", "\u2581S", "ens", "ing", "\u2581Th", "inking", "\u2581Jud", "ging", "\u2581type", "."]], "entropy": [[3.9075398445129395, 0.013639913871884346, 0.5832672715187073, 0.6558061838150024, 1.0038485527038574, 0.0166937205940485, 0.014498351141810417, 0.27565303444862366, 0.0012798320967704058, 0.8093611001968384, 0.4581526517868042, 0.6182441115379333, 1.9666002988815308]], "avg_entropy": [0.7941988359312885], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Extroverted Thinking (Te)", "score_of_biggest_cluster": 0.41635963320732117, "score_of_first_answer": 0.41635963320732117, "max_score_of_first_answer": 0.9998921155929565, "min_score_of_first_answer": 0.001443434040993452, "avg_score": 0.41635963320732117, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33743254772132014, "std_of_first_answer": 0.33743254772132014, "avg_entropy_of_first_answer": 0.7941988359312885, "lex_sim": 1.0}
{"question": "how long do nhl players stay on waivers", "answer": ["All players signed to an NHL team must first go through waivers, a 24-hour period in which clubs can claim players for half or less of their remaining contract, depending on their status."], "scores": [[0.002211308805271983, 0.4458295702934265, 0.0023668089415878057, 0.5731266140937805, 0.35163795948028564, 0.8199986219406128, 0.9998791217803955, 0.16966627538204193, 0.212091863155365, 0.0984945073723793, 0.14569242298603058, 0.8788606524467468, 0.611610472202301, 0.9998002648353577, 0.11854717880487442, 0.11977824568748474, 0.11816970258951187, 0.8795560002326965, 0.9911518692970276, 0.9428051710128784, 0.9867355823516846, 0.7823665738105774, 0.2978222966194153, 0.9954267144203186, 0.0022625711280852556, 0.42389288544654846, 0.5408955216407776, 0.1325775533914566, 0.1482560932636261, 0.06472119688987732, 0.004545712377876043, 0.29980650544166565, 0.8435913920402527, 0.702570915222168, 0.08315445482730865, 0.5806618332862854, 0.07076112926006317, 0.06982474029064178, 0.9587142467498779, 0.19465601444244385, 0.15686827898025513, 0.6614455580711365, 0.06547665596008301, 0.995503842830658, 0.11314921826124191, 0.04977646842598915, 0.4620651602745056, 0.9999629259109497, 0.055872321128845215, 0.465351402759552]], "normalized_score": [0.2144310027360916], "tokens": [["\u2581All", "\u2581players", "\u2581signed", "\u2581to", "\u2581an", "\u2581N", "HL", "\u2581team", "\u2581must", "\u2581first", "\u2581go", "\u2581through", "\u2581wa", "ivers", ",", "\u2581a", "\u2581", "2", "4", "-", "hour", "\u2581period", "\u2581in", "\u2581which", "\u2581clubs", "\u2581can", "\u2581claim", "\u2581players", "\u2581for", "\u2581half", "\u2581or", "\u2581less", "\u2581of", "\u2581their", "\u2581remaining", "\u2581contract", ",", "\u2581depending", "\u2581on", "\u2581their", "\u2581status", ".", "\u2581Play", "ers", "\u2581on", "\u2581an", "\u2581N", "HL", "\u2581club", "'"]], "entropy": [[1.8234882354736328, 3.0294618606567383, 2.9183313846588135, 1.9318910837173462, 2.132190465927124, 0.8111981153488159, 0.001612944295629859, 1.94429612159729, 3.155261516571045, 2.140604257583618, 1.7628239393234253, 0.4947938024997711, 1.2620410919189453, 0.0022515468299388885, 2.451608180999756, 3.4627552032470703, 2.149650812149048, 0.4847901463508606, 0.06741992384195328, 0.27255111932754517, 0.10195847600698471, 1.1174657344818115, 1.8590490818023682, 0.04256915673613548, 2.4471793174743652, 2.275308609008789, 2.403160810470581, 2.379044532775879, 2.9681925773620605, 2.0276947021484375, 1.3185992240905762, 2.502633810043335, 0.6181866526603699, 0.9638246893882751, 2.2476353645324707, 1.3201932907104492, 2.1677277088165283, 2.9161572456359863, 0.23616915941238403, 2.2905774116516113, 3.319098711013794, 1.443023443222046, 2.7842345237731934, 0.03522341698408127, 2.8765087127685547, 1.9503238201141357, 1.463509440422058, 0.0005536128883250058, 2.204289674758911, 1.3045545816421509]], "avg_entropy": [1.6776733849023002], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "24 hours", "score_of_biggest_cluster": 0.2144310027360916, "score_of_first_answer": 0.2144310027360916, "max_score_of_first_answer": 0.9999629259109497, "min_score_of_first_answer": 0.002211308805271983, "avg_score": 0.2144310027360916, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.36180746944404635, "std_of_first_answer": 0.36180746944404635, "avg_entropy_of_first_answer": 1.6776733849023002, "lex_sim": 1.0}
{"question": "who are nominated for president of india 2017", "answer": ["Ramnath Goenka"], "scores": [[0.7713572382926941, 0.04313955828547478, 0.9970533847808838, 0.008920520544052124, 0.9708768725395203, 0.9845747947692871]], "normalized_score": [0.25621747970581055], "tokens": [["\u2581Ram", "n", "ath", "\u2581Go", "en", "ka"]], "entropy": [[1.7230782508850098, 0.2996053099632263, 0.02428497187793255, 0.461309015750885, 0.25062888860702515, 0.13503660261631012]], "avg_entropy": [0.48232383995006484], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Meira Kumar", "score_of_biggest_cluster": 0.25621747970581055, "score_of_first_answer": 0.25621747970581055, "max_score_of_first_answer": 0.9970533847808838, "min_score_of_first_answer": 0.008920520544052124, "avg_score": 0.25621747970581055, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43335368735915913, "std_of_first_answer": 0.43335368735915913, "avg_entropy_of_first_answer": 0.48232383995006484, "lex_sim": 1.0}
{"question": "who has scored the most points for wales v france", "answer": ["Shane Williams"], "scores": [[0.4131356477737427, 0.9915025234222412, 0.9673308730125427]], "normalized_score": [0.7344921827316284], "tokens": [["\u2581Sh", "ane", "\u2581Williams"]], "entropy": [[2.6120195388793945, 0.06305122375488281, 0.22703054547309875]], "avg_entropy": [0.9673671027024587], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Neil Jenkins", "score_of_biggest_cluster": 0.7344921827316284, "score_of_first_answer": 0.7344921827316284, "max_score_of_first_answer": 0.9915025234222412, "min_score_of_first_answer": 0.4131356477737427, "avg_score": 0.7344921827316284, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26712977686851813, "std_of_first_answer": 0.26712977686851813, "avg_entropy_of_first_answer": 0.9673671027024587, "lex_sim": 1.0}
{"question": "who says that which we call a rose", "answer": ["Bill Shakespeare"], "scores": [[0.0035169676411896944, 0.7266777157783508]], "normalized_score": [0.05055395886301994], "tokens": [["\u2581Bill", "\u2581Shakespeare"]], "entropy": [[2.2266900539398193, 1.5279669761657715]], "avg_entropy": [1.8773285150527954], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Juliet", "score_of_biggest_cluster": 0.05055395886301994, "score_of_first_answer": 0.05055395886301994, "max_score_of_first_answer": 0.7266777157783508, "min_score_of_first_answer": 0.0035169676411896944, "avg_score": 0.05055395886301994, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36158037406858057, "std_of_first_answer": 0.36158037406858057, "avg_entropy_of_first_answer": 1.8773285150527954, "lex_sim": 1.0}
{"question": "what is the big gold dome in jerusalem", "answer": ["Dome of the Rock"], "scores": [[0.6105225086212158, 0.9990220069885254, 0.9918498992919922, 0.9749369621276855, 0.9921013712882996]], "normalized_score": [0.898361325263977], "tokens": [["\u2581D", "ome", "\u2581of", "\u2581the", "\u2581Rock"]], "entropy": [[1.4732403755187988, 0.009611852467060089, 0.058880921453237534, 0.14226067066192627, 0.05769502371549606]], "avg_entropy": [0.34833776876330375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Dome of the Rock", "score_of_biggest_cluster": 0.898361325263977, "score_of_first_answer": 0.898361325263977, "max_score_of_first_answer": 0.9990220069885254, "min_score_of_first_answer": 0.6105225086212158, "avg_score": 0.898361325263977, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.15178971148308235, "std_of_first_answer": 0.15178971148308235, "avg_entropy_of_first_answer": 0.34833776876330375, "lex_sim": 1.0}
{"question": "pace maker is associated with which body organ", "answer": ["Heart"], "scores": [[0.28440535068511963]], "normalized_score": [0.28440535068511963], "tokens": [["\u2581Heart"]], "entropy": [[0.9137870669364929]], "avg_entropy": [0.9137870669364929], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "heart", "score_of_biggest_cluster": 0.28440535068511963, "score_of_first_answer": 0.28440535068511963, "max_score_of_first_answer": 0.28440535068511963, "min_score_of_first_answer": 0.28440535068511963, "avg_score": 0.28440535068511963, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.9137870669364929, "lex_sim": 1.0}
{"question": "legislation regarding data protection and security in uk", "answer": ["GDPR"], "scores": [[0.051172979176044464, 0.9931557774543762, 0.9980570673942566]], "normalized_score": [0.37017279863357544], "tokens": [["\u2581G", "D", "PR"]], "entropy": [[2.0361557006835938, 0.06902679055929184, 0.019449284300208092]], "avg_entropy": [0.7082105918476979], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Data Protection Act 1998", "score_of_biggest_cluster": 0.37017279863357544, "score_of_first_answer": 0.37017279863357544, "max_score_of_first_answer": 0.9980570673942566, "min_score_of_first_answer": 0.051172979176044464, "avg_score": 0.37017279863357544, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4452146912091571, "std_of_first_answer": 0.4452146912091571, "avg_entropy_of_first_answer": 0.7082105918476979, "lex_sim": 1.0}
{"question": "who has the most followers on the instagram", "answer": ["Selena Gomez"], "scores": [[0.2868369221687317, 0.9918511509895325, 0.9642971754074097, 0.9980078339576721, 0.9993442893028259]], "normalized_score": [0.7716641426086426], "tokens": [["\u2581Sel", "ena", "\u2581G", "ome", "z"]], "entropy": [[2.751985788345337, 0.08346496522426605, 0.24635463953018188, 0.0181671604514122, 0.007501004729419947]], "avg_entropy": [0.6214947116561234], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Instagram's own account", "score_of_biggest_cluster": 0.7716641426086426, "score_of_first_answer": 0.7716641426086426, "max_score_of_first_answer": 0.9993442893028259, "min_score_of_first_answer": 0.2868369221687317, "avg_score": 0.7716641426086426, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2809019786185627, "std_of_first_answer": 0.2809019786185627, "avg_entropy_of_first_answer": 0.6214947116561234, "lex_sim": 1.0}
{"question": "when does the movie jeepers creepers come out", "answer": ["August 31, 2001"], "scores": [[0.3762674331665039, 0.9963782429695129, 0.7146874666213989, 0.9566462635993958, 0.8884466290473938, 0.9984344840049744, 0.9969528913497925, 0.99996018409729, 0.8499124646186829, 0.9935696125030518]], "normalized_score": [0.847611665725708], "tokens": [["\u2581August", "\u2581", "3", "1", ",", "\u2581", "2", "0", "0", "1"]], "entropy": [[2.3214664459228516, 0.03079356625676155, 0.8158776760101318, 0.1913509964942932, 0.43320733308792114, 0.014838283881545067, 0.022079864516854286, 0.000556057901121676, 0.42359063029289246, 0.05024023354053497]], "avg_entropy": [0.4304001087904908], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "September 26, 2017", "score_of_biggest_cluster": 0.847611665725708, "score_of_first_answer": 0.847611665725708, "max_score_of_first_answer": 0.99996018409729, "min_score_of_first_answer": 0.3762674331665039, "avg_score": 0.847611665725708, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18878041950338106, "std_of_first_answer": 0.18878041950338106, "avg_entropy_of_first_answer": 0.4304001087904908, "lex_sim": 1.0}
{"question": "who dies in season 6 of once upon a time", "answer": ["Weaver"], "scores": [[0.011783841997385025, 0.9049162268638611]], "normalized_score": [0.10326366871595383], "tokens": [["\u2581We", "aver"]], "entropy": [[4.7835893630981445, 0.627409815788269]], "avg_entropy": [2.705499589443207], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hyde", "score_of_biggest_cluster": 0.10326366871595383, "score_of_first_answer": 0.10326366871595383, "max_score_of_first_answer": 0.9049162268638611, "min_score_of_first_answer": 0.011783841997385025, "avg_score": 0.10326366871595383, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44656619243323803, "std_of_first_answer": 0.44656619243323803, "avg_entropy_of_first_answer": 2.705499589443207, "lex_sim": 1.0}
{"question": "when did red dead redemption 1 come out", "answer": ["May 18, 2010"], "scores": [[0.7813170552253723, 0.9964751601219177, 0.938234806060791, 0.9598808288574219, 0.9858787655830383, 0.9988042116165161, 0.9998694658279419, 0.9999727010726929, 0.9985873699188232, 0.9981983304023743]], "normalized_score": [0.9633100628852844], "tokens": [["\u2581May", "\u2581", "1", "8", ",", "\u2581", "2", "0", "1", "0"]], "entropy": [[1.0785515308380127, 0.028356388211250305, 0.27538537979125977, 0.22672885656356812, 0.09483925998210907, 0.010669003240764141, 0.001358934328891337, 0.0003879881405737251, 0.010977154597640038, 0.01594129204750061]], "avg_entropy": [0.174319578774157], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "May 18, 2010", "score_of_biggest_cluster": 0.9633100628852844, "score_of_first_answer": 0.9633100628852844, "max_score_of_first_answer": 0.9999727010726929, "min_score_of_first_answer": 0.7813170552253723, "avg_score": 0.9633100628852844, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06458741214243856, "std_of_first_answer": 0.06458741214243856, "avg_entropy_of_first_answer": 0.174319578774157, "lex_sim": 1.0}
{"question": "where are the hormones of the anterior pituitary created", "answer": ["in the hypothalamus"], "scores": [[0.03829215094447136, 0.8652669191360474, 0.708555281162262, 0.9959107637405396, 0.9876245260238647, 0.9940022826194763]], "normalized_score": [0.5330961346626282], "tokens": [["\u2581in", "\u2581the", "\u2581hypoth", "al", "am", "us"]], "entropy": [[2.3724465370178223, 0.927513599395752, 1.6646255254745483, 0.03964242711663246, 0.07804065942764282, 0.04330211877822876]], "avg_entropy": [0.8542618112017711], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The pars distalis", "score_of_biggest_cluster": 0.5330961346626282, "score_of_first_answer": 0.5330961346626282, "max_score_of_first_answer": 0.9959107637405396, "min_score_of_first_answer": 0.03829215094447136, "avg_score": 0.5330961346626282, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3407509464127915, "std_of_first_answer": 0.3407509464127915, "avg_entropy_of_first_answer": 0.8542618112017711, "lex_sim": 1.0}
{"question": "how long prime minister stay in office canada", "answer": ["At the Governor General\u2019s pleasure until the next election."], "scores": [[0.008225169964134693, 0.465835839509964, 0.044862743467092514, 0.9459777474403381, 0.19220636785030365, 0.9997997879981995, 0.805779755115509, 0.0016208344604820013, 0.8095229864120483, 0.8266924023628235, 0.5708726644515991, 0.1698514074087143]], "normalized_score": [0.19286204874515533], "tokens": [["\u2581At", "\u2581the", "\u2581Governor", "\u2581General", "\u2019", "s", "\u2581pleasure", "\u2581until", "\u2581the", "\u2581next", "\u2581election", "."]], "entropy": [[3.2076663970947266, 1.6932005882263184, 2.582317352294922, 0.28435754776000977, 0.6642534732818604, 0.002816563006490469, 0.6878596544265747, 1.709399938583374, 1.1764581203460693, 1.0679802894592285, 1.4310849905014038, 1.849998116493225]], "avg_entropy": [1.3631160859561835], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "four years", "score_of_biggest_cluster": 0.19286204874515533, "score_of_first_answer": 0.19286204874515533, "max_score_of_first_answer": 0.9997997879981995, "min_score_of_first_answer": 0.0016208344604820013, "avg_score": 0.19286204874515533, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37080192240549786, "std_of_first_answer": 0.37080192240549786, "avg_entropy_of_first_answer": 1.3631160859561835, "lex_sim": 1.0}
{"question": "who is the first wife on sister wives", "answer": ["Meri Barber"], "scores": [[0.9048895835876465, 0.9903422594070435, 0.09206614643335342, 0.9988279938697815]], "normalized_score": [0.5357878804206848], "tokens": [["\u2581Mer", "i", "\u2581Bar", "ber"]], "entropy": [[0.6208597421646118, 0.08745773881673813, 1.3450700044631958, 0.012820624746382236]], "avg_entropy": [0.516552027547732], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Meri", "score_of_biggest_cluster": 0.5357878804206848, "score_of_first_answer": 0.5357878804206848, "max_score_of_first_answer": 0.9988279938697815, "min_score_of_first_answer": 0.09206614643335342, "avg_score": 0.5357878804206848, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.37963778423593836, "std_of_first_answer": 0.37963778423593836, "avg_entropy_of_first_answer": 0.516552027547732, "lex_sim": 1.0}
{"question": "winner of worst cooks in america season 5", "answer": ["Laila Ali"], "scores": [[0.024528387933969498, 0.0087382011115551, 0.986462414264679, 0.9439168572425842]], "normalized_score": [0.11885742843151093], "tokens": [["\u2581L", "ail", "a", "\u2581Ali"]], "entropy": [[6.084628105163574, 3.9236292839050293, 0.12762029469013214, 0.43025538325309753]], "avg_entropy": [2.6415332667529583], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Amber Brauner", "score_of_biggest_cluster": 0.11885742843151093, "score_of_first_answer": 0.11885742843151093, "max_score_of_first_answer": 0.986462414264679, "min_score_of_first_answer": 0.0087382011115551, "avg_score": 0.11885742843151093, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.47454948627629895, "std_of_first_answer": 0.47454948627629895, "avg_entropy_of_first_answer": 2.6415332667529583, "lex_sim": 1.0}
{"question": "who sang smoke gets in your eyes first", "answer": ["Jeremy Spencer"], "scores": [[0.07389391958713531, 0.19700974225997925, 0.13554097712039948, 0.9145126938819885]], "normalized_score": [0.2061053216457367], "tokens": [["\u2581Jer", "emy", "\u2581Sp", "encer"]], "entropy": [[2.826862335205078, 1.2493858337402344, 4.578100204467773, 0.4536164402961731]], "avg_entropy": [2.2769912034273148], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gertrude Niesen", "score_of_biggest_cluster": 0.2061053216457367, "score_of_first_answer": 0.2061053216457367, "max_score_of_first_answer": 0.9145126938819885, "min_score_of_first_answer": 0.07389391958713531, "avg_score": 0.2061053216457367, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3401271471248362, "std_of_first_answer": 0.3401271471248362, "avg_entropy_of_first_answer": 2.2769912034273148, "lex_sim": 1.0}
{"question": "who won the womens 2017 ncaa basketball tournament", "answer": ["SCORCHERS, SOUTH CAROLINA"], "scores": [[0.03643310070037842, 0.47505679726600647, 0.16235458850860596, 0.4218636155128479, 0.009698118083178997, 0.059300124645233154, 0.9740245342254639, 0.7838902473449707, 0.84345942735672, 0.9998557567596436, 0.9994953870773315, 0.9988411068916321]], "normalized_score": [0.29521864652633667], "tokens": [["\u2581SC", "OR", "CH", "ERS", ",", "\u2581SO", "UT", "H", "\u2581CA", "RO", "LIN", "A"]], "entropy": [[0.7859411835670471, 2.8218822479248047, 4.229574680328369, 1.9269707202911377, 1.5850751399993896, 4.431227684020996, 0.22145485877990723, 0.7098077535629272, 0.819186806678772, 0.0017823813250288367, 0.005364228971302509, 0.0129842609167099]], "avg_entropy": [1.462604328863866], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "South Carolina", "score_of_biggest_cluster": 0.29521864652633667, "score_of_first_answer": 0.29521864652633667, "max_score_of_first_answer": 0.9998557567596436, "min_score_of_first_answer": 0.009698118083178997, "avg_score": 0.29521864652633667, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3971789657406862, "std_of_first_answer": 0.3971789657406862, "avg_entropy_of_first_answer": 1.462604328863866, "lex_sim": 1.0}
{"question": "who played the mad hatter in the batman tv show", "answer": ["David Wayne"], "scores": [[0.7695401310920715, 0.9968738555908203]], "normalized_score": [0.8758621215820312], "tokens": [["\u2581David", "\u2581Wayne"]], "entropy": [[1.0481382608413696, 0.03481461852788925]], "avg_entropy": [0.5414764396846294], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Roddy McDowall", "score_of_biggest_cluster": 0.8758621215820312, "score_of_first_answer": 0.8758621215820312, "max_score_of_first_answer": 0.9968738555908203, "min_score_of_first_answer": 0.7695401310920715, "avg_score": 0.8758621215820312, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11366686224937439, "std_of_first_answer": 0.11366686224937439, "avg_entropy_of_first_answer": 0.5414764396846294, "lex_sim": 1.0}
{"question": "when does the cannes film festival take place", "answer": ["Mid-May"], "scores": [[0.04437495023012161, 0.628892719745636, 0.9053661227226257]], "normalized_score": [0.2934356927871704], "tokens": [["\u2581Mid", "-", "May"]], "entropy": [[2.0490307807922363, 1.1887272596359253, 0.599124550819397]], "avg_entropy": [1.2789608637491863], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cannes, France", "score_of_biggest_cluster": 0.2934356927871704, "score_of_first_answer": 0.2934356927871704, "max_score_of_first_answer": 0.9053661227226257, "min_score_of_first_answer": 0.04437495023012161, "avg_score": 0.2934356927871704, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35891880303413193, "std_of_first_answer": 0.35891880303413193, "avg_entropy_of_first_answer": 1.2789608637491863, "lex_sim": 1.0}
{"question": "what is the general structure of an url", "answer": ["http://"], "scores": [[0.247286856174469, 0.734131932258606]], "normalized_score": [0.4260764718055725], "tokens": [["\u2581http", "://"]], "entropy": [[3.7990915775299072, 1.348422884941101]], "avg_entropy": [2.573757231235504], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "a protocol", "score_of_biggest_cluster": 0.4260764718055725, "score_of_first_answer": 0.4260764718055725, "max_score_of_first_answer": 0.734131932258606, "min_score_of_first_answer": 0.247286856174469, "avg_score": 0.4260764718055725, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24342253804206848, "std_of_first_answer": 0.24342253804206848, "avg_entropy_of_first_answer": 2.573757231235504, "lex_sim": 1.0}
{"question": "what tectonic setting is responsible for the folded mountains of pennsylvania and the high himalaya", "answer": ["fold mountains"], "scores": [[0.10305408388376236, 0.7913150787353516, 0.580592155456543]], "normalized_score": [0.3617667257785797], "tokens": [["\u2581fol", "d", "\u2581mountains"]], "entropy": [[3.71866512298584, 0.7502164840698242, 1.5659788846969604]], "avg_entropy": [2.0116201639175415], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "a convergent plate boundary", "score_of_biggest_cluster": 0.3617667257785797, "score_of_first_answer": 0.3617667257785797, "max_score_of_first_answer": 0.7913150787353516, "min_score_of_first_answer": 0.10305408388376236, "avg_score": 0.3617667257785797, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28793324018496524, "std_of_first_answer": 0.28793324018496524, "avg_entropy_of_first_answer": 2.0116201639175415, "lex_sim": 1.0}
{"question": "bruce springsteen we shall overcome the seeger sessions songs", "answer": ["We Shall Overcome"], "scores": [[0.5093207955360413, 0.9727359414100647, 0.9999545812606812, 0.9983476400375366, 0.9988881945610046]], "normalized_score": [0.8684664964675903], "tokens": [["\u2581We", "\u2581Sh", "all", "\u2581Over", "come"]], "entropy": [[3.100590229034424, 0.18644970655441284, 0.0005770697025582194, 0.01785464957356453, 0.010496918112039566]], "avg_entropy": [0.6631937145953998], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\"Old Dan Tucker\"", "score_of_biggest_cluster": 0.8684664964675903, "score_of_first_answer": 0.8684664964675903, "max_score_of_first_answer": 0.9999545812606812, "min_score_of_first_answer": 0.5093207955360413, "avg_score": 0.8684664964675903, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19353380763037195, "std_of_first_answer": 0.19353380763037195, "avg_entropy_of_first_answer": 0.6631937145953998, "lex_sim": 1.0}
{"question": "who was the successful commanding general of the northern forces in the civil war", "answer": ["Ulysses S. Grant"], "scores": [[0.7838876247406006, 0.988471508026123, 0.9983887672424316, 0.9993019104003906, 0.8891670107841492, 0.969611644744873, 0.9983053207397461]], "normalized_score": [0.9434582591056824], "tokens": [["\u2581U", "ly", "ss", "es", "\u2581S", ".", "\u2581Grant"]], "entropy": [[1.4378156661987305, 0.07738840579986572, 0.015218513086438179, 0.007542637642472982, 0.5127763152122498, 0.15138711035251617, 0.016709407791495323]], "avg_entropy": [0.3169768651548241], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George B. McClellan", "score_of_biggest_cluster": 0.9434582591056824, "score_of_first_answer": 0.9434582591056824, "max_score_of_first_answer": 0.9993019104003906, "min_score_of_first_answer": 0.7838876247406006, "avg_score": 0.9434582591056824, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07577175094683597, "std_of_first_answer": 0.07577175094683597, "avg_entropy_of_first_answer": 0.3169768651548241, "lex_sim": 1.0}
{"question": "what was the first capital city of australia", "answer": ["Sydney"], "scores": [[0.6654915809631348]], "normalized_score": [0.6654915809631348], "tokens": [["\u2581Sydney"]], "entropy": [[1.539472222328186]], "avg_entropy": [1.539472222328186], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "New South Wales", "score_of_biggest_cluster": 0.6654915809631348, "score_of_first_answer": 0.6654915809631348, "max_score_of_first_answer": 0.6654915809631348, "min_score_of_first_answer": 0.6654915809631348, "avg_score": 0.6654915809631348, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.539472222328186, "lex_sim": 1.0}
{"question": "where in the constitution is the executive branch referenced", "answer": ["The President shall be Commander in Chief of the Army and Navy of the United States, and of the Militia of the several States, when called into the actual Service of the United States"], "scores": [[0.041721951216459274, 0.10909248888492584, 0.3874661922454834, 0.9725818634033203, 0.8996450304985046, 0.9978068470954895, 0.999506950378418, 0.9792887568473816, 0.999537467956543, 0.9684245586395264, 0.9916489124298096, 0.9979695677757263, 0.9768532514572144, 0.9948590993881226, 0.9869869351387024, 0.9979028701782227, 0.8088897466659546, 0.9818713665008545, 0.997473418712616, 0.998992383480072, 0.9883145093917847, 0.9999754428863525, 0.9946920275688171, 0.9988794922828674, 0.9949790835380554, 0.997947633266449, 0.9288887977600098, 0.9128080010414124, 0.9996020197868347, 0.9994308352470398, 0.9729183316230774, 0.9844824075698853, 0.9956451654434204, 0.9962104558944702, 0.999176561832428, 0.9977748990058899, 0.9976937174797058]], "normalized_score": [0.8261517882347107], "tokens": [["\u2581The", "\u2581President", "\u2581shall", "\u2581be", "\u2581Commander", "\u2581in", "\u2581Chief", "\u2581of", "\u2581the", "\u2581Army", "\u2581and", "\u2581Navy", "\u2581of", "\u2581the", "\u2581United", "\u2581States", ",", "\u2581and", "\u2581of", "\u2581the", "\u2581Milit", "ia", "\u2581of", "\u2581the", "\u2581several", "\u2581States", ",", "\u2581when", "\u2581called", "\u2581into", "\u2581the", "\u2581actual", "\u2581Service", "\u2581of", "\u2581the", "\u2581United", "\u2581States"]], "entropy": [[1.8928143978118896, 2.572852611541748, 2.155599594116211, 0.19404488801956177, 0.40061819553375244, 0.018962576985359192, 0.005227378569543362, 0.14574363827705383, 0.005369918420910835, 0.20161095261573792, 0.06570374965667725, 0.01860644295811653, 0.15086683630943298, 0.043992310762405396, 0.09582000970840454, 0.019245576113462448, 0.8505567908287048, 0.12655679881572723, 0.02463676407933235, 0.00997486524283886, 0.08443280309438705, 0.00029921982786618173, 0.04541440308094025, 0.01169961504638195, 0.04370550066232681, 0.017646770924329758, 0.42793285846710205, 0.5735571980476379, 0.004511788487434387, 0.006373170763254166, 0.15607765316963196, 0.09687440097332001, 0.03444556146860123, 0.03213956952095032, 0.008572353050112724, 0.01960528828203678, 0.021124597638845444]], "avg_entropy": [0.28603289321275754], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Article Two", "score_of_biggest_cluster": 0.8261517882347107, "score_of_first_answer": 0.8261517882347107, "max_score_of_first_answer": 0.9999754428863525, "min_score_of_first_answer": 0.041721951216459274, "avg_score": 0.8261517882347107, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.22556120212980274, "std_of_first_answer": 0.22556120212980274, "avg_entropy_of_first_answer": 0.28603289321275754, "lex_sim": 1.0}
{"question": "who has the most 70 point games in nba history", "answer": ["Wilt Chamberlin"], "scores": [[0.6597222685813904, 0.9933466911315918, 0.992939829826355, 0.021522419527173042]], "normalized_score": [0.34400835633277893], "tokens": [["\u2581W", "ilt", "\u2581Chamber", "lin"]], "entropy": [[1.8272374868392944, 0.04746852070093155, 0.059905994683504105, 0.14423753321170807]], "avg_entropy": [0.5197123838588595], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Wilt Chamberlain*", "score_of_biggest_cluster": 0.34400835633277893, "score_of_first_answer": 0.34400835633277893, "max_score_of_first_answer": 0.9933466911315918, "min_score_of_first_answer": 0.021522419527173042, "avg_score": 0.34400835633277893, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3966841163968483, "std_of_first_answer": 0.3966841163968483, "avg_entropy_of_first_answer": 0.5197123838588595, "lex_sim": 1.0}
{"question": "who has the most catches in nfl history", "answer": ["Jason Witten"], "scores": [[0.10419489443302155, 0.9140515923500061, 0.998189389705658]], "normalized_score": [0.4563975930213928], "tokens": [["\u2581Jason", "\u2581W", "itten"]], "entropy": [[2.479417324066162, 0.6567191481590271, 0.01657569780945778]], "avg_entropy": [1.0509040566782157], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jerry Rice", "score_of_biggest_cluster": 0.4563975930213928, "score_of_first_answer": 0.4563975930213928, "max_score_of_first_answer": 0.998189389705658, "min_score_of_first_answer": 0.10419489443302155, "avg_score": 0.4563975930213928, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40306784618895647, "std_of_first_answer": 0.40306784618895647, "avg_entropy_of_first_answer": 1.0509040566782157, "lex_sim": 1.0}
{"question": "who played bailey in the sisterhood of the traveling pants", "answer": ["Amber Tamblyn"], "scores": [[0.4907556474208832, 0.9996583461761475, 0.9465034008026123, 0.9968531727790833]], "normalized_score": [0.8248359560966492], "tokens": [["\u2581Am", "ber", "\u2581Tamb", "lyn"]], "entropy": [[2.3663041591644287, 0.004106710199266672, 0.4215708076953888, 0.028629107400774956]], "avg_entropy": [0.7051526961149648], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jenna Boyd", "score_of_biggest_cluster": 0.8248359560966492, "score_of_first_answer": 0.8248359560966492, "max_score_of_first_answer": 0.9996583461761475, "min_score_of_first_answer": 0.4907556474208832, "avg_score": 0.8248359560966492, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21333528386089118, "std_of_first_answer": 0.21333528386089118, "avg_entropy_of_first_answer": 0.7051526961149648, "lex_sim": 1.0}
{"question": "which president supported the creation of the environmental protection agency (epa)", "answer": ["Richard Milhous Nixon"], "scores": [[0.8324168920516968, 0.5746067762374878, 0.9500675201416016, 0.9995620846748352, 0.9924523234367371, 0.9983355402946472]], "normalized_score": [0.8754076361656189], "tokens": [["\u2581Richard", "\u2581Mil", "h", "ous", "\u2581N", "ixon"]], "entropy": [[1.0829535722732544, 0.9073933362960815, 0.24001479148864746, 0.005475206300616264, 0.06077025085687637, 0.01437523402273655]], "avg_entropy": [0.38516373187303543], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Richard Nixon", "score_of_biggest_cluster": 0.8754076361656189, "score_of_first_answer": 0.8754076361656189, "max_score_of_first_answer": 0.9995620846748352, "min_score_of_first_answer": 0.5746067762374878, "avg_score": 0.8754076361656189, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.15309283498338583, "std_of_first_answer": 0.15309283498338583, "avg_entropy_of_first_answer": 0.38516373187303543, "lex_sim": 1.0}
{"question": "how many episodes are there in ordeal by innocence", "answer": ["3"], "scores": [[0.8347030878067017, 0.6755620241165161]], "normalized_score": [0.7509285807609558], "tokens": [["\u2581", "3"]], "entropy": [[1.122745156288147, 1.1501153707504272]], "avg_entropy": [1.136430263519287], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "three part", "score_of_biggest_cluster": 0.7509285807609558, "score_of_first_answer": 0.7509285807609558, "max_score_of_first_answer": 0.8347030878067017, "min_score_of_first_answer": 0.6755620241165161, "avg_score": 0.7509285807609558, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.07957053184509277, "std_of_first_answer": 0.07957053184509277, "avg_entropy_of_first_answer": 1.136430263519287, "lex_sim": 1.0}
{"question": "where are the mitochondria located in the sperm", "answer": ["atypical head with two centrioles, atypical tail with both centrioles and flagellum."], "scores": [[0.008293711580336094, 0.0005356788169592619, 0.9693007469177246, 0.1550012230873108, 0.011347752995789051, 0.011616255156695843, 0.13649693131446838, 0.9839350581169128, 0.9806967377662659, 0.10572218149900436, 0.03254111483693123, 0.6410364508628845, 0.9963099360466003, 0.19832970201969147, 0.6698445677757263, 0.0020681454334408045, 0.1383715569972992, 0.9863461852073669, 0.8781512975692749, 0.7260640263557434, 0.12586760520935059, 0.5797807574272156, 0.9389649033546448, 0.07048604637384415]], "normalized_score": [0.13948170840740204], "tokens": [["\u2581at", "yp", "ical", "\u2581head", "\u2581with", "\u2581two", "\u2581cent", "ri", "oles", ",", "\u2581at", "yp", "ical", "\u2581tail", "\u2581with", "\u2581both", "\u2581cent", "ri", "oles", "\u2581and", "\u2581flag", "ell", "um", "."]], "entropy": [[4.513879776000977, 0.580778956413269, 0.1763373166322708, 3.2704436779022217, 2.0264480113983154, 4.169114112854004, 4.808976173400879, 0.11705958843231201, 0.1222413182258606, 2.469764232635498, 4.794600486755371, 1.3545622825622559, 0.03128693252801895, 2.8791494369506836, 1.7785828113555908, 3.6725564002990723, 3.703364849090576, 0.09930059313774109, 0.4803336262702942, 1.5658810138702393, 3.934802532196045, 0.7555698156356812, 0.27208253741264343, 1.9976420402526855]], "avg_entropy": [2.065614938425521], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "midpiece", "score_of_biggest_cluster": 0.13948170840740204, "score_of_first_answer": 0.13948170840740204, "max_score_of_first_answer": 0.9963099360466003, "min_score_of_first_answer": 0.0005356788169592619, "avg_score": 0.13948170840740204, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40229900443037864, "std_of_first_answer": 0.40229900443037864, "avg_entropy_of_first_answer": 2.065614938425521, "lex_sim": 1.0}
{"question": "where can i get a state issued id", "answer": ["the nearest state office"], "scores": [[0.02078290283679962, 0.058962512761354446, 0.05476950854063034, 0.16073383390903473]], "normalized_score": [0.057310208678245544], "tokens": [["\u2581the", "\u2581nearest", "\u2581state", "\u2581office"]], "entropy": [[4.8025712966918945, 3.6555495262145996, 3.467780113220215, 3.0904958248138428]], "avg_entropy": [3.754099190235138], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "each state's DMV", "score_of_biggest_cluster": 0.057310208678245544, "score_of_first_answer": 0.057310208678245544, "max_score_of_first_answer": 0.16073383390903473, "min_score_of_first_answer": 0.02078290283679962, "avg_score": 0.057310208678245544, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05232259098167151, "std_of_first_answer": 0.05232259098167151, "avg_entropy_of_first_answer": 3.754099190235138, "lex_sim": 1.0}
{"question": "who is responsible for establishing local licensing forum", "answer": ["Department of Local Government"], "scores": [[0.04094459116458893, 0.7581782937049866, 0.18658889830112457, 0.9601597785949707]], "normalized_score": [0.27308589220046997], "tokens": [["\u2581Department", "\u2581of", "\u2581Local", "\u2581Government"]], "entropy": [[4.888500213623047, 0.7099124789237976, 3.4366488456726074, 0.3009497821331024]], "avg_entropy": [2.3340028300881386], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "unitary authorities", "score_of_biggest_cluster": 0.27308589220046997, "score_of_first_answer": 0.27308589220046997, "max_score_of_first_answer": 0.9601597785949707, "min_score_of_first_answer": 0.04094459116458893, "avg_score": 0.27308589220046997, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3829585383137834, "std_of_first_answer": 0.3829585383137834, "avg_entropy_of_first_answer": 2.3340028300881386, "lex_sim": 1.0}
{"question": "what hangs from the trees in the bayou", "answer": ["moss"], "scores": [[0.26042261719703674, 0.9984373450279236]], "normalized_score": [0.5099173188209534], "tokens": [["\u2581m", "oss"]], "entropy": [[1.6583871841430664, 0.01611250825226307]], "avg_entropy": [0.8372498461976647], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Spanish moss", "score_of_biggest_cluster": 0.5099173188209534, "score_of_first_answer": 0.5099173188209534, "max_score_of_first_answer": 0.9984373450279236, "min_score_of_first_answer": 0.26042261719703674, "avg_score": 0.5099173188209534, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3690073639154434, "std_of_first_answer": 0.3690073639154434, "avg_entropy_of_first_answer": 0.8372498461976647, "lex_sim": 1.0}
{"question": "what football player is called the honey badger", "answer": ["Tyrann Mathieu"], "scores": [[0.6376028060913086, 0.9342000484466553, 0.9974905252456665, 0.9653618335723877, 0.9991016387939453]], "normalized_score": [0.8946223258972168], "tokens": [["\u2581Ty", "ran", "n", "\u2581Math", "ieu"]], "entropy": [[2.373847007751465, 0.33789658546447754, 0.02717241272330284, 0.23683305084705353, 0.00930822640657425]], "avg_entropy": [0.5970114566385746], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Tyrann Devine Mathieu", "score_of_biggest_cluster": 0.8946223258972168, "score_of_first_answer": 0.8946223258972168, "max_score_of_first_answer": 0.9991016387939453, "min_score_of_first_answer": 0.6376028060913086, "avg_score": 0.8946223258972168, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.13666874718366312, "std_of_first_answer": 0.13666874718366312, "avg_entropy_of_first_answer": 0.5970114566385746, "lex_sim": 1.0}
{"question": "when was the last time a hurricane hit massachusetts", "answer": ["Hurricane Carol - 1954"], "scores": [[0.09701871871948242, 0.9979944229125977, 0.9978660941123962, 0.19235548377037048, 0.022727517411112785, 0.41686269640922546, 0.9765285849571228, 0.991763710975647, 0.9907057881355286, 0.9867283701896667]], "normalized_score": [0.41898080706596375], "tokens": [["\u2581H", "urr", "icane", "\u2581Carol", "\u2581-", "\u2581", "1", "9", "5", "4"]], "entropy": [[2.4431631565093994, 0.01998562552034855, 0.01580890268087387, 2.922903060913086, 2.2431092262268066, 1.7127773761749268, 0.15727481245994568, 0.06486975401639938, 0.06787055730819702, 0.0901118740439415]], "avg_entropy": [0.9737874345853925], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "November 3, 2007", "score_of_biggest_cluster": 0.41898080706596375, "score_of_first_answer": 0.41898080706596375, "max_score_of_first_answer": 0.9979944229125977, "min_score_of_first_answer": 0.022727517411112785, "avg_score": 0.41898080706596375, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4068344271253818, "std_of_first_answer": 0.4068344271253818, "avg_entropy_of_first_answer": 0.9737874345853925, "lex_sim": 1.0}
{"question": "who are the two teams in super bowl 2018", "answer": ["Patriots, Eagles"], "scores": [[0.08635521680116653, 0.9977357387542725, 0.13264283537864685, 0.7617917060852051, 0.9987940788269043, 0.9997554421424866]], "normalized_score": [0.4534529745578766], "tokens": [["\u2581Patri", "ots", ",", "\u2581E", "ag", "les"]], "entropy": [[1.5499234199523926, 0.017116565257310867, 1.70640230178833, 0.921013593673706, 0.010973850265145302, 0.0028453536797314882]], "avg_entropy": [0.7013791807694361], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "New England Patriots", "score_of_biggest_cluster": 0.4534529745578766, "score_of_first_answer": 0.4534529745578766, "max_score_of_first_answer": 0.9997554421424866, "min_score_of_first_answer": 0.08635521680116653, "avg_score": 0.4534529745578766, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40036805829873495, "std_of_first_answer": 0.40036805829873495, "avg_entropy_of_first_answer": 0.7013791807694361, "lex_sim": 1.0}
{"question": "what do you call initials that stand for something", "answer": ["Acronyms"], "scores": [[0.09285333007574081, 0.9899861216545105, 0.9978487491607666, 0.296936571598053]], "normalized_score": [0.4062456488609314], "tokens": [["\u2581Ac", "ron", "ym", "s"]], "entropy": [[1.4913333654403687, 0.0725565105676651, 0.02066034823656082, 0.8396437168121338]], "avg_entropy": [0.6060484852641821], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "acronym", "score_of_biggest_cluster": 0.4062456488609314, "score_of_first_answer": 0.4062456488609314, "max_score_of_first_answer": 0.9978487491607666, "min_score_of_first_answer": 0.09285333007574081, "avg_score": 0.4062456488609314, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.405984244164838, "std_of_first_answer": 0.405984244164838, "avg_entropy_of_first_answer": 0.6060484852641821, "lex_sim": 1.0}
{"question": "when did lord howe island became a world heritage site", "answer": ["June 29, 2008"], "scores": [[0.02832668460905552, 0.9876179695129395, 0.3569275140762329, 0.3014681935310364, 0.9860734939575195, 0.9989566802978516, 0.1518879383802414, 0.9926349520683289, 0.7298658490180969, 0.5493975281715393]], "normalized_score": [0.4220369756221771], "tokens": [["\u2581June", "\u2581", "2", "9", ",", "\u2581", "2", "0", "0", "8"]], "entropy": [[2.1945106983184814, 0.07696135342121124, 1.640163779258728, 2.1938438415527344, 0.09372024238109589, 0.009601611644029617, 0.4349689483642578, 0.05271228030323982, 0.7276226878166199, 1.562138319015503]], "avg_entropy": [0.8986243762075901], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in 1982", "score_of_biggest_cluster": 0.4220369756221771, "score_of_first_answer": 0.4220369756221771, "max_score_of_first_answer": 0.9989566802978516, "min_score_of_first_answer": 0.02832668460905552, "avg_score": 0.4220369756221771, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36154993943427977, "std_of_first_answer": 0.36154993943427977, "avg_entropy_of_first_answer": 0.8986243762075901, "lex_sim": 1.0}
{"question": "who played mr thatcher in when calls the heart", "answer": ["Stephen Huszar"], "scores": [[0.015284924767911434, 0.37134379148483276, 0.970440149307251, 0.8984181880950928]], "normalized_score": [0.2652294337749481], "tokens": [["\u2581Stephen", "\u2581H", "usz", "ar"]], "entropy": [[4.926570892333984, 2.7968757152557373, 0.23851442337036133, 0.3737179636955261]], "avg_entropy": [2.0839197486639023], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Garwin Sanford", "score_of_biggest_cluster": 0.2652294337749481, "score_of_first_answer": 0.2652294337749481, "max_score_of_first_answer": 0.970440149307251, "min_score_of_first_answer": 0.015284924767911434, "avg_score": 0.2652294337749481, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.392184172993255, "std_of_first_answer": 0.392184172993255, "avg_entropy_of_first_answer": 2.0839197486639023, "lex_sim": 1.0}
{"question": "who was the movie citizen kane based on", "answer": ["Charles Foster Kane"], "scores": [[0.011560093611478806, 0.9861271381378174, 0.9989063739776611, 0.9837132692337036, 0.999947190284729]], "normalized_score": [0.4072423279285431], "tokens": [["\u2581Charles", "\u2581F", "oster", "\u2581K", "ane"]], "entropy": [[0.620689868927002, 0.12475360929965973, 0.010511680506169796, 0.12929901480674744, 0.0007622519042342901]], "avg_entropy": [0.17720328508876265], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "William Randolph Hearst", "score_of_biggest_cluster": 0.4072423279285431, "score_of_first_answer": 0.4072423279285431, "max_score_of_first_answer": 0.999947190284729, "min_score_of_first_answer": 0.011560093611478806, "avg_score": 0.4072423279285431, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39229988706439406, "std_of_first_answer": 0.39229988706439406, "avg_entropy_of_first_answer": 0.17720328508876265, "lex_sim": 1.0}
{"question": "what percentage of global man-made co2 emissions comes from worldwide aviation", "answer": ["3%"], "scores": [[0.8938140273094177, 0.17273783683776855, 0.32220348715782166]], "normalized_score": [0.3677801787853241], "tokens": [["\u2581", "3", "%"]], "entropy": [[0.7983803153038025, 1.4348887205123901, 1.22031831741333]], "avg_entropy": [1.1511957844098408], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "4 percent cumulative effect", "score_of_biggest_cluster": 0.3677801787853241, "score_of_first_answer": 0.3677801787853241, "max_score_of_first_answer": 0.8938140273094177, "min_score_of_first_answer": 0.17273783683776855, "avg_score": 0.3677801787853241, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31073916584369204, "std_of_first_answer": 0.31073916584369204, "avg_entropy_of_first_answer": 1.1511957844098408, "lex_sim": 1.0}
{"question": "who sings the song rock you like a hurricane", "answer": ["Scorpions"], "scores": [[0.813541054725647, 0.9975625276565552, 0.9920531511306763]], "normalized_score": [0.930289626121521], "tokens": [["\u2581Sc", "orp", "ions"]], "entropy": [[0.6716012954711914, 0.01949520781636238, 0.04950017109513283]], "avg_entropy": [0.2468655581275622], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "German rock band Scorpions", "score_of_biggest_cluster": 0.930289626121521, "score_of_first_answer": 0.930289626121521, "max_score_of_first_answer": 0.9975625276565552, "min_score_of_first_answer": 0.813541054725647, "avg_score": 0.930289626121521, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08547957799789463, "std_of_first_answer": 0.08547957799789463, "avg_entropy_of_first_answer": 0.2468655581275622, "lex_sim": 1.0}
{"question": "who has the most ncaa basketball tournament appearances", "answer": ["North Carolina Tar Heels men\u2019s basketball"], "scores": [[0.1606457531452179, 0.9903923273086548, 0.4963035583496094, 0.9885442852973938, 0.9967964291572571, 0.47840142250061035, 0.21365618705749512, 0.9998476505279541, 0.9961439371109009]], "normalized_score": [0.5841606259346008], "tokens": [["\u2581North", "\u2581Carolina", "\u2581Tar", "\u2581He", "els", "\u2581men", "\u2019", "s", "\u2581basketball"]], "entropy": [[2.9002017974853516, 0.0809452086687088, 1.2091662883758545, 0.0803566575050354, 0.023062219843268394, 1.2680628299713135, 0.6504890322685242, 0.002009130083024502, 0.03719589114189148]], "avg_entropy": [0.694609895038108], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kentucky", "score_of_biggest_cluster": 0.5841606259346008, "score_of_first_answer": 0.5841606259346008, "max_score_of_first_answer": 0.9998476505279541, "min_score_of_first_answer": 0.1606457531452179, "avg_score": 0.5841606259346008, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34177107288497616, "std_of_first_answer": 0.34177107288497616, "avg_entropy_of_first_answer": 0.694609895038108, "lex_sim": 1.0}
{"question": "where is final four being held this year", "answer": ["77th edition of the NCAA Division I Men's Basketball Tournament, the national championship tournament for"], "scores": [[0.022168518975377083, 0.046195220202207565, 0.11409458518028259, 0.13140259683132172, 0.09564919024705887, 0.3695138692855835, 0.7040992975234985, 0.5975416302680969, 0.6352695226669312, 0.982225775718689, 0.9031984210014343, 0.7643448710441589, 0.9998441934585571, 0.9701377153396606, 0.9639703631401062, 0.09745940566062927, 0.03696190565824509, 0.0429377481341362, 0.9041965007781982, 0.8446326851844788, 0.7159631848335266]], "normalized_score": [0.29579558968544006], "tokens": [["\u2581", "7", "7", "th", "\u2581edition", "\u2581of", "\u2581the", "\u2581NCAA", "\u2581Division", "\u2581I", "\u2581Men", "'", "s", "\u2581Basketball", "\u2581Tournament", ",", "\u2581the", "\u2581national", "\u2581championship", "\u2581tournament", "\u2581for"]], "entropy": [[4.251398086547852, 0.6977999210357666, 3.033964157104492, 2.7855095863342285, 3.362837791442871, 2.641171455383301, 1.2876482009887695, 1.9850555658340454, 1.3503978252410889, 0.12824124097824097, 0.5721420049667358, 0.6760635375976562, 0.0021304558031260967, 0.2156364470720291, 0.21062497794628143, 2.4470646381378174, 3.3862781524658203, 2.947800397872925, 0.5294038653373718, 0.7326244711875916, 0.8177114725112915]], "avg_entropy": [1.6219763929423476], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Alamodome", "score_of_biggest_cluster": 0.29579558968544006, "score_of_first_answer": 0.29579558968544006, "max_score_of_first_answer": 0.9998441934585571, "min_score_of_first_answer": 0.022168518975377083, "avg_score": 0.29579558968544006, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3795370310369147, "std_of_first_answer": 0.3795370310369147, "avg_entropy_of_first_answer": 1.6219763929423476, "lex_sim": 1.0}
{"question": "who was elected president of the united states in 1928", "answer": ["Herbert Hoover"], "scores": [[0.9724258184432983, 0.6913019418716431, 0.9999244213104248]], "normalized_score": [0.8759859800338745], "tokens": [["\u2581Herbert", "\u2581Ho", "over"]], "entropy": [[0.24704937636852264, 0.8028643131256104, 0.0010225048754364252]], "avg_entropy": [0.3503120647898565], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Herbert Hoover", "score_of_biggest_cluster": 0.8759859800338745, "score_of_first_answer": 0.8759859800338745, "max_score_of_first_answer": 0.9999244213104248, "min_score_of_first_answer": 0.6913019418716431, "avg_score": 0.8759859800338745, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.13945713875726218, "std_of_first_answer": 0.13945713875726218, "avg_entropy_of_first_answer": 0.3503120647898565, "lex_sim": 1.0}
{"question": "when did i'm coming out come out", "answer": ["June 16, 1978"], "scores": [[0.03379777818918228, 0.9819210171699524, 0.4844866991043091, 0.06035836786031723, 0.9628874659538269, 0.9956082701683044, 0.6294947862625122, 0.9987342953681946, 0.4063563644886017, 0.5225315690040588]], "normalized_score": [0.4068281352519989], "tokens": [["\u2581June", "\u2581", "1", "6", ",", "\u2581", "1", "9", "7", "8"]], "entropy": [[3.0195183753967285, 0.11257071048021317, 1.4596195220947266, 2.1283676624298096, 0.21225155889987946, 0.034379489719867706, 0.6633756160736084, 0.010972578078508377, 1.145659327507019, 1.6215753555297852]], "avg_entropy": [1.0408290196210146], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "August\u00a022,\u00a01980", "score_of_biggest_cluster": 0.4068281352519989, "score_of_first_answer": 0.4068281352519989, "max_score_of_first_answer": 0.9987342953681946, "min_score_of_first_answer": 0.03379777818918228, "avg_score": 0.4068281352519989, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3552430376492666, "std_of_first_answer": 0.3552430376492666, "avg_entropy_of_first_answer": 1.0408290196210146, "lex_sim": 1.0}
{"question": "active absorption of water from the soil by the root is mainly affected by", "answer": ["water potential and the availability of water in the soil"], "scores": [[0.10307081788778305, 0.8139124512672424, 0.05315563082695007, 0.11122078448534012, 0.017973866313695908, 0.9952980875968933, 0.9909331798553467, 0.307412713766098, 0.3917692303657532, 0.9104724526405334, 0.8704245686531067]], "normalized_score": [0.28029900789260864], "tokens": [["\u2581water", "\u2581potential", "\u2581and", "\u2581the", "\u2581avail", "ability", "\u2581of", "\u2581water", "\u2581in", "\u2581the", "\u2581soil"]], "entropy": [[4.014110565185547, 1.0499769449234009, 1.4102187156677246, 3.731905460357666, 4.582070350646973, 0.04074174538254738, 0.07463069260120392, 2.4729080200195312, 1.784081220626831, 0.47852087020874023, 0.7804217338562012]], "avg_entropy": [1.856326029043306], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "SURFACE AREA OF ROOTS", "score_of_biggest_cluster": 0.28029900789260864, "score_of_first_answer": 0.28029900789260864, "max_score_of_first_answer": 0.9952980875968933, "min_score_of_first_answer": 0.017973866313695908, "avg_score": 0.28029900789260864, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3907698425580291, "std_of_first_answer": 0.3907698425580291, "avg_entropy_of_first_answer": 1.856326029043306, "lex_sim": 1.0}
{"question": "who is the best director in bollywood 2017", "answer": ["Karan Johar"], "scores": [[0.04877730831503868, 0.9883660078048706, 0.9752210974693298, 0.998954176902771]], "normalized_score": [0.4655282497406006], "tokens": [["\u2581Kar", "an", "\u2581Joh", "ar"]], "entropy": [[3.8406529426574707, 0.0914740115404129, 0.18227985501289368, 0.011612620204687119]], "avg_entropy": [1.031504857353866], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nitesh Tiwari", "score_of_biggest_cluster": 0.4655282497406006, "score_of_first_answer": 0.4655282497406006, "max_score_of_first_answer": 0.998954176902771, "min_score_of_first_answer": 0.04877730831503868, "avg_score": 0.4655282497406006, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.406571738543974, "std_of_first_answer": 0.406571738543974, "avg_entropy_of_first_answer": 1.031504857353866, "lex_sim": 1.0}
{"question": "when was the last time a hurricane hit the uk", "answer": ["October 2017"], "scores": [[0.3344786763191223, 0.9889722466468811, 0.22904616594314575, 0.36249691247940063, 0.795009195804596, 0.3387146592140198]], "normalized_score": [0.44139862060546875], "tokens": [["\u2581October", "\u2581", "2", "0", "1", "7"]], "entropy": [[2.227193593978882, 0.07464706897735596, 0.8288586735725403, 1.9990453720092773, 0.6941046118736267, 1.471595048904419]], "avg_entropy": [1.2159073948860168], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "the autumn of 2017", "score_of_biggest_cluster": 0.44139862060546875, "score_of_first_answer": 0.44139862060546875, "max_score_of_first_answer": 0.9889722466468811, "min_score_of_first_answer": 0.22904616594314575, "avg_score": 0.44139862060546875, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2803159113763972, "std_of_first_answer": 0.2803159113763972, "avg_entropy_of_first_answer": 1.2159073948860168, "lex_sim": 1.0}
{"question": "the type of display that google glass presents to its users is called what", "answer": ["Heads-up display"], "scores": [[0.07241825759410858, 0.9990358352661133, 0.768215000629425, 0.8894737958908081, 0.9075338840484619]], "normalized_score": [0.5375038385391235], "tokens": [["\u2581He", "ads", "-", "up", "\u2581display"]], "entropy": [[4.04840612411499, 0.009575272910296917, 0.7363399863243103, 0.35570046305656433, 0.4398662745952606]], "avg_entropy": [1.1179776242002846], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "head-up display", "score_of_biggest_cluster": 0.5375038385391235, "score_of_first_answer": 0.5375038385391235, "max_score_of_first_answer": 0.9990358352661133, "min_score_of_first_answer": 0.07241825759410858, "avg_score": 0.5375038385391235, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.33560961987520954, "std_of_first_answer": 0.33560961987520954, "avg_entropy_of_first_answer": 1.1179776242002846, "lex_sim": 1.0}
{"question": "what is a another name for the water cycle", "answer": ["hydrological cycle"], "scores": [[0.4687863290309906, 0.2526770830154419, 0.9776131510734558, 0.988318145275116]], "normalized_score": [0.5816358327865601], "tokens": [["\u2581hydro", "log", "ical", "\u2581cycle"]], "entropy": [[1.9541345834732056, 0.6076892018318176, 0.11569327116012573, 0.09209751337766647]], "avg_entropy": [0.6924036424607038], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "the hydrological cycle", "score_of_biggest_cluster": 0.5816358327865601, "score_of_first_answer": 0.5816358327865601, "max_score_of_first_answer": 0.988318145275116, "min_score_of_first_answer": 0.2526770830154419, "avg_score": 0.5816358327865601, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.32038413674915067, "std_of_first_answer": 0.32038413674915067, "avg_entropy_of_first_answer": 0.6924036424607038, "lex_sim": 1.0}
{"question": "where does the highway of heroes start and end", "answer": ["Canadian Forces Base Trenton"], "scores": [[0.03638323396444321, 0.8223855495452881, 0.7376214265823364, 0.959881067276001, 0.9994680285453796, 0.9997913241386414]], "normalized_score": [0.5259579420089722], "tokens": [["\u2581Canadian", "\u2581Forces", "\u2581Base", "\u2581Tr", "ent", "on"]], "entropy": [[4.593835830688477, 0.9671590328216553, 1.6562719345092773, 0.29562699794769287, 0.005774389021098614, 0.0028709913603961468]], "avg_entropy": [1.2535898627247661], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Glen Miller Road in Trenton", "score_of_biggest_cluster": 0.5259579420089722, "score_of_first_answer": 0.5259579420089722, "max_score_of_first_answer": 0.9997913241386414, "min_score_of_first_answer": 0.03638323396444321, "avg_score": 0.5259579420089722, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33734450963856755, "std_of_first_answer": 0.33734450963856755, "avg_entropy_of_first_answer": 1.2535898627247661, "lex_sim": 1.0}
{"question": "what is an open university grade 3 pass", "answer": ["Lower second class degree"], "scores": [[0.0038760832976549864, 0.2069604992866516, 0.5762815475463867, 0.027446113526821136]], "normalized_score": [0.059682779014110565], "tokens": [["\u2581Lower", "\u2581second", "\u2581class", "\u2581degree"]], "entropy": [[3.3819832801818848, 0.8514604568481445, 1.4007458686828613, 1.4382284879684448]], "avg_entropy": [1.7681045234203339], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "55\u201369%", "score_of_biggest_cluster": 0.059682779014110565, "score_of_first_answer": 0.059682779014110565, "max_score_of_first_answer": 0.5762815475463867, "min_score_of_first_answer": 0.0038760832976549864, "avg_score": 0.059682779014110565, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2290320156198481, "std_of_first_answer": 0.2290320156198481, "avg_entropy_of_first_answer": 1.7681045234203339, "lex_sim": 1.0}
{"question": "how many episodes of the killing on netflix", "answer": ["39"], "scores": [[0.8960332870483398, 0.2727329432964325, 0.5556609034538269]], "normalized_score": [0.513992965221405], "tokens": [["\u2581", "3", "9"]], "entropy": [[0.7705762386322021, 1.4793790578842163, 1.7336760759353638]], "avg_entropy": [1.327877124150594], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "6", "score_of_biggest_cluster": 0.513992965221405, "score_of_first_answer": 0.513992965221405, "max_score_of_first_answer": 0.8960332870483398, "min_score_of_first_answer": 0.2727329432964325, "avg_score": 0.513992965221405, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2548212682284237, "std_of_first_answer": 0.2548212682284237, "avg_entropy_of_first_answer": 1.327877124150594, "lex_sim": 1.0}
{"question": "when was corporal punishment banned in south africa", "answer": ["February 19, 1997"], "scores": [[0.023499809205532074, 0.9897984266281128, 0.536485493183136, 0.1022224947810173, 0.2875592112541199, 0.9978321194648743, 0.8896961808204651, 0.9986109733581543, 0.991671621799469, 0.6154524683952332]], "normalized_score": [0.42635372281074524], "tokens": [["\u2581February", "\u2581", "1", "9", ",", "\u2581", "1", "9", "9", "7"]], "entropy": [[2.6191065311431885, 0.06759686022996902, 1.4531469345092773, 2.041717290878296, 0.750567615032196, 0.02304840460419655, 0.35219863057136536, 0.01289830170571804, 0.06329359114170074, 1.0446221828460693]], "avg_entropy": [0.8428196342661977], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "1997", "score_of_biggest_cluster": 0.42635372281074524, "score_of_first_answer": 0.42635372281074524, "max_score_of_first_answer": 0.9986109733581543, "min_score_of_first_answer": 0.023499809205532074, "avg_score": 0.42635372281074524, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3700982854414838, "std_of_first_answer": 0.3700982854414838, "avg_entropy_of_first_answer": 0.8428196342661977, "lex_sim": 1.0}
{"question": "when are the opening ceremonies for the 2018 olympic games", "answer": ["February 9"], "scores": [[0.7949061989784241, 0.9959483742713928, 0.8768010139465332]], "normalized_score": [0.8854238986968994], "tokens": [["\u2581February", "\u2581", "9"]], "entropy": [[1.1178081035614014, 0.03591625764966011, 0.5418913960456848]], "avg_entropy": [0.5652052524189154], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "9\u00a0February\u00a02018", "score_of_biggest_cluster": 0.8854238986968994, "score_of_first_answer": 0.8854238986968994, "max_score_of_first_answer": 0.9959483742713928, "min_score_of_first_answer": 0.7949061989784241, "avg_score": 0.8854238986968994, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0825434635931422, "std_of_first_answer": 0.0825434635931422, "avg_entropy_of_first_answer": 0.5652052524189154, "lex_sim": 1.0}
{"question": "when does part 5 of jojo take place", "answer": ["Italy under Nazi occupation (1938)"], "scores": [[0.015887724235653877, 0.003024469828233123, 0.10330035537481308, 0.9803943634033203, 0.4052581489086151, 0.03732532635331154, 0.68667072057724, 0.9909239411354065, 0.4189646244049072, 0.11297319829463959, 0.13794930279254913]], "normalized_score": [0.13730672001838684], "tokens": [["\u2581Italy", "\u2581under", "\u2581Naz", "i", "\u2581occupation", "\u2581(", "1", "9", "3", "8", ")"]], "entropy": [[3.0442862510681152, 1.8542168140411377, 2.717623710632324, 0.11946791410446167, 1.8347599506378174, 1.7363858222961426, 1.9330344200134277, 0.07441715151071548, 0.8474633693695068, 1.8471274375915527, 1.6468005180358887]], "avg_entropy": [1.6050530326637356], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The manga begins in 2001", "score_of_biggest_cluster": 0.13730672001838684, "score_of_first_answer": 0.13730672001838684, "max_score_of_first_answer": 0.9909239411354065, "min_score_of_first_answer": 0.003024469828233123, "avg_score": 0.13730672001838684, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35941469730722386, "std_of_first_answer": 0.35941469730722386, "avg_entropy_of_first_answer": 1.6050530326637356, "lex_sim": 1.0}
{"question": "the boards name on ed edd and eddy", "answer": ["Nazz"], "scores": [[0.004558640066534281, 0.9654427766799927]], "normalized_score": [0.06634082645177841], "tokens": [["\u2581N", "azz"]], "entropy": [[4.099181175231934, 0.31616294384002686]], "avg_entropy": [2.2076720595359802], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Plank", "score_of_biggest_cluster": 0.06634082645177841, "score_of_first_answer": 0.06634082645177841, "max_score_of_first_answer": 0.9654427766799927, "min_score_of_first_answer": 0.004558640066534281, "avg_score": 0.06634082645177841, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4804420683067292, "std_of_first_answer": 0.4804420683067292, "avg_entropy_of_first_answer": 2.2076720595359802, "lex_sim": 1.0}
{"question": "who plays jimmy's mom in 8 mile", "answer": ["Tony Scott"], "scores": [[0.040129244327545166, 0.2161872237920761]], "normalized_score": [0.09314200282096863], "tokens": [["\u2581Tony", "\u2581Scott"]], "entropy": [[4.236884593963623, 4.4163103103637695]], "avg_entropy": [4.326597452163696], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kim Basinger", "score_of_biggest_cluster": 0.09314200282096863, "score_of_first_answer": 0.09314200282096863, "max_score_of_first_answer": 0.2161872237920761, "min_score_of_first_answer": 0.040129244327545166, "avg_score": 0.09314200282096863, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08802898973226547, "std_of_first_answer": 0.08802898973226547, "avg_entropy_of_first_answer": 4.326597452163696, "lex_sim": 1.0}
{"question": "with a land area of 54 314 square miles where does wisconsin rank among the 50 states", "answer": ["24th"], "scores": [[0.8907278180122375, 0.7760429382324219, 0.1858297437429428, 0.7025204300880432]], "normalized_score": [0.5480892062187195], "tokens": [["\u2581", "2", "4", "th"]], "entropy": [[0.7904574275016785, 0.7849835157394409, 1.7810053825378418, 1.0272001028060913]], "avg_entropy": [1.0959116071462631], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "25", "score_of_biggest_cluster": 0.5480892062187195, "score_of_first_answer": 0.5480892062187195, "max_score_of_first_answer": 0.8907278180122375, "min_score_of_first_answer": 0.1858297437429428, "avg_score": 0.5480892062187195, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2699748109936608, "std_of_first_answer": 0.2699748109936608, "avg_entropy_of_first_answer": 1.0959116071462631, "lex_sim": 1.0}
{"question": "last world series that didn't go 7 games", "answer": ["1997 World Series"], "scores": [[0.886114239692688, 0.5250627398490906, 0.9969496130943298, 0.5041675567626953, 0.025304485112428665, 0.14371761679649353, 0.997713565826416]], "normalized_score": [0.36411410570144653], "tokens": [["\u2581", "1", "9", "9", "7", "\u2581World", "\u2581Series"]], "entropy": [[0.8596265912055969, 0.7262991666793823, 0.028795728459954262, 1.6714333295822144, 1.7389452457427979, 1.91087806224823, 0.021474123001098633]], "avg_entropy": [0.9939217495598963], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2015", "score_of_biggest_cluster": 0.36411410570144653, "score_of_first_answer": 0.36411410570144653, "max_score_of_first_answer": 0.997713565826416, "min_score_of_first_answer": 0.025304485112428665, "avg_score": 0.36411410570144653, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3681583299513341, "std_of_first_answer": 0.3681583299513341, "avg_entropy_of_first_answer": 0.9939217495598963, "lex_sim": 1.0}
{"question": "the first vice president of india who become the president letter was", "answer": ["V.V."], "scores": [[0.10406554490327835, 0.6842146515846252, 0.14915983378887177, 0.9419768452644348, 0.9817168116569519, 0.9985203146934509]], "normalized_score": [0.46265342831611633], "tokens": [["\u2581V", ".", "V", ".", "\u2581Gi", "ri"]], "entropy": [[3.570582866668701, 1.1006041765213013, 0.47981590032577515, 0.2473050057888031, 0.11591803282499313, 0.015861421823501587]], "avg_entropy": [0.9216812339921793], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Sarvepalli Radhakrishnan", "score_of_biggest_cluster": 0.46265342831611633, "score_of_first_answer": 0.46265342831611633, "max_score_of_first_answer": 0.9985203146934509, "min_score_of_first_answer": 0.10406554490327835, "avg_score": 0.46265342831611633, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3800301687297169, "std_of_first_answer": 0.3800301687297169, "avg_entropy_of_first_answer": 0.9216812339921793, "lex_sim": 1.0}
{"question": "genre of diary of a wimpy kid the getaway", "answer": ["comedy"], "scores": [[0.112857885658741]], "normalized_score": [0.1128578931093216], "tokens": [["\u2581comedy"]], "entropy": [[3.4526257514953613]], "avg_entropy": [3.4526257514953613], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Children's novel", "score_of_biggest_cluster": 0.1128578931093216, "score_of_first_answer": 0.1128578931093216, "max_score_of_first_answer": 0.112857885658741, "min_score_of_first_answer": 0.112857885658741, "avg_score": 0.1128578931093216, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.4526257514953613, "lex_sim": 1.0}
{"question": "which body part(s) occupy the greatest portion of the primary motor cortex", "answer": ["arm / hand / leg / foot"], "scores": [[0.0535251721739769, 0.0019889145623892546, 0.5794404149055481, 0.10165392607450485, 0.32708755135536194, 0.8735538125038147, 0.8801891803741455]], "normalized_score": [0.14829261600971222], "tokens": [["\u2581arm", "\u2581/", "\u2581hand", "\u2581/", "\u2581leg", "\u2581/", "\u2581foot"]], "entropy": [[3.4424819946289062, 1.4829009771347046, 1.8007681369781494, 1.0784717798233032, 2.7500462532043457, 0.4915391802787781, 0.7719271779060364]], "avg_entropy": [1.688305071422032], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "face", "score_of_biggest_cluster": 0.14829261600971222, "score_of_first_answer": 0.14829261600971222, "max_score_of_first_answer": 0.8801891803741455, "min_score_of_first_answer": 0.0019889145623892546, "avg_score": 0.14829261600971222, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3504638792957881, "std_of_first_answer": 0.3504638792957881, "avg_entropy_of_first_answer": 1.688305071422032, "lex_sim": 1.0}
{"question": "the removal of temperature in fire fighting method is known as", "answer": ["cooling"], "scores": [[0.3243389129638672, 0.9276989102363586]], "normalized_score": [0.5485333204269409], "tokens": [["\u2581cool", "ing"]], "entropy": [[3.7524874210357666, 0.47323572635650635]], "avg_entropy": [2.1128615736961365], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "cooling", "score_of_biggest_cluster": 0.5485333204269409, "score_of_first_answer": 0.5485333204269409, "max_score_of_first_answer": 0.9276989102363586, "min_score_of_first_answer": 0.3243389129638672, "avg_score": 0.5485333204269409, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3016799986362457, "std_of_first_answer": 0.3016799986362457, "avg_entropy_of_first_answer": 2.1128615736961365, "lex_sim": 1.0}
{"question": "who is jojo in horton hears a who", "answer": ["Zachary Gordon"], "scores": [[0.003674400271847844, 0.6810773015022278, 0.719884991645813]], "normalized_score": [0.1216789186000824], "tokens": [["\u2581Zach", "ary", "\u2581Gordon"]], "entropy": [[3.2099030017852783, 1.1385776996612549, 1.5013985633850098]], "avg_entropy": [1.9499597549438477], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jesse McCartney", "score_of_biggest_cluster": 0.1216789186000824, "score_of_first_answer": 0.1216789186000824, "max_score_of_first_answer": 0.719884991645813, "min_score_of_first_answer": 0.003674400271847844, "avg_score": 0.1216789186000824, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32885970304909684, "std_of_first_answer": 0.32885970304909684, "avg_entropy_of_first_answer": 1.9499597549438477, "lex_sim": 1.0}
{"question": "what are bulls used for on a farm", "answer": ["work"], "scores": [[0.011670367792248726]], "normalized_score": [0.011670369654893875], "tokens": [["\u2581work"]], "entropy": [[3.8517088890075684]], "avg_entropy": [3.8517088890075684], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "as work oxen", "score_of_biggest_cluster": 0.011670369654893875, "score_of_first_answer": 0.011670369654893875, "max_score_of_first_answer": 0.011670367792248726, "min_score_of_first_answer": 0.011670367792248726, "avg_score": 0.011670369654893875, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.8517088890075684, "lex_sim": 1.0}
{"question": "gabbar is back is a remake of which telugu movie", "answer": ["Sholay (Movie)"], "scores": [[0.5111453533172607, 0.8552369475364685, 0.9989597797393799, 0.04518440365791321, 0.005294006317853928, 0.9100780487060547]], "normalized_score": [0.2136346697807312], "tokens": [["\u2581Sh", "ol", "ay", "\u2581(", "Movie", ")"]], "entropy": [[2.883443832397461, 0.5776758193969727, 0.011885038577020168, 0.5027041435241699, 1.592519998550415, 0.5946730375289917]], "avg_entropy": [1.0271503116625051], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tagore", "score_of_biggest_cluster": 0.2136346697807312, "score_of_first_answer": 0.2136346697807312, "max_score_of_first_answer": 0.9989597797393799, "min_score_of_first_answer": 0.005294006317853928, "avg_score": 0.2136346697807312, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40359467693491463, "std_of_first_answer": 0.40359467693491463, "avg_entropy_of_first_answer": 1.0271503116625051, "lex_sim": 1.0}
{"question": "who is the originator of the plan-do-check-act model of performance improvement", "answer": ["William Edwards Deming"], "scores": [[0.48527494072914124, 0.9090393781661987, 0.9908417463302612, 0.9924070239067078]], "normalized_score": [0.8115516304969788], "tokens": [["\u2581William", "\u2581Edwards", "\u2581Dem", "ing"]], "entropy": [[2.3375720977783203, 0.5528031587600708, 0.07764624059200287, 0.054168276488780975]], "avg_entropy": [0.7555474434047937], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "W. Edwards Deming", "score_of_biggest_cluster": 0.8115516304969788, "score_of_first_answer": 0.8115516304969788, "max_score_of_first_answer": 0.9924070239067078, "min_score_of_first_answer": 0.48527494072914124, "avg_score": 0.8115516304969788, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.21005970654755815, "std_of_first_answer": 0.21005970654755815, "avg_entropy_of_first_answer": 0.7555474434047937, "lex_sim": 1.0}
{"question": "who is president of india in present time", "answer": ["Ram Nath Kovind"], "scores": [[0.8499730825424194, 0.9863837361335754, 0.9996914863586426, 0.9890856146812439, 0.9976964592933655, 0.9991289973258972]], "normalized_score": [0.9687130451202393], "tokens": [["\u2581Ram", "\u2581N", "ath", "\u2581K", "ov", "ind"]], "entropy": [[1.0383509397506714, 0.09853407740592957, 0.003869774052873254, 0.08671300858259201, 0.023802626878023148, 0.009657694026827812]], "avg_entropy": [0.21015468678281954], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Ram Nath Kovind", "score_of_biggest_cluster": 0.9687130451202393, "score_of_first_answer": 0.9687130451202393, "max_score_of_first_answer": 0.9996914863586426, "min_score_of_first_answer": 0.8499730825424194, "avg_score": 0.9687130451202393, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.05406120799202293, "std_of_first_answer": 0.05406120799202293, "avg_entropy_of_first_answer": 0.21015468678281954, "lex_sim": 1.0}
{"question": "who played emma in h2o just add water", "answer": ["Cariba Heine"], "scores": [[0.30808743834495544, 0.9994187355041504, 0.9972898960113525, 0.9978378415107727]], "normalized_score": [0.7440047860145569], "tokens": [["\u2581Car", "iba", "\u2581He", "ine"]], "entropy": [[2.484234094619751, 0.006263009272515774, 0.03337943181395531, 0.023954838514328003]], "avg_entropy": [0.6369578435551375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Claire Rhiannon Holt", "score_of_biggest_cluster": 0.7440047860145569, "score_of_first_answer": 0.7440047860145569, "max_score_of_first_answer": 0.9994187355041504, "min_score_of_first_answer": 0.30808743834495544, "avg_score": 0.7440047860145569, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2988208012526382, "std_of_first_answer": 0.2988208012526382, "avg_entropy_of_first_answer": 0.6369578435551375, "lex_sim": 1.0}
{"question": "where does water come from in new york city", "answer": ["Delaware Aqueduct"], "scores": [[0.07972520589828491, 0.9971326589584351, 0.4094623923301697, 0.9970690608024597, 0.9993821382522583]], "normalized_score": [0.5037370324134827], "tokens": [["\u2581Del", "aware", "\u2581Aqu", "ed", "uct"]], "entropy": [[3.9013113975524902, 0.029878392815589905, 2.181680917739868, 0.026263419538736343, 0.007282312028110027]], "avg_entropy": [1.229283287934959], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Catskill Aqueduct", "score_of_biggest_cluster": 0.5037370324134827, "score_of_first_answer": 0.5037370324134827, "max_score_of_first_answer": 0.9993821382522583, "min_score_of_first_answer": 0.07972520589828491, "avg_score": 0.5037370324134827, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3834738754425615, "std_of_first_answer": 0.3834738754425615, "avg_entropy_of_first_answer": 1.229283287934959, "lex_sim": 1.0}
{"question": "who does the voice of mrs. wolowitz on the big bang theory", "answer": ["Carol Ann Susi"], "scores": [[0.606269896030426, 0.9803802371025085, 0.9858778715133667, 0.994918167591095]], "normalized_score": [0.8738115429878235], "tokens": [["\u2581Carol", "\u2581Ann", "\u2581Sus", "i"]], "entropy": [[2.1254143714904785, 0.17982101440429688, 0.1202695220708847, 0.03757374733686447]], "avg_entropy": [0.6157696638256311], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Carol Ann Susi", "score_of_biggest_cluster": 0.8738115429878235, "score_of_first_answer": 0.8738115429878235, "max_score_of_first_answer": 0.994918167591095, "min_score_of_first_answer": 0.606269896030426, "avg_score": 0.8738115429878235, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.16496809231155157, "std_of_first_answer": 0.16496809231155157, "avg_entropy_of_first_answer": 0.6157696638256311, "lex_sim": 1.0}
{"question": "is there a name for the at symbol", "answer": ["the at sign or atmark"], "scores": [[0.03187688812613487, 0.47996172308921814, 0.5113332271575928, 0.07583140581846237, 0.32513952255249023, 0.03984411060810089]], "normalized_score": [0.14047908782958984], "tokens": [["\u2581the", "\u2581at", "\u2581sign", "\u2581or", "\u2581at", "mark"]], "entropy": [[3.0852713584899902, 2.8943710327148438, 1.257934808731079, 1.6468372344970703, 2.533411979675293, 2.625112533569336]], "avg_entropy": [2.3404898246129355], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "commercial at", "score_of_biggest_cluster": 0.14047908782958984, "score_of_first_answer": 0.14047908782958984, "max_score_of_first_answer": 0.5113332271575928, "min_score_of_first_answer": 0.03187688812613487, "avg_score": 0.14047908782958984, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.20358664372960705, "std_of_first_answer": 0.20358664372960705, "avg_entropy_of_first_answer": 2.3404898246129355, "lex_sim": 1.0}
{"question": "who does the voice of nelson on simpsons", "answer": ["The Nelson from Simpsons doesn't have the voice actor."], "scores": [[0.009419858455657959, 0.02140173502266407, 0.03972554951906204, 0.016279898583889008, 0.9642809629440308, 0.9998052716255188, 0.001959980931133032, 0.5978094935417175, 0.9997358918190002, 0.7060321569442749, 0.00989165622740984, 0.34625282883644104, 0.10088180750608444, 0.258514940738678]], "normalized_score": [0.09947212785482407], "tokens": [["\u2581The", "\u2581Nelson", "\u2581from", "\u2581Sim", "ps", "ons", "\u2581doesn", "'", "t", "\u2581have", "\u2581the", "\u2581voice", "\u2581actor", "."]], "entropy": [[3.3744821548461914, 2.472271680831909, 3.1350622177124023, 1.7921545505523682, 0.17283454537391663, 0.0025855861604213715, 2.5746779441833496, 0.7548278570175171, 0.003700464963912964, 1.632246971130371, 0.9980767965316772, 2.553510904312134, 3.1388864517211914, 3.4781477451324463]], "avg_entropy": [1.8631047050335579], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nancy Jean Cartwright", "score_of_biggest_cluster": 0.09947212785482407, "score_of_first_answer": 0.09947212785482407, "max_score_of_first_answer": 0.9998052716255188, "min_score_of_first_answer": 0.001959980931133032, "avg_score": 0.09947212785482407, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39157741263465135, "std_of_first_answer": 0.39157741263465135, "avg_entropy_of_first_answer": 1.8631047050335579, "lex_sim": 1.0}
{"question": "a town in west yorkshire on the river aire home to a rugby league team", "answer": ["Leeds"], "scores": [[0.4032701253890991, 0.9990106821060181]], "normalized_score": [0.6347213387489319], "tokens": [["\u2581Le", "eds"]], "entropy": [[1.6881276369094849, 0.010567260906100273]], "avg_entropy": [0.8493474489077926], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Castleford", "score_of_biggest_cluster": 0.6347213387489319, "score_of_first_answer": 0.6347213387489319, "max_score_of_first_answer": 0.9990106821060181, "min_score_of_first_answer": 0.4032701253890991, "avg_score": 0.6347213387489319, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2978702783584595, "std_of_first_answer": 0.2978702783584595, "avg_entropy_of_first_answer": 0.8493474489077926, "lex_sim": 1.0}
{"question": "who were farmers who kept a small portion of their crops & gave the rest to the landowners", "answer": ["sharecroppers"], "scores": [[0.12000834196805954, 0.9789853096008301, 0.7109974026679993]], "normalized_score": [0.43713799118995667], "tokens": [["\u2581share", "cro", "ppers"]], "entropy": [[2.5977988243103027, 0.13801831007003784, 0.6400558352470398]], "avg_entropy": [1.1252909898757935], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sharecropping", "score_of_biggest_cluster": 0.43713799118995667, "score_of_first_answer": 0.43713799118995667, "max_score_of_first_answer": 0.9789853096008301, "min_score_of_first_answer": 0.12000834196805954, "avg_score": 0.43713799118995667, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35884490909534633, "std_of_first_answer": 0.35884490909534633, "avg_entropy_of_first_answer": 1.1252909898757935, "lex_sim": 1.0}
{"question": "who won the medals in men's figure skating 2018", "answer": ["Yuzuru Hanyu"], "scores": [[0.629816472530365, 0.9997287392616272, 0.9945882558822632, 0.9921342134475708, 0.9947558641433716, 0.7698190212249756]], "normalized_score": [0.8835595846176147], "tokens": [["\u2581Y", "uz", "uru", "\u2581H", "any", "u"]], "entropy": [[2.3428900241851807, 0.0032500342931598425, 0.04216364771127701, 0.07358436286449432, 0.04572699964046478, 0.828559398651123]], "avg_entropy": [0.5560290778909499], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Yuzuru Hanyu", "score_of_biggest_cluster": 0.8835595846176147, "score_of_first_answer": 0.8835595846176147, "max_score_of_first_answer": 0.9997287392616272, "min_score_of_first_answer": 0.629816472530365, "avg_score": 0.8835595846176147, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.14505470493175604, "std_of_first_answer": 0.14505470493175604, "avg_entropy_of_first_answer": 0.5560290778909499, "lex_sim": 1.0}
{"question": "when was nepal declared a secular state in bs", "answer": ["November 4, 2018"], "scores": [[0.042111266404390335, 0.9860134720802307, 0.014301015064120293, 0.961523711681366, 0.9881523251533508, 0.9804779887199402, 0.9991672039031982, 0.04309545084834099, 0.2244037240743637]], "normalized_score": [0.2595486640930176], "tokens": [["\u2581November", "\u2581", "4", ",", "\u2581", "2", "0", "1", "8"]], "entropy": [[2.438615083694458, 0.0892602950334549, 1.637380599975586, 0.24722972512245178, 0.09557051211595535, 0.09920945763587952, 0.009364504367113113, 0.22491709887981415, 2.093843460083008]], "avg_entropy": [0.7705989707675245], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "January 15, 2007", "score_of_biggest_cluster": 0.2595486640930176, "score_of_first_answer": 0.2595486640930176, "max_score_of_first_answer": 0.9991672039031982, "min_score_of_first_answer": 0.014301015064120293, "avg_score": 0.2595486640930176, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45179801870505576, "std_of_first_answer": 0.45179801870505576, "avg_entropy_of_first_answer": 0.7705989707675245, "lex_sim": 1.0}
{"question": "who plays poppy in the beat goes on", "answer": ["Rachel Ray"], "scores": [[0.017539437860250473, 0.8572003245353699, 0.0019222341943532228]], "normalized_score": [0.030687961727380753], "tokens": [["\u2581Rach", "el", "\u2581Ray"]], "entropy": [[5.77056884765625, 0.5448917150497437, 4.236886024475098]], "avg_entropy": [3.5174488623936973], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Amanda Leighton", "score_of_biggest_cluster": 0.030687961727380753, "score_of_first_answer": 0.030687961727380753, "max_score_of_first_answer": 0.8572003245353699, "min_score_of_first_answer": 0.0019222341943532228, "avg_score": 0.030687961727380753, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39955182023525726, "std_of_first_answer": 0.39955182023525726, "avg_entropy_of_first_answer": 3.5174488623936973, "lex_sim": 1.0}
{"question": "where does the president get authority to issue executive orders", "answer": ["as Commander-in-Chief"], "scores": [[0.0016422119224444032, 0.05805739760398865, 0.24300527572631836, 0.9593022465705872, 0.9971989393234253, 0.9549447298049927, 0.9997972846031189]], "normalized_score": [0.21489118039608002], "tokens": [["\u2581as", "\u2581Commander", "-", "in", "-", "Ch", "ief"]], "entropy": [[3.1852850914001465, 3.1388821601867676, 0.6710874438285828, 0.18104328215122223, 0.022258572280406952, 0.1913813054561615, 0.0023632885422557592]], "avg_entropy": [1.0560430205493634], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Constitution", "score_of_biggest_cluster": 0.21489118039608002, "score_of_first_answer": 0.21489118039608002, "max_score_of_first_answer": 0.9997972846031189, "min_score_of_first_answer": 0.0016422119224444032, "avg_score": 0.21489118039608002, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4394542067902483, "std_of_first_answer": 0.4394542067902483, "avg_entropy_of_first_answer": 1.0560430205493634, "lex_sim": 1.0}
{"question": "the basic steps of rational decision making include defining goals and", "answer": ["generating alternatives"], "scores": [[0.027804000303149223, 0.6090691089630127]], "normalized_score": [0.13013285398483276], "tokens": [["\u2581generating", "\u2581alternatives"]], "entropy": [[4.9073805809021, 1.6185917854309082]], "avg_entropy": [3.262986183166504], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Implementation of plans/policies", "score_of_biggest_cluster": 0.13013285398483276, "score_of_first_answer": 0.13013285398483276, "max_score_of_first_answer": 0.6090691089630127, "min_score_of_first_answer": 0.027804000303149223, "avg_score": 0.13013285398483276, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29063255432993174, "std_of_first_answer": 0.29063255432993174, "avg_entropy_of_first_answer": 3.262986183166504, "lex_sim": 1.0}
{"question": "who said beware of the ides of march", "answer": ["soothsayer"], "scores": [[0.11142663657665253, 0.9975025057792664, 0.9912822246551514, 0.9997779726982117]], "normalized_score": [0.5761041641235352], "tokens": [["\u2581so", "oth", "s", "ayer"]], "entropy": [[3.335111618041992, 0.03217315673828125, 0.06524694710969925, 0.002944153267890215]], "avg_entropy": [0.8588689687894657], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "the soothsayer", "score_of_biggest_cluster": 0.5761041641235352, "score_of_first_answer": 0.5761041641235352, "max_score_of_first_answer": 0.9997779726982117, "min_score_of_first_answer": 0.11142663657665253, "avg_score": 0.5761041641235352, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.38312534222063516, "std_of_first_answer": 0.38312534222063516, "avg_entropy_of_first_answer": 0.8588689687894657, "lex_sim": 1.0}
{"question": "what type of planet is neptune known as", "answer": ["gas giant"], "scores": [[0.23768943548202515, 0.6618760228157043]], "normalized_score": [0.3966370224952698], "tokens": [["\u2581gas", "\u2581giant"]], "entropy": [[2.2469115257263184, 0.9736583232879639]], "avg_entropy": [1.6102849245071411], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "giant", "score_of_biggest_cluster": 0.3966370224952698, "score_of_first_answer": 0.3966370224952698, "max_score_of_first_answer": 0.6618760228157043, "min_score_of_first_answer": 0.23768943548202515, "avg_score": 0.3966370224952698, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2120932936668396, "std_of_first_answer": 0.2120932936668396, "avg_entropy_of_first_answer": 1.6102849245071411, "lex_sim": 1.0}
{"question": "who shot first in the shot heard around the world", "answer": ["The American Revolution in the UK"], "scores": [[0.04944824427366257, 0.04569875821471214, 0.24222305417060852, 0.0017344324151054025, 0.1099158301949501, 0.008129202760756016]], "normalized_score": [0.030767295509576797], "tokens": [["\u2581The", "\u2581American", "\u2581Revolution", "\u2581in", "\u2581the", "\u2581UK"]], "entropy": [[5.098267555236816, 3.5341033935546875, 3.4891457557678223, 1.701552152633667, 3.8312971591949463, 4.771871089935303]], "avg_entropy": [3.737706184387207], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Americans acting under orders", "score_of_biggest_cluster": 0.030767295509576797, "score_of_first_answer": 0.030767295509576797, "max_score_of_first_answer": 0.24222305417060852, "min_score_of_first_answer": 0.0017344324151054025, "avg_score": 0.030767295509576797, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08218388697976182, "std_of_first_answer": 0.08218388697976182, "avg_entropy_of_first_answer": 3.737706184387207, "lex_sim": 1.0}
{"question": "who sings in everybody dies in their nightmares", "answer": ["Matt Heafy and Travis Smith"], "scores": [[0.005818961188197136, 0.1875847727060318, 0.9722511172294617, 0.9974088072776794, 0.012069640681147575, 0.23101745545864105, 0.9964227080345154, 0.9905923008918762]], "normalized_score": [0.2032574713230133], "tokens": [["\u2581Matt", "\u2581He", "af", "y", "\u2581and", "\u2581Tra", "vis", "\u2581Smith"]], "entropy": [[6.0151190757751465, 4.383936882019043, 0.1884869933128357, 0.0198528990149498, 0.7139790654182434, 3.3023247718811035, 0.03590458631515503, 0.08433413505554199]], "avg_entropy": [1.8429923010990024], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "XXXTentacion", "score_of_biggest_cluster": 0.2032574713230133, "score_of_first_answer": 0.2032574713230133, "max_score_of_first_answer": 0.9974088072776794, "min_score_of_first_answer": 0.005818961188197136, "avg_score": 0.2032574713230133, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4458806389484229, "std_of_first_answer": 0.4458806389484229, "avg_entropy_of_first_answer": 1.8429923010990024, "lex_sim": 1.0}
{"question": "what role does the president play in foreign affairs", "answer": ["The president is responsible for foreign policy negotiations and foreign treaty commitments.\u30c4\u30a4\u30f3\u30fb\u30aa\u30fc\u30eb\u30fb\u30c9\u30ea\u30fc\u30de\u30fc\u30ba\u3000Sisters"], "scores": [[0.21936722099781036, 0.29622960090637207, 0.2798820436000824, 0.1261155903339386, 0.9715495109558105, 0.10020346939563751, 0.5505720973014832, 0.0032340253237634897, 0.9926882982254028, 0.19813242554664612, 0.017155418172478676, 0.025704527273774147, 0.15344242751598358, 0.01068040356040001, 0.9940242171287537, 0.6384259462356567, 5.71288687467586e-08, 6.725746061420068e-05, 0.14805865287780762, 0.012362144887447357, 0.014307103119790554, 0.3210427761077881, 0.29787951707839966, 0.16564281284809113, 0.03680691123008728, 0.08595702797174454, 0.7220278382301331, 0.030733592808246613, 0.8171948194503784, 0.19547408819198608, 0.007525284308940172, 0.002766881138086319, 0.005733146332204342, 0.5124219059944153]], "normalized_score": [0.05288548022508621], "tokens": [["\u2581The", "\u2581president", "\u2581is", "\u2581responsible", "\u2581for", "\u2581foreign", "\u2581policy", "\u2581negoti", "ations", "\u2581and", "\u2581foreign", "\u2581treat", "y", "\u2581commit", "ments", ".", "\u30c4", "\u30a4", "\u30f3", "\u30fb", "\u30aa", "\u30fc", "\u30eb", "\u30fb", "\u30c9", "\u30ea", "\u30fc", "\u30de", "\u30fc", "\u30ba", "\u3000", "S", "ister", "s"]], "entropy": [[4.281165599822998, 1.8229771852493286, 3.1858954429626465, 1.9841662645339966, 0.21034914255142212, 3.2365224361419678, 1.289849042892456, 2.8049750328063965, 0.053605884313583374, 2.096863031387329, 4.367459774017334, 2.171708583831787, 0.4654696583747864, 3.386492967605591, 0.04140808433294296, 1.5523736476898193, 1.5414001941680908, 1.1809155941009521, 2.3897743225097656, 5.135807514190674, 4.022500038146973, 2.3823447227478027, 2.8676419258117676, 4.4591240882873535, 3.496527910232544, 2.92311954498291, 1.553407907485962, 2.670095682144165, 1.3712012767791748, 4.452810287475586, 4.817755699157715, 6.013589859008789, 5.909580230712891, 3.1917476654052734]], "avg_entropy": [2.745018418878317], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "negotiates treaties with foreign nations", "score_of_biggest_cluster": 0.05288548022508621, "score_of_first_answer": 0.05288548022508621, "max_score_of_first_answer": 0.9940242171287537, "min_score_of_first_answer": 5.71288687467586e-08, "avg_score": 0.05288548022508621, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31228020256947364, "std_of_first_answer": 0.31228020256947364, "avg_entropy_of_first_answer": 2.745018418878317, "lex_sim": 1.0}
{"question": "when is the new tappan zee bridge going to be finished", "answer": ["December 10, 2018"], "scores": [[0.08236953616142273, 0.9648312926292419, 0.5470236539840698, 0.10040629655122757, 0.9580137729644775, 0.9978711605072021, 0.9991939663887024, 0.9998038411140442, 0.9637788534164429, 0.7896579504013062]], "normalized_score": [0.5625241994857788], "tokens": [["\u2581December", "\u2581", "1", "0", ",", "\u2581", "2", "0", "1", "8"]], "entropy": [[3.051753520965576, 0.19399061799049377, 1.149065613746643, 1.9434715509414673, 0.24437472224235535, 0.018809674307703972, 0.0072853141464293, 0.002241282258182764, 0.1735769808292389, 0.7779752016067505]], "avg_entropy": [0.7562544479034841], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "by June 15, 2018", "score_of_biggest_cluster": 0.5625241994857788, "score_of_first_answer": 0.5625241994857788, "max_score_of_first_answer": 0.9998038411140442, "min_score_of_first_answer": 0.08236953616142273, "avg_score": 0.5625241994857788, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3507985202724646, "std_of_first_answer": 0.3507985202724646, "avg_entropy_of_first_answer": 0.7562544479034841, "lex_sim": 1.0}
{"question": "when was the last time america hosted the summer olympics", "answer": ["Summer Olympic Games of 2004 in Athens"], "scores": [[0.010543595999479294, 0.16244971752166748, 0.8168587684631348, 0.08529164642095566, 0.9516627192497253, 0.08749505132436752, 0.9947020411491394, 0.4890349805355072, 0.20440822839736938, 0.20409579575061798, 0.9533562064170837, 0.9995470643043518]], "normalized_score": [0.275615930557251], "tokens": [["\u2581Summer", "\u2581Olympic", "\u2581Games", "\u2581of", "\u2581", "2", "0", "0", "4", "\u2581in", "\u2581Ath", "ens"]], "entropy": [[1.6789534091949463, 1.262156367301941, 1.1749587059020996, 2.945767402648926, 0.28767961263656616, 0.3045582175254822, 0.04763099551200867, 0.9615260362625122, 1.2832434177398682, 1.8747766017913818, 0.37194621562957764, 0.005718223750591278]], "avg_entropy": [1.0165762671579917], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Salt Lake City", "score_of_biggest_cluster": 0.275615930557251, "score_of_first_answer": 0.275615930557251, "max_score_of_first_answer": 0.9995470643043518, "min_score_of_first_answer": 0.010543595999479294, "avg_score": 0.275615930557251, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39526109593356956, "std_of_first_answer": 0.39526109593356956, "avg_entropy_of_first_answer": 1.0165762671579917, "lex_sim": 1.0}
{"question": "when is the new season of wentworth coming out", "answer": ["August 21, 2018"], "scores": [[0.1141057088971138, 0.9714877009391785, 0.40428081154823303, 0.04766212776303291, 0.8101823925971985, 0.996932864189148, 0.9989134073257446, 0.9998688697814941, 0.995293915271759, 0.7142627835273743]], "normalized_score": [0.5114615559577942], "tokens": [["\u2581August", "\u2581", "2", "1", ",", "\u2581", "2", "0", "1", "8"]], "entropy": [[3.532687187194824, 0.1792665421962738, 1.5002260208129883, 1.7754398584365845, 0.6733473539352417, 0.027281299233436584, 0.009832163341343403, 0.0016340066213160753, 0.03367496281862259, 0.921872615814209]], "avg_entropy": [0.8655262010404841], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "19 June 2018", "score_of_biggest_cluster": 0.5114615559577942, "score_of_first_answer": 0.5114615559577942, "max_score_of_first_answer": 0.9998688697814941, "min_score_of_first_answer": 0.04766212776303291, "avg_score": 0.5114615559577942, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.360093488265568, "std_of_first_answer": 0.360093488265568, "avg_entropy_of_first_answer": 0.8655262010404841, "lex_sim": 1.0}
{"question": "what part of brain is responsible for complex thinking", "answer": ["prefrontal cortex"], "scores": [[0.0415102019906044, 0.8928425908088684, 0.9993138313293457, 0.9473386406898499, 0.9967911839485168]], "normalized_score": [0.5113852620124817], "tokens": [["\u2581pre", "front", "al", "\u2581cor", "tex"]], "entropy": [[2.6678988933563232, 0.48599231243133545, 0.008295143954455853, 0.33022934198379517, 0.02359630912542343]], "avg_entropy": [0.7032024001702666], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the frontal lobe", "score_of_biggest_cluster": 0.5113852620124817, "score_of_first_answer": 0.5113852620124817, "max_score_of_first_answer": 0.9993138313293457, "min_score_of_first_answer": 0.0415102019906044, "avg_score": 0.5113852620124817, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3690802696555396, "std_of_first_answer": 0.3690802696555396, "avg_entropy_of_first_answer": 0.7032024001702666, "lex_sim": 1.0}
{"question": "who has won the cma entertainer of the year the most", "answer": ["Garth Brooks"], "scores": [[0.8485047221183777, 0.9996747970581055, 0.9872512817382812, 0.9989122152328491]], "normalized_score": [0.9563503265380859], "tokens": [["\u2581Gar", "th", "\u2581Bro", "oks"]], "entropy": [[0.9951422810554504, 0.004217539913952351, 0.09669236838817596, 0.01222938671708107]], "avg_entropy": [0.27707039401866496], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Garth Brooks", "score_of_biggest_cluster": 0.9563503265380859, "score_of_first_answer": 0.9563503265380859, "max_score_of_first_answer": 0.9996747970581055, "min_score_of_first_answer": 0.8485047221183777, "avg_score": 0.9563503265380859, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06374574298655607, "std_of_first_answer": 0.06374574298655607, "avg_entropy_of_first_answer": 0.27707039401866496, "lex_sim": 1.0}
{"question": "the fellowship of the ring director's cut length", "answer": ["294 minutes"], "scores": [[0.9720384478569031, 0.4775458574295044, 0.0037700114771723747, 0.0849703773856163, 0.5118871927261353]], "normalized_score": [0.1500709354877472], "tokens": [["\u2581", "2", "9", "4", "\u2581minutes"]], "entropy": [[0.2647242546081543, 1.2808632850646973, 1.4050720930099487, 2.544326066970825, 1.7161693572998047]], "avg_entropy": [1.442231011390686], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "228 minutes", "score_of_biggest_cluster": 0.1500709354877472, "score_of_first_answer": 0.1500709354877472, "max_score_of_first_answer": 0.9720384478569031, "min_score_of_first_answer": 0.0037700114771723747, "avg_score": 0.1500709354877472, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3468422436612334, "std_of_first_answer": 0.3468422436612334, "avg_entropy_of_first_answer": 1.442231011390686, "lex_sim": 1.0}
{"question": "what is a buckeye the mascot for ohio state university", "answer": ["\"The Ohio State Buckeyes\""], "scores": [[0.0018742409301921725, 0.1741459220647812, 0.5128100514411926, 0.9622071981430054, 0.4274244010448456, 0.9985108971595764, 0.9697264432907104, 0.5428749322891235]], "normalized_score": [0.27849385142326355], "tokens": [["\u2581\"", "The", "\u2581Ohio", "\u2581State", "\u2581Buck", "ey", "es", "\""]], "entropy": [[3.079275131225586, 1.8467637300491333, 1.6446237564086914, 0.25674158334732056, 0.8205261826515198, 0.015829414129257202, 0.13638003170490265, 1.7505161762237549]], "avg_entropy": [1.1938320007175207], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Anthropomorphic buckeye nut", "score_of_biggest_cluster": 0.27849385142326355, "score_of_first_answer": 0.27849385142326355, "max_score_of_first_answer": 0.9985108971595764, "min_score_of_first_answer": 0.0018742409301921725, "avg_score": 0.27849385142326355, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3537162645523584, "std_of_first_answer": 0.3537162645523584, "avg_entropy_of_first_answer": 1.1938320007175207, "lex_sim": 1.0}
{"question": "what is the current rate of interest on ppf", "answer": ["7.9 percent"], "scores": [[0.9437695741653442, 0.45749592781066895, 0.9905416369438171, 0.3909396231174469, 0.018408169969916344]], "normalized_score": [0.3145206868648529], "tokens": [["\u2581", "7", ".", "9", "\u2581percent"]], "entropy": [[0.4911481738090515, 0.7918057441711426, 0.0771617442369461, 0.9561560153961182, 0.4850638210773468]], "avg_entropy": [0.560267099738121], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "7.6% Per Annum", "score_of_biggest_cluster": 0.3145206868648529, "score_of_first_answer": 0.3145206868648529, "max_score_of_first_answer": 0.9905416369438171, "min_score_of_first_answer": 0.018408169969916344, "avg_score": 0.3145206868648529, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36470676621804665, "std_of_first_answer": 0.36470676621804665, "avg_entropy_of_first_answer": 0.560267099738121, "lex_sim": 1.0}
{"question": "architectural elements forming rib vaults eg wells cathedral", "answer": ["flying buttresses"], "scores": [[0.23191916942596436, 0.9783194661140442, 0.9919672608375549, 0.4840148985385895]], "normalized_score": [0.5745043754577637], "tokens": [["\u2581flying", "\u2581but", "tr", "esses"]], "entropy": [[4.4227142333984375, 0.15298664569854736, 0.056828971952199936, 0.7006422281265259]], "avg_entropy": [1.3332930197939277], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "an armature of piped masonry", "score_of_biggest_cluster": 0.5745043754577637, "score_of_first_answer": 0.5745043754577637, "max_score_of_first_answer": 0.9919672608375549, "min_score_of_first_answer": 0.23191916942596436, "avg_score": 0.5745043754577637, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3260442484333178, "std_of_first_answer": 0.3260442484333178, "avg_entropy_of_first_answer": 1.3332930197939277, "lex_sim": 1.0}
{"question": "los angeles stadium at hollywood park opening date", "answer": ["2020"], "scores": [[0.39152291417121887, 0.9368832111358643, 0.983526885509491, 0.9004851579666138, 0.9822959303855896]], "normalized_score": [0.7957736849784851], "tokens": [["\u2581", "2", "0", "2", "0"]], "entropy": [[2.7922446727752686, 0.3500770330429077, 0.12191645801067352, 0.33708199858665466, 0.11669863760471344]], "avg_entropy": [0.7436037600040436], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "2020", "score_of_biggest_cluster": 0.7957736849784851, "score_of_first_answer": 0.7957736849784851, "max_score_of_first_answer": 0.983526885509491, "min_score_of_first_answer": 0.39152291417121887, "avg_score": 0.7957736849784851, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.22584021695285106, "std_of_first_answer": 0.22584021695285106, "avg_entropy_of_first_answer": 0.7436037600040436, "lex_sim": 1.0}
{"question": "what does g stand for in ncis los angeles", "answer": ["the Office of Special Projects"], "scores": [[0.008036164566874504, 0.048041511327028275, 0.9482113718986511, 0.9434543251991272, 0.9889892935752869, 0.9956041574478149]], "normalized_score": [0.2641972303390503], "tokens": [["\u2581the", "\u2581Office", "\u2581of", "\u2581Special", "\u2581Project", "s"]], "entropy": [[3.8224356174468994, 5.576988220214844, 0.399949848651886, 0.4413847327232361, 0.07379641383886337, 0.04000289738178253]], "avg_entropy": [1.7257596217095852], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Grisha", "score_of_biggest_cluster": 0.2641972303390503, "score_of_first_answer": 0.2641972303390503, "max_score_of_first_answer": 0.9956041574478149, "min_score_of_first_answer": 0.008036164566874504, "avg_score": 0.2641972303390503, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4441656499994369, "std_of_first_answer": 0.4441656499994369, "avg_entropy_of_first_answer": 1.7257596217095852, "lex_sim": 1.0}
{"question": "what time do tam tams start in montreal", "answer": ["2 to 5"], "scores": [[0.6607205867767334, 0.0555068701505661, 0.003300972981378436, 0.9956117272377014, 0.4486835300922394]], "normalized_score": [0.14015458524227142], "tokens": [["\u2581", "2", "\u2581to", "\u2581", "5"]], "entropy": [[2.1744730472564697, 2.0137197971343994, 2.1000354290008545, 0.04928424581885338, 1.7435848712921143]], "avg_entropy": [1.6162194781005383], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "around 10:30am", "score_of_biggest_cluster": 0.14015458524227142, "score_of_first_answer": 0.14015458524227142, "max_score_of_first_answer": 0.9956117272377014, "min_score_of_first_answer": 0.003300972981378436, "avg_score": 0.14015458524227142, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37303528909351535, "std_of_first_answer": 0.37303528909351535, "avg_entropy_of_first_answer": 1.6162194781005383, "lex_sim": 1.0}
{"question": "when did the isle of wight become an island", "answer": ["the Pleistocene epoch"], "scores": [[0.012771617621183395, 0.01611212268471718, 0.9987348914146423, 0.9982078075408936, 0.9976556301116943, 0.2888442277908325, 0.9993054866790771]], "normalized_score": [0.24883762001991272], "tokens": [["\u2581the", "\u2581Ple", "ist", "oc", "ene", "\u2581epo", "ch"]], "entropy": [[3.622166633605957, 3.8784358501434326, 0.012559376657009125, 0.01763002574443817, 0.02195010520517826, 2.357240676879883, 0.007131690625101328]], "avg_entropy": [1.4167306226944285], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "During the last Ice Age", "score_of_biggest_cluster": 0.24883762001991272, "score_of_first_answer": 0.24883762001991272, "max_score_of_first_answer": 0.9993054866790771, "min_score_of_first_answer": 0.012771617621183395, "avg_score": 0.24883762001991272, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4497513198493442, "std_of_first_answer": 0.4497513198493442, "avg_entropy_of_first_answer": 1.4167306226944285, "lex_sim": 1.0}
{"question": "how old is baby in dirty dancing movie", "answer": ["18"], "scores": [[0.6601850986480713, 0.9107739925384521, 0.09768622368574142]], "normalized_score": [0.38871970772743225], "tokens": [["\u2581", "1", "8"]], "entropy": [[2.192141532897949, 0.47156378626823425, 1.5492768287658691]], "avg_entropy": [1.4043273826440175], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "17-year-old", "score_of_biggest_cluster": 0.38871970772743225, "score_of_first_answer": 0.38871970772743225, "max_score_of_first_answer": 0.9107739925384521, "min_score_of_first_answer": 0.09768622368574142, "avg_score": 0.38871970772743225, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3399855385703879, "std_of_first_answer": 0.3399855385703879, "avg_entropy_of_first_answer": 1.4043273826440175, "lex_sim": 1.0}
{"question": "when did little polveir win the grand national", "answer": ["April 1929"], "scores": [[0.5036485195159912, 0.9950358271598816, 0.24883902072906494, 0.07626145333051682, 0.008304396644234657, 0.14759552478790283]], "normalized_score": [0.15057812631130219], "tokens": [["\u2581April", "\u2581", "1", "9", "2", "9"]], "entropy": [[1.2872602939605713, 0.037837568670511246, 2.1075589656829834, 2.289889097213745, 1.5366629362106323, 2.2563633918762207]], "avg_entropy": [1.5859287089357774], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1989", "score_of_biggest_cluster": 0.15057812631130219, "score_of_first_answer": 0.15057812631130219, "max_score_of_first_answer": 0.9950358271598816, "min_score_of_first_answer": 0.008304396644234657, "avg_score": 0.15057812631130219, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3366888404223622, "std_of_first_answer": 0.3366888404223622, "avg_entropy_of_first_answer": 1.5859287089357774, "lex_sim": 1.0}
{"question": "what nba player has scored the most 3 pointers", "answer": ["Stephen Curry"], "scores": [[0.4944717288017273, 0.8965283036231995, 0.9997482895851135]], "normalized_score": [0.762427806854248], "tokens": [["\u2581Stephen", "\u2581Cur", "ry"]], "entropy": [[1.4723678827285767, 0.7545326352119446, 0.003170907963067293]], "avg_entropy": [0.7433571419678628], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ray Allen", "score_of_biggest_cluster": 0.762427806854248, "score_of_first_answer": 0.762427806854248, "max_score_of_first_answer": 0.9997482895851135, "min_score_of_first_answer": 0.4944717288017273, "avg_score": 0.762427806854248, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21797254101689018, "std_of_first_answer": 0.21797254101689018, "avg_entropy_of_first_answer": 0.7433571419678628, "lex_sim": 1.0}
{"question": "where does route 66 start on the west coast", "answer": ["Los Angeles, California"], "scores": [[0.26787298917770386, 0.9920284152030945, 0.4340401589870453, 0.7119655609130859]], "normalized_score": [0.5353163480758667], "tokens": [["\u2581Los", "\u2581Angeles", ",", "\u2581California"]], "entropy": [[3.9732778072357178, 0.06487268954515457, 0.9998111128807068, 0.7618671655654907]], "avg_entropy": [1.4499571938067675], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in Santa Monica", "score_of_biggest_cluster": 0.5353163480758667, "score_of_first_answer": 0.5353163480758667, "max_score_of_first_answer": 0.9920284152030945, "min_score_of_first_answer": 0.26787298917770386, "avg_score": 0.5353163480758667, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.275710383271429, "std_of_first_answer": 0.275710383271429, "avg_entropy_of_first_answer": 1.4499571938067675, "lex_sim": 1.0}
{"question": "what is a home run with a girl", "answer": ["A girl who is \"home run\" is very attractive, and it's considered a major feat to accomplish having such a girl."], "scores": [[0.09765969216823578, 0.040805164724588394, 0.34136566519737244, 0.23339472711086273, 0.006601665634661913, 0.14288724958896637, 0.5506397485733032, 0.839428186416626, 0.4224347472190857, 0.06766378879547119, 0.644341766834259, 0.9987800717353821, 0.13450688123703003, 0.19105038046836853, 0.03265201672911644, 0.2349039763212204, 0.9950634837150574, 0.036891356110572815, 0.6301590800285339, 0.059164393693208694, 0.048787474632263184, 0.9760288596153259, 0.5829499363899231, 0.0025383527390658855, 0.08663566410541534, 0.002543872920796275, 0.835302472114563, 0.5859706401824951, 0.2829318642616272, 0.001814187620766461, 0.016272064298391342, 0.21363364160060883, 0.27183452248573303, 0.1720184087753296, 0.9721058011054993, 0.5639759302139282, 0.015040443278849125, 0.610526978969574, 0.9935646653175354, 0.03153543546795845, 0.09349443763494492, 0.9987389445304871, 0.06330493837594986, 0.7814826965332031, 0.3162761628627777, 0.6027834415435791, 0.022813230752944946, 0.21843221783638, 0.4343758523464203, 0.49164214730262756]], "normalized_score": [0.15710078179836273], "tokens": [["\u2581A", "\u2581girl", "\u2581who", "\u2581is", "\u2581\"", "home", "\u2581run", "\"", "\u2581is", "\u2581very", "\u2581attract", "ive", ",", "\u2581and", "\u2581it", "'", "s", "\u2581considered", "\u2581a", "\u2581major", "\u2581fe", "at", "\u2581to", "\u2581accomplish", "\u2581having", "\u2581such", "\u2581a", "\u2581girl", ".", "\u2581Getting", "\u2581one", "\u2581is", "\u2581a", "\u2581home", "\u2581run", ".", "\u2581It", "'", "s", "\u2581an", "\u2581anal", "ogy", "\u2581from", "\u2581baseball", ",", "\u2581where", "\u2581you", "\u2581get", "\u2581a", "\u2581home"]], "entropy": [[5.350968360900879, 4.748032569885254, 3.3783164024353027, 3.747586488723755, 4.945703029632568, 4.231023788452148, 1.4027228355407715, 0.9023560285568237, 1.9840196371078491, 3.491443634033203, 1.5816662311553955, 0.012470431625843048, 1.9296256303787231, 4.199594974517822, 4.559564590454102, 2.2024941444396973, 0.035010989755392075, 3.8632636070251465, 1.7966105937957764, 3.2756059169769287, 2.0054287910461426, 0.13747349381446838, 1.5691664218902588, 2.9029808044433594, 3.3962249755859375, 1.2167456150054932, 0.8951476216316223, 1.9920998811721802, 2.967500686645508, 2.2062039375305176, 2.084991216659546, 3.1764512062072754, 3.566340446472168, 2.9797306060791016, 0.1795634627342224, 1.9028635025024414, 2.2498302459716797, 1.9534484148025513, 0.04401280730962753, 4.115300178527832, 3.7493343353271484, 0.013199418783187866, 2.298583507537842, 1.1370294094085693, 1.9992135763168335, 2.1271045207977295, 1.9133437871932983, 2.897869348526001, 2.1055197715759277, 2.024116039276123]], "avg_entropy": [2.3888979583233594], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\"full\" sexual intercourse", "score_of_biggest_cluster": 0.15710078179836273, "score_of_first_answer": 0.15710078179836273, "max_score_of_first_answer": 0.9987800717353821, "min_score_of_first_answer": 0.001814187620766461, "avg_score": 0.15710078179836273, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3345637764571545, "std_of_first_answer": 0.3345637764571545, "avg_entropy_of_first_answer": 2.3888979583233594, "lex_sim": 1.0}
{"question": "who won the battle of saratoga in 1777", "answer": ["General John Burgoyne"], "scores": [[0.044694434851408005, 0.5913618803024292, 0.9748677015304565, 0.9991663694381714, 0.9976254105567932, 0.9998351335525513]], "normalized_score": [0.5431640148162842], "tokens": [["\u2581General", "\u2581John", "\u2581Bur", "go", "y", "ne"]], "entropy": [[3.512084484100342, 1.3372364044189453, 0.21931079030036926, 0.01059371791779995, 0.021333130076527596, 0.0021609098184853792]], "avg_entropy": [0.8504532394387448], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Americans", "score_of_biggest_cluster": 0.5431640148162842, "score_of_first_answer": 0.5431640148162842, "max_score_of_first_answer": 0.9998351335525513, "min_score_of_first_answer": 0.044694434851408005, "avg_score": 0.5431640148162842, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35521810832994616, "std_of_first_answer": 0.35521810832994616, "avg_entropy_of_first_answer": 0.8504532394387448, "lex_sim": 1.0}
{"question": "what is the origin of the name marvin", "answer": ["1. it is a name of a boy or baby"], "scores": [[0.003979266621172428, 0.58548504114151, 0.4855538606643677, 0.009254489094018936, 0.5034295320510864, 0.378652960062027, 0.03682221099734306, 0.20672650635242462, 0.11739332228899002, 0.01052101794630289, 0.17755623161792755, 0.016750987619161606]], "normalized_score": [0.07862415909767151], "tokens": [["\u2581", "1", ".", "\u2581it", "\u2581is", "\u2581a", "\u2581name", "\u2581of", "\u2581a", "\u2581boy", "\u2581or", "\u2581baby"]], "entropy": [[4.715378761291504, 2.1857247352600098, 2.348095417022705, 5.375276565551758, 2.065044403076172, 2.8226773738861084, 4.778929710388184, 3.2493860721588135, 3.9736077785491943, 5.501494407653809, 3.227231740951538, 1.8166210651397705]], "avg_entropy": [3.5049556692441306], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Welsh", "score_of_biggest_cluster": 0.07862415909767151, "score_of_first_answer": 0.07862415909767151, "max_score_of_first_answer": 0.58548504114151, "min_score_of_first_answer": 0.003979266621172428, "avg_score": 0.07862415909767151, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2103492354648571, "std_of_first_answer": 0.2103492354648571, "avg_entropy_of_first_answer": 3.5049556692441306, "lex_sim": 1.0}
{"question": "who does the democratic republic of congo trade with", "answer": ["Belgium-Luxembourg, France, Germany, Italy, Pakistan, Russia, Saudi Arabia, South Africa"], "scores": [[0.18708975613117218, 0.9959897398948669, 0.08081211894750595, 0.9971703886985779, 0.9986613988876343, 0.9977123737335205, 0.9970309734344482, 0.4905514419078827, 0.3571521043777466, 0.9134355783462524, 0.36111924052238464, 0.948147177696228, 0.07196202874183655, 0.9835029244422913, 0.0043646348640322685, 0.983954668045044, 0.11439405381679535, 0.9687909483909607, 0.16249258816242218, 0.9974918365478516, 0.9888566136360168, 0.9993701577186584, 0.9792769551277161, 0.3920535147190094, 0.7083156108856201]], "normalized_score": [0.4397011995315552], "tokens": [["\u2581Belg", "ium", "-", "L", "ux", "emb", "ourg", ",", "\u2581France", ",", "\u2581Germany", ",", "\u2581Italy", ",", "\u2581Pakistan", ",", "\u2581Russia", ",", "\u2581Sa", "udi", "\u2581Arab", "ia", ",", "\u2581South", "\u2581Africa"]], "entropy": [[3.488889455795288, 0.03031345084309578, 1.7057421207427979, 0.03008943423628807, 0.01198183186352253, 0.02336246892809868, 0.024273507297039032, 1.2877719402313232, 2.2942185401916504, 0.4099404215812683, 2.5481443405151367, 0.25970613956451416, 3.17996883392334, 0.10383687913417816, 2.5588669776916504, 0.10558931529521942, 3.0557162761688232, 0.18030816316604614, 2.9325976371765137, 0.02208588272333145, 0.08836747705936432, 0.006561501417309046, 0.13298656046390533, 2.432938575744629, 0.6128364205360413]], "avg_entropy": [1.101083766091615], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Group of 77", "score_of_biggest_cluster": 0.4397011995315552, "score_of_first_answer": 0.4397011995315552, "max_score_of_first_answer": 0.9993701577186584, "min_score_of_first_answer": 0.0043646348640322685, "avg_score": 0.4397011995315552, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.380674050763643, "std_of_first_answer": 0.380674050763643, "avg_entropy_of_first_answer": 1.101083766091615, "lex_sim": 1.0}
{"question": "who played the first nfl thursday night football game", "answer": ["Houston Texas"], "scores": [[0.028705855831503868, 0.009202149696648121]], "normalized_score": [0.01625286042690277], "tokens": [["\u2581Houston", "\u2581Texas"]], "entropy": [[4.701957702636719, 0.5050615072250366]], "avg_entropy": [2.6035096049308777], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Denver Broncos", "score_of_biggest_cluster": 0.01625286042690277, "score_of_first_answer": 0.01625286042690277, "max_score_of_first_answer": 0.028705855831503868, "min_score_of_first_answer": 0.009202149696648121, "avg_score": 0.01625286042690277, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.009751853067427874, "std_of_first_answer": 0.009751853067427874, "avg_entropy_of_first_answer": 2.6035096049308777, "lex_sim": 1.0}
{"question": "who has climbed mount everest the number of times", "answer": ["Apa Success Sherpa"], "scores": [[0.5836361050605774, 0.9775238037109375, 0.00017347931861877441, 0.767402172088623, 0.994212806224823]], "normalized_score": [0.14983175694942474], "tokens": [["\u2581A", "pa", "\u2581Success", "\u2581Sher", "pa"]], "entropy": [[2.5003228187561035, 0.20969711244106293, 0.5041288137435913, 1.3944892883300781, 0.05300254002213478]], "avg_entropy": [0.9323281146585941], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Phurba Tashi Sherpa", "score_of_biggest_cluster": 0.14983175694942474, "score_of_first_answer": 0.14983175694942474, "max_score_of_first_answer": 0.994212806224823, "min_score_of_first_answer": 0.00017347931861877441, "avg_score": 0.14983175694942474, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3647333131694403, "std_of_first_answer": 0.3647333131694403, "avg_entropy_of_first_answer": 0.9323281146585941, "lex_sim": 1.0}
{"question": "who did michael showers play on vampire diaries", "answer": ["George Lockwood"], "scores": [[0.006042302120476961, 0.9261439442634583, 0.9946594834327698]], "normalized_score": [0.177222341299057], "tokens": [["\u2581George", "\u2581Lock", "wood"]], "entropy": [[5.002402305603027, 0.4994041919708252, 0.03943036496639252]], "avg_entropy": [1.8470789541800816], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Man", "score_of_biggest_cluster": 0.177222341299057, "score_of_first_answer": 0.177222341299057, "max_score_of_first_answer": 0.9946594834327698, "min_score_of_first_answer": 0.006042302120476961, "avg_score": 0.177222341299057, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45075804573675116, "std_of_first_answer": 0.45075804573675116, "avg_entropy_of_first_answer": 1.8470789541800816, "lex_sim": 1.0}
{"question": "who does demetrius love in a midsummer night dream", "answer": ["Helena"], "scores": [[0.3275745213031769, 0.9968525767326355]], "normalized_score": [0.5714398622512817], "tokens": [["\u2581Hel", "ena"]], "entropy": [[1.4153350591659546, 0.029225338250398636]], "avg_entropy": [0.7222801987081766], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Helena", "score_of_biggest_cluster": 0.5714398622512817, "score_of_first_answer": 0.5714398622512817, "max_score_of_first_answer": 0.9968525767326355, "min_score_of_first_answer": 0.3275745213031769, "avg_score": 0.5714398622512817, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3346390277147293, "std_of_first_answer": 0.3346390277147293, "avg_entropy_of_first_answer": 0.7222801987081766, "lex_sim": 1.0}
{"question": "what was the first video game for nintendo", "answer": ["Super Mario Bros."], "scores": [[0.3341200649738312, 0.9752883315086365, 0.8661974668502808, 0.9986786246299744, 0.8105942606925964]], "normalized_score": [0.7443485856056213], "tokens": [["\u2581Super", "\u2581Mario", "\u2581B", "ros", "."]], "entropy": [[2.6182618141174316, 0.1912154257297516, 0.7290936708450317, 0.011531108990311623, 0.6576356887817383]], "avg_entropy": [0.841547541692853], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Magnavox Odyssey", "score_of_biggest_cluster": 0.7443485856056213, "score_of_first_answer": 0.7443485856056213, "max_score_of_first_answer": 0.9986786246299744, "min_score_of_first_answer": 0.3341200649738312, "avg_score": 0.7443485856056213, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24153340201160503, "std_of_first_answer": 0.24153340201160503, "avg_entropy_of_first_answer": 0.841547541692853, "lex_sim": 1.0}
{"question": "when did the continental congress vote to adopt the declaration of independence", "answer": ["August 2 1776"], "scores": [[0.08875007927417755, 0.9957530498504639, 0.9057923555374146, 0.010673092678189278, 0.9893744587898254, 0.9947286248207092, 0.9980484247207642, 0.9982490539550781]], "normalized_score": [0.41246479749679565], "tokens": [["\u2581August", "\u2581", "2", "\u2581", "1", "7", "7", "6"]], "entropy": [[0.7535883188247681, 0.035722002387046814, 0.4294866919517517, 0.525683581829071, 0.0788608267903328, 0.041197270154953, 0.01827158033847809, 0.0168963260948658]], "avg_entropy": [0.23746332479640841], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "July 2, 1776", "score_of_biggest_cluster": 0.41246479749679565, "score_of_first_answer": 0.41246479749679565, "max_score_of_first_answer": 0.9982490539550781, "min_score_of_first_answer": 0.010673092678189278, "avg_score": 0.41246479749679565, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40447898734181176, "std_of_first_answer": 0.40447898734181176, "avg_entropy_of_first_answer": 0.23746332479640841, "lex_sim": 1.0}
{"question": "who is the actor that plays ducky on ncis", "answer": ["Miguel Sandoval"], "scores": [[0.013475285843014717, 0.13804879784584045, 0.9864343404769897]], "normalized_score": [0.12242765724658966], "tokens": [["\u2581Miguel", "\u2581Sand", "oval"]], "entropy": [[0.7200019359588623, 1.563111662864685, 0.12596644461154938]], "avg_entropy": [0.8030266811450323], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "David McCallum", "score_of_biggest_cluster": 0.12242765724658966, "score_of_first_answer": 0.12242765724658966, "max_score_of_first_answer": 0.9864343404769897, "min_score_of_first_answer": 0.013475285843014717, "avg_score": 0.12242765724658966, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4322969543718384, "std_of_first_answer": 0.4322969543718384, "avg_entropy_of_first_answer": 0.8030266811450323, "lex_sim": 1.0}
{"question": "region of antarctica where march of the penguins filmed", "answer": ["Antarctic Peninsula"], "scores": [[0.24952475726604462, 0.7381764054298401, 0.9997072815895081, 0.8121309876441956, 0.9990676045417786, 0.9973596930503845]], "normalized_score": [0.7281204462051392], "tokens": [["\u2581Ant", "ar", "ctic", "\u2581Pen", "ins", "ula"]], "entropy": [[4.236236572265625, 0.7142155766487122, 0.0038567455485463142, 0.7786101698875427, 0.009135941043496132, 0.021903380751609802]], "avg_entropy": [0.9606597310242554], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dumont d'Urville in Ad\u00e9lie Land", "score_of_biggest_cluster": 0.7281204462051392, "score_of_first_answer": 0.7281204462051392, "max_score_of_first_answer": 0.9997072815895081, "min_score_of_first_answer": 0.24952475726604462, "avg_score": 0.7281204462051392, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2662866116602027, "std_of_first_answer": 0.2662866116602027, "avg_entropy_of_first_answer": 0.9606597310242554, "lex_sim": 1.0}
{"question": "when was the last time dallas cowboys won the super bowl", "answer": ["February 3, 1996"], "scores": [[0.2942564785480499, 0.9978938698768616, 0.5783916711807251, 0.9923673272132874, 0.997849702835083, 0.9948569536209106, 0.9999769926071167, 0.9995586276054382, 0.9954061508178711]], "normalized_score": [0.8193718194961548], "tokens": [["\u2581February", "\u2581", "3", ",", "\u2581", "1", "9", "9", "6"]], "entropy": [[1.9781712293624878, 0.019161071628332138, 1.2423332929611206, 0.05594354495406151, 0.0173389483243227, 0.032771866768598557, 0.0003266100538894534, 0.004401838406920433, 0.03607525676488876]], "avg_entropy": [0.37628040658051354], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1995", "score_of_biggest_cluster": 0.8193718194961548, "score_of_first_answer": 0.8193718194961548, "max_score_of_first_answer": 0.9999769926071167, "min_score_of_first_answer": 0.2942564785480499, "avg_score": 0.8193718194961548, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24247351087452196, "std_of_first_answer": 0.24247351087452196, "avg_entropy_of_first_answer": 0.37628040658051354, "lex_sim": 1.0}
{"question": "who has the oldest team in the nba", "answer": ["Boston Celtics"], "scores": [[0.39039209485054016, 0.9805153608322144, 0.9999064207077026, 0.9969190359115601]], "normalized_score": [0.7859477400779724], "tokens": [["\u2581Boston", "\u2581C", "elt", "ics"]], "entropy": [[2.5838260650634766, 0.13651420176029205, 0.0012861122377216816, 0.022897517308592796]], "avg_entropy": [0.6861309740925208], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sacramento Kings", "score_of_biggest_cluster": 0.7859477400779724, "score_of_first_answer": 0.7859477400779724, "max_score_of_first_answer": 0.9999064207077026, "min_score_of_first_answer": 0.39039209485054016, "avg_score": 0.7859477400779724, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26080190410275367, "std_of_first_answer": 0.26080190410275367, "avg_entropy_of_first_answer": 0.6861309740925208, "lex_sim": 1.0}
{"question": "who played young clark kent in 1978 superman", "answer": ["Jeff East"], "scores": [[0.9434268474578857, 0.9875624179840088]], "normalized_score": [0.9652423858642578], "tokens": [["\u2581Jeff", "\u2581East"]], "entropy": [[0.44798824191093445, 0.09873813390731812]], "avg_entropy": [0.2733631879091263], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Jeff East", "score_of_biggest_cluster": 0.9652423858642578, "score_of_first_answer": 0.9652423858642578, "max_score_of_first_answer": 0.9875624179840088, "min_score_of_first_answer": 0.9434268474578857, "avg_score": 0.9652423858642578, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.022067785263061523, "std_of_first_answer": 0.022067785263061523, "avg_entropy_of_first_answer": 0.2733631879091263, "lex_sim": 1.0}
{"question": "when was son of a preacher man released", "answer": ["24 September 1968"], "scores": [[0.33171260356903076, 0.1927127093076706, 0.044504616409540176, 0.11599414050579071, 0.9696084260940552, 0.5462706685066223, 0.9969634413719177, 0.6932221055030823, 0.1474638283252716]], "normalized_score": [0.2966921925544739], "tokens": [["\u2581", "2", "4", "\u2581September", "\u2581", "1", "9", "6", "8"]], "entropy": [[2.5706686973571777, 0.9137566089630127, 1.8545818328857422, 2.713331699371338, 0.1607191264629364, 0.696768045425415, 0.02737542986869812, 0.8585230112075806, 0.7730261087417603]], "avg_entropy": [1.1743056178092957], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "late 1968", "score_of_biggest_cluster": 0.2966921925544739, "score_of_first_answer": 0.2966921925544739, "max_score_of_first_answer": 0.9969634413719177, "min_score_of_first_answer": 0.044504616409540176, "avg_score": 0.2966921925544739, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3472551662635362, "std_of_first_answer": 0.3472551662635362, "avg_entropy_of_first_answer": 1.1743056178092957, "lex_sim": 1.0}
{"question": "what was the main characteristic of post-world war ii american society", "answer": ["Consumerism"], "scores": [[0.04096193611621857, 0.8903780579566956, 0.8988535404205322]], "normalized_score": [0.32004764676094055], "tokens": [["\u2581Cons", "umer", "ism"]], "entropy": [[4.712837219238281, 0.5053277015686035, 0.6517326831817627]], "avg_entropy": [1.9566325346628826], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "strong economic growth", "score_of_biggest_cluster": 0.32004764676094055, "score_of_first_answer": 0.32004764676094055, "max_score_of_first_answer": 0.8988535404205322, "min_score_of_first_answer": 0.04096193611621857, "avg_score": 0.32004764676094055, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40243116548483443, "std_of_first_answer": 0.40243116548483443, "avg_entropy_of_first_answer": 1.9566325346628826, "lex_sim": 1.0}
{"question": "when does season 7 game of thrones dvd release", "answer": ["December 3"], "scores": [[0.31300613284111023, 0.9684407711029053, 0.19332662224769592]], "normalized_score": [0.3884238302707672], "tokens": [["\u2581December", "\u2581", "3"]], "entropy": [[3.407231330871582, 0.1815912276506424, 1.4961371421813965]], "avg_entropy": [1.694986566901207], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "December 12, 2017", "score_of_biggest_cluster": 0.3884238302707672, "score_of_first_answer": 0.3884238302707672, "max_score_of_first_answer": 0.9684407711029053, "min_score_of_first_answer": 0.19332662224769592, "avg_score": 0.3884238302707672, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34070510123122033, "std_of_first_answer": 0.34070510123122033, "avg_entropy_of_first_answer": 1.694986566901207, "lex_sim": 1.0}
{"question": "where did the battle of bonhomme richard take place", "answer": ["Chesapeake Bay"], "scores": [[0.2536962330341339, 0.9853978157043457, 0.9973719120025635, 0.9971240162849426, 0.9760563373565674]], "normalized_score": [0.7533579468727112], "tokens": [["\u2581Ch", "es", "ape", "ake", "\u2581Bay"]], "entropy": [[4.329325199127197, 0.1274532526731491, 0.02386004477739334, 0.022344045341014862, 0.16965924203395844]], "avg_entropy": [0.9345283567905426], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "near Flamborough Head", "score_of_biggest_cluster": 0.7533579468727112, "score_of_first_answer": 0.7533579468727112, "max_score_of_first_answer": 0.9973719120025635, "min_score_of_first_answer": 0.2536962330341339, "avg_score": 0.7533579468727112, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2942241403452353, "std_of_first_answer": 0.2942241403452353, "avg_entropy_of_first_answer": 0.9345283567905426, "lex_sim": 1.0}
{"question": "towns on the border of texas and oklahoma", "answer": ["Guymon"], "scores": [[0.012468735687434673, 0.9927129149436951, 0.9958892464637756]], "normalized_score": [0.23100373148918152], "tokens": [["\u2581Gu", "ym", "on"]], "entropy": [[3.528367280960083, 0.05790510028600693, 0.04105212539434433]], "avg_entropy": [1.2091081688801448], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Texhoma", "score_of_biggest_cluster": 0.23100373148918152, "score_of_first_answer": 0.23100373148918152, "max_score_of_first_answer": 0.9958892464637756, "min_score_of_first_answer": 0.012468735687434673, "avg_score": 0.23100373148918152, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4628420226123841, "std_of_first_answer": 0.4628420226123841, "avg_entropy_of_first_answer": 1.2091081688801448, "lex_sim": 1.0}
{"question": "what is the population of fayetteville north carolina", "answer": ["204"], "scores": [[0.9258679151535034, 0.8418148756027222, 0.9487688541412354, 0.6745752692222595]], "normalized_score": [0.8404059410095215], "tokens": [["\u2581", "2", "0", "4"]], "entropy": [[0.5572291612625122, 0.7503963708877563, 0.31026557087898254, 1.1971994638442993]], "avg_entropy": [0.7037726417183876], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "204,408 in 2013", "score_of_biggest_cluster": 0.8404059410095215, "score_of_first_answer": 0.8404059410095215, "max_score_of_first_answer": 0.9487688541412354, "min_score_of_first_answer": 0.6745752692222595, "avg_score": 0.8404059410095215, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10762426592130575, "std_of_first_answer": 0.10762426592130575, "avg_entropy_of_first_answer": 0.7037726417183876, "lex_sim": 1.0}
{"question": "where was the world chess tournament 2017 held", "answer": ["UAE"], "scores": [[0.015916582196950912, 0.5126135349273682]], "normalized_score": [0.09032750129699707], "tokens": [["\u2581U", "AE"]], "entropy": [[3.543278694152832, 1.8490893840789795]], "avg_entropy": [2.6961840391159058], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tbilisi, Georgia", "score_of_biggest_cluster": 0.09032750129699707, "score_of_first_answer": 0.09032750129699707, "max_score_of_first_answer": 0.5126135349273682, "min_score_of_first_answer": 0.015916582196950912, "avg_score": 0.09032750129699707, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24834847636520863, "std_of_first_answer": 0.24834847636520863, "avg_entropy_of_first_answer": 2.6961840391159058, "lex_sim": 1.0}
{"question": "what was the result of the revolt of 1857", "answer": ["India"], "scores": [[0.14463604986667633]], "normalized_score": [0.14463604986667633], "tokens": [["\u2581India"]], "entropy": [[3.8950355052948]], "avg_entropy": [3.8950355052948], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "British victory", "score_of_biggest_cluster": 0.14463604986667633, "score_of_first_answer": 0.14463604986667633, "max_score_of_first_answer": 0.14463604986667633, "min_score_of_first_answer": 0.14463604986667633, "avg_score": 0.14463604986667633, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.8950355052948, "lex_sim": 1.0}
{"question": "when was the term social justice first used", "answer": ["It appears there are no mentions of it prior to the early 1910s"], "scores": [[0.008533312007784843, 0.024867532774806023, 0.024187427014112473, 0.25435787439346313, 0.3973090946674347, 0.004821645561605692, 0.8843883275985718, 0.04478126019239426, 0.09551065415143967, 0.9859952330589294, 0.28974172472953796, 0.05416436493396759, 0.9164295196533203, 0.6790743470191956, 0.7334398031234741, 0.002943957457318902, 0.9762250185012817, 0.7750202417373657]], "normalized_score": [0.13203959167003632], "tokens": [["\u2581It", "\u2581appears", "\u2581there", "\u2581are", "\u2581no", "\u2581mentions", "\u2581of", "\u2581it", "\u2581prior", "\u2581to", "\u2581the", "\u2581early", "\u2581", "1", "9", "1", "0", "s"]], "entropy": [[2.4693288803100586, 2.133495569229126, 2.2939608097076416, 1.4251699447631836, 2.4892146587371826, 3.7778313159942627, 0.6467654705047607, 2.1543772220611572, 2.419806957244873, 0.10635096579790115, 1.5971771478652954, 2.1627695560455322, 0.4985937476158142, 0.664374053478241, 0.6477826833724976, 1.3086795806884766, 0.16389116644859314, 0.7808672189712524]], "avg_entropy": [1.541135386046436], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the 1840s", "score_of_biggest_cluster": 0.13203959167003632, "score_of_first_answer": 0.13203959167003632, "max_score_of_first_answer": 0.9859952330589294, "min_score_of_first_answer": 0.002943957457318902, "avg_score": 0.13203959167003632, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3820434866566685, "std_of_first_answer": 0.3820434866566685, "avg_entropy_of_first_answer": 1.541135386046436, "lex_sim": 1.0}
{"question": "when does wentworth season 6 start in australia", "answer": ["April 16, 2018"], "scores": [[0.16273130476474762, 0.9813526272773743, 0.4163622260093689, 0.2641206681728363, 0.757334291934967, 0.9976004958152771, 0.9987006187438965, 0.9998652935028076, 0.9946225881576538, 0.8201281428337097]], "normalized_score": [0.6358769536018372], "tokens": [["\u2581April", "\u2581", "1", "6", ",", "\u2581", "2", "0", "1", "8"]], "entropy": [[3.215643882751465, 0.12414588034152985, 1.2890889644622803, 1.9977188110351562, 0.7290765047073364, 0.02188865840435028, 0.012059659697115421, 0.001704459311440587, 0.03772404417395592, 0.7219504117965698]], "avg_entropy": [0.81510012766812], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2019", "score_of_biggest_cluster": 0.6358769536018372, "score_of_first_answer": 0.6358769536018372, "max_score_of_first_answer": 0.9998652935028076, "min_score_of_first_answer": 0.16273130476474762, "avg_score": 0.6358769536018372, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31544496863387345, "std_of_first_answer": 0.31544496863387345, "avg_entropy_of_first_answer": 0.81510012766812, "lex_sim": 1.0}
{"question": "who appoints the members of the board of governors of the federal reserve", "answer": ["President of the United States"], "scores": [[0.22098931670188904, 0.8333275318145752, 0.9785140156745911, 0.987008273601532, 0.9973835349082947]], "normalized_score": [0.7075993418693542], "tokens": [["\u2581President", "\u2581of", "\u2581the", "\u2581United", "\u2581States"]], "entropy": [[2.1226797103881836, 0.7604920268058777, 0.1285279244184494, 0.09634086489677429, 0.023148728534579277]], "avg_entropy": [0.6262378510087728], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "the President", "score_of_biggest_cluster": 0.7075993418693542, "score_of_first_answer": 0.7075993418693542, "max_score_of_first_answer": 0.9973835349082947, "min_score_of_first_answer": 0.22098931670188904, "avg_score": 0.7075993418693542, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2973564795661235, "std_of_first_answer": 0.2973564795661235, "avg_entropy_of_first_answer": 0.6262378510087728, "lex_sim": 1.0}
{"question": "when does star trek discovery air on tv", "answer": ["January 17, 2018"], "scores": [[0.06392855197191238, 0.9809470772743225, 0.3443254828453064, 0.8126177787780762, 0.8476531505584717, 0.9972403049468994, 0.997111439704895, 0.9997422099113464, 0.9772210121154785, 0.08688519150018692]], "normalized_score": [0.5127177834510803], "tokens": [["\u2581January", "\u2581", "1", "7", ",", "\u2581", "2", "0", "1", "8"]], "entropy": [[2.926236629486084, 0.12770777940750122, 1.4809470176696777, 0.8710373640060425, 0.5894453525543213, 0.02570704184472561, 0.021954452618956566, 0.0030680035706609488, 0.11859932541847229, 0.7319241166114807]], "avg_entropy": [0.6896627083187923], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "September 19, 2017", "score_of_biggest_cluster": 0.5127177834510803, "score_of_first_answer": 0.5127177834510803, "max_score_of_first_answer": 0.9997422099113464, "min_score_of_first_answer": 0.06392855197191238, "avg_score": 0.5127177834510803, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3691315555380981, "std_of_first_answer": 0.3691315555380981, "avg_entropy_of_first_answer": 0.6896627083187923, "lex_sim": 1.0}
{"question": "what episode does marnie come back in geordie shore", "answer": ["The Hangover"], "scores": [[0.01925000548362732, 0.008303729817271233, 0.5425822734832764, 0.9886411428451538]], "normalized_score": [0.09622811526060104], "tokens": [["\u2581The", "\u2581H", "ang", "over"]], "entropy": [[3.468919277191162, 5.573991298675537, 1.9306604862213135, 0.10008184611797333]], "avg_entropy": [2.7684132270514965], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "5", "score_of_biggest_cluster": 0.09622811526060104, "score_of_first_answer": 0.09622811526060104, "max_score_of_first_answer": 0.9886411428451538, "min_score_of_first_answer": 0.008303729817271233, "avg_score": 0.09622811526060104, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4076762794651045, "std_of_first_answer": 0.4076762794651045, "avg_entropy_of_first_answer": 2.7684132270514965, "lex_sim": 1.0}
{"question": "who wrote the song rainy days and mondays", "answer": ["Paul William Scott"], "scores": [[0.4509463608264923, 0.04196499288082123, 0.0031287632882595062]], "normalized_score": [0.038975790143013], "tokens": [["\u2581Paul", "\u2581William", "\u2581Scott"]], "entropy": [[2.2545580863952637, 0.950185239315033, 3.077904224395752]], "avg_entropy": [2.0942158500353494], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paul Williams", "score_of_biggest_cluster": 0.038975790143013, "score_of_first_answer": 0.038975790143013, "max_score_of_first_answer": 0.4509463608264923, "min_score_of_first_answer": 0.0031287632882595062, "avg_score": 0.038975790143013, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20257086900380022, "std_of_first_answer": 0.20257086900380022, "avg_entropy_of_first_answer": 2.0942158500353494, "lex_sim": 1.0}
{"question": "who plays spock in the new star trek movies", "answer": ["Zachary Quinto"], "scores": [[0.9260615110397339, 0.9974355101585388, 0.9770969748497009, 0.9992030262947083]], "normalized_score": [0.974493682384491], "tokens": [["\u2581Zach", "ary", "\u2581Qu", "into"]], "entropy": [[0.505996823310852, 0.02266893908381462, 0.2085513174533844, 0.00788920372724533]], "avg_entropy": [0.1862765708938241], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Zachary John Quinto", "score_of_biggest_cluster": 0.974493682384491, "score_of_first_answer": 0.974493682384491, "max_score_of_first_answer": 0.9992030262947083, "min_score_of_first_answer": 0.9260615110397339, "avg_score": 0.974493682384491, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.02953176807756463, "std_of_first_answer": 0.02953176807756463, "avg_entropy_of_first_answer": 0.1862765708938241, "lex_sim": 1.0}
{"question": "who said i'll gladly pay you tuesday", "answer": ["Harold Gray"], "scores": [[0.0015338037628680468, 0.3330917954444885]], "normalized_score": [0.022603044286370277], "tokens": [["\u2581Harold", "\u2581Gray"]], "entropy": [[2.85174822807312, 3.617241382598877]], "avg_entropy": [3.2344948053359985], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wimpy", "score_of_biggest_cluster": 0.022603044286370277, "score_of_first_answer": 0.022603044286370277, "max_score_of_first_answer": 0.3330917954444885, "min_score_of_first_answer": 0.0015338037628680468, "avg_score": 0.022603044286370277, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.16577899584081024, "std_of_first_answer": 0.16577899584081024, "avg_entropy_of_first_answer": 3.2344948053359985, "lex_sim": 1.0}
{"question": "who played the hobbits in the lord of the rings", "answer": ["Elijah Wood, Dominic Monaghan, Sean Astin, Billy Boyd"], "scores": [[0.6341808438301086, 0.9992109537124634, 0.9994435906410217, 0.982254683971405, 0.3122725784778595, 0.03312525525689125, 0.9992117881774902, 0.993213415145874, 0.9979593753814697, 0.999504804611206, 0.8194829821586609, 0.2223033457994461, 0.9992916584014893, 0.9064187407493591, 0.9994474053382874, 0.5301592946052551, 0.5812957882881165, 0.9772342443466187, 0.9999227523803711]], "normalized_score": [0.6543331146240234], "tokens": [["\u2581E", "lij", "ah", "\u2581Wood", ",", "\u2581Domin", "ic", "\u2581Mon", "ag", "han", ",", "\u2581Se", "an", "\u2581Ast", "in", ",", "\u2581Billy", "\u2581Bo", "yd"]], "entropy": [[2.2383220195770264, 0.009976468048989773, 0.006193714216351509, 0.1520596593618393, 1.3595809936523438, 1.3848210573196411, 0.009002620354294777, 0.06349820643663406, 0.01834261044859886, 0.005097190383821726, 0.664862871170044, 1.5347541570663452, 0.006093702279031277, 0.411388635635376, 0.006777556613087654, 0.9782195091247559, 1.469178318977356, 0.18550172448158264, 0.0010241842828691006]], "avg_entropy": [0.5528786947068415], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dominic Monaghan", "score_of_biggest_cluster": 0.6543331146240234, "score_of_first_answer": 0.6543331146240234, "max_score_of_first_answer": 0.9999227523803711, "min_score_of_first_answer": 0.03312525525689125, "avg_score": 0.6543331146240234, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3023335963801723, "std_of_first_answer": 0.3023335963801723, "avg_entropy_of_first_answer": 0.5528786947068415, "lex_sim": 1.0}
{"question": "who has more super bowl wins nfc or afc", "answer": ["AFC"], "scores": [[0.2252192497253418, 0.9981909394264221]], "normalized_score": [0.47414323687553406], "tokens": [["\u2581A", "FC"]], "entropy": [[1.5305554866790771, 0.019014529883861542]], "avg_entropy": [0.7747850082814693], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "NFC", "score_of_biggest_cluster": 0.47414323687553406, "score_of_first_answer": 0.47414323687553406, "max_score_of_first_answer": 0.9981909394264221, "min_score_of_first_answer": 0.2252192497253418, "avg_score": 0.47414323687553406, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38648584485054016, "std_of_first_answer": 0.38648584485054016, "avg_entropy_of_first_answer": 0.7747850082814693, "lex_sim": 1.0}
{"question": "who wrote the phantom of the opera music", "answer": ["Andrew Lloyd Webber"], "scores": [[0.8200815320014954, 0.986580491065979, 0.9914930462837219, 0.9981327652931213]], "normalized_score": [0.9459471702575684], "tokens": [["\u2581Andrew", "\u2581Lloyd", "\u2581Web", "ber"]], "entropy": [[1.0384101867675781, 0.11567506194114685, 0.06477315723896027, 0.017208561301231384]], "avg_entropy": [0.30901674181222916], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Andrew Lloyd Webber", "score_of_biggest_cluster": 0.9459471702575684, "score_of_first_answer": 0.9459471702575684, "max_score_of_first_answer": 0.9981327652931213, "min_score_of_first_answer": 0.8200815320014954, "avg_score": 0.9459471702575684, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.07458540674197844, "std_of_first_answer": 0.07458540674197844, "avg_entropy_of_first_answer": 0.30901674181222916, "lex_sim": 1.0}
{"question": "who sings i want to be a rockstar", "answer": ["Travis"], "scores": [[0.5400007963180542, 0.5976089239120483]], "normalized_score": [0.5680750608444214], "tokens": [["\u2581Tra", "vis"]], "entropy": [[2.2989120483398438, 1.2151522636413574]], "avg_entropy": [1.7570321559906006], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nickelback", "score_of_biggest_cluster": 0.5680750608444214, "score_of_first_answer": 0.5680750608444214, "max_score_of_first_answer": 0.5976089239120483, "min_score_of_first_answer": 0.5400007963180542, "avg_score": 0.5680750608444214, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02880406379699707, "std_of_first_answer": 0.02880406379699707, "avg_entropy_of_first_answer": 1.7570321559906006, "lex_sim": 1.0}
{"question": "who played shaggy in the new scooby doo movie", "answer": ["Casey Kasem"], "scores": [[0.061771735548973083, 0.9961206316947937, 0.9691104292869568, 0.7723115682601929]], "normalized_score": [0.4632515609264374], "tokens": [["\u2581Case", "y", "\u2581Kas", "em"]], "entropy": [[2.0158300399780273, 0.046051643788814545, 0.29035189747810364, 0.6582031846046448]], "avg_entropy": [0.7526091914623976], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Matthew Lyn Lillard", "score_of_biggest_cluster": 0.4632515609264374, "score_of_first_answer": 0.4632515609264374, "max_score_of_first_answer": 0.9961206316947937, "min_score_of_first_answer": 0.061771735548973083, "avg_score": 0.4632515609264374, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37837547971256086, "std_of_first_answer": 0.37837547971256086, "avg_entropy_of_first_answer": 0.7526091914623976, "lex_sim": 1.0}
{"question": "where was robin hood prince of thieves made", "answer": ["England"], "scores": [[0.2118326723575592]], "normalized_score": [0.2118326723575592], "tokens": [["\u2581England"]], "entropy": [[4.34706974029541]], "avg_entropy": [4.34706974029541], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "the United Kingdom", "score_of_biggest_cluster": 0.2118326723575592, "score_of_first_answer": 0.2118326723575592, "max_score_of_first_answer": 0.2118326723575592, "min_score_of_first_answer": 0.2118326723575592, "avg_score": 0.2118326723575592, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.34706974029541, "lex_sim": 1.0}
{"question": "which animal is the carrier of the h1n1 virus", "answer": ["pigs"], "scores": [[0.3595428466796875, 0.5424259305000305]], "normalized_score": [0.4416167438030243], "tokens": [["\u2581p", "igs"]], "entropy": [[2.641465663909912, 0.748531699180603]], "avg_entropy": [1.6949986815452576], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Pig", "score_of_biggest_cluster": 0.4416167438030243, "score_of_first_answer": 0.4416167438030243, "max_score_of_first_answer": 0.5424259305000305, "min_score_of_first_answer": 0.3595428466796875, "avg_score": 0.4416167438030243, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.09144154191017151, "std_of_first_answer": 0.09144154191017151, "avg_entropy_of_first_answer": 1.6949986815452576, "lex_sim": 1.0}
{"question": "which songs did liam write as you were", "answer": ["Betterman"], "scores": [[0.009345750324428082, 0.2724274694919586]], "normalized_score": [0.05045830085873604], "tokens": [["\u2581Better", "man"]], "entropy": [[4.8486714363098145, 0.9446585178375244]], "avg_entropy": [2.8966649770736694], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "I've All I Need", "score_of_biggest_cluster": 0.05045830085873604, "score_of_first_answer": 0.05045830085873604, "max_score_of_first_answer": 0.2724274694919586, "min_score_of_first_answer": 0.009345750324428082, "avg_score": 0.05045830085873604, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13154085958376527, "std_of_first_answer": 0.13154085958376527, "avg_entropy_of_first_answer": 2.8966649770736694, "lex_sim": 1.0}
{"question": "how many times has the saints won the super bowl", "answer": ["None"], "scores": [[0.023894447833299637]], "normalized_score": [0.023894447833299637], "tokens": [["\u2581None"]], "entropy": [[2.520777702331543]], "avg_entropy": [2.520777702331543], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1", "score_of_biggest_cluster": 0.023894447833299637, "score_of_first_answer": 0.023894447833299637, "max_score_of_first_answer": 0.023894447833299637, "min_score_of_first_answer": 0.023894447833299637, "avg_score": 0.023894447833299637, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.520777702331543, "lex_sim": 1.0}
{"question": "who played major hochstetter in hogan's heroes", "answer": ["Leon Askin"], "scores": [[0.711901843547821, 0.9918815493583679, 0.994586706161499]], "normalized_score": [0.8888753652572632], "tokens": [["\u2581Leon", "\u2581Ask", "in"]], "entropy": [[1.354495644569397, 0.06067737936973572, 0.039975348860025406]], "avg_entropy": [0.48504945759971935], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Howard Caine", "score_of_biggest_cluster": 0.8888753652572632, "score_of_first_answer": 0.8888753652572632, "max_score_of_first_answer": 0.994586706161499, "min_score_of_first_answer": 0.711901843547821, "avg_score": 0.8888753652572632, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13262590876848723, "std_of_first_answer": 0.13262590876848723, "avg_entropy_of_first_answer": 0.48504945759971935, "lex_sim": 1.0}
{"question": "where is the 7th game of the world series played", "answer": ["Houston"], "scores": [[0.008941658772528172]], "normalized_score": [0.008941656909883022], "tokens": [["\u2581Houston"]], "entropy": [[4.550782680511475]], "avg_entropy": [4.550782680511475], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dodger Stadium, Los Angeles", "score_of_biggest_cluster": 0.008941656909883022, "score_of_first_answer": 0.008941656909883022, "max_score_of_first_answer": 0.008941658772528172, "min_score_of_first_answer": 0.008941658772528172, "avg_score": 0.008941656909883022, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.550782680511475, "lex_sim": 1.0}
{"question": "when does elena turn into a vampire in the tv series", "answer": ["I am not entirely sure."], "scores": [[0.015063569881021976, 0.08811730891466141, 0.4256936013698578, 0.0006784057477489114, 0.7639067769050598, 0.28682026267051697, 0.017086340114474297, 0.7991690039634705, 0.13196688890457153, 0.8944573402404785, 0.9483301639556885, 0.8211705088615417]], "normalized_score": [0.14742717146873474], "tokens": [["\u2581I", "\u2581am", "\u2581not", "\u2581entirely", "\u2581sure", ".", "\u2581Can", "\u2581you", "\u2581be", "\u2581more", "\u2581specific", "?"]], "entropy": [[3.1689987182617188, 2.541018486022949, 1.7444157600402832, 1.318427324295044, 0.7129147052764893, 2.485914468765259, 2.554842948913574, 0.676517903804779, 2.9960203170776367, 0.41941165924072266, 0.2918317914009094, 0.9240503907203674]], "avg_entropy": [1.6528637061516445], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "season four", "score_of_biggest_cluster": 0.14742717146873474, "score_of_first_answer": 0.14742717146873474, "max_score_of_first_answer": 0.9483301639556885, "min_score_of_first_answer": 0.0006784057477489114, "avg_score": 0.14742717146873474, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3694936045421073, "std_of_first_answer": 0.3694936045421073, "avg_entropy_of_first_answer": 1.6528637061516445, "lex_sim": 1.0}
{"question": "where did they get the vibranium to make captain americas shield", "answer": ["The country of Wakanda"], "scores": [[0.05288023129105568, 0.0032721932511776686, 0.850269615650177, 0.9183883666992188, 0.9984678626060486, 0.9961395859718323]], "normalized_score": [0.22632277011871338], "tokens": [["\u2581The", "\u2581country", "\u2581of", "\u2581W", "ak", "anda"]], "entropy": [[4.419812202453613, 4.92034912109375, 0.9745419025421143, 0.6393730640411377, 0.01849869266152382, 0.03689923509955406]], "avg_entropy": [1.8349123696486156], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Wakanda", "score_of_biggest_cluster": 0.22632277011871338, "score_of_first_answer": 0.22632277011871338, "max_score_of_first_answer": 0.9984678626060486, "min_score_of_first_answer": 0.0032721932511776686, "avg_score": 0.22632277011871338, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.43341845172620286, "std_of_first_answer": 0.43341845172620286, "avg_entropy_of_first_answer": 1.8349123696486156, "lex_sim": 1.0}
{"question": "who wrote the song to make you feel my love", "answer": ["Bob Dylan"], "scores": [[0.9174065589904785, 0.9970596432685852, 0.9996588230133057, 0.9987395405769348]], "normalized_score": [0.9775674939155579], "tokens": [["\u2581Bob", "\u2581D", "yl", "an"]], "entropy": [[0.6666914224624634, 0.03089193068444729, 0.004108095075935125, 0.011035733856260777]], "avg_entropy": [0.17818179551977664], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Bob Dylan", "score_of_biggest_cluster": 0.9775674939155579, "score_of_first_answer": 0.9775674939155579, "max_score_of_first_answer": 0.9996588230133057, "min_score_of_first_answer": 0.9174065589904785, "avg_score": 0.9775674939155579, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03512079648882256, "std_of_first_answer": 0.03512079648882256, "avg_entropy_of_first_answer": 0.17818179551977664, "lex_sim": 1.0}
{"question": "what happened to western rome after it fell", "answer": ["It fell to the Visigoths and Vandals"], "scores": [[0.1695338636636734, 0.04569803550839424, 0.2163030505180359, 0.5107921957969666, 0.07420353591442108, 0.9891340136528015, 0.9994118213653564, 0.9412990808486938, 0.10405688732862473, 0.0825021043419838, 0.9619426131248474, 0.9980977177619934]], "normalized_score": [0.2978328466415405], "tokens": [["\u2581It", "\u2581fell", "\u2581to", "\u2581the", "\u2581Vis", "ig", "oth", "s", "\u2581and", "\u2581V", "and", "als"]], "entropy": [[3.983752489089966, 3.0631532669067383, 2.9000277519226074, 2.558764934539795, 3.5786728858947754, 0.08424209803342819, 0.007632809691131115, 0.3906819224357605, 2.3559529781341553, 3.501286029815674, 0.17392714321613312, 0.0204601027071476]], "avg_entropy": [1.8848795343656093], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "divided into several successor polities", "score_of_biggest_cluster": 0.2978328466415405, "score_of_first_answer": 0.2978328466415405, "max_score_of_first_answer": 0.9994118213653564, "min_score_of_first_answer": 0.04569803550839424, "avg_score": 0.2978328466415405, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4135978043703911, "std_of_first_answer": 0.4135978043703911, "avg_entropy_of_first_answer": 1.8848795343656093, "lex_sim": 1.0}
{"question": "who plays connor's mom in madeas christmas", "answer": ["Lynn Whitfield"], "scores": [[0.033988114446401596, 0.941574215888977, 0.8408153653144836, 0.988059937953949]], "normalized_score": [0.4038000702857971], "tokens": [["\u2581Lyn", "n", "\u2581Whit", "field"]], "entropy": [[3.8362784385681152, 0.29057109355926514, 0.8404166102409363, 0.09181429445743561]], "avg_entropy": [1.264770109206438], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kathy Najimy", "score_of_biggest_cluster": 0.4038000702857971, "score_of_first_answer": 0.4038000702857971, "max_score_of_first_answer": 0.988059937953949, "min_score_of_first_answer": 0.033988114446401596, "avg_score": 0.4038000702857971, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3888227440928956, "std_of_first_answer": 0.3888227440928956, "avg_entropy_of_first_answer": 1.264770109206438, "lex_sim": 1.0}
{"question": "who holds the record for most platinum albums", "answer": ["The Beatles"], "scores": [[0.34222081303596497, 0.9198499917984009, 0.9997870326042175]], "normalized_score": [0.6802110075950623], "tokens": [["\u2581The", "\u2581Beat", "les"]], "entropy": [[3.4346680641174316, 0.39585357904434204, 0.0025605051778256893]], "avg_entropy": [1.2776940494465332], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "The Beatles", "score_of_biggest_cluster": 0.6802110075950623, "score_of_first_answer": 0.6802110075950623, "max_score_of_first_answer": 0.9997870326042175, "min_score_of_first_answer": 0.34222081303596497, "avg_score": 0.6802110075950623, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.29296164553057497, "std_of_first_answer": 0.29296164553057497, "avg_entropy_of_first_answer": 1.2776940494465332, "lex_sim": 1.0}
{"question": "who carried florida by 537 votes in 2000", "answer": ["George W. Bush"], "scores": [[0.8363145589828491, 0.7761829495429993, 0.819145679473877, 0.9956015348434448]], "normalized_score": [0.8529922962188721], "tokens": [["\u2581George", "\u2581W", ".", "\u2581Bush"]], "entropy": [[0.8978967666625977, 0.7857663035392761, 0.5047322511672974, 0.0389152467250824]], "avg_entropy": [0.5568276420235634], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "George W. Bush", "score_of_biggest_cluster": 0.8529922962188721, "score_of_first_answer": 0.8529922962188721, "max_score_of_first_answer": 0.9956015348434448, "min_score_of_first_answer": 0.7761829495429993, "avg_score": 0.8529922962188721, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08306996380642138, "std_of_first_answer": 0.08306996380642138, "avg_entropy_of_first_answer": 0.5568276420235634, "lex_sim": 1.0}
{"question": "when did gaurdians of the galaxy 2 come out", "answer": ["April 28, 2017"], "scores": [[0.8256569504737854, 0.9972037076950073, 0.9060754776000977, 0.4308675229549408, 0.8995519280433655, 0.9981260895729065, 0.9994331002235413, 0.9998459815979004, 0.9970663189888, 0.9962916374206543]], "normalized_score": [0.8824905753135681], "tokens": [["\u2581April", "\u2581", "2", "8", ",", "\u2581", "2", "0", "1", "7"]], "entropy": [[1.0501806735992432, 0.024070054292678833, 0.4018905460834503, 1.4758620262145996, 0.40610313415527344, 0.01739651896059513, 0.005163732450455427, 0.001844732090830803, 0.020894214510917664, 0.032999392598867416]], "avg_entropy": [0.34364050249569117], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2017", "score_of_biggest_cluster": 0.8824905753135681, "score_of_first_answer": 0.8824905753135681, "max_score_of_first_answer": 0.9998459815979004, "min_score_of_first_answer": 0.4308675229549408, "avg_score": 0.8824905753135681, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.168233615873237, "std_of_first_answer": 0.168233615873237, "avg_entropy_of_first_answer": 0.34364050249569117, "lex_sim": 1.0}
{"question": "where is the meridian that is opposite the prime meridian located", "answer": ["180\u00b0 longitude"], "scores": [[0.1728065460920334, 0.9830726385116577, 0.992293119430542, 0.9993256330490112, 0.4459177255630493, 0.21974942088127136]], "normalized_score": [0.5045984387397766], "tokens": [["\u2581", "1", "8", "0", "\u00b0", "\u2581longitude"]], "entropy": [[2.5561063289642334, 0.11475993692874908, 0.06278305500745773, 0.007723947521299124, 1.8245134353637695, 2.5857410430908203]], "avg_entropy": [1.1919379578127216], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "the 180th meridian", "score_of_biggest_cluster": 0.5045984387397766, "score_of_first_answer": 0.5045984387397766, "max_score_of_first_answer": 0.9993256330490112, "min_score_of_first_answer": 0.1728065460920334, "avg_score": 0.5045984387397766, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3659126423396437, "std_of_first_answer": 0.3659126423396437, "avg_entropy_of_first_answer": 1.1919379578127216, "lex_sim": 1.0}
{"question": "who sings i don't want to be lonely", "answer": ["Rihanna"], "scores": [[0.11318379640579224, 0.3414073586463928, 0.9969602227210999]], "normalized_score": [0.3377367854118347], "tokens": [["\u2581R", "ih", "anna"]], "entropy": [[3.311953544616699, 1.9930819272994995, 0.02452833764255047]], "avg_entropy": [1.7765212698529165], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ronnie Dyson", "score_of_biggest_cluster": 0.3377367854118347, "score_of_first_answer": 0.3377367854118347, "max_score_of_first_answer": 0.9969602227210999, "min_score_of_first_answer": 0.11318379640579224, "avg_score": 0.3377367854118347, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37459553353049957, "std_of_first_answer": 0.37459553353049957, "avg_entropy_of_first_answer": 1.7765212698529165, "lex_sim": 1.0}
{"question": "where does the donkey talk in the bible", "answer": ["Numbers 22:28"], "scores": [[0.8620603680610657, 0.9485452771186829, 0.8759016394615173, 0.9942454695701599, 0.9685289263725281, 0.6281481385231018, 0.9243835806846619, 0.793550968170166]], "normalized_score": [0.8664994835853577], "tokens": [["\u2581Num", "bers", "\u2581", "2", "2", ":", "2", "8"]], "entropy": [[0.9504978060722351, 0.28520137071609497, 0.542561411857605, 0.04378828406333923, 0.19066208600997925, 1.194309949874878, 0.3460617661476135, 0.852775514125824]], "avg_entropy": [0.5507322736084461], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Numbers 22:28", "score_of_biggest_cluster": 0.8664994835853577, "score_of_first_answer": 0.8664994835853577, "max_score_of_first_answer": 0.9942454695701599, "min_score_of_first_answer": 0.6281481385231018, "avg_score": 0.8664994835853577, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.11098994048033119, "std_of_first_answer": 0.11098994048033119, "avg_entropy_of_first_answer": 0.5507322736084461, "lex_sim": 1.0}
{"question": "where was the louisiana purchase signed in 1803", "answer": ["New Orleans"], "scores": [[0.31684452295303345, 0.9775773882865906]], "normalized_score": [0.5565429329872131], "tokens": [["\u2581New", "\u2581Orleans"]], "entropy": [[3.219414234161377, 0.18599802255630493]], "avg_entropy": [1.702706128358841], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paris", "score_of_biggest_cluster": 0.5565429329872131, "score_of_first_answer": 0.5565429329872131, "max_score_of_first_answer": 0.9775773882865906, "min_score_of_first_answer": 0.31684452295303345, "avg_score": 0.5565429329872131, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33036643266677856, "std_of_first_answer": 0.33036643266677856, "avg_entropy_of_first_answer": 1.702706128358841, "lex_sim": 1.0}
{"question": "where was the first cell phone call made", "answer": ["in a city in New Zealand - the call was made by Martin Cooper."], "scores": [[0.0158491563051939, 0.14524510502815247, 0.10185860097408295, 0.15589626133441925, 0.3046424388885498, 0.02027956210076809, 0.0031017111614346504, 0.11107074469327927, 0.057789187878370285, 0.8752906322479248, 0.8210157752037048, 0.3750823140144348, 0.06852498650550842, 0.8462494611740112, 0.04315297305583954]], "normalized_score": [0.1076766848564148], "tokens": [["\u2581in", "\u2581a", "\u2581city", "\u2581in", "\u2581New", "\u2581Zealand", "\u2581-", "\u2581the", "\u2581call", "\u2581was", "\u2581made", "\u2581by", "\u2581Martin", "\u2581Cooper", "."]], "entropy": [[4.609796524047852, 3.490114688873291, 4.129699230194092, 2.8965554237365723, 3.317741632461548, 1.3135309219360352, 1.7899823188781738, 5.048398017883301, 3.724454879760742, 0.8596929311752319, 0.9801443815231323, 1.9414660930633545, 3.799027919769287, 0.9147804975509644, 2.20654296875]], "avg_entropy": [2.7347952286402384], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "April 3, 1973", "score_of_biggest_cluster": 0.1076766848564148, "score_of_first_answer": 0.1076766848564148, "max_score_of_first_answer": 0.8752906322479248, "min_score_of_first_answer": 0.0031017111614346504, "avg_score": 0.1076766848564148, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30886382815800617, "std_of_first_answer": 0.30886382815800617, "avg_entropy_of_first_answer": 2.7347952286402384, "lex_sim": 1.0}
{"question": "what was the city of beijing previously known as", "answer": ["Peking"], "scores": [[0.861279308795929, 0.9837304353713989, 0.9810258746147156]], "normalized_score": [0.9402287602424622], "tokens": [["\u2581P", "ek", "ing"]], "entropy": [[0.7924013137817383, 0.10761173814535141, 0.10364768654108047]], "avg_entropy": [0.3345535794893901], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Peking", "score_of_biggest_cluster": 0.9402287602424622, "score_of_first_answer": 0.9402287602424622, "max_score_of_first_answer": 0.9837304353713989, "min_score_of_first_answer": 0.861279308795929, "avg_score": 0.9402287602424622, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.05709722028054076, "std_of_first_answer": 0.05709722028054076, "avg_entropy_of_first_answer": 0.3345535794893901, "lex_sim": 1.0}
{"question": "when were the winnie the pooh books written", "answer": ["Between 1924 \u2013 1928"], "scores": [[0.020216645672917366, 0.9249641299247742, 0.9954826831817627, 0.9946527481079102, 0.9737694263458252, 0.6819902062416077, 0.01709332875907421, 0.9588415026664734, 0.9687708616256714, 0.9993332028388977, 0.5878546237945557, 0.6362885236740112]], "normalized_score": [0.4521337151527405], "tokens": [["\u2581Between", "\u2581", "1", "9", "2", "4", "\u2581\u2013", "\u2581", "1", "9", "2", "8"]], "entropy": [[2.227430582046509, 0.47583964467048645, 0.03686372935771942, 0.041543833911418915, 0.1661297082901001, 1.1044495105743408, 0.7680315971374512, 0.3116908669471741, 0.16878847777843475, 0.007321027107536793, 1.3901065587997437, 0.8594253063201904]], "avg_entropy": [0.6298017369117588], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1924", "score_of_biggest_cluster": 0.4521337151527405, "score_of_first_answer": 0.4521337151527405, "max_score_of_first_answer": 0.9993332028388977, "min_score_of_first_answer": 0.01709332875907421, "avg_score": 0.4521337151527405, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3491892257236317, "std_of_first_answer": 0.3491892257236317, "avg_entropy_of_first_answer": 0.6298017369117588, "lex_sim": 1.0}
{"question": "how many episodes in season 2 of fargo", "answer": ["10"], "scores": [[0.9204618334770203, 0.9808807373046875, 0.9876627922058105]], "normalized_score": [0.9625210165977478], "tokens": [["\u2581", "1", "0"]], "entropy": [[0.5374064445495605, 0.1296902447938919, 0.1024339497089386]], "avg_entropy": [0.2565102130174637], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "10", "score_of_biggest_cluster": 0.9625210165977478, "score_of_first_answer": 0.9625210165977478, "max_score_of_first_answer": 0.9876627922058105, "min_score_of_first_answer": 0.9204618334770203, "avg_score": 0.9625210165977478, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03020744768814348, "std_of_first_answer": 0.03020744768814348, "avg_entropy_of_first_answer": 0.2565102130174637, "lex_sim": 1.0}
{"question": "how many inches is the iphone 5s screen", "answer": ["101.60 cm^2 (~4 inches)"], "scores": [[0.9647887945175171, 0.014624721370637417, 0.11977948993444443, 0.02991652861237526, 0.48837992548942566, 0.8573142290115356, 0.213065043091774, 0.02684955671429634, 0.00732648279517889, 0.8286619782447815, 0.04500911384820938, 0.20484669506549835, 0.5562874674797058, 0.08930835872888565, 0.9006045460700989]], "normalized_score": [0.14637243747711182], "tokens": [["\u2581", "1", "0", "1", ".", "6", "0", "\u2581cm", "^", "2", "\u2581(", "~", "4", "\u2581inches", ")"]], "entropy": [[0.31925278902053833, 0.5258699059486389, 2.2927770614624023, 2.8467018604278564, 1.9751642942428589, 0.716475784778595, 2.3138270378112793, 2.102639675140381, 1.0698238611221313, 0.5002774596214294, 0.6011896729469299, 2.1493756771087646, 1.4620722532272339, 1.2118194103240967, 0.6521913409233093]], "avg_entropy": [1.3826305389404296], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "4\u00a0in", "score_of_biggest_cluster": 0.14637243747711182, "score_of_first_answer": 0.14637243747711182, "max_score_of_first_answer": 0.9647887945175171, "min_score_of_first_answer": 0.00732648279517889, "avg_score": 0.14637243747711182, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3577036052120628, "std_of_first_answer": 0.3577036052120628, "avg_entropy_of_first_answer": 1.3826305389404296, "lex_sim": 1.0}
{"question": "who played the female lead in the 1942 film casablanca", "answer": ["Ingrid Bergman"], "scores": [[0.9786399602890015, 0.9993690848350525, 0.9958847165107727, 0.9962247610092163]], "normalized_score": [0.992496132850647], "tokens": [["\u2581In", "grid", "\u2581Berg", "man"]], "entropy": [[0.21438068151474, 0.007438098080456257, 0.03850308805704117, 0.02863745577633381]], "avg_entropy": [0.0722398308571428], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "romantic drama", "score_of_biggest_cluster": 0.992496132850647, "score_of_first_answer": 0.992496132850647, "max_score_of_first_answer": 0.9993690848350525, "min_score_of_first_answer": 0.9786399602890015, "avg_score": 0.992496132850647, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.008133444316391921, "std_of_first_answer": 0.008133444316391921, "avg_entropy_of_first_answer": 0.0722398308571428, "lex_sim": 1.0}
{"question": "what is the rank of pakistan in population", "answer": ["Ranks 5 among 144 independent countries."], "scores": [[0.00027079269057139754, 0.83172607421875, 0.5298275947570801, 0.42804771661758423, 0.015032309107482433, 0.6912210583686829, 0.5676114559173584, 0.03480735048651695, 0.08110636472702026, 0.4317724406719208, 0.9200437068939209, 0.07246532291173935]], "normalized_score": [0.13053834438323975], "tokens": [["\u2581R", "anks", "\u2581", "5", "\u2581among", "\u2581", "1", "4", "4", "\u2581independent", "\u2581countries", "."]], "entropy": [[0.9150509238243103, 1.0352814197540283, 2.226454973220825, 0.9257919788360596, 0.9457505941390991, 1.2935980558395386, 0.8991705179214478, 1.1641426086425781, 2.465977668762207, 1.5803911685943604, 0.32435134053230286, 1.7254594564437866]], "avg_entropy": [1.2917850588758786], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "5", "score_of_biggest_cluster": 0.13053834438323975, "score_of_first_answer": 0.13053834438323975, "max_score_of_first_answer": 0.9200437068939209, "min_score_of_first_answer": 0.00027079269057139754, "avg_score": 0.13053834438323975, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.320871902249209, "std_of_first_answer": 0.320871902249209, "avg_entropy_of_first_answer": 1.2917850588758786, "lex_sim": 1.0}
{"question": "where was held the first session of muslim league", "answer": ["Dhaka"], "scores": [[0.40186238288879395, 0.8386504054069519, 0.9991794228553772]], "normalized_score": [0.6957191228866577], "tokens": [["\u2581D", "h", "aka"]], "entropy": [[3.069596529006958, 0.5285916328430176, 0.008914751932024956]], "avg_entropy": [1.2023676379273336], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Dhaka, Bangladesh", "score_of_biggest_cluster": 0.6957191228866577, "score_of_first_answer": 0.6957191228866577, "max_score_of_first_answer": 0.9991794228553772, "min_score_of_first_answer": 0.40186238288879395, "avg_score": 0.6957191228866577, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2523976114283705, "std_of_first_answer": 0.2523976114283705, "avg_entropy_of_first_answer": 1.2023676379273336, "lex_sim": 1.0}
{"question": "where is the oldest house in america located", "answer": ["St. Augustine, Florida"], "scores": [[0.3872559368610382, 0.8874136209487915, 0.9896429181098938, 0.9992881417274475, 0.7481347918510437, 0.8529121279716492]], "normalized_score": [0.7751092314720154], "tokens": [["\u2581St", ".", "\u2581August", "ine", ",", "\u2581Florida"]], "entropy": [[3.095888137817383, 0.5424784421920776, 0.08458787202835083, 0.007610322907567024, 0.853056788444519, 0.5719886422157288]], "avg_entropy": [0.8592683676009377], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Taos Pueblo", "score_of_biggest_cluster": 0.7751092314720154, "score_of_first_answer": 0.7751092314720154, "max_score_of_first_answer": 0.9992881417274475, "min_score_of_first_answer": 0.3872559368610382, "avg_score": 0.7751092314720154, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20755406726344416, "std_of_first_answer": 0.20755406726344416, "avg_entropy_of_first_answer": 0.8592683676009377, "lex_sim": 1.0}
{"question": "when was the last easter that fell on april 1st", "answer": ["April 1, 2018"], "scores": [[0.33700698614120483, 0.9857466220855713, 0.9031004905700684, 0.8027147650718689, 0.9949880838394165, 0.18940263986587524, 0.998873770236969, 0.794372022151947, 0.967789888381958]], "normalized_score": [0.688691258430481], "tokens": [["\u2581April", "\u2581", "1", ",", "\u2581", "2", "0", "1", "8"]], "entropy": [[1.3902150392532349, 0.09540455788373947, 0.4902482032775879, 1.0073134899139404, 0.03910030424594879, 0.49169936776161194, 0.011326927691698074, 0.5394439101219177, 0.1969325691461563]], "avg_entropy": [0.47352048547731507], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "2018", "score_of_biggest_cluster": 0.688691258430481, "score_of_first_answer": 0.688691258430481, "max_score_of_first_answer": 0.998873770236969, "min_score_of_first_answer": 0.18940263986587524, "avg_score": 0.688691258430481, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.28527013653024963, "std_of_first_answer": 0.28527013653024963, "avg_entropy_of_first_answer": 0.47352048547731507, "lex_sim": 1.0}
{"question": "a concave mirror can form a real image which is a copy of an object that forms", "answer": ["concave mirrors are also referred to as diverging mirrors meaning they also diverge."], "scores": [[0.006667249836027622, 0.9891924262046814, 0.999897837638855, 0.628929615020752, 0.13669753074645996, 0.0868147611618042, 0.07407932728528976, 0.020168498158454895, 0.9809715151786804, 0.9885063171386719, 0.07559359818696976, 0.9749811291694641, 0.8093029260635376, 0.9851480722427368, 0.0018471002113074064, 0.16631293296813965, 0.007936271838843822, 0.2993077337741852, 0.9935718774795532, 0.028722049668431282, 0.0015704401303082705, 0.4186978042125702, 0.11140618473291397, 0.0007459084154106677, 0.7460864186286926, 0.0012133263517171144, 0.07214623689651489, 0.7393549084663391, 0.041882991790771484]], "normalized_score": [0.09987454861402512], "tokens": [["\u2581con", "ca", "ve", "\u2581mirror", "s", "\u2581are", "\u2581also", "\u2581referred", "\u2581to", "\u2581as", "\u2581diver", "ging", "\u2581mirror", "s", "\u2581meaning", "\u2581they", "\u2581also", "\u2581diver", "ge", ".", "\u2581Also", ",", "\u2581a", "\u2581true", "\u2581image", "\u2581could", "\u2581not", "\u2581be", "\u2581produced"]], "entropy": [[4.042820930480957, 0.10246429592370987, 0.0014000616502016783, 1.6133551597595215, 1.4579623937606812, 2.7186779975891113, 2.9878692626953125, 1.9676125049591064, 0.10796526819467545, 0.1128416359424591, 1.338880181312561, 0.12715525925159454, 0.725089967250824, 0.11558417975902557, 2.110844135284424, 1.9129629135131836, 3.2971529960632324, 3.4133119583129883, 0.054529689252376556, 2.6300408840179443, 2.3743107318878174, 3.269449472427368, 3.8677473068237305, 2.1069042682647705, 1.4690580368041992, 2.253335952758789, 1.8093886375427246, 1.2655705213546753, 2.549107551574707]], "avg_entropy": [1.7863239363590577], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "inverted", "score_of_biggest_cluster": 0.09987454861402512, "score_of_first_answer": 0.09987454861402512, "max_score_of_first_answer": 0.999897837638855, "min_score_of_first_answer": 0.0007459084154106677, "avg_score": 0.09987454861402512, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.409322042459492, "std_of_first_answer": 0.409322042459492, "avg_entropy_of_first_answer": 1.7863239363590577, "lex_sim": 1.0}
{"question": "when did the us dollar leave the gold standard", "answer": ["March 17, 1968"], "scores": [[0.12948021292686462, 0.9967586398124695, 0.9297717213630676, 0.16365192830562592, 0.9914014935493469, 0.9976748824119568, 0.9971840977668762, 0.9998342990875244, 0.9906182885169983, 0.9974086880683899]], "normalized_score": [0.6732575297355652], "tokens": [["\u2581March", "\u2581", "1", "7", ",", "\u2581", "1", "9", "6", "8"]], "entropy": [[1.5594301223754883, 0.026544418185949326, 0.33794888854026794, 1.7837921380996704, 0.06554146111011505, 0.018293876200914383, 0.020124129951000214, 0.0017939121462404728, 0.06125612556934357, 0.02218548208475113]], "avg_entropy": [0.38969105542637406], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "October 1976", "score_of_biggest_cluster": 0.6732575297355652, "score_of_first_answer": 0.6732575297355652, "max_score_of_first_answer": 0.9998342990875244, "min_score_of_first_answer": 0.12948021292686462, "avg_score": 0.6732575297355652, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3370705875633009, "std_of_first_answer": 0.3370705875633009, "avg_entropy_of_first_answer": 0.38969105542637406, "lex_sim": 1.0}
{"question": "another name for a hairpin bend crossword clue", "answer": ["dogleg"], "scores": [[0.03291456028819084, 0.6045858860015869]], "normalized_score": [0.14106622338294983], "tokens": [["\u2581dog", "leg"]], "entropy": [[5.176743984222412, 1.1142661571502686]], "avg_entropy": [3.1455050706863403], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "hairpin bend", "score_of_biggest_cluster": 0.14106622338294983, "score_of_first_answer": 0.14106622338294983, "max_score_of_first_answer": 0.6045858860015869, "min_score_of_first_answer": 0.03291456028819084, "avg_score": 0.14106622338294983, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28583566285669804, "std_of_first_answer": 0.28583566285669804, "avg_entropy_of_first_answer": 3.1455050706863403, "lex_sim": 1.0}
{"question": "who had the best strike rate among batsmen who scored over 200 runs in the world cup", "answer": ["Shivnarine Chanderpaul"], "scores": [[0.04786519706249237, 0.015093818306922913, 0.8377783298492432, 0.9956825971603394, 0.9931715726852417, 0.994628369808197, 0.9999550580978394, 0.9999189376831055]], "normalized_score": [0.3952186405658722], "tokens": [["\u2581Sh", "iv", "nar", "ine", "\u2581Ch", "ander", "pa", "ul"]], "entropy": [[4.506877899169922, 1.8178043365478516, 0.766671359539032, 0.02992076240479946, 0.07206828892230988, 0.04449150711297989, 0.0006023541791364551, 0.000991201726719737]], "avg_entropy": [0.9049284637003439], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chris Gayle", "score_of_biggest_cluster": 0.3952186405658722, "score_of_first_answer": 0.3952186405658722, "max_score_of_first_answer": 0.9999550580978394, "min_score_of_first_answer": 0.015093818306922913, "avg_score": 0.3952186405658722, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4097832555329082, "std_of_first_answer": 0.4097832555329082, "avg_entropy_of_first_answer": 0.9049284637003439, "lex_sim": 1.0}
{"question": "who plays dr. sloan on kevin saves the world", "answer": ["Joel Gretsch"], "scores": [[0.12736031413078308, 0.5410637855529785, 0.11158627271652222, 0.9564566612243652, 0.9850319623947144]], "normalized_score": [0.37325167655944824], "tokens": [["\u2581Jo", "el", "\u2581G", "ret", "sch"]], "entropy": [[5.0841450691223145, 1.4754329919815063, 3.831206798553467, 0.2970990538597107, 0.13309884071350098]], "avg_entropy": [2.1641965508461], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Richard Masur", "score_of_biggest_cluster": 0.37325167655944824, "score_of_first_answer": 0.37325167655944824, "max_score_of_first_answer": 0.9850319623947144, "min_score_of_first_answer": 0.11158627271652222, "avg_score": 0.37325167655944824, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3808433065143561, "std_of_first_answer": 0.3808433065143561, "avg_entropy_of_first_answer": 2.1641965508461, "lex_sim": 1.0}
{"question": "what episode does caroline get turned into a vampire", "answer": ["episode 18 season 2"], "scores": [[0.02104206383228302, 0.9238453507423401, 0.4715859293937683, 0.07739440351724625, 0.13835012912750244, 0.9511043429374695, 0.3670153319835663]], "normalized_score": [0.23020851612091064], "tokens": [["\u2581episode", "\u2581", "1", "8", "\u2581season", "\u2581", "2"]], "entropy": [[3.874788761138916, 0.5941293239593506, 1.7581478357315063, 2.6537728309631348, 2.3371362686157227, 0.2916661202907562, 1.4219653606414795]], "avg_entropy": [1.8473723573344094], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brave New World", "score_of_biggest_cluster": 0.23020851612091064, "score_of_first_answer": 0.23020851612091064, "max_score_of_first_answer": 0.9511043429374695, "min_score_of_first_answer": 0.02104206383228302, "avg_score": 0.23020851612091064, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3580219180478069, "std_of_first_answer": 0.3580219180478069, "avg_entropy_of_first_answer": 1.8473723573344094, "lex_sim": 1.0}
{"question": "ajay devgan preity zinta and madhuri dixit movie", "answer": ["Rani Mukherji"], "scores": [[0.005602661520242691, 0.06615091860294342, 0.7319977879524231, 0.991611659526825, 0.35392335057258606, 0.4442620575428009]], "normalized_score": [0.18666161596775055], "tokens": [["\u2581R", "ani", "\u2581M", "uk", "her", "ji"]], "entropy": [[3.6540591716766357, 1.826676607131958, 1.53713059425354, 0.06532637774944305, 0.7442657947540283, 0.7162094712257385]], "avg_entropy": [1.423944669465224], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yeh Raaste Hain Pyaar Ke", "score_of_biggest_cluster": 0.18666161596775055, "score_of_first_answer": 0.18666161596775055, "max_score_of_first_answer": 0.991611659526825, "min_score_of_first_answer": 0.005602661520242691, "avg_score": 0.18666161596775055, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34762768505700103, "std_of_first_answer": 0.34762768505700103, "avg_entropy_of_first_answer": 1.423944669465224, "lex_sim": 1.0}
{"question": "when did the us cut off trade with cuba", "answer": ["January 3, 1961"], "scores": [[0.1144595667719841, 0.994355320930481, 0.6526612639427185, 0.9846358299255371, 0.9980865716934204, 0.9822585582733154, 0.9997928738594055, 0.8931383490562439, 0.9736528992652893]], "normalized_score": [0.7346612215042114], "tokens": [["\u2581January", "\u2581", "3", ",", "\u2581", "1", "9", "6", "1"]], "entropy": [[2.441028118133545, 0.04204712435603142, 1.2089166641235352, 0.1034710705280304, 0.015122219920158386, 0.08938811719417572, 0.0020841287914663553, 0.36627471446990967, 0.1506931632757187]], "avg_entropy": [0.49100281342139673], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "1961", "score_of_biggest_cluster": 0.7346612215042114, "score_of_first_answer": 0.7346612215042114, "max_score_of_first_answer": 0.9997928738594055, "min_score_of_first_answer": 0.1144595667719841, "avg_score": 0.7346612215042114, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2784037663009513, "std_of_first_answer": 0.2784037663009513, "avg_entropy_of_first_answer": 0.49100281342139673, "lex_sim": 1.0}
{"question": "what is the current mac os operating system", "answer": ["Mac OS X"], "scores": [[0.13070707023143768, 0.5306187868118286, 0.7016453146934509]], "normalized_score": [0.36508986353874207], "tokens": [["\u2581Mac", "\u2581OS", "\u2581X"]], "entropy": [[1.4646447896957397, 0.8062924146652222, 1.1981236934661865]], "avg_entropy": [1.1563536326090496], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "10.13.4", "score_of_biggest_cluster": 0.36508986353874207, "score_of_first_answer": 0.36508986353874207, "max_score_of_first_answer": 0.7016453146934509, "min_score_of_first_answer": 0.13070707023143768, "avg_score": 0.36508986353874207, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23924648686306854, "std_of_first_answer": 0.23924648686306854, "avg_entropy_of_first_answer": 1.1563536326090496, "lex_sim": 1.0}
{"question": "where does the great outdoors movie take place", "answer": ["the Appalachian mountains"], "scores": [[0.04279790073633194, 0.01098466757684946, 0.9852004051208496, 0.9976174235343933, 0.9636163711547852, 0.08930666744709015]], "normalized_score": [0.18474853038787842], "tokens": [["\u2581the", "\u2581App", "al", "ach", "ian", "\u2581mountains"]], "entropy": [[4.622060298919678, 4.4861345291137695, 0.1073070615530014, 0.02529551275074482, 0.19088268280029297, 1.508947730064392]], "avg_entropy": [1.823437969200313], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in Pechoggin, Wisconsin", "score_of_biggest_cluster": 0.18474853038787842, "score_of_first_answer": 0.18474853038787842, "max_score_of_first_answer": 0.9976174235343933, "min_score_of_first_answer": 0.01098466757684946, "avg_score": 0.18474853038787842, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46788276671570117, "std_of_first_answer": 0.46788276671570117, "avg_entropy_of_first_answer": 1.823437969200313, "lex_sim": 1.0}
{"question": "who has been ranked no. 1 in the latest football rankings announced by fifa", "answer": ["Belgium"], "scores": [[0.0048651560209691525, 0.9907668828964233]], "normalized_score": [0.06942792236804962], "tokens": [["\u2581Belg", "ium"]], "entropy": [[3.363903522491455, 0.056637976318597794]], "avg_entropy": [1.7102707494050264], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Germany", "score_of_biggest_cluster": 0.06942792236804962, "score_of_first_answer": 0.06942792236804962, "max_score_of_first_answer": 0.9907668828964233, "min_score_of_first_answer": 0.0048651560209691525, "avg_score": 0.06942792236804962, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4929508634377271, "std_of_first_answer": 0.4929508634377271, "avg_entropy_of_first_answer": 1.7102707494050264, "lex_sim": 1.0}
{"question": "who voiced mewtwo in pokemon the first movie", "answer": ["Gary Chalk"], "scores": [[0.19116921722888947, 0.30631542205810547, 0.9894357919692993]], "normalized_score": [0.3869529664516449], "tokens": [["\u2581Gary", "\u2581Ch", "alk"]], "entropy": [[4.667906761169434, 2.790371894836426, 0.07959607243537903]], "avg_entropy": [2.512624909480413], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Showtaro Morikubo", "score_of_biggest_cluster": 0.3869529664516449, "score_of_first_answer": 0.3869529664516449, "max_score_of_first_answer": 0.9894357919692993, "min_score_of_first_answer": 0.19116921722888947, "avg_score": 0.3869529664516449, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3523164001957402, "std_of_first_answer": 0.3523164001957402, "avg_entropy_of_first_answer": 2.512624909480413, "lex_sim": 1.0}
{"question": "who has the power (judicial) to make decisions in courts of law", "answer": ["judge"], "scores": [[0.31838685274124146]], "normalized_score": [0.31838685274124146], "tokens": [["\u2581judge"]], "entropy": [[2.3470988273620605]], "avg_entropy": [2.3470988273620605], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "judges", "score_of_biggest_cluster": 0.31838685274124146, "score_of_first_answer": 0.31838685274124146, "max_score_of_first_answer": 0.31838685274124146, "min_score_of_first_answer": 0.31838685274124146, "avg_score": 0.31838685274124146, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.3470988273620605, "lex_sim": 1.0}
{"question": "who made the nuclear model of an atom", "answer": ["Niels Bohr"], "scores": [[0.24809572100639343, 0.9045785069465637, 0.9641761183738708, 0.9999589920043945]], "normalized_score": [0.6820261478424072], "tokens": [["\u2581N", "iels", "\u2581Bo", "hr"]], "entropy": [[2.054865837097168, 0.6108932495117188, 0.21338556706905365, 0.0005886796861886978]], "avg_entropy": [0.7199333333410323], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ernest Rutherford", "score_of_biggest_cluster": 0.6820261478424072, "score_of_first_answer": 0.6820261478424072, "max_score_of_first_answer": 0.9999589920043945, "min_score_of_first_answer": 0.24809572100639343, "avg_score": 0.6820261478424072, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3085215641033572, "std_of_first_answer": 0.3085215641033572, "avg_entropy_of_first_answer": 0.7199333333410323, "lex_sim": 1.0}
{"question": "when does the next star trek episode come out", "answer": ["2013-05-16"], "scores": [[0.11231274902820587, 0.7690600156784058, 0.8446673154830933, 0.9033297300338745, 0.14047330617904663, 0.03643101453781128, 0.7975355982780457, 0.8273503184318542, 0.9824076294898987, 0.8504281640052795, 0.15094177424907684]], "normalized_score": [0.3856530487537384], "tokens": [["\u2581", "2", "0", "1", "3", "-", "0", "5", "-", "1", "6"]], "entropy": [[3.6936287879943848, 1.0052671432495117, 0.7617546319961548, 0.4621742069721222, 1.6120777130126953, 0.7086181640625, 0.8140739798545837, 0.800916850566864, 0.10558236390352249, 0.5464410185813904, 0.8310433626174927]], "avg_entropy": [1.031052565710111], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2018", "score_of_biggest_cluster": 0.3856530487537384, "score_of_first_answer": 0.3856530487537384, "max_score_of_first_answer": 0.9824076294898987, "min_score_of_first_answer": 0.03643101453781128, "avg_score": 0.3856530487537384, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3624743396295961, "std_of_first_answer": 0.3624743396295961, "avg_entropy_of_first_answer": 1.031052565710111, "lex_sim": 1.0}
{"question": "where is the setting for beauty and the beast", "answer": ["French town of Villeneuve sur Lot"], "scores": [[0.047868095338344574, 0.015402584336698055, 0.5247393250465393, 0.8783648014068604, 0.9715547561645508, 0.9959574341773987, 0.004588391166180372, 0.366290807723999]], "normalized_score": [0.1651226133108139], "tokens": [["\u2581French", "\u2581town", "\u2581of", "\u2581Vill", "ene", "uve", "\u2581sur", "\u2581Lot"]], "entropy": [[2.750593662261963, 2.5799598693847656, 1.344665765762329, 0.8718829154968262, 0.2054586112499237, 0.033746231347322464, 0.7902472615242004, 2.3925955295562744]], "avg_entropy": [1.3711437308229506], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rococo-era France", "score_of_biggest_cluster": 0.1651226133108139, "score_of_first_answer": 0.1651226133108139, "max_score_of_first_answer": 0.9959574341773987, "min_score_of_first_answer": 0.004588391166180372, "avg_score": 0.1651226133108139, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4046470401260742, "std_of_first_answer": 0.4046470401260742, "avg_entropy_of_first_answer": 1.3711437308229506, "lex_sim": 1.0}
{"question": "name the tissue type of the inner most layer of cells", "answer": ["mesothelium"], "scores": [[0.0041105905547738075, 0.6554346084594727, 0.988135576248169, 0.919142484664917]], "normalized_score": [0.22241199016571045], "tokens": [["\u2581mes", "oth", "el", "ium"]], "entropy": [[2.079868793487549, 1.0797836780548096, 0.09410515427589417, 0.29578498005867004]], "avg_entropy": [0.8873856514692307], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Epithelium", "score_of_biggest_cluster": 0.22241199016571045, "score_of_first_answer": 0.22241199016571045, "max_score_of_first_answer": 0.988135576248169, "min_score_of_first_answer": 0.0041105905547738075, "avg_score": 0.22241199016571045, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3884909378474724, "std_of_first_answer": 0.3884909378474724, "avg_entropy_of_first_answer": 0.8873856514692307, "lex_sim": 1.0}
{"question": "when did linkin park release one more light", "answer": ["May 19, 2017"], "scores": [[0.4718073904514313, 0.9967261552810669, 0.706186830997467, 0.960702121257782, 0.9857736825942993, 0.9981682300567627, 0.9999085664749146, 0.9999713897705078, 0.9996634721755981, 0.9988037347793579]], "normalized_score": [0.8904533386230469], "tokens": [["\u2581May", "\u2581", "1", "9", ",", "\u2581", "2", "0", "1", "7"]], "entropy": [[2.057587146759033, 0.02704170159995556, 0.9994902014732361, 0.24581992626190186, 0.09467289596796036, 0.01486587431281805, 0.0010094251483678818, 0.00040806178003549576, 0.003315768437460065, 0.011664720252156258]], "avg_entropy": [0.3455875721992925], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "May 19, 2017", "score_of_biggest_cluster": 0.8904533386230469, "score_of_first_answer": 0.8904533386230469, "max_score_of_first_answer": 0.9999713897705078, "min_score_of_first_answer": 0.4718073904514313, "avg_score": 0.8904533386230469, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1700684164294856, "std_of_first_answer": 0.1700684164294856, "avg_entropy_of_first_answer": 0.3455875721992925, "lex_sim": 1.0}
{"question": "who is edmund on days of our lives", "answer": ["Eric Winter"], "scores": [[0.1356382519006729, 0.43726402521133423]], "normalized_score": [0.24353589117527008], "tokens": [["\u2581Eric", "\u2581Winter"]], "entropy": [[4.197933197021484, 1.00792396068573]], "avg_entropy": [2.602928578853607], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Adam Caine", "score_of_biggest_cluster": 0.24353589117527008, "score_of_first_answer": 0.24353589117527008, "max_score_of_first_answer": 0.43726402521133423, "min_score_of_first_answer": 0.1356382519006729, "avg_score": 0.24353589117527008, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.15081288665533066, "std_of_first_answer": 0.15081288665533066, "avg_entropy_of_first_answer": 2.602928578853607, "lex_sim": 1.0}
{"question": "when did seattle slew win the triple crown", "answer": ["1977"], "scores": [[0.39257732033729553, 0.959679126739502, 0.9945468902587891, 0.997096061706543, 0.9845579266548157]], "normalized_score": [0.8187116384506226], "tokens": [["\u2581", "1", "9", "7", "7"]], "entropy": [[1.3371756076812744, 0.21778419613838196, 0.048177141696214676, 0.02523138001561165, 0.1033545508980751]], "avg_entropy": [0.34634457528591156], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "1977", "score_of_biggest_cluster": 0.8187116384506226, "score_of_first_answer": 0.8187116384506226, "max_score_of_first_answer": 0.997096061706543, "min_score_of_first_answer": 0.39257732033729553, "avg_score": 0.8187116384506226, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23692648598236113, "std_of_first_answer": 0.23692648598236113, "avg_entropy_of_first_answer": 0.34634457528591156, "lex_sim": 1.0}
{"question": "when does the team find out prentiss is alive", "answer": ["The ninth season finale"], "scores": [[0.1322208195924759, 0.005636051297187805, 0.9236871600151062, 0.7052695155143738, 0.09046344459056854]], "normalized_score": [0.13443909585475922], "tokens": [["\u2581The", "\u2581nin", "th", "\u2581season", "\u2581finale"]], "entropy": [[3.8397088050842285, 4.606442928314209, 0.3094857931137085, 1.045241355895996, 2.4148645401000977]], "avg_entropy": [2.443148684501648], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "In the season seven premiere", "score_of_biggest_cluster": 0.13443909585475922, "score_of_first_answer": 0.13443909585475922, "max_score_of_first_answer": 0.9236871600151062, "min_score_of_first_answer": 0.005636051297187805, "avg_score": 0.13443909585475922, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3705145718697054, "std_of_first_answer": 0.3705145718697054, "avg_entropy_of_first_answer": 2.443148684501648, "lex_sim": 1.0}
{"question": "who laid foundation stone of islamia college peshawar", "answer": ["Muhammad Ali Jinnah"], "scores": [[0.2569738030433655, 0.9712080955505371, 0.9791921377182007, 0.9961737990379333, 0.999201238155365]], "normalized_score": [0.7537224292755127], "tokens": [["\u2581Muhammad", "\u2581Ali", "\u2581J", "inn", "ah"]], "entropy": [[3.868107557296753, 0.1929437220096588, 0.15438762307167053, 0.03526030480861664, 0.008994946256279945]], "avg_entropy": [0.8519388306885958], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Haji Sahib", "score_of_biggest_cluster": 0.7537224292755127, "score_of_first_answer": 0.7537224292755127, "max_score_of_first_answer": 0.999201238155365, "min_score_of_first_answer": 0.2569738030433655, "avg_score": 0.7537224292755127, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2919737454407133, "std_of_first_answer": 0.2919737454407133, "avg_entropy_of_first_answer": 0.8519388306885958, "lex_sim": 1.0}
{"question": "who sang i knew the bride when she used to rock and roll", "answer": ["Nicky Byrne"], "scores": [[0.8228264451026917, 0.06407713890075684, 0.046017833054065704, 0.9558914303779602, 0.9996815919876099]], "normalized_score": [0.29719504714012146], "tokens": [["\u2581Nick", "y", "\u2581By", "r", "ne"]], "entropy": [[1.1164302825927734, 0.8253750801086426, 2.5038108825683594, 0.25779467821121216, 0.0029501100070774555]], "avg_entropy": [0.941272206697613], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dave Edmunds", "score_of_biggest_cluster": 0.29719504714012146, "score_of_first_answer": 0.29719504714012146, "max_score_of_first_answer": 0.9996815919876099, "min_score_of_first_answer": 0.046017833054065704, "avg_score": 0.29719504714012146, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43073856715235537, "std_of_first_answer": 0.43073856715235537, "avg_entropy_of_first_answer": 0.941272206697613, "lex_sim": 1.0}
{"question": "who won season 2 of great british baking show", "answer": ["Iain Watkins"], "scores": [[0.004802930634468794, 0.3102961480617523, 0.6369878649711609, 0.05337102711200714]], "normalized_score": [0.08436840027570724], "tokens": [["\u2581I", "ain", "\u2581Wat", "kins"]], "entropy": [[3.981111764907837, 3.2375051975250244, 2.090158462524414, 1.528452754020691]], "avg_entropy": [2.7093070447444916], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joanne Wheatley", "score_of_biggest_cluster": 0.08436840027570724, "score_of_first_answer": 0.08436840027570724, "max_score_of_first_answer": 0.6369878649711609, "min_score_of_first_answer": 0.004802930634468794, "avg_score": 0.08436840027570724, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2510838214367849, "std_of_first_answer": 0.2510838214367849, "avg_entropy_of_first_answer": 2.7093070447444916, "lex_sim": 1.0}
{"question": "when does dragon ball super episode 130 comes out", "answer": ["February 26, 2019"], "scores": [[0.041226837784051895, 0.9775076508522034, 0.3364827036857605, 0.02822575904428959, 0.719062089920044, 0.9942905902862549, 0.9968731999397278, 0.9996540546417236, 0.9772273898124695, 0.17604409158229828]], "normalized_score": [0.3690876364707947], "tokens": [["\u2581February", "\u2581", "2", "6", ",", "\u2581", "2", "0", "1", "9"]], "entropy": [[3.437478542327881, 0.1434725821018219, 1.5628907680511475, 1.7629969120025635, 0.8403254747390747, 0.04607445001602173, 0.024621540680527687, 0.004118059296160936, 0.11923149973154068, 1.1459404230117798]], "avg_entropy": [0.9087150251958519], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "March 18, 2018", "score_of_biggest_cluster": 0.3690876364707947, "score_of_first_answer": 0.3690876364707947, "max_score_of_first_answer": 0.9996540546417236, "min_score_of_first_answer": 0.02822575904428959, "avg_score": 0.3690876364707947, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40668629311850585, "std_of_first_answer": 0.40668629311850585, "avg_entropy_of_first_answer": 0.9087150251958519, "lex_sim": 1.0}
{"question": "bible verse taking the lord's name in vain", "answer": ["Matthew 23:24"], "scores": [[0.02435382455587387, 0.9911231994628906, 0.11205997318029404, 0.6026812195777893, 0.8748829960823059, 0.40969473123550415, 0.06973686814308167]], "normalized_score": [0.23597842454910278], "tokens": [["\u2581Matthew", "\u2581", "2", "3", ":", "2", "4"]], "entropy": [[2.2553038597106934, 0.07689426839351654, 1.3836522102355957, 1.4675509929656982, 0.5328097343444824, 1.5843086242675781, 2.1648285388946533]], "avg_entropy": [1.352192604116031], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Exodus 20:7", "score_of_biggest_cluster": 0.23597842454910278, "score_of_first_answer": 0.23597842454910278, "max_score_of_first_answer": 0.9911231994628906, "min_score_of_first_answer": 0.02435382455587387, "avg_score": 0.23597842454910278, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36599021176453844, "std_of_first_answer": 0.36599021176453844, "avg_entropy_of_first_answer": 1.352192604116031, "lex_sim": 1.0}
{"question": "baga beach is in north or south goa", "answer": ["North Goa."], "scores": [[0.5483162999153137, 0.6452204585075378, 0.9995131492614746, 0.011417247354984283]], "normalized_score": [0.25207066535949707], "tokens": [["\u2581North", "\u2581Go", "a", "."]], "entropy": [[1.522721290588379, 0.8342432379722595, 0.005291756242513657, 0.3357877731323242]], "avg_entropy": [0.6745110144838691], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "North Goa", "score_of_biggest_cluster": 0.25207066535949707, "score_of_first_answer": 0.25207066535949707, "max_score_of_first_answer": 0.9995131492614746, "min_score_of_first_answer": 0.011417247354984283, "avg_score": 0.25207066535949707, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3539767769577107, "std_of_first_answer": 0.3539767769577107, "avg_entropy_of_first_answer": 0.6745110144838691, "lex_sim": 1.0}
{"question": "who plays yoda in revenge of the sith", "answer": ["Frank Oz"], "scores": [[0.7423765063285828, 0.996224045753479, 0.9987718462944031]], "normalized_score": [0.9039599299430847], "tokens": [["\u2581Frank", "\u2581O", "z"]], "entropy": [[1.9111363887786865, 0.03190785273909569, 0.012885977514088154]], "avg_entropy": [0.6519767396772901], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Frank Oz", "score_of_biggest_cluster": 0.9039599299430847, "score_of_first_answer": 0.9039599299430847, "max_score_of_first_answer": 0.9987718462944031, "min_score_of_first_answer": 0.7423765063285828, "avg_score": 0.9039599299430847, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1202698978297743, "std_of_first_answer": 0.1202698978297743, "avg_entropy_of_first_answer": 0.6519767396772901, "lex_sim": 1.0}
{"question": "who sings the song it ain't me", "answer": ["bennie mann"], "scores": [[0.0028244759887456894, 0.0024183979257941246, 0.9855965971946716, 0.6079018115997314, 0.8710318207740784]], "normalized_score": [0.0813591405749321], "tokens": [["\u2581b", "enn", "ie", "\u2581man", "n"]], "entropy": [[4.055482864379883, 1.3478693962097168, 0.1535119116306305, 1.8142154216766357, 0.64112788438797]], "avg_entropy": [1.6024414956569673], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Selena Gomez", "score_of_biggest_cluster": 0.0813591405749321, "score_of_first_answer": 0.0813591405749321, "max_score_of_first_answer": 0.9855965971946716, "min_score_of_first_answer": 0.0024183979257941246, "avg_score": 0.0813591405749321, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41945194898211285, "std_of_first_answer": 0.41945194898211285, "avg_entropy_of_first_answer": 1.6024414956569673, "lex_sim": 1.0}
{"question": "who owns and operates the panama canal today", "answer": ["The people of Panama"], "scores": [[0.17144867777824402, 0.003556692274287343, 0.9333878755569458, 0.895654022693634, 0.9805936217308044]], "normalized_score": [0.2186625599861145], "tokens": [["\u2581The", "\u2581people", "\u2581of", "\u2581Pan", "ama"]], "entropy": [[2.0426487922668457, 1.7734346389770508, 0.43737393617630005, 0.4639713764190674, 0.10321815311908722]], "avg_entropy": [0.9641293793916702], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Panama Canal Authority", "score_of_biggest_cluster": 0.2186625599861145, "score_of_first_answer": 0.2186625599861145, "max_score_of_first_answer": 0.9805936217308044, "min_score_of_first_answer": 0.003556692274287343, "avg_score": 0.2186625599861145, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.42018185497769567, "std_of_first_answer": 0.42018185497769567, "avg_entropy_of_first_answer": 0.9641293793916702, "lex_sim": 1.0}
{"question": "who sings far from the home i love in fiddler on the roof", "answer": ["Sheldon Harnick"], "scores": [[0.035103488713502884, 0.8443800806999207, 0.9882334470748901, 0.9555929899215698, 0.9975569248199463, 0.9984431862831116]], "normalized_score": [0.5506559014320374], "tokens": [["\u2581Sh", "eld", "on", "\u2581H", "arn", "ick"]], "entropy": [[4.630285263061523, 0.9269267320632935, 0.11885850131511688, 0.3656579852104187, 0.029952822253108025, 0.016308162361383438]], "avg_entropy": [1.0146649110441406], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hodel", "score_of_biggest_cluster": 0.5506559014320374, "score_of_first_answer": 0.5506559014320374, "max_score_of_first_answer": 0.9984431862831116, "min_score_of_first_answer": 0.035103488713502884, "avg_score": 0.5506559014320374, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34761654247294344, "std_of_first_answer": 0.34761654247294344, "avg_entropy_of_first_answer": 1.0146649110441406, "lex_sim": 1.0}
{"question": "who was the great wall of china built to defend against", "answer": ["Huns"], "scores": [[0.06499366462230682, 0.9917858242988586]], "normalized_score": [0.2538893520832062], "tokens": [["\u2581H", "uns"]], "entropy": [[3.1190309524536133, 0.07003536820411682]], "avg_entropy": [1.594533160328865], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "nomads from Inner Asia", "score_of_biggest_cluster": 0.2538893520832062, "score_of_first_answer": 0.2538893520832062, "max_score_of_first_answer": 0.9917858242988586, "min_score_of_first_answer": 0.06499366462230682, "avg_score": 0.2538893520832062, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4633960798382759, "std_of_first_answer": 0.4633960798382759, "avg_entropy_of_first_answer": 1.594533160328865, "lex_sim": 1.0}
{"question": "who does the voice of marceline on adventure time", "answer": ["Olivia Olson"], "scores": [[0.9893478155136108, 0.9996154308319092, 0.9560387134552002, 0.9952613711357117]], "normalized_score": [0.9849147796630859], "tokens": [["\u2581Oliv", "ia", "\u2581Ol", "son"]], "entropy": [[0.08835627138614655, 0.004488721024245024, 0.31014904379844666, 0.0535745769739151]], "avg_entropy": [0.11414215329568833], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cloris Leachman", "score_of_biggest_cluster": 0.9849147796630859, "score_of_first_answer": 0.9849147796630859, "max_score_of_first_answer": 0.9996154308319092, "min_score_of_first_answer": 0.9560387134552002, "avg_score": 0.9849147796630859, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01715042884494448, "std_of_first_answer": 0.01715042884494448, "avg_entropy_of_first_answer": 0.11414215329568833, "lex_sim": 1.0}
{"question": "who is in charge of new issues not mentioned in the constitution", "answer": ["President"], "scores": [[0.02391205169260502]], "normalized_score": [0.02391205169260502], "tokens": [["\u2581President"]], "entropy": [[4.1781487464904785]], "avg_entropy": [4.1781487464904785], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Congress", "score_of_biggest_cluster": 0.02391205169260502, "score_of_first_answer": 0.02391205169260502, "max_score_of_first_answer": 0.02391205169260502, "min_score_of_first_answer": 0.02391205169260502, "avg_score": 0.02391205169260502, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.1781487464904785, "lex_sim": 1.0}
{"question": "who plays chummy's mother in call the midwife", "answer": ["Lesley Nichol"], "scores": [[0.039508502930402756, 0.9811496734619141, 0.029503781348466873, 0.9270909428596497]], "normalized_score": [0.18044984340667725], "tokens": [["\u2581Les", "ley", "\u2581Nich", "ol"]], "entropy": [[4.3249921798706055, 0.10849302262067795, 2.118959426879883, 0.2973392605781555]], "avg_entropy": [1.7124459724873304], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cheryl Campbell", "score_of_biggest_cluster": 0.18044984340667725, "score_of_first_answer": 0.18044984340667725, "max_score_of_first_answer": 0.9811496734619141, "min_score_of_first_answer": 0.029503781348466873, "avg_score": 0.18044984340667725, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46021772965392904, "std_of_first_answer": 0.46021772965392904, "avg_entropy_of_first_answer": 1.7124459724873304, "lex_sim": 1.0}
{"question": "when did the log flume closed at alton towers", "answer": ["September 13, 2008"], "scores": [[0.07194234430789948, 0.9825224876403809, 0.2956862151622772, 0.06925594806671143, 0.9464735984802246, 0.9966963529586792, 0.8929958343505859, 0.9997455477714539, 0.16851051151752472, 0.12627273797988892]], "normalized_score": [0.3478533625602722], "tokens": [["\u2581September", "\u2581", "1", "3", ",", "\u2581", "2", "0", "0", "8"]], "entropy": [[2.9213311672210693, 0.11179080605506897, 1.661193609237671, 2.4032504558563232, 0.30652934312820435, 0.029419660568237305, 0.3434101939201355, 0.0031169713474810123, 0.47079208493232727, 2.1479427814483643]], "avg_entropy": [1.0398777073714882], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "10\u00a0October\u00a02015", "score_of_biggest_cluster": 0.3478533625602722, "score_of_first_answer": 0.3478533625602722, "max_score_of_first_answer": 0.9997455477714539, "min_score_of_first_answer": 0.06925594806671143, "avg_score": 0.3478533625602722, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41386946261547586, "std_of_first_answer": 0.41386946261547586, "avg_entropy_of_first_answer": 1.0398777073714882, "lex_sim": 1.0}
{"question": "who were the three elves who got rings", "answer": ["C\u00edrdan, Gil-galad, Finrod"], "scores": [[0.48630863428115845, 0.3417854607105255, 0.997033953666687, 0.9992947578430176, 0.9117026329040527, 0.10517552495002747, 0.9673385620117188, 0.995814859867096, 0.9972791075706482, 0.5393179059028625, 0.0008653824334032834, 0.9669007062911987]], "normalized_score": [0.3713816702365875], "tokens": [["\u2581C", "\u00ed", "rd", "an", ",", "\u2581Gil", "-", "gal", "ad", ",", "\u2581Fin", "rod"]], "entropy": [[1.978524923324585, 1.1818561553955078, 0.03155096247792244, 0.0076591698452830315, 0.5508679151535034, 2.623805284500122, 0.18733088672161102, 0.029090553522109985, 0.027562513947486877, 1.0017505884170532, 1.2232438325881958, 0.1798393726348877]], "avg_entropy": [0.7519235132106891], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "C\u00edrdan", "score_of_biggest_cluster": 0.3713816702365875, "score_of_first_answer": 0.3713816702365875, "max_score_of_first_answer": 0.9992947578430176, "min_score_of_first_answer": 0.0008653824334032834, "avg_score": 0.3713816702365875, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3631946936626937, "std_of_first_answer": 0.3631946936626937, "avg_entropy_of_first_answer": 0.7519235132106891, "lex_sim": 1.0}
{"question": "who wrote from now on from the greatest showman", "answer": ["Justin Paul"], "scores": [[0.13390327990055084, 0.7578706741333008]], "normalized_score": [0.3185614347457886], "tokens": [["\u2581Justin", "\u2581Paul"]], "entropy": [[2.377591848373413, 1.0987969636917114]], "avg_entropy": [1.7381944060325623], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joseph Trapanese", "score_of_biggest_cluster": 0.3185614347457886, "score_of_first_answer": 0.3185614347457886, "max_score_of_first_answer": 0.7578706741333008, "min_score_of_first_answer": 0.13390327990055084, "avg_score": 0.3185614347457886, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31198369711637497, "std_of_first_answer": 0.31198369711637497, "avg_entropy_of_first_answer": 1.7381944060325623, "lex_sim": 1.0}
{"question": "who played marius in the movie les miserables", "answer": ["Eddie Redmayne"], "scores": [[0.8840593695640564, 0.9983065128326416, 0.9942511320114136, 0.9954608082771301, 0.9995622038841248]], "normalized_score": [0.9732291102409363], "tokens": [["\u2581Edd", "ie", "\u2581Red", "may", "ne"]], "entropy": [[0.5438359379768372, 0.016538996249437332, 0.056423891335725784, 0.043194323778152466, 0.0050011081621050835]], "avg_entropy": [0.13299885150045157], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Eddie Redmayne", "score_of_biggest_cluster": 0.9732291102409363, "score_of_first_answer": 0.9732291102409363, "max_score_of_first_answer": 0.9995622038841248, "min_score_of_first_answer": 0.8840593695640564, "avg_score": 0.9732291102409363, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.045174520559567, "std_of_first_answer": 0.045174520559567, "avg_entropy_of_first_answer": 0.13299885150045157, "lex_sim": 1.0}
{"question": "where did students for a democratic society start", "answer": ["Upper West Side, New York City."], "scores": [[0.0018157324520871043, 0.786772608757019, 0.9580052495002747, 0.45532047748565674, 0.6418171525001526, 0.9988892674446106, 0.5818848609924316, 0.003914115019142628]], "normalized_score": [0.1757410317659378], "tokens": [["\u2581Upper", "\u2581West", "\u2581Side", ",", "\u2581New", "\u2581York", "\u2581City", "."]], "entropy": [[2.9676308631896973, 1.1735941171646118, 0.2283032238483429, 1.6088606119155884, 0.9855391979217529, 0.012169981375336647, 1.0458213090896606, 0.8448419570922852]], "avg_entropy": [1.1083451576996595], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ann Arbor, Michigan", "score_of_biggest_cluster": 0.1757410317659378, "score_of_first_answer": 0.1757410317659378, "max_score_of_first_answer": 0.9988892674446106, "min_score_of_first_answer": 0.0018157324520871043, "avg_score": 0.1757410317659378, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36073833987759885, "std_of_first_answer": 0.36073833987759885, "avg_entropy_of_first_answer": 1.1083451576996595, "lex_sim": 1.0}
{"question": "when does middle-earth shadow of mordor come out", "answer": ["September 30 2014"], "scores": [[0.29441073536872864, 0.9895188212394714, 0.9290223121643066, 0.9978359341621399, 0.013143996708095074, 0.993678629398346, 0.998969554901123, 0.9978944659233093, 0.9804238080978394]], "normalized_score": [0.5325832366943359], "tokens": [["\u2581September", "\u2581", "3", "0", "\u2581", "2", "0", "1", "4"]], "entropy": [[1.9628301858901978, 0.07451817393302917, 0.35723838210105896, 0.018390018492937088, 0.63032466173172, 0.05165679007768631, 0.011360079981386662, 0.01951785944402218, 0.13414981961250305]], "avg_entropy": [0.3622206634738379], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "September 2014", "score_of_biggest_cluster": 0.5325832366943359, "score_of_first_answer": 0.5325832366943359, "max_score_of_first_answer": 0.998969554901123, "min_score_of_first_answer": 0.013143996708095074, "avg_score": 0.5325832366943359, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3520224811907227, "std_of_first_answer": 0.3520224811907227, "avg_entropy_of_first_answer": 0.3622206634738379, "lex_sim": 1.0}
{"question": "when did men's curling start in the olympics", "answer": ["1998"], "scores": [[0.7219623327255249, 0.9548734426498413, 0.9973450303077698, 0.37860873341560364, 0.9842081665992737]], "normalized_score": [0.7615820169448853], "tokens": [["\u2581", "1", "9", "9", "8"]], "entropy": [[1.5540481805801392, 0.2043541669845581, 0.024480072781443596, 0.7680220007896423, 0.09564433246850967]], "avg_entropy": [0.5293097507208586], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1924", "score_of_biggest_cluster": 0.7615820169448853, "score_of_first_answer": 0.7615820169448853, "max_score_of_first_answer": 0.9973450303077698, "min_score_of_first_answer": 0.37860873341560364, "avg_score": 0.7615820169448853, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23674889815545674, "std_of_first_answer": 0.23674889815545674, "avg_entropy_of_first_answer": 0.5293097507208586, "lex_sim": 1.0}
{"question": "what type of rock dominates the earth's surface", "answer": ["igneous rock"], "scores": [[0.36871138215065, 0.9990830421447754, 0.9987199306488037, 0.3200331926345825]], "normalized_score": [0.585776150226593], "tokens": [["\u2581", "igne", "ous", "\u2581rock"]], "entropy": [[2.617973566055298, 0.01100067887455225, 0.013030888512730598, 1.0284507274627686]], "avg_entropy": [0.9176139652263373], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sedimentary", "score_of_biggest_cluster": 0.585776150226593, "score_of_first_answer": 0.585776150226593, "max_score_of_first_answer": 0.9990830421447754, "min_score_of_first_answer": 0.3200331926345825, "avg_score": 0.585776150226593, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32771684475370844, "std_of_first_answer": 0.32771684475370844, "avg_entropy_of_first_answer": 0.9176139652263373, "lex_sim": 1.0}
{"question": "where are they playing the superbowl at this year", "answer": ["Cowboys Stadium"], "scores": [[0.008164853788912296, 0.9803254008293152, 0.9997798800468445, 0.9332273602485657]], "normalized_score": [0.29396969079971313], "tokens": [["\u2581Cow", "bo", "ys", "\u2581Stadium"]], "entropy": [[3.920330047607422, 0.11160509288311005, 0.0028793453238904476, 0.46651896834373474]], "avg_entropy": [1.1253333635395393], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Minneapolis, Minnesota", "score_of_biggest_cluster": 0.29396969079971313, "score_of_first_answer": 0.29396969079971313, "max_score_of_first_answer": 0.9997798800468445, "min_score_of_first_answer": 0.008164853788912296, "avg_score": 0.29396969079971313, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4176693571519449, "std_of_first_answer": 0.4176693571519449, "avg_entropy_of_first_answer": 1.1253333635395393, "lex_sim": 1.0}
{"question": "who did the voiceover in michael jackson's thriller", "answer": ["Vincent Price"], "scores": [[0.8823882937431335, 0.9316583275794983]], "normalized_score": [0.9066886901855469], "tokens": [["\u2581Vincent", "\u2581Price"]], "entropy": [[0.8987212181091309, 0.4089207649230957]], "avg_entropy": [0.6538209915161133], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Vincent Price", "score_of_biggest_cluster": 0.9066886901855469, "score_of_first_answer": 0.9066886901855469, "max_score_of_first_answer": 0.9316583275794983, "min_score_of_first_answer": 0.8823882937431335, "avg_score": 0.9066886901855469, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.024635016918182373, "std_of_first_answer": 0.024635016918182373, "avg_entropy_of_first_answer": 0.6538209915161133, "lex_sim": 1.0}
{"question": "who played sandy's jock boyfriend in grease", "answer": ["Jeff Conaway"], "scores": [[0.5072293281555176, 0.9965379238128662, 0.9566607475280762]], "normalized_score": [0.7849079370498657], "tokens": [["\u2581Jeff", "\u2581Con", "away"]], "entropy": [[2.490640163421631, 0.037321481853723526, 0.19713877141475677]], "avg_entropy": [0.9083668055633703], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Travolta", "score_of_biggest_cluster": 0.7849079370498657, "score_of_first_answer": 0.7849079370498657, "max_score_of_first_answer": 0.9965379238128662, "min_score_of_first_answer": 0.5072293281555176, "avg_score": 0.7849079370498657, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22186124084207015, "std_of_first_answer": 0.22186124084207015, "avg_entropy_of_first_answer": 0.9083668055633703, "lex_sim": 1.0}
{"question": "when was catch me if you can made", "answer": ["December 25, 2002"], "scores": [[0.5928575992584229, 0.9977703094482422, 0.9487393498420715, 0.9129660725593567, 0.9884032607078552, 0.999026894569397, 0.9989513158798218, 0.9999743700027466, 0.9982736110687256, 0.9975220561027527]], "normalized_score": [0.9336410164833069], "tokens": [["\u2581December", "\u2581", "2", "5", ",", "\u2581", "2", "0", "0", "2"]], "entropy": [[1.431309461593628, 0.019391145557165146, 0.2626310884952545, 0.45220527052879333, 0.08019447326660156, 0.009297746233642101, 0.008534436114132404, 0.0003747201117221266, 0.013541636057198048, 0.022353261709213257]], "avg_entropy": [0.22998332396673504], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "2002", "score_of_biggest_cluster": 0.9336410164833069, "score_of_first_answer": 0.9336410164833069, "max_score_of_first_answer": 0.9999743700027466, "min_score_of_first_answer": 0.5928575992584229, "avg_score": 0.9336410164833069, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.12005722694444884, "std_of_first_answer": 0.12005722694444884, "avg_entropy_of_first_answer": 0.22998332396673504, "lex_sim": 1.0}
{"question": "who was the chief guest of 2014 republic day", "answer": ["Shinzo Abe"], "scores": [[0.3407018780708313, 0.8721262216567993, 0.9904830455780029, 0.9981901049613953]], "normalized_score": [0.7362130880355835], "tokens": [["\u2581Shin", "zo", "\u2581A", "be"]], "entropy": [[2.7031354904174805, 0.4189843535423279, 0.08682753145694733, 0.01718142256140709]], "avg_entropy": [0.8065321994945407], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Prime Minister Shinzo Abe", "score_of_biggest_cluster": 0.7362130880355835, "score_of_first_answer": 0.7362130880355835, "max_score_of_first_answer": 0.9981901049613953, "min_score_of_first_answer": 0.3407018780708313, "avg_score": 0.7362130880355835, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2700553191372726, "std_of_first_answer": 0.2700553191372726, "avg_entropy_of_first_answer": 0.8065321994945407, "lex_sim": 1.0}
{"question": "where did iowa state football coach come from", "answer": ["Northern Illinois"], "scores": [[0.05881817266345024, 0.21089281141757965]], "normalized_score": [0.11137473583221436], "tokens": [["\u2581Northern", "\u2581Illinois"]], "entropy": [[5.148270130157471, 1.2420299053192139]], "avg_entropy": [3.1951500177383423], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Massillon, Ohio", "score_of_biggest_cluster": 0.11137473583221436, "score_of_first_answer": 0.11137473583221436, "max_score_of_first_answer": 0.21089281141757965, "min_score_of_first_answer": 0.05881817266345024, "avg_score": 0.11137473583221436, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0760373193770647, "std_of_first_answer": 0.0760373193770647, "avg_entropy_of_first_answer": 3.1951500177383423, "lex_sim": 1.0}
{"question": "under the federal unemployment tax act which party pays unemployment taxes", "answer": ["Both the employer and the employee do."], "scores": [[0.11434634029865265, 0.41461533308029175, 0.5993049144744873, 0.9603626132011414, 0.972279965877533, 0.6919991374015808, 0.934730052947998, 0.0014036049833521247, 0.6152405738830566]], "normalized_score": [0.2906865179538727], "tokens": [["\u2581Both", "\u2581the", "\u2581employ", "er", "\u2581and", "\u2581the", "\u2581employee", "\u2581do", "."]], "entropy": [[3.0019216537475586, 2.0396933555603027, 1.5086286067962646, 0.19969719648361206, 0.20644515752792358, 0.7692328691482544, 0.40876778960227966, 1.6100807189941406, 1.3538165092468262]], "avg_entropy": [1.2331426507896848], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Employers", "score_of_biggest_cluster": 0.2906865179538727, "score_of_first_answer": 0.2906865179538727, "max_score_of_first_answer": 0.972279965877533, "min_score_of_first_answer": 0.0014036049833521247, "avg_score": 0.2906865179538727, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3361086147954879, "std_of_first_answer": 0.3361086147954879, "avg_entropy_of_first_answer": 1.2331426507896848, "lex_sim": 1.0}
{"question": "latest series of keeping up with the kardashians", "answer": ["Season 17 (2020)"], "scores": [[0.5432782769203186, 0.9869335889816284, 0.9433607459068298, 0.057204294949769974, 0.04263484477996826, 0.7566146850585938, 0.9980134963989258, 0.17743320763111115, 0.955346941947937, 0.88533616065979]], "normalized_score": [0.41167041659355164], "tokens": [["\u2581Season", "\u2581", "1", "7", "\u2581(", "2", "0", "2", "0", ")"]], "entropy": [[1.8563241958618164, 0.12397747486829758, 0.3280395567417145, 1.786224365234375, 0.8074365854263306, 1.5709528923034668, 0.020327217876911163, 0.48730871081352234, 0.26903626322746277, 0.6245200037956238]], "avg_entropy": [0.787414726614952], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "14", "score_of_biggest_cluster": 0.41167041659355164, "score_of_first_answer": 0.41167041659355164, "max_score_of_first_answer": 0.9980134963989258, "min_score_of_first_answer": 0.04263484477996826, "avg_score": 0.41167041659355164, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3785929222554031, "std_of_first_answer": 0.3785929222554031, "avg_entropy_of_first_answer": 0.787414726614952, "lex_sim": 1.0}
{"question": "when does the boy in the striped pajamas take place", "answer": ["1943"], "scores": [[0.4683186411857605, 0.9825802445411682, 0.9978572726249695, 0.888174295425415, 0.0674315094947815]], "normalized_score": [0.48738011717796326], "tokens": [["\u2581", "1", "9", "4", "3"]], "entropy": [[2.5305047035217285, 0.10919055342674255, 0.021838746964931488, 0.3960539400577545, 1.1544792652130127]], "avg_entropy": [0.842413441836834], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "World War II", "score_of_biggest_cluster": 0.48738011717796326, "score_of_first_answer": 0.48738011717796326, "max_score_of_first_answer": 0.9978572726249695, "min_score_of_first_answer": 0.0674315094947815, "avg_score": 0.48738011717796326, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36220710520574073, "std_of_first_answer": 0.36220710520574073, "avg_entropy_of_first_answer": 0.842413441836834, "lex_sim": 1.0}
{"question": "when was the cleveland browns last winning game", "answer": ["September 21, 2001"], "scores": [[0.07985864579677582, 0.9977419376373291, 0.6090044379234314, 0.09304814040660858, 0.9723603129386902, 0.9959567189216614, 0.9803218245506287, 0.9997277855873108, 0.21193012595176697, 0.013067936524748802]], "normalized_score": [0.32169076800346375], "tokens": [["\u2581September", "\u2581", "2", "1", ",", "\u2581", "2", "0", "0", "1"]], "entropy": [[2.051687717437744, 0.01991584151983261, 1.146097183227539, 2.1534645557403564, 0.17387232184410095, 0.031726595014333725, 0.09864673018455505, 0.0032630497589707375, 0.5232294797897339, 1.2142200469970703]], "avg_entropy": [0.7416123521514237], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2016", "score_of_biggest_cluster": 0.32169076800346375, "score_of_first_answer": 0.32169076800346375, "max_score_of_first_answer": 0.9997277855873108, "min_score_of_first_answer": 0.013067936524748802, "avg_score": 0.32169076800346375, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4219661841968252, "std_of_first_answer": 0.4219661841968252, "avg_entropy_of_first_answer": 0.7416123521514237, "lex_sim": 1.0}
{"question": "who came up with the term baby boomer", "answer": ["William Strauss and Neil Howe"], "scores": [[0.39514583349227905, 0.9538940191268921, 0.9984682202339172, 0.5158126354217529, 0.9780535697937012, 0.9955449104309082, 0.9970953464508057]], "normalized_score": [0.7878860831260681], "tokens": [["\u2581William", "\u2581Stra", "uss", "\u2581and", "\u2581Neil", "\u2581How", "e"]], "entropy": [[3.910288095474243, 0.4309229254722595, 0.013938638381659985, 1.2159262895584106, 0.19309884309768677, 0.04525517672300339, 0.027692876756191254]], "avg_entropy": [0.833874692209065], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Washington Post", "score_of_biggest_cluster": 0.7878860831260681, "score_of_first_answer": 0.7878860831260681, "max_score_of_first_answer": 0.9984682202339172, "min_score_of_first_answer": 0.39514583349227905, "avg_score": 0.7878860831260681, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24163330000081917, "std_of_first_answer": 0.24163330000081917, "avg_entropy_of_first_answer": 0.833874692209065, "lex_sim": 1.0}
{"question": "who founded mission nuestra senora de la soledad", "answer": ["Esteban"], "scores": [[0.03019212745130062, 0.7433716654777527, 0.9640193581581116]], "normalized_score": [0.27865180373191833], "tokens": [["\u2581Est", "eb", "an"]], "entropy": [[3.6891939640045166, 1.1353040933609009, 0.18176142871379852]], "avg_entropy": [1.6687531620264053], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the Franciscan order", "score_of_biggest_cluster": 0.27865180373191833, "score_of_first_answer": 0.27865180373191833, "max_score_of_first_answer": 0.9640193581581116, "min_score_of_first_answer": 0.03019212745130062, "avg_score": 0.27865180373191833, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3985172176057894, "std_of_first_answer": 0.3985172176057894, "avg_entropy_of_first_answer": 1.6687531620264053, "lex_sim": 1.0}
{"question": "where was one direction what makes you beautiful filmed", "answer": ["Hanger Lane Sports & Leisure, London (where the gym footage in the video was shot)"], "scores": [[0.03156239911913872, 0.03279968723654747, 0.9150230884552002, 0.0027626375667750835, 0.007156123872846365, 0.3569675385951996, 0.9987313151359558, 0.9999368190765381, 0.011858029291033745, 0.13842655718326569, 0.004181923344731331, 0.0038878656923770905, 0.2874865233898163, 0.01733330450952053, 0.8874907493591309, 0.004131907131522894, 0.995771586894989, 0.007976965047419071, 0.7925329208374023, 0.6803309917449951, 0.778544008731842, 0.16072583198547363, 0.7813003063201904]], "normalized_score": [0.09170074760913849], "tokens": [["\u2581H", "anger", "\u2581Lane", "\u2581Sports", "\u2581&", "\u2581Le", "is", "ure", ",", "\u2581London", "\u2581(", "where", "\u2581the", "\u2581g", "ym", "\u2581foot", "age", "\u2581in", "\u2581the", "\u2581video", "\u2581was", "\u2581shot", ")"]], "entropy": [[5.551505088806152, 2.272637367248535, 0.5786800384521484, 2.515172243118286, 0.6705813407897949, 1.891232967376709, 0.012392232194542885, 0.0008550286293029785, 1.337341070175171, 4.009132385253906, 1.241258978843689, 4.810510635375977, 3.7205758094787598, 4.882726192474365, 0.5377233624458313, 1.8317804336547852, 0.04238174483180046, 0.9613149166107178, 1.3258063793182373, 1.5875598192214966, 1.0118688344955444, 0.8088223338127136, 1.0384410619735718]], "avg_entropy": [1.8539260984600887], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Malibu, California", "score_of_biggest_cluster": 0.09170074760913849, "score_of_first_answer": 0.09170074760913849, "max_score_of_first_answer": 0.9999368190765381, "min_score_of_first_answer": 0.0027626375667750835, "avg_score": 0.09170074760913849, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40311562683805724, "std_of_first_answer": 0.40311562683805724, "avg_entropy_of_first_answer": 1.8539260984600887, "lex_sim": 1.0}
{"question": "what year did seven nation army come out", "answer": ["2003"], "scores": [[0.8067235350608826, 0.9848697185516357, 0.9987891316413879, 0.9947149157524109, 0.7834137678146362]], "normalized_score": [0.9083504676818848], "tokens": [["\u2581", "2", "0", "0", "3"]], "entropy": [[1.1761219501495361, 0.10106230527162552, 0.013116604648530483, 0.038817550987005234, 0.7266441583633423]], "avg_entropy": [0.4111525138840079], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "2003", "score_of_biggest_cluster": 0.9083504676818848, "score_of_first_answer": 0.9083504676818848, "max_score_of_first_answer": 0.9987891316413879, "min_score_of_first_answer": 0.7834137678146362, "avg_score": 0.9083504676818848, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.09724934144764139, "std_of_first_answer": 0.09724934144764139, "avg_entropy_of_first_answer": 0.4111525138840079, "lex_sim": 1.0}
{"question": "who has the most gold gloves at shortstop", "answer": ["Barry Larkin"], "scores": [[0.021099286153912544, 0.9371928572654724, 0.9348925948143005, 0.9988160133361816]], "normalized_score": [0.3686259984970093], "tokens": [["\u2581Barry", "\u2581L", "ark", "in"]], "entropy": [[1.3824260234832764, 0.32582589983940125, 0.25488901138305664, 0.013124151155352592]], "avg_entropy": [0.4940662714652717], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ozzie Smith", "score_of_biggest_cluster": 0.3686259984970093, "score_of_first_answer": 0.3686259984970093, "max_score_of_first_answer": 0.9988160133361816, "min_score_of_first_answer": 0.021099286153912544, "avg_score": 0.3686259984970093, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4060529944326402, "std_of_first_answer": 0.4060529944326402, "avg_entropy_of_first_answer": 0.4940662714652717, "lex_sim": 1.0}
{"question": "which is the first private medical college in kerala", "answer": ["Pariyaram Medical College in Kannur"], "scores": [[0.22104398906230927, 0.3358016908168793, 0.90321284532547, 0.9615103006362915, 0.7801461219787598, 0.9763358235359192, 0.01040708553045988, 0.7580695152282715, 0.8814848065376282, 0.9977437257766724]], "normalized_score": [0.4500235915184021], "tokens": [["\u2581P", "ari", "yar", "am", "\u2581Medical", "\u2581College", "\u2581in", "\u2581K", "ann", "ur"]], "entropy": [[3.7354860305786133, 2.722853660583496, 0.7132652997970581, 0.2934332489967346, 1.0111074447631836, 0.12426303327083588, 1.1123864650726318, 1.3398544788360596, 0.7588419318199158, 0.024956800043582916]], "avg_entropy": [1.1836448393762111], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Government Medical College, Thiruvananthapuram", "score_of_biggest_cluster": 0.4500235915184021, "score_of_first_answer": 0.4500235915184021, "max_score_of_first_answer": 0.9977437257766724, "min_score_of_first_answer": 0.01040708553045988, "avg_score": 0.4500235915184021, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3394067871966504, "std_of_first_answer": 0.3394067871966504, "avg_entropy_of_first_answer": 1.1836448393762111, "lex_sim": 1.0}
{"question": "how tall is the tallest building in las vegas", "answer": ["581 feet or 176.7 metres"], "scores": [[0.7196349501609802, 0.07971535623073578, 0.04363114759325981, 0.3535999655723572, 0.4398970603942871, 0.008731409907341003, 0.9805904626846313, 0.9567397236824036, 0.9635958671569824, 0.06973666697740555, 0.8470450043678284, 0.1625920683145523, 0.06230593100190163]], "normalized_score": [0.21283544600009918], "tokens": [["\u2581", "5", "8", "1", "\u2581feet", "\u2581or", "\u2581", "1", "7", "6", ".", "7", "\u2581metres"]], "entropy": [[1.3960649967193604, 1.6342332363128662, 2.0858983993530273, 2.3454387187957764, 1.5556126832962036, 1.0134456157684326, 0.171248659491539, 0.2673834264278412, 0.24375571310520172, 0.3387320637702942, 0.6292964816093445, 1.147843360900879, 2.438312530517578]], "avg_entropy": [1.1744050681591034], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Drew Las Vegas", "score_of_biggest_cluster": 0.21283544600009918, "score_of_first_answer": 0.21283544600009918, "max_score_of_first_answer": 0.9805904626846313, "min_score_of_first_answer": 0.008731409907341003, "avg_score": 0.21283544600009918, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3837352908692495, "std_of_first_answer": 0.3837352908692495, "avg_entropy_of_first_answer": 1.1744050681591034, "lex_sim": 1.0}
{"question": "nuclear power plant that blew up in russia", "answer": ["Chernobyl"], "scores": [[0.9480099081993103, 0.9950017333030701, 0.9983376264572144, 0.999372661113739]], "normalized_score": [0.9849414229393005], "tokens": [["\u2581Ch", "ern", "ob", "yl"]], "entropy": [[0.4482526481151581, 0.03770335391163826, 0.016575424000620842, 0.0074209836311638355]], "avg_entropy": [0.12748810241464525], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Chernobyl Nuclear Power Plant", "score_of_biggest_cluster": 0.9849414229393005, "score_of_first_answer": 0.9849414229393005, "max_score_of_first_answer": 0.999372661113739, "min_score_of_first_answer": 0.9480099081993103, "avg_score": 0.9849414229393005, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.021521134566368887, "std_of_first_answer": 0.021521134566368887, "avg_entropy_of_first_answer": 0.12748810241464525, "lex_sim": 1.0}
{"question": "who played tom on as the world turns", "answer": ["Tom Pelphrey"], "scores": [[0.07906555384397507, 0.15206246078014374, 0.9957678318023682, 0.9956886172294617]], "normalized_score": [0.3304249048233032], "tokens": [["\u2581Tom", "\u2581Pel", "ph", "rey"]], "entropy": [[4.148792743682861, 3.3215768337249756, 0.046333253383636475, 0.03267613425850868]], "avg_entropy": [1.8873447412624955], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jason Kincaid (1984)", "score_of_biggest_cluster": 0.3304249048233032, "score_of_first_answer": 0.3304249048233032, "max_score_of_first_answer": 0.9957678318023682, "min_score_of_first_answer": 0.07906555384397507, "avg_score": 0.3304249048233032, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4408382148233844, "std_of_first_answer": 0.4408382148233844, "avg_entropy_of_first_answer": 1.8873447412624955, "lex_sim": 1.0}
{"question": "rizal finished all the chapters of the novel noli me tangere in", "answer": ["Hong Kong"], "scores": [[0.03164757415652275, 0.925815224647522]], "normalized_score": [0.1711718738079071], "tokens": [["\u2581Hong", "\u2581Kong"]], "entropy": [[3.484066963195801, 0.3254929780960083]], "avg_entropy": [1.9047799706459045], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "December 1886", "score_of_biggest_cluster": 0.1711718738079071, "score_of_first_answer": 0.1711718738079071, "max_score_of_first_answer": 0.925815224647522, "min_score_of_first_answer": 0.03164757415652275, "avg_score": 0.1711718738079071, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4470838252454996, "std_of_first_answer": 0.4470838252454996, "avg_entropy_of_first_answer": 1.9047799706459045, "lex_sim": 1.0}
{"question": "when did the dust bowl end in oklahoma", "answer": ["It ended in 1939, as persistent and regular rains began to return, a signal that the drought was over."], "scores": [[0.01751117967069149, 0.06188128516077995, 0.6623398065567017, 0.7200430035591125, 0.9932314157485962, 0.9980754852294922, 0.4603113830089569, 0.7770670056343079, 0.17214134335517883, 0.03213658928871155, 0.0016855655703693628, 0.017263013869524002, 0.043747428804636, 0.84689861536026, 0.22066734731197357, 0.07826437801122665, 0.6676950454711914, 0.1840657740831375, 0.04228893294930458, 0.010418754070997238, 0.00246837898157537, 0.4209793508052826, 0.8544853329658508, 0.5633288025856018, 0.9994912147521973, 0.6837575435638428, 0.14004167914390564, 0.7336443662643433]], "normalized_score": [0.157904252409935], "tokens": [["\u2581It", "\u2581ended", "\u2581in", "\u2581", "1", "9", "3", "9", ",", "\u2581as", "\u2581persistent", "\u2581and", "\u2581regular", "\u2581ra", "ins", "\u2581began", "\u2581to", "\u2581return", ",", "\u2581a", "\u2581signal", "\u2581that", "\u2581the", "\u2581dr", "ought", "\u2581was", "\u2581over", "."]], "entropy": [[2.8764474391937256, 3.0165867805480957, 1.819638729095459, 1.0135151147842407, 0.04643391817808151, 0.01698066107928753, 0.9527021050453186, 0.9037436246871948, 2.122620105743408, 3.2379298210144043, 2.9623985290527344, 1.2498738765716553, 3.255612850189209, 0.799126386642456, 0.5944418907165527, 2.4080817699432373, 1.631652593612671, 2.834807872772217, 1.7835476398468018, 4.095876693725586, 3.2955455780029297, 1.5914909839630127, 1.0703918933868408, 2.4031052589416504, 0.0056649474427104, 1.6736406087875366, 2.23103666305542, 1.0957404375076294]], "avg_entropy": [1.8210226704832166], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1940", "score_of_biggest_cluster": 0.157904252409935, "score_of_first_answer": 0.157904252409935, "max_score_of_first_answer": 0.9994912147521973, "min_score_of_first_answer": 0.0016855655703693628, "avg_score": 0.157904252409935, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3610012813175222, "std_of_first_answer": 0.3610012813175222, "avg_entropy_of_first_answer": 1.8210226704832166, "lex_sim": 1.0}
{"question": "who are the judges on the fisa court", "answer": ["Colleen Kollar-Kotelly"], "scores": [[0.10708662122488022, 0.945081353187561, 0.9996422529220581, 0.9626713991165161, 0.9974455833435059, 0.9577771425247192, 0.9935923218727112, 0.9693003296852112, 0.9911822080612183]], "normalized_score": [0.764129102230072], "tokens": [["\u2581Col", "le", "en", "\u2581K", "ollar", "-", "K", "ot", "elly"]], "entropy": [[4.797534465789795, 0.35333994030952454, 0.004834105260670185, 0.2870791256427765, 0.02662423811852932, 0.3457694351673126, 0.046589817851781845, 0.24882511794567108, 0.0889468714594841]], "avg_entropy": [0.6888381241717272], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Thomas Russell", "score_of_biggest_cluster": 0.764129102230072, "score_of_first_answer": 0.764129102230072, "max_score_of_first_answer": 0.9996422529220581, "min_score_of_first_answer": 0.10708662122488022, "avg_score": 0.764129102230072, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2740356652388806, "std_of_first_answer": 0.2740356652388806, "avg_entropy_of_first_answer": 0.6888381241717272, "lex_sim": 1.0}
{"question": "how did leo dalton die in silent witness", "answer": ["He shot himself"], "scores": [[0.15150201320648193, 0.028143616393208504, 0.9382525086402893]], "normalized_score": [0.15874719619750977], "tokens": [["\u2581He", "\u2581shot", "\u2581himself"]], "entropy": [[4.842182159423828, 2.2036876678466797, 0.5014760494232178]], "avg_entropy": [2.5157819588979087], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "in an explosion", "score_of_biggest_cluster": 0.15874719619750977, "score_of_first_answer": 0.15874719619750977, "max_score_of_first_answer": 0.9382525086402893, "min_score_of_first_answer": 0.028143616393208504, "avg_score": 0.15874719619750977, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40311176171226465, "std_of_first_answer": 0.40311176171226465, "avg_entropy_of_first_answer": 2.5157819588979087, "lex_sim": 1.0}
{"question": "what nba team did derrick rose play for", "answer": ["Chicago Bulls"], "scores": [[0.9390472769737244, 0.9760175347328186, 0.9983649849891663]], "normalized_score": [0.9708335995674133], "tokens": [["\u2581Chicago", "\u2581Bull", "s"]], "entropy": [[0.4750451147556305, 0.1385027915239334, 0.01398545689880848]], "avg_entropy": [0.20917778772612414], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Minnesota Timberwolves", "score_of_biggest_cluster": 0.9708335995674133, "score_of_first_answer": 0.9708335995674133, "max_score_of_first_answer": 0.9983649849891663, "min_score_of_first_answer": 0.9390472769737244, "avg_score": 0.9708335995674133, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.024460396507958695, "std_of_first_answer": 0.024460396507958695, "avg_entropy_of_first_answer": 0.20917778772612414, "lex_sim": 1.0}
{"question": "who was president when the panama canal was built", "answer": ["Woodrow Wilson"], "scores": [[0.22542276978492737, 0.9975000023841858, 0.9940967559814453]], "normalized_score": [0.6068943738937378], "tokens": [["\u2581Wood", "row", "\u2581Wilson"]], "entropy": [[1.764073371887207, 0.026726864278316498, 0.05494731664657593]], "avg_entropy": [0.6152491842706999], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Theodore Roosevelt", "score_of_biggest_cluster": 0.6068943738937378, "score_of_first_answer": 0.6068943738937378, "max_score_of_first_answer": 0.9975000023841858, "min_score_of_first_answer": 0.22542276978492737, "avg_score": 0.6068943738937378, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3631612026878929, "std_of_first_answer": 0.3631612026878929, "avg_entropy_of_first_answer": 0.6152491842706999, "lex_sim": 1.0}
{"question": "where does new york drinking water come from", "answer": ["There is no data for this."], "scores": [[0.0017672397661954165, 0.13116760551929474, 0.5336833000183105, 0.010152692906558514, 0.28937411308288574, 0.6405118703842163, 0.07315721362829208]], "normalized_score": [0.07765614986419678], "tokens": [["\u2581There", "\u2581is", "\u2581no", "\u2581data", "\u2581for", "\u2581this", "."]], "entropy": [[3.6249141693115234, 0.7108052968978882, 2.046696424484253, 4.554461479187012, 2.1338768005371094, 1.6855565309524536, 2.90962290763855]], "avg_entropy": [2.52370480128697], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "the eastern Catskill Mountains", "score_of_biggest_cluster": 0.07765614986419678, "score_of_first_answer": 0.07765614986419678, "max_score_of_first_answer": 0.6405118703842163, "min_score_of_first_answer": 0.0017672397661954165, "avg_score": 0.07765614986419678, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23855609559239752, "std_of_first_answer": 0.23855609559239752, "avg_entropy_of_first_answer": 2.52370480128697, "lex_sim": 1.0}
{"question": "when did the eagles win last super bowl", "answer": ["2004"], "scores": [[0.1543016880750656, 0.6311423778533936, 0.9393497109413147, 0.11685235798358917, 0.2817382514476776]], "normalized_score": [0.31315672397613525], "tokens": [["\u2581", "2", "0", "0", "4"]], "entropy": [[2.2473950386047363, 1.1760718822479248, 0.3909110426902771, 0.41173240542411804, 1.7299972772598267]], "avg_entropy": [1.1912215292453765], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2017", "score_of_biggest_cluster": 0.31315672397613525, "score_of_first_answer": 0.31315672397613525, "max_score_of_first_answer": 0.9393497109413147, "min_score_of_first_answer": 0.11685235798358917, "avg_score": 0.31315672397613525, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3148987954910252, "std_of_first_answer": 0.3148987954910252, "avg_entropy_of_first_answer": 1.1912215292453765, "lex_sim": 1.0}
{"question": "what is the setting of a dolls house", "answer": ["late nineteenth-century suburbia"], "scores": [[0.011807980015873909, 0.22748704254627228, 0.9995096921920776, 0.9958263039588928, 0.10847952216863632, 0.9891409873962402, 0.0028875195421278477, 0.42136624455451965, 0.07542094588279724]], "normalized_score": [0.14382044970989227], "tokens": [["\u2581late", "\u2581nin", "ete", "enth", "-", "century", "\u2581sub", "urb", "ia"]], "entropy": [[3.9210829734802246, 1.0381247997283936, 0.005430692341178656, 0.03339102864265442, 0.436149001121521, 0.08723485469818115, 1.5249218940734863, 0.8097537755966187, 1.446890115737915]], "avg_entropy": [1.0336643483800192], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "a Norwegian town circa 1879", "score_of_biggest_cluster": 0.14382044970989227, "score_of_first_answer": 0.14382044970989227, "max_score_of_first_answer": 0.9995096921920776, "min_score_of_first_answer": 0.0028875195421278477, "avg_score": 0.14382044970989227, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41958652545725306, "std_of_first_answer": 0.41958652545725306, "avg_entropy_of_first_answer": 1.0336643483800192, "lex_sim": 1.0}
{"question": "when did muhammad ali win an olympic gold medal", "answer": ["1960's"], "scores": [[0.6582556962966919, 0.9885118007659912, 0.9926779270172119, 0.9919793605804443, 0.992994487285614, 0.0013928537955507636, 0.9818503260612488]], "normalized_score": [0.3654246926307678], "tokens": [["\u2581", "1", "9", "6", "0", "'", "s"]], "entropy": [[1.4845404624938965, 0.08169165253639221, 0.05964001640677452, 0.0643363669514656, 0.0522942878305912, 1.6540664434432983, 0.1258660852909088]], "avg_entropy": [0.5032050449933324], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "1960", "score_of_biggest_cluster": 0.3654246926307678, "score_of_first_answer": 0.3654246926307678, "max_score_of_first_answer": 0.992994487285614, "min_score_of_first_answer": 0.0013928537955507636, "avg_score": 0.3654246926307678, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3459336031861682, "std_of_first_answer": 0.3459336031861682, "avg_entropy_of_first_answer": 0.5032050449933324, "lex_sim": 1.0}
{"question": "when was the last time minnesota vikings was in the super bowl", "answer": ["January 4, 1977"], "scores": [[0.2815476059913635, 0.9956616759300232, 0.12966777384281158, 0.9931621551513672, 0.9986953139305115, 0.9323766827583313, 0.9999619722366333, 0.9145190715789795, 0.6904352903366089]], "normalized_score": [0.6517683267593384], "tokens": [["\u2581January", "\u2581", "4", ",", "\u2581", "1", "9", "7", "7"]], "entropy": [[2.0787415504455566, 0.03370189294219017, 1.6483452320098877, 0.053498756140470505, 0.01208842359483242, 0.24823127686977386, 0.0005203773616813123, 0.3555077016353607, 0.9975878596305847]], "avg_entropy": [0.6031358967367042], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1969", "score_of_biggest_cluster": 0.6517683267593384, "score_of_first_answer": 0.6517683267593384, "max_score_of_first_answer": 0.9999619722366333, "min_score_of_first_answer": 0.12966777384281158, "avg_score": 0.6517683267593384, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3176472801365132, "std_of_first_answer": 0.3176472801365132, "avg_entropy_of_first_answer": 0.6031358967367042, "lex_sim": 1.0}
{"question": "who plays max voice in a goofy movie", "answer": ["Jason Marsden"], "scores": [[0.42914825677871704, 0.9686138033866882, 0.9982643723487854]], "normalized_score": [0.7458781003952026], "tokens": [["\u2581Jason", "\u2581Mars", "den"]], "entropy": [[2.822359561920166, 0.25319159030914307, 0.021221529692411423]], "avg_entropy": [1.0322575606405735], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Jason Marsden", "score_of_biggest_cluster": 0.7458781003952026, "score_of_first_answer": 0.7458781003952026, "max_score_of_first_answer": 0.9982643723487854, "min_score_of_first_answer": 0.42914825677871704, "avg_score": 0.7458781003952026, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2615754374338472, "std_of_first_answer": 0.2615754374338472, "avg_entropy_of_first_answer": 1.0322575606405735, "lex_sim": 1.0}
{"question": "who is the head of the department of homeland security 2017", "answer": ["Kirstjen Nielsen"], "scores": [[0.796471118927002, 0.9989626407623291, 0.998956561088562, 0.829684317111969, 0.994746744632721, 0.9912329316139221]], "normalized_score": [0.9307745695114136], "tokens": [["\u2581K", "irst", "jen", "\u2581N", "iel", "sen"]], "entropy": [[1.1045854091644287, 0.010916894301772118, 0.009127517230808735, 0.5175917744636536, 0.04214860498905182, 0.052547987550497055]], "avg_entropy": [0.289486364616702], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Kirstjen Nielsen", "score_of_biggest_cluster": 0.9307745695114136, "score_of_first_answer": 0.9307745695114136, "max_score_of_first_answer": 0.9989626407623291, "min_score_of_first_answer": 0.796471118927002, "avg_score": 0.9307745695114136, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08679009224704892, "std_of_first_answer": 0.08679009224704892, "avg_entropy_of_first_answer": 0.289486364616702, "lex_sim": 1.0}
{"question": "who won the first medal in olympics for india", "answer": ["Rajyavardhan Singh Rathore"], "scores": [[0.10831556469202042, 0.9875156879425049, 0.8256633877754211, 0.999064028263092, 0.9973872303962708, 0.9170568585395813, 0.9941773414611816, 0.9999855756759644, 0.9738118648529053]], "normalized_score": [0.753321647644043], "tokens": [["\u2581Raj", "y", "av", "ard", "han", "\u2581Singh", "\u2581R", "ath", "ore"]], "entropy": [[3.110464096069336, 0.09950260818004608, 1.0752947330474854, 0.009800042025744915, 0.022287480533123016, 0.3062497079372406, 0.04407094791531563, 0.00021643246873281896, 0.1350143998861313]], "avg_entropy": [0.5336556053403506], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Norman Pritchard", "score_of_biggest_cluster": 0.753321647644043, "score_of_first_answer": 0.753321647644043, "max_score_of_first_answer": 0.9999855756759644, "min_score_of_first_answer": 0.10831556469202042, "avg_score": 0.753321647644043, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2736685463523935, "std_of_first_answer": 0.2736685463523935, "avg_entropy_of_first_answer": 0.5336556053403506, "lex_sim": 1.0}
{"question": "when did one child policy end in china", "answer": ["it\u2019s still in place"], "scores": [[0.0013501204084604979, 0.05383867770433426, 0.9970470070838928, 0.5406832098960876, 0.5157800316810608, 0.23833408951759338]], "normalized_score": [0.12995591759681702], "tokens": [["\u2581it", "\u2019", "s", "\u2581still", "\u2581in", "\u2581place"]], "entropy": [[2.043757915496826, 2.7771449089050293, 0.028468571603298187, 2.4951295852661133, 2.0093343257904053, 1.1562707424163818]], "avg_entropy": [1.7516843415796757], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "January 1, 2016", "score_of_biggest_cluster": 0.12995591759681702, "score_of_first_answer": 0.12995591759681702, "max_score_of_first_answer": 0.9970470070838928, "min_score_of_first_answer": 0.0013501204084604979, "avg_score": 0.12995591759681702, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3401175244609459, "std_of_first_answer": 0.3401175244609459, "avg_entropy_of_first_answer": 1.7516843415796757, "lex_sim": 1.0}
{"question": "who is covering the winter olympics in canada", "answer": ["CTV Sportsnet"], "scores": [[0.6284680366516113, 0.36144891381263733, 0.06420944631099701, 0.3176664710044861]], "normalized_score": [0.260900616645813], "tokens": [["\u2581C", "TV", "\u2581Sports", "net"]], "entropy": [[2.6250576972961426, 0.76690274477005, 2.007366895675659, 1.6602768898010254]], "avg_entropy": [1.7649010568857193], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "TLN", "score_of_biggest_cluster": 0.260900616645813, "score_of_first_answer": 0.260900616645813, "max_score_of_first_answer": 0.6284680366516113, "min_score_of_first_answer": 0.06420944631099701, "avg_score": 0.260900616645813, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20012390671921237, "std_of_first_answer": 0.20012390671921237, "avg_entropy_of_first_answer": 1.7649010568857193, "lex_sim": 1.0}
{"question": "which battle ended britain's support for the south", "answer": ["Battle of Gettyburg"], "scores": [[0.4395373463630676, 0.9882694482803345, 0.0013389390660449862, 0.5982924103736877, 0.8563903570175171]], "normalized_score": [0.1971711814403534], "tokens": [["\u2581Battle", "\u2581of", "\u2581Get", "ty", "burg"]], "entropy": [[2.2797675132751465, 0.0809415802359581, 1.2905383110046387, 1.3292938470840454, 1.0075054168701172]], "avg_entropy": [1.1976093336939813], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Battle of Antietam", "score_of_biggest_cluster": 0.1971711814403534, "score_of_first_answer": 0.1971711814403534, "max_score_of_first_answer": 0.9882694482803345, "min_score_of_first_answer": 0.0013389390660449862, "avg_score": 0.1971711814403534, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34581389826358555, "std_of_first_answer": 0.34581389826358555, "avg_entropy_of_first_answer": 1.1976093336939813, "lex_sim": 1.0}
{"question": "who has the best batting average in test cricket", "answer": ["Brian Lara"], "scores": [[0.24431297183036804, 0.7541624307632446, 0.995016872882843]], "normalized_score": [0.568085789680481], "tokens": [["\u2581Brian", "\u2581L", "ara"]], "entropy": [[3.776744842529297, 0.6879168152809143, 0.053431808948516846]], "avg_entropy": [1.5060311555862427], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Australia's Sir Donald Bradman", "score_of_biggest_cluster": 0.568085789680481, "score_of_first_answer": 0.568085789680481, "max_score_of_first_answer": 0.995016872882843, "min_score_of_first_answer": 0.24431297183036804, "avg_score": 0.568085789680481, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.312963199845586, "std_of_first_answer": 0.312963199845586, "avg_entropy_of_first_answer": 1.5060311555862427, "lex_sim": 1.0}
{"question": "who founded amazon where is the headquarters of amazon", "answer": ["Amazon.com, Inc. was founded in July 1994 (24 years ago) by Jeff Bezos."], "scores": [[0.08423055708408356, 0.5470038652420044, 0.9899734258651733, 0.8021657466888428, 0.9687276482582092, 0.6400953531265259, 0.11735809594392776, 0.9475666880607605, 0.598743200302124, 0.11738713085651398, 0.9944137334823608, 0.9981719255447388, 0.9999581575393677, 0.9998741149902344, 0.9837702512741089, 0.002361926017329097, 0.6064649224281311, 0.5626063346862793, 0.9716639518737793, 0.9915869235992432, 0.7558170557022095, 0.6539353132247925, 0.9072473049163818, 0.9875231385231018, 0.9985734224319458, 0.15318800508975983, 0.08563261479139328, 0.9313862919807434, 0.3274519443511963, 0.7312021851539612, 0.9783225655555725, 0.8334061503410339, 0.8714451789855957, 0.2956966161727905]], "normalized_score": [0.49319732189178467], "tokens": [["\u2581Amazon", ".", "com", ",", "\u2581Inc", ".", "\u2581was", "\u2581founded", "\u2581in", "\u2581July", "\u2581", "1", "9", "9", "4", "\u2581(", "2", "4", "\u2581years", "\u2581ago", ")", "\u2581by", "\u2581Jeff", "\u2581Bez", "os", ".", "\u2581Its", "\u2581headquarters", "\u2581is", "\u2581in", "\u2581Seattle", ",", "\u2581Washington", "."]], "entropy": [[1.5318139791488647, 1.9242745637893677, 0.09390677511692047, 1.080465316772461, 0.24002748727798462, 1.1629616022109985, 2.54190731048584, 0.3697265684604645, 0.9864161610603333, 0.4822162687778473, 0.042509935796260834, 0.013746954500675201, 0.0005665098433382809, 0.0014722178457304835, 0.08530208468437195, 1.307183027267456, 2.525785446166992, 1.0405504703521729, 0.20482462644577026, 0.06845924258232117, 0.7213807702064514, 1.314434289932251, 0.5896332263946533, 0.07386193424463272, 0.013526728376746178, 1.7806994915008545, 2.6721949577331543, 0.44070547819137573, 0.8041818737983704, 0.9943997859954834, 0.1716253161430359, 0.6863692998886108, 0.5938729047775269, 1.1474554538726807]], "avg_entropy": [0.814954648813], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Seattle, Washington", "score_of_biggest_cluster": 0.49319732189178467, "score_of_first_answer": 0.49319732189178467, "max_score_of_first_answer": 0.9999581575393677, "min_score_of_first_answer": 0.002361926017329097, "avg_score": 0.49319732189178467, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33459690979100365, "std_of_first_answer": 0.33459690979100365, "avg_entropy_of_first_answer": 0.814954648813, "lex_sim": 1.0}
{"question": "who is recognized as the founder of islam", "answer": ["The Prophet Muhammad"], "scores": [[0.019179536029696465, 0.7492743134498596, 0.9998106360435486, 0.9992245435714722, 0.8905666470527649]], "normalized_score": [0.4181627035140991], "tokens": [["\u2581The", "\u2581Pro", "ph", "et", "\u2581Muhammad"]], "entropy": [[1.0957732200622559, 1.368116855621338, 0.0024563882034271955, 0.007426150608807802, 0.5821870565414429]], "avg_entropy": [0.6111919342074543], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "the Islamic prophet Muhammad", "score_of_biggest_cluster": 0.4181627035140991, "score_of_first_answer": 0.4181627035140991, "max_score_of_first_answer": 0.9998106360435486, "min_score_of_first_answer": 0.019179536029696465, "avg_score": 0.4181627035140991, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3678819198339439, "std_of_first_answer": 0.3678819198339439, "avg_entropy_of_first_answer": 0.6111919342074543, "lex_sim": 1.0}
{"question": "what word is used to describe knowledge about the universe and method of obtaining that knowledge", "answer": ["\"scientific theory\""], "scores": [[0.0030183817725628614, 0.32160842418670654, 0.17642872035503387, 0.8662408590316772, 0.030388018116354942, 0.8719781041145325]], "normalized_score": [0.12562797963619232], "tokens": [["\u2581\"", "s", "cient", "ific", "\u2581theory", "\""]], "entropy": [[2.898930549621582, 3.240990161895752, 0.5499063730239868, 0.5907605886459351, 1.8756661415100098, 0.7558275461196899]], "avg_entropy": [1.6520135601361592], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "epistemology", "score_of_biggest_cluster": 0.12562797963619232, "score_of_first_answer": 0.12562797963619232, "max_score_of_first_answer": 0.8719781041145325, "min_score_of_first_answer": 0.0030183817725628614, "avg_score": 0.12562797963619232, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3623187679551659, "std_of_first_answer": 0.3623187679551659, "avg_entropy_of_first_answer": 1.6520135601361592, "lex_sim": 1.0}
{"question": "where does hydrogen peroxide come from in the body", "answer": ["the body synthesizes it to regulate oxidative damage"], "scores": [[0.05996798723936081, 0.058274973183870316, 0.033901482820510864, 0.9743991494178772, 0.9276008605957031, 0.3522736132144928, 0.006218337919563055, 0.007951648905873299, 0.9814146161079407, 0.023156700655817986, 0.9979190230369568, 0.7580350041389465, 0.03566429018974304]], "normalized_score": [0.12061631679534912], "tokens": [["\u2581the", "\u2581body", "\u2581synth", "es", "izes", "\u2581it", "\u2581to", "\u2581reg", "ulate", "\u2581ox", "id", "ative", "\u2581damage"]], "entropy": [[5.3014068603515625, 4.790494918823242, 2.700864791870117, 0.1596001386642456, 0.3194620907306671, 1.7394956350326538, 2.1139116287231445, 3.7631723880767822, 0.10146134346723557, 3.601801633834839, 0.02272859960794449, 0.675358772277832, 2.1999495029449463]], "avg_entropy": [2.1145929464927087], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "nearly all living cells", "score_of_biggest_cluster": 0.12061631679534912, "score_of_first_answer": 0.12061631679534912, "max_score_of_first_answer": 0.9979190230369568, "min_score_of_first_answer": 0.006218337919563055, "avg_score": 0.12061631679534912, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42825545580597224, "std_of_first_answer": 0.42825545580597224, "avg_entropy_of_first_answer": 2.1145929464927087, "lex_sim": 1.0}
{"question": "when did canada get rid of the death penalty", "answer": ["July 14, 1976"], "scores": [[0.3117411732673645, 0.995508074760437, 0.9673243761062622, 0.9528050422668457, 0.9842146635055542, 0.9982841610908508, 0.9947435855865479, 0.9996776580810547, 0.9939286708831787, 0.9976709485054016]], "normalized_score": [0.8795636892318726], "tokens": [["\u2581July", "\u2581", "1", "4", ",", "\u2581", "1", "9", "7", "6"]], "entropy": [[2.0318455696105957, 0.03311702236533165, 0.19506989419460297, 0.28716975450515747, 0.09883054345846176, 0.014700156636536121, 0.03311654552817345, 0.0031362646259367466, 0.04108264669775963, 0.02057870849967003]], "avg_entropy": [0.27586471061222256], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "July 14, 1976", "score_of_biggest_cluster": 0.8795636892318726, "score_of_first_answer": 0.8795636892318726, "max_score_of_first_answer": 0.9996776580810547, "min_score_of_first_answer": 0.3117411732673645, "avg_score": 0.8795636892318726, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.20314518083502414, "std_of_first_answer": 0.20314518083502414, "avg_entropy_of_first_answer": 0.27586471061222256, "lex_sim": 1.0}
{"question": "who wrote the song balls to the wall", "answer": ["Tom Sawyer"], "scores": [[0.18811587989330292, 0.0010792019311338663, 0.33775174617767334, 0.9886341094970703]], "normalized_score": [0.09073823690414429], "tokens": [["\u2581Tom", "\u2581S", "aw", "yer"]], "entropy": [[2.646660804748535, 2.230649471282959, 2.8638112545013428, 0.11443488299846649]], "avg_entropy": [1.9638891033828259], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Deaffy", "score_of_biggest_cluster": 0.09073823690414429, "score_of_first_answer": 0.09073823690414429, "max_score_of_first_answer": 0.9886341094970703, "min_score_of_first_answer": 0.0010792019311338663, "avg_score": 0.09073823690414429, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37169073129560576, "std_of_first_answer": 0.37169073129560576, "avg_entropy_of_first_answer": 1.9638891033828259, "lex_sim": 1.0}
{"question": "who does the voice of stewie family guy", "answer": ["Seth MacFarlane"], "scores": [[0.9241757392883301, 0.9998007416725159, 0.697135865688324, 0.9638685584068298, 0.9998052716255188, 0.9848220944404602]], "normalized_score": [0.92125403881073], "tokens": [["\u2581S", "eth", "\u2581Mac", "F", "ar", "lane"]], "entropy": [[0.6783385276794434, 0.002544241026043892, 1.0359150171279907, 0.16958370804786682, 0.002628573914989829, 0.09784890711307526]], "avg_entropy": [0.33114316248490167], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Seth MacFarlane", "score_of_biggest_cluster": 0.92125403881073, "score_of_first_answer": 0.92125403881073, "max_score_of_first_answer": 0.9998052716255188, "min_score_of_first_answer": 0.697135865688324, "avg_score": 0.92125403881073, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.10656695065298889, "std_of_first_answer": 0.10656695065298889, "avg_entropy_of_first_answer": 0.33114316248490167, "lex_sim": 1.0}
{"question": "who sang our god is an awesome god", "answer": ["Chris Tomlin"], "scores": [[0.2572469413280487, 0.9725540280342102, 0.9930395483970642]], "normalized_score": [0.6286517977714539], "tokens": [["\u2581Chris", "\u2581Tom", "lin"]], "entropy": [[3.297853946685791, 0.13761776685714722, 0.05097772181034088]], "avg_entropy": [1.1621498117844264], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rich Mullins", "score_of_biggest_cluster": 0.6286517977714539, "score_of_first_answer": 0.6286517977714539, "max_score_of_first_answer": 0.9930395483970642, "min_score_of_first_answer": 0.2572469413280487, "avg_score": 0.6286517977714539, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3421297098817626, "std_of_first_answer": 0.3421297098817626, "avg_entropy_of_first_answer": 1.1621498117844264, "lex_sim": 1.0}
{"question": "when is the second half of vikings season 5 coming out", "answer": ["Fall 2017"], "scores": [[0.004694073460996151, 0.8152637481689453, 0.9984917640686035, 0.9991176724433899, 0.9971729516983032, 0.09358632564544678]], "normalized_score": [0.26625558733940125], "tokens": [["\u2581Fall", "\u2581", "2", "0", "1", "7"]], "entropy": [[2.7061715126037598, 0.7558068633079529, 0.013764841482043266, 0.009668465703725815, 0.02146937884390354, 0.46834319829940796]], "avg_entropy": [0.6625373767067989], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2018", "score_of_biggest_cluster": 0.26625558733940125, "score_of_first_answer": 0.26625558733940125, "max_score_of_first_answer": 0.9991176724433899, "min_score_of_first_answer": 0.004694073460996151, "avg_score": 0.26625558733940125, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4315041877591166, "std_of_first_answer": 0.4315041877591166, "avg_entropy_of_first_answer": 0.6625373767067989, "lex_sim": 1.0}
{"question": "what happens when iron reacts with oxygen and water", "answer": ["rust"], "scores": [[0.34806951880455017]], "normalized_score": [0.34806951880455017], "tokens": [["\u2581rust"]], "entropy": [[2.6236486434936523]], "avg_entropy": [2.6236486434936523], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Rust", "score_of_biggest_cluster": 0.34806951880455017, "score_of_first_answer": 0.34806951880455017, "max_score_of_first_answer": 0.34806951880455017, "min_score_of_first_answer": 0.34806951880455017, "avg_score": 0.34806951880455017, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.6236486434936523, "lex_sim": 1.0}
{"question": "where was the original planet of the apes filmed at", "answer": ["California"], "scores": [[0.2774815261363983]], "normalized_score": [0.2774815261363983], "tokens": [["\u2581California"]], "entropy": [[4.265995979309082]], "avg_entropy": [4.265995979309082], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Utah", "score_of_biggest_cluster": 0.2774815261363983, "score_of_first_answer": 0.2774815261363983, "max_score_of_first_answer": 0.2774815261363983, "min_score_of_first_answer": 0.2774815261363983, "avg_score": 0.2774815261363983, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.265995979309082, "lex_sim": 1.0}
{"question": "what percentage of sunlight is captured by plants to convert it into food energy", "answer": ["20%"], "scores": [[0.7807831764221191, 0.06747204810380936, 0.12502962350845337, 0.6564028859138489]], "normalized_score": [0.2564244568347931], "tokens": [["\u2581", "2", "0", "%"]], "entropy": [[1.470929503440857, 1.634221076965332, 2.2989182472229004, 1.4643642902374268]], "avg_entropy": [1.717108279466629], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "45%", "score_of_biggest_cluster": 0.2564244568347931, "score_of_first_answer": 0.2564244568347931, "max_score_of_first_answer": 0.7807831764221191, "min_score_of_first_answer": 0.06747204810380936, "avg_score": 0.2564244568347931, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31492120987022365, "std_of_first_answer": 0.31492120987022365, "avg_entropy_of_first_answer": 1.717108279466629, "lex_sim": 1.0}
{"question": "who has most followers on instagram in world", "answer": ["133,313,975"], "scores": [[0.024541201069951057, 0.28340181708335876, 0.13696902990341187, 0.12983183562755585, 0.3425068259239197, 0.1367315798997879, 0.09308315813541412, 0.09467300027608871, 0.988640308380127, 0.09969254583120346, 0.11056201905012131, 0.08969934284687042]], "normalized_score": [0.13851691782474518], "tokens": [["\u2581", "1", "3", "3", ",", "3", "1", "3", ",", "9", "7", "5"]], "entropy": [[2.4923408031463623, 2.127163887023926, 2.762178659439087, 2.4088027477264404, 1.8988367319107056, 2.3226518630981445, 2.3524587154388428, 2.331505298614502, 0.09734731912612915, 2.3104395866394043, 2.3000025749206543, 2.3180487155914307]], "avg_entropy": [2.1434814085563025], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Instagram's own account", "score_of_biggest_cluster": 0.13851691782474518, "score_of_first_answer": 0.13851691782474518, "max_score_of_first_answer": 0.988640308380127, "min_score_of_first_answer": 0.024541201069951057, "avg_score": 0.13851691782474518, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24903800957417507, "std_of_first_answer": 0.24903800957417507, "avg_entropy_of_first_answer": 2.1434814085563025, "lex_sim": 1.0}
{"question": "when did billy don't be a hero come out", "answer": ["1974."], "scores": [[0.3065456449985504, 0.8215813040733337, 0.9021129012107849, 0.86955326795578, 0.9170790314674377, 0.003734791651368141, 0.01683039776980877, 0.10646289587020874, 0.45705747604370117, 0.36494114995002747, 0.10318130254745483, 0.5320197939872742, 0.997076153755188, 0.9913572072982788, 0.9919039607048035, 0.09766893833875656, 0.057418763637542725, 0.987981379032135, 0.008968109264969826, 0.8787117004394531, 0.9865070581436157, 0.9997146725654602, 0.9995655417442322, 0.999457061290741, 0.9921002984046936, 0.5010164380073547]], "normalized_score": [0.3266712725162506], "tokens": [["\u2581", "1", "9", "7", "4", ".", "\u2581The", "\u2581single", "\u2581was", "\u2581released", "\u2581by", "\u2581P", "aper", "\u2581L", "ace", "\u2581on", "\u2581January", "\u2581", "6", ",", "\u2581", "1", "9", "7", "4", "."]], "entropy": [[2.833042621612549, 0.7385936975479126, 0.5802198648452759, 0.5019321441650391, 0.438728392124176, 0.77886563539505, 2.868398666381836, 2.34602689743042, 2.665062427520752, 3.032576560974121, 1.9017013311386108, 2.757068634033203, 0.03137243911623955, 0.07754762470722198, 0.049180202186107635, 2.0985829830169678, 2.9079318046569824, 0.092470683157444, 1.3683435916900635, 0.5685591697692871, 0.10597562044858932, 0.003053444204851985, 0.004825204610824585, 0.00594695657491684, 0.05679960921406746, 1.7651994228363037]], "avg_entropy": [1.1760771395907237], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "1974", "score_of_biggest_cluster": 0.3266712725162506, "score_of_first_answer": 0.3266712725162506, "max_score_of_first_answer": 0.9997146725654602, "min_score_of_first_answer": 0.003734791651368141, "avg_score": 0.3266712725162506, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.39435277422903936, "std_of_first_answer": 0.39435277422903936, "avg_entropy_of_first_answer": 1.1760771395907237, "lex_sim": 1.0}
{"question": "who won the ncaa women's championship last year 2017", "answer": ["South Carolina Gamecocks"], "scores": [[0.8823035955429077, 0.9955226182937622, 0.3631346523761749, 0.997484564781189, 0.9998562335968018]], "normalized_score": [0.7952727675437927], "tokens": [["\u2581South", "\u2581Carolina", "\u2581Game", "co", "cks"]], "entropy": [[0.7934309244155884, 0.04387481510639191, 1.0634146928787231, 0.021562974900007248, 0.0019053107826039195]], "avg_entropy": [0.3848377436166629], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "South Carolina Gamecocks", "score_of_biggest_cluster": 0.7952727675437927, "score_of_first_answer": 0.7952727675437927, "max_score_of_first_answer": 0.9998562335968018, "min_score_of_first_answer": 0.3631346523761749, "avg_score": 0.7952727675437927, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.24634911764533954, "std_of_first_answer": 0.24634911764533954, "avg_entropy_of_first_answer": 0.3848377436166629, "lex_sim": 1.0}
{"question": "what is the process to become a santero", "answer": ["Santeros are not people who practice religion; they are people who have been chosen by the orichas, in Santer\u00eda."], "scores": [[0.150319442152977, 0.9433924555778503, 0.08297481387853622, 0.31102538108825684, 0.03285566717386246, 0.011440563946962357, 0.5318881273269653, 0.037368293851614, 0.020337382331490517, 0.07994666695594788, 0.7286533117294312, 0.8781418204307556, 0.3844093978404999, 0.8133485913276672, 0.14049503207206726, 0.4134591817855835, 0.21434544026851654, 0.6613486409187317, 0.6806252002716064, 0.19450590014457703, 0.021595580503344536, 0.9358428716659546, 0.07616553455591202, 0.008581391535699368, 0.019703971222043037, 0.9835664629936218, 0.22540131211280823, 0.24924995005130768]], "normalized_score": [0.16574585437774658], "tokens": [["\u2581San", "ter", "os", "\u2581are", "\u2581not", "\u2581people", "\u2581who", "\u2581practice", "\u2581religion", ";", "\u2581they", "\u2581are", "\u2581people", "\u2581who", "\u2581have", "\u2581been", "\u2581chosen", "\u2581by", "\u2581the", "\u2581or", "ich", "as", ",", "\u2581in", "\u2581San", "ter", "\u00eda", "."]], "entropy": [[5.263238906860352, 0.3059054911136627, 1.1242725849151611, 3.711663246154785, 5.062506675720215, 4.655795574188232, 2.233306884765625, 3.980912685394287, 3.441879987716675, 2.6348671913146973, 1.3344194889068604, 0.8642125129699707, 3.5302157402038574, 1.2302346229553223, 3.855100154876709, 2.771327495574951, 3.376959800720215, 1.305434226989746, 1.5805208683013916, 2.7343859672546387, 0.3251723647117615, 0.28881174325942993, 2.0516128540039062, 3.082266092300415, 3.4006218910217285, 0.11618319153785706, 0.8084609508514404, 2.6281776428222656]], "avg_entropy": [2.41780238705022], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "a cleansing ritual", "score_of_biggest_cluster": 0.16574585437774658, "score_of_first_answer": 0.16574585437774658, "max_score_of_first_answer": 0.9835664629936218, "min_score_of_first_answer": 0.008581391535699368, "avg_score": 0.16574585437774658, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.333066644123649, "std_of_first_answer": 0.333066644123649, "avg_entropy_of_first_answer": 2.41780238705022, "lex_sim": 1.0}
{"question": "where did you go to drink during prohibition", "answer": ["speakeasy"], "scores": [[0.4146743416786194, 0.9984968900680542, 0.8320208191871643]], "normalized_score": [0.7010183334350586], "tokens": [["\u2581spe", "ake", "asy"]], "entropy": [[2.6484713554382324, 0.014386232942342758, 0.4676094055175781]], "avg_entropy": [1.043488997966051], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Speakeasies", "score_of_biggest_cluster": 0.7010183334350586, "score_of_first_answer": 0.7010183334350586, "max_score_of_first_answer": 0.9984968900680542, "min_score_of_first_answer": 0.4146743416786194, "avg_score": 0.7010183334350586, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.24556988877861305, "std_of_first_answer": 0.24556988877861305, "avg_entropy_of_first_answer": 1.043488997966051, "lex_sim": 1.0}
{"question": "where did the ashes from ash wednesday originate", "answer": ["Mount Cannelton"], "scores": [[0.08681394904851913, 0.0003258618526160717, 0.10231391340494156, 0.1811225265264511]], "normalized_score": [0.026908064261078835], "tokens": [["\u2581Mount", "\u2581C", "annel", "ton"]], "entropy": [[5.261597156524658, 1.6088104248046875, 4.342179775238037, 3.828733444213867]], "avg_entropy": [3.7603302001953125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pope Gregory I the Great", "score_of_biggest_cluster": 0.026908064261078835, "score_of_first_answer": 0.026908064261078835, "max_score_of_first_answer": 0.1811225265264511, "min_score_of_first_answer": 0.0003258618526160717, "avg_score": 0.026908064261078835, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06418447048669869, "std_of_first_answer": 0.06418447048669869, "avg_entropy_of_first_answer": 3.7603302001953125, "lex_sim": 1.0}
{"question": "who is the girl who played in the grinch", "answer": ["Taylor Momsen"], "scores": [[0.8436838984489441, 0.9568700790405273, 0.9922717809677124, 0.9993358254432678]], "normalized_score": [0.9458966851234436], "tokens": [["\u2581Taylor", "\u2581M", "oms", "en"]], "entropy": [[1.1819669008255005, 0.4015430808067322, 0.06900633871555328, 0.00716182217001915]], "avg_entropy": [0.4149195356294513], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Taylor Michel Momsen", "score_of_biggest_cluster": 0.9458966851234436, "score_of_first_answer": 0.9458966851234436, "max_score_of_first_answer": 0.9993358254432678, "min_score_of_first_answer": 0.8436838984489441, "avg_score": 0.9458966851234436, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06236160353464894, "std_of_first_answer": 0.06236160353464894, "avg_entropy_of_first_answer": 0.4149195356294513, "lex_sim": 1.0}
{"question": "where did the term 'robot' come from", "answer": ["Robot"], "scores": [[0.07196331024169922, 0.667131245136261]], "normalized_score": [0.21910949051380157], "tokens": [["\u2581Rob", "ot"]], "entropy": [[3.727027416229248, 1.227428913116455]], "avg_entropy": [2.4772281646728516], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Czech", "score_of_biggest_cluster": 0.21910949051380157, "score_of_first_answer": 0.21910949051380157, "max_score_of_first_answer": 0.667131245136261, "min_score_of_first_answer": 0.07196331024169922, "avg_score": 0.21910949051380157, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2975839674472809, "std_of_first_answer": 0.2975839674472809, "avg_entropy_of_first_answer": 2.4772281646728516, "lex_sim": 1.0}
{"question": "who was suite judy blue eyes written about", "answer": ["Crosby Stephen"], "scores": [[0.04152355343103409, 0.9164382815361023, 0.9984797835350037, 0.00014121078129392117]], "normalized_score": [0.04812837764620781], "tokens": [["\u2581C", "ros", "by", "\u2581Stephen"]], "entropy": [[1.8909754753112793, 0.3814620077610016, 0.01332608051598072, 1.1168113946914673]], "avg_entropy": [0.8506437395699322], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "singer/songwriter Judy Collins", "score_of_biggest_cluster": 0.04812837764620781, "score_of_first_answer": 0.04812837764620781, "max_score_of_first_answer": 0.9984797835350037, "min_score_of_first_answer": 0.00014121078129392117, "avg_score": 0.04812837764620781, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4694387967193607, "std_of_first_answer": 0.4694387967193607, "avg_entropy_of_first_answer": 0.8506437395699322, "lex_sim": 1.0}
{"question": "how many rooms is there in buckingham palace", "answer": ["775"], "scores": [[0.9222720861434937, 0.958132803440094, 0.9850548505783081, 0.9919628500938416]], "normalized_score": [0.9639624953269958], "tokens": [["\u2581", "7", "7", "5"]], "entropy": [[0.6386280655860901, 0.2514341175556183, 0.10787587612867355, 0.06362061202526093]], "avg_entropy": [0.2653896678239107], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "775", "score_of_biggest_cluster": 0.9639624953269958, "score_of_first_answer": 0.9639624953269958, "max_score_of_first_answer": 0.9919628500938416, "min_score_of_first_answer": 0.9222720861434937, "avg_score": 0.9639624953269958, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.027387791841325913, "std_of_first_answer": 0.027387791841325913, "avg_entropy_of_first_answer": 0.2653896678239107, "lex_sim": 1.0}
{"question": "which foreign currency option is the\u200b right but not the\u200b obligation to buy foreign\u200b currency", "answer": ["call"], "scores": [[0.5823647379875183]], "normalized_score": [0.5823647379875183], "tokens": [["\u2581call"]], "entropy": [[1.9267706871032715]], "avg_entropy": [1.9267706871032715], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "foreign exchange option", "score_of_biggest_cluster": 0.5823647379875183, "score_of_first_answer": 0.5823647379875183, "max_score_of_first_answer": 0.5823647379875183, "min_score_of_first_answer": 0.5823647379875183, "avg_score": 0.5823647379875183, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.9267706871032715, "lex_sim": 1.0}
{"question": "who gets to race in the daytona clash", "answer": ["2019 Clash Champions"], "scores": [[0.10657932609319687, 0.6774489283561707, 0.9564679861068726, 0.3799999952316284, 0.7169530987739563, 0.10768342763185501, 0.9699794054031372, 0.005759717430919409]], "normalized_score": [0.24083846807479858], "tokens": [["\u2581", "2", "0", "1", "9", "\u2581Cl", "ash", "\u2581Champions"]], "entropy": [[4.778262138366699, 1.1685625314712524, 0.2867405116558075, 2.077609062194824, 0.842876136302948, 2.55157470703125, 0.15190160274505615, 3.6868228912353516]], "avg_entropy": [1.9430436976253986], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Daytona Pole Award winners", "score_of_biggest_cluster": 0.24083846807479858, "score_of_first_answer": 0.24083846807479858, "max_score_of_first_answer": 0.9699794054031372, "min_score_of_first_answer": 0.005759717430919409, "avg_score": 0.24083846807479858, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36646680501682033, "std_of_first_answer": 0.36646680501682033, "avg_entropy_of_first_answer": 1.9430436976253986, "lex_sim": 1.0}
{"question": "what are the 5 boroughs of new york city", "answer": ["Manhattan, Queens, The Bronx, Staten Island, Brooklyn"], "scores": [[0.18806365132331848, 0.9996296167373657, 0.9202451705932617, 0.07510499656200409, 0.9945873022079468, 0.1859220415353775, 0.9996883869171143, 0.9997606873512268, 0.9924312829971313, 0.26506420969963074, 0.9999455213546753, 0.9955809712409973, 0.8364992737770081, 0.6771340370178223, 0.9986351132392883]], "normalized_score": [0.5889433026313782], "tokens": [["\u2581Manh", "attan", ",", "\u2581Queens", ",", "\u2581The", "\u2581Bron", "x", ",", "\u2581St", "aten", "\u2581Island", ",", "\u2581Brook", "lyn"]], "entropy": [[2.2017059326171875, 0.004437373019754887, 0.46893244981765747, 1.6506986618041992, 0.050804391503334045, 1.4882564544677734, 0.003618791000917554, 0.0031453901901841164, 0.06431280076503754, 0.7710423469543457, 0.0008144102175720036, 0.03730945661664009, 0.6253763437271118, 0.8203762173652649, 0.013571847230196]], "avg_entropy": [0.5469601911531451], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Manhattan", "score_of_biggest_cluster": 0.5889433026313782, "score_of_first_answer": 0.5889433026313782, "max_score_of_first_answer": 0.9999455213546753, "min_score_of_first_answer": 0.07510499656200409, "avg_score": 0.5889433026313782, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35172219827915463, "std_of_first_answer": 0.35172219827915463, "avg_entropy_of_first_answer": 0.5469601911531451, "lex_sim": 1.0}
{"question": "the first element on the periodic table is", "answer": ["hydrogen"], "scores": [[0.6036965847015381, 0.9997726082801819]], "normalized_score": [0.7768908143043518], "tokens": [["\u2581hydro", "gen"]], "entropy": [[0.836359977722168, 0.0026814094744622707]], "avg_entropy": [0.4195206935983151], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Hydro\u00adgen", "score_of_biggest_cluster": 0.7768908143043518, "score_of_first_answer": 0.7768908143043518, "max_score_of_first_answer": 0.9997726082801819, "min_score_of_first_answer": 0.6036965847015381, "avg_score": 0.7768908143043518, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1980380117893219, "std_of_first_answer": 0.1980380117893219, "avg_entropy_of_first_answer": 0.4195206935983151, "lex_sim": 1.0}
{"question": "when did south sudan join east african community", "answer": ["On 23 November 2018"], "scores": [[0.0039017023518681526, 0.46888458728790283, 0.3580075800418854, 0.06971980631351471, 0.1355939358472824, 0.9660090208053589, 0.9977104663848877, 0.9999115467071533, 0.888401985168457, 0.04945025220513344]], "normalized_score": [0.2197117805480957], "tokens": [["\u2581On", "\u2581", "2", "3", "\u2581November", "\u2581", "2", "0", "1", "8"]], "entropy": [[3.3177390098571777, 2.2134463787078857, 1.422515630722046, 2.5010786056518555, 2.5450963973999023, 0.17641589045524597, 0.016704654321074486, 0.0011602138401940465, 0.35337939858436584, 1.977709412574768]], "avg_entropy": [1.4525245592114515], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "April 2016", "score_of_biggest_cluster": 0.2197117805480957, "score_of_first_answer": 0.2197117805480957, "max_score_of_first_answer": 0.9999115467071533, "min_score_of_first_answer": 0.0039017023518681526, "avg_score": 0.2197117805480957, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4066238703912086, "std_of_first_answer": 0.4066238703912086, "avg_entropy_of_first_answer": 1.4525245592114515, "lex_sim": 1.0}
{"question": "sweet leavened bread prepared for easter in romania", "answer": ["Pascha"], "scores": [[0.2710638642311096, 0.09031037986278534]], "normalized_score": [0.1564604789018631], "tokens": [["\u2581Pas", "cha"]], "entropy": [[2.0855720043182373, 0.6926865577697754]], "avg_entropy": [1.3891292810440063], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kozunak", "score_of_biggest_cluster": 0.1564604789018631, "score_of_first_answer": 0.1564604789018631, "max_score_of_first_answer": 0.2710638642311096, "min_score_of_first_answer": 0.09031037986278534, "avg_score": 0.1564604789018631, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09037674218416214, "std_of_first_answer": 0.09037674218416214, "avg_entropy_of_first_answer": 1.3891292810440063, "lex_sim": 1.0}
{"question": "who sings it's my party and i cry if i want to", "answer": ["Lesley Gore"], "scores": [[0.959702730178833, 0.9774658679962158, 0.9874158501625061, 0.9956122636795044]], "normalized_score": [0.9799573421478271], "tokens": [["\u2581Les", "ley", "\u2581G", "ore"]], "entropy": [[0.2532534599304199, 0.11660248041152954, 0.12445805966854095, 0.0454634390771389]], "avg_entropy": [0.13494435977190733], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Lesley Gore", "score_of_biggest_cluster": 0.9799573421478271, "score_of_first_answer": 0.9799573421478271, "max_score_of_first_answer": 0.9956122636795044, "min_score_of_first_answer": 0.959702730178833, "avg_score": 0.9799573421478271, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.013389631474881595, "std_of_first_answer": 0.013389631474881595, "avg_entropy_of_first_answer": 0.13494435977190733, "lex_sim": 1.0}
{"question": "how many episodes in series 7 of game of thrones are there", "answer": ["7"], "scores": [[0.8272318243980408, 0.8371943831443787]], "normalized_score": [0.8321982026100159], "tokens": [["\u2581", "7"]], "entropy": [[1.0722168684005737, 0.6539144515991211]], "avg_entropy": [0.8630656599998474], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "7", "score_of_biggest_cluster": 0.8321982026100159, "score_of_first_answer": 0.8321982026100159, "max_score_of_first_answer": 0.8371943831443787, "min_score_of_first_answer": 0.8272318243980408, "avg_score": 0.8321982026100159, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.004981279373168945, "std_of_first_answer": 0.004981279373168945, "avg_entropy_of_first_answer": 0.8630656599998474, "lex_sim": 1.0}
{"question": "who got the first arjun award in athletics", "answer": ["Krishna Prakash, 2012"], "scores": [[0.019957615062594414, 0.9779524207115173, 0.7095467448234558, 0.34790951013565063, 0.04210352897644043, 0.9979215264320374, 0.002673884155228734, 0.1391064077615738, 0.4236680567264557, 0.9761475324630737, 0.2775495946407318, 0.11447414755821228]], "normalized_score": [0.1776699423789978], "tokens": [["\u2581Kr", "ish", "na", "\u2581P", "rak", "ash", ",", "\u2581", "2", "0", "1", "2"]], "entropy": [[3.36173152923584, 0.18565505743026733, 1.118922233581543, 1.2530019283294678, 1.2117066383361816, 0.01772966794669628, 1.1338715553283691, 4.747167587280273, 0.7857929468154907, 0.17429567873477936, 0.6184280514717102, 2.027200937271118]], "avg_entropy": [1.386291984313478], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gurbachan Singh Randhawa", "score_of_biggest_cluster": 0.1776699423789978, "score_of_first_answer": 0.1776699423789978, "max_score_of_first_answer": 0.9979215264320374, "min_score_of_first_answer": 0.002673884155228734, "avg_score": 0.1776699423789978, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37784279514466335, "std_of_first_answer": 0.37784279514466335, "avg_entropy_of_first_answer": 1.386291984313478, "lex_sim": 1.0}
{"question": "who sang what i like about you originally", "answer": ["Dan Seals"], "scores": [[0.005616164766252041, 0.19285798072814941, 0.9651046991348267]], "normalized_score": [0.1014886125922203], "tokens": [["\u2581Dan", "\u2581Se", "als"]], "entropy": [[1.6305265426635742, 2.8744616508483887, 0.2120085060596466]], "avg_entropy": [1.5723322331905365], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Romantics.", "score_of_biggest_cluster": 0.1014886125922203, "score_of_first_answer": 0.1014886125922203, "max_score_of_first_answer": 0.9651046991348267, "min_score_of_first_answer": 0.005616164766252041, "avg_score": 0.1014886125922203, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4152700245114077, "std_of_first_answer": 0.4152700245114077, "avg_entropy_of_first_answer": 1.5723322331905365, "lex_sim": 1.0}
{"question": "what season does bart bass die in gossip girl", "answer": ["season 3"], "scores": [[0.2170308530330658, 0.8992273807525635, 0.1469520777463913]], "normalized_score": [0.3060944974422455], "tokens": [["\u2581season", "\u2581", "3"]], "entropy": [[2.241199016571045, 0.5443457365036011, 1.7189594507217407]], "avg_entropy": [1.5015014012654622], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": ")", "score_of_biggest_cluster": 0.3060944974422455, "score_of_first_answer": 0.3060944974422455, "max_score_of_first_answer": 0.8992273807525635, "min_score_of_first_answer": 0.1469520777463913, "avg_score": 0.3060944974422455, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.339316513776223, "std_of_first_answer": 0.339316513776223, "avg_entropy_of_first_answer": 1.5015014012654622, "lex_sim": 1.0}
{"question": "what languages are spoken in india the most", "answer": ["English"], "scores": [[0.04818344488739967]], "normalized_score": [0.048183441162109375], "tokens": [["\u2581English"]], "entropy": [[1.6052966117858887]], "avg_entropy": [1.6052966117858887], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bengali", "score_of_biggest_cluster": 0.048183441162109375, "score_of_first_answer": 0.048183441162109375, "max_score_of_first_answer": 0.04818344488739967, "min_score_of_first_answer": 0.04818344488739967, "avg_score": 0.048183441162109375, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.6052966117858887, "lex_sim": 1.0}
{"question": "who holds the most home runs in mlb history", "answer": ["Barry Bonds"], "scores": [[0.8889600038528442, 0.8694661855697632, 0.9977436065673828]], "normalized_score": [0.9170323014259338], "tokens": [["\u2581Barry", "\u2581B", "onds"]], "entropy": [[0.7507700324058533, 0.5080910325050354, 0.02444063499569893]], "avg_entropy": [0.42776723330219585], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Barry Bonds", "score_of_biggest_cluster": 0.9170323014259338, "score_of_first_answer": 0.9170323014259338, "max_score_of_first_answer": 0.9977436065673828, "min_score_of_first_answer": 0.8694661855697632, "avg_score": 0.9170323014259338, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0564397199660437, "std_of_first_answer": 0.0564397199660437, "avg_entropy_of_first_answer": 0.42776723330219585, "lex_sim": 1.0}
{"question": "when was the dome of the rock completed", "answer": ["October 692"], "scores": [[0.008856563828885555, 0.9788784980773926, 0.2400497943162918, 0.763070285320282, 0.6914382576942444]], "normalized_score": [0.2559308707714081], "tokens": [["\u2581October", "\u2581", "6", "9", "2"]], "entropy": [[1.4305869340896606, 0.13384495675563812, 1.755669355392456, 0.754097044467926, 0.8420262932777405]], "avg_entropy": [0.9832449167966842], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "691 CE", "score_of_biggest_cluster": 0.2559308707714081, "score_of_first_answer": 0.2559308707714081, "max_score_of_first_answer": 0.9788784980773926, "min_score_of_first_answer": 0.008856563828885555, "avg_score": 0.2559308707714081, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3570235300638462, "std_of_first_answer": 0.3570235300638462, "avg_entropy_of_first_answer": 0.9832449167966842, "lex_sim": 1.0}
{"question": "where was because i could not stop for death published", "answer": ["Springfield, Massachusetts"], "scores": [[0.05909312888979912, 0.9514480233192444, 0.7789613008499146, 0.7952460646629333]], "normalized_score": [0.4320010840892792], "tokens": [["\u2581Spring", "field", ",", "\u2581Massachusetts"]], "entropy": [[4.1122026443481445, 0.40993964672088623, 0.9750996828079224, 0.8509833812713623]], "avg_entropy": [1.5870563387870789], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1890", "score_of_biggest_cluster": 0.4320010840892792, "score_of_first_answer": 0.4320010840892792, "max_score_of_first_answer": 0.9514480233192444, "min_score_of_first_answer": 0.05909312888979912, "avg_score": 0.4320010840892792, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3455832481390622, "std_of_first_answer": 0.3455832481390622, "avg_entropy_of_first_answer": 1.5870563387870789, "lex_sim": 1.0}
{"question": "who's the guy in call me maybe", "answer": ["Drake Bell / Toby"], "scores": [[0.02104201540350914, 0.9992009997367859, 0.22514478862285614, 0.0010862210765480995, 0.0080329654738307, 0.15679295361042023]], "normalized_score": [0.04317382350564003], "tokens": [["\u2581Dra", "ke", "\u2581Bell", "\u2581/", "\u2581T", "oby"]], "entropy": [[3.180981397628784, 0.009517597034573555, 1.2678680419921875, 0.6358523368835449, 4.337311744689941, 3.6181721687316895]], "avg_entropy": [2.174950547826787], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Holden Nowell", "score_of_biggest_cluster": 0.04317382350564003, "score_of_first_answer": 0.04317382350564003, "max_score_of_first_answer": 0.9992009997367859, "min_score_of_first_answer": 0.0010862210765480995, "avg_score": 0.04317382350564003, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3517158094155589, "std_of_first_answer": 0.3517158094155589, "avg_entropy_of_first_answer": 2.174950547826787, "lex_sim": 1.0}
{"question": "when was united nations convention on the rights of the child created", "answer": ["November 20, 1989"], "scores": [[0.7000266313552856, 0.9868081212043762, 0.9796863198280334, 0.9672868847846985, 0.9392043352127075, 0.996434211730957, 0.9958725571632385, 0.9991841912269592, 0.9925610423088074, 0.9983227849006653]], "normalized_score": [0.9508402943611145], "tokens": [["\u2581November", "\u2581", "2", "0", ",", "\u2581", "1", "9", "8", "9"]], "entropy": [[1.1943480968475342, 0.08766813576221466, 0.11686607450246811, 0.1961565911769867, 0.31975433230400085, 0.02697238326072693, 0.02786286547780037, 0.00730707123875618, 0.054905131459236145, 0.01604732684791088]], "avg_entropy": [0.2047888008877635], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "20 November 1989", "score_of_biggest_cluster": 0.9508402943611145, "score_of_first_answer": 0.9508402943611145, "max_score_of_first_answer": 0.9991841912269592, "min_score_of_first_answer": 0.7000266313552856, "avg_score": 0.9508402943611145, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08698129474566246, "std_of_first_answer": 0.08698129474566246, "avg_entropy_of_first_answer": 0.2047888008877635, "lex_sim": 1.0}
{"question": "who does bryce dallas howard play in the grinch", "answer": ["Bugs Howard"], "scores": [[0.024992726743221283, 0.1464112550020218, 0.04649621248245239]], "normalized_score": [0.05541175976395607], "tokens": [["\u2581B", "ugs", "\u2581Howard"]], "entropy": [[4.367855072021484, 3.8407678604125977, 1.3521907329559326]], "avg_entropy": [3.1869378884633384], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Surprised Who", "score_of_biggest_cluster": 0.05541175976395607, "score_of_first_answer": 0.05541175976395607, "max_score_of_first_answer": 0.1464112550020218, "min_score_of_first_answer": 0.024992726743221283, "avg_score": 0.05541175976395607, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.052902294242581256, "std_of_first_answer": 0.052902294242581256, "avg_entropy_of_first_answer": 3.1869378884633384, "lex_sim": 1.0}
{"question": "what is the origin of the name robinson", "answer": ["It means \u201cson of Robert\u201d."], "scores": [[0.0815802663564682, 0.10729695111513138, 0.10556137561798096, 0.7700427174568176, 0.9929940104484558, 0.41472774744033813, 0.24113769829273224]], "normalized_score": [0.2552841603755951], "tokens": [["\u2581It", "\u2581means", "\u2581\u201c", "son", "\u2581of", "\u2581Robert", "\u201d."]], "entropy": [[4.582274436950684, 1.877923846244812, 2.5196714401245117, 1.460649847984314, 0.061734214425086975, 1.3353400230407715, 1.6479647159576416]], "avg_entropy": [1.926508360675403], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "English language patronymic surname", "score_of_biggest_cluster": 0.2552841603755951, "score_of_first_answer": 0.2552841603755951, "max_score_of_first_answer": 0.9929940104484558, "min_score_of_first_answer": 0.0815802663564682, "avg_score": 0.2552841603755951, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.335243007795658, "std_of_first_answer": 0.335243007795658, "avg_entropy_of_first_answer": 1.926508360675403, "lex_sim": 1.0}
{"question": "where does the bob and tom show broadcast from", "answer": ["Indianapolis"], "scores": [[0.5315921306610107, 0.996475875377655]], "normalized_score": [0.7278178334236145], "tokens": [["\u2581Indian", "apolis"]], "entropy": [[2.6271443367004395, 0.030271675437688828]], "avg_entropy": [1.3287080060690641], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "WFBQ in Indianapolis, Indiana", "score_of_biggest_cluster": 0.7278178334236145, "score_of_first_answer": 0.7278178334236145, "max_score_of_first_answer": 0.996475875377655, "min_score_of_first_answer": 0.5315921306610107, "avg_score": 0.7278178334236145, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23244187235832214, "std_of_first_answer": 0.23244187235832214, "avg_entropy_of_first_answer": 1.3287080060690641, "lex_sim": 1.0}
{"question": "what proposition made the insurance commissioner an elected position", "answer": ["Proposition 108"], "scores": [[0.6185778379440308, 0.9965758919715881, 0.6848801970481873, 0.967228889465332, 0.043636519461870193]], "normalized_score": [0.4468684792518616], "tokens": [["\u2581Proposition", "\u2581", "1", "0", "8"]], "entropy": [[2.230639696121216, 0.03556593880057335, 0.9399980306625366, 0.21365010738372803, 0.3409885764122009]], "avg_entropy": [0.7521684698760509], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Proposition 103", "score_of_biggest_cluster": 0.4468684792518616, "score_of_first_answer": 0.4468684792518616, "max_score_of_first_answer": 0.9965758919715881, "min_score_of_first_answer": 0.043636519461870193, "avg_score": 0.4468684792518616, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3434786521254628, "std_of_first_answer": 0.3434786521254628, "avg_entropy_of_first_answer": 0.7521684698760509, "lex_sim": 1.0}
{"question": "what is an example of an americanized mexican food created by tex-mex cuisine", "answer": ["fajitas"], "scores": [[0.04610288515686989, 0.960345983505249, 0.7652527689933777]], "normalized_score": [0.3235839009284973], "tokens": [["\u2581f", "aj", "itas"]], "entropy": [[3.396965503692627, 0.22509095072746277, 0.5583643317222595]], "avg_entropy": [1.393473595380783], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "fajitas", "score_of_biggest_cluster": 0.3235839009284973, "score_of_first_answer": 0.3235839009284973, "max_score_of_first_answer": 0.960345983505249, "min_score_of_first_answer": 0.04610288515686989, "avg_score": 0.3235839009284973, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3931466167910561, "std_of_first_answer": 0.3931466167910561, "avg_entropy_of_first_answer": 1.393473595380783, "lex_sim": 1.0}
{"question": "what is the name of son of lord krishna", "answer": ["Balram"], "scores": [[0.04738364741206169, 0.1067119687795639]], "normalized_score": [0.0711083933711052], "tokens": [["\u2581Bal", "ram"]], "entropy": [[3.3725192546844482, 0.8669692873954773]], "avg_entropy": [2.1197442710399628], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pradyumna", "score_of_biggest_cluster": 0.0711083933711052, "score_of_first_answer": 0.0711083933711052, "max_score_of_first_answer": 0.1067119687795639, "min_score_of_first_answer": 0.04738364741206169, "avg_score": 0.0711083933711052, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.029664160683751106, "std_of_first_answer": 0.029664160683751106, "avg_entropy_of_first_answer": 2.1197442710399628, "lex_sim": 1.0}
{"question": "the king of kalinga in the mauryan period was", "answer": ["Chand Ashoka"], "scores": [[0.05247959494590759, 0.6671732664108276, 0.1348905861377716, 0.9180272221565247]], "normalized_score": [0.256605863571167], "tokens": [["\u2581Ch", "and", "\u2581Ash", "oka"]], "entropy": [[3.247751235961914, 1.9539927244186401, 1.7298616170883179, 0.3244301676750183]], "avg_entropy": [1.8140089362859726], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ashoka", "score_of_biggest_cluster": 0.256605863571167, "score_of_first_answer": 0.256605863571167, "max_score_of_first_answer": 0.9180272221565247, "min_score_of_first_answer": 0.05247959494590759, "avg_score": 0.256605863571167, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3617119127915266, "std_of_first_answer": 0.3617119127915266, "avg_entropy_of_first_answer": 1.8140089362859726, "lex_sim": 1.0}
{"question": "who heads the executive department of west virginia government", "answer": ["Jim Justice"], "scores": [[0.7202177047729492, 0.9908909201622009]], "normalized_score": [0.8447823524475098], "tokens": [["\u2581Jim", "\u2581Justice"]], "entropy": [[1.5719740390777588, 0.08744246512651443]], "avg_entropy": [0.8297082521021366], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "the Governor of West Virginia", "score_of_biggest_cluster": 0.8447823524475098, "score_of_first_answer": 0.8447823524475098, "max_score_of_first_answer": 0.9908909201622009, "min_score_of_first_answer": 0.7202177047729492, "avg_score": 0.8447823524475098, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.13533660769462585, "std_of_first_answer": 0.13533660769462585, "avg_entropy_of_first_answer": 0.8297082521021366, "lex_sim": 1.0}
{"question": "branch of science that deals with the structure of human body parts", "answer": ["anatomy"], "scores": [[0.7173689007759094, 0.995468020439148, 0.9974617958068848]], "normalized_score": [0.8930765986442566], "tokens": [["\u2581an", "atom", "y"]], "entropy": [[0.9912311434745789, 0.04537900537252426, 0.02276667393743992]], "avg_entropy": [0.3531256075948477], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Anatomy", "score_of_biggest_cluster": 0.8930765986442566, "score_of_first_answer": 0.8930765986442566, "max_score_of_first_answer": 0.9974617958068848, "min_score_of_first_answer": 0.7173689007759094, "avg_score": 0.8930765986442566, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.13156963738919494, "std_of_first_answer": 0.13156963738919494, "avg_entropy_of_first_answer": 0.3531256075948477, "lex_sim": 1.0}
{"question": "last team to win fa cup outside top flight", "answer": ["Southampton Football Club"], "scores": [[0.03889574110507965, 0.7948956489562988, 0.9988692402839661, 0.013060685247182846, 0.9914504885673523]], "normalized_score": [0.2091180682182312], "tokens": [["\u2581South", "am", "pton", "\u2581Football", "\u2581Club"]], "entropy": [[2.4315226078033447, 0.5410244464874268, 0.010729148983955383, 1.231781244277954, 0.06494792550802231]], "avg_entropy": [0.8560010746121407], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Arsenal", "score_of_biggest_cluster": 0.2091180682182312, "score_of_first_answer": 0.2091180682182312, "max_score_of_first_answer": 0.9988692402839661, "min_score_of_first_answer": 0.013060685247182846, "avg_score": 0.2091180682182312, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4481846972190904, "std_of_first_answer": 0.4481846972190904, "avg_entropy_of_first_answer": 0.8560010746121407, "lex_sim": 1.0}
{"question": "what does a bull shark tooth look like", "answer": ["It's big and sharp."], "scores": [[0.04573071002960205, 0.06669358909130096, 0.9946476817131042, 0.0031516440212726593, 0.26470959186553955, 0.12583564221858978, 0.4391927123069763]], "normalized_score": [0.10491032153367996], "tokens": [["\u2581It", "'", "s", "\u2581big", "\u2581and", "\u2581sharp", "."]], "entropy": [[5.083773136138916, 2.187314987182617, 0.04092343524098396, 4.519543647766113, 2.1075758934020996, 3.6736884117126465, 1.9050071239471436]], "avg_entropy": [2.7882609479129314], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "needle-like teeth", "score_of_biggest_cluster": 0.10491032153367996, "score_of_first_answer": 0.10491032153367996, "max_score_of_first_answer": 0.9946476817131042, "min_score_of_first_answer": 0.0031516440212726593, "avg_score": 0.10491032153367996, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3244849653408483, "std_of_first_answer": 0.3244849653408483, "avg_entropy_of_first_answer": 2.7882609479129314, "lex_sim": 1.0}
{"question": "chris cornell she's going to change the world", "answer": ["Yes I would."], "scores": [[0.008365511894226074, 0.14419101178646088, 0.015189630910754204, 0.038674697279930115, 0.045364510267972946, 0.2823171615600586, 0.33733898401260376, 0.07875490188598633, 0.9915614724159241, 0.01701282523572445, 0.026829762384295464, 0.003848334774374962, 0.14230741560459137, 0.019531048834323883, 0.9897046685218811, 0.6133694052696228, 0.005529969464987516, 0.7479203343391418, 0.9641027450561523, 0.0911843478679657, 0.006150241009891033, 0.007485017646104097, 0.7793008089065552, 0.9898180365562439, 0.6064503788948059]], "normalized_score": [0.0890769362449646], "tokens": [["\u2581Yes", "\u2581I", "\u2581would", ".", "\u2581I", "'", "m", "\u2581g", "onna", "\u2581get", "\u2581the", "\u2581words", "\u2581out", "\u2581ton", "ight", ".", "\u2581There", "'", "s", "\u2581something", "\u2581else", "\u2581on", "\u2581my", "\u2581mind", "."]], "entropy": [[5.704412460327148, 2.2304232120513916, 2.278038740158081, 1.6993238925933838, 1.9918887615203857, 3.0963563919067383, 1.2029061317443848, 4.826544761657715, 0.07661929726600647, 3.1182146072387695, 3.963106870651245, 4.804558753967285, 3.4049184322357178, 2.8439435958862305, 0.06830739229917526, 1.533200979232788, 2.8800883293151855, 1.2721612453460693, 0.18147021532058716, 3.0628702640533447, 3.439441680908203, 3.3088889122009277, 1.170159935951233, 0.10403653979301453, 1.925944447517395]], "avg_entropy": [2.4075130340456963], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Can't Change Me", "score_of_biggest_cluster": 0.0890769362449646, "score_of_first_answer": 0.0890769362449646, "max_score_of_first_answer": 0.9915614724159241, "min_score_of_first_answer": 0.003848334774374962, "avg_score": 0.0890769362449646, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37456754954322097, "std_of_first_answer": 0.37456754954322097, "avg_entropy_of_first_answer": 2.4075130340456963, "lex_sim": 1.0}
{"question": "where do they get the hair for a hair transplant", "answer": ["scalp"], "scores": [[0.06117492541670799, 0.995757520198822]], "normalized_score": [0.2468104511499405], "tokens": [["\u2581scal", "p"]], "entropy": [[3.5597355365753174, 0.032022908329963684]], "avg_entropy": [1.7958792224526405], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "chest", "score_of_biggest_cluster": 0.2468104511499405, "score_of_first_answer": 0.2468104511499405, "max_score_of_first_answer": 0.995757520198822, "min_score_of_first_answer": 0.06117492541670799, "avg_score": 0.2468104511499405, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.467291297391057, "std_of_first_answer": 0.467291297391057, "avg_entropy_of_first_answer": 1.7958792224526405, "lex_sim": 1.0}
{"question": "what's the biggest country in western europe", "answer": ["France"], "scores": [[0.7526965141296387]], "normalized_score": [0.7526965141296387], "tokens": [["\u2581France"]], "entropy": [[0.9123632907867432]], "avg_entropy": [0.9123632907867432], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Russia*", "score_of_biggest_cluster": 0.7526965141296387, "score_of_first_answer": 0.7526965141296387, "max_score_of_first_answer": 0.7526965141296387, "min_score_of_first_answer": 0.7526965141296387, "avg_score": 0.7526965141296387, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.9123632907867432, "lex_sim": 1.0}
{"question": "when did end of the road come out", "answer": ["September 30, 2007,"], "scores": [[0.11808304488658905, 0.9860739707946777, 0.13754084706306458, 0.3757436275482178, 0.9491123557090759, 0.9885100722312927, 0.9014124274253845, 0.9996700286865234, 0.35508430004119873, 0.25858408212661743, 0.017743930220603943]], "normalized_score": [0.34517860412597656], "tokens": [["\u2581September", "\u2581", "3", "0", ",", "\u2581", "2", "0", "0", "7", ","]], "entropy": [[2.9656052589416504, 0.09002852439880371, 1.6764638423919678, 0.8399149179458618, 0.2787339389324188, 0.0797094851732254, 0.32669809460639954, 0.003930041566491127, 0.6668503284454346, 1.8997759819030762, 1.5053552389144897]], "avg_entropy": [0.9393696048381653], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1992", "score_of_biggest_cluster": 0.34517860412597656, "score_of_first_answer": 0.34517860412597656, "max_score_of_first_answer": 0.9996700286865234, "min_score_of_first_answer": 0.017743930220603943, "avg_score": 0.34517860412597656, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38853569331937965, "std_of_first_answer": 0.38853569331937965, "avg_entropy_of_first_answer": 0.9393696048381653, "lex_sim": 1.0}
