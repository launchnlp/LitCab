{"question": "Which British film, starring Ralph Fiennes, won nine Oscars in 1997?", "answer": ["The English Patient"], "scores": [[0.129447340965271, 0.5582643747329712, 0.9305132031440735]], "normalized_score": [0.4066478908061981], "tokens": [["\u0120The", "\u0120English", "\u0120Patient"]], "entropy": [[3.2664504051208496, 2.8499231338500977, 0.3611805737018585]], "avg_entropy": [2.1591847042242684], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The English Patient", "score_of_biggest_cluster": 0.4066478908061981, "score_of_first_answer": 0.4066478908061981, "max_score_of_first_answer": 0.9305132031440735, "min_score_of_first_answer": 0.129447340965271, "avg_score": 0.4066478908061981, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.32730545608566985, "std_of_first_answer": 0.32730545608566985, "avg_entropy_of_first_answer": 2.1591847042242684, "lex_sim": 1.0}
{"question": "In which North African country was Mohammed Al-Fayed, owner of Harrods department store in London, born?", "answer": ["Morocco"], "scores": [[0.17683373391628265]], "normalized_score": [0.17683373391628265], "tokens": [["\u0120Morocco"]], "entropy": [[3.366342067718506]], "avg_entropy": [3.366342067718506], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "EGY", "score_of_biggest_cluster": 0.17683373391628265, "score_of_first_answer": 0.17683373391628265, "max_score_of_first_answer": 0.17683373391628265, "min_score_of_first_answer": 0.17683373391628265, "avg_score": 0.17683373391628265, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.366342067718506, "lex_sim": 1.0}
{"question": "\"Which British composer wrote \"\"The Dam Busters March\"\"?\"", "answer": ["Anthony Monti"], "scores": [[0.0037797903642058372, 0.005676145199686289, 0.0017421699594706297]], "normalized_score": [0.0033435195218771696], "tokens": [["\u0120Anthony", "\u0120Mont", "i"]], "entropy": [[4.616100311279297, 4.823477745056152, 0.45083585381507874]], "avg_entropy": [3.2968046367168427], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eric Coates", "score_of_biggest_cluster": 0.0033435195218771696, "score_of_first_answer": 0.0033435195218771696, "max_score_of_first_answer": 0.005676145199686289, "min_score_of_first_answer": 0.0017421699594706297, "avg_score": 0.0033435195218771696, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0016063837844492417, "std_of_first_answer": 0.0016063837844492417, "avg_entropy_of_first_answer": 3.2968046367168427, "lex_sim": 1.0}
{"question": "How many pounds are in a UK (long) ton?", "answer": ["The unit is not relevant."], "scores": [[0.0012380207190290093, 0.004478038288652897, 0.1949988454580307, 0.08814212679862976, 0.002053326228633523, 0.13218607008457184, 0.0022563012316823006, 0.24638839066028595, 0.20346926152706146, 0.42169350385665894, 0.10560506582260132, 0.16648530960083008, 0.4718114733695984, 0.004530486650764942, 0.35243549942970276, 0.12353435903787613, 0.31470558047294617]], "normalized_score": [0.056638769805431366], "tokens": [["\u0120The", "\u0120unit", "\u0120is", "\u0120not", "\u0120relevant", ".", "\u01201", "\u0120ton", "\u0120=", "\u01202", ".", "20", "46", "\u00c3\u0139", "101", "1", "\u0120kg"]], "entropy": [[3.829124689102173, 5.26285457611084, 2.4280383586883545, 4.84343147277832, 4.280180931091309, 2.4405617713928223, 2.110628843307495, 4.03565788269043, 2.3925468921661377, 2.5988054275512695, 1.791952133178711, 2.1542630195617676, 3.045515298843384, 5.025450706481934, 2.1113243103027344, 1.3113293647766113, 3.0708184242248535]], "avg_entropy": [3.1019108295440674], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2240", "score_of_biggest_cluster": 0.056638769805431366, "score_of_first_answer": 0.056638769805431366, "max_score_of_first_answer": 0.4718114733695984, "min_score_of_first_answer": 0.0012380207190290093, "avg_score": 0.056638769805431366, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14755300274719219, "std_of_first_answer": 0.14755300274719219, "avg_entropy_of_first_answer": 3.1019108295440674, "lex_sim": 1.0}
{"question": "Lake Neuchatel is in which country?", "answer": ["Switzerland"], "scores": [[0.9091690182685852]], "normalized_score": [0.9091690182685852], "tokens": [["\u0120Switzerland"]], "entropy": [[0.655591607093811]], "avg_entropy": [0.655591607093811], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ISO 3166-1:CH", "score_of_biggest_cluster": 0.9091690182685852, "score_of_first_answer": 0.9091690182685852, "max_score_of_first_answer": 0.9091690182685852, "min_score_of_first_answer": 0.9091690182685852, "avg_score": 0.9091690182685852, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.655591607093811, "lex_sim": 1.0}
{"question": "Which Indian born author who lived from 1903 to 1950 had the real name Eric Arthur Blair ?", "answer": ["George Orwell / (novelist), George Orwell / Animal Farm (novel), (novelist), Orwell (film) / (novelist), (novelist), (novel)"], "scores": [[0.9055579304695129, 0.9887670874595642, 7.431673293467611e-05, 0.007549475412815809, 0.009865961968898773, 0.4250349998474121, 0.5842918753623962, 0.017014840617775917, 0.07461025565862656, 0.29087692499160767, 0.2404995858669281, 0.0006198711926117539, 0.9688471555709839, 0.5541857481002808, 0.8338281512260437, 0.9994246959686279, 0.2644367516040802, 0.04832770302891731, 0.3243183493614197, 0.9973340034484863, 0.9557006359100342, 0.37912026047706604, 0.010578015819191933, 0.28705358505249023, 0.0021365326829254627, 0.21061421930789948, 0.10595777630805969, 0.2478601634502411, 0.2752350866794586, 0.9997469782829285, 0.8510680198669434, 0.5429162979125977, 0.3563552796840668, 0.8085861206054688, 0.9998154044151306, 0.9510674476623535, 0.5614810585975647, 0.4532829225063324, 0.7673556208610535, 0.9998642206192017, 0.048938971012830734]], "normalized_score": [0.1827315390110016], "tokens": [["\u0120George", "\u0120Orwell", "\u0120/", "\u0120(", "no", "vel", "ist", "),", "\u0120George", "\u0120Orwell", "\u0120/", "\u0120Animal", "\u0120Farm", "\u0120(", "no", "vel", "),", "\u0120(", "no", "vel", "ist", "),", "\u0120Orwell", "\u0120(", "film", ")", "\u0120/", "\u0120(", "no", "vel", "ist", "),", "\u0120(", "no", "vel", "ist", "),", "\u0120(", "no", "vel", ")"]], "entropy": [[0.8093576431274414, 0.12883417308330536, 0.30003514885902405, 5.324749946594238, 5.066065311431885, 3.8073606491088867, 1.8710922002792358, 2.4549496173858643, 5.216204643249512, 4.6240692138671875, 1.8504765033721924, 0.8640422224998474, 0.18876883387565613, 2.140160322189331, 1.115800142288208, 0.005960758775472641, 1.2873363494873047, 3.687925100326538, 4.428886413574219, 0.03425832465291023, 0.30194419622421265, 1.2180520296096802, 4.131543159484863, 3.585230827331543, 2.6395022869110107, 2.4654417037963867, 1.213060736656189, 3.703664779663086, 3.2744879722595215, 0.0034494041465222836, 0.6572199463844299, 0.9237668514251709, 3.7490596771240234, 1.325009822845459, 0.002501383889466524, 0.2851921319961548, 0.8966064453125, 3.4452826976776123, 1.4275346994400024, 0.0018073050305247307, 0.40518486499786377]], "avg_entropy": [1.9722408887862068], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Geoge orwell", "score_of_biggest_cluster": 0.1827315390110016, "score_of_first_answer": 0.1827315390110016, "max_score_of_first_answer": 0.9998642206192017, "min_score_of_first_answer": 7.431673293467611e-05, "avg_score": 0.1827315390110016, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3694223820064379, "std_of_first_answer": 0.3694223820064379, "avg_entropy_of_first_answer": 1.9722408887862068, "lex_sim": 1.0}
{"question": "In the poem \u2018Monday\u2019s Child\u2019, what is Wednesday\u2019s child full of?", "answer": ["Courage"], "scores": [[0.007202593609690666]], "normalized_score": [0.007202591747045517], "tokens": [["\u0120Courage"]], "entropy": [[6.602150917053223]], "avg_entropy": [6.602150917053223], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Woe", "score_of_biggest_cluster": 0.007202591747045517, "score_of_first_answer": 0.007202591747045517, "max_score_of_first_answer": 0.007202593609690666, "min_score_of_first_answer": 0.007202593609690666, "avg_score": 0.007202591747045517, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.602150917053223, "lex_sim": 1.0}
{"question": "What cartoon character featured on a $100,000 auctioned poster in Texas, Nov 2012?", "answer": ["Astro the T-Rex"], "scores": [[0.00218488578684628, 0.013343296945095062, 0.006370374467223883, 0.4069999158382416, 0.3946095108985901]], "normalized_score": [0.03125530481338501], "tokens": [["\u0120Astro", "\u0120the", "\u0120T", "-", "Rex"]], "entropy": [[6.682387828826904, 3.7118349075317383, 3.952589988708496, 3.5167644023895264, 2.0278384685516357]], "avg_entropy": [3.9782831192016603], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mickey Mouse Operation", "score_of_biggest_cluster": 0.03125530481338501, "score_of_first_answer": 0.03125530481338501, "max_score_of_first_answer": 0.4069999158382416, "min_score_of_first_answer": 0.00218488578684628, "avg_score": 0.03125530481338501, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19285015770427227, "std_of_first_answer": 0.19285015770427227, "avg_entropy_of_first_answer": 3.9782831192016603, "lex_sim": 1.0}
{"question": "Who was known as 'The Great One' in North American ice hockey?", "answer": ["Bobby Orr"], "scores": [[0.08998580276966095, 0.7082138061523438, 0.9999216794967651]], "normalized_score": [0.39942455291748047], "tokens": [["\u0120Bobby", "\u0120Or", "r"]], "entropy": [[3.774470329284668, 1.0465054512023926, 0.0009690744918771088]], "avg_entropy": [1.6073149516596459], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gretzky's Office", "score_of_biggest_cluster": 0.39942455291748047, "score_of_first_answer": 0.39942455291748047, "max_score_of_first_answer": 0.9999216794967651, "min_score_of_first_answer": 0.08998580276966095, "avg_score": 0.39942455291748047, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3793682829964084, "std_of_first_answer": 0.3793682829964084, "avg_entropy_of_first_answer": 1.6073149516596459, "lex_sim": 1.0}
{"question": "What relation is Joey Richardson to Vanessa Redgrave?", "answer": ["Half-sister"], "scores": [[0.017814239487051964, 0.7920732498168945, 0.4148467481136322, 0.9618549346923828]], "normalized_score": [0.2739255130290985], "tokens": [["\u0120Half", "-", "s", "ister"]], "entropy": [[5.020188808441162, 1.0849058628082275, 1.2510316371917725, 0.19207032024860382]], "avg_entropy": [1.8870491571724415], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Daughterhood", "score_of_biggest_cluster": 0.2739255130290985, "score_of_first_answer": 0.2739255130290985, "max_score_of_first_answer": 0.9618549346923828, "min_score_of_first_answer": 0.017814239487051964, "avg_score": 0.2739255130290985, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3638911472265471, "std_of_first_answer": 0.3638911472265471, "avg_entropy_of_first_answer": 1.8870491571724415, "lex_sim": 1.0}
{"question": "Which film of 1988 sees the lead male character, named Dan Gallagher, terrorised by an obsessed woman named Alex Forest, an editor for a publishing company?", "answer": ["Unfriended"], "scores": [[0.0041547599248588085, 0.01130529772490263, 0.6313956379890442]], "normalized_score": [0.030953504145145416], "tokens": [["\u0120Un", "friend", "ed"]], "entropy": [[5.934440612792969, 1.2315442562103271, 1.3819396495819092]], "avg_entropy": [2.849308172861735], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fatal attraction (interpersonal communication)", "score_of_biggest_cluster": 0.030953504145145416, "score_of_first_answer": 0.030953504145145416, "max_score_of_first_answer": 0.6313956379890442, "min_score_of_first_answer": 0.0041547599248588085, "avg_score": 0.030953504145145416, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.294013280008925, "std_of_first_answer": 0.294013280008925, "avg_entropy_of_first_answer": 2.849308172861735, "lex_sim": 1.0}
{"question": "What was once the most abundant bird in North America but was driven to extinction in 1914?", "answer": ["Passenger Pigeon"], "scores": [[0.3969622254371643, 0.3616807460784912, 0.8643917441368103, 0.9466827511787415]], "normalized_score": [0.5854602456092834], "tokens": [["\u0120Passenger", "\u0120P", "ige", "on"]], "entropy": [[2.8012585639953613, 1.6246514320373535, 0.6760046482086182, 0.2177347093820572]], "avg_entropy": [1.3299123384058475], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Columba canadensis", "score_of_biggest_cluster": 0.5854602456092834, "score_of_first_answer": 0.5854602456092834, "max_score_of_first_answer": 0.9466827511787415, "min_score_of_first_answer": 0.3616807460784912, "avg_score": 0.5854602456092834, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26500534163710676, "std_of_first_answer": 0.26500534163710676, "avg_entropy_of_first_answer": 1.3299123384058475, "lex_sim": 1.0}
{"question": "In World War II, which company manufactured the 'Beaufighter'?", "answer": ["Beaufighter"], "scores": [[0.14495687186717987, 0.5931433439254761]], "normalized_score": [0.2932238280773163], "tokens": [["\u0120Beau", "fighter"]], "entropy": [[4.4984130859375, 1.3988730907440186]], "avg_entropy": [2.9486430883407593], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bristol (England)", "score_of_biggest_cluster": 0.2932238280773163, "score_of_first_answer": 0.2932238280773163, "max_score_of_first_answer": 0.5931433439254761, "min_score_of_first_answer": 0.14495687186717987, "avg_score": 0.2932238280773163, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2240932360291481, "std_of_first_answer": 0.2240932360291481, "avg_entropy_of_first_answer": 2.9486430883407593, "lex_sim": 1.0}
{"question": "Which Spaniard, from 1518, overcame Montezuma, captured Mexico City and became Governor-General of much of Mexico?", "answer": ["Hernan Cortes"], "scores": [[0.6867822408676147, 0.7393124103546143, 0.8542929291725159, 0.9962066411972046, 0.5845156311988831]], "normalized_score": [0.7594162821769714], "tokens": [["\u0120H", "ern", "an", "\u0120Cort", "es"]], "entropy": [[1.5775206089019775, 0.5919328331947327, 0.8807775974273682, 0.042577121406793594, 0.9701669216156006]], "avg_entropy": [0.8125950165092946], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hern\u00e1n Cort\u00e9z", "score_of_biggest_cluster": 0.7594162821769714, "score_of_first_answer": 0.7594162821769714, "max_score_of_first_answer": 0.9962066411972046, "min_score_of_first_answer": 0.5845156311988831, "avg_score": 0.7594162821769714, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.14178958447446333, "std_of_first_answer": 0.14178958447446333, "avg_entropy_of_first_answer": 0.8125950165092946, "lex_sim": 1.0}
{"question": "What is the name of the sport in which your opponent tries to knock you off a floating spinning log", "answer": ["Logjams"], "scores": [[0.019687267020344734, 0.008891458623111248, 0.31761306524276733]], "normalized_score": [0.03816678002476692], "tokens": [["\u0120Log", "j", "ams"]], "entropy": [[6.463780879974365, 3.533573627471924, 2.9557628631591797]], "avg_entropy": [4.317705790201823], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Birling", "score_of_biggest_cluster": 0.03816678002476692, "score_of_first_answer": 0.03816678002476692, "max_score_of_first_answer": 0.31761306524276733, "min_score_of_first_answer": 0.008891458623111248, "avg_score": 0.03816678002476692, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14305607335712772, "std_of_first_answer": 0.14305607335712772, "avg_entropy_of_first_answer": 4.317705790201823, "lex_sim": 1.0}
{"question": "What is the common word for the monosaccharide substance found in certain fruit, flower nectar and honey, with the chemical formula C6H12O6?", "answer": ["Glucose"], "scores": [[0.5054909586906433, 0.995016872882843, 0.9879872798919678]], "normalized_score": [0.7920727729797363], "tokens": [["\u0120Gl", "uc", "ose"]], "entropy": [[2.7053377628326416, 0.04480691999197006, 0.09643875807523727]], "avg_entropy": [0.9488611469666163], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ATCvet code QV06DC02", "score_of_biggest_cluster": 0.7920727729797363, "score_of_first_answer": 0.7920727729797363, "max_score_of_first_answer": 0.995016872882843, "min_score_of_first_answer": 0.5054909586906433, "avg_score": 0.7920727729797363, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.229125811101465, "std_of_first_answer": 0.229125811101465, "avg_entropy_of_first_answer": 0.9488611469666163, "lex_sim": 1.0}
{"question": "The English cities of Gloucester and Worcester are on which river?", "answer": ["Darl Brook"], "scores": [[0.0008772899745963514, 0.004157216753810644, 0.02399265766143799]], "normalized_score": [0.004439574666321278], "tokens": [["\u0120D", "arl", "\u0120Brook"]], "entropy": [[1.7673476934432983, 5.29121208190918, 4.602969169616699]], "avg_entropy": [3.8871763149897256], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Seven Boar", "score_of_biggest_cluster": 0.004439574666321278, "score_of_first_answer": 0.004439574666321278, "max_score_of_first_answer": 0.02399265766143799, "min_score_of_first_answer": 0.0008772899745963514, "avg_score": 0.004439574666321278, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.010211773482792985, "std_of_first_answer": 0.010211773482792985, "avg_entropy_of_first_answer": 3.8871763149897256, "lex_sim": 1.0}
{"question": "Who wrote the 1993 novel \u201cThe Joy Luck Club\u201d?", "answer": ["Amy Tan"], "scores": [[0.9026859998703003, 0.9972280859947205]], "normalized_score": [0.9487801790237427], "tokens": [["\u0120Amy", "\u0120Tan"]], "entropy": [[0.803301215171814, 0.03236638754606247]], "avg_entropy": [0.4178338013589382], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tan En-mei", "score_of_biggest_cluster": 0.9487801790237427, "score_of_first_answer": 0.9487801790237427, "max_score_of_first_answer": 0.9972280859947205, "min_score_of_first_answer": 0.9026859998703003, "avg_score": 0.9487801790237427, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04727104306221008, "std_of_first_answer": 0.04727104306221008, "avg_entropy_of_first_answer": 0.4178338013589382, "lex_sim": 1.0}
{"question": "Center, Offensive guard, Offensive tackle, Tight end, Wide receiver, Fullback, Running back, Quarterback, Defensive end, Defensive tackle, Nose guard, Linebacker, Cornerback, Safety, Nickelback and Dimeback are positions in which sport?", "answer": ["American football"], "scores": [[0.22982901334762573, 0.8228550553321838]], "normalized_score": [0.43487465381622314], "tokens": [["\u0120American", "\u0120football"]], "entropy": [[3.4703636169433594, 0.7287102937698364]], "avg_entropy": [2.099536955356598], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "U.s. football", "score_of_biggest_cluster": 0.43487465381622314, "score_of_first_answer": 0.43487465381622314, "max_score_of_first_answer": 0.8228550553321838, "min_score_of_first_answer": 0.22982901334762573, "avg_score": 0.43487465381622314, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.29651302099227905, "std_of_first_answer": 0.29651302099227905, "avg_entropy_of_first_answer": 2.099536955356598, "lex_sim": 1.0}
{"question": "In the cartoon series, what kind of dog is Scooby Doo?", "answer": ["Scrappy Doo"], "scores": [[0.0006241346709430218, 0.542883574962616, 0.9445145726203918, 0.15633432567119598, 0.9850031733512878]], "normalized_score": [0.13757425546646118], "tokens": [["\u0120Sc", "ra", "ppy", "\u0120Do", "o"]], "entropy": [[4.184989929199219, 2.878429412841797, 0.3964879512786865, 2.223665714263916, 0.13392648100852966]], "avg_entropy": [1.9634998977184295], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Great daine", "score_of_biggest_cluster": 0.13757425546646118, "score_of_first_answer": 0.13757425546646118, "max_score_of_first_answer": 0.9850031733512878, "min_score_of_first_answer": 0.0006241346709430218, "avg_score": 0.13757425546646118, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39969884773837466, "std_of_first_answer": 0.39969884773837466, "avg_entropy_of_first_answer": 1.9634998977184295, "lex_sim": 1.0}
{"question": "What's the food 'head cheese' alternatively called, which separately refers to a man's physical strength?", "answer": ["Head cheese"], "scores": [[0.010536381043493748, 0.030581634491682053]], "normalized_score": [0.017950475215911865], "tokens": [["\u0120Head", "\u0120cheese"]], "entropy": [[6.8858795166015625, 3.859445571899414]], "avg_entropy": [5.372662544250488], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Souses", "score_of_biggest_cluster": 0.017950475215911865, "score_of_first_answer": 0.017950475215911865, "max_score_of_first_answer": 0.030581634491682053, "min_score_of_first_answer": 0.010536381043493748, "avg_score": 0.017950475215911865, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.010022626724094152, "std_of_first_answer": 0.010022626724094152, "avg_entropy_of_first_answer": 5.372662544250488, "lex_sim": 1.0}
{"question": "What is the largest city and capital of Madeira?", "answer": ["Funchal"], "scores": [[0.6397547125816345, 0.9991274476051331, 0.9942488670349121]], "normalized_score": [0.8597585558891296], "tokens": [["\u0120F", "unch", "al"]], "entropy": [[1.9327715635299683, 0.009673790074884892, 0.062125205993652344]], "avg_entropy": [0.6681901865328351], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Funchal", "score_of_biggest_cluster": 0.8597585558891296, "score_of_first_answer": 0.8597585558891296, "max_score_of_first_answer": 0.9991274476051331, "min_score_of_first_answer": 0.6397547125816345, "avg_score": 0.8597585558891296, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.16827182664044046, "std_of_first_answer": 0.16827182664044046, "avg_entropy_of_first_answer": 0.6681901865328351, "lex_sim": 1.0}
{"question": "In what modern-day country was the Norse settlement of Vinland?", "answer": ["Vinland"], "scores": [[0.008546318858861923, 0.9019764065742493]], "normalized_score": [0.08779849857091904], "tokens": [["\u0120Vin", "land"]], "entropy": [[3.949633836746216, 0.642955482006073]], "avg_entropy": [2.2962946593761444], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Canada", "score_of_biggest_cluster": 0.08779849857091904, "score_of_first_answer": 0.08779849857091904, "max_score_of_first_answer": 0.9019764065742493, "min_score_of_first_answer": 0.008546318858861923, "avg_score": 0.08779849857091904, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44671504385769367, "std_of_first_answer": 0.44671504385769367, "avg_entropy_of_first_answer": 2.2962946593761444, "lex_sim": 1.0}
{"question": "A fictional species of great apes called 'Mangani' are responsible for raising which popular character?", "answer": ["A baby"], "scores": [[0.003919226583093405, 0.002296876860782504]], "normalized_score": [0.0030003301799297333], "tokens": [["\u0120A", "\u0120baby"]], "entropy": [[6.636804580688477, 5.313218116760254]], "avg_entropy": [5.975011348724365], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lord Greystoke", "score_of_biggest_cluster": 0.0030003301799297333, "score_of_first_answer": 0.0030003301799297333, "max_score_of_first_answer": 0.003919226583093405, "min_score_of_first_answer": 0.002296876860782504, "avg_score": 0.0030003301799297333, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0008111748611554503, "std_of_first_answer": 0.0008111748611554503, "avg_entropy_of_first_answer": 5.975011348724365, "lex_sim": 1.0}
{"question": "What is the name of a calculating machine that originally consisted of beans or stones moved in grooves in sand or on tablets of wood, stone, or metal?", "answer": ["A mechanical abacus"], "scores": [[0.015169086866080761, 0.06624506413936615, 0.11997430771589279, 0.9994575381278992]], "normalized_score": [0.10477106273174286], "tokens": [["\u0120A", "\u0120mechanical", "\u0120ab", "acus"]], "entropy": [[4.896122932434082, 6.107304573059082, 3.449294090270996, 0.006127960979938507]], "avg_entropy": [3.6147123891860247], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Abacus Calculation", "score_of_biggest_cluster": 0.10477106273174286, "score_of_first_answer": 0.10477106273174286, "max_score_of_first_answer": 0.9994575381278992, "min_score_of_first_answer": 0.015169086866080761, "avg_score": 0.10477106273174286, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40540718349412536, "std_of_first_answer": 0.40540718349412536, "avg_entropy_of_first_answer": 3.6147123891860247, "lex_sim": 1.0}
{"question": "Who is the narrator on the UK television show The Only Way Is Essex?", "answer": ["Jack"], "scores": [[0.004705024883151054]], "normalized_score": [0.004705024883151054], "tokens": [["\u0120Jack"]], "entropy": [[6.404096603393555]], "avg_entropy": [6.404096603393555], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Denise van Outen", "score_of_biggest_cluster": 0.004705024883151054, "score_of_first_answer": 0.004705024883151054, "max_score_of_first_answer": 0.004705024883151054, "min_score_of_first_answer": 0.004705024883151054, "avg_score": 0.004705024883151054, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.404096603393555, "lex_sim": 1.0}
{"question": "The plot of which Arthur Miller play takes place entirely within the mind of a new York Jewish intellectual called 'Quentin'?", "answer": ["A View from the Bridge/Death of a Salesman"], "scores": [[0.02286417782306671, 0.8658899664878845, 0.5451003313064575, 0.986939549446106, 0.9578956365585327, 0.0003799486148636788, 0.4733797311782837, 0.9807741045951843, 0.9744805097579956, 0.9903439283370972, 0.9927769899368286]], "normalized_score": [0.2992652356624603], "tokens": [["\u0120A", "\u0120View", "\u0120from", "\u0120the", "\u0120Bridge", "/", "Death", "\u0120of", "\u0120a", "\u0120Sales", "man"]], "entropy": [[3.18709135055542, 1.0686465501785278, 1.0310815572738647, 0.11884480714797974, 0.384906530380249, 0.4323849081993103, 3.147606372833252, 0.13756364583969116, 0.19327032566070557, 0.0960298627614975, 0.055814556777477264]], "avg_entropy": [0.8957491334189068], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "After the Fall (album)", "score_of_biggest_cluster": 0.2992652356624603, "score_of_first_answer": 0.2992652356624603, "max_score_of_first_answer": 0.9927769899368286, "min_score_of_first_answer": 0.0003799486148636788, "avg_score": 0.2992652356624603, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37211159422541307, "std_of_first_answer": 0.37211159422541307, "avg_entropy_of_first_answer": 0.8957491334189068, "lex_sim": 1.0}
{"question": "\"What are the words called in crosswords that appear under the headings \"\"Across\"\" and \"\"Down\"\"?\"", "answer": ["Scented Cross"], "scores": [[0.0022264202125370502, 0.006340634543448687, 0.17162229120731354, 0.017090313136577606]], "normalized_score": [0.014264805242419243], "tokens": [["\u0120S", "cent", "ed", "\u0120Cross"]], "entropy": [[5.886220932006836, 5.943876266479492, 3.564014434814453, 4.888983726501465]], "avg_entropy": [5.0707738399505615], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Clue (disambiguation)", "score_of_biggest_cluster": 0.014264805242419243, "score_of_first_answer": 0.014264805242419243, "max_score_of_first_answer": 0.17162229120731354, "min_score_of_first_answer": 0.0022264202125370502, "avg_score": 0.014264805242419243, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07081954931741952, "std_of_first_answer": 0.07081954931741952, "avg_entropy_of_first_answer": 5.0707738399505615, "lex_sim": 1.0}
{"question": "Sinhala is the official language of which country?", "answer": ["Sri Lanka"], "scores": [[0.8341101408004761, 0.9586210250854492]], "normalized_score": [0.8942010402679443], "tokens": [["\u0120Sri", "\u0120Lanka"]], "entropy": [[1.1676454544067383, 0.26256173849105835]], "avg_entropy": [0.7151035964488983], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sri Lanka government", "score_of_biggest_cluster": 0.8942010402679443, "score_of_first_answer": 0.8942010402679443, "max_score_of_first_answer": 0.9586210250854492, "min_score_of_first_answer": 0.8341101408004761, "avg_score": 0.8942010402679443, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06225544214248657, "std_of_first_answer": 0.06225544214248657, "avg_entropy_of_first_answer": 0.7151035964488983, "lex_sim": 1.0}
{"question": "Which musical instrument can have 21, 22, or 23 strings?", "answer": ["The lautenwerke"], "scores": [[0.08818791806697845, 0.01987328566610813, 0.00037507337401621044, 0.40280842781066895, 0.24902670085430145, 0.5462067127227783]], "normalized_score": [0.05746667832136154], "tokens": [["\u0120The", "\u0120l", "aut", "en", "wer", "ke"]], "entropy": [[4.720924377441406, 4.531785011291504, 0.3799375295639038, 3.8050143718719482, 3.8296170234680176, 1.6918848752975464]], "avg_entropy": [3.159860531489054], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sitar", "score_of_biggest_cluster": 0.05746667832136154, "score_of_first_answer": 0.05746667832136154, "max_score_of_first_answer": 0.5462067127227783, "min_score_of_first_answer": 0.00037507337401621044, "avg_score": 0.05746667832136154, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20260883974174934, "std_of_first_answer": 0.20260883974174934, "avg_entropy_of_first_answer": 3.159860531489054, "lex_sim": 1.0}
{"question": "Who directed the 2014 film 'Jersey Boys'?", "answer": ["Brian Helgeland"], "scores": [[0.00784893985837698, 0.042880043387413025, 0.98093181848526, 0.9972991347312927]], "normalized_score": [0.13470466434955597], "tokens": [["\u0120Brian", "\u0120Hel", "g", "eland"]], "entropy": [[4.136728286743164, 4.628939628601074, 0.1624540090560913, 0.02658873423933983]], "avg_entropy": [2.2386776646599174], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Crint Eastwood", "score_of_biggest_cluster": 0.13470466434955597, "score_of_first_answer": 0.13470466434955597, "max_score_of_first_answer": 0.9972991347312927, "min_score_of_first_answer": 0.00784893985837698, "avg_score": 0.13470466434955597, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4820693660377292, "std_of_first_answer": 0.4820693660377292, "avg_entropy_of_first_answer": 2.2386776646599174, "lex_sim": 1.0}
{"question": "Who played Claude Greengrass in 'Heartbeat'?", "answer": ["John-Olivier Gachon"], "scores": [[0.02355967089533806, 0.0030289702117443085, 0.006162554956972599, 0.4879395663738251, 0.9993263483047485, 0.060887087136507034, 0.10168708860874176, 0.9681320786476135]], "normalized_score": [0.07737986743450165], "tokens": [["\u0120John", "-", "O", "liv", "ier", "\u0120G", "ach", "on"]], "entropy": [[6.489039421081543, 5.844738483428955, 3.4338440895080566, 2.8983664512634277, 0.007177814841270447, 5.398956298828125, 4.959672927856445, 0.2528730034828186]], "avg_entropy": [3.6605835612863302], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Walter Frederick George Williams", "score_of_biggest_cluster": 0.07737986743450165, "score_of_first_answer": 0.07737986743450165, "max_score_of_first_answer": 0.9993263483047485, "min_score_of_first_answer": 0.0030289702117443085, "avg_score": 0.07737986743450165, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40471839113031033, "std_of_first_answer": 0.40471839113031033, "avg_entropy_of_first_answer": 3.6605835612863302, "lex_sim": 1.0}
{"question": "Which company produces the 'Magentis' car model?", "answer": ["Magntis"], "scores": [[0.126519575715065, 0.009411986917257309, 0.9741326570510864]], "normalized_score": [0.10507167130708694], "tokens": [["\u0120Mag", "nt", "is"]], "entropy": [[5.395453453063965, 1.1704447269439697, 0.2357282042503357]], "avg_entropy": [2.2672087947527566], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "K. I. A.", "score_of_biggest_cluster": 0.10507167130708694, "score_of_first_answer": 0.10507167130708694, "max_score_of_first_answer": 0.9741326570510864, "min_score_of_first_answer": 0.009411986917257309, "avg_score": 0.10507167130708694, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4298382246573763, "std_of_first_answer": 0.4298382246573763, "avg_entropy_of_first_answer": 2.2672087947527566, "lex_sim": 1.0}
{"question": "Which part of the body can suffer from cirrhosis?", "answer": ["Liver \u34a1"], "scores": [[0.44927722215652466, 0.007006064057350159, 5.418323416961357e-05, 0.003984007053077221, 0.05742286145687103]], "normalized_score": [0.008284220471978188], "tokens": [["\u0120Liver", "\u0120", "\u00e3", "\u0134", "\u00a1"]], "entropy": [[2.556288003921509, 0.28997570276260376, 1.3494253158569336, 2.4962799549102783, 4.029208660125732]], "avg_entropy": [2.1442355275154115], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Impressio duodenalis", "score_of_biggest_cluster": 0.008284220471978188, "score_of_first_answer": 0.008284220471978188, "max_score_of_first_answer": 0.44927722215652466, "min_score_of_first_answer": 5.418323416961357e-05, "avg_score": 0.008284220471978188, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1741266939045612, "std_of_first_answer": 0.1741266939045612, "avg_entropy_of_first_answer": 2.1442355275154115, "lex_sim": 1.0}
{"question": "Suger Mice was a 1987 single for which British rock band?", "answer": ["The Who"], "scores": [[0.29434043169021606, 0.0023624550085514784]], "normalized_score": [0.026369797065854073], "tokens": [["\u0120The", "\u0120Who"]], "entropy": [[5.103635311126709, 5.360917568206787]], "avg_entropy": [5.232276439666748], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "In The Last Waking Moments", "score_of_biggest_cluster": 0.026369797065854073, "score_of_first_answer": 0.026369797065854073, "max_score_of_first_answer": 0.29434043169021606, "min_score_of_first_answer": 0.0023624550085514784, "avg_score": 0.026369797065854073, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1459889883408323, "std_of_first_answer": 0.1459889883408323, "avg_entropy_of_first_answer": 5.232276439666748, "lex_sim": 1.0}
{"question": "The Kalahari Desert lies chiefly in which country?", "answer": ["South Africa"], "scores": [[0.37143298983573914, 0.9563593864440918]], "normalized_score": [0.5960062146186829], "tokens": [["\u0120South", "\u0120Africa"]], "entropy": [[2.5633413791656494, 0.2767046093940735]], "avg_entropy": [1.4200229942798615], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Country BWA", "score_of_biggest_cluster": 0.5960062146186829, "score_of_first_answer": 0.5960062146186829, "max_score_of_first_answer": 0.9563593864440918, "min_score_of_first_answer": 0.37143298983573914, "avg_score": 0.5960062146186829, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29246319830417633, "std_of_first_answer": 0.29246319830417633, "avg_entropy_of_first_answer": 1.4200229942798615, "lex_sim": 1.0}
{"question": "The name of which musical instrument comes from the German for bells and play?", "answer": ["The Bell (instrument)"], "scores": [[0.19313529133796692, 0.004975324496626854, 0.005184403154999018, 0.03597811982035637, 0.9446333050727844, 0.9396851658821106]], "normalized_score": [0.07361127436161041], "tokens": [["\u0120The", "\u0120Bell", "\u0120(", "in", "strument", ")"]], "entropy": [[4.741865158081055, 5.340028762817383, 2.471219062805176, 6.031240940093994, 0.48354271054267883, 0.41930294036865234]], "avg_entropy": [3.247866595784823], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Glockenspiel", "score_of_biggest_cluster": 0.07361127436161041, "score_of_first_answer": 0.07361127436161041, "max_score_of_first_answer": 0.9446333050727844, "min_score_of_first_answer": 0.004975324496626854, "avg_score": 0.07361127436161041, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.420789080632436, "std_of_first_answer": 0.420789080632436, "avg_entropy_of_first_answer": 3.247866595784823, "lex_sim": 1.0}
{"question": "In the 'Disc World' novels, what is the name of the horse ridden by 'Death'?", "answer": ["Black Death"], "scores": [[0.022552363574504852, 0.02348443865776062]], "normalized_score": [0.023013683035969734], "tokens": [["\u0120Black", "\u0120Death"]], "entropy": [[6.864699363708496, 6.342240333557129]], "avg_entropy": [6.6034698486328125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Binky", "score_of_biggest_cluster": 0.023013683035969734, "score_of_first_answer": 0.023013683035969734, "max_score_of_first_answer": 0.02348443865776062, "min_score_of_first_answer": 0.022552363574504852, "avg_score": 0.023013683035969734, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0004660375416278839, "std_of_first_answer": 0.0004660375416278839, "avg_entropy_of_first_answer": 6.6034698486328125, "lex_sim": 1.0}
{"question": "The white wine, Chablis, originates from which wine producing region of France?", "answer": ["Beaujolais"], "scores": [[0.010735751129686832, 0.969430685043335, 0.9238542914390564, 0.9942626357078552]], "normalized_score": [0.3126896023750305], "tokens": [["\u0120Beau", "j", "ola", "is"]], "entropy": [[3.187816619873047, 0.23912262916564941, 0.44304412603378296, 0.042239341884851456]], "avg_entropy": [0.9780556792393327], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bourgogne (region)", "score_of_biggest_cluster": 0.3126896023750305, "score_of_first_answer": 0.3126896023750305, "max_score_of_first_answer": 0.9942626357078552, "min_score_of_first_answer": 0.010735751129686832, "avg_score": 0.3126896023750305, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4129056903539609, "std_of_first_answer": 0.4129056903539609, "avg_entropy_of_first_answer": 0.9780556792393327, "lex_sim": 1.0}
{"question": "Who was captain of the US team for the 2014 Ryder Cup?", "answer": ["Captain Tom Watson"], "scores": [[0.00510796345770359, 0.17613452672958374, 0.867409348487854]], "normalized_score": [0.09206729382276535], "tokens": [["\u0120Captain", "\u0120Tom", "\u0120Watson"]], "entropy": [[4.822155475616455, 4.431290626525879, 0.8645392656326294]], "avg_entropy": [3.3726617892583213], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tommy Watson", "score_of_biggest_cluster": 0.09206729382276535, "score_of_first_answer": 0.09206729382276535, "max_score_of_first_answer": 0.867409348487854, "min_score_of_first_answer": 0.00510796345770359, "avg_score": 0.09206729382276535, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3727785525492513, "std_of_first_answer": 0.3727785525492513, "avg_entropy_of_first_answer": 3.3726617892583213, "lex_sim": 1.0}
{"question": "Which home of champagne in France was also where the German High Command surrendered in WWII?", "answer": ["Reims"], "scores": [[0.06554462015628815, 0.9979845285415649]], "normalized_score": [0.25575870275497437], "tokens": [["\u0120Re", "ims"]], "entropy": [[5.020132064819336, 0.019560961052775383]], "avg_entropy": [2.5198465129360557], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Reims", "score_of_biggest_cluster": 0.25575870275497437, "score_of_first_answer": 0.25575870275497437, "max_score_of_first_answer": 0.9979845285415649, "min_score_of_first_answer": 0.06554462015628815, "avg_score": 0.25575870275497437, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4662199541926384, "std_of_first_answer": 0.4662199541926384, "avg_entropy_of_first_answer": 2.5198465129360557, "lex_sim": 1.0}
{"question": "Who was the seventh president of the United States?", "answer": ["Ulysses S Grant"], "scores": [[0.018012760207057, 0.9129517674446106, 0.9959239959716797, 0.8696632385253906, 0.2890697419643402]], "normalized_score": [0.3333662152290344], "tokens": [["\u0120U", "lyss", "es", "\u0120S", "\u0120Grant"]], "entropy": [[4.309227466583252, 0.6603575944900513, 0.03929685056209564, 0.6904276013374329, 1.3545429706573486]], "avg_entropy": [1.4107704967260362], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Seventh President of the United States", "score_of_biggest_cluster": 0.3333662152290344, "score_of_first_answer": 0.3333662152290344, "max_score_of_first_answer": 0.9959239959716797, "min_score_of_first_answer": 0.018012760207057, "avg_score": 0.3333662152290344, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3902136455316506, "std_of_first_answer": 0.3902136455316506, "avg_entropy_of_first_answer": 1.4107704967260362, "lex_sim": 1.0}
{"question": "Thomas Edison, who invented the lightbulb, was afraid of the dark?", "answer": ["Edison was afraid of the dark only at night."], "scores": [[0.005300347693264484, 0.16190257668495178, 0.570993959903717, 0.9024525880813599, 0.22855675220489502, 0.6976855397224426, 0.0034079086035490036, 0.05014370009303093, 0.8951980471611023, 0.33699095249176025]], "normalized_score": [0.14323557913303375], "tokens": [["\u0120Edison", "\u0120was", "\u0120afraid", "\u0120of", "\u0120the", "\u0120dark", "\u0120only", "\u0120at", "\u0120night", "."]], "entropy": [[3.730070114135742, 3.346824884414673, 2.5424251556396484, 0.6164146065711975, 4.6749587059021, 2.435760736465454, 2.6870832443237305, 2.9496188163757324, 0.7292718887329102, 1.8501012325286865]], "avg_entropy": [2.5562529385089876], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "True (disambiguation)", "score_of_biggest_cluster": 0.14323557913303375, "score_of_first_answer": 0.14323557913303375, "max_score_of_first_answer": 0.9024525880813599, "min_score_of_first_answer": 0.0034079086035490036, "avg_score": 0.14323557913303375, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3374743613448896, "std_of_first_answer": 0.3374743613448896, "avg_entropy_of_first_answer": 2.5562529385089876, "lex_sim": 1.0}
{"question": "Which archipelago in the North Sea belonged to Britain until 1990 when ownership passed to Germany?", "answer": ["\u32c0"], "scores": [[0.0018050415674224496, 9.066672646440566e-06, 0.020035896450281143, 0.028701238334178925]], "normalized_score": [0.0017515039071440697], "tokens": [["\u0120", "\u00e3", "\u012d", "\u0122"]], "entropy": [[4.4410319328308105, 4.0791473388671875, 2.56453013420105, 3.828132390975952]], "avg_entropy": [3.72821044921875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "HELIGOLAND", "score_of_biggest_cluster": 0.0017515039071440697, "score_of_first_answer": 0.0017515039071440697, "max_score_of_first_answer": 0.028701238334178925, "min_score_of_first_answer": 9.066672646440566e-06, "avg_score": 0.0017515039071440697, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.012140834432969991, "std_of_first_answer": 0.012140834432969991, "avg_entropy_of_first_answer": 3.72821044921875, "lex_sim": 0.0}
{"question": "Name the four terrestrial planets of the Solar System?", "answer": ["Mars, Jupiter, Saturn and Earth"], "scores": [[0.11500879377126694, 0.9241015911102295, 0.5240369439125061, 0.9930382370948792, 0.7743900418281555, 0.13491296768188477, 0.16271333396434784]], "normalized_score": [0.3694895803928375], "tokens": [["\u0120Mars", ",", "\u0120Jupiter", ",", "\u0120Saturn", "\u0120and", "\u0120Earth"]], "entropy": [[2.57112455368042, 0.5100516080856323, 1.6920748949050903, 0.06521955132484436, 1.049771785736084, 0.5323695540428162, 1.8076051473617554]], "avg_entropy": [1.1754595850195204], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mercury, Venus, Earth, Mars", "score_of_biggest_cluster": 0.3694895803928375, "score_of_first_answer": 0.3694895803928375, "max_score_of_first_answer": 0.9930382370948792, "min_score_of_first_answer": 0.11500879377126694, "avg_score": 0.3694895803928375, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3569188856041528, "std_of_first_answer": 0.3569188856041528, "avg_entropy_of_first_answer": 1.1754595850195204, "lex_sim": 1.0}
{"question": "Who had parts in \u2018St. Elmo\u2019s Fire\u2019, \u2018About Last Night \u2026\u2019 and \u2018Wayne\u2019s World\u2019?", "answer": ["Rob Reiner"], "scores": [[0.01883273385465145, 0.12517380714416504, 0.9971587657928467]], "normalized_score": [0.13296286761760712], "tokens": [["\u0120Rob", "\u0120Re", "iner"]], "entropy": [[5.842132568359375, 1.6845006942749023, 0.028970081359148026]], "avg_entropy": [2.5185344479978085], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rob Lowe", "score_of_biggest_cluster": 0.13296286761760712, "score_of_first_answer": 0.13296286761760712, "max_score_of_first_answer": 0.9971587657928467, "min_score_of_first_answer": 0.01883273385465145, "avg_score": 0.13296286761760712, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43827794539105525, "std_of_first_answer": 0.43827794539105525, "avg_entropy_of_first_answer": 2.5185344479978085, "lex_sim": 1.0}
{"question": "Until 1796, there was a state in the United States called Franklin. Today it's known as what?", "answer": ["Vermont"], "scores": [[0.12432767450809479]], "normalized_score": [0.12432768195867538], "tokens": [["\u0120Vermont"]], "entropy": [[4.956234931945801]], "avg_entropy": [4.956234931945801], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tennessee!", "score_of_biggest_cluster": 0.12432768195867538, "score_of_first_answer": 0.12432768195867538, "max_score_of_first_answer": 0.12432767450809479, "min_score_of_first_answer": 0.12432767450809479, "avg_score": 0.12432768195867538, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.956234931945801, "lex_sim": 1.0}
{"question": "In a 1763 swap, what expanse of land did Britain gain from Spain in exchange for Havana?", "answer": ["Caribbean"], "scores": [[0.0024414125364273787]], "normalized_score": [0.0024414120707660913], "tokens": [["\u0120Caribbean"]], "entropy": [[5.192652702331543]], "avg_entropy": [5.192652702331543], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rear View (song)", "score_of_biggest_cluster": 0.0024414120707660913, "score_of_first_answer": 0.0024414120707660913, "max_score_of_first_answer": 0.0024414125364273787, "min_score_of_first_answer": 0.0024414125364273787, "avg_score": 0.0024414120707660913, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.192652702331543, "lex_sim": 1.0}
{"question": "Which English king was born in Pembroke Castle in West Wales in 1457?", "answer": ["Charles II"], "scores": [[0.009218749590218067, 0.1530326008796692]], "normalized_score": [0.03756020963191986], "tokens": [["\u0120Charles", "\u0120II"]], "entropy": [[2.718705654144287, 2.5306053161621094]], "avg_entropy": [2.6246554851531982], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Solomon of England", "score_of_biggest_cluster": 0.03756020963191986, "score_of_first_answer": 0.03756020963191986, "max_score_of_first_answer": 0.1530326008796692, "min_score_of_first_answer": 0.009218749590218067, "avg_score": 0.03756020963191986, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07190692564472556, "std_of_first_answer": 0.07190692564472556, "avg_entropy_of_first_answer": 2.6246554851531982, "lex_sim": 1.0}
{"question": "What food staple is the focus of a 1980 documentary ___ Is as Good as Ten Mothers that was filmed primarily in Gilroy, California?", "answer": ["Fungus"], "scores": [[0.003602426266297698, 0.0253860205411911, 0.6125967502593994]], "normalized_score": [0.038263797760009766], "tokens": [["\u0120F", "ung", "us"]], "entropy": [[5.83712100982666, 4.264423847198486, 1.5133674144744873]], "avg_entropy": [3.8716374238332114], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Garlic scape", "score_of_biggest_cluster": 0.038263797760009766, "score_of_first_answer": 0.038263797760009766, "max_score_of_first_answer": 0.6125967502593994, "min_score_of_first_answer": 0.003602426266297698, "avg_score": 0.038263797760009766, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2820884519932511, "std_of_first_answer": 0.2820884519932511, "avg_entropy_of_first_answer": 3.8716374238332114, "lex_sim": 1.0}
{"question": "To whom did human-rights lawyer Amal Alamuddin become engaged in April 2014?", "answer": ["Ms. Ali"], "scores": [[0.004030399490147829, 0.7339892983436584, 0.0034758271649479866]], "normalized_score": [0.02174529805779457], "tokens": [["\u0120Ms", ".", "\u0120Ali"]], "entropy": [[4.862748622894287, 2.276482582092285, 6.225555419921875]], "avg_entropy": [4.454928874969482], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Max (pig)", "score_of_biggest_cluster": 0.02174529805779457, "score_of_first_answer": 0.02174529805779457, "max_score_of_first_answer": 0.7339892983436584, "min_score_of_first_answer": 0.0034758271649479866, "avg_score": 0.02174529805779457, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3442367133140705, "std_of_first_answer": 0.3442367133140705, "avg_entropy_of_first_answer": 4.454928874969482, "lex_sim": 1.0}
{"question": "In 2012, Bradford City FC presented the Dalai Lama with a personalised club shirt with which number on the back?", "answer": ["11"], "scores": [[0.00929500162601471]], "normalized_score": [0.00929500162601471], "tokens": [["\u012011"]], "entropy": [[5.732369422912598]], "avg_entropy": [5.732369422912598], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "fourteen", "score_of_biggest_cluster": 0.00929500162601471, "score_of_first_answer": 0.00929500162601471, "max_score_of_first_answer": 0.00929500162601471, "min_score_of_first_answer": 0.00929500162601471, "avg_score": 0.00929500162601471, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.732369422912598, "lex_sim": 1.0}
{"question": "Which letter of the Greek alphabet when written looks like a capital P ?", "answer": ["\u03a6"], "scores": [[0.024164268746972084, 0.04441132768988609]], "normalized_score": [0.032759230583906174], "tokens": [["\u0120\u00ce", "\u00a6"]], "entropy": [[3.9921035766601562, 1.8881616592407227]], "avg_entropy": [2.9401326179504395], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u03a1", "score_of_biggest_cluster": 0.032759230583906174, "score_of_first_answer": 0.032759230583906174, "max_score_of_first_answer": 0.04441132768988609, "min_score_of_first_answer": 0.024164268746972084, "avg_score": 0.032759230583906174, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.010123529471457005, "std_of_first_answer": 0.010123529471457005, "avg_entropy_of_first_answer": 2.9401326179504395, "lex_sim": 0.0}
{"question": "Who directed the 1975 film 'French Connection 2'?", "answer": ["Billy Wilder"], "scores": [[0.017653614282608032, 0.34537366032600403, 0.9933945536613464]], "normalized_score": [0.18228383362293243], "tokens": [["\u0120Billy", "\u0120Wild", "er"]], "entropy": [[4.545246601104736, 2.6218690872192383, 0.054425716400146484]], "avg_entropy": [2.4071804682413735], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Rainmaker (1982)", "score_of_biggest_cluster": 0.18228383362293243, "score_of_first_answer": 0.18228383362293243, "max_score_of_first_answer": 0.9933945536613464, "min_score_of_first_answer": 0.017653614282608032, "avg_score": 0.18228383362293243, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4054355528396486, "std_of_first_answer": 0.4054355528396486, "avg_entropy_of_first_answer": 2.4071804682413735, "lex_sim": 1.0}
{"question": "What is the next in the series: Nigeria, Sierra Leone, Tanganyika, Uganda, Zanzibar, Kenya, Malawi, Zambia, Gambia, Botswana, Lesotho, Mauritius", "answer": ["Kenya, Lesotho, Mauritius"], "scores": [[0.010747437365353107, 0.4048249423503876, 0.09151455760002136, 0.9971725940704346, 0.9948741793632507, 0.883894145488739, 0.4420769512653351, 0.9978755712509155]], "normalized_score": [0.33376970887184143], "tokens": [["\u0120Kenya", ",", "\u0120Les", "oth", "o", ",", "\u0120Maurit", "ius"]], "entropy": [[5.45999813079834, 1.1486310958862305, 3.3997011184692383, 0.030104011297225952, 0.054369259625673294, 0.48230481147766113, 2.196176767349243, 0.023963946849107742]], "avg_entropy": [1.59940614271909], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kingdom of Swaziland", "score_of_biggest_cluster": 0.33376970887184143, "score_of_first_answer": 0.33376970887184143, "max_score_of_first_answer": 0.9978755712509155, "min_score_of_first_answer": 0.010747437365353107, "avg_score": 0.33376970887184143, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3907229758693942, "std_of_first_answer": 0.3907229758693942, "avg_entropy_of_first_answer": 1.59940614271909, "lex_sim": 1.0}
{"question": "Which Cole Porter play won the first Tony Award for best musical in 1949?", "answer": ["You Are the One, That's All"], "scores": [[0.004681840538978577, 0.006252472288906574, 0.0905083641409874, 0.605475664138794, 0.020516060292720795, 0.004031323827803135, 0.4262317419052124, 0.46824556589126587]], "normalized_score": [0.047628216445446014], "tokens": [["\u0120You", "\u0120Are", "\u0120the", "\u0120One", ",", "\u0120That", "'s", "\u0120All"]], "entropy": [[4.233677864074707, 2.2427685260772705, 3.6339938640594482, 2.4992644786834717, 1.8697233200073242, 3.454130172729492, 1.9618488550186157, 2.526926279067993]], "avg_entropy": [2.8027916699647903], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kiss Me Kate", "score_of_biggest_cluster": 0.047628216445446014, "score_of_first_answer": 0.047628216445446014, "max_score_of_first_answer": 0.605475664138794, "min_score_of_first_answer": 0.004031323827803135, "avg_score": 0.047628216445446014, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23604998984332942, "std_of_first_answer": 0.23604998984332942, "avg_entropy_of_first_answer": 2.8027916699647903, "lex_sim": 1.0}
{"question": "Which is the most southerly country of the EU?", "answer": ["Malta"], "scores": [[0.10744677484035492]], "normalized_score": [0.10744678229093552], "tokens": [["\u0120Malta"]], "entropy": [[3.862147092819214]], "avg_entropy": [3.862147092819214], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Health in Malta", "score_of_biggest_cluster": 0.10744678229093552, "score_of_first_answer": 0.10744678229093552, "max_score_of_first_answer": 0.10744677484035492, "min_score_of_first_answer": 0.10744677484035492, "avg_score": 0.10744678229093552, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.862147092819214, "lex_sim": 1.0}
{"question": "John F Kennedy suffered from chronic back complaints and what other medical problem?", "answer": ["A stammer"], "scores": [[0.024260487407445908, 0.012413090094923973, 0.3394434154033661, 0.994724690914154]], "normalized_score": [0.10041819512844086], "tokens": [["\u0120A", "\u0120st", "am", "mer"]], "entropy": [[6.340457916259766, 6.387132167816162, 2.443343162536621, 0.062262583523988724]], "avg_entropy": [3.8082989575341344], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Autoimmune adrenalitis", "score_of_biggest_cluster": 0.10041819512844086, "score_of_first_answer": 0.10041819512844086, "max_score_of_first_answer": 0.994724690914154, "min_score_of_first_answer": 0.012413090094923973, "avg_score": 0.10041819512844086, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39863516167837454, "std_of_first_answer": 0.39863516167837454, "avg_entropy_of_first_answer": 3.8082989575341344, "lex_sim": 1.0}
{"question": "Who was Bridget Jones's true love?", "answer": ["Mark Darcy"], "scores": [[0.40107113122940063, 0.9889154434204102, 0.9964041709899902]], "normalized_score": [0.7338467240333557], "tokens": [["\u0120Mark", "\u0120Dar", "cy"]], "entropy": [[3.9882969856262207, 0.12245303392410278, 0.03730509802699089]], "avg_entropy": [1.3826850391924381], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mark Darcy", "score_of_biggest_cluster": 0.7338467240333557, "score_of_first_answer": 0.7338467240333557, "max_score_of_first_answer": 0.9964041709899902, "min_score_of_first_answer": 0.40107113122940063, "avg_score": 0.7338467240333557, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.278894333763109, "std_of_first_answer": 0.278894333763109, "avg_entropy_of_first_answer": 1.3826850391924381, "lex_sim": 1.0}
{"question": "In the Old Testament Goliath was a member of which race?", "answer": ["Hebrews"], "scores": [[0.13037003576755524, 0.3458736836910248]], "normalized_score": [0.21234774589538574], "tokens": [["\u0120Hebrew", "s"]], "entropy": [[3.0097169876098633, 1.0078094005584717]], "avg_entropy": [2.0087631940841675], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Philist\u00e9an", "score_of_biggest_cluster": 0.21234774589538574, "score_of_first_answer": 0.21234774589538574, "max_score_of_first_answer": 0.3458736836910248, "min_score_of_first_answer": 0.13037003576755524, "avg_score": 0.21234774589538574, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10775182396173477, "std_of_first_answer": 0.10775182396173477, "avg_entropy_of_first_answer": 2.0087631940841675, "lex_sim": 1.0}
{"question": "Who lead the enquiry into the Iraq war?", "answer": ["John Chilcot"], "scores": [[0.1482447385787964, 0.9482137560844421, 0.9596739411354065, 0.7141678333282471]], "normalized_score": [0.557124674320221], "tokens": [["\u0120John", "\u0120Ch", "il", "cot"]], "entropy": [[4.148196220397949, 0.4792117476463318, 0.3114668130874634, 0.8410423994064331]], "avg_entropy": [1.4449792951345444], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Chilcot", "score_of_biggest_cluster": 0.557124674320221, "score_of_first_answer": 0.557124674320221, "max_score_of_first_answer": 0.9596739411354065, "min_score_of_first_answer": 0.1482447385787964, "avg_score": 0.557124674320221, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.32918638410407236, "std_of_first_answer": 0.32918638410407236, "avg_entropy_of_first_answer": 1.4449792951345444, "lex_sim": 1.0}
{"question": "What word links take, write and stand", "answer": ["Take Write Stand"], "scores": [[0.01325851958245039, 0.005002514459192753, 0.7545207738876343]], "normalized_score": [0.0368511900305748], "tokens": [["\u0120Take", "\u0120Write", "\u0120Stand"]], "entropy": [[8.01179027557373, 4.042632102966309, 1.4184280633926392]], "avg_entropy": [4.490950147310893], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Under (disambiguation)", "score_of_biggest_cluster": 0.0368511900305748, "score_of_first_answer": 0.0368511900305748, "max_score_of_first_answer": 0.7545207738876343, "min_score_of_first_answer": 0.005002514459192753, "avg_score": 0.0368511900305748, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3513965016734165, "std_of_first_answer": 0.3513965016734165, "avg_entropy_of_first_answer": 4.490950147310893, "lex_sim": 1.0}
{"question": "On 31 May 2010, the President of which country announced his resignation after local politicians criticised comments he made in relation to overseas military deployments?", "answer": ["Pakistan President Asif Ali Zardari"], "scores": [[0.0105041628703475, 0.004466276615858078, 0.31328967213630676, 0.993960976600647, 0.7639822363853455, 0.985358476638794, 0.9772451519966125, 0.9856367111206055]], "normalized_score": [0.23885124921798706], "tokens": [["\u0120Pakistan", "\u0120President", "\u0120As", "if", "\u0120Ali", "\u0120Z", "ard", "ari"]], "entropy": [[6.1795654296875, 0.4389830529689789, 2.771852731704712, 0.06092235445976257, 0.8317592740058899, 0.14426414668560028, 0.19420252740383148, 0.13507868349552155]], "avg_entropy": [1.3445785250514746], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "United States of Germany", "score_of_biggest_cluster": 0.23885124921798706, "score_of_first_answer": 0.23885124921798706, "max_score_of_first_answer": 0.993960976600647, "min_score_of_first_answer": 0.004466276615858078, "avg_score": 0.23885124921798706, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41819915245832706, "std_of_first_answer": 0.41819915245832706, "avg_entropy_of_first_answer": 1.3445785250514746, "lex_sim": 1.0}
{"question": "How many feathers, traditionally, form the cone of a badminton shuttlecock?", "answer": ["100"], "scores": [[0.0014029811136424541]], "normalized_score": [0.0014029814628884196], "tokens": [["\u0120100"]], "entropy": [[4.450215816497803]], "avg_entropy": [4.450215816497803], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "sixteen", "score_of_biggest_cluster": 0.0014029814628884196, "score_of_first_answer": 0.0014029814628884196, "max_score_of_first_answer": 0.0014029811136424541, "min_score_of_first_answer": 0.0014029811136424541, "avg_score": 0.0014029814628884196, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.450215816497803, "lex_sim": 1.0}
{"question": "What is a form of word play that deliberately exploits ambiguity between similar-sounding words for humorous or rhetorical effect?", "answer": ["Metaphore"], "scores": [[0.015337157063186169, 0.6853561997413635, 0.005580402445048094]], "normalized_score": [0.038854580372571945], "tokens": [["\u0120Met", "aph", "ore"]], "entropy": [[5.768773078918457, 1.0555071830749512, 1.2735595703125]], "avg_entropy": [2.6992799441019693], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Punsters", "score_of_biggest_cluster": 0.038854580372571945, "score_of_first_answer": 0.038854580372571945, "max_score_of_first_answer": 0.6853561997413635, "min_score_of_first_answer": 0.005580402445048094, "avg_score": 0.038854580372571945, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31817462822270365, "std_of_first_answer": 0.31817462822270365, "avg_entropy_of_first_answer": 2.6992799441019693, "lex_sim": 1.0}
{"question": "Which flower has the Latin name Hyacinthoides non scripta?", "answer": ["Bluebell"], "scores": [[0.046089306473731995, 0.8290933966636658]], "normalized_score": [0.19547975063323975], "tokens": [["\u0120Blue", "bell"]], "entropy": [[5.319857120513916, 1.2955634593963623]], "avg_entropy": [3.307710289955139], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bluebelle", "score_of_biggest_cluster": 0.19547975063323975, "score_of_first_answer": 0.19547975063323975, "max_score_of_first_answer": 0.8290933966636658, "min_score_of_first_answer": 0.046089306473731995, "avg_score": 0.19547975063323975, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3915020450949669, "std_of_first_answer": 0.3915020450949669, "avg_entropy_of_first_answer": 3.307710289955139, "lex_sim": 1.0}
{"question": "For a point each, name the 9 countries surrounding Germany.", "answer": ["FRANCE"], "scores": [[0.002527486765757203, 0.6296268105506897]], "normalized_score": [0.03989202529191971], "tokens": [["\u0120FR", "ANCE"]], "entropy": [[4.537978172302246, 1.6882449388504028]], "avg_entropy": [3.1131115555763245], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Denmark, Poland, Czech Republic, Austria, Switzerland, France, Luxembourg, Belgium, and the Netherlands", "score_of_biggest_cluster": 0.03989202529191971, "score_of_first_answer": 0.03989202529191971, "max_score_of_first_answer": 0.6296268105506897, "min_score_of_first_answer": 0.002527486765757203, "avg_score": 0.03989202529191971, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31354966189246625, "std_of_first_answer": 0.31354966189246625, "avg_entropy_of_first_answer": 3.1131115555763245, "lex_sim": 1.0}
{"question": "In the Lake District which is the only body of water to actually be called lake?", "answer": ["Screes"], "scores": [[0.004530560690909624, 0.07496851682662964]], "normalized_score": [0.018429581075906754], "tokens": [["\u0120Sc", "rees"]], "entropy": [[3.5534920692443848, 4.215488910675049]], "avg_entropy": [3.884490489959717], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lake Bassenthwaite", "score_of_biggest_cluster": 0.018429581075906754, "score_of_first_answer": 0.018429581075906754, "max_score_of_first_answer": 0.07496851682662964, "min_score_of_first_answer": 0.004530560690909624, "avg_score": 0.018429581075906754, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.03521897806786001, "std_of_first_answer": 0.03521897806786001, "avg_entropy_of_first_answer": 3.884490489959717, "lex_sim": 1.0}
{"question": "What name was given to the eleven states that fought the Union in the US Civil War?", "answer": ["Secession States"], "scores": [[0.012599681504070759, 0.9570527076721191, 0.05650607496500015]], "normalized_score": [0.08799611777067184], "tokens": [["\u0120Sec", "ession", "\u0120States"]], "entropy": [[3.728243589401245, 0.2899344861507416, 1.7147645950317383]], "avg_entropy": [1.910980890194575], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Confederacies", "score_of_biggest_cluster": 0.08799611777067184, "score_of_first_answer": 0.08799611777067184, "max_score_of_first_answer": 0.9570527076721191, "min_score_of_first_answer": 0.012599681504070759, "avg_score": 0.08799611777067184, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4352398480154823, "std_of_first_answer": 0.4352398480154823, "avg_entropy_of_first_answer": 1.910980890194575, "lex_sim": 1.0}
{"question": "What was named after American frontiersman James Bowie?", "answer": ["Bowie Knife"], "scores": [[0.49894312024116516, 0.2411831170320511]], "normalized_score": [0.3468957543373108], "tokens": [["\u0120Bowie", "\u0120Knife"]], "entropy": [[3.6963796615600586, 0.9942363500595093]], "avg_entropy": [2.345308005809784], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ansall", "score_of_biggest_cluster": 0.3468957543373108, "score_of_first_answer": 0.3468957543373108, "max_score_of_first_answer": 0.49894312024116516, "min_score_of_first_answer": 0.2411831170320511, "avg_score": 0.3468957543373108, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12888000160455704, "std_of_first_answer": 0.12888000160455704, "avg_entropy_of_first_answer": 2.345308005809784, "lex_sim": 1.0}
{"question": "Bedloe\u2019s Island in New York Harbour, is the former name of which island?", "answer": ["Rondout (in New York)"], "scores": [[0.004683874547481537, 0.00274178315885365, 0.9338918328285217, 0.011033196933567524, 0.01863883063197136, 0.08497846126556396, 0.9292292594909668, 0.4205712378025055]], "normalized_score": [0.054848622530698776], "tokens": [["\u0120R", "ond", "out", "\u0120(", "in", "\u0120New", "\u0120York", ")"]], "entropy": [[4.4871673583984375, 2.017017364501953, 0.6261631846427917, 1.0939592123031616, 5.869500637054443, 4.935320854187012, 0.4095686972141266, 2.5532846450805664]], "avg_entropy": [2.7489977441728115], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bedloe's Island", "score_of_biggest_cluster": 0.054848622530698776, "score_of_first_answer": 0.054848622530698776, "max_score_of_first_answer": 0.9338918328285217, "min_score_of_first_answer": 0.00274178315885365, "avg_score": 0.054848622530698776, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38677445384071696, "std_of_first_answer": 0.38677445384071696, "avg_entropy_of_first_answer": 2.7489977441728115, "lex_sim": 1.0}
{"question": "Which musician founded the Red Hot Peppers?", "answer": ["Charlie Chaplin"], "scores": [[0.01477057859301567, 0.03481414169073105, 0.9856410026550293]], "normalized_score": [0.07973042875528336], "tokens": [["\u0120Charlie", "\u0120Cha", "plin"]], "entropy": [[6.096502304077148, 4.32818603515625, 0.11335144937038422]], "avg_entropy": [3.512679929534594], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ferdinand Joseph La Menthe", "score_of_biggest_cluster": 0.07973042875528336, "score_of_first_answer": 0.07973042875528336, "max_score_of_first_answer": 0.9856410026550293, "min_score_of_first_answer": 0.01477057859301567, "avg_score": 0.07973042875528336, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4530223006862082, "std_of_first_answer": 0.4530223006862082, "avg_entropy_of_first_answer": 3.512679929534594, "lex_sim": 1.0}
{"question": "Otology is a branch of medicine concerned with which part of the body?", "answer": ["The ear"], "scores": [[0.18368294835090637, 0.5760436058044434]], "normalized_score": [0.32528355717658997], "tokens": [["\u0120The", "\u0120ear"]], "entropy": [[3.0588386058807373, 1.8480496406555176]], "avg_entropy": [2.4534441232681274], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\ud83d\udc42", "score_of_biggest_cluster": 0.32528355717658997, "score_of_first_answer": 0.32528355717658997, "max_score_of_first_answer": 0.5760436058044434, "min_score_of_first_answer": 0.18368294835090637, "avg_score": 0.32528355717658997, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1961803287267685, "std_of_first_answer": 0.1961803287267685, "avg_entropy_of_first_answer": 2.4534441232681274, "lex_sim": 1.0}
{"question": "Who was John McCain's running mate?", "answer": ["Sarah Palin"], "scores": [[0.6614573001861572, 0.8593482375144958]], "normalized_score": [0.7539377808570862], "tokens": [["\u0120Sarah", "\u0120Palin"]], "entropy": [[2.4206466674804688, 1.3074464797973633]], "avg_entropy": [1.864046573638916], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sarrah Palin", "score_of_biggest_cluster": 0.7539377808570862, "score_of_first_answer": 0.7539377808570862, "max_score_of_first_answer": 0.8593482375144958, "min_score_of_first_answer": 0.6614573001861572, "avg_score": 0.7539377808570862, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09894546866416931, "std_of_first_answer": 0.09894546866416931, "avg_entropy_of_first_answer": 1.864046573638916, "lex_sim": 1.0}
{"question": "\"By what 4 letter word is the \"\"bulb\"\" of a crocus more correctly known?\"", "answer": ["A"], "scores": [[0.012870564125478268]], "normalized_score": [0.012870565988123417], "tokens": [["\u0120A"]], "entropy": [[5.893815994262695]], "avg_entropy": [5.893815994262695], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bulbo-tubers", "score_of_biggest_cluster": 0.012870565988123417, "score_of_first_answer": 0.012870565988123417, "max_score_of_first_answer": 0.012870564125478268, "min_score_of_first_answer": 0.012870564125478268, "avg_score": 0.012870565988123417, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.893815994262695, "lex_sim": 1.0}
{"question": "In which town or city would you find the 'Caledonian Brewery'?", "answer": ["St Andrews"], "scores": [[0.04453226178884506, 0.2724740207195282]], "normalized_score": [0.1101539209485054], "tokens": [["\u0120St", "\u0120Andrews"]], "entropy": [[4.642414093017578, 2.1354029178619385]], "avg_entropy": [3.3889085054397583], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dun Eideann", "score_of_biggest_cluster": 0.1101539209485054, "score_of_first_answer": 0.1101539209485054, "max_score_of_first_answer": 0.2724740207195282, "min_score_of_first_answer": 0.04453226178884506, "avg_score": 0.1101539209485054, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11397087946534157, "std_of_first_answer": 0.11397087946534157, "avg_entropy_of_first_answer": 3.3889085054397583, "lex_sim": 1.0}
{"question": "The Battle of the Plains of Abraham in 1759, a pivotal battle in the Seven Years' War fought between the English and the French, is the only major confrontation ever to have taken place in what country?", "answer": ["Canada"], "scores": [[0.6056934595108032]], "normalized_score": [0.6056934595108032], "tokens": [["\u0120Canada"]], "entropy": [[2.3886990547180176]], "avg_entropy": [2.3886990547180176], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Canada", "score_of_biggest_cluster": 0.6056934595108032, "score_of_first_answer": 0.6056934595108032, "max_score_of_first_answer": 0.6056934595108032, "min_score_of_first_answer": 0.6056934595108032, "avg_score": 0.6056934595108032, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.3886990547180176, "lex_sim": 1.0}
{"question": "Saloth Sar was the real name of which infamous dictator?", "answer": ["Slobodan Milosevic"], "scores": [[0.0068162912502884865, 0.4362187087535858, 0.7535819411277771, 0.4925365149974823, 0.8198686838150024, 0.9665971994400024]], "normalized_score": [0.30924442410469055], "tokens": [["\u0120Sl", "ob", "odan", "\u0120Mil", "ose", "vic"]], "entropy": [[5.537023544311523, 3.2728517055511475, 1.2222837209701538, 3.3767123222351074, 0.6900728344917297, 0.207957923412323]], "avg_entropy": [2.3844836751619973], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Polpot", "score_of_biggest_cluster": 0.30924442410469055, "score_of_first_answer": 0.30924442410469055, "max_score_of_first_answer": 0.9665971994400024, "min_score_of_first_answer": 0.0068162912502884865, "avg_score": 0.30924442410469055, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31463851133972137, "std_of_first_answer": 0.31463851133972137, "avg_entropy_of_first_answer": 2.3844836751619973, "lex_sim": 1.0}
{"question": "What former Egyptian president was born on Christmas day in 1918?", "answer": ["Gamal Abdel Nasser"], "scores": [[0.5220633745193481, 0.9815219640731812, 0.7575443983078003, 0.8770552277565002, 0.9967814683914185]], "normalized_score": [0.8056225776672363], "tokens": [["\u0120Gam", "al", "\u0120Abdel", "\u0120N", "asser"]], "entropy": [[2.503823757171631, 0.12459950149059296, 1.1114134788513184, 0.6658785939216614, 0.029145127162337303]], "avg_entropy": [0.8869720917195082], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Muhammad Anwar al-Sadat", "score_of_biggest_cluster": 0.8056225776672363, "score_of_first_answer": 0.8056225776672363, "max_score_of_first_answer": 0.9967814683914185, "min_score_of_first_answer": 0.5220633745193481, "avg_score": 0.8056225776672363, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1749392672652653, "std_of_first_answer": 0.1749392672652653, "avg_entropy_of_first_answer": 0.8869720917195082, "lex_sim": 1.0}
{"question": "Summerisle is the setting for which 1973 horror film?", "answer": ["The Wicker Man"], "scores": [[0.4832383394241333, 0.35389989614486694, 0.9988928437232971, 0.9900942444801331]], "normalized_score": [0.6412971615791321], "tokens": [["\u0120The", "\u0120W", "icker", "\u0120Man"]], "entropy": [[3.427229642868042, 1.6439228057861328, 0.011255403980612755, 0.08702340722084045]], "avg_entropy": [1.292357814963907], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Wicker Man (disambiguation)", "score_of_biggest_cluster": 0.6412971615791321, "score_of_first_answer": 0.6412971615791321, "max_score_of_first_answer": 0.9988928437232971, "min_score_of_first_answer": 0.35389989614486694, "avg_score": 0.6412971615791321, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2915869805727298, "std_of_first_answer": 0.2915869805727298, "avg_entropy_of_first_answer": 1.292357814963907, "lex_sim": 1.0}
{"question": "In which activity were hickory and persimmon replaced by graphite and titanium?", "answer": ["Tennis"], "scores": [[0.010261716321110725]], "normalized_score": [0.010261716321110725], "tokens": [["\u0120Tennis"]], "entropy": [[7.188568115234375]], "avg_entropy": [7.188568115234375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Scramble (golf)", "score_of_biggest_cluster": 0.010261716321110725, "score_of_first_answer": 0.010261716321110725, "max_score_of_first_answer": 0.010261716321110725, "min_score_of_first_answer": 0.010261716321110725, "avg_score": 0.010261716321110725, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 7.188568115234375, "lex_sim": 1.0}
{"question": "Which act won the Eurovision Song Contest for the United Kingdom singing Love Shine A Light?", "answer": ["Joe McElderry & Ell & Nikki"], "scores": [[0.00040840153815224767, 0.08851388096809387, 0.9620818495750427, 0.9348729848861694, 0.9928880333900452, 0.006385423708707094, 0.05559712275862694, 0.15320661664009094, 0.7944716811180115]], "normalized_score": [0.1037677600979805], "tokens": [["\u0120Joe", "\u0120Mc", "E", "lder", "ry", "\u0120&", "\u0120Ell", "\u0120&", "\u0120Nikki"]], "entropy": [[6.770261764526367, 3.9669156074523926, 0.31634217500686646, 0.3119348883628845, 0.0729811042547226, 1.140763759613037, 5.556215286254883, 0.7125691175460815, 1.69150972366333]], "avg_entropy": [2.2821659362978406], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Katrina and the Waves", "score_of_biggest_cluster": 0.1037677600979805, "score_of_first_answer": 0.1037677600979805, "max_score_of_first_answer": 0.9928880333900452, "min_score_of_first_answer": 0.00040840153815224767, "avg_score": 0.1037677600979805, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4325037160986054, "std_of_first_answer": 0.4325037160986054, "avg_entropy_of_first_answer": 2.2821659362978406, "lex_sim": 1.0}
{"question": "In 1403, Venice imposed a waiting period for those wishing to enter the city, to see if they would develop the plague. How long was this period of isolation?", "answer": ["28 years"], "scores": [[0.0024481366854161024, 0.15563607215881348]], "normalized_score": [0.019519690424203873], "tokens": [["\u012028", "\u0120years"]], "entropy": [[4.637263774871826, 1.6038892269134521]], "avg_entropy": [3.120576500892639], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "40 time", "score_of_biggest_cluster": 0.019519690424203873, "score_of_first_answer": 0.019519690424203873, "max_score_of_first_answer": 0.15563607215881348, "min_score_of_first_answer": 0.0024481366854161024, "avg_score": 0.019519690424203873, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07659396773669869, "std_of_first_answer": 0.07659396773669869, "avg_entropy_of_first_answer": 3.120576500892639, "lex_sim": 1.0}
{"question": "What relation was Flo Jo to Jackie Joyner Kersee?", "answer": ["She whose hair grows like weeds"], "scores": [[0.03269890323281288, 9.611766245143372e-07, 0.005025114398449659, 0.0023570647463202477, 0.02641550451517105, 0.02591405063867569]], "normalized_score": [0.0025179176591336727], "tokens": [["\u0120She", "\u0120whose", "\u0120hair", "\u0120grows", "\u0120like", "\u0120weeds"]], "entropy": [[5.2219719886779785, 1.4901142120361328, 5.827654838562012, 4.213259220123291, 4.600151062011719, 4.631808757781982]], "avg_entropy": [4.3308266798655195], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brother in law", "score_of_biggest_cluster": 0.0025179176591336727, "score_of_first_answer": 0.0025179176591336727, "max_score_of_first_answer": 0.03269890323281288, "min_score_of_first_answer": 9.611766245143372e-07, "avg_score": 0.0025179176591336727, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.013203695436970838, "std_of_first_answer": 0.013203695436970838, "avg_entropy_of_first_answer": 4.3308266798655195, "lex_sim": 1.0}
{"question": "Petroliam Nasional Berhad, the huge Malaysian gas/oil corporation gave its abbreviated name to what world-leading Kuala Lumpa landmark?", "answer": ["Petronas Twin Towers"], "scores": [[0.6861396431922913, 0.9622504711151123, 0.9942101240158081, 0.503756046295166, 0.9504337310791016]], "normalized_score": [0.7933488488197327], "tokens": [["\u0120Pet", "ron", "as", "\u0120Twin", "\u0120Towers"]], "entropy": [[2.135484218597412, 0.20696786046028137, 0.052586570382118225, 1.3527705669403076, 0.22160375118255615]], "avg_entropy": [0.7938825935125351], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Petronas Tower 2", "score_of_biggest_cluster": 0.7933488488197327, "score_of_first_answer": 0.7933488488197327, "max_score_of_first_answer": 0.9942101240158081, "min_score_of_first_answer": 0.503756046295166, "avg_score": 0.7933488488197327, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19262611174241034, "std_of_first_answer": 0.19262611174241034, "avg_entropy_of_first_answer": 0.7938825935125351, "lex_sim": 1.0}
{"question": "Which cartoon title character has a friend called Captain Haddock?", "answer": ["Gumby"], "scores": [[0.0065437983721494675, 0.9010652899742126]], "normalized_score": [0.07678796350955963], "tokens": [["\u0120Gum", "by"]], "entropy": [[5.885262489318848, 0.6360923051834106]], "avg_entropy": [3.260677397251129], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tin-tin", "score_of_biggest_cluster": 0.07678796350955963, "score_of_first_answer": 0.07678796350955963, "max_score_of_first_answer": 0.9010652899742126, "min_score_of_first_answer": 0.0065437983721494675, "avg_score": 0.07678796350955963, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4472607458010316, "std_of_first_answer": 0.4472607458010316, "avg_entropy_of_first_answer": 3.260677397251129, "lex_sim": 1.0}
{"question": "Which film sees Clint Eastwood, Tommy Lee Jones, James Garner and Donald Sutherland as retired astronauts?", "answer": ["Gravity (2012 film)"], "scores": [[0.027011269703507423, 0.034771159291267395, 0.05954466387629509, 0.02414199896156788, 0.9717972278594971]], "normalized_score": [0.06661791354417801], "tokens": [["\u0120Gravity", "\u0120(", "2012", "\u0120film", ")"]], "entropy": [[3.9694535732269287, 0.8710833191871643, 3.4890241622924805, 0.37615758180618286, 0.23783619701862335]], "avg_entropy": [1.788710966706276], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Space Cowboys", "score_of_biggest_cluster": 0.06661791354417801, "score_of_first_answer": 0.06661791354417801, "max_score_of_first_answer": 0.9717972278594971, "min_score_of_first_answer": 0.02414199896156788, "avg_score": 0.06661791354417801, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3743795110769758, "std_of_first_answer": 0.3743795110769758, "avg_entropy_of_first_answer": 1.788710966706276, "lex_sim": 1.0}
{"question": "What in the world of the internet is an ISP ?", "answer": ["An Internet Service Provider"], "scores": [[0.08425331115722656, 0.5432063937187195, 0.8685266375541687, 0.9558206796646118]], "normalized_score": [0.4414970874786377], "tokens": [["\u0120An", "\u0120Internet", "\u0120Service", "\u0120Provider"]], "entropy": [[2.3964877128601074, 2.1067187786102295, 0.5322179794311523, 0.29268357157707214]], "avg_entropy": [1.3320270106196404], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Internet Service Provider", "score_of_biggest_cluster": 0.4414970874786377, "score_of_first_answer": 0.4414970874786377, "max_score_of_first_answer": 0.9558206796646118, "min_score_of_first_answer": 0.08425331115722656, "avg_score": 0.4414970874786377, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3417835865117249, "std_of_first_answer": 0.3417835865117249, "avg_entropy_of_first_answer": 1.3320270106196404, "lex_sim": 1.0}
{"question": "What was the middle name of Senator Edward M Kennedy, who died in August 2009?", "answer": ["Patrick"], "scores": [[0.01732339896261692]], "normalized_score": [0.01732340268790722], "tokens": [["\u0120Patrick"]], "entropy": [[4.498390197753906]], "avg_entropy": [4.498390197753906], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Moore Theater Building", "score_of_biggest_cluster": 0.01732340268790722, "score_of_first_answer": 0.01732340268790722, "max_score_of_first_answer": 0.01732339896261692, "min_score_of_first_answer": 0.01732339896261692, "avg_score": 0.01732340268790722, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.498390197753906, "lex_sim": 1.0}
{"question": "\"In the \"\"Peanuts\"\" comic strip, who is Lucy's brother?\"", "answer": ["Don"], "scores": [[0.0011511498596519232]], "normalized_score": [0.0011511498596519232], "tokens": [["\u0120Don"]], "entropy": [[3.377084732055664]], "avg_entropy": [3.377084732055664], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Linus (disambiguation)", "score_of_biggest_cluster": 0.0011511498596519232, "score_of_first_answer": 0.0011511498596519232, "max_score_of_first_answer": 0.0011511498596519232, "min_score_of_first_answer": 0.0011511498596519232, "avg_score": 0.0011511498596519232, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.377084732055664, "lex_sim": 1.0}
{"question": "\"In the film of the same name, what was the name of \"\"The Hustler\"\"?\"", "answer": ["\"The Hustler\" (film)"], "scores": [[0.12677563726902008, 0.23045068979263306, 0.5131401419639587, 0.9383106827735901, 0.8022642135620117, 0.01818004809319973, 0.04272808879613876, 0.844171404838562]], "normalized_score": [0.2283792495727539], "tokens": [["\u0120\"", "The", "\u0120Hust", "ler", "\"", "\u0120(", "film", ")"]], "entropy": [[5.40250301361084, 4.961722373962402, 4.010342597961426, 0.287311315536499, 1.0893126726150513, 0.8341667652130127, 4.102392196655273, 1.1462581157684326]], "avg_entropy": [2.729251131415367], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\"\"\"Fast Eddie\"\" Felson\"", "score_of_biggest_cluster": 0.2283792495727539, "score_of_first_answer": 0.2283792495727539, "max_score_of_first_answer": 0.9383106827735901, "min_score_of_first_answer": 0.01818004809319973, "avg_score": 0.2283792495727539, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3581250446382094, "std_of_first_answer": 0.3581250446382094, "avg_entropy_of_first_answer": 2.729251131415367, "lex_sim": 1.0}
{"question": "In which film did Johnny Depp play a policeman called Ichabod Crane?", "answer": ["Cry, the Beloved Country (film)"], "scores": [[0.004910246469080448, 0.04562706872820854, 0.6100234985351562, 0.9939098358154297, 0.9963387250900269, 0.9808768630027771, 0.07684239000082016, 0.0019553883466869593, 0.7737693786621094]], "normalized_score": [0.1355358064174652], "tokens": [["\u0120Cry", ",", "\u0120the", "\u0120Bel", "oved", "\u0120Country", "\u0120(", "film", ")"]], "entropy": [[3.487787961959839, 3.390169620513916, 1.944574236869812, 0.07546345889568329, 0.03468167036771774, 0.10660556703805923, 0.910857081413269, 4.45695161819458, 1.4857573509216309]], "avg_entropy": [1.7658720629082785], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sleepy Hollow (disambiguation)", "score_of_biggest_cluster": 0.1355358064174652, "score_of_first_answer": 0.1355358064174652, "max_score_of_first_answer": 0.9963387250900269, "min_score_of_first_answer": 0.0019553883466869593, "avg_score": 0.1355358064174652, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4329804112113185, "std_of_first_answer": 0.4329804112113185, "avg_entropy_of_first_answer": 1.7658720629082785, "lex_sim": 1.0}
{"question": "Name the year in which Driving Miss Daisy won the Best Picture Oscar, Mikhail Gorbachev is elected as the first executive President of the Soviet Union and Iraq invaded Kuwait?", "answer": ["1989"], "scores": [[0.17839311063289642]], "normalized_score": [0.17839312553405762], "tokens": [["\u01201989"]], "entropy": [[3.9452717304229736]], "avg_entropy": [3.9452717304229736], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1990", "score_of_biggest_cluster": 0.17839312553405762, "score_of_first_answer": 0.17839312553405762, "max_score_of_first_answer": 0.17839311063289642, "min_score_of_first_answer": 0.17839311063289642, "avg_score": 0.17839312553405762, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.9452717304229736, "lex_sim": 1.0}
{"question": "The Dodecanese Campaign of WWII that was an attempt by the Allied forces to capture islands in the Aegean Sea was the inspiration for which acclaimed 1961 commando film?", "answer": ["The Man Who Would Be King (film)"], "scores": [[0.3654305040836334, 0.034725964069366455, 0.6630495190620422, 0.1762860268354416, 0.9581699967384338, 0.9904378056526184, 0.03179511800408363, 0.08946023881435394, 0.9576156735420227]], "normalized_score": [0.2501416504383087], "tokens": [["\u0120The", "\u0120Man", "\u0120Who", "\u0120Would", "\u0120Be", "\u0120King", "\u0120(", "film", ")"]], "entropy": [[4.213086128234863, 3.7310853004455566, 1.419891119003296, 2.2030043601989746, 0.23487913608551025, 0.07266512513160706, 0.594925045967102, 4.384708404541016, 0.35522449016571045]], "avg_entropy": [1.912163234419293], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Guns of Navarone", "score_of_biggest_cluster": 0.2501416504383087, "score_of_first_answer": 0.2501416504383087, "max_score_of_first_answer": 0.9904378056526184, "min_score_of_first_answer": 0.03179511800408363, "avg_score": 0.2501416504383087, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39546750645737255, "std_of_first_answer": 0.39546750645737255, "avg_entropy_of_first_answer": 1.912163234419293, "lex_sim": 1.0}
{"question": "Hafidh are people who have memorized what?", "answer": ["The Holy Quran"], "scores": [[0.2802596092224121, 0.014746883884072304, 0.5115432143211365]], "normalized_score": [0.12834560871124268], "tokens": [["\u0120The", "\u0120Holy", "\u0120Quran"]], "entropy": [[3.1632680892944336, 1.9552971124649048, 1.2907354831695557]], "avg_entropy": [2.136433561642965], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Curaan", "score_of_biggest_cluster": 0.12834560871124268, "score_of_first_answer": 0.12834560871124268, "max_score_of_first_answer": 0.5115432143211365, "min_score_of_first_answer": 0.014746883884072304, "avg_score": 0.12834560871124268, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.20297665635891018, "std_of_first_answer": 0.20297665635891018, "avg_entropy_of_first_answer": 2.136433561642965, "lex_sim": 1.0}
{"question": "The Beatles won an Oscar for what?", "answer": ["A Hard Day's Night"], "scores": [[0.07098627835512161, 0.25716865062713623, 0.8133183717727661, 0.966786801815033, 0.9899786710739136]], "normalized_score": [0.42709246277809143], "tokens": [["\u0120A", "\u0120Hard", "\u0120Day", "'s", "\u0120Night"]], "entropy": [[4.2641730308532715, 4.709166526794434, 0.7527986764907837, 0.26168620586395264, 0.08974915742874146]], "avg_entropy": [2.0155147194862364], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Best original score for the film 'Let it Be'", "score_of_biggest_cluster": 0.42709246277809143, "score_of_first_answer": 0.42709246277809143, "max_score_of_first_answer": 0.9899786710739136, "min_score_of_first_answer": 0.07098627835512161, "avg_score": 0.42709246277809143, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38146541916300336, "std_of_first_answer": 0.38146541916300336, "avg_entropy_of_first_answer": 2.0155147194862364, "lex_sim": 1.0}
{"question": "Which bird is the symbol of the Durrell Wildlife Conservation Trust, which began its overseas work in Mauritius in 1970?", "answer": ["Dodo"], "scores": [[0.05180996656417847, 0.9519761204719543]], "normalized_score": [0.22208523750305176], "tokens": [["\u0120Dod", "o"]], "entropy": [[5.654366493225098, 0.40411442518234253]], "avg_entropy": [3.02924045920372], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Raphus", "score_of_biggest_cluster": 0.22208523750305176, "score_of_first_answer": 0.22208523750305176, "max_score_of_first_answer": 0.9519761204719543, "min_score_of_first_answer": 0.05180996656417847, "avg_score": 0.22208523750305176, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.45008307695388794, "std_of_first_answer": 0.45008307695388794, "avg_entropy_of_first_answer": 3.02924045920372, "lex_sim": 1.0}
{"question": "Ardbeg is whisky distillery on which Scottish island?", "answer": ["Islay"], "scores": [[0.46225666999816895, 0.9151414036750793]], "normalized_score": [0.6504077315330505], "tokens": [["\u0120Is", "lay"]], "entropy": [[2.8886609077453613, 0.37831026315689087]], "avg_entropy": [1.633485585451126], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Isle of Islay", "score_of_biggest_cluster": 0.6504077315330505, "score_of_first_answer": 0.6504077315330505, "max_score_of_first_answer": 0.9151414036750793, "min_score_of_first_answer": 0.46225666999816895, "avg_score": 0.6504077315330505, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2264423668384552, "std_of_first_answer": 0.2264423668384552, "avg_entropy_of_first_answer": 1.633485585451126, "lex_sim": 1.0}
{"question": "What was the name of Gary Puckett's backing group?", "answer": ["Partners in Rhythm"], "scores": [[0.004813251551240683, 0.3824438452720642, 0.017552567645907402]], "normalized_score": [0.03185045346617699], "tokens": [["\u0120Partners", "\u0120in", "\u0120Rhythm"]], "entropy": [[2.051130771636963, 1.8518242835998535, 1.9085416793823242]], "avg_entropy": [1.9371655782063801], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gary Puckett & the Union Gap", "score_of_biggest_cluster": 0.03185045346617699, "score_of_first_answer": 0.03185045346617699, "max_score_of_first_answer": 0.3824438452720642, "min_score_of_first_answer": 0.004813251551240683, "avg_score": 0.03185045346617699, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17509134123401945, "std_of_first_answer": 0.17509134123401945, "avg_entropy_of_first_answer": 1.9371655782063801, "lex_sim": 1.0}
{"question": "What is Koo Stark's real first name ?", "answer": ["Koo"], "scores": [[0.07546060532331467, 0.5757303833961487]], "normalized_score": [0.20843453705310822], "tokens": [["\u0120K", "oo"]], "entropy": [[6.9173903465271, 3.3654985427856445]], "avg_entropy": [5.141444444656372], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kathleen", "score_of_biggest_cluster": 0.20843453705310822, "score_of_first_answer": 0.20843453705310822, "max_score_of_first_answer": 0.5757303833961487, "min_score_of_first_answer": 0.07546060532331467, "avg_score": 0.20843453705310822, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.250134889036417, "std_of_first_answer": 0.250134889036417, "avg_entropy_of_first_answer": 5.141444444656372, "lex_sim": 1.0}
{"question": "Which celebrated battle was fought near Brno on 2nd December 1805?", "answer": ["Battle of Austerlitz"], "scores": [[0.26087403297424316, 0.9762946963310242, 0.5086462497711182, 0.997517466545105, 0.9991126656532288, 0.9995020627975464]], "normalized_score": [0.7108714580535889], "tokens": [["\u0120Battle", "\u0120of", "\u0120Aust", "er", "l", "itz"]], "entropy": [[4.107607364654541, 0.2017640769481659, 3.0334603786468506, 0.027386389672756195, 0.01041321735829115, 0.005566774867475033]], "avg_entropy": [1.2310330336913466], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Austerlitz (disambiguation)", "score_of_biggest_cluster": 0.7108714580535889, "score_of_first_answer": 0.7108714580535889, "max_score_of_first_answer": 0.9995020627975464, "min_score_of_first_answer": 0.26087403297424316, "avg_score": 0.7108714580535889, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2956693039781984, "std_of_first_answer": 0.2956693039781984, "avg_entropy_of_first_answer": 1.2310330336913466, "lex_sim": 1.0}
{"question": "The Greek god of marriage is?", "answer": ["Zeus"], "scores": [[0.18485988676548004]], "normalized_score": [0.18485988676548004], "tokens": [["\u0120Zeus"]], "entropy": [[3.5458261966705322]], "avg_entropy": [3.5458261966705322], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Microperforate hymen", "score_of_biggest_cluster": 0.18485988676548004, "score_of_first_answer": 0.18485988676548004, "max_score_of_first_answer": 0.18485988676548004, "min_score_of_first_answer": 0.18485988676548004, "avg_score": 0.18485988676548004, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.5458261966705322, "lex_sim": 1.0}
{"question": "What was the home ground of Leyton Orient called before it became the Matchroom Stadium?", "answer": ["East Farm"], "scores": [[0.005584020167589188, 0.007614749018102884]], "normalized_score": [0.0065208058804273605], "tokens": [["\u0120East", "\u0120Farm"]], "entropy": [[5.6886444091796875, 4.307616233825684]], "avg_entropy": [4.9981303215026855], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brisbane Road Stadium", "score_of_biggest_cluster": 0.0065208058804273605, "score_of_first_answer": 0.0065208058804273605, "max_score_of_first_answer": 0.007614749018102884, "min_score_of_first_answer": 0.005584020167589188, "avg_score": 0.0065208058804273605, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0010153644252568483, "std_of_first_answer": 0.0010153644252568483, "avg_entropy_of_first_answer": 4.9981303215026855, "lex_sim": 1.0}
{"question": "What is the English name of the city of Firenze?", "answer": ["Florence"], "scores": [[0.5960044860839844]], "normalized_score": [0.5960044860839844], "tokens": [["\u0120Florence"]], "entropy": [[1.7302448749542236]], "avg_entropy": [1.7302448749542236], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "UN/LOCODE:ITFLR", "score_of_biggest_cluster": 0.5960044860839844, "score_of_first_answer": 0.5960044860839844, "max_score_of_first_answer": 0.5960044860839844, "min_score_of_first_answer": 0.5960044860839844, "avg_score": 0.5960044860839844, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.7302448749542236, "lex_sim": 1.0}
{"question": "What is the name given to a distinguishing device stamped into the substance of a sheet of paper when it is wet?", "answer": ["Stiffened ink"], "scores": [[0.02282552421092987, 0.07906320691108704, 0.04597488418221474, 0.027325011789798737]], "normalized_score": [0.0388033390045166], "tokens": [["\u0120St", "iff", "ened", "\u0120ink"]], "entropy": [[4.669485569000244, 3.8618805408477783, 2.5846753120422363, 2.885593891143799]], "avg_entropy": [3.5004088282585144], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Watermarked", "score_of_biggest_cluster": 0.0388033390045166, "score_of_first_answer": 0.0388033390045166, "max_score_of_first_answer": 0.07906320691108704, "min_score_of_first_answer": 0.02282552421092987, "avg_score": 0.0388033390045166, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02213355891896396, "std_of_first_answer": 0.02213355891896396, "avg_entropy_of_first_answer": 3.5004088282585144, "lex_sim": 1.0}
{"question": "What does Sally sell by the sea shore?", "answer": ["Sea Salt"], "scores": [[0.049846161156892776, 0.012115408666431904]], "normalized_score": [0.024574508890509605], "tokens": [["\u0120Sea", "\u0120Salt"]], "entropy": [[6.560820579528809, 3.1073312759399414]], "avg_entropy": [4.834075927734375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sea Shells", "score_of_biggest_cluster": 0.024574508890509605, "score_of_first_answer": 0.024574508890509605, "max_score_of_first_answer": 0.049846161156892776, "min_score_of_first_answer": 0.012115408666431904, "avg_score": 0.024574508890509605, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.018865376245230436, "std_of_first_answer": 0.018865376245230436, "avg_entropy_of_first_answer": 4.834075927734375, "lex_sim": 1.0}
{"question": "\"What does the word \"\"fine\"\" mean on sheet music?\"", "answer": ["\"Dotless\""], "scores": [[0.04235430806875229, 0.004472979810088873, 0.009897919371724129, 0.0034875329583883286, 0.5978193879127502]], "normalized_score": [0.02081734500825405], "tokens": [["\u0120\"", "D", "ot", "less", "\""]], "entropy": [[6.218039512634277, 6.096989631652832, 4.59883975982666, 3.6740174293518066, 2.722484588623047]], "avg_entropy": [4.662074184417724], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The End.", "score_of_biggest_cluster": 0.02081734500825405, "score_of_first_answer": 0.02081734500825405, "max_score_of_first_answer": 0.5978193879127502, "min_score_of_first_answer": 0.0034875329583883286, "avg_score": 0.02081734500825405, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2335426260410244, "std_of_first_answer": 0.2335426260410244, "avg_entropy_of_first_answer": 4.662074184417724, "lex_sim": 1.0}
{"question": "What drinking vessel supposedly confers eternal life on those who drink from it?", "answer": ["The Grail."], "scores": [[0.33494246006011963, 0.0919509306550026, 0.01311194896697998]], "normalized_score": [0.07391476631164551], "tokens": [["\u0120The", "\u0120Grail", "."]], "entropy": [[4.031403064727783, 4.695791721343994, 0.665483832359314]], "avg_entropy": [3.1308928728103638], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sangrael", "score_of_biggest_cluster": 0.07391476631164551, "score_of_first_answer": 0.07391476631164551, "max_score_of_first_answer": 0.33494246006011963, "min_score_of_first_answer": 0.01311194896697998, "avg_score": 0.07391476631164551, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1369652617218094, "std_of_first_answer": 0.1369652617218094, "avg_entropy_of_first_answer": 3.1308928728103638, "lex_sim": 1.0}
{"question": "What car manufacturing company makes the model Octavia?", "answer": ["MINI"], "scores": [[0.0026679756119847298, 0.9907286167144775]], "normalized_score": [0.051412444561719894], "tokens": [["\u0120MIN", "I"]], "entropy": [[5.0751142501831055, 0.08947509527206421]], "avg_entropy": [2.582294672727585], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Skoda cars", "score_of_biggest_cluster": 0.051412444561719894, "score_of_first_answer": 0.051412444561719894, "max_score_of_first_answer": 0.9907286167144775, "min_score_of_first_answer": 0.0026679756119847298, "avg_score": 0.051412444561719894, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4940303205512464, "std_of_first_answer": 0.4940303205512464, "avg_entropy_of_first_answer": 2.582294672727585, "lex_sim": 1.0}
{"question": "Which film starring Reese Witherspoon had the same title as a hit by Lynyrd Skynyrd?", "answer": ["Elf"], "scores": [[0.0020850994624197483]], "normalized_score": [0.0020850992295891047], "tokens": [["\u0120Elf"]], "entropy": [[4.947948455810547]], "avg_entropy": [4.947948455810547], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sweet Home Alabama", "score_of_biggest_cluster": 0.0020850992295891047, "score_of_first_answer": 0.0020850992295891047, "max_score_of_first_answer": 0.0020850994624197483, "min_score_of_first_answer": 0.0020850994624197483, "avg_score": 0.0020850992295891047, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.947948455810547, "lex_sim": 1.0}
{"question": "Who was the first US president Barbara Walters interviewed?", "answer": ["George Washington"], "scores": [[0.10838701575994492, 0.29654210805892944]], "normalized_score": [0.17927998304367065], "tokens": [["\u0120George", "\u0120Washington"]], "entropy": [[4.3445634841918945, 2.313030958175659]], "avg_entropy": [3.328797221183777], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "I'm not a crook", "score_of_biggest_cluster": 0.17927998304367065, "score_of_first_answer": 0.17927998304367065, "max_score_of_first_answer": 0.29654210805892944, "min_score_of_first_answer": 0.10838701575994492, "avg_score": 0.17927998304367065, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09407754614949226, "std_of_first_answer": 0.09407754614949226, "avg_entropy_of_first_answer": 3.328797221183777, "lex_sim": 1.0}
{"question": "What are the first names of the two dancing instructors in the UK television series \u2018Hi De Hi\u2019?", "answer": ["Dora, Dora, Deirdre, Deirdre, Doreen"], "scores": [[0.007027024868875742, 0.05038251727819443, 0.012374493293464184, 0.029406903311610222, 0.2410527765750885, 0.10434447973966599, 0.004550376906991005, 0.18693062663078308, 0.9851962327957153, 0.19509151577949524, 0.2532385587692261, 0.961111843585968, 0.9533258676528931, 0.4587916433811188, 0.25582417845726013, 0.1452743113040924, 0.9189839959144592]], "normalized_score": [0.1362680345773697], "tokens": [["\u0120D", "ora", ",", "\u0120D", "ora", ",", "\u0120De", "ird", "re", ",", "\u0120De", "ird", "re", ",", "\u0120D", "ore", "en"]], "entropy": [[6.759261131286621, 4.311714172363281, 2.883979320526123, 6.223282337188721, 3.2214622497558594, 2.6056041717529297, 4.521533966064453, 4.286264419555664, 0.10677452385425568, 1.7547967433929443, 4.222858428955078, 0.32107412815093994, 0.26107057929039, 1.6185646057128906, 3.8877429962158203, 1.5588085651397705, 0.5348237156867981]], "avg_entropy": [2.8870362385230908], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Barry and Yvonne", "score_of_biggest_cluster": 0.1362680345773697, "score_of_first_answer": 0.1362680345773697, "max_score_of_first_answer": 0.9851962327957153, "min_score_of_first_answer": 0.004550376906991005, "avg_score": 0.1362680345773697, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35981283772531464, "std_of_first_answer": 0.35981283772531464, "avg_entropy_of_first_answer": 2.8870362385230908, "lex_sim": 1.0}
{"question": "Which is the only non-European country to have hosted the Winter Olympics only once?", "answer": ["Norway"], "scores": [[0.06978365033864975]], "normalized_score": [0.06978365778923035], "tokens": [["\u0120Norway"]], "entropy": [[4.935943603515625]], "avg_entropy": [4.935943603515625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "CANADA (1988)", "score_of_biggest_cluster": 0.06978365778923035, "score_of_first_answer": 0.06978365778923035, "max_score_of_first_answer": 0.06978365033864975, "min_score_of_first_answer": 0.06978365033864975, "avg_score": 0.06978365778923035, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.935943603515625, "lex_sim": 1.0}
{"question": "The word sacerdotal means relating to what?", "answer": ["The priesthood"], "scores": [[0.20897351205348969, 0.27304428815841675]], "normalized_score": [0.23887032270431519], "tokens": [["\u0120The", "\u0120priesthood"]], "entropy": [[4.483555793762207, 4.164516448974609]], "avg_entropy": [4.324036121368408], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Priest (religion)", "score_of_biggest_cluster": 0.23887032270431519, "score_of_first_answer": 0.23887032270431519, "max_score_of_first_answer": 0.27304428815841675, "min_score_of_first_answer": 0.20897351205348969, "avg_score": 0.23887032270431519, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03203538805246353, "std_of_first_answer": 0.03203538805246353, "avg_entropy_of_first_answer": 4.324036121368408, "lex_sim": 1.0}
{"question": "\"What year did Jean-Francois Champollion publish the first correct translation of Egyptian hieroglyphs from the Rosetta Stone, the Roman Catholic Church take Galileo Galilei's \"\"Dialogue\"\" off their list of banned books, and Britain repeal the death penalty for over 100 crimes?\"", "answer": ["1829"], "scores": [[0.07281085848808289, 0.03796862065792084]], "normalized_score": [0.05257878080010414], "tokens": [["\u012018", "29"]], "entropy": [[1.8663033246994019, 3.195549964904785]], "avg_entropy": [2.5309266448020935], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, eight hundred and twenty-two", "score_of_biggest_cluster": 0.05257878080010414, "score_of_first_answer": 0.05257878080010414, "max_score_of_first_answer": 0.07281085848808289, "min_score_of_first_answer": 0.03796862065792084, "avg_score": 0.05257878080010414, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.017421118915081024, "std_of_first_answer": 0.017421118915081024, "avg_entropy_of_first_answer": 2.5309266448020935, "lex_sim": 1.0}
{"question": "Which tennis player was dubbed \u2018Superbrat\u2019 by the British press?", "answer": ["Serena"], "scores": [[0.011861488223075867, 0.9983494281768799]], "normalized_score": [0.10882055759429932], "tokens": [["\u0120Sere", "na"]], "entropy": [[4.839503765106201, 0.016543889418244362]], "avg_entropy": [2.4280238272622228], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Patrick McEnroe Jr.", "score_of_biggest_cluster": 0.10882055759429932, "score_of_first_answer": 0.10882055759429932, "max_score_of_first_answer": 0.9983494281768799, "min_score_of_first_answer": 0.011861488223075867, "avg_score": 0.10882055759429932, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.493243969976902, "std_of_first_answer": 0.493243969976902, "avg_entropy_of_first_answer": 2.4280238272622228, "lex_sim": 1.0}
{"question": "Featuring the Trouble Alert, at what Metropolis location did the Super Friends congregate in the 1970s animated show The Super Friends?", "answer": ["Metropolis"], "scores": [[0.28252536058425903, 0.9876856803894043]], "normalized_score": [0.5282483100891113], "tokens": [["\u0120Met", "ropolis"]], "entropy": [[5.287585258483887, 0.11198578029870987]], "avg_entropy": [2.6997855193912983], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hall of Justice", "score_of_biggest_cluster": 0.5282483100891113, "score_of_first_answer": 0.5282483100891113, "max_score_of_first_answer": 0.9876856803894043, "min_score_of_first_answer": 0.28252536058425903, "avg_score": 0.5282483100891113, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35258015990257263, "std_of_first_answer": 0.35258015990257263, "avg_entropy_of_first_answer": 2.6997855193912983, "lex_sim": 1.0}
{"question": "\"What group of people in the United States speak a dialect called \"\"Pennsylvania German\"\"?\"", "answer": ["Pennsylvania Dutch"], "scores": [[0.18576835095882416, 0.6775113940238953]], "normalized_score": [0.354767769575119], "tokens": [["\u0120Pennsylvania", "\u0120Dutch"]], "entropy": [[3.637289524078369, 1.4424238204956055]], "avg_entropy": [2.5398566722869873], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Amish", "score_of_biggest_cluster": 0.354767769575119, "score_of_first_answer": 0.354767769575119, "max_score_of_first_answer": 0.6775113940238953, "min_score_of_first_answer": 0.18576835095882416, "avg_score": 0.354767769575119, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24587152153253555, "std_of_first_answer": 0.24587152153253555, "avg_entropy_of_first_answer": 2.5398566722869873, "lex_sim": 1.0}
{"question": "Who had a Too Legit To Quit Tour?", "answer": ["BTS, the boys or BTS"], "scores": [[0.004705157596617937, 0.012644540518522263, 0.013673439621925354, 0.056218866258859634, 0.010187591426074505, 0.001517985831014812, 0.07745686173439026, 0.9222218990325928]], "normalized_score": [0.02177377976477146], "tokens": [["\u0120B", "TS", ",", "\u0120the", "\u0120boys", "\u0120or", "\u0120B", "TS"]], "entropy": [[6.802340507507324, 4.563563823699951, 1.2980048656463623, 6.036015510559082, 4.481224060058594, 2.7205042839050293, 4.465728759765625, 0.7006144523620605]], "avg_entropy": [3.8834995329380035], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "M. C. Hammer", "score_of_biggest_cluster": 0.02177377976477146, "score_of_first_answer": 0.02177377976477146, "max_score_of_first_answer": 0.9222218990325928, "min_score_of_first_answer": 0.001517985831014812, "avg_score": 0.02177377976477146, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29775324403144615, "std_of_first_answer": 0.29775324403144615, "avg_entropy_of_first_answer": 3.8834995329380035, "lex_sim": 1.0}
{"question": "Who became Prime Minister of Canada in November last year?", "answer": ["Justin Trudeau"], "scores": [[0.4803648293018341, 0.9173842072486877]], "normalized_score": [0.663836658000946], "tokens": [["\u0120Justin", "\u0120Trudeau"]], "entropy": [[3.123260498046875, 0.7851900458335876]], "avg_entropy": [1.9542252719402313], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Trudeau, justin", "score_of_biggest_cluster": 0.663836658000946, "score_of_first_answer": 0.663836658000946, "max_score_of_first_answer": 0.9173842072486877, "min_score_of_first_answer": 0.4803648293018341, "avg_score": 0.663836658000946, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.21850968897342682, "std_of_first_answer": 0.21850968897342682, "avg_entropy_of_first_answer": 1.9542252719402313, "lex_sim": 1.0}
{"question": "October 27, 1858 saw the birth of what totally bad assed US president, the 26th, who spent time as a North Dakota rancher before becoming assistant Secretary of the Navy in 1899?", "answer": ["Theodore Roosevelt"], "scores": [[0.19138360023498535, 0.7693297266960144]], "normalized_score": [0.38371485471725464], "tokens": [["\u0120Theodore", "\u0120Roosevelt"]], "entropy": [[3.907891035079956, 1.8182647228240967]], "avg_entropy": [2.8630778789520264], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "T Ros", "score_of_biggest_cluster": 0.38371485471725464, "score_of_first_answer": 0.38371485471725464, "max_score_of_first_answer": 0.7693297266960144, "min_score_of_first_answer": 0.19138360023498535, "avg_score": 0.38371485471725464, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2889730632305145, "std_of_first_answer": 0.2889730632305145, "avg_entropy_of_first_answer": 2.8630778789520264, "lex_sim": 1.0}
{"question": "Astana is the capital of which country?", "answer": ["Kazakistan"], "scores": [[0.009458036161959171, 0.7885133624076843, 0.2346990555524826]], "normalized_score": [0.12051479518413544], "tokens": [["\u0120Kaz", "ak", "istan"]], "entropy": [[0.5426633358001709, 1.0210974216461182, 1.1491036415100098]], "avg_entropy": [0.904288132985433], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Name of Kazakhstan", "score_of_biggest_cluster": 0.12051479518413544, "score_of_first_answer": 0.12051479518413544, "max_score_of_first_answer": 0.7885133624076843, "min_score_of_first_answer": 0.009458036161959171, "avg_score": 0.12051479518413544, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32734131094240376, "std_of_first_answer": 0.32734131094240376, "avg_entropy_of_first_answer": 0.904288132985433, "lex_sim": 1.0}
{"question": "Who was the commander of the German sixth army which fought and lost the Battle of Stalingrad ?", "answer": ["Paul von Kleist"], "scores": [[0.2236984819173813, 0.05607128143310547, 0.6503164172172546, 0.9686952233314514]], "normalized_score": [0.29814592003822327], "tokens": [["\u0120Paul", "\u0120von", "\u0120Kle", "ist"]], "entropy": [[3.726924419403076, 1.1611182689666748, 2.0303165912628174, 0.21490579843521118]], "avg_entropy": [1.7833162695169449], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Friedrich Wilhelm Ernst Paulus", "score_of_biggest_cluster": 0.29814592003822327, "score_of_first_answer": 0.29814592003822327, "max_score_of_first_answer": 0.9686952233314514, "min_score_of_first_answer": 0.05607128143310547, "avg_score": 0.29814592003822327, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35816343080806035, "std_of_first_answer": 0.35816343080806035, "avg_entropy_of_first_answer": 1.7833162695169449, "lex_sim": 1.0}
{"question": "Amerigo Vespucci international airport serves which European city?", "answer": ["Thessaloniki"], "scores": [[0.0032172701321542263, 0.27410051226615906, 0.9308407306671143, 0.9129083752632141]], "normalized_score": [0.16545312106609344], "tokens": [["\u0120The", "ss", "alon", "iki"]], "entropy": [[5.157981872558594, 4.947101593017578, 0.4050659239292145, 0.5122010707855225]], "avg_entropy": [2.755587615072727], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "UN/LOCODE:ITFLR", "score_of_biggest_cluster": 0.16545312106609344, "score_of_first_answer": 0.16545312106609344, "max_score_of_first_answer": 0.9308407306671143, "min_score_of_first_answer": 0.0032172701321542263, "avg_score": 0.16545312106609344, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40319859342365205, "std_of_first_answer": 0.40319859342365205, "avg_entropy_of_first_answer": 2.755587615072727, "lex_sim": 1.0}
{"question": "Who wrote the music for 'Jerusalem'?", "answer": ["\uabd1"], "scores": [[0.0021606069058179855, 1.229820463777287e-05, 0.0023768346291035414, 0.007826653309166431]], "normalized_score": [0.0008384906686842442], "tokens": [["\u0120", "\u00ea", "\u00af", "\u0133"]], "entropy": [[4.744772911071777, 5.322286605834961, 3.317497730255127, 3.82342529296875]], "avg_entropy": [4.301995635032654], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "C. H. H. P.", "score_of_biggest_cluster": 0.0008384906686842442, "score_of_first_answer": 0.0008384906686842442, "max_score_of_first_answer": 0.007826653309166431, "min_score_of_first_answer": 1.229820463777287e-05, "avg_score": 0.0008384906686842442, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0028844601790959366, "std_of_first_answer": 0.0028844601790959366, "avg_entropy_of_first_answer": 4.301995635032654, "lex_sim": 0.0}
{"question": "Which US Olympic swimmer is nicknamed the \u2018Baltimore Bullet\u2019?", "answer": ["Allison Schmitt"], "scores": [[0.00549729447811842, 0.3347640335559845, 0.954064130783081]], "normalized_score": [0.12063921242952347], "tokens": [["\u0120Allison", "\u0120Sch", "mitt"]], "entropy": [[5.155874252319336, 4.135087013244629, 0.3466188907623291]], "avg_entropy": [3.212526718775431], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Phelps II", "score_of_biggest_cluster": 0.12063921242952347, "score_of_first_answer": 0.12063921242952347, "max_score_of_first_answer": 0.954064130783081, "min_score_of_first_answer": 0.00549729447811842, "avg_score": 0.12063921242952347, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3932384403649703, "std_of_first_answer": 0.3932384403649703, "avg_entropy_of_first_answer": 3.212526718775431, "lex_sim": 1.0}
{"question": "What river separates Texas and Mexico?", "answer": ["Rio Grande"], "scores": [[0.7933789491653442, 0.9397624135017395]], "normalized_score": [0.8634741902351379], "tokens": [["\u0120Rio", "\u0120Grande"]], "entropy": [[1.249635100364685, 0.37852901220321655]], "avg_entropy": [0.8140820562839508], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rio Grande del Norte", "score_of_biggest_cluster": 0.8634741902351379, "score_of_first_answer": 0.8634741902351379, "max_score_of_first_answer": 0.9397624135017395, "min_score_of_first_answer": 0.7933789491653442, "avg_score": 0.8634741902351379, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.07319173216819763, "std_of_first_answer": 0.07319173216819763, "avg_entropy_of_first_answer": 0.8140820562839508, "lex_sim": 1.0}
{"question": "What video game, created by Notch, is the best-selling PC game of all-time?", "answer": ["Minecraft"], "scores": [[0.7382757067680359]], "normalized_score": [0.7382757067680359], "tokens": [["\u0120Minecraft"]], "entropy": [[1.7821500301361084]], "avg_entropy": [1.7821500301361084], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Minecraft (film)", "score_of_biggest_cluster": 0.7382757067680359, "score_of_first_answer": 0.7382757067680359, "max_score_of_first_answer": 0.7382757067680359, "min_score_of_first_answer": 0.7382757067680359, "avg_score": 0.7382757067680359, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.7821500301361084, "lex_sim": 1.0}
{"question": "Which Italian city is the setting for Romeo and Juliet?", "answer": ["Pisa"], "scores": [[0.06780995428562164, 0.5716139674186707]], "normalized_score": [0.19687843322753906], "tokens": [["\u0120P", "isa"]], "entropy": [[2.513077735900879, 1.7968862056732178]], "avg_entropy": [2.1549819707870483], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "San Michele Extra", "score_of_biggest_cluster": 0.19687843322753906, "score_of_first_answer": 0.19687843322753906, "max_score_of_first_answer": 0.5716139674186707, "min_score_of_first_answer": 0.06780995428562164, "avg_score": 0.19687843322753906, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2519020065665245, "std_of_first_answer": 0.2519020065665245, "avg_entropy_of_first_answer": 2.1549819707870483, "lex_sim": 1.0}
{"question": "Ingvar Kamprad, one of the world's richest men, founded which company in the 1940's?", "answer": ["Arrma"], "scores": [[0.0005847266875207424, 0.0012497813440859318, 0.06771980226039886]], "normalized_score": [0.00367142166942358], "tokens": [["\u0120Ar", "r", "ma"]], "entropy": [[1.3071885108947754, 2.162933349609375, 4.152073383331299]], "avg_entropy": [2.54073174794515], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ikea International Group", "score_of_biggest_cluster": 0.00367142166942358, "score_of_first_answer": 0.00367142166942358, "max_score_of_first_answer": 0.06771980226039886, "min_score_of_first_answer": 0.0005847266875207424, "avg_score": 0.00367142166942358, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0314921936543846, "std_of_first_answer": 0.0314921936543846, "avg_entropy_of_first_answer": 2.54073174794515, "lex_sim": 1.0}
{"question": "What standard test characteristic has been produced in genetically modified laboratory animals so that researchers can see easily that genetic modification has occurred?", "answer": ["Transgene"], "scores": [[0.0703665018081665, 0.12120691686868668, 0.9621540904045105]], "normalized_score": [0.20170311629772186], "tokens": [["\u0120Trans", "g", "ene"]], "entropy": [[6.425084114074707, 1.2019473314285278, 0.3141654431819916]], "avg_entropy": [2.6470656295617423], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Glow-in-the-dark", "score_of_biggest_cluster": 0.20170311629772186, "score_of_first_answer": 0.20170311629772186, "max_score_of_first_answer": 0.9621540904045105, "min_score_of_first_answer": 0.0703665018081665, "avg_score": 0.20170311629772186, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4089365611838561, "std_of_first_answer": 0.4089365611838561, "avg_entropy_of_first_answer": 2.6470656295617423, "lex_sim": 1.0}
{"question": "Which Roman Emperor succeeded his father Vespasian in 79?", "answer": ["Titus"], "scores": [[0.6678100824356079]], "normalized_score": [0.6678100824356079], "tokens": [["\u0120Titus"]], "entropy": [[1.8275922536849976]], "avg_entropy": [1.8275922536849976], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Roman Emperor Titus", "score_of_biggest_cluster": 0.6678100824356079, "score_of_first_answer": 0.6678100824356079, "max_score_of_first_answer": 0.6678100824356079, "min_score_of_first_answer": 0.6678100824356079, "avg_score": 0.6678100824356079, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.8275922536849976, "lex_sim": 1.0}
{"question": "What is the name of 11 Roman Emperors, the first of which transferred the capital to Byzantium in 330 AD, and the last of which died when it was captured by the Turks in 1453?", "answer": ["Justinian I"], "scores": [[0.009290915913879871, 0.9530643224716187, 0.2368602752685547]], "normalized_score": [0.12800423800945282], "tokens": [["\u0120Justin", "ian", "\u0120I"]], "entropy": [[5.006988048553467, 0.34853431582450867, 2.1832315921783447]], "avg_entropy": [2.5129179855187735], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Byzantine Constantine", "score_of_biggest_cluster": 0.12800423800945282, "score_of_first_answer": 0.12800423800945282, "max_score_of_first_answer": 0.9530643224716187, "min_score_of_first_answer": 0.009290915913879871, "avg_score": 0.12800423800945282, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4021393198980706, "std_of_first_answer": 0.4021393198980706, "avg_entropy_of_first_answer": 2.5129179855187735, "lex_sim": 1.0}
{"question": "Which 'literary' nickname has been given to the extinct species of man, discovered in Indonesia 2003, named Homo Floresiensis?", "answer": ["The Hominoid"], "scores": [[0.03367666155099869, 0.023877818137407303, 0.2518012821674347, 0.49521464109420776]], "normalized_score": [0.10006768256425858], "tokens": [["\u0120The", "\u0120H", "omin", "oid"]], "entropy": [[3.029404640197754, 2.3767757415771484, 2.152437210083008, 1.3129903078079224]], "avg_entropy": [2.217901974916458], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hobbits", "score_of_biggest_cluster": 0.10006768256425858, "score_of_first_answer": 0.10006768256425858, "max_score_of_first_answer": 0.49521464109420776, "min_score_of_first_answer": 0.023877818137407303, "avg_score": 0.10006768256425858, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19268647517512213, "std_of_first_answer": 0.19268647517512213, "avg_entropy_of_first_answer": 2.217901974916458, "lex_sim": 1.0}
{"question": "In the George Orwell novel \u2018Animal Farm\u2019 who is the owner of Foxwood Farm?", "answer": ["Mr Jones"], "scores": [[0.23663832247257233, 0.14980439841747284]], "normalized_score": [0.18828026950359344], "tokens": [["\u0120Mr", "\u0120Jones"]], "entropy": [[4.245328426361084, 3.8662161827087402]], "avg_entropy": [4.055772304534912], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mr Pilkington", "score_of_biggest_cluster": 0.18828026950359344, "score_of_first_answer": 0.18828026950359344, "max_score_of_first_answer": 0.23663832247257233, "min_score_of_first_answer": 0.14980439841747284, "avg_score": 0.18828026950359344, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.043416962027549744, "std_of_first_answer": 0.043416962027549744, "avg_entropy_of_first_answer": 4.055772304534912, "lex_sim": 1.0}
{"question": "What animated character does Gene Kelly dance with in the film Anchors Aweigh?", "answer": ["Betty Boop"], "scores": [[0.03856350854039192, 0.26634669303894043, 0.9924647808074951]], "normalized_score": [0.21682681143283844], "tokens": [["\u0120Betty", "\u0120Bo", "op"]], "entropy": [[6.099452972412109, 3.178119659423828, 0.06034787371754646]], "avg_entropy": [3.112640168517828], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jerry Mouse", "score_of_biggest_cluster": 0.21682681143283844, "score_of_first_answer": 0.21682681143283844, "max_score_of_first_answer": 0.9924647808074951, "min_score_of_first_answer": 0.03856350854039192, "avg_score": 0.21682681143283844, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4067568607299234, "std_of_first_answer": 0.4067568607299234, "avg_entropy_of_first_answer": 3.112640168517828, "lex_sim": 1.0}
{"question": "Clyde Tombaugh discovered which celestial object in our solar system in 1930?", "answer": ["Planet Pluto"], "scores": [[0.04623710364103317, 0.8251217007637024]], "normalized_score": [0.1953234076499939], "tokens": [["\u0120Planet", "\u0120Pluto"]], "entropy": [[2.852773666381836, 1.1862443685531616]], "avg_entropy": [2.019509017467499], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Operation PLUTO", "score_of_biggest_cluster": 0.1953234076499939, "score_of_first_answer": 0.1953234076499939, "max_score_of_first_answer": 0.8251217007637024, "min_score_of_first_answer": 0.04623710364103317, "avg_score": 0.1953234076499939, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3894422985613346, "std_of_first_answer": 0.3894422985613346, "avg_entropy_of_first_answer": 2.019509017467499, "lex_sim": 1.0}
{"question": "Raclette, Tilsit and Sbrinz are all types of which foodstuff?", "answer": ["All are types of cheese"], "scores": [[0.0009004740859381855, 0.13202907145023346, 0.2625509798526764, 0.98808753490448, 0.7121446132659912]], "normalized_score": [0.11704250425100327], "tokens": [["\u0120All", "\u0120are", "\u0120types", "\u0120of", "\u0120cheese"]], "entropy": [[4.3770856857299805, 4.594411373138428, 3.030400276184082, 0.1009974479675293, 2.0700485706329346]], "avg_entropy": [2.8345886707305907], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cheeses", "score_of_biggest_cluster": 0.11704250425100327, "score_of_first_answer": 0.11704250425100327, "max_score_of_first_answer": 0.98808753490448, "min_score_of_first_answer": 0.0009004740859381855, "avg_score": 0.11704250425100327, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.37186828539071226, "std_of_first_answer": 0.37186828539071226, "avg_entropy_of_first_answer": 2.8345886707305907, "lex_sim": 1.0}
{"question": "What was the name of the female that politician John Profumo had an affair with which ended his political career in 1963?", "answer": ["Christine Keeler"], "scores": [[0.8291858434677124, 0.9598238468170166, 0.9993531107902527, 0.9993922710418701]], "normalized_score": [0.9442230463027954], "tokens": [["\u0120Christine", "\u0120Ke", "el", "er"]], "entropy": [[1.3314831256866455, 0.3981631398200989, 0.008026952855288982, 0.00654140580445528]], "avg_entropy": [0.43605365604162216], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "CHRISTINE KEELER", "score_of_biggest_cluster": 0.9442230463027954, "score_of_first_answer": 0.9442230463027954, "max_score_of_first_answer": 0.9993922710418701, "min_score_of_first_answer": 0.8291858434677124, "avg_score": 0.9442230463027954, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06987562129155801, "std_of_first_answer": 0.06987562129155801, "avg_entropy_of_first_answer": 0.43605365604162216, "lex_sim": 1.0}
{"question": "Which American president was assassinated by Charles Guiteau in 1881?", "answer": ["Chester Arthur"], "scores": [[0.06765808165073395, 0.1933012157678604]], "normalized_score": [0.11436079442501068], "tokens": [["\u0120Chester", "\u0120Arthur"]], "entropy": [[1.806139588356018, 1.4779244661331177]], "avg_entropy": [1.6420320272445679], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Presidency of James Garfield", "score_of_biggest_cluster": 0.11436079442501068, "score_of_first_answer": 0.11436079442501068, "max_score_of_first_answer": 0.1933012157678604, "min_score_of_first_answer": 0.06765808165073395, "avg_score": 0.11436079442501068, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06282156705856323, "std_of_first_answer": 0.06282156705856323, "avg_entropy_of_first_answer": 1.6420320272445679, "lex_sim": 1.0}
{"question": "Which two word named plant, also known as Dionaea Muscipala is characterised by having hinged two lobed leaves that snap closed when the sensitive hairs on it's surface are touched ?", "answer": ["Dionaea"], "scores": [[0.20479467511177063, 0.9323756694793701]], "normalized_score": [0.4369731843471527], "tokens": [["\u0120Dion", "aea"]], "entropy": [[3.968397617340088, 0.6239339113235474]], "avg_entropy": [2.2961657643318176], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dionaea heterodoxa", "score_of_biggest_cluster": 0.4369731843471527, "score_of_first_answer": 0.4369731843471527, "max_score_of_first_answer": 0.9323756694793701, "min_score_of_first_answer": 0.20479467511177063, "avg_score": 0.4369731843471527, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36379049718379974, "std_of_first_answer": 0.36379049718379974, "avg_entropy_of_first_answer": 2.2961657643318176, "lex_sim": 1.0}
{"question": "In which US state is New Haven?", "answer": ["Connecticut"], "scores": [[0.9229077100753784]], "normalized_score": [0.9229077100753784], "tokens": [["\u0120Connecticut"]], "entropy": [[0.6511435508728027]], "avg_entropy": [0.6511435508728027], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Economy of Connecticut", "score_of_biggest_cluster": 0.9229077100753784, "score_of_first_answer": 0.9229077100753784, "max_score_of_first_answer": 0.9229077100753784, "min_score_of_first_answer": 0.9229077100753784, "avg_score": 0.9229077100753784, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.6511435508728027, "lex_sim": 1.0}
{"question": "In which sci fi classic did the space ship Nostromo first appear?", "answer": ["In Space, no one can hear you shout (novel)"], "scores": [[0.0033794676419347525, 0.012189969420433044, 0.1310301125049591, 0.002209680387750268, 0.14849576354026794, 0.9281357526779175, 0.9758643507957458, 0.9637888669967651, 0.008135905489325523, 0.03701040893793106, 0.003740668063983321, 0.19561311602592468, 0.37632638216018677]], "normalized_score": [0.0599142424762249], "tokens": [["\u0120In", "\u0120Space", ",", "\u0120no", "\u0120one", "\u0120can", "\u0120hear", "\u0120you", "\u0120shout", "\u0120(", "no", "vel", ")"]], "entropy": [[4.075140476226807, 4.1847991943359375, 3.876351833343506, 4.51981782913208, 3.8795623779296875, 0.5631322860717773, 0.21774376928806305, 0.24930454790592194, 1.1827566623687744, 1.7437002658843994, 5.144067764282227, 4.727238655090332, 2.466304302215576]], "avg_entropy": [2.8330707664673147], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alien (disambiguation)", "score_of_biggest_cluster": 0.0599142424762249, "score_of_first_answer": 0.0599142424762249, "max_score_of_first_answer": 0.9758643507957458, "min_score_of_first_answer": 0.002209680387750268, "avg_score": 0.0599142424762249, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3783368061817433, "std_of_first_answer": 0.3783368061817433, "avg_entropy_of_first_answer": 2.8330707664673147, "lex_sim": 1.0}
{"question": "Which golfer won the Open Championship five times between 1975 and 1983?", "answer": ["Gary Player"], "scores": [[0.05784986913204193, 0.9874545335769653]], "normalized_score": [0.23900653421878815], "tokens": [["\u0120Gary", "\u0120Player"]], "entropy": [[3.966986656188965, 0.1368941366672516]], "avg_entropy": [2.051940396428108], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tommy Watson", "score_of_biggest_cluster": 0.23900653421878815, "score_of_first_answer": 0.23900653421878815, "max_score_of_first_answer": 0.9874545335769653, "min_score_of_first_answer": 0.05784986913204193, "avg_score": 0.23900653421878815, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4648023322224617, "std_of_first_answer": 0.4648023322224617, "avg_entropy_of_first_answer": 2.051940396428108, "lex_sim": 1.0}
{"question": "Which American won the Nobel Peace Prize in 2002?", "answer": ["Steven Chu"], "scores": [[0.014054902829229832, 0.6011015772819519]], "normalized_score": [0.0919153243303299], "tokens": [["\u0120Steven", "\u0120Chu"]], "entropy": [[5.436072826385498, 2.735816240310669]], "avg_entropy": [4.0859445333480835], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "James Earl Carter Jr.", "score_of_biggest_cluster": 0.0919153243303299, "score_of_first_answer": 0.0919153243303299, "max_score_of_first_answer": 0.6011015772819519, "min_score_of_first_answer": 0.014054902829229832, "avg_score": 0.0919153243303299, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29352333722636104, "std_of_first_answer": 0.29352333722636104, "avg_entropy_of_first_answer": 4.0859445333480835, "lex_sim": 1.0}
{"question": "The song 'Bewitched, Bothered And Bewildered' came from which musical?", "answer": ["The Wizard of Oz"], "scores": [[0.12409447878599167, 0.5850093364715576, 0.6789280772209167, 0.9567306637763977]], "normalized_score": [0.46599599719047546], "tokens": [["\u0120The", "\u0120Wizard", "\u0120of", "\u0120Oz"]], "entropy": [[5.121304512023926, 3.081266403198242, 0.7384083867073059, 0.268717497587204]], "avg_entropy": [2.3024241998791695], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pal Joey (disambiguation)", "score_of_biggest_cluster": 0.46599599719047546, "score_of_first_answer": 0.46599599719047546, "max_score_of_first_answer": 0.9567306637763977, "min_score_of_first_answer": 0.12409447878599167, "avg_score": 0.46599599719047546, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2997642403495061, "std_of_first_answer": 0.2997642403495061, "avg_entropy_of_first_answer": 2.3024241998791695, "lex_sim": 1.0}
{"question": "What movie takes place at Shermer High School's detention?", "answer": ["The Breakfast Club (film)"], "scores": [[0.08469495922327042, 0.35343044996261597, 0.9834670424461365, 0.0461120679974556, 0.004899649880826473, 0.8947296738624573]], "normalized_score": [0.1346166580915451], "tokens": [["\u0120The", "\u0120Breakfast", "\u0120Club", "\u0120(", "film", ")"]], "entropy": [[5.797135353088379, 4.704995155334473, 0.15940362215042114, 0.5595738887786865, 2.2600579261779785, 0.7757555246353149]], "avg_entropy": [2.3761535783608756], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Carl Reed", "score_of_biggest_cluster": 0.1346166580915451, "score_of_first_answer": 0.1346166580915451, "max_score_of_first_answer": 0.9834670424461365, "min_score_of_first_answer": 0.004899649880826473, "avg_score": 0.1346166580915451, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4016508310157785, "std_of_first_answer": 0.4016508310157785, "avg_entropy_of_first_answer": 2.3761535783608756, "lex_sim": 1.0}
{"question": "Which Tennis star was Brooke Shields once married to?", "answer": ["Venus Williams"], "scores": [[0.00667657982558012, 0.47571149468421936]], "normalized_score": [0.05635712295770645], "tokens": [["\u0120Venus", "\u0120Williams"]], "entropy": [[4.455523490905762, 2.262387752532959]], "avg_entropy": [3.3589556217193604], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Andre Agassi College Preparatory Academy", "score_of_biggest_cluster": 0.05635712295770645, "score_of_first_answer": 0.05635712295770645, "max_score_of_first_answer": 0.47571149468421936, "min_score_of_first_answer": 0.00667657982558012, "avg_score": 0.05635712295770645, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23451745742931962, "std_of_first_answer": 0.23451745742931962, "avg_entropy_of_first_answer": 3.3589556217193604, "lex_sim": 1.0}
{"question": "What name is given to the person in a restaurant responsible for the wine and assisting customers with their choice of wine?", "answer": ["Sommelier"], "scores": [[0.4581329822540283, 0.997262716293335, 0.9987773299217224]], "normalized_score": [0.7698804140090942], "tokens": [["\u0120S", "ommel", "ier"]], "entropy": [[2.7831568717956543, 0.02073059417307377, 0.01215889398008585]], "avg_entropy": [0.9386821199829379], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wine Butler", "score_of_biggest_cluster": 0.7698804140090942, "score_of_first_answer": 0.7698804140090942, "max_score_of_first_answer": 0.9987773299217224, "min_score_of_first_answer": 0.4581329822540283, "avg_score": 0.7698804140090942, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2545059429222655, "std_of_first_answer": 0.2545059429222655, "avg_entropy_of_first_answer": 0.9386821199829379, "lex_sim": 1.0}
{"question": "Where was the final of the 1999 Rugby World Cup held?", "answer": ["New Zealand"], "scores": [[0.035102494060993195, 0.6763525009155273]], "normalized_score": [0.15408329665660858], "tokens": [["\u0120New", "\u0120Zealand"]], "entropy": [[4.869754791259766, 1.37655508518219]], "avg_entropy": [3.123154938220978], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Welsh nation", "score_of_biggest_cluster": 0.15408329665660858, "score_of_first_answer": 0.15408329665660858, "max_score_of_first_answer": 0.6763525009155273, "min_score_of_first_answer": 0.035102494060993195, "avg_score": 0.15408329665660858, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3206250034272671, "std_of_first_answer": 0.3206250034272671, "avg_entropy_of_first_answer": 3.123154938220978, "lex_sim": 1.0}
{"question": "What is the name of the woollen gown worn by a junior barrister in the UK who has not taken silk?", "answer": ["Livery gown"], "scores": [[0.007717595901340246, 0.5134085416793823, 0.04778748378157616]], "normalized_score": [0.057423073798418045], "tokens": [["\u0120L", "ivery", "\u0120gown"]], "entropy": [[6.657011032104492, 2.9674696922302246, 1.2826933860778809]], "avg_entropy": [3.6357247034708657], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stuff Gown", "score_of_biggest_cluster": 0.057423073798418045, "score_of_first_answer": 0.057423073798418045, "max_score_of_first_answer": 0.5134085416793823, "min_score_of_first_answer": 0.007717595901340246, "avg_score": 0.057423073798418045, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.22952412076451126, "std_of_first_answer": 0.22952412076451126, "avg_entropy_of_first_answer": 3.6357247034708657, "lex_sim": 1.0}
{"question": "Between 1987 and 2006, Ratu Sir Kamisese Mara, Commodore Frank Bainimarama, Mahendra Chaudhry, Laisenia Qarase, Lieutenant Colonel Sitiveni Rabuka, George Speight and Ratu Josefa Iloilo were involved in the politics of which country?", "answer": ["Solomon Islands"], "scores": [[0.1604417860507965, 0.9573790431022644]], "normalized_score": [0.3919229805469513], "tokens": [["\u0120Solomon", "\u0120Islands"]], "entropy": [[2.2235889434814453, 0.32658740878105164]], "avg_entropy": [1.2750881761312485], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fiji Islands", "score_of_biggest_cluster": 0.3919229805469513, "score_of_first_answer": 0.3919229805469513, "max_score_of_first_answer": 0.9573790431022644, "min_score_of_first_answer": 0.1604417860507965, "avg_score": 0.3919229805469513, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39846862852573395, "std_of_first_answer": 0.39846862852573395, "avg_entropy_of_first_answer": 1.2750881761312485, "lex_sim": 1.0}
{"question": "What was 'Bifrost', the bridge that led to 'Asgard' in Scandinavian mythology?", "answer": ["\u3148"], "scores": [[0.001682775910012424, 1.664271258050576e-05, 0.4740780293941498, 0.0752016007900238]], "normalized_score": [0.005621235352009535], "tokens": [["\u0120", "\u00e3", "\u0127", "\u012a"]], "entropy": [[5.3386430740356445, 4.968572616577148, 2.2438628673553467, 3.392080068588257]], "avg_entropy": [3.985789656639099], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Colors of rainbow", "score_of_biggest_cluster": 0.005621235352009535, "score_of_first_answer": 0.005621235352009535, "max_score_of_first_answer": 0.4740780293941498, "min_score_of_first_answer": 1.664271258050576e-05, "avg_score": 0.005621235352009535, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19654109791043917, "std_of_first_answer": 0.19654109791043917, "avg_entropy_of_first_answer": 3.985789656639099, "lex_sim": 0.0}
{"question": "Which country since the late 1900s has governed/overseen the increasing independence of the New Caledonia archipelago, east of Australia?", "answer": ["France"], "scores": [[0.33462339639663696]], "normalized_score": [0.33462339639663696], "tokens": [["\u0120France"]], "entropy": [[2.580087423324585]], "avg_entropy": [2.580087423324585], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "La Republique francaise", "score_of_biggest_cluster": 0.33462339639663696, "score_of_first_answer": 0.33462339639663696, "max_score_of_first_answer": 0.33462339639663696, "min_score_of_first_answer": 0.33462339639663696, "avg_score": 0.33462339639663696, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.580087423324585, "lex_sim": 1.0}
{"question": "Which Dickens novel takes place during the French revolution", "answer": ["Oliver Twist"], "scores": [[0.058964937925338745, 0.9686227440834045]], "normalized_score": [0.23898698389530182], "tokens": [["\u0120Oliver", "\u0120Twist"]], "entropy": [[3.2262277603149414, 0.2878674268722534]], "avg_entropy": [1.7570475935935974], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "It was the best of times, it was the worst of times", "score_of_biggest_cluster": 0.23898698389530182, "score_of_first_answer": 0.23898698389530182, "max_score_of_first_answer": 0.9686227440834045, "min_score_of_first_answer": 0.058964937925338745, "avg_score": 0.23898698389530182, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4548289030790329, "std_of_first_answer": 0.4548289030790329, "avg_entropy_of_first_answer": 1.7570475935935974, "lex_sim": 1.0}
{"question": "What product is advertised with the slogan \"Great taste, less filling\"?", "answer": ["Diet sodas"], "scores": [[0.017358748242259026, 0.009048081934452057, 0.9911091327667236]], "normalized_score": [0.05379379540681839], "tokens": [["\u0120Diet", "\u0120sod", "as"]], "entropy": [[6.817251205444336, 3.939281702041626, 0.08090312033891678]], "avg_entropy": [3.612478675941626], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Miller Lite Beer", "score_of_biggest_cluster": 0.05379379540681839, "score_of_first_answer": 0.05379379540681839, "max_score_of_first_answer": 0.9911091327667236, "min_score_of_first_answer": 0.009048081934452057, "avg_score": 0.05379379540681839, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4610016613729774, "std_of_first_answer": 0.4610016613729774, "avg_entropy_of_first_answer": 3.612478675941626, "lex_sim": 1.0}
{"question": "In the Sudanese wars, which British general won the Battle of Omdurman?", "answer": ["Lord Kitchener"], "scores": [[0.045588359236717224, 0.8441271185874939, 0.9982720613479614]], "normalized_score": [0.3374195396900177], "tokens": [["\u0120Lord", "\u0120Kitchen", "er"]], "entropy": [[4.405013084411621, 0.993241548538208, 0.015114184468984604]], "avg_entropy": [1.8044562724729378], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "(Sir Herbert) Kitchener", "score_of_biggest_cluster": 0.3374195396900177, "score_of_first_answer": 0.3374195396900177, "max_score_of_first_answer": 0.9982720613479614, "min_score_of_first_answer": 0.045588359236717224, "avg_score": 0.3374195396900177, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4175365652427596, "std_of_first_answer": 0.4175365652427596, "avg_entropy_of_first_answer": 1.8044562724729378, "lex_sim": 1.0}
{"question": "Which pop singer shot himself to death in 1990 whilst suffering from depression?", "answer": ["Kurt Cobain"], "scores": [[0.08809782564640045, 0.9549647569656372, 0.9987159967422485]], "normalized_score": [0.437990665435791], "tokens": [["\u0120Kurt", "\u0120Cob", "ain"]], "entropy": [[5.177672863006592, 0.40407776832580566, 0.014608386904001236]], "avg_entropy": [1.8654530060787995], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "DEL SHANNON", "score_of_biggest_cluster": 0.437990665435791, "score_of_first_answer": 0.437990665435791, "max_score_of_first_answer": 0.9987159967422485, "min_score_of_first_answer": 0.08809782564640045, "avg_score": 0.437990665435791, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41933782471452247, "std_of_first_answer": 0.41933782471452247, "avg_entropy_of_first_answer": 1.8654530060787995, "lex_sim": 1.0}
{"question": "Which group of around 200 Greek islands in the Aegean Sea includes Andros, Naxos, Paros and Milos?", "answer": ["Cyclades"], "scores": [[0.449561208486557, 0.9069802761077881]], "normalized_score": [0.6385476589202881], "tokens": [["\u0120Cycl", "ades"]], "entropy": [[3.3128199577331543, 0.39094823598861694]], "avg_entropy": [1.8518840968608856], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cyclades Prefecture", "score_of_biggest_cluster": 0.6385476589202881, "score_of_first_answer": 0.6385476589202881, "max_score_of_first_answer": 0.9069802761077881, "min_score_of_first_answer": 0.449561208486557, "avg_score": 0.6385476589202881, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.22870953381061554, "std_of_first_answer": 0.22870953381061554, "avg_entropy_of_first_answer": 1.8518840968608856, "lex_sim": 1.0}
{"question": "Which is the fourth largest of the Balearic Islands?", "answer": ["Majorca"], "scores": [[0.5814192891120911, 0.9993315935134888]], "normalized_score": [0.7622536420822144], "tokens": [["\u0120Major", "ca"]], "entropy": [[1.4839179515838623, 0.00741933099925518]], "avg_entropy": [0.7456686412915587], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Formentera", "score_of_biggest_cluster": 0.7622536420822144, "score_of_first_answer": 0.7622536420822144, "max_score_of_first_answer": 0.9993315935134888, "min_score_of_first_answer": 0.5814192891120911, "avg_score": 0.7622536420822144, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20895615220069885, "std_of_first_answer": 0.20895615220069885, "avg_entropy_of_first_answer": 0.7456686412915587, "lex_sim": 1.0}
{"question": "John Calvin, 16th century French theologian, was the autocrat of which city for 13 years, where he improved the city and provided harsh punishments for moral transgressions?", "answer": ["Geneva"], "scores": [[0.48304232954978943]], "normalized_score": [0.48304232954978943], "tokens": [["\u0120Geneva"]], "entropy": [[2.512601137161255]], "avg_entropy": [2.512601137161255], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Geneva Palexpo", "score_of_biggest_cluster": 0.48304232954978943, "score_of_first_answer": 0.48304232954978943, "max_score_of_first_answer": 0.48304232954978943, "min_score_of_first_answer": 0.48304232954978943, "avg_score": 0.48304232954978943, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.512601137161255, "lex_sim": 1.0}
{"question": "What building in Scotland is said to house the Holy Grail and features prominently at the end of The Da Vinci Code?", "answer": ["Rosslyn Chapel"], "scores": [[0.04866502061486244, 0.9973710775375366, 0.92148756980896]], "normalized_score": [0.35496681928634644], "tokens": [["\u0120Ross", "lyn", "\u0120Chapel"]], "entropy": [[4.813345909118652, 0.03048623353242874, 0.5412683486938477]], "avg_entropy": [1.7950334971149762], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cathedral of Codes", "score_of_biggest_cluster": 0.35496681928634644, "score_of_first_answer": 0.35496681928634644, "max_score_of_first_answer": 0.9973710775375366, "min_score_of_first_answer": 0.04866502061486244, "avg_score": 0.35496681928634644, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4304546293769781, "std_of_first_answer": 0.4304546293769781, "avg_entropy_of_first_answer": 1.7950334971149762, "lex_sim": 1.0}
{"question": "Adam Richman takes on food challenges in which US television series?", "answer": ["Man v. Food"], "scores": [[0.7366663217544556, 0.6293312907218933, 0.5591419339179993, 0.9653509259223938]], "normalized_score": [0.7072767019271851], "tokens": [["\u0120Man", "\u0120v", ".", "\u0120Food"]], "entropy": [[1.5779832601547241, 1.1470141410827637, 1.6559454202651978, 0.24342170357704163]], "avg_entropy": [1.1560911312699318], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Man vs Food", "score_of_biggest_cluster": 0.7072767019271851, "score_of_first_answer": 0.7072767019271851, "max_score_of_first_answer": 0.9653509259223938, "min_score_of_first_answer": 0.5591419339179993, "avg_score": 0.7072767019271851, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.15373960362837635, "std_of_first_answer": 0.15373960362837635, "avg_entropy_of_first_answer": 1.1560911312699318, "lex_sim": 1.0}
{"question": "What is the US state capital of Vermont?", "answer": ["Montpelier"], "scores": [[0.9580471515655518, 0.7034629583358765, 0.9958574175834656]], "normalized_score": [0.875538170337677], "tokens": [["\u0120Mont", "pel", "ier"]], "entropy": [[0.3268221318721771, 1.8223330974578857, 0.040280912071466446]], "avg_entropy": [0.7298120471338431], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mountpelier", "score_of_biggest_cluster": 0.875538170337677, "score_of_first_answer": 0.875538170337677, "max_score_of_first_answer": 0.9958574175834656, "min_score_of_first_answer": 0.7034629583358765, "avg_score": 0.875538170337677, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12984488500490804, "std_of_first_answer": 0.12984488500490804, "avg_entropy_of_first_answer": 0.7298120471338431, "lex_sim": 1.0}
{"question": "\"How many adjectives are there in the sentence \"\"Jack ran down the path.\"\"?\"", "answer": ["\u32c1 jack ran down the path"], "scores": [[0.00804511271417141, 1.355096264887834e-05, 0.04994069039821625, 0.027853256091475487, 0.001484299311414361, 0.32545143365859985, 0.8340225219726562, 0.8605959415435791, 0.9418076276779175]], "normalized_score": [0.03323856741189957], "tokens": [["\u0120", "\u00e3", "\u012d", "\u0123", "\u0120jack", "\u0120ran", "\u0120down", "\u0120the", "\u0120path"]], "entropy": [[3.4164035320281982, 4.059067726135254, 1.928058385848999, 3.8320884704589844, 4.256882667541504, 3.3220033645629883, 1.131293535232544, 0.9075015783309937, 0.5223221778869629]], "avg_entropy": [2.597291270891825], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "None", "score_of_biggest_cluster": 0.03323856741189957, "score_of_first_answer": 0.03323856741189957, "max_score_of_first_answer": 0.9418076276779175, "min_score_of_first_answer": 1.355096264887834e-05, "avg_score": 0.03323856741189957, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39432040664287804, "std_of_first_answer": 0.39432040664287804, "avg_entropy_of_first_answer": 2.597291270891825, "lex_sim": 1.0}
{"question": "What popular opera, penned by George Bizet, is a story about a female cigarette factory worker?", "answer": ["Carmen"], "scores": [[0.8107879757881165]], "normalized_score": [0.8107879757881165], "tokens": [["\u0120Carmen"]], "entropy": [[1.0738565921783447]], "avg_entropy": [1.0738565921783447], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Les tringles des sistres tintaient", "score_of_biggest_cluster": 0.8107879757881165, "score_of_first_answer": 0.8107879757881165, "max_score_of_first_answer": 0.8107879757881165, "min_score_of_first_answer": 0.8107879757881165, "avg_score": 0.8107879757881165, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.0738565921783447, "lex_sim": 1.0}
{"question": "Who wrote the BBC tv serial drama \u2018The Singing Detective\u2019 (1986)?", "answer": ["Alan Moore"], "scores": [[0.03431958332657814, 0.21285013854503632]], "normalized_score": [0.08546886593103409], "tokens": [["\u0120Alan", "\u0120Moore"]], "entropy": [[5.2359209060668945, 2.6789755821228027]], "avg_entropy": [3.9574482440948486], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dennis C. G. Potter", "score_of_biggest_cluster": 0.08546886593103409, "score_of_first_answer": 0.08546886593103409, "max_score_of_first_answer": 0.21285013854503632, "min_score_of_first_answer": 0.03431958332657814, "avg_score": 0.08546886593103409, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08926527760922909, "std_of_first_answer": 0.08926527760922909, "avg_entropy_of_first_answer": 3.9574482440948486, "lex_sim": 1.0}
{"question": "What was the name of the island where Nelson Mandela was imprisoned for 18 years?", "answer": ["Robben Island"], "scores": [[0.9803240895271301, 0.9957938194274902, 0.9621272087097168]], "normalized_score": [0.9793182611465454], "tokens": [["\u0120Rob", "ben", "\u0120Island"]], "entropy": [[0.19385726749897003, 0.043621957302093506, 0.25248968601226807]], "avg_entropy": [0.16332297027111053], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Robbeneiland", "score_of_biggest_cluster": 0.9793182611465454, "score_of_first_answer": 0.9793182611465454, "max_score_of_first_answer": 0.9957938194274902, "min_score_of_first_answer": 0.9621272087097168, "avg_score": 0.9793182611465454, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.013759359206232995, "std_of_first_answer": 0.013759359206232995, "avg_entropy_of_first_answer": 0.16332297027111053, "lex_sim": 1.0}
{"question": "The cover of which of Pink Floyd\u2019s LP covers features Battersea Power Station?", "answer": ["The Dark Side of the Moon"], "scores": [[0.16298210620880127, 0.5380344986915588, 0.9880471229553223, 0.9264073967933655, 0.9491944909095764, 0.9911954402923584]], "normalized_score": [0.6501407027244568], "tokens": [["\u0120The", "\u0120Dark", "\u0120Side", "\u0120of", "\u0120the", "\u0120Moon"]], "entropy": [[3.4125494956970215, 3.00866961479187, 0.07756900787353516, 0.3105570375919342, 0.2343577891588211, 0.08729104697704315]], "avg_entropy": [1.1884989986817043], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Animal Phylogeny", "score_of_biggest_cluster": 0.6501407027244568, "score_of_first_answer": 0.6501407027244568, "max_score_of_first_answer": 0.9911954402923584, "min_score_of_first_answer": 0.16298210620880127, "avg_score": 0.6501407027244568, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30947239864354376, "std_of_first_answer": 0.30947239864354376, "avg_entropy_of_first_answer": 1.1884989986817043, "lex_sim": 1.0}
{"question": "Dave and Ansell Collins had a U.K No 1 hit with which song?", "answer": ["You Really Got Me"], "scores": [[0.016431337222456932, 0.01636359468102455, 0.9827852845191956, 0.9847944378852844]], "normalized_score": [0.1270103007555008], "tokens": [["\u0120You", "\u0120Really", "\u0120Got", "\u0120Me"]], "entropy": [[5.368434906005859, 4.045715808868408, 0.14909183979034424, 0.1259552538394928]], "avg_entropy": [2.422299452126026], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Double barrel", "score_of_biggest_cluster": 0.1270103007555008, "score_of_first_answer": 0.1270103007555008, "max_score_of_first_answer": 0.9847944378852844, "min_score_of_first_answer": 0.01636359468102455, "avg_score": 0.1270103007555008, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4836967198130581, "std_of_first_answer": 0.4836967198130581, "avg_entropy_of_first_answer": 2.422299452126026, "lex_sim": 1.0}
{"question": "What is the capital city of Saudi Arabia?", "answer": ["Riyad"], "scores": [[0.014237252064049244, 0.6752679944038391]], "normalized_score": [0.0980507880449295], "tokens": [["\u0120Riy", "ad"]], "entropy": [[1.5291244983673096, 1.694337010383606]], "avg_entropy": [1.6117307543754578], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "RIYADH", "score_of_biggest_cluster": 0.0980507880449295, "score_of_first_answer": 0.0980507880449295, "max_score_of_first_answer": 0.6752679944038391, "min_score_of_first_answer": 0.014237252064049244, "avg_score": 0.0980507880449295, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.33051537116989493, "std_of_first_answer": 0.33051537116989493, "avg_entropy_of_first_answer": 1.6117307543754578, "lex_sim": 1.0}
{"question": "Angela Grisanti Vecchio, Stanley Zbornak, Clayton Hollingsworth and Salvadore Petrillo are characters in what US television series?", "answer": ["S.W.A.T., NYPD Blue"], "scores": [[0.0024237744510173798, 0.1146991029381752, 0.632923424243927, 0.9699811935424805, 0.988072395324707, 0.9916969537734985, 0.99695885181427, 0.009579548612236977, 0.05322283133864403, 0.9179250001907349]], "normalized_score": [0.19463732838630676], "tokens": [["\u0120S", ".", "W", ".", "A", ".", "T", ".,", "\u0120NYPD", "\u0120Blue"]], "entropy": [[5.050728797912598, 4.916227340698242, 1.9176961183547974, 0.31353282928466797, 0.10988505184650421, 0.07821955531835556, 0.02719809114933014, 0.8884106874465942, 5.729829788208008, 0.5511390566825867]], "avg_entropy": [1.9582867316901684], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Golden Girls", "score_of_biggest_cluster": 0.19463732838630676, "score_of_first_answer": 0.19463732838630676, "max_score_of_first_answer": 0.99695885181427, "min_score_of_first_answer": 0.0024237744510173798, "avg_score": 0.19463732838630676, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4393658532285313, "std_of_first_answer": 0.4393658532285313, "avg_entropy_of_first_answer": 1.9582867316901684, "lex_sim": 1.0}
{"question": "In which of Shakespeare\u2019s plays does the fool Feste appear?", "answer": ["Twelfth Night"], "scores": [[0.2313278168439865, 0.9793591499328613, 0.8587213754653931]], "normalized_score": [0.5794385671615601], "tokens": [["\u0120Tw", "elfth", "\u0120Night"]], "entropy": [[3.5389955043792725, 0.16969233751296997, 0.891787052154541]], "avg_entropy": [1.5334916313489277], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Twelth Night", "score_of_biggest_cluster": 0.5794385671615601, "score_of_first_answer": 0.5794385671615601, "max_score_of_first_answer": 0.9793591499328613, "min_score_of_first_answer": 0.2313278168439865, "avg_score": 0.5794385671615601, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.327910391775764, "std_of_first_answer": 0.327910391775764, "avg_entropy_of_first_answer": 1.5334916313489277, "lex_sim": 1.0}
{"question": "What is an alternative name for vitamin B9, essential in promoting cell growth, particularly during pregnancy?", "answer": ["Pregciclovir"], "scores": [[0.03145195171236992, 0.4314222037792206, 0.004122854210436344, 0.09640754759311676, 0.0456121563911438, 0.932928204536438]], "normalized_score": [0.07824644446372986], "tokens": [["\u0120P", "reg", "c", "ic", "lov", "ir"]], "entropy": [[3.2276010513305664, 2.5747761726379395, 4.291280746459961, 4.617186069488525, 4.063233375549316, 0.49807876348495483]], "avg_entropy": [3.2120260298252106], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Folic acid chemdata supplement", "score_of_biggest_cluster": 0.07824644446372986, "score_of_first_answer": 0.07824644446372986, "max_score_of_first_answer": 0.932928204536438, "min_score_of_first_answer": 0.004122854210436344, "avg_score": 0.07824644446372986, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3348122786224345, "std_of_first_answer": 0.3348122786224345, "avg_entropy_of_first_answer": 3.2120260298252106, "lex_sim": 1.0}
{"question": "What are the French speaking citizens of Belgium called?", "answer": ["Walloon"], "scores": [[0.42535343766212463, 0.08446983993053436]], "normalized_score": [0.18955089151859283], "tokens": [["\u0120Wall", "oon"]], "entropy": [[2.6670279502868652, 0.6207131147384644]], "avg_entropy": [1.6438705325126648], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wallonian", "score_of_biggest_cluster": 0.18955089151859283, "score_of_first_answer": 0.18955089151859283, "max_score_of_first_answer": 0.42535343766212463, "min_score_of_first_answer": 0.08446983993053436, "avg_score": 0.18955089151859283, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.17044179886579514, "std_of_first_answer": 0.17044179886579514, "avg_entropy_of_first_answer": 1.6438705325126648, "lex_sim": 1.0}
{"question": "What is the name of the monster\u2019s creator in the 1818 novel of the same name by Mary Shelley?", "answer": ["The Monster of the Alps"], "scores": [[0.05031455680727959, 0.0417306050658226, 0.10561200976371765, 0.24600179493427277, 0.11737972497940063]], "normalized_score": [0.09147001057863235], "tokens": [["\u0120The", "\u0120Monster", "\u0120of", "\u0120the", "\u0120Alps"]], "entropy": [[4.727629661560059, 5.059404373168945, 0.8198695182800293, 2.6951773166656494, 5.313056945800781]], "avg_entropy": [3.723027563095093], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Frankenstein (Victor)", "score_of_biggest_cluster": 0.09147001057863235, "score_of_first_answer": 0.09147001057863235, "max_score_of_first_answer": 0.24600179493427277, "min_score_of_first_answer": 0.0417306050658226, "avg_score": 0.09147001057863235, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07316952220151507, "std_of_first_answer": 0.07316952220151507, "avg_entropy_of_first_answer": 3.723027563095093, "lex_sim": 1.0}
{"question": "Who was the 8th President of the United States from 1837- 1841?", "answer": ["Jackson"], "scores": [[0.001258106087334454]], "normalized_score": [0.001258106087334454], "tokens": [["\u0120Jackson"]], "entropy": [[2.45684814453125]], "avg_entropy": [2.45684814453125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Martin Van Buren", "score_of_biggest_cluster": 0.001258106087334454, "score_of_first_answer": 0.001258106087334454, "max_score_of_first_answer": 0.001258106087334454, "min_score_of_first_answer": 0.001258106087334454, "avg_score": 0.001258106087334454, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.45684814453125, "lex_sim": 1.0}
{"question": "\"Who dubbed the singing voice for Audrey Hepburn in the film version of \"\"My Fair Lady\"\"?\"", "answer": ["No one"], "scores": [[0.0071572656743228436, 0.561531126499176]], "normalized_score": [0.06339580565690994], "tokens": [["\u0120No", "\u0120one"]], "entropy": [[5.434836387634277, 2.770176410675049]], "avg_entropy": [4.102506399154663], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marnie Nixon", "score_of_biggest_cluster": 0.06339580565690994, "score_of_first_answer": 0.06339580565690994, "max_score_of_first_answer": 0.561531126499176, "min_score_of_first_answer": 0.0071572656743228436, "avg_score": 0.06339580565690994, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2771869304124266, "std_of_first_answer": 0.2771869304124266, "avg_entropy_of_first_answer": 4.102506399154663, "lex_sim": 1.0}
{"question": "What stone was used to decipher Ancient Egyptian hieroglyphics?", "answer": ["Rosette"], "scores": [[0.44815492630004883, 0.016725581139326096]], "normalized_score": [0.08657743781805038], "tokens": [["\u0120Ros", "ette"]], "entropy": [[3.9404168128967285, 0.1413661241531372]], "avg_entropy": [2.040891468524933], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stone of Rosetta", "score_of_biggest_cluster": 0.08657743781805038, "score_of_first_answer": 0.08657743781805038, "max_score_of_first_answer": 0.44815492630004883, "min_score_of_first_answer": 0.016725581139326096, "avg_score": 0.08657743781805038, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.21571467258036137, "std_of_first_answer": 0.21571467258036137, "avg_entropy_of_first_answer": 2.040891468524933, "lex_sim": 1.0}
{"question": "The disease pertussis is more commonly known by what name?", "answer": ["whooping cough..."], "scores": [[0.2214386761188507, 0.9960979223251343, 0.9688352942466736, 5.34795290150214e-05]], "normalized_score": [0.05814312398433685], "tokens": [["\u0120who", "oping", "\u0120cough", "..."]], "entropy": [[1.270164966583252, 0.0334734283387661, 0.2187248021364212, 0.2654533386230469]], "avg_entropy": [0.44695413392037153], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "100 days' cough", "score_of_biggest_cluster": 0.05814312398433685, "score_of_first_answer": 0.05814312398433685, "max_score_of_first_answer": 0.9960979223251343, "min_score_of_first_answer": 5.34795290150214e-05, "avg_score": 0.05814312398433685, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4429373576783552, "std_of_first_answer": 0.4429373576783552, "avg_entropy_of_first_answer": 0.44695413392037153, "lex_sim": 1.0}
{"question": "Who memorably failed to win the Marathon in the 1954 Commonwealth Games?", "answer": ["Fred Whitfield"], "scores": [[0.007565174717456102, 0.031148621812462807, 0.1318260282278061]], "normalized_score": [0.03143544867634773], "tokens": [["\u0120Fred", "\u0120Whit", "field"]], "entropy": [[6.053600311279297, 5.693799018859863, 2.34043288230896]], "avg_entropy": [4.69594407081604], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jim Peters", "score_of_biggest_cluster": 0.03143544867634773, "score_of_first_answer": 0.03143544867634773, "max_score_of_first_answer": 0.1318260282278061, "min_score_of_first_answer": 0.007565174717456102, "avg_score": 0.03143544867634773, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05388555656257254, "std_of_first_answer": 0.05388555656257254, "avg_entropy_of_first_answer": 4.69594407081604, "lex_sim": 1.0}
{"question": "In which Canadian city is the Lion's Gate Bridge?", "answer": ["Niagara Falls, NY, USA"], "scores": [[0.017352286726236343, 0.8019354343414307, 0.22504881024360657, 0.041205715388059616, 0.05345369130373001, 0.6616827249526978]], "normalized_score": [0.12879320979118347], "tokens": [["\u0120Niagara", "\u0120Falls", ",", "\u0120NY", ",", "\u0120USA"]], "entropy": [[3.9172611236572266, 0.8429180979728699, 0.8013849258422852, 1.8872430324554443, 0.6189110279083252, 1.53704833984375]], "avg_entropy": [1.6007944246133168], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Vancouver, Canada", "score_of_biggest_cluster": 0.12879320979118347, "score_of_first_answer": 0.12879320979118347, "max_score_of_first_answer": 0.8019354343414307, "min_score_of_first_answer": 0.017352286726236343, "avg_score": 0.12879320979118347, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31517732357018147, "std_of_first_answer": 0.31517732357018147, "avg_entropy_of_first_answer": 1.6007944246133168, "lex_sim": 1.0}
{"question": "Men Against the Sea and Pitcairn's Island were two sequels to what famous novel?", "answer": ["Heart of Darkness"], "scores": [[0.008239727467298508, 0.9070054888725281, 0.9629718661308289]], "normalized_score": [0.19306880235671997], "tokens": [["\u0120Heart", "\u0120of", "\u0120Darkness"]], "entropy": [[4.240938186645508, 0.4886411726474762, 0.26616913080215454]], "avg_entropy": [1.6652494966983795], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "HMS Bounty mutineers", "score_of_biggest_cluster": 0.19306880235671997, "score_of_first_answer": 0.19306880235671997, "max_score_of_first_answer": 0.9629718661308289, "min_score_of_first_answer": 0.008239727467298508, "avg_score": 0.19306880235671997, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4374707083343192, "std_of_first_answer": 0.4374707083343192, "avg_entropy_of_first_answer": 1.6652494966983795, "lex_sim": 1.0}
{"question": "In Roman mythology, who was the god of war?", "answer": ["Mars"], "scores": [[0.8764419555664062]], "normalized_score": [0.8764419555664062], "tokens": [["\u0120Mars"]], "entropy": [[0.8363035917282104]], "avg_entropy": [0.8363035917282104], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fourth planet", "score_of_biggest_cluster": 0.8764419555664062, "score_of_first_answer": 0.8764419555664062, "max_score_of_first_answer": 0.8764419555664062, "min_score_of_first_answer": 0.8764419555664062, "avg_score": 0.8764419555664062, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.8363035917282104, "lex_sim": 1.0}
{"question": "Which actor has been portrayed most on screen by other actors?", "answer": ["Joseph Calleia - 3"], "scores": [[0.003337118774652481, 0.08029662072658539, 0.9820844531059265, 0.9517477750778198, 0.002649636473506689, 0.0031820309814065695]], "normalized_score": [0.03581830859184265], "tokens": [["\u0120Joseph", "\u0120Cal", "le", "ia", "\u0120-", "\u01203"]], "entropy": [[5.649491310119629, 3.8781075477600098, 0.1597282737493515, 0.27772819995880127, 1.2078092098236084, 6.7194976806640625]], "avg_entropy": [2.9820603703459105], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Roy Export Company Establishment", "score_of_biggest_cluster": 0.03581830859184265, "score_of_first_answer": 0.03581830859184265, "max_score_of_first_answer": 0.9820844531059265, "min_score_of_first_answer": 0.002649636473506689, "avg_score": 0.03581830859184265, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44618767412141414, "std_of_first_answer": 0.44618767412141414, "avg_entropy_of_first_answer": 2.9820603703459105, "lex_sim": 1.0}
{"question": "Minyan is a quorum of ten adult males required for public worship in?", "answer": ["Israel"], "scores": [[0.02007134072482586]], "normalized_score": [0.02007134072482586], "tokens": [["\u0120Israel"]], "entropy": [[2.7048263549804688]], "avg_entropy": [2.7048263549804688], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Moshe La Khaim Eche Laerry La Jewess", "score_of_biggest_cluster": 0.02007134072482586, "score_of_first_answer": 0.02007134072482586, "max_score_of_first_answer": 0.02007134072482586, "min_score_of_first_answer": 0.02007134072482586, "avg_score": 0.02007134072482586, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.7048263549804688, "lex_sim": 1.0}
{"question": "Which six letter Japanese word describes a class of professional women in Japan who are trained from girlhood to dance and entertain social gatherings of men?", "answer": ["Geisha"], "scores": [[0.1828991323709488, 0.7129125595092773]], "normalized_score": [0.3610970675945282], "tokens": [["\u0120Ge", "isha"]], "entropy": [[5.535243988037109, 1.2702010869979858]], "avg_entropy": [3.4027225375175476], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tachikata", "score_of_biggest_cluster": 0.3610970675945282, "score_of_first_answer": 0.3610970675945282, "max_score_of_first_answer": 0.7129125595092773, "min_score_of_first_answer": 0.1828991323709488, "avg_score": 0.3610970675945282, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2650067135691643, "std_of_first_answer": 0.2650067135691643, "avg_entropy_of_first_answer": 3.4027225375175476, "lex_sim": 1.0}
{"question": "In Roman times what was the occupation of a publican?", "answer": ["Publican"], "scores": [[0.03780807927250862, 0.8218619227409363]], "normalized_score": [0.17627540230751038], "tokens": [["\u0120Public", "an"]], "entropy": [[6.216568470001221, 1.1396722793579102]], "avg_entropy": [3.6781203746795654], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Exciseman", "score_of_biggest_cluster": 0.17627540230751038, "score_of_first_answer": 0.17627540230751038, "max_score_of_first_answer": 0.8218619227409363, "min_score_of_first_answer": 0.03780807927250862, "avg_score": 0.17627540230751038, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.39202692173421383, "std_of_first_answer": 0.39202692173421383, "avg_entropy_of_first_answer": 3.6781203746795654, "lex_sim": 1.0}
{"question": "In which US state was the American Declaration of Independence signed on 4th July 1776?", "answer": ["Massachusetts"], "scores": [[0.05474347621202469]], "normalized_score": [0.05474347993731499], "tokens": [["\u0120Massachusetts"]], "entropy": [[3.3423633575439453]], "avg_entropy": [3.3423633575439453], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pennyslvania, United States", "score_of_biggest_cluster": 0.05474347993731499, "score_of_first_answer": 0.05474347993731499, "max_score_of_first_answer": 0.05474347621202469, "min_score_of_first_answer": 0.05474347621202469, "avg_score": 0.05474347993731499, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.3423633575439453, "lex_sim": 1.0}
{"question": "Who killed Goliath with a sling and a stone?", "answer": ["David"], "scores": [[0.7103585004806519]], "normalized_score": [0.7103585004806519], "tokens": [["\u0120David"]], "entropy": [[2.071937084197998]], "avg_entropy": [2.071937084197998], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "King David", "score_of_biggest_cluster": 0.7103585004806519, "score_of_first_answer": 0.7103585004806519, "max_score_of_first_answer": 0.7103585004806519, "min_score_of_first_answer": 0.7103585004806519, "avg_score": 0.7103585004806519, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.071937084197998, "lex_sim": 1.0}
{"question": "What famous road began in Chicago, Illinois, ending at Santa Monica, California?", "answer": ["I-5"], "scores": [[0.04179980605840683, 0.7647024989128113, 0.22356967628002167]], "normalized_score": [0.19261638820171356], "tokens": [["\u0120I", "-", "5"]], "entropy": [[4.498558044433594, 1.6909468173980713, 3.0374367237091064]], "avg_entropy": [3.0756471951802573], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Route 66 (public transport)", "score_of_biggest_cluster": 0.19261638820171356, "score_of_first_answer": 0.19261638820171356, "max_score_of_first_answer": 0.7647024989128113, "min_score_of_first_answer": 0.04179980605840683, "avg_score": 0.19261638820171356, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3070384197434622, "std_of_first_answer": 0.3070384197434622, "avg_entropy_of_first_answer": 3.0756471951802573, "lex_sim": 1.0}
{"question": "What was the parliamentary constituency of Tony Blair when he was leader of the Labour Party?", "answer": ["\u32c7"], "scores": [[0.004914102144539356, 5.691463957191445e-05, 0.021272867918014526, 0.0024314112961292267]], "normalized_score": [0.0019502409268170595], "tokens": [["\u0120", "\u00e3", "\u012d", "\u0129"]], "entropy": [[4.144931793212891, 4.687161922454834, 2.6920390129089355, 3.863168239593506]], "avg_entropy": [3.8468252420425415], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Segfield", "score_of_biggest_cluster": 0.0019502409268170595, "score_of_first_answer": 0.0019502409268170595, "max_score_of_first_answer": 0.021272867918014526, "min_score_of_first_answer": 5.691463957191445e-05, "avg_score": 0.0019502409268170595, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.00832211150136363, "std_of_first_answer": 0.00832211150136363, "avg_entropy_of_first_answer": 3.8468252420425415, "lex_sim": 0.0}
{"question": "What is the name of the cookie recipe printed on the back of each bag of Nestle \"semi-sweet chocolate morsels\"?", "answer": ["Chocolate Chip Cookie"], "scores": [[0.06111501157283783, 0.25121021270751953, 0.2111596018075943]], "normalized_score": [0.14800122380256653], "tokens": [["\u0120Chocolate", "\u0120Chip", "\u0120Cookie"]], "entropy": [[6.220300674438477, 4.269300937652588, 2.581819772720337]], "avg_entropy": [4.357140461603801], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chocolate-chip cookie", "score_of_biggest_cluster": 0.14800122380256653, "score_of_first_answer": 0.14800122380256653, "max_score_of_first_answer": 0.25121021270751953, "min_score_of_first_answer": 0.06111501157283783, "avg_score": 0.14800122380256653, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08182203994483549, "std_of_first_answer": 0.08182203994483549, "avg_entropy_of_first_answer": 4.357140461603801, "lex_sim": 1.0}
{"question": "When sending an email you are given the option to 'Add Cc' or 'Add Bcc'. For which word is the 'B' the initial letter?", "answer": ["B"], "scores": [[0.09320621192455292]], "normalized_score": [0.09320620447397232], "tokens": [["\u0120B"]], "entropy": [[5.809415817260742]], "avg_entropy": [5.809415817260742], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Blind carbon copy", "score_of_biggest_cluster": 0.09320620447397232, "score_of_first_answer": 0.09320620447397232, "max_score_of_first_answer": 0.09320621192455292, "min_score_of_first_answer": 0.09320621192455292, "avg_score": 0.09320620447397232, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.809415817260742, "lex_sim": 1.0}
{"question": "Who wrote the political treatise \u2018The Prince\u2019, published in 1532?", "answer": ["King Henry VIII (novel), Machiavelli (play and novel), Niccolo Machiavelli (novel)"], "scores": [[0.0007794174016453326, 0.40444502234458923, 0.7522210478782654, 0.002459050389006734, 0.0020555362571030855, 0.265936940908432, 0.009600508958101273, 0.2746737599372864, 0.9974272847175598, 0.9944815635681152, 0.9973986148834229, 0.7670987248420715, 0.02251366898417473, 0.013423596508800983, 0.2121765911579132, 0.37277308106422424, 0.1046091690659523, 0.3223554193973541, 0.892890989780426, 0.9993495345115662, 0.9963395595550537, 0.9985106587409973, 0.675544023513794, 0.3298172652721405, 0.9980493783950806, 0.5581789016723633]], "normalized_score": [0.18186131119728088], "tokens": [["\u0120King", "\u0120Henry", "\u0120VIII", "\u0120(", "no", "vel", "),", "\u0120Mach", "ia", "ve", "lli", "\u0120(", "play", "\u0120and", "\u0120novel", "),", "\u0120Nic", "colo", "\u0120Mach", "ia", "ve", "lli", "\u0120(", "no", "vel", ")"]], "entropy": [[2.2145280838012695, 2.5352249145507812, 1.26296067237854, 0.5477889776229858, 5.178834438323975, 4.154212951660156, 2.531903028488159, 3.816105365753174, 0.02656920626759529, 0.044134773313999176, 0.023050932213664055, 0.9058501124382019, 3.9294793605804443, 1.159642219543457, 3.9034793376922607, 0.9785935878753662, 4.504717826843262, 0.7816754579544067, 0.747524619102478, 0.008148875087499619, 0.02793051302433014, 0.014240511693060398, 1.0463346242904663, 3.6949350833892822, 0.02548888325691223, 1.5287076234817505]], "avg_entropy": [1.753540845408749], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Niccol\u00f2 Macchiavelli", "score_of_biggest_cluster": 0.18186131119728088, "score_of_first_answer": 0.18186131119728088, "max_score_of_first_answer": 0.9993495345115662, "min_score_of_first_answer": 0.0007794174016453326, "avg_score": 0.18186131119728088, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3890631571953863, "std_of_first_answer": 0.3890631571953863, "avg_entropy_of_first_answer": 1.753540845408749, "lex_sim": 1.0}
{"question": "In Britain, a Christmas tree is traditionally of what species? (Two words.)", "answer": ["evergreen fir"], "scores": [[0.014309782534837723, 0.9780492186546326, 0.039774924516677856]], "normalized_score": [0.08226234465837479], "tokens": [["\u0120ever", "green", "\u0120fir"]], "entropy": [[3.538238763809204, 0.16256988048553467, 1.5009077787399292]], "avg_entropy": [1.7339054743448894], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "European spruce", "score_of_biggest_cluster": 0.08226234465837479, "score_of_first_answer": 0.08226234465837479, "max_score_of_first_answer": 0.9780492186546326, "min_score_of_first_answer": 0.014309782534837723, "avg_score": 0.08226234465837479, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4484294599833627, "std_of_first_answer": 0.4484294599833627, "avg_entropy_of_first_answer": 1.7339054743448894, "lex_sim": 1.0}
{"question": "\"The first line of which book is, \"\"Christmas won't be Christmas, without any presents, grumbled Jo, lying on the rug\"\"?\"", "answer": ["'Migraine' and 'Migraine'"], "scores": [[0.005595483351498842, 0.003910975530743599, 0.003926034551113844, 0.658912718296051, 0.5033203959465027, 0.003984780982136726, 0.8084738254547119, 0.044663943350315094, 0.1769251972436905, 0.8688865900039673, 0.734987199306488]], "normalized_score": [0.07563275843858719], "tokens": [["\u0120'", "M", "ig", "raine", "'", "\u0120and", "\u0120'", "M", "ig", "raine", "'"]], "entropy": [[3.7445645332336426, 3.332282781600952, 4.059182167053223, 1.8575941324234009, 3.305521011352539, 2.4883813858032227, 1.658865213394165, 6.329282760620117, 4.695791244506836, 0.8015294075012207, 1.9655110836029053]], "avg_entropy": [3.112591429190202], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Good Wives", "score_of_biggest_cluster": 0.07563275843858719, "score_of_first_answer": 0.07563275843858719, "max_score_of_first_answer": 0.8688865900039673, "min_score_of_first_answer": 0.003910975530743599, "avg_score": 0.07563275843858719, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3500053455452512, "std_of_first_answer": 0.3500053455452512, "avg_entropy_of_first_answer": 3.112591429190202, "lex_sim": 1.0}
{"question": "Which former Taoiseach of the Republic of Ireland died in August", "answer": ["Liam Horgan"], "scores": [[0.0709860548377037, 0.00041226320900022984, 0.29927000403404236]], "normalized_score": [0.020612794905900955], "tokens": [["\u0120Liam", "\u0120Hor", "gan"]], "entropy": [[4.200216770172119, 0.20366491377353668, 0.9272040128707886]], "avg_entropy": [1.7770285656054814], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Reynolds, Albert", "score_of_biggest_cluster": 0.020612794905900955, "score_of_first_answer": 0.020612794905900955, "max_score_of_first_answer": 0.29927000403404236, "min_score_of_first_answer": 0.00041226320900022984, "avg_score": 0.020612794905900955, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12754527334618448, "std_of_first_answer": 0.12754527334618448, "avg_entropy_of_first_answer": 1.7770285656054814, "lex_sim": 1.0}
{"question": "The books 'The Edge of Reason' and 'Mad About The Boy' are sequels to which novel?", "answer": ["The Edge of Reason"], "scores": [[0.11920621991157532, 0.08180271089076996, 0.9285588264465332, 0.16337627172470093]], "normalized_score": [0.19611744582653046], "tokens": [["\u0120The", "\u0120Edge", "\u0120of", "\u0120Reason"]], "entropy": [[4.291977405548096, 6.4890828132629395, 0.4245051145553589, 4.076805114746094]], "avg_entropy": [3.820592612028122], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Diary of Bridget Jones", "score_of_biggest_cluster": 0.19611744582653046, "score_of_first_answer": 0.19611744582653046, "max_score_of_first_answer": 0.9285588264465332, "min_score_of_first_answer": 0.08180271089076996, "avg_score": 0.19611744582653046, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35067400858721476, "std_of_first_answer": 0.35067400858721476, "avg_entropy_of_first_answer": 3.820592612028122, "lex_sim": 1.0}
{"question": "What is the name of the perfume launched by British boyband JLS in January 2013?", "answer": ["I"], "scores": [[0.004206381272524595]], "normalized_score": [0.004206380341202021], "tokens": [["\u0120I"]], "entropy": [[6.995924472808838]], "avg_entropy": [6.995924472808838], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Idealized love", "score_of_biggest_cluster": 0.004206380341202021, "score_of_first_answer": 0.004206380341202021, "max_score_of_first_answer": 0.004206381272524595, "min_score_of_first_answer": 0.004206381272524595, "avg_score": 0.004206380341202021, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.995924472808838, "lex_sim": 1.0}
{"question": "In which country will you find the 'Carisbrook Rugby Stadium'?", "answer": ["Canada"], "scores": [[0.0355716235935688]], "normalized_score": [0.0355716198682785], "tokens": [["\u0120Canada"]], "entropy": [[4.293111801147461]], "avg_entropy": [4.293111801147461], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "N Z", "score_of_biggest_cluster": 0.0355716198682785, "score_of_first_answer": 0.0355716198682785, "max_score_of_first_answer": 0.0355716235935688, "min_score_of_first_answer": 0.0355716235935688, "avg_score": 0.0355716198682785, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.293111801147461, "lex_sim": 1.0}
{"question": "In Arthurian legend, who threw Excalibur back into the Lake?", "answer": ["Arthur"], "scores": [[0.06066511571407318]], "normalized_score": [0.06066511198878288], "tokens": [["\u0120Arthur"]], "entropy": [[3.696267604827881]], "avg_entropy": [3.696267604827881], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sir Bedevere", "score_of_biggest_cluster": 0.06066511198878288, "score_of_first_answer": 0.06066511198878288, "max_score_of_first_answer": 0.06066511571407318, "min_score_of_first_answer": 0.06066511571407318, "avg_score": 0.06066511198878288, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.696267604827881, "lex_sim": 1.0}
{"question": "Founded in 1775, George Washington was the Commander-in-Chief of what Army?", "answer": ["US Army"], "scores": [[0.04553654044866562, 0.505215048789978]], "normalized_score": [0.15167644619941711], "tokens": [["\u0120US", "\u0120Army"]], "entropy": [[3.0100045204162598, 2.1706743240356445]], "avg_entropy": [2.590339422225952], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "American Continental Army", "score_of_biggest_cluster": 0.15167644619941711, "score_of_first_answer": 0.15167644619941711, "max_score_of_first_answer": 0.505215048789978, "min_score_of_first_answer": 0.04553654044866562, "avg_score": 0.15167644619941711, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2298392541706562, "std_of_first_answer": 0.2298392541706562, "avg_entropy_of_first_answer": 2.590339422225952, "lex_sim": 1.0}
{"question": "Which English town is known for its collection of concrete cows?", "answer": ["Milford-on-Sea \u32d0"], "scores": [[0.0032439210917800665, 0.6438363790512085, 0.3204478323459625, 0.7762776017189026, 0.9969111084938049, 0.8458176255226135, 0.0036433294881135225, 6.5588310462771915e-06, 0.024351296946406364, 0.05703362077474594]], "normalized_score": [0.041328832507133484], "tokens": [["\u0120Mil", "ford", "-", "on", "-", "Sea", "\u0120", "\u00e3", "\u012d", "\u0132"]], "entropy": [[6.380749225616455, 1.8230392932891846, 2.159956932067871, 1.117891550064087, 0.03495921194553375, 0.9084547758102417, 0.6616224050521851, 1.3969287872314453, 2.1278116703033447, 3.7590420246124268]], "avg_entropy": [2.0370455875992777], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Milton keynes", "score_of_biggest_cluster": 0.041328832507133484, "score_of_first_answer": 0.041328832507133484, "max_score_of_first_answer": 0.9969111084938049, "min_score_of_first_answer": 6.5588310462771915e-06, "avg_score": 0.041328832507133484, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3853421528933225, "std_of_first_answer": 0.3853421528933225, "avg_entropy_of_first_answer": 2.0370455875992777, "lex_sim": 1.0}
{"question": "Which film, released in 2008, and starring Ryan Reynolds and Rachel Weisz, shares its title with an album released by 'Oasis' in 1994?", "answer": ["Deadpool (film)"], "scores": [[0.009954010136425495, 0.010488413274288177, 0.11254832148551941, 0.9244734048843384]], "normalized_score": [0.05740969628095627], "tokens": [["\u0120Deadpool", "\u0120(", "film", ")"]], "entropy": [[5.461303234100342, 0.6561602354049683, 3.941889762878418, 0.4875895380973816]], "avg_entropy": [2.6367356926202774], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Digsy's Dinner", "score_of_biggest_cluster": 0.05740969628095627, "score_of_first_answer": 0.05740969628095627, "max_score_of_first_answer": 0.9244734048843384, "min_score_of_first_answer": 0.009954010136425495, "avg_score": 0.05740969628095627, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3833959060555653, "std_of_first_answer": 0.3833959060555653, "avg_entropy_of_first_answer": 2.6367356926202774, "lex_sim": 1.0}
{"question": "Which crop that was of great nutritional importance in pre-Columbian Andean civilizations, and which has now entered the mainstream world market, was called as the 'mother of all grains' by the Incas?", "answer": ["Maize"], "scores": [[0.15581025183200836, 0.974388599395752]], "normalized_score": [0.38964053988456726], "tokens": [["\u0120Ma", "ize"]], "entropy": [[3.9806556701660156, 0.15878240764141083]], "avg_entropy": [2.0697190389037132], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kinua", "score_of_biggest_cluster": 0.38964053988456726, "score_of_first_answer": 0.38964053988456726, "max_score_of_first_answer": 0.974388599395752, "min_score_of_first_answer": 0.15581025183200836, "avg_score": 0.38964053988456726, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4092891737818718, "std_of_first_answer": 0.4092891737818718, "avg_entropy_of_first_answer": 2.0697190389037132, "lex_sim": 1.0}
{"question": "What airline had two high-profile plane crashes in 2014?", "answer": ["American Airlines"], "scores": [[0.026745211333036423, 0.7332682609558105]], "normalized_score": [0.14004075527191162], "tokens": [["\u0120American", "\u0120Airlines"]], "entropy": [[5.846972942352295, 1.7392245531082153]], "avg_entropy": [3.793098747730255], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "MalaysiaAirlines", "score_of_biggest_cluster": 0.14004075527191162, "score_of_first_answer": 0.14004075527191162, "max_score_of_first_answer": 0.7332682609558105, "min_score_of_first_answer": 0.026745211333036423, "avg_score": 0.14004075527191162, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35326152481138706, "std_of_first_answer": 0.35326152481138706, "avg_entropy_of_first_answer": 3.793098747730255, "lex_sim": 1.0}
{"question": "\"The men's aftershaves or colognes \"\"Encounter\"\" and \"\"Euphoria\"\" are marketed under which perfume company's name?\"", "answer": ["Parfum de Rosalie"], "scores": [[0.015701165422797203, 0.9777863621711731, 0.1515379548072815, 0.1465533822774887, 0.03633148968219757, 0.009205823764204979, 0.40329691767692566]], "normalized_score": [0.08949711173772812], "tokens": [["\u0120Par", "f", "um", "\u0120de", "\u0120Ros", "al", "ie"]], "entropy": [[5.876123428344727, 0.1680665910243988, 0.7414626479148865, 5.414463996887207, 5.036569595336914, 2.3151960372924805, 2.514632225036621]], "avg_entropy": [3.152359217405319], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Calvin klein", "score_of_biggest_cluster": 0.08949711173772812, "score_of_first_answer": 0.08949711173772812, "max_score_of_first_answer": 0.9777863621711731, "min_score_of_first_answer": 0.009205823764204979, "avg_score": 0.08949711173772812, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32339445013994556, "std_of_first_answer": 0.32339445013994556, "avg_entropy_of_first_answer": 3.152359217405319, "lex_sim": 1.0}
{"question": "'Quinsy' is a term for an abscess on which part of the body?", "answer": ["Tongue"], "scores": [[0.06803388148546219, 0.9977893829345703]], "normalized_score": [0.2605445981025696], "tokens": [["\u0120Tong", "ue"]], "entropy": [[4.473597526550293, 0.020995374768972397]], "avg_entropy": [2.2472964506596327], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tonsil", "score_of_biggest_cluster": 0.2605445981025696, "score_of_first_answer": 0.2605445981025696, "max_score_of_first_answer": 0.9977893829345703, "min_score_of_first_answer": 0.06803388148546219, "avg_score": 0.2605445981025696, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46487775072455406, "std_of_first_answer": 0.46487775072455406, "avg_entropy_of_first_answer": 2.2472964506596327, "lex_sim": 1.0}
{"question": "\"What was the name of the dog in the children\u2019s TV series \"\"The Herbs\"\"?\"", "answer": ["\"The dog who looks like a pig\""], "scores": [[0.002927907044067979, 0.05001750588417053, 0.010124530643224716, 0.0153809217736125, 0.010267945006489754, 0.5101245641708374, 0.6507921814918518, 0.024761797860264778, 0.5293447375297546]], "normalized_score": [0.04651287570595741], "tokens": [["\u0120\"", "The", "\u0120dog", "\u0120who", "\u0120looks", "\u0120like", "\u0120a", "\u0120pig", "\""]], "entropy": [[6.886264801025391, 5.880831241607666, 5.391234397888184, 3.1444950103759766, 5.247568130493164, 2.3282694816589355, 2.651090621948242, 5.273855209350586, 2.320065498352051]], "avg_entropy": [4.3470749325222435], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dill", "score_of_biggest_cluster": 0.04651287570595741, "score_of_first_answer": 0.04651287570595741, "max_score_of_first_answer": 0.6507921814918518, "min_score_of_first_answer": 0.002927907044067979, "avg_score": 0.04651287570595741, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2594937320740575, "std_of_first_answer": 0.2594937320740575, "avg_entropy_of_first_answer": 4.3470749325222435, "lex_sim": 1.0}
{"question": "How many herbs and spices make up the secret blend used on Colonel Harland Sanders' most famous creation: his Original Recipe Kentucky Fried Chicken?", "answer": ["11"], "scores": [[0.007122305687516928]], "normalized_score": [0.00712230522185564], "tokens": [["\u012011"]], "entropy": [[5.234864711761475]], "avg_entropy": [5.234864711761475], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "11", "score_of_biggest_cluster": 0.00712230522185564, "score_of_first_answer": 0.00712230522185564, "max_score_of_first_answer": 0.007122305687516928, "min_score_of_first_answer": 0.007122305687516928, "avg_score": 0.00712230522185564, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.234864711761475, "lex_sim": 1.0}
{"question": "What is the American version of Strictly Come Dancing called?", "answer": ["Dancing with the Stars"], "scores": [[0.4177858829498291, 0.6779988408088684, 0.9636080265045166, 0.922141432762146]], "normalized_score": [0.708304762840271], "tokens": [["\u0120Dancing", "\u0120with", "\u0120the", "\u0120Stars"]], "entropy": [[3.225306987762451, 0.7774697542190552, 0.2940579950809479, 0.34394359588623047]], "avg_entropy": [1.1601945832371712], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Strictly Keep Dancing", "score_of_biggest_cluster": 0.708304762840271, "score_of_first_answer": 0.708304762840271, "max_score_of_first_answer": 0.9636080265045166, "min_score_of_first_answer": 0.4177858829498291, "avg_score": 0.708304762840271, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21836105522374794, "std_of_first_answer": 0.21836105522374794, "avg_entropy_of_first_answer": 1.1601945832371712, "lex_sim": 1.0}
{"question": "On the banks of which river does the Welsh city of Carmarthenstand?", "answer": ["river Teifi"], "scores": [[0.0024734116159379482, 0.18803875148296356, 0.7731745839118958]], "normalized_score": [0.07111159712076187], "tokens": [["\u0120river", "\u0120Te", "ifi"]], "entropy": [[2.754594087600708, 4.151719570159912, 0.7555412650108337]], "avg_entropy": [2.553951640923818], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tywi", "score_of_biggest_cluster": 0.07111159712076187, "score_of_first_answer": 0.07111159712076187, "max_score_of_first_answer": 0.7731745839118958, "min_score_of_first_answer": 0.0024734116159379482, "avg_score": 0.07111159712076187, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.32843039762053217, "std_of_first_answer": 0.32843039762053217, "avg_entropy_of_first_answer": 2.553951640923818, "lex_sim": 1.0}
{"question": "Michael Jackson was one of ten children. Who is the youngest, born in 1966?", "answer": ["Prince Michael"], "scores": [[0.18481647968292236, 0.5255394577980042]], "normalized_score": [0.3116542398929596], "tokens": [["\u0120Prince", "\u0120Michael"]], "entropy": [[5.167908668518066, 2.8725619316101074]], "avg_entropy": [4.020235300064087], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joint Academic Network", "score_of_biggest_cluster": 0.3116542398929596, "score_of_first_answer": 0.3116542398929596, "max_score_of_first_answer": 0.5255394577980042, "min_score_of_first_answer": 0.18481647968292236, "avg_score": 0.3116542398929596, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1703614890575409, "std_of_first_answer": 0.1703614890575409, "avg_entropy_of_first_answer": 4.020235300064087, "lex_sim": 1.0}
{"question": "The TV presenter Sir Patrick Moore, CBE, was a keen amateur musician, composing and playing pieces for which instrument?", "answer": ["The double bass"], "scores": [[0.2886371314525604, 0.0037804932799190283, 0.4501008987426758]], "normalized_score": [0.07889875024557114], "tokens": [["\u0120The", "\u0120double", "\u0120bass"]], "entropy": [[4.1066436767578125, 4.850608825683594, 1.5151078701019287]], "avg_entropy": [3.4907867908477783], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Zylophone", "score_of_biggest_cluster": 0.07889875024557114, "score_of_first_answer": 0.07889875024557114, "max_score_of_first_answer": 0.4501008987426758, "min_score_of_first_answer": 0.0037804932799190283, "avg_score": 0.07889875024557114, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1845161109469773, "std_of_first_answer": 0.1845161109469773, "avg_entropy_of_first_answer": 3.4907867908477783, "lex_sim": 1.0}
{"question": "Who was the youngest brother in the Beach Boys?", "answer": ["Brian Wilson"], "scores": [[0.060090571641922, 0.10182143747806549]], "normalized_score": [0.078220896422863], "tokens": [["\u0120Brian", "\u0120Wilson"]], "entropy": [[3.5164613723754883, 1.7772115468978882]], "avg_entropy": [2.6468364596366882], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Carl Wilson", "score_of_biggest_cluster": 0.078220896422863, "score_of_first_answer": 0.078220896422863, "max_score_of_first_answer": 0.10182143747806549, "min_score_of_first_answer": 0.060090571641922, "avg_score": 0.078220896422863, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.020865432918071747, "std_of_first_answer": 0.020865432918071747, "avg_entropy_of_first_answer": 2.6468364596366882, "lex_sim": 1.0}
{"question": "The thoroughfare where fictional Sweeny Todd had his barber shop?", "answer": ["Little Italy"], "scores": [[0.011443674564361572, 0.5530931949615479]], "normalized_score": [0.0795576274394989], "tokens": [["\u0120Little", "\u0120Italy"]], "entropy": [[5.856684684753418, 2.791337013244629]], "avg_entropy": [4.324010848999023], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fleet Street", "score_of_biggest_cluster": 0.0795576274394989, "score_of_first_answer": 0.0795576274394989, "max_score_of_first_answer": 0.5530931949615479, "min_score_of_first_answer": 0.011443674564361572, "avg_score": 0.0795576274394989, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27082476019859314, "std_of_first_answer": 0.27082476019859314, "avg_entropy_of_first_answer": 4.324010848999023, "lex_sim": 1.0}
{"question": "What is the state capital of Florida?", "answer": ["Tallahassee"], "scores": [[0.8027226328849792, 0.9728990197181702, 0.9813357591629028, 0.9977445602416992]], "normalized_score": [0.9351204633712769], "tokens": [["\u0120Tall", "ah", "as", "see"]], "entropy": [[1.2869977951049805, 0.28236809372901917, 0.13350069522857666, 0.02697841078042984]], "avg_entropy": [0.43246124871075153], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tallahasee", "score_of_biggest_cluster": 0.9351204633712769, "score_of_first_answer": 0.9351204633712769, "max_score_of_first_answer": 0.9977445602416992, "min_score_of_first_answer": 0.8027226328849792, "avg_score": 0.9351204633712769, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07899918190775428, "std_of_first_answer": 0.07899918190775428, "avg_entropy_of_first_answer": 0.43246124871075153, "lex_sim": 1.0}
{"question": "Which Mediterranean island gained independence in 1959 under the presidency of Archbishop Makarios?", "answer": ["Cyprus"], "scores": [[0.6975402235984802]], "normalized_score": [0.6975402235984802], "tokens": [["\u0120Cyprus"]], "entropy": [[1.904163122177124]], "avg_entropy": [1.904163122177124], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Culture of Cyprus", "score_of_biggest_cluster": 0.6975402235984802, "score_of_first_answer": 0.6975402235984802, "max_score_of_first_answer": 0.6975402235984802, "min_score_of_first_answer": 0.6975402235984802, "avg_score": 0.6975402235984802, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.904163122177124, "lex_sim": 1.0}
{"question": "Which US comedian/actor is credited with saying \u2018A man is only as old as the woman he feels\u2019?", "answer": ["Jerry Lewis"], "scores": [[0.02905523218214512, 0.689795196056366]], "normalized_score": [0.1415703296661377], "tokens": [["\u0120Jerry", "\u0120Lewis"]], "entropy": [[5.445078372955322, 1.8228998184204102]], "avg_entropy": [3.633989095687866], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "I don't want to belong to any club that will accept me as a member", "score_of_biggest_cluster": 0.1415703296661377, "score_of_first_answer": 0.1415703296661377, "max_score_of_first_answer": 0.689795196056366, "min_score_of_first_answer": 0.02905523218214512, "avg_score": 0.1415703296661377, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3303699819371104, "std_of_first_answer": 0.3303699819371104, "avg_entropy_of_first_answer": 3.633989095687866, "lex_sim": 1.0}
{"question": "What amateur rowing club based at Henley on Thames, England, was founded in 1818?", "answer": ["Cherwell"], "scores": [[0.008793414570391178, 0.24826768040657043]], "normalized_score": [0.046723879873752594], "tokens": [["\u0120Cher", "well"]], "entropy": [[3.536231517791748, 1.0736862421035767]], "avg_entropy": [2.3049588799476624], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Leander Club", "score_of_biggest_cluster": 0.046723879873752594, "score_of_first_answer": 0.046723879873752594, "max_score_of_first_answer": 0.24826768040657043, "min_score_of_first_answer": 0.008793414570391178, "avg_score": 0.046723879873752594, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11973713291808963, "std_of_first_answer": 0.11973713291808963, "avg_entropy_of_first_answer": 2.3049588799476624, "lex_sim": 1.0}
{"question": "On a standard dartboard, which number lies opposite number 4?", "answer": ["144"], "scores": [[0.0011216877028346062]], "normalized_score": [0.0011216874700039625], "tokens": [["\u0120144"]], "entropy": [[5.386256217956543]], "avg_entropy": [5.386256217956543], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "sixteen", "score_of_biggest_cluster": 0.0011216874700039625, "score_of_first_answer": 0.0011216874700039625, "max_score_of_first_answer": 0.0011216877028346062, "min_score_of_first_answer": 0.0011216877028346062, "avg_score": 0.0011216874700039625, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.386256217956543, "lex_sim": 1.0}
{"question": "An apartment in which building has been the official London residence of the MP for Buckingham since June 2009 ?", "answer": ["St. Mary's House"], "scores": [[0.018414758145809174, 0.4952593147754669, 0.014752174727618694, 0.29042571783065796, 0.3137636184692383]], "normalized_score": [0.104159414768219], "tokens": [["\u0120St", ".", "\u0120Mary", "'s", "\u0120House"]], "entropy": [[5.3802490234375, 2.2404332160949707, 2.2641866207122803, 1.9447638988494873, 3.3852009773254395]], "avg_entropy": [3.0429667472839355], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Westminster Palace", "score_of_biggest_cluster": 0.104159414768219, "score_of_first_answer": 0.104159414768219, "max_score_of_first_answer": 0.4952593147754669, "min_score_of_first_answer": 0.014752174727618694, "avg_score": 0.104159414768219, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18550989865786588, "std_of_first_answer": 0.18550989865786588, "avg_entropy_of_first_answer": 3.0429667472839355, "lex_sim": 1.0}
{"question": "What was photographed for the first time in October 1959", "answer": ["A star, A planet, and a meteorite falling to the surface of Earth"], "scores": [[0.07137580215930939, 0.00987648218870163, 0.020369958132505417, 0.0020263392943888903, 0.15337975323200226, 0.629790723323822, 0.23112399876117706, 0.36043307185173035, 0.017617857083678246, 0.13007210195064545, 0.0069721355102956295, 0.3418923020362854, 0.11317533254623413, 0.05756765604019165, 0.9383704662322998, 0.05651681497693062]], "normalized_score": [0.07125824689865112], "tokens": [["\u0120A", "\u0120star", ",", "\u0120A", "\u0120planet", ",", "\u0120and", "\u0120a", "\u0120meteor", "ite", "\u0120falling", "\u0120to", "\u0120the", "\u0120surface", "\u0120of", "\u0120Earth"]], "entropy": [[5.737976551055908, 7.182987213134766, 3.5221340656280518, 6.002131462097168, 5.411711692810059, 1.3683598041534424, 2.344698667526245, 2.7977395057678223, 4.354313373565674, 2.4327597618103027, 2.0089221000671387, 2.6333351135253906, 1.09272301197052, 1.4555175304412842, 0.38160985708236694, 1.8876816034317017]], "avg_entropy": [3.16341258212924], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dark Side of the Moon: A Piece for Assorted Lunatics", "score_of_biggest_cluster": 0.07125824689865112, "score_of_first_answer": 0.07125824689865112, "max_score_of_first_answer": 0.9383704662322998, "min_score_of_first_answer": 0.0020263392943888903, "avg_score": 0.07125824689865112, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25331329861521734, "std_of_first_answer": 0.25331329861521734, "avg_entropy_of_first_answer": 3.16341258212924, "lex_sim": 1.0}
{"question": "Tennis player Martina Hingis was born in which European country?", "answer": ["Slovenia"], "scores": [[0.013592037372291088]], "normalized_score": [0.013592036440968513], "tokens": [["\u0120Slovenia"]], "entropy": [[2.283295154571533]], "avg_entropy": [2.283295154571533], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ISO 3166-1:CH", "score_of_biggest_cluster": 0.013592036440968513, "score_of_first_answer": 0.013592036440968513, "max_score_of_first_answer": 0.013592037372291088, "min_score_of_first_answer": 0.013592037372291088, "avg_score": 0.013592036440968513, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.283295154571533, "lex_sim": 1.0}
{"question": "The music of which band finally became available for download from iTunes in November 2010?", "answer": ["New Found Glory"], "scores": [[0.008484439924359322, 0.03895072638988495, 0.8688465356826782]], "normalized_score": [0.06597213447093964], "tokens": [["\u0120New", "\u0120Found", "\u0120Glory"]], "entropy": [[6.173433780670166, 3.863762378692627, 1.0181514024734497]], "avg_entropy": [3.685115853945414], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fab 4", "score_of_biggest_cluster": 0.06597213447093964, "score_of_first_answer": 0.06597213447093964, "max_score_of_first_answer": 0.8688465356826782, "min_score_of_first_answer": 0.008484439924359322, "avg_score": 0.06597213447093964, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3985917132313586, "std_of_first_answer": 0.3985917132313586, "avg_entropy_of_first_answer": 3.685115853945414, "lex_sim": 1.0}
{"question": "\"The song \"\"Consider Yourself\"\" is from what musical?\"", "answer": ["\"Consider Me\" (song) is a hit by American group \"New Kids on The Block\""], "scores": [[0.0799686536192894, 0.06847813725471497, 0.0072803501971066, 0.7017130851745605, 0.07335352897644043, 0.03923230245709419, 0.7649419903755188, 0.008086532354354858, 0.2135080248117447, 0.013061684556305408, 0.15496455132961273, 0.02569359727203846, 0.04601427540183067, 0.05187378451228142, 0.006416977383196354, 0.3301381766796112, 0.7876377701759338, 0.06081635504961014, 0.958462655544281, 0.7361458539962769]], "normalized_score": [0.08887366950511932], "tokens": [["\u0120\"", "Consider", "\u0120Me", "\"", "\u0120(", "song", ")", "\u0120is", "\u0120a", "\u0120hit", "\u0120by", "\u0120American", "\u0120group", "\u0120\"", "New", "\u0120Kids", "\u0120on", "\u0120The", "\u0120Block", "\""]], "entropy": [[5.873337745666504, 5.757258415222168, 0.5036463141441345, 1.54281485080719, 1.4871479272842407, 5.485102653503418, 1.4008538722991943, 1.8139257431030273, 1.8179922103881836, 4.137114524841309, 2.5155131816864014, 6.003250598907471, 3.270319700241089, 5.601011753082275, 5.366945266723633, 2.986957550048828, 0.6739464998245239, 0.2851008176803589, 0.3353957235813141, 1.2383978366851807]], "avg_entropy": [2.904801659286022], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Oliver!", "score_of_biggest_cluster": 0.08887366950511932, "score_of_first_answer": 0.08887366950511932, "max_score_of_first_answer": 0.958462655544281, "min_score_of_first_answer": 0.006416977383196354, "avg_score": 0.08887366950511932, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32014479044611655, "std_of_first_answer": 0.32014479044611655, "avg_entropy_of_first_answer": 2.904801659286022, "lex_sim": 1.0}
{"question": "By what more familiar name do we know the 'Angevin Dynasty' of English kings?", "answer": ["House of Plantagenet"], "scores": [[0.026224814355373383, 0.9777590036392212, 0.7175679802894592, 0.9934588074684143, 0.9489449858665466]], "normalized_score": [0.44446709752082825], "tokens": [["\u0120House", "\u0120of", "\u0120Plant", "agen", "et"]], "entropy": [[2.0024912357330322, 0.17703276872634888, 1.3614323139190674, 0.0490405336022377, 0.3193872570991516]], "avg_entropy": [0.7818768218159675], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "House of Plantagenet", "score_of_biggest_cluster": 0.44446709752082825, "score_of_first_answer": 0.44446709752082825, "max_score_of_first_answer": 0.9934588074684143, "min_score_of_first_answer": 0.026224814355373383, "avg_score": 0.44446709752082825, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3671912551488135, "std_of_first_answer": 0.3671912551488135, "avg_entropy_of_first_answer": 0.7818768218159675, "lex_sim": 1.0}
{"question": "What is the word for the projection on a ship that holds a lifeboat?", "answer": ["Lifeboat"], "scores": [[0.07496620714664459, 0.36849161982536316]], "normalized_score": [0.16620595753192902], "tokens": [["\u0120Life", "boat"]], "entropy": [[6.250417709350586, 2.3562636375427246]], "avg_entropy": [4.303340673446655], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Davit crane", "score_of_biggest_cluster": 0.16620595753192902, "score_of_first_answer": 0.16620595753192902, "max_score_of_first_answer": 0.36849161982536316, "min_score_of_first_answer": 0.07496620714664459, "avg_score": 0.16620595753192902, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14676270633935928, "std_of_first_answer": 0.14676270633935928, "avg_entropy_of_first_answer": 4.303340673446655, "lex_sim": 1.0}
{"question": "The dance fitness programme, the Zumba, originated in which country?", "answer": ["Cuba"], "scores": [[0.21010561287403107]], "normalized_score": [0.21010562777519226], "tokens": [["\u0120Cuba"]], "entropy": [[3.5563251972198486]], "avg_entropy": [3.5563251972198486], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rep\u00fablica de Colombia", "score_of_biggest_cluster": 0.21010562777519226, "score_of_first_answer": 0.21010562777519226, "max_score_of_first_answer": 0.21010561287403107, "min_score_of_first_answer": 0.21010561287403107, "avg_score": 0.21010562777519226, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.5563251972198486, "lex_sim": 1.0}
{"question": "Which American scene artist painted 'Cape Cod Evening'?", "answer": ["Thomas Cole"], "scores": [[0.03532452508807182, 0.143654927611351]], "normalized_score": [0.07123582810163498], "tokens": [["\u0120Thomas", "\u0120Cole"]], "entropy": [[5.672032356262207, 4.74375057220459]], "avg_entropy": [5.207891464233398], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Edward Hopper", "score_of_biggest_cluster": 0.07123582810163498, "score_of_first_answer": 0.07123582810163498, "max_score_of_first_answer": 0.143654927611351, "min_score_of_first_answer": 0.03532452508807182, "avg_score": 0.07123582810163498, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.054165201261639595, "std_of_first_answer": 0.054165201261639595, "avg_entropy_of_first_answer": 5.207891464233398, "lex_sim": 1.0}
{"question": "Which golfer is nicknamed 'El Ni\u00f1o'?", "answer": ["Gary Player"], "scores": [[0.018036197870969772, 0.8793059587478638]], "normalized_score": [0.12593384087085724], "tokens": [["\u0120Gary", "\u0120Player"]], "entropy": [[5.3629560470581055, 0.8902227878570557]], "avg_entropy": [3.1265894174575806], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sergio Garc\u00eda", "score_of_biggest_cluster": 0.12593384087085724, "score_of_first_answer": 0.12593384087085724, "max_score_of_first_answer": 0.8793059587478638, "min_score_of_first_answer": 0.018036197870969772, "avg_score": 0.12593384087085724, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.430634880438447, "std_of_first_answer": 0.430634880438447, "avg_entropy_of_first_answer": 3.1265894174575806, "lex_sim": 1.0}
{"question": "Which religion's holy book is the Adi Granth?", "answer": ["Guru Granth Sahib"], "scores": [[0.048773158341646194, 0.9436755776405334, 0.9988924860954285, 0.8993788361549377, 0.995293915271759]], "normalized_score": [0.5283032655715942], "tokens": [["\u0120Guru", "\u0120Gran", "th", "\u0120Sah", "ib"]], "entropy": [[1.5482500791549683, 0.43538084626197815, 0.011587677523493767, 0.5460373759269714, 0.04494117572903633]], "avg_entropy": [0.5172394309192896], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sikh faith", "score_of_biggest_cluster": 0.5283032655715942, "score_of_first_answer": 0.5283032655715942, "max_score_of_first_answer": 0.9988924860954285, "min_score_of_first_answer": 0.048773158341646194, "avg_score": 0.5283032655715942, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36604916714397095, "std_of_first_answer": 0.36604916714397095, "avg_entropy_of_first_answer": 0.5172394309192896, "lex_sim": 1.0}
{"question": "In which city is the majority of Shakespeare's Romeo and Juliet set?", "answer": ["Verona"], "scores": [[0.44778868556022644, 0.9968323111534119]], "normalized_score": [0.6681094765663147], "tokens": [["\u0120Ver", "ona"]], "entropy": [[3.521077871322632, 0.03455853834748268]], "avg_entropy": [1.7778182048350573], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "San Michele Extra", "score_of_biggest_cluster": 0.6681094765663147, "score_of_first_answer": 0.6681094765663147, "max_score_of_first_answer": 0.9968323111534119, "min_score_of_first_answer": 0.44778868556022644, "avg_score": 0.6681094765663147, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2745218127965927, "std_of_first_answer": 0.2745218127965927, "avg_entropy_of_first_answer": 1.7778182048350573, "lex_sim": 1.0}
{"question": "Who was King of England in 1350?", "answer": ["Edward the II"], "scores": [[0.4430948495864868, 0.08923999220132828, 0.19690574705600739]], "normalized_score": [0.19820056855678558], "tokens": [["\u0120Edward", "\u0120the", "\u0120II"]], "entropy": [[2.0802125930786133, 1.2603068351745605, 2.468841314315796]], "avg_entropy": [1.9364535808563232], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Edward III of England", "score_of_biggest_cluster": 0.19820056855678558, "score_of_first_answer": 0.19820056855678558, "max_score_of_first_answer": 0.4430948495864868, "min_score_of_first_answer": 0.08923999220132828, "avg_score": 0.19820056855678558, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14810441101851146, "std_of_first_answer": 0.14810441101851146, "avg_entropy_of_first_answer": 1.9364535808563232, "lex_sim": 1.0}
{"question": "The line 'The mirror crack'd from side to side' comes from which poem?", "answer": ["The Raven"], "scores": [[0.19107596576213837, 0.04073727875947952]], "normalized_score": [0.08822650462388992], "tokens": [["\u0120The", "\u0120Raven"]], "entropy": [[4.474025249481201, 5.704679489135742]], "avg_entropy": [5.089352369308472], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Lady of Shallott", "score_of_biggest_cluster": 0.08822650462388992, "score_of_first_answer": 0.08822650462388992, "max_score_of_first_answer": 0.19107596576213837, "min_score_of_first_answer": 0.04073727875947952, "avg_score": 0.08822650462388992, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07516934350132942, "std_of_first_answer": 0.07516934350132942, "avg_entropy_of_first_answer": 5.089352369308472, "lex_sim": 1.0}
{"question": "Who was the deviser, producer and director 'That Was The Week That Was'?", "answer": ["Jimmy Tarling"], "scores": [[0.019996484741568565, 0.045508116483688354, 0.00048678196617402136]], "normalized_score": [0.007622998673468828], "tokens": [["\u0120Jimmy", "\u0120Tar", "ling"]], "entropy": [[3.7632503509521484, 3.629176139831543, 0.08667491376399994]], "avg_entropy": [2.4930338015158973], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ned Sherrin", "score_of_biggest_cluster": 0.007622998673468828, "score_of_first_answer": 0.007622998673468828, "max_score_of_first_answer": 0.045508116483688354, "min_score_of_first_answer": 0.00048678196617402136, "avg_score": 0.007622998673468828, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01843424475826233, "std_of_first_answer": 0.01843424475826233, "avg_entropy_of_first_answer": 2.4930338015158973, "lex_sim": 1.0}
{"question": "What are the membranes enveloping the brain and spinal cord called ?", "answer": ["Dandy-walker"], "scores": [[0.1745481789112091, 0.0002341973886359483, 0.5439192652702332, 0.06189141422510147]], "normalized_score": [0.03425038978457451], "tokens": [["\u0120D", "andy", "-", "walker"]], "entropy": [[4.299428462982178, 1.4089350700378418, 3.324479103088379, 3.1260809898376465]], "avg_entropy": [3.0397309064865112], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cerebral meninges", "score_of_biggest_cluster": 0.03425038978457451, "score_of_first_answer": 0.03425038978457451, "max_score_of_first_answer": 0.5439192652702332, "min_score_of_first_answer": 0.0002341973886359483, "avg_score": 0.03425038978457451, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21084025718990168, "std_of_first_answer": 0.21084025718990168, "avg_entropy_of_first_answer": 3.0397309064865112, "lex_sim": 1.0}
{"question": "\"What \"\"Glee\"\" character has the initials RB?\"", "answer": ["Renata Barzilai"], "scores": [[0.006738133262842894, 0.10777398198843002, 0.05126978084445, 0.08074360340833664, 0.06144335865974426, 0.294408917427063]], "normalized_score": [0.061551906168460846], "tokens": [["\u0120Ren", "ata", "\u0120Bar", "z", "il", "ai"]], "entropy": [[4.85677433013916, 3.587768077850342, 3.9530622959136963, 4.455692291259766, 2.968597888946533, 1.3694148063659668]], "avg_entropy": [3.531884948412577], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rachel Berry (Glee)", "score_of_biggest_cluster": 0.061551906168460846, "score_of_first_answer": 0.061551906168460846, "max_score_of_first_answer": 0.294408917427063, "min_score_of_first_answer": 0.006738133262842894, "avg_score": 0.061551906168460846, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.09200013869166115, "std_of_first_answer": 0.09200013869166115, "avg_entropy_of_first_answer": 3.531884948412577, "lex_sim": 1.0}
{"question": "\"Charlie Sheen starred as which character in the TV series \"\"Two and a Half Men\"\"?\"", "answer": ["Waldo"], "scores": [[0.3629200756549835, 0.24662934243679047]], "normalized_score": [0.29917675256729126], "tokens": [["\u0120Wald", "o"]], "entropy": [[3.8604183197021484, 1.336238145828247]], "avg_entropy": [2.5983282327651978], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charlie", "score_of_biggest_cluster": 0.29917675256729126, "score_of_first_answer": 0.29917675256729126, "max_score_of_first_answer": 0.3629200756549835, "min_score_of_first_answer": 0.24662934243679047, "avg_score": 0.29917675256729126, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05814536660909653, "std_of_first_answer": 0.05814536660909653, "avg_entropy_of_first_answer": 2.5983282327651978, "lex_sim": 1.0}
{"question": "What whole two-digit number is the square root of 9801?", "answer": ["17"], "scores": [[0.018841946497559547]], "normalized_score": [0.018841946497559547], "tokens": [["\u012017"]], "entropy": [[5.449288845062256]], "avg_entropy": [5.449288845062256], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "99", "score_of_biggest_cluster": 0.018841946497559547, "score_of_first_answer": 0.018841946497559547, "max_score_of_first_answer": 0.018841946497559547, "min_score_of_first_answer": 0.018841946497559547, "avg_score": 0.018841946497559547, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.449288845062256, "lex_sim": 1.0}
{"question": "Which group has had hits with the albums 'Hopes and Fears', and `Under the Iron Sea'?", "answer": ["The Strokes"], "scores": [[0.2491818368434906, 0.0033991567324846983, 0.9913231134414673]], "normalized_score": [0.09434111416339874], "tokens": [["\u0120The", "\u0120Stro", "kes"]], "entropy": [[5.284493446350098, 5.647046089172363, 0.06302251666784286]], "avg_entropy": [3.664854017396768], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Keane", "score_of_biggest_cluster": 0.09434111416339874, "score_of_first_answer": 0.09434111416339874, "max_score_of_first_answer": 0.9913231134414673, "min_score_of_first_answer": 0.0033991567324846983, "avg_score": 0.09434111416339874, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4199439836338698, "std_of_first_answer": 0.4199439836338698, "avg_entropy_of_first_answer": 3.664854017396768, "lex_sim": 1.0}
{"question": "January 3, 1924 saw Howard Carter discover what famous Egyptian pharaoh?", "answer": ["Tut"], "scores": [[0.5980097651481628]], "normalized_score": [0.5980097651481628], "tokens": [["\u0120Tut"]], "entropy": [[2.373462438583374]], "avg_entropy": [2.373462438583374], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "King Tut", "score_of_biggest_cluster": 0.5980097651481628, "score_of_first_answer": 0.5980097651481628, "max_score_of_first_answer": 0.5980097651481628, "min_score_of_first_answer": 0.5980097651481628, "avg_score": 0.5980097651481628, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.373462438583374, "lex_sim": 1.0}
{"question": "Which cup competition was founded by Charles William Alcock in the eighteen seventies", "answer": ["FA Cup"], "scores": [[0.06406410783529282, 0.7170686721801758]], "normalized_score": [0.21433238685131073], "tokens": [["\u0120FA", "\u0120Cup"]], "entropy": [[4.472529411315918, 1.5731067657470703]], "avg_entropy": [3.022818088531494], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Football Association Challenge Cup", "score_of_biggest_cluster": 0.21433238685131073, "score_of_first_answer": 0.21433238685131073, "max_score_of_first_answer": 0.7170686721801758, "min_score_of_first_answer": 0.06406410783529282, "avg_score": 0.21433238685131073, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3265022821724415, "std_of_first_answer": 0.3265022821724415, "avg_entropy_of_first_answer": 3.022818088531494, "lex_sim": 1.0}
{"question": "\u2018Divine Wind\u2019 is the English translation of which Japanese word?", "answer": ["Kyoiku"], "scores": [[0.07242121547460556, 0.0324581004679203, 0.23196657001972198]], "normalized_score": [0.08169674128293991], "tokens": [["\u0120K", "yo", "iku"]], "entropy": [[5.7547926902771, 2.530916690826416, 2.9993395805358887]], "avg_entropy": [3.7616829872131348], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tokubetsu K\u014dgekitai", "score_of_biggest_cluster": 0.08169674128293991, "score_of_first_answer": 0.08169674128293991, "max_score_of_first_answer": 0.23196657001972198, "min_score_of_first_answer": 0.0324581004679203, "avg_score": 0.08169674128293991, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08618803741488505, "std_of_first_answer": 0.08618803741488505, "avg_entropy_of_first_answer": 3.7616829872131348, "lex_sim": 1.0}
{"question": "Introduced in 1940, the 'Local Defence Volunteers' later became known by what two word name?", "answer": ["LDF"], "scores": [[0.004795241169631481, 0.0782582238316536]], "normalized_score": [0.01937180943787098], "tokens": [["\u0120L", "DF"]], "entropy": [[3.4239282608032227, 2.560300350189209]], "avg_entropy": [2.992114305496216], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Home guard", "score_of_biggest_cluster": 0.01937180943787098, "score_of_first_answer": 0.01937180943787098, "max_score_of_first_answer": 0.0782582238316536, "min_score_of_first_answer": 0.004795241169631481, "avg_score": 0.01937180943787098, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.03673149133101106, "std_of_first_answer": 0.03673149133101106, "avg_entropy_of_first_answer": 2.992114305496216, "lex_sim": 1.0}
{"question": "Which mythological creatures are half horse and half man?", "answer": ["Halflings"], "scores": [[0.018454967066645622, 0.09007581323385239]], "normalized_score": [0.040771882981061935], "tokens": [["\u0120Half", "lings"]], "entropy": [[2.633833169937134, 3.017019748687744]], "avg_entropy": [2.825426459312439], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Centaurs", "score_of_biggest_cluster": 0.040771882981061935, "score_of_first_answer": 0.040771882981061935, "max_score_of_first_answer": 0.09007581323385239, "min_score_of_first_answer": 0.018454967066645622, "avg_score": 0.040771882981061935, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.03581042308360338, "std_of_first_answer": 0.03581042308360338, "avg_entropy_of_first_answer": 2.825426459312439, "lex_sim": 1.0}
{"question": "The Bet365 Gold Cup, formerly the Beffred and the Whitbread Gold Cup, is the last major 'race of the National Hunt season. On which course is it run?", "answer": ["Cheltenham Gold Cup"], "scores": [[0.3857349753379822, 0.9796299934387207, 0.999861478805542, 0.08892479538917542, 0.9737541675567627]], "normalized_score": [0.5046062469482422], "tokens": [["\u0120Chel", "ten", "ham", "\u0120Gold", "\u0120Cup"]], "entropy": [[3.869997978210449, 0.15100926160812378, 0.0019557992927730083, 1.4115667343139648, 0.23384396731853485]], "avg_entropy": [1.1336747481487692], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sandown", "score_of_biggest_cluster": 0.5046062469482422, "score_of_first_answer": 0.5046062469482422, "max_score_of_first_answer": 0.999861478805542, "min_score_of_first_answer": 0.08892479538917542, "avg_score": 0.5046062469482422, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3779383731262121, "std_of_first_answer": 0.3779383731262121, "avg_entropy_of_first_answer": 1.1336747481487692, "lex_sim": 1.0}
{"question": "Who wrote the plays \u2018The Cocktail Party\u2019 and \u2018The Family Reunion\u2019?", "answer": ["No\u00ebl Coward"], "scores": [[0.17497357726097107, 0.2636600434780121, 0.9965742826461792, 0.9921485185623169, 0.998195469379425]], "normalized_score": [0.5390930771827698], "tokens": [["\u0120No", "\u00c3\u00ab", "l", "\u0120Cow", "ard"]], "entropy": [[4.47608757019043, 3.6080331802368164, 0.04421155899763107, 0.09046600013971329, 0.01798318699002266]], "avg_entropy": [1.6473562993109225], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "TS Elliot", "score_of_biggest_cluster": 0.5390930771827698, "score_of_first_answer": 0.5390930771827698, "max_score_of_first_answer": 0.998195469379425, "min_score_of_first_answer": 0.17497357726097107, "avg_score": 0.5390930771827698, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3813566304755745, "std_of_first_answer": 0.3813566304755745, "avg_entropy_of_first_answer": 1.6473562993109225, "lex_sim": 1.0}
{"question": "What does the Latin phrase \u2018Fortes fortuna juvat\u2019 translate to in English?", "answer": ["Fortune favors the bold"], "scores": [[0.4410201907157898, 0.23422615230083466, 0.9868205189704895, 0.18453527987003326]], "normalized_score": [0.3703419268131256], "tokens": [["\u0120Fortune", "\u0120favors", "\u0120the", "\u0120bold"]], "entropy": [[3.131464958190918, 1.1495139598846436, 0.11750348657369614, 0.8209887742996216]], "avg_entropy": [1.3048677947372198], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fortune Favours the Bold", "score_of_biggest_cluster": 0.3703419268131256, "score_of_first_answer": 0.3703419268131256, "max_score_of_first_answer": 0.9868205189704895, "min_score_of_first_answer": 0.18453527987003326, "avg_score": 0.3703419268131256, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.31809740553826127, "std_of_first_answer": 0.31809740553826127, "avg_entropy_of_first_answer": 1.3048677947372198, "lex_sim": 1.0}
{"question": "Marble Arch in London was once the ceremonial entrance to which British palace?", "answer": ["The Palace of Westminster"], "scores": [[0.10333516448736191, 0.3619266152381897, 0.93768310546875, 0.753603458404541]], "normalized_score": [0.40319666266441345], "tokens": [["\u0120The", "\u0120Palace", "\u0120of", "\u0120Westminster"]], "entropy": [[3.5157766342163086, 3.444854736328125, 0.3774833083152771, 1.1997027397155762]], "avg_entropy": [2.1344543546438217], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Buckigham palace", "score_of_biggest_cluster": 0.40319666266441345, "score_of_first_answer": 0.40319666266441345, "max_score_of_first_answer": 0.93768310546875, "min_score_of_first_answer": 0.10333516448736191, "avg_score": 0.40319666266441345, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3264052773273122, "std_of_first_answer": 0.3264052773273122, "avg_entropy_of_first_answer": 2.1344543546438217, "lex_sim": 1.0}
{"question": "Which car manufacturer originally made the model known as the Matiz?", "answer": ["Toyota"], "scores": [[0.04465692490339279]], "normalized_score": [0.04465692862868309], "tokens": [["\u0120Toyota"]], "entropy": [[5.319178581237793]], "avg_entropy": [5.319178581237793], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Taeu", "score_of_biggest_cluster": 0.04465692862868309, "score_of_first_answer": 0.04465692862868309, "max_score_of_first_answer": 0.04465692490339279, "min_score_of_first_answer": 0.04465692490339279, "avg_score": 0.04465692862868309, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.319178581237793, "lex_sim": 1.0}
{"question": "Which British Cathedral has three spires?", "answer": ["Canterbury Cathedral"], "scores": [[0.04405546560883522, 0.20447544753551483]], "normalized_score": [0.0949118584394455], "tokens": [["\u0120Canterbury", "\u0120Cathedral"]], "entropy": [[4.166866302490234, 1.4239107370376587]], "avg_entropy": [2.7953885197639465], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Licidfelth", "score_of_biggest_cluster": 0.0949118584394455, "score_of_first_answer": 0.0949118584394455, "max_score_of_first_answer": 0.20447544753551483, "min_score_of_first_answer": 0.04405546560883522, "avg_score": 0.0949118584394455, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0802099909633398, "std_of_first_answer": 0.0802099909633398, "avg_entropy_of_first_answer": 2.7953885197639465, "lex_sim": 1.0}
{"question": "When Jim Laker took 19 wickets in the Old Trafford Test Match in 1956, who was the Australian captain?", "answer": ["Allan Border"], "scores": [[0.009852465242147446, 0.5590326189994812]], "normalized_score": [0.0742148905992508], "tokens": [["\u0120Allan", "\u0120Border"]], "entropy": [[3.8457627296447754, 2.9376537799835205]], "avg_entropy": [3.391708254814148], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Richie Benaud", "score_of_biggest_cluster": 0.0742148905992508, "score_of_first_answer": 0.0742148905992508, "max_score_of_first_answer": 0.5590326189994812, "min_score_of_first_answer": 0.009852465242147446, "avg_score": 0.0742148905992508, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2745900768786669, "std_of_first_answer": 0.2745900768786669, "avg_entropy_of_first_answer": 3.391708254814148, "lex_sim": 1.0}
{"question": "Which TV series was about a pop group called 'Little Ladies' featuring Charlotte Cornwell, Julie Covington and Rula Lenska?", "answer": ["'90s pop group Spice Girls"], "scores": [[0.07918444275856018, 0.00025963422376662493, 0.7673893570899963, 0.018985796719789505, 0.2755134701728821, 0.026244910433888435, 0.9233427047729492]], "normalized_score": [0.05718541517853737], "tokens": [["\u0120'", "90", "s", "\u0120pop", "\u0120group", "\u0120Spice", "\u0120Girls"]], "entropy": [[5.936648845672607, 5.544406890869141, 1.3555536270141602, 4.538194179534912, 3.4162731170654297, 4.553261756896973, 0.4472907483577728]], "avg_entropy": [3.6845184522015706], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rock Follies", "score_of_biggest_cluster": 0.05718541517853737, "score_of_first_answer": 0.05718541517853737, "max_score_of_first_answer": 0.9233427047729492, "min_score_of_first_answer": 0.00025963422376662493, "avg_score": 0.05718541517853737, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35859076960568703, "std_of_first_answer": 0.35859076960568703, "avg_entropy_of_first_answer": 3.6845184522015706, "lex_sim": 1.0}
{"question": "In which country is the southern pillar of Hercules", "answer": ["Spain"], "scores": [[0.01890600472688675]], "normalized_score": [0.0189060028642416], "tokens": [["\u0120Spain"]], "entropy": [[5.102377891540527]], "avg_entropy": [5.102377891540527], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ISO 3166-1:MA", "score_of_biggest_cluster": 0.0189060028642416, "score_of_first_answer": 0.0189060028642416, "max_score_of_first_answer": 0.01890600472688675, "min_score_of_first_answer": 0.01890600472688675, "avg_score": 0.0189060028642416, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.102377891540527, "lex_sim": 1.0}
{"question": "\"Which 19th century English writer wrote \"\"Swallows and Amazons\"\", \"\"Peter Duck\"\" and \"\"Secret Water\"\"?\"", "answer": ["James Hilton"], "scores": [[0.0024731664452701807, 0.09475113451480865]], "normalized_score": [0.015308018773794174], "tokens": [["\u0120James", "\u0120Hilton"]], "entropy": [[2.7997193336486816, 4.359157562255859]], "avg_entropy": [3.5794384479522705], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Arthur Michell Ransome", "score_of_biggest_cluster": 0.015308018773794174, "score_of_first_answer": 0.015308018773794174, "max_score_of_first_answer": 0.09475113451480865, "min_score_of_first_answer": 0.0024731664452701807, "avg_score": 0.015308018773794174, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04613898403476924, "std_of_first_answer": 0.04613898403476924, "avg_entropy_of_first_answer": 3.5794384479522705, "lex_sim": 1.0}
{"question": "In which year did divorce become officially legal in the Republic of Ireland?", "answer": ["1996"], "scores": [[0.012322012335062027]], "normalized_score": [0.012322012335062027], "tokens": [["\u01201996"]], "entropy": [[5.197423458099365]], "avg_entropy": [5.197423458099365], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and ninety-seven", "score_of_biggest_cluster": 0.012322012335062027, "score_of_first_answer": 0.012322012335062027, "max_score_of_first_answer": 0.012322012335062027, "min_score_of_first_answer": 0.012322012335062027, "avg_score": 0.012322012335062027, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.197423458099365, "lex_sim": 1.0}
{"question": "David John Moore Cornwell is better known as what spy-story writer?", "answer": ["Ian Fleming"], "scores": [[0.026080038398504257, 0.9473046064376831]], "normalized_score": [0.15718060731887817], "tokens": [["\u0120Ian", "\u0120Fleming"]], "entropy": [[3.9348812103271484, 0.5034939050674438]], "avg_entropy": [2.219187557697296], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John le Carre", "score_of_biggest_cluster": 0.15718060731887817, "score_of_first_answer": 0.15718060731887817, "max_score_of_first_answer": 0.9473046064376831, "min_score_of_first_answer": 0.026080038398504257, "avg_score": 0.15718060731887817, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4606122840195894, "std_of_first_answer": 0.4606122840195894, "avg_entropy_of_first_answer": 2.219187557697296, "lex_sim": 1.0}
{"question": "What song was a hit for Tom Jones and Cerys Matthews in 1999?", "answer": ["Who Loves You"], "scores": [[0.002400008961558342, 0.00885658711194992, 0.9951176643371582, 0.9472737312316895]], "normalized_score": [0.06690480560064316], "tokens": [["\u0120Who", "\u0120L", "oves", "\u0120You"]], "entropy": [[5.745677471160889, 3.392570972442627, 0.03288235515356064, 0.32909947633743286]], "avg_entropy": [2.3750575687736273], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "It's Cold Outside", "score_of_biggest_cluster": 0.06690480560064316, "score_of_first_answer": 0.06690480560064316, "max_score_of_first_answer": 0.9951176643371582, "min_score_of_first_answer": 0.002400008961558342, "avg_score": 0.06690480560064316, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4830853361787342, "std_of_first_answer": 0.4830853361787342, "avg_entropy_of_first_answer": 2.3750575687736273, "lex_sim": 1.0}
{"question": "Who won the Best Actress Oscar for the 1988 film, The Accused?", "answer": ["Winona Ryder"], "scores": [[0.007215431425720453, 0.9601882696151733, 0.9896818995475769]], "normalized_score": [0.18997861444950104], "tokens": [["\u0120Win", "ona", "\u0120Ryder"]], "entropy": [[4.932563781738281, 0.39575862884521484, 0.10313193500041962]], "avg_entropy": [1.8104847818613052], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jody Foster", "score_of_biggest_cluster": 0.18997861444950104, "score_of_first_answer": 0.18997861444950104, "max_score_of_first_answer": 0.9896818995475769, "min_score_of_first_answer": 0.007215431425720453, "avg_score": 0.18997861444950104, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4563462946195923, "std_of_first_answer": 0.4563462946195923, "avg_entropy_of_first_answer": 1.8104847818613052, "lex_sim": 1.0}
{"question": "What is the young of a hawk called?", "answer": ["Young hawk"], "scores": [[0.041181594133377075, 0.15739549696445465]], "normalized_score": [0.08050961792469025], "tokens": [["\u0120Young", "\u0120hawk"]], "entropy": [[6.363058567047119, 3.660837173461914]], "avg_entropy": [5.011947870254517], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Falco (bird)", "score_of_biggest_cluster": 0.08050961792469025, "score_of_first_answer": 0.08050961792469025, "max_score_of_first_answer": 0.15739549696445465, "min_score_of_first_answer": 0.041181594133377075, "avg_score": 0.08050961792469025, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05810695141553879, "std_of_first_answer": 0.05810695141553879, "avg_entropy_of_first_answer": 5.011947870254517, "lex_sim": 1.0}
{"question": "Who won the 2013 British Open Golf Championship?", "answer": ["Adam Scott"], "scores": [[0.051734570413827896, 0.9508569240570068]], "normalized_score": [0.22179308533668518], "tokens": [["\u0120Adam", "\u0120Scott"]], "entropy": [[4.175582408905029, 0.3504965901374817]], "avg_entropy": [2.2630394995212555], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Philip Mickelson", "score_of_biggest_cluster": 0.22179308533668518, "score_of_first_answer": 0.22179308533668518, "max_score_of_first_answer": 0.9508569240570068, "min_score_of_first_answer": 0.051734570413827896, "avg_score": 0.22179308533668518, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44956117682158947, "std_of_first_answer": 0.44956117682158947, "avg_entropy_of_first_answer": 2.2630394995212555, "lex_sim": 1.0}
{"question": "Breve, Mocha and Americano are all types of which drink?", "answer": ["Americano"], "scores": [[0.008278856053948402, 0.7031492590904236]], "normalized_score": [0.07629724591970444], "tokens": [["\u0120American", "o"]], "entropy": [[2.695977210998535, 1.5425413846969604]], "avg_entropy": [2.119259297847748], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kawha", "score_of_biggest_cluster": 0.07629724591970444, "score_of_first_answer": 0.07629724591970444, "max_score_of_first_answer": 0.7031492590904236, "min_score_of_first_answer": 0.008278856053948402, "avg_score": 0.07629724591970444, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3474352015182376, "std_of_first_answer": 0.3474352015182376, "avg_entropy_of_first_answer": 2.119259297847748, "lex_sim": 1.0}
{"question": "Arthur Dent and Ford Prefect first appear in which book?", "answer": ["Hitchhiker's Guide to the galaxy"], "scores": [[0.037037286907434464, 0.7946233153343201, 0.4867914319038391, 0.9403884410858154, 0.8377342820167542, 0.8909339308738708, 0.9831908345222473, 0.02267291583120823]], "normalized_score": [0.34979891777038574], "tokens": [["\u0120Hitch", "h", "iker", "'s", "\u0120Guide", "\u0120to", "\u0120the", "\u0120galaxy"]], "entropy": [[3.1526975631713867, 0.9191924333572388, 0.7828696966171265, 0.4402865767478943, 0.8841695785522461, 0.558623194694519, 0.134695366024971, 0.3383110761642456]], "avg_entropy": [0.9013556856662035], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hitchhiker's Guide to the Galaxy", "score_of_biggest_cluster": 0.34979891777038574, "score_of_first_answer": 0.34979891777038574, "max_score_of_first_answer": 0.9831908345222473, "min_score_of_first_answer": 0.02267291583120823, "avg_score": 0.34979891777038574, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3708290590320346, "std_of_first_answer": 0.3708290590320346, "avg_entropy_of_first_answer": 0.9013556856662035, "lex_sim": 1.0}
{"question": "The name of which chemical element, with the symbol 'Tc', is derived from the Greek for 'artificial'?", "answer": ["Technetium, one of three artificial elements added to the periodic table in 1967"], "scores": [[0.5871754288673401, 0.9640046954154968, 0.9994383454322815, 0.026332871988415718, 0.005263907369226217, 0.898642361164093, 0.03430435061454773, 0.08850052952766418, 0.612503707408905, 0.006801525596529245, 0.8946589827537537, 0.8752493858337402, 0.751147985458374, 0.9880504012107849, 0.27786606550216675, 0.0071028294041752815]], "normalized_score": [0.1780729442834854], "tokens": [["\u0120Techn", "et", "ium", ",", "\u0120one", "\u0120of", "\u0120three", "\u0120artificial", "\u0120elements", "\u0120added", "\u0120to", "\u0120the", "\u0120periodic", "\u0120table", "\u0120in", "\u01201967"]], "entropy": [[2.09004545211792, 0.24147829413414001, 0.006519464310258627, 0.7156703472137451, 4.668648719787598, 0.8061687350273132, 1.2591288089752197, 3.944762945175171, 2.1081156730651855, 3.3872976303100586, 0.5847196578979492, 1.0766311883926392, 1.163564682006836, 0.0969030112028122, 2.5048341751098633, 3.8350720405578613]], "avg_entropy": [1.7805975515802857], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Technetium tc 99m sulfur colloid", "score_of_biggest_cluster": 0.1780729442834854, "score_of_first_answer": 0.1780729442834854, "max_score_of_first_answer": 0.9994383454322815, "min_score_of_first_answer": 0.005263907369226217, "avg_score": 0.1780729442834854, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40563176007295193, "std_of_first_answer": 0.40563176007295193, "avg_entropy_of_first_answer": 1.7805975515802857, "lex_sim": 1.0}
{"question": "In 2009 Artem Silchenko was first in the Red Bull World Series held at Antalya, Turkey & Sisikon, Switzerland in which sport?", "answer": ["Speed Skating"], "scores": [[0.014864479191601276, 0.23611852526664734, 0.9797523021697998]], "normalized_score": [0.15093804895877838], "tokens": [["\u0120Speed", "\u0120Sk", "ating"]], "entropy": [[5.55864953994751, 3.6672024726867676, 0.1379927694797516]], "avg_entropy": [3.1212815940380096], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cliffdiving (disambiguation)", "score_of_biggest_cluster": 0.15093804895877838, "score_of_first_answer": 0.15093804895877838, "max_score_of_first_answer": 0.9797523021697998, "min_score_of_first_answer": 0.014864479191601276, "avg_score": 0.15093804895877838, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4127082717297526, "std_of_first_answer": 0.4127082717297526, "avg_entropy_of_first_answer": 3.1212815940380096, "lex_sim": 1.0}
{"question": "The first (joint-)winner of the Nobel Peace Prize, Swiss Jean Henri Dunant, in 1901, founded which international organization?", "answer": ["La Red Cross"], "scores": [[0.004455229267477989, 0.06967931240797043, 0.7208417057991028]], "normalized_score": [0.060711558908224106], "tokens": [["\u0120La", "\u0120Red", "\u0120Cross"]], "entropy": [[2.2719218730926514, 3.7510905265808105, 1.0768356323242188]], "avg_entropy": [2.3666160106658936], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Red Cross (disambiguation)", "score_of_biggest_cluster": 0.060711558908224106, "score_of_first_answer": 0.060711558908224106, "max_score_of_first_answer": 0.7208417057991028, "min_score_of_first_answer": 0.004455229267477989, "avg_score": 0.060711558908224106, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3234323263143109, "std_of_first_answer": 0.3234323263143109, "avg_entropy_of_first_answer": 2.3666160106658936, "lex_sim": 1.0}
{"question": "What monkey is friends with the Man in the Yellow Hat?", "answer": ["The Banana King"], "scores": [[0.08328329026699066, 0.0022747896146029234, 0.006739675533026457]], "normalized_score": [0.010848743841052055], "tokens": [["\u0120The", "\u0120Banana", "\u0120King"]], "entropy": [[6.049674987792969, 6.59260368347168, 2.668915033340454]], "avg_entropy": [5.103731234868367], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Man in the Yellow Hat", "score_of_biggest_cluster": 0.010848743841052055, "score_of_first_answer": 0.010848743841052055, "max_score_of_first_answer": 0.08328329026699066, "min_score_of_first_answer": 0.0022747896146029234, "avg_score": 0.010848743841052055, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.03718009823182586, "std_of_first_answer": 0.03718009823182586, "avg_entropy_of_first_answer": 5.103731234868367, "lex_sim": 1.0}
{"question": "Whose horse was Black Nell?", "answer": ["George IV's black racehorse"], "scores": [[0.01722109690308571, 0.09084354341030121, 0.3742779493331909, 0.005453913938254118, 0.001864374615252018, 0.6384471654891968]], "normalized_score": [0.039505038410425186], "tokens": [["\u0120George", "\u0120IV", "'s", "\u0120black", "\u0120race", "horse"]], "entropy": [[6.096650123596191, 4.790802955627441, 1.4334392547607422, 1.9141359329223633, 2.660428047180176, 1.186413288116455]], "avg_entropy": [3.0136449337005615], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wild Bill Hickok's", "score_of_biggest_cluster": 0.039505038410425186, "score_of_first_answer": 0.039505038410425186, "max_score_of_first_answer": 0.6384471654891968, "min_score_of_first_answer": 0.001864374615252018, "avg_score": 0.039505038410425186, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23950521105442305, "std_of_first_answer": 0.23950521105442305, "avg_entropy_of_first_answer": 3.0136449337005615, "lex_sim": 1.0}
{"question": "Whose hit 'I Will Survive' became an anthem for Women's' Lib?", "answer": ["Brenda Fossey"], "scores": [[0.0009396079112775624, 0.08647949248552322, 0.019295107573270798, 0.13552099466323853]], "normalized_score": [0.02146981842815876], "tokens": [["\u0120Brenda", "\u0120F", "os", "sey"]], "entropy": [[2.812284231185913, 3.1449694633483887, 2.6102797985076904, 2.612943172454834]], "avg_entropy": [2.7951191663742065], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "I Will Survive (album)", "score_of_biggest_cluster": 0.02146981842815876, "score_of_first_answer": 0.02146981842815876, "max_score_of_first_answer": 0.13552099466323853, "min_score_of_first_answer": 0.0009396079112775624, "avg_score": 0.02146981842815876, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05373163245625899, "std_of_first_answer": 0.05373163245625899, "avg_entropy_of_first_answer": 2.7951191663742065, "lex_sim": 1.0}
{"question": "Which car company advertises with the slogan The best or nothing", "answer": ["Hyundai"], "scores": [[0.0026933038607239723]], "normalized_score": [0.0026933043263852596], "tokens": [["\u0120Hyundai"]], "entropy": [[5.432906627655029]], "avg_entropy": [5.432906627655029], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mercedes benz", "score_of_biggest_cluster": 0.0026933043263852596, "score_of_first_answer": 0.0026933043263852596, "max_score_of_first_answer": 0.0026933038607239723, "min_score_of_first_answer": 0.0026933038607239723, "avg_score": 0.0026933043263852596, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.432906627655029, "lex_sim": 1.0}
{"question": "What is Australian slang for an outside lavatory?", "answer": ["A back door"], "scores": [[0.02408500574529171, 0.007310985121876001, 0.08894310891628265]], "normalized_score": [0.025019491091370583], "tokens": [["\u0120A", "\u0120back", "\u0120door"]], "entropy": [[6.182593822479248, 7.063137054443359, 4.7734904289245605]], "avg_entropy": [6.006407101949056], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dunny", "score_of_biggest_cluster": 0.025019491091370583, "score_of_first_answer": 0.025019491091370583, "max_score_of_first_answer": 0.08894310891628265, "min_score_of_first_answer": 0.007310985121876001, "avg_score": 0.025019491091370583, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.035200607554729965, "std_of_first_answer": 0.035200607554729965, "avg_entropy_of_first_answer": 6.006407101949056, "lex_sim": 1.0}
{"question": "In which year did Picasso die?", "answer": ["1986"], "scores": [[0.0032005661632865667]], "normalized_score": [0.0032005656976252794], "tokens": [["\u01201986"]], "entropy": [[4.816738128662109]], "avg_entropy": [4.816738128662109], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and seventy-three", "score_of_biggest_cluster": 0.0032005656976252794, "score_of_first_answer": 0.0032005656976252794, "max_score_of_first_answer": 0.0032005661632865667, "min_score_of_first_answer": 0.0032005661632865667, "avg_score": 0.0032005656976252794, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.816738128662109, "lex_sim": 1.0}
{"question": "Which South Korean city was the venue of the 2011 World Athletics Championships?", "answer": ["Seocho"], "scores": [[0.024412065744400024, 0.05101565271615982, 0.9479060173034668]], "normalized_score": [0.10568772256374359], "tokens": [["\u0120Se", "och", "o"]], "entropy": [[3.5286030769348145, 2.5377378463745117, 0.3573346734046936]], "avg_entropy": [2.141225198904673], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gu Nam Middle School", "score_of_biggest_cluster": 0.10568772256374359, "score_of_first_answer": 0.10568772256374359, "max_score_of_first_answer": 0.9479060173034668, "min_score_of_first_answer": 0.024412065744400024, "avg_score": 0.10568772256374359, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42920613481888426, "std_of_first_answer": 0.42920613481888426, "avg_entropy_of_first_answer": 2.141225198904673, "lex_sim": 1.0}
{"question": "Which guitar innovator and player has a range of Gibson Guitars named after him?", "answer": ["Jimmy Page"], "scores": [[0.030557429417967796, 0.7112013101577759]], "normalized_score": [0.14741940796375275], "tokens": [["\u0120Jimmy", "\u0120Page"]], "entropy": [[4.723746299743652, 2.1082239151000977]], "avg_entropy": [3.415985107421875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Les Paul", "score_of_biggest_cluster": 0.14741940796375275, "score_of_first_answer": 0.14741940796375275, "max_score_of_first_answer": 0.7112013101577759, "min_score_of_first_answer": 0.030557429417967796, "avg_score": 0.14741940796375275, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34032194036990404, "std_of_first_answer": 0.34032194036990404, "avg_entropy_of_first_answer": 3.415985107421875, "lex_sim": 1.0}
{"question": "In which European city is Kurtfurstendamm the main shopping street?", "answer": ["Munich"], "scores": [[0.05368189886212349]], "normalized_score": [0.05368189141154289], "tokens": [["\u0120Munich"]], "entropy": [[3.4455556869506836]], "avg_entropy": [3.4455556869506836], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Berlin-Zentrum", "score_of_biggest_cluster": 0.05368189141154289, "score_of_first_answer": 0.05368189141154289, "max_score_of_first_answer": 0.05368189886212349, "min_score_of_first_answer": 0.05368189886212349, "avg_score": 0.05368189141154289, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.4455556869506836, "lex_sim": 1.0}
{"question": "\"Which book of the New Testament, as with Genesis, starts with the words \"\"In the beginning ....\"\" ?\"", "answer": ["The Gospel of Luke"], "scores": [[0.1389317512512207, 0.42048022150993347, 0.6340695023536682, 0.21726886928081512]], "normalized_score": [0.2995162308216095], "tokens": [["\u0120The", "\u0120Gospel", "\u0120of", "\u0120Luke"]], "entropy": [[3.868154525756836, 3.215646266937256, 1.19657301902771, 1.8065769672393799]], "avg_entropy": [2.5217376947402954], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gospel Of John", "score_of_biggest_cluster": 0.2995162308216095, "score_of_first_answer": 0.2995162308216095, "max_score_of_first_answer": 0.6340695023536682, "min_score_of_first_answer": 0.1389317512512207, "avg_score": 0.2995162308216095, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19222474476980306, "std_of_first_answer": 0.19222474476980306, "avg_entropy_of_first_answer": 2.5217376947402954, "lex_sim": 1.0}
{"question": "Director of Home Alone and the first two Harry Potter films and a 15th/16th century Genoese explorer.", "answer": ["John Smith"], "scores": [[0.11292453855276108, 0.011228520423173904]], "normalized_score": [0.03560864180326462], "tokens": [["\u0120John", "\u0120Smith"]], "entropy": [[4.201657772064209, 5.462841987609863]], "avg_entropy": [4.832249879837036], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Christoffa Corombo", "score_of_biggest_cluster": 0.03560864180326462, "score_of_first_answer": 0.03560864180326462, "max_score_of_first_answer": 0.11292453855276108, "min_score_of_first_answer": 0.011228520423173904, "avg_score": 0.03560864180326462, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05084800906479359, "std_of_first_answer": 0.05084800906479359, "avg_entropy_of_first_answer": 4.832249879837036, "lex_sim": 1.0}
{"question": "First appearing on TV in 1967, which detective solved all of his crimes sitting down?", "answer": ["Hercule Poirot"], "scores": [[0.043559350073337555, 0.8090837001800537, 0.9592369794845581, 0.9989954829216003, 0.9984293580055237]], "normalized_score": [0.5076639652252197], "tokens": [["\u0120Her", "cule", "\u0120Po", "iro", "t"]], "entropy": [[4.33144474029541, 0.9497328400611877, 0.36644232273101807, 0.01042969711124897, 0.01735735312104225]], "avg_entropy": [1.1350813906639814], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ironside (disambiguation)", "score_of_biggest_cluster": 0.5076639652252197, "score_of_first_answer": 0.5076639652252197, "max_score_of_first_answer": 0.9989954829216003, "min_score_of_first_answer": 0.043559350073337555, "avg_score": 0.5076639652252197, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3658802733072857, "std_of_first_answer": 0.3658802733072857, "avg_entropy_of_first_answer": 1.1350813906639814, "lex_sim": 1.0}
{"question": "The name of which chemical element, with the symbol 'Rh', is derived from the Greek word for 'rose'?", "answer": ["Rhodium"], "scores": [[0.35053130984306335, 0.9815772175788879]], "normalized_score": [0.586577832698822], "tokens": [["\u0120Rhod", "ium"]], "entropy": [[3.4641735553741455, 0.16453565657138824]], "avg_entropy": [1.8143546059727669], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rodium", "score_of_biggest_cluster": 0.586577832698822, "score_of_first_answer": 0.586577832698822, "max_score_of_first_answer": 0.9815772175788879, "min_score_of_first_answer": 0.35053130984306335, "avg_score": 0.586577832698822, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3155229538679123, "std_of_first_answer": 0.3155229538679123, "avg_entropy_of_first_answer": 1.8143546059727669, "lex_sim": 1.0}
{"question": "What name is given to a word or phrase in a computer document that can be clicked on In order to move to a related document?", "answer": ["Auto-link"], "scores": [[0.0004970312002114952, 0.727044403553009, 0.09804460406303406]], "normalized_score": [0.03284400328993797], "tokens": [["\u0120Auto", "-", "link"]], "entropy": [[2.795624256134033, 2.185418128967285, 4.4221510887146]], "avg_entropy": [3.134397824605306], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Weblinks", "score_of_biggest_cluster": 0.03284400328993797, "score_of_first_answer": 0.03284400328993797, "max_score_of_first_answer": 0.727044403553009, "min_score_of_first_answer": 0.0004970312002114952, "avg_score": 0.03284400328993797, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3219778047910568, "std_of_first_answer": 0.3219778047910568, "avg_entropy_of_first_answer": 3.134397824605306, "lex_sim": 1.0}
{"question": "\"What event was the song \"\"Waka Waka\"\" written for?\"", "answer": ["World Cup song"], "scores": [[0.030261896550655365, 0.834429919719696, 0.010276151821017265]], "normalized_score": [0.06378309428691864], "tokens": [["\u0120World", "\u0120Cup", "\u0120song"]], "entropy": [[5.550281047821045, 1.2590781450271606, 3.5299324989318848]], "avg_entropy": [3.4464305639266968], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "FIFA 2010 Cup", "score_of_biggest_cluster": 0.06378309428691864, "score_of_first_answer": 0.06378309428691864, "max_score_of_first_answer": 0.834429919719696, "min_score_of_first_answer": 0.010276151821017265, "avg_score": 0.06378309428691864, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.38388584423289035, "std_of_first_answer": 0.38388584423289035, "avg_entropy_of_first_answer": 3.4464305639266968, "lex_sim": 1.0}
{"question": "Ursus arctos horribilis is the scientific name of which animal?", "answer": ["Great White shark"], "scores": [[0.004819266498088837, 0.04139542579650879, 0.03540014103055]], "normalized_score": [0.019185781478881836], "tokens": [["\u0120Great", "\u0120White", "\u0120shark"]], "entropy": [[3.894322395324707, 2.8857765197753906, 1.9380285739898682]], "avg_entropy": [2.906042496363322], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Silvertip Bear", "score_of_biggest_cluster": 0.019185781478881836, "score_of_first_answer": 0.019185781478881836, "max_score_of_first_answer": 0.04139542579650879, "min_score_of_first_answer": 0.004819266498088837, "avg_score": 0.019185781478881836, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.016017173962200218, "std_of_first_answer": 0.016017173962200218, "avg_entropy_of_first_answer": 2.906042496363322, "lex_sim": 1.0}
{"question": "Which prominent Irish Nationalist was hung for treason in 1916 due to his involvement in the Easter Uprising?", "answer": ["James Connolly"], "scores": [[0.08470378071069717, 0.9187459349632263, 0.9952337145805359]], "normalized_score": [0.426259845495224], "tokens": [["\u0120James", "\u0120Conn", "olly"]], "entropy": [[3.9883244037628174, 0.5911011099815369, 0.042713411152362823]], "avg_entropy": [1.5407129749655724], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Roger Casement", "score_of_biggest_cluster": 0.426259845495224, "score_of_first_answer": 0.426259845495224, "max_score_of_first_answer": 0.9952337145805359, "min_score_of_first_answer": 0.08470378071069717, "avg_score": 0.426259845495224, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4123835129765601, "std_of_first_answer": 0.4123835129765601, "avg_entropy_of_first_answer": 1.5407129749655724, "lex_sim": 1.0}
{"question": "John Lennon had two number one hits in 1981. 'Imagine' was one, what was the other?", "answer": ["'Whatever Gets You Thru the Night'"], "scores": [[0.4330795109272003, 0.026435542851686478, 0.9163084626197815, 0.967755913734436, 0.312565416097641, 0.9799661040306091, 0.599657416343689, 0.9889453053474426, 0.9042253494262695]], "normalized_score": [0.4912932813167572], "tokens": [["\u0120'", "Whatever", "\u0120Gets", "\u0120You", "\u0120Th", "ru", "\u0120the", "\u0120Night", "'"]], "entropy": [[3.422173261642456, 4.586419105529785, 0.49749502539634705, 0.17542535066604614, 1.3961811065673828, 0.1626550853252411, 0.8086403608322144, 0.08398328721523285, 0.6035779714584351]], "avg_entropy": [1.3040611727370157], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A women", "score_of_biggest_cluster": 0.4912932813167572, "score_of_first_answer": 0.4912932813167572, "max_score_of_first_answer": 0.9889453053474426, "min_score_of_first_answer": 0.026435542851686478, "avg_score": 0.4912932813167572, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33398594137002624, "std_of_first_answer": 0.33398594137002624, "avg_entropy_of_first_answer": 1.3040611727370157, "lex_sim": 1.0}
{"question": "The Tumut Hydroelectric Power Station is in which Australian state?", "answer": ["Horseshoe Falls"], "scores": [[0.00019682254060171545, 0.012453109957277775, 0.10190783441066742, 0.9863471388816833, 0.012084035202860832]], "normalized_score": [0.019713344052433968], "tokens": [["\u0120H", "ors", "esh", "oe", "\u0120Falls"]], "entropy": [[1.9320048093795776, 4.971475124359131, 0.947955846786499, 0.11323767155408859, 3.0231404304504395]], "avg_entropy": [2.197562776505947], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "AU-NSW", "score_of_biggest_cluster": 0.019713344052433968, "score_of_first_answer": 0.019713344052433968, "max_score_of_first_answer": 0.9863471388816833, "min_score_of_first_answer": 0.00019682254060171545, "avg_score": 0.019713344052433968, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.383619121299571, "std_of_first_answer": 0.383619121299571, "avg_entropy_of_first_answer": 2.197562776505947, "lex_sim": 1.0}
{"question": "With which sport is O J Simpson associated?", "answer": ["Baseball"], "scores": [[0.08003059774637222]], "normalized_score": [0.08003059774637222], "tokens": [["\u0120Baseball"]], "entropy": [[3.5208170413970947]], "avg_entropy": [3.5208170413970947], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "U.s. football", "score_of_biggest_cluster": 0.08003059774637222, "score_of_first_answer": 0.08003059774637222, "max_score_of_first_answer": 0.08003059774637222, "min_score_of_first_answer": 0.08003059774637222, "avg_score": 0.08003059774637222, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.5208170413970947, "lex_sim": 1.0}
{"question": "Who is the presenter of the TV series The School of Hard Sums?", "answer": ["Tomoaki Nagata"], "scores": [[0.010749254375696182, 0.0013755161780864, 0.024553559720516205, 0.196867436170578, 0.11010759323835373]], "normalized_score": [0.023943627253174782], "tokens": [["\u0120Tom", "o", "aki", "\u0120Nag", "ata"]], "entropy": [[6.005705833435059, 6.215095520019531, 5.4441680908203125, 3.6059060096740723, 2.658390522003174]], "avg_entropy": [4.7858531951904295], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dara \u00d3 Briain", "score_of_biggest_cluster": 0.023943627253174782, "score_of_first_answer": 0.023943627253174782, "max_score_of_first_answer": 0.196867436170578, "min_score_of_first_answer": 0.0013755161780864, "avg_score": 0.023943627253174782, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07480809982634708, "std_of_first_answer": 0.07480809982634708, "avg_entropy_of_first_answer": 4.7858531951904295, "lex_sim": 1.0}
{"question": "What word for a surprise attack originally referred to hiding in the woods?", "answer": ["Forest"], "scores": [[0.008588986471295357]], "normalized_score": [0.008588985539972782], "tokens": [["\u0120Forest"]], "entropy": [[6.385451316833496]], "avg_entropy": [6.385451316833496], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ambushes", "score_of_biggest_cluster": 0.008588985539972782, "score_of_first_answer": 0.008588985539972782, "max_score_of_first_answer": 0.008588986471295357, "min_score_of_first_answer": 0.008588986471295357, "avg_score": 0.008588985539972782, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.385451316833496, "lex_sim": 1.0}
{"question": "For which scientific discovery did Crick and Watson receive the 1962 Nobel Prize for Medicine?", "answer": ["DNA"], "scores": [[0.37663063406944275]], "normalized_score": [0.37663063406944275], "tokens": [["\u0120DNA"]], "entropy": [[3.181849479675293]], "avg_entropy": [3.181849479675293], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dna structure", "score_of_biggest_cluster": 0.37663063406944275, "score_of_first_answer": 0.37663063406944275, "max_score_of_first_answer": 0.37663063406944275, "min_score_of_first_answer": 0.37663063406944275, "avg_score": 0.37663063406944275, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.181849479675293, "lex_sim": 1.0}
{"question": "What Detroit-area stadium was sold in 2009 for just $583,000?", "answer": ["Comiskey"], "scores": [[0.042770594358444214, 0.011591042391955853, 0.9979623556137085]], "normalized_score": [0.07909102737903595], "tokens": [["\u0120Com", "is", "key"]], "entropy": [[5.312856197357178, 0.2537161111831665, 0.02025054767727852]], "avg_entropy": [1.8622742854058743], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Silverdome", "score_of_biggest_cluster": 0.07909102737903595, "score_of_first_answer": 0.07909102737903595, "max_score_of_first_answer": 0.9979623556137085, "min_score_of_first_answer": 0.011591042391955853, "avg_score": 0.07909102737903595, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45780779964929597, "std_of_first_answer": 0.45780779964929597, "avg_entropy_of_first_answer": 1.8622742854058743, "lex_sim": 1.0}
{"question": "Annually in July or August, the Obon (or Bon) Buddhist festival to honour the dead is held in which country?", "answer": ["Japan"], "scores": [[0.8086773157119751]], "normalized_score": [0.8086773157119751], "tokens": [["\u0120Japan"]], "entropy": [[1.4849295616149902]], "avg_entropy": [1.4849295616149902], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u65e5\u672c\u570b", "score_of_biggest_cluster": 0.8086773157119751, "score_of_first_answer": 0.8086773157119751, "max_score_of_first_answer": 0.8086773157119751, "min_score_of_first_answer": 0.8086773157119751, "avg_score": 0.8086773157119751, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.4849295616149902, "lex_sim": 1.0}
{"question": "The Kina is the monetary unit of which Oceania country?", "answer": ["Solomon Islands"], "scores": [[0.002107734326273203, 0.8094561696052551]], "normalized_score": [0.04130518063902855], "tokens": [["\u0120Solomon", "\u0120Islands"]], "entropy": [[3.6767117977142334, 1.0400694608688354]], "avg_entropy": [2.3583906292915344], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ISO 3166-1:PG", "score_of_biggest_cluster": 0.04130518063902855, "score_of_first_answer": 0.04130518063902855, "max_score_of_first_answer": 0.8094561696052551, "min_score_of_first_answer": 0.002107734326273203, "avg_score": 0.04130518063902855, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40367421763949096, "std_of_first_answer": 0.40367421763949096, "avg_entropy_of_first_answer": 2.3583906292915344, "lex_sim": 1.0}
{"question": "\"In what sport did the slang word \"\"tubular\"\" originate?\"", "answer": ["Rugby"], "scores": [[0.11054576188325882]], "normalized_score": [0.11054574698209763], "tokens": [["\u0120Rugby"]], "entropy": [[5.6159539222717285]], "avg_entropy": [5.6159539222717285], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cutback (surfing move)", "score_of_biggest_cluster": 0.11054574698209763, "score_of_first_answer": 0.11054574698209763, "max_score_of_first_answer": 0.11054576188325882, "min_score_of_first_answer": 0.11054576188325882, "avg_score": 0.11054574698209763, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.6159539222717285, "lex_sim": 1.0}
{"question": "The OK Corral is in which US town?", "answer": ["Oatman, Arizona"], "scores": [[0.016036948189139366, 0.7271790504455566, 0.8059642314910889, 0.559884786605835, 0.9104487299919128]], "normalized_score": [0.3436264395713806], "tokens": [["\u0120O", "at", "man", ",", "\u0120Arizona"]], "entropy": [[1.1643520593643188, 1.7381970882415771, 1.2117340564727783, 1.4045138359069824, 0.6127088069915771]], "avg_entropy": [1.2263011693954469], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gravestone", "score_of_biggest_cluster": 0.3436264395713806, "score_of_first_answer": 0.3436264395713806, "max_score_of_first_answer": 0.9104487299919128, "min_score_of_first_answer": 0.016036948189139366, "avg_score": 0.3436264395713806, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31544258065239544, "std_of_first_answer": 0.31544258065239544, "avg_entropy_of_first_answer": 1.2263011693954469, "lex_sim": 1.0}
{"question": "\"Who said \"\"You're not drunk if you can lie on the floor without holding on\"\"?\"", "answer": ["Plato"], "scores": [[0.0029851377476006746]], "normalized_score": [0.0029851379804313183], "tokens": [["\u0120Plato"]], "entropy": [[5.9748687744140625]], "avg_entropy": [5.9748687744140625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dino Paul Crocetti", "score_of_biggest_cluster": 0.0029851379804313183, "score_of_first_answer": 0.0029851379804313183, "max_score_of_first_answer": 0.0029851377476006746, "min_score_of_first_answer": 0.0029851377476006746, "avg_score": 0.0029851379804313183, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.9748687744140625, "lex_sim": 1.0}
{"question": "What invention by a Mr Schick in 1931 might be said to have changed the face of man ?", "answer": ["Electric razor"], "scores": [[0.025019068270921707, 0.880706787109375]], "normalized_score": [0.14844009280204773], "tokens": [["\u0120Electric", "\u0120razor"]], "entropy": [[4.545315742492676, 0.7796816229820251]], "avg_entropy": [2.6624986827373505], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Electric shaver", "score_of_biggest_cluster": 0.14844009280204773, "score_of_first_answer": 0.14844009280204773, "max_score_of_first_answer": 0.880706787109375, "min_score_of_first_answer": 0.025019068270921707, "avg_score": 0.14844009280204773, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.42784385941922665, "std_of_first_answer": 0.42784385941922665, "avg_entropy_of_first_answer": 2.6624986827373505, "lex_sim": 1.0}
{"question": "\"Who has hosted the US series \"\"Power of 10\"\", \"\"Whose Line Is It Anyway?\"\" and \"\"The Price Is Right\"\"?\"", "answer": ["FOX"], "scores": [[3.450309668551199e-05]], "normalized_score": [3.450308577157557e-05], "tokens": [["\u0120FOX"]], "entropy": [[3.0432193279266357]], "avg_entropy": [3.0432193279266357], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Drew Carey", "score_of_biggest_cluster": 3.450308577157557e-05, "score_of_first_answer": 3.450308577157557e-05, "max_score_of_first_answer": 3.450309668551199e-05, "min_score_of_first_answer": 3.450309668551199e-05, "avg_score": 3.450308577157557e-05, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.0432193279266357, "lex_sim": 1.0}
{"question": "What was the name of the German encryption machine whose code was deciphered at Bletchley Park during World War II ?", "answer": ["Enigma machine"], "scores": [[0.8300677537918091, 0.9945279955863953, 0.034514132887125015]], "normalized_score": [0.3054282069206238], "tokens": [["\u0120En", "igma", "\u0120machine"]], "entropy": [[1.3333594799041748, 0.04516466334462166, 0.5423813462257385]], "avg_entropy": [0.640301829824845], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Steckerbrett", "score_of_biggest_cluster": 0.3054282069206238, "score_of_first_answer": 0.3054282069206238, "max_score_of_first_answer": 0.9945279955863953, "min_score_of_first_answer": 0.034514132887125015, "avg_score": 0.3054282069206238, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41920286145127883, "std_of_first_answer": 0.41920286145127883, "avg_entropy_of_first_answer": 0.640301829824845, "lex_sim": 1.0}
{"question": "Mary Mallon was effectively imprisoned for life in New York for being an irresponsible immune carrier of what disease?", "answer": ["Typhoid"], "scores": [[0.6014565229415894, 0.9910176992416382]], "normalized_score": [0.772045373916626], "tokens": [["\u0120Typh", "oid"]], "entropy": [[2.713526725769043, 0.05639193207025528]], "avg_entropy": [1.3849593289196491], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Westminster fever", "score_of_biggest_cluster": 0.772045373916626, "score_of_first_answer": 0.772045373916626, "max_score_of_first_answer": 0.9910176992416382, "min_score_of_first_answer": 0.6014565229415894, "avg_score": 0.772045373916626, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19478058815002441, "std_of_first_answer": 0.19478058815002441, "avg_entropy_of_first_answer": 1.3849593289196491, "lex_sim": 1.0}
{"question": "Who became Secretary -General of the Soviet Communist Party in 1985?", "answer": ["Mikhail Gorbachev"], "scores": [[0.38332611322402954, 0.9719887375831604, 0.9645178914070129, 0.9963666200637817]], "normalized_score": [0.7735524773597717], "tokens": [["\u0120Mikhail", "\u0120Gor", "b", "achev"]], "entropy": [[3.845628499984741, 0.21974578499794006, 0.26501214504241943, 0.0361761674284935]], "avg_entropy": [1.0916406493633986], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "M. Gorbachev", "score_of_biggest_cluster": 0.7735524773597717, "score_of_first_answer": 0.7735524773597717, "max_score_of_first_answer": 0.9963666200637817, "min_score_of_first_answer": 0.38332611322402954, "avg_score": 0.7735524773597717, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.25760806804925296, "std_of_first_answer": 0.25760806804925296, "avg_entropy_of_first_answer": 1.0916406493633986, "lex_sim": 1.0}
{"question": "Buddy Holly and the Big Bopper were two of the three musicians who died in a plane crash in February 1959. Who was the other one?", "answer": ["Ritchie Valens"], "scores": [[0.18450218439102173, 0.9947269558906555, 0.9988107681274414, 0.9856707453727722]], "normalized_score": [0.6519737243652344], "tokens": [["\u0120R", "itchie", "\u0120Val", "ens"]], "entropy": [[4.183503150939941, 0.05739767849445343, 0.014930114150047302, 0.12453991919755936]], "avg_entropy": [1.0950927156955004], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ritchie S. Valens", "score_of_biggest_cluster": 0.6519737243652344, "score_of_first_answer": 0.6519737243652344, "max_score_of_first_answer": 0.9988107681274414, "min_score_of_first_answer": 0.18450218439102173, "avg_score": 0.6519737243652344, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3501522049117836, "std_of_first_answer": 0.3501522049117836, "avg_entropy_of_first_answer": 1.0950927156955004, "lex_sim": 1.0}
{"question": "What items would be collected by a horologist?", "answer": ["Clocks"], "scores": [[0.06376214325428009, 0.9983260035514832]], "normalized_score": [0.2523002326488495], "tokens": [["\u0120Cl", "ocks"]], "entropy": [[5.1973958015441895, 0.017958303913474083]], "avg_entropy": [2.6076770527288318], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Analog Clocks", "score_of_biggest_cluster": 0.2523002326488495, "score_of_first_answer": 0.2523002326488495, "max_score_of_first_answer": 0.9983260035514832, "min_score_of_first_answer": 0.06376214325428009, "avg_score": 0.2523002326488495, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46728193014860153, "std_of_first_answer": 0.46728193014860153, "avg_entropy_of_first_answer": 2.6076770527288318, "lex_sim": 1.0}
{"question": "Some Americans claim to have seen a Sasquatch; by what name is it better known ?", "answer": ["Yowie"], "scores": [[0.005405702628195286, 0.7455436587333679, 0.6839006543159485]], "normalized_score": [0.14020800590515137], "tokens": [["\u0120Y", "ow", "ie"]], "entropy": [[2.3818893432617188, 2.0060629844665527, 2.225780963897705]], "avg_entropy": [2.2045777638753257], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rickmat", "score_of_biggest_cluster": 0.14020800590515137, "score_of_first_answer": 0.14020800590515137, "max_score_of_first_answer": 0.7455436587333679, "min_score_of_first_answer": 0.005405702628195286, "avg_score": 0.14020800590515137, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33532065143794576, "std_of_first_answer": 0.33532065143794576, "avg_entropy_of_first_answer": 2.2045777638753257, "lex_sim": 1.0}
{"question": "Russia's TU-144 plane was the broad equivalent of which western airliner?", "answer": ["Boeing 727"], "scores": [[0.23208442330360413, 0.44528308510780334, 0.2279113382101059]], "normalized_score": [0.28664830327033997], "tokens": [["\u0120Boeing", "\u01207", "27"]], "entropy": [[4.119162082672119, 2.1224842071533203, 0.9567342400550842]], "avg_entropy": [2.399460176626841], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "CONCORDE", "score_of_biggest_cluster": 0.28664830327033997, "score_of_first_answer": 0.28664830327033997, "max_score_of_first_answer": 0.44528308510780334, "min_score_of_first_answer": 0.2279113382101059, "avg_score": 0.28664830327033997, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10150071722989916, "std_of_first_answer": 0.10150071722989916, "avg_entropy_of_first_answer": 2.399460176626841, "lex_sim": 1.0}
{"question": "\"What was the mission of \"\"Earthview 01: The Founders Flight\"\" which was launched on 21 April 1997?\"", "answer": ["ISS"], "scores": [[0.00014826889673713595]], "normalized_score": [0.00014826883852947503], "tokens": [["\u0120ISS"]], "entropy": [[4.557404518127441]], "avg_entropy": [4.557404518127441], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The first space burial", "score_of_biggest_cluster": 0.00014826883852947503, "score_of_first_answer": 0.00014826883852947503, "max_score_of_first_answer": 0.00014826889673713595, "min_score_of_first_answer": 0.00014826889673713595, "avg_score": 0.00014826883852947503, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.557404518127441, "lex_sim": 1.0}
{"question": "In what country did the Romani ethnic group originate?", "answer": ["Romania"], "scores": [[0.14173898100852966]], "normalized_score": [0.14173899590969086], "tokens": [["\u0120Romania"]], "entropy": [[3.3453197479248047]], "avg_entropy": [3.3453197479248047], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u0d2d\u0d3e\u0d30\u0d24 \u0d2e\u0d39\u0d3e\u0d30\u0d3e\u0d1c\u0d4d\u0d2f\u0d02", "score_of_biggest_cluster": 0.14173899590969086, "score_of_first_answer": 0.14173899590969086, "max_score_of_first_answer": 0.14173898100852966, "min_score_of_first_answer": 0.14173898100852966, "avg_score": 0.14173899590969086, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.3453197479248047, "lex_sim": 1.0}
{"question": "What radio station do you find at 90.3 on your FM dial?", "answer": ["ZZZ"], "scores": [[0.008994609117507935, 0.047747913748025894]], "normalized_score": [0.020723750814795494], "tokens": [["\u0120Z", "ZZ"]], "entropy": [[6.298463821411133, 5.589818000793457]], "avg_entropy": [5.944140911102295], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "KCMU", "score_of_biggest_cluster": 0.020723750814795494, "score_of_first_answer": 0.020723750814795494, "max_score_of_first_answer": 0.047747913748025894, "min_score_of_first_answer": 0.008994609117507935, "avg_score": 0.020723750814795494, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01937665231525898, "std_of_first_answer": 0.01937665231525898, "avg_entropy_of_first_answer": 5.944140911102295, "lex_sim": 1.0}
{"question": "The New Zealand government proposes to create a 620,000 sq km Ocean Sanctuary in 2016 round what island or islands?", "answer": ["South Island"], "scores": [[0.016064448282122612, 0.4104052484035492]], "normalized_score": [0.08119688928127289], "tokens": [["\u0120South", "\u0120Island"]], "entropy": [[5.049497604370117, 2.9181089401245117]], "avg_entropy": [3.9838032722473145], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Kermadecs", "score_of_biggest_cluster": 0.08119688928127289, "score_of_first_answer": 0.08119688928127289, "max_score_of_first_answer": 0.4104052484035492, "min_score_of_first_answer": 0.016064448282122612, "avg_score": 0.08119688928127289, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1971704000607133, "std_of_first_answer": 0.1971704000607133, "avg_entropy_of_first_answer": 3.9838032722473145, "lex_sim": 1.0}
{"question": "What did Manchester United footballer Carlos Tevez put in his mouth in celebrating a goal at Christmas 2007?", "answer": ["A ham"], "scores": [[0.29200559854507446, 0.0039051766507327557]], "normalized_score": [0.033768825232982635], "tokens": [["\u0120A", "\u0120ham"]], "entropy": [[5.0848493576049805, 6.406052589416504]], "avg_entropy": [5.745450973510742], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Adult pacifier", "score_of_biggest_cluster": 0.033768825232982635, "score_of_first_answer": 0.033768825232982635, "max_score_of_first_answer": 0.29200559854507446, "min_score_of_first_answer": 0.0039051766507327557, "avg_score": 0.033768825232982635, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14405021094717085, "std_of_first_answer": 0.14405021094717085, "avg_entropy_of_first_answer": 5.745450973510742, "lex_sim": 1.0}
{"question": "For a point each, name the 4 countries surrounding Belgium.", "answer": ["Belgium, Germany, France, the Netherlands"], "scores": [[0.13200540840625763, 0.693655252456665, 0.20838892459869385, 0.9842090010643005, 0.176217183470726, 0.7627401947975159, 0.04088224098086357, 0.9746782183647156]], "normalized_score": [0.31645703315734863], "tokens": [["\u0120Belgium", ",", "\u0120Germany", ",", "\u0120France", ",", "\u0120the", "\u0120Netherlands"]], "entropy": [[3.4986939430236816, 1.6735278367996216, 2.2710304260253906, 0.1409066617488861, 1.7318916320800781, 0.6748055815696716, 1.8006162643432617, 0.17950227856636047]], "avg_entropy": [1.496371828019619], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "France, Germany, Luxembourg, and the Netherlands", "score_of_biggest_cluster": 0.31645703315734863, "score_of_first_answer": 0.31645703315734863, "max_score_of_first_answer": 0.9842090010643005, "min_score_of_first_answer": 0.04088224098086357, "avg_score": 0.31645703315734863, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37119761612715924, "std_of_first_answer": 0.37119761612715924, "avg_entropy_of_first_answer": 1.496371828019619, "lex_sim": 1.0}
{"question": "Name the book and the author: 'The Time Traveler (for so it will be convenient to speak of him) was expounding a recondite matter to us.'", "answer": ["1984 by George Orwell"], "scores": [[0.004601560067385435, 0.19400177896022797, 0.8653205633163452, 0.9951947331428528]], "normalized_score": [0.16651330888271332], "tokens": [["\u01201984", "\u0120by", "\u0120George", "\u0120Orwell"]], "entropy": [[4.420051097869873, 2.390122413635254, 0.8271515965461731, 0.053990066051483154]], "avg_entropy": [1.9228287935256958], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Time Machine, by H.G. Wells", "score_of_biggest_cluster": 0.16651330888271332, "score_of_first_answer": 0.16651330888271332, "max_score_of_first_answer": 0.9951947331428528, "min_score_of_first_answer": 0.004601560067385435, "avg_score": 0.16651330888271332, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42333725031466074, "std_of_first_answer": 0.42333725031466074, "avg_entropy_of_first_answer": 1.9228287935256958, "lex_sim": 1.0}
{"question": "Former tennis player Vijay Armitrage apears in which James Bond film?", "answer": ["Golden Eye"], "scores": [[0.08774244040250778, 0.0526697151362896]], "normalized_score": [0.06798065453767776], "tokens": [["\u0120Golden", "\u0120Eye"]], "entropy": [[3.7655205726623535, 0.7246853709220886]], "avg_entropy": [2.245102971792221], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kamal kahn", "score_of_biggest_cluster": 0.06798065453767776, "score_of_first_answer": 0.06798065453767776, "max_score_of_first_answer": 0.08774244040250778, "min_score_of_first_answer": 0.0526697151362896, "avg_score": 0.06798065453767776, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.017536362633109093, "std_of_first_answer": 0.017536362633109093, "avg_entropy_of_first_answer": 2.245102971792221, "lex_sim": 1.0}
{"question": "Which river runs through Hamburg?", "answer": ["Elbe"], "scores": [[0.7412494421005249, 0.9933102130889893]], "normalized_score": [0.8580737709999084], "tokens": [["\u0120El", "be"]], "entropy": [[1.5339829921722412, 0.05505282059311867]], "avg_entropy": [0.7945179063826799], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Elbe", "score_of_biggest_cluster": 0.8580737709999084, "score_of_first_answer": 0.8580737709999084, "max_score_of_first_answer": 0.9933102130889893, "min_score_of_first_answer": 0.7412494421005249, "avg_score": 0.8580737709999084, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.12603038549423218, "std_of_first_answer": 0.12603038549423218, "avg_entropy_of_first_answer": 0.7945179063826799, "lex_sim": 1.0}
{"question": "Which is the biggest of the London merchant banks", "answer": ["Barings Bank"], "scores": [[0.07231156527996063, 0.97135990858078, 0.0343630313873291]], "normalized_score": [0.13414043188095093], "tokens": [["\u0120Bar", "ings", "\u0120Bank"]], "entropy": [[5.183029651641846, 0.25006139278411865, 0.626294732093811]], "avg_entropy": [2.0197952588399253], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Deutsche Morgan Grenfell", "score_of_biggest_cluster": 0.13414043188095093, "score_of_first_answer": 0.13414043188095093, "max_score_of_first_answer": 0.97135990858078, "min_score_of_first_answer": 0.0343630313873291, "avg_score": 0.13414043188095093, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4330372273818226, "std_of_first_answer": 0.4330372273818226, "avg_entropy_of_first_answer": 2.0197952588399253, "lex_sim": 1.0}
{"question": "Which 17th/18th century Irish politician leader was nicknamed 'the Liberator\u201d?", "answer": ["\u32cb"], "scores": [[0.0014847950078547, 2.3680790945945773e-06, 0.031922657042741776, 0.011642130091786385]], "normalized_score": [0.0010691744973883033], "tokens": [["\u0120", "\u00e3", "\u012d", "\u012d"]], "entropy": [[3.524812698364258, 4.010204315185547, 3.0361125469207764, 3.865849733352661]], "avg_entropy": [3.6092448234558105], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "'monster meetings'", "score_of_biggest_cluster": 0.0010691744973883033, "score_of_first_answer": 0.0010691744973883033, "max_score_of_first_answer": 0.031922657042741776, "min_score_of_first_answer": 2.3680790945945773e-06, "avg_score": 0.0010691744973883033, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.012741470606606967, "std_of_first_answer": 0.012741470606606967, "avg_entropy_of_first_answer": 3.6092448234558105, "lex_sim": 0.0}
{"question": "Whose 1977-79 albums, Low, \u201cHeroes\u201d and Lodger, are known as his Berlin Trilogy?", "answer": ["\u32ce"], "scores": [[0.002639525569975376, 1.7590499510333757e-06, 0.021652312949299812, 0.02576279081404209]], "normalized_score": [0.0012686018599197268], "tokens": [["\u0120", "\u00e3", "\u012d", "\u0130"]], "entropy": [[4.66500186920166, 4.920298099517822, 2.076551675796509, 3.7432000637054443]], "avg_entropy": [3.851262927055359], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hermione Farthingale", "score_of_biggest_cluster": 0.0012686018599197268, "score_of_first_answer": 0.0012686018599197268, "max_score_of_first_answer": 0.02576279081404209, "min_score_of_first_answer": 1.7590499510333757e-06, "avg_score": 0.0012686018599197268, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0113258624441714, "std_of_first_answer": 0.0113258624441714, "avg_entropy_of_first_answer": 3.851262927055359, "lex_sim": 0.0}
{"question": "At which battle of 2 December 1805 did Napoleon defeat a combined Austrian-Russian army?", "answer": ["Battle of Friedland"], "scores": [[0.01235466543585062, 0.9731585383415222, 0.01767253316938877, 0.9359774589538574]], "normalized_score": [0.11875301599502563], "tokens": [["\u0120Battle", "\u0120of", "\u0120Fried", "land"]], "entropy": [[3.520951271057129, 0.2348816692829132, 2.2960357666015625, 0.35435885190963745]], "avg_entropy": [1.6015568897128105], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Battle of Slavkov", "score_of_biggest_cluster": 0.11875301599502563, "score_of_first_answer": 0.11875301599502563, "max_score_of_first_answer": 0.9731585383415222, "min_score_of_first_answer": 0.01235466543585062, "avg_score": 0.11875301599502563, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4699648458979424, "std_of_first_answer": 0.4699648458979424, "avg_entropy_of_first_answer": 1.6015568897128105, "lex_sim": 1.0}
{"question": "Which US folk singer had the words \u201cThis machine kills fascists\u201d written on his guitar?", "answer": ["Bob Gibson"], "scores": [[0.07179117202758789, 0.0025127711705863476]], "normalized_score": [0.013431110419332981], "tokens": [["\u0120Bob", "\u0120Gibson"]], "entropy": [[4.055857181549072, 0.6606664657592773]], "avg_entropy": [2.358261823654175], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Woody Guthry", "score_of_biggest_cluster": 0.013431110419332981, "score_of_first_answer": 0.013431110419332981, "max_score_of_first_answer": 0.07179117202758789, "min_score_of_first_answer": 0.0025127711705863476, "avg_score": 0.013431110419332981, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.03463920042850077, "std_of_first_answer": 0.03463920042850077, "avg_entropy_of_first_answer": 2.358261823654175, "lex_sim": 1.0}
{"question": "Which actor played the title role in the ITV series 'Tarzan' from 1966 to 1968?", "answer": ["David Taylor"], "scores": [[0.06081072613596916, 0.014135652221739292]], "normalized_score": [0.029318928718566895], "tokens": [["\u0120David", "\u0120Taylor"]], "entropy": [[4.845972061157227, 4.468299865722656]], "avg_entropy": [4.657135963439941], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ron Ely", "score_of_biggest_cluster": 0.029318928718566895, "score_of_first_answer": 0.029318928718566895, "max_score_of_first_answer": 0.06081072613596916, "min_score_of_first_answer": 0.014135652221739292, "avg_score": 0.029318928718566895, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.023337536957114935, "std_of_first_answer": 0.023337536957114935, "avg_entropy_of_first_answer": 4.657135963439941, "lex_sim": 1.0}
{"question": "What is prussic acid, found in bitter almonds and laurel leaves, also known as?", "answer": ["Prussic Acid"], "scores": [[0.009734210558235645, 0.8385422229766846, 0.9523248672485352, 0.06986816972494125]], "normalized_score": [0.15265904366970062], "tokens": [["\u0120Pr", "uss", "ic", "\u0120Acid"]], "entropy": [[5.427027702331543, 0.5599530935287476, 0.3907334804534912, 1.5997062921524048]], "avg_entropy": [1.9943551421165466], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Formic anammonide", "score_of_biggest_cluster": 0.15265904366970062, "score_of_first_answer": 0.15265904366970062, "max_score_of_first_answer": 0.9523248672485352, "min_score_of_first_answer": 0.009734210558235645, "avg_score": 0.15265904366970062, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43022901414959647, "std_of_first_answer": 0.43022901414959647, "avg_entropy_of_first_answer": 1.9943551421165466, "lex_sim": 1.0}
{"question": "\"Who says \"\"Is this a dagger that I see before me\"\" in a Shakespearean play?\"", "answer": ["The Bard"], "scores": [[0.01969822309911251, 0.029751252382993698]], "normalized_score": [0.024208402261137962], "tokens": [["\u0120The", "\u0120Bard"]], "entropy": [[3.9808220863342285, 5.264744758605957]], "avg_entropy": [4.622783422470093], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Macbeth/Act V Scene v", "score_of_biggest_cluster": 0.024208402261137962, "score_of_first_answer": 0.024208402261137962, "max_score_of_first_answer": 0.029751252382993698, "min_score_of_first_answer": 0.01969822309911251, "avg_score": 0.024208402261137962, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.005026514641940594, "std_of_first_answer": 0.005026514641940594, "avg_entropy_of_first_answer": 4.622783422470093, "lex_sim": 1.0}
{"question": "\u2018Lofsongur\u2019 is the national anthem of which country?", "answer": ["India"], "scores": [[0.004265521187335253]], "normalized_score": [0.0042655207216739655], "tokens": [["\u0120India"]], "entropy": [[3.0734128952026367]], "avg_entropy": [3.0734128952026367], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "IJsland", "score_of_biggest_cluster": 0.0042655207216739655, "score_of_first_answer": 0.0042655207216739655, "max_score_of_first_answer": 0.004265521187335253, "min_score_of_first_answer": 0.004265521187335253, "avg_score": 0.0042655207216739655, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.0734128952026367, "lex_sim": 1.0}
{"question": "What type of beverage does a barista prepare and serve ?", "answer": ["Black or White (coffee)"], "scores": [[0.0016818515723571181, 0.004758264869451523, 0.2275513857603073, 0.009729304350912571, 0.21815834939479828, 0.985178530216217, 0.7597265243530273]], "normalized_score": [0.06028255820274353], "tokens": [["\u0120Black", "\u0120or", "\u0120White", "\u0120(", "co", "ffee", ")"]], "entropy": [[3.7004973888397217, 2.6932425498962402, 3.481072425842285, 2.345212936401367, 4.723062992095947, 0.09622116386890411, 1.3980944156646729]], "avg_entropy": [2.6339148389441625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "COFFEE", "score_of_biggest_cluster": 0.06028255820274353, "score_of_first_answer": 0.06028255820274353, "max_score_of_first_answer": 0.985178530216217, "min_score_of_first_answer": 0.0016818515723571181, "avg_score": 0.06028255820274353, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3686954702443352, "std_of_first_answer": 0.3686954702443352, "avg_entropy_of_first_answer": 2.6339148389441625, "lex_sim": 1.0}
{"question": "If the Staten Island Ferry is to New York, the Star Ferry that was originally called the Kowloon Ferry Company is to what city?", "answer": ["Hong Kong"], "scores": [[0.901470422744751, 0.9401586651802063]], "normalized_score": [0.9206113219261169], "tokens": [["\u0120Hong", "\u0120Kong"]], "entropy": [[0.7199355363845825, 0.36286163330078125]], "avg_entropy": [0.5413985848426819], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kong Hong", "score_of_biggest_cluster": 0.9206113219261169, "score_of_first_answer": 0.9206113219261169, "max_score_of_first_answer": 0.9401586651802063, "min_score_of_first_answer": 0.901470422744751, "avg_score": 0.9206113219261169, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01934412121772766, "std_of_first_answer": 0.01934412121772766, "avg_entropy_of_first_answer": 0.5413985848426819, "lex_sim": 1.0}
{"question": "What is Harrison Ford's real name?", "answer": ["Henry Harrison"], "scores": [[0.14298999309539795, 0.055252887308597565]], "normalized_score": [0.0888853669166565], "tokens": [["\u0120Henry", "\u0120Harrison"]], "entropy": [[4.556820869445801, 4.650116920471191]], "avg_entropy": [4.603468894958496], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Harrison Ford (actor)", "score_of_biggest_cluster": 0.0888853669166565, "score_of_first_answer": 0.0888853669166565, "max_score_of_first_answer": 0.14298999309539795, "min_score_of_first_answer": 0.055252887308597565, "avg_score": 0.0888853669166565, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04386855289340019, "std_of_first_answer": 0.04386855289340019, "avg_entropy_of_first_answer": 4.603468894958496, "lex_sim": 1.0}
{"question": "Which American city is home to the Basketball side 'The Wizards'?", "answer": ["Baltimore Washington Capital"], "scores": [[0.002317956183105707, 0.0011730339610949159, 0.003019951982423663]], "normalized_score": [0.002017460996285081], "tokens": [["\u0120Baltimore", "\u0120Washington", "\u0120Capital"]], "entropy": [[2.356698989868164, 0.9078050851821899, 2.175830602645874]], "avg_entropy": [1.8134448925654094], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Washington", "score_of_biggest_cluster": 0.002017460996285081, "score_of_first_answer": 0.002017460996285081, "max_score_of_first_answer": 0.003019951982423663, "min_score_of_first_answer": 0.0011730339610949159, "avg_score": 0.002017460996285081, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0007611943228479161, "std_of_first_answer": 0.0007611943228479161, "avg_entropy_of_first_answer": 1.8134448925654094, "lex_sim": 1.0}
{"question": "How many grooves are there on a 45rpm record?", "answer": ["Two"], "scores": [[0.0513431541621685]], "normalized_score": [0.051343150436878204], "tokens": [["\u0120Two"]], "entropy": [[4.248388290405273]], "avg_entropy": [4.248388290405273], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2", "score_of_biggest_cluster": 0.051343150436878204, "score_of_first_answer": 0.051343150436878204, "max_score_of_first_answer": 0.0513431541621685, "min_score_of_first_answer": 0.0513431541621685, "avg_score": 0.051343150436878204, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.248388290405273, "lex_sim": 1.0}
{"question": "Brass is an alloy of what 2 metals?", "answer": ["Copper and Zinc"], "scores": [[0.7019465565681458, 0.8312393426895142, 0.009913461282849312, 0.9742833375930786]], "normalized_score": [0.27399033308029175], "tokens": [["\u0120Copper", "\u0120and", "\u0120Z", "inc"]], "entropy": [[1.7521581649780273, 0.8219589591026306, 0.19933374226093292, 0.16972213983535767]], "avg_entropy": [0.7357932515442371], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Copper and Zinc", "score_of_biggest_cluster": 0.27399033308029175, "score_of_first_answer": 0.27399033308029175, "max_score_of_first_answer": 0.9742833375930786, "min_score_of_first_answer": 0.009913461282849312, "avg_score": 0.27399033308029175, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.37037487749687553, "std_of_first_answer": 0.37037487749687553, "avg_entropy_of_first_answer": 0.7357932515442371, "lex_sim": 1.0}
{"question": "What name is given to a substance that accelerates a chemical reaction without itself being affected?", "answer": ["Reagent"], "scores": [[0.032449860125780106, 0.915113627910614]], "normalized_score": [0.1723232865333557], "tokens": [["\u0120Re", "agent"]], "entropy": [[4.1208319664001465, 0.46548745036125183]], "avg_entropy": [2.293159708380699], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Catalytes", "score_of_biggest_cluster": 0.1723232865333557, "score_of_first_answer": 0.1723232865333557, "max_score_of_first_answer": 0.915113627910614, "min_score_of_first_answer": 0.032449860125780106, "avg_score": 0.1723232865333557, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44133188389241695, "std_of_first_answer": 0.44133188389241695, "avg_entropy_of_first_answer": 2.293159708380699, "lex_sim": 1.0}
{"question": "Which author died in a car crash in 1960, three years after receiving the Nobel Prize for Literature?", "answer": ["Jules Verne"], "scores": [[0.022575879469513893, 0.1760353296995163, 0.4435679018497467, 0.995678722858429]], "normalized_score": [0.2046826332807541], "tokens": [["\u0120J", "ules", "\u0120Ver", "ne"]], "entropy": [[5.711481094360352, 2.5585477352142334, 2.258633613586426, 0.046830251812934875]], "avg_entropy": [2.6438731737434864], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Blood of the Hungarians", "score_of_biggest_cluster": 0.2046826332807541, "score_of_first_answer": 0.2046826332807541, "max_score_of_first_answer": 0.995678722858429, "min_score_of_first_answer": 0.022575879469513893, "avg_score": 0.2046826332807541, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37046664470043217, "std_of_first_answer": 0.37046664470043217, "avg_entropy_of_first_answer": 2.6438731737434864, "lex_sim": 1.0}
{"question": "Who wrote the poem \u2018The Lady of the Lake\u2019?", "answer": ["Lord of the Lake"], "scores": [[0.008203028701245785, 0.022056739777326584, 0.9693067073822021, 0.3331979811191559]], "normalized_score": [0.08743187040090561], "tokens": [["\u0120Lord", "\u0120of", "\u0120the", "\u0120Lake"]], "entropy": [[2.680415153503418, 2.7723004817962646, 0.3153756856918335, 3.181753158569336]], "avg_entropy": [2.237461119890213], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sir Walter Scotts's", "score_of_biggest_cluster": 0.08743187040090561, "score_of_first_answer": 0.08743187040090561, "max_score_of_first_answer": 0.9693067073822021, "min_score_of_first_answer": 0.008203028701245785, "avg_score": 0.08743187040090561, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3895718491498819, "std_of_first_answer": 0.3895718491498819, "avg_entropy_of_first_answer": 2.237461119890213, "lex_sim": 1.0}
{"question": "Named in honour of the Scottish physicist who presented the unified theory of electromagnetism, what is the CGS unit of magnetic flux?", "answer": ["Webers per square meter"], "scores": [[0.05645809695124626, 0.4708557426929474, 0.011143145151436329, 0.21352067589759827, 0.16709277033805847]], "normalized_score": [0.10111228376626968], "tokens": [["\u0120We", "bers", "\u0120per", "\u0120square", "\u0120meter"]], "entropy": [[5.888469696044922, 1.176092505455017, 1.062079906463623, 2.887007236480713, 2.022045373916626]], "avg_entropy": [2.6071389436721804], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Maxwell", "score_of_biggest_cluster": 0.10111228376626968, "score_of_first_answer": 0.10111228376626968, "max_score_of_first_answer": 0.4708557426929474, "min_score_of_first_answer": 0.011143145151436329, "avg_score": 0.10111228376626968, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1609905576845777, "std_of_first_answer": 0.1609905576845777, "avg_entropy_of_first_answer": 2.6071389436721804, "lex_sim": 1.0}
{"question": "From what illness did Mary II die in 1694?", "answer": ["Smallpox"], "scores": [[0.38745951652526855, 0.9450691938400269]], "normalized_score": [0.605124831199646], "tokens": [["\u0120Small", "pox"]], "entropy": [[3.847424030303955, 0.2931872308254242]], "avg_entropy": [2.0703056305646896], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Small pox", "score_of_biggest_cluster": 0.605124831199646, "score_of_first_answer": 0.605124831199646, "max_score_of_first_answer": 0.9450691938400269, "min_score_of_first_answer": 0.38745951652526855, "avg_score": 0.605124831199646, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.27880483865737915, "std_of_first_answer": 0.27880483865737915, "avg_entropy_of_first_answer": 2.0703056305646896, "lex_sim": 1.0}
{"question": "Who was the demon barber of Fleet Street?", "answer": ["The Devil."], "scores": [[0.03366818651556969, 0.15053093433380127, 0.011330864392220974]], "normalized_score": [0.03858064115047455], "tokens": [["\u0120The", "\u0120Devil", "."]], "entropy": [[5.659398555755615, 5.7132182121276855, 0.9017452001571655]], "avg_entropy": [4.091453989346822], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sweeny tod", "score_of_biggest_cluster": 0.03858064115047455, "score_of_first_answer": 0.03858064115047455, "max_score_of_first_answer": 0.15053093433380127, "min_score_of_first_answer": 0.011330864392220974, "avg_score": 0.03858064115047455, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06103962033795088, "std_of_first_answer": 0.06103962033795088, "avg_entropy_of_first_answer": 4.091453989346822, "lex_sim": 1.0}
{"question": "Which model of Ford is named after the Italian resort which was host to the 1956 Winter Olympics?", "answer": ["Mustang"], "scores": [[0.018229948356747627]], "normalized_score": [0.01822994463145733], "tokens": [["\u0120Mustang"]], "entropy": [[5.03795051574707]], "avg_entropy": [5.03795051574707], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cortina", "score_of_biggest_cluster": 0.01822994463145733, "score_of_first_answer": 0.01822994463145733, "max_score_of_first_answer": 0.018229948356747627, "min_score_of_first_answer": 0.018229948356747627, "avg_score": 0.01822994463145733, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.03795051574707, "lex_sim": 1.0}
{"question": "Betty, Earl and Joyce are the puppet pensioners in ads for which company?", "answer": ["Kwik Save"], "scores": [[0.0058952695690095425, 0.3418533504009247, 0.37607237696647644]], "normalized_score": [0.09117414057254791], "tokens": [["\u0120K", "wik", "\u0120Save"]], "entropy": [[7.419694900512695, 4.213835716247559, 2.145914077758789]], "avg_entropy": [4.593148231506348], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wonga", "score_of_biggest_cluster": 0.09117414057254791, "score_of_first_answer": 0.09117414057254791, "max_score_of_first_answer": 0.37607237696647644, "min_score_of_first_answer": 0.0058952695690095425, "avg_score": 0.09117414057254791, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.16702290750955667, "std_of_first_answer": 0.16702290750955667, "avg_entropy_of_first_answer": 4.593148231506348, "lex_sim": 1.0}
{"question": "In the late 60's, Owen Finlay Maclaren pioneered what aid to mothers and young children?", "answer": ["Pre-School education, \u3161\u3161"], "scores": [[0.00566933723166585, 0.6609544157981873, 0.016185490414500237, 0.07015059143304825, 0.005005005747079849, 0.009781017899513245, 2.8023048798786476e-05, 0.22619487345218658, 0.48009923100471497, 0.07164888083934784, 0.9528101086616516, 0.9241153597831726]], "normalized_score": [0.04300247132778168], "tokens": [["\u0120Pre", "-", "School", "\u0120education", ",", "\u0120", "\u00e3", "\u0127", "\u00a1", "\u00e3", "\u0127", "\u00a1"]], "entropy": [[6.199951648712158, 2.2016665935516357, 3.400264263153076, 3.654649019241333, 0.674083411693573, 6.2575788497924805, 5.09014892578125, 2.5741889476776123, 2.1555209159851074, 6.012725353240967, 0.22285006940364838, 0.4482012093067169]], "avg_entropy": [3.2409857672949633], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Baby Buggy", "score_of_biggest_cluster": 0.04300247132778168, "score_of_first_answer": 0.04300247132778168, "max_score_of_first_answer": 0.9528101086616516, "min_score_of_first_answer": 2.8023048798786476e-05, "avg_score": 0.04300247132778168, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3552721044983359, "std_of_first_answer": 0.3552721044983359, "avg_entropy_of_first_answer": 3.2409857672949633, "lex_sim": 1.0}
{"question": "\"What latin phrase means \"\"unaccompanied\"\" when applied to music?\"", "answer": ["\"\"Without a guide\"\" or  \"\"Alone\"\" when applied to music is the answer."], "scores": [[0.029068918898701668, 0.005143654067069292, 0.07767471671104431, 0.02279144525527954, 0.5925554633140564, 0.018967783078551292, 0.005241770762950182, 0.39532166719436646, 0.006528505124151707, 0.8316363096237183, 0.683601975440979, 0.0006929424125701189, 0.5136115550994873, 0.9806622266769409, 0.5969346761703491, 0.003615498309955001, 0.18889227509498596, 0.056581154465675354, 0.40376636385917664]], "normalized_score": [0.06844457238912582], "tokens": [["\u0120\"\"", "Without", "\u0120a", "\u0120guide", "\"\"", "\u0120or", "\u0120", "\u0120\"\"", "Al", "one", "\"\"", "\u0120when", "\u0120applied", "\u0120to", "\u0120music", "\u0120is", "\u0120the", "\u0120answer", "."]], "entropy": [[5.4247822761535645, 5.717068672180176, 5.11189603805542, 6.465801239013672, 1.9343340396881104, 0.563002347946167, 0.9265187978744507, 3.239534378051758, 4.5312323570251465, 1.112382411956787, 1.8852636814117432, 0.6576465368270874, 3.061947822570801, 0.17568841576576233, 2.0955162048339844, 1.226804494857788, 4.285330772399902, 5.387882232666016, 1.7412433624267578]], "avg_entropy": [2.92336189903711], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Acappella", "score_of_biggest_cluster": 0.06844457238912582, "score_of_first_answer": 0.06844457238912582, "max_score_of_first_answer": 0.9806622266769409, "min_score_of_first_answer": 0.0006929424125701189, "avg_score": 0.06844457238912582, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31752620400127374, "std_of_first_answer": 0.31752620400127374, "avg_entropy_of_first_answer": 2.92336189903711, "lex_sim": 1.0}
{"question": "Who was the founder of the 'Mormon Church'?", "answer": ["Joseph Smith"], "scores": [[0.8399500250816345, 0.9882931113243103]], "normalized_score": [0.9111074805259705], "tokens": [["\u0120Joseph", "\u0120Smith"]], "entropy": [[1.266526222229004, 0.1231306865811348]], "avg_entropy": [0.6948284544050694], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joseph Smith Junior", "score_of_biggest_cluster": 0.9111074805259705, "score_of_first_answer": 0.9111074805259705, "max_score_of_first_answer": 0.9882931113243103, "min_score_of_first_answer": 0.8399500250816345, "avg_score": 0.9111074805259705, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.07417154312133789, "std_of_first_answer": 0.07417154312133789, "avg_entropy_of_first_answer": 0.6948284544050694, "lex_sim": 1.0}
{"question": "Which is the oldest of the eight Ivy League Universities, founded in 1636 as New College?", "answer": ["Columbia"], "scores": [[0.09070176631212234]], "normalized_score": [0.09070176631212234], "tokens": [["\u0120Columbia"]], "entropy": [[2.3172683715820312]], "avg_entropy": [2.3172683715820312], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Harvard University Crimson", "score_of_biggest_cluster": 0.09070176631212234, "score_of_first_answer": 0.09070176631212234, "max_score_of_first_answer": 0.09070176631212234, "min_score_of_first_answer": 0.09070176631212234, "avg_score": 0.09070176631212234, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.3172683715820312, "lex_sim": 1.0}
{"question": "\"Literally, what old French word translates approximately as \"\"white food\"\"?\"", "answer": ["Creuset"], "scores": [[0.006328364834189415, 0.01799512840807438, 0.458139032125473]], "normalized_score": [0.03736639395356178], "tokens": [["\u0120Cre", "us", "et"]], "entropy": [[6.356385231018066, 3.479048728942871, 3.2565412521362305]], "avg_entropy": [4.363991737365723], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Blancmange", "score_of_biggest_cluster": 0.03736639395356178, "score_of_first_answer": 0.03736639395356178, "max_score_of_first_answer": 0.458139032125473, "min_score_of_first_answer": 0.006328364834189415, "avg_score": 0.03736639395356178, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2102896543210168, "std_of_first_answer": 0.2102896543210168, "avg_entropy_of_first_answer": 4.363991737365723, "lex_sim": 1.0}
{"question": "According to the film Gremlins, if you are looking after a Gremlin you should not feed him after midnight, not get him wet and not expose him to what?", "answer": ["Lightning"], "scores": [[0.005500732455402613]], "normalized_score": [0.005500731524080038], "tokens": [["\u0120Lightning"]], "entropy": [[5.2358551025390625]], "avg_entropy": [5.2358551025390625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bright Light (disambiguation)", "score_of_biggest_cluster": 0.005500731524080038, "score_of_first_answer": 0.005500731524080038, "max_score_of_first_answer": 0.005500732455402613, "min_score_of_first_answer": 0.005500732455402613, "avg_score": 0.005500731524080038, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.2358551025390625, "lex_sim": 1.0}
{"question": "Who duetted with Aretha Franklin on Sisters Are Doing It For Themselves", "answer": ["The two of them are not twin sisters"], "scores": [[0.03520652651786804, 0.004790388513356447, 0.34071487188339233, 0.7363653182983398, 0.04010641574859619, 0.007116050459444523, 0.0013270977651700377, 0.8187433481216431]], "normalized_score": [0.04362622648477554], "tokens": [["\u0120The", "\u0120two", "\u0120of", "\u0120them", "\u0120are", "\u0120not", "\u0120twin", "\u0120sisters"]], "entropy": [[6.216588020324707, 5.639482498168945, 4.541408538818359, 1.136878490447998, 2.697890281677246, 4.401752471923828, 2.937088966369629, 1.0695499181747437]], "avg_entropy": [3.580079898238182], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Annie lennox", "score_of_biggest_cluster": 0.04362622648477554, "score_of_first_answer": 0.04362622648477554, "max_score_of_first_answer": 0.8187433481216431, "min_score_of_first_answer": 0.0013270977651700377, "avg_score": 0.04362622648477554, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32391530559472176, "std_of_first_answer": 0.32391530559472176, "avg_entropy_of_first_answer": 3.580079898238182, "lex_sim": 1.0}
{"question": "Who was said to have presented the baby Jesus with Frankincense?", "answer": ["St. Joseph"], "scores": [[0.08662491291761398, 0.6266056895256042, 0.1300426423549652]], "normalized_score": [0.19182604551315308], "tokens": [["\u0120St", ".", "\u0120Joseph"]], "entropy": [[4.607786178588867, 2.284824848175049, 3.992250442504883]], "avg_entropy": [3.628287156422933], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Balthazar", "score_of_biggest_cluster": 0.19182604551315308, "score_of_first_answer": 0.19182604551315308, "max_score_of_first_answer": 0.6266056895256042, "min_score_of_first_answer": 0.08662491291761398, "avg_score": 0.19182604551315308, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24495786443818673, "std_of_first_answer": 0.24495786443818673, "avg_entropy_of_first_answer": 3.628287156422933, "lex_sim": 1.0}
{"question": "Born on this day in 1962 John Hannah played which detective on TV, the role being taken over after the first series by Ken Stott?", "answer": ["Inspector Reginald Wexford"], "scores": [[0.14348851144313812, 0.02960287407040596, 0.5566855072975159, 0.9977245926856995, 0.02028845250606537, 0.8959394693374634, 0.9658609628677368]], "normalized_score": [0.23653076589107513], "tokens": [["\u0120Inspector", "\u0120Reg", "inal", "d", "\u0120We", "x", "ford"]], "entropy": [[5.507160663604736, 4.807243347167969, 3.21087646484375, 0.029252789914608, 5.89021110534668, 0.6593811511993408, 0.24240198731422424]], "avg_entropy": [2.906646787055901], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Inspector Rebus novels", "score_of_biggest_cluster": 0.23653076589107513, "score_of_first_answer": 0.23653076589107513, "max_score_of_first_answer": 0.9977245926856995, "min_score_of_first_answer": 0.02028845250606537, "avg_score": 0.23653076589107513, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4143012813399642, "std_of_first_answer": 0.4143012813399642, "avg_entropy_of_first_answer": 2.906646787055901, "lex_sim": 1.0}
{"question": "What is the only country to share a land border with Denmark?", "answer": ["Liechtenstein"], "scores": [[0.024825463071465492, 0.9837618470191956, 0.9932177662849426, 0.9793540239334106]], "normalized_score": [0.39259329438209534], "tokens": [["\u0120Lie", "ch", "ten", "stein"]], "entropy": [[4.416975975036621, 0.15717707574367523, 0.07344014942646027, 0.17173007130622864]], "avg_entropy": [1.2048308178782463], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "United States of Germany", "score_of_biggest_cluster": 0.39259329438209534, "score_of_first_answer": 0.39259329438209534, "max_score_of_first_answer": 0.9932177662849426, "min_score_of_first_answer": 0.024825463071465492, "avg_score": 0.39259329438209534, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.415990419176672, "std_of_first_answer": 0.415990419176672, "avg_entropy_of_first_answer": 1.2048308178782463, "lex_sim": 1.0}
{"question": "In the Hindu religion what name is given to the triad of chief gods Brahma, Vishnu and Siva?", "answer": ["Trimurti"], "scores": [[0.3228673040866852, 0.8652095198631287, 0.9267151355743408, 0.9648998975753784]], "normalized_score": [0.7069577574729919], "tokens": [["\u0120Tr", "im", "urt", "i"]], "entropy": [[3.6210989952087402, 0.7802115082740784, 0.43399161100387573, 0.2525172531604767]], "avg_entropy": [1.2719548419117928], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hindu Trinity", "score_of_biggest_cluster": 0.7069577574729919, "score_of_first_answer": 0.7069577574729919, "max_score_of_first_answer": 0.9648998975753784, "min_score_of_first_answer": 0.3228673040866852, "avg_score": 0.7069577574729919, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2605465795602782, "std_of_first_answer": 0.2605465795602782, "avg_entropy_of_first_answer": 1.2719548419117928, "lex_sim": 1.0}
{"question": "Films. Which Dominic starred as Merry (Meriadoc) in \u2018The Lord of the Rings\u2019?", "answer": ["Meriadoc Merry"], "scores": [[0.002784847281873226, 0.8504114151000977, 0.9981278777122498, 0.9988192915916443, 0.017581257969141006]], "normalized_score": [0.13293233513832092], "tokens": [["\u0120Mer", "i", "ad", "oc", "\u0120Merry"]], "entropy": [[6.048465251922607, 1.0695569515228271, 0.021039530634880066, 0.015075217001140118, 2.865799903869629]], "avg_entropy": [2.0039873709902167], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Monaghan Urban Council", "score_of_biggest_cluster": 0.13293233513832092, "score_of_first_answer": 0.13293233513832092, "max_score_of_first_answer": 0.9988192915916443, "min_score_of_first_answer": 0.002784847281873226, "avg_score": 0.13293233513832092, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4631731186779573, "std_of_first_answer": 0.4631731186779573, "avg_entropy_of_first_answer": 2.0039873709902167, "lex_sim": 1.0}
{"question": "What is the distinguishing feature of an animal described as \u2018palmiped\u2019?", "answer": ["Palmiped is a scientific name for the order of Paedophryini of the family Pedetidae"], "scores": [[0.011472848244011402, 0.6638752818107605, 0.06683806329965591, 0.36946210265159607, 0.012111090123653412, 0.1418714076280594, 0.5130084753036499, 0.11708060652017593, 0.06371074169874191, 0.2557283639907837, 0.007649841718375683, 0.005689650774002075, 0.15464083850383759, 0.32575491070747375, 0.0032897256314754486, 0.028278741985559464, 0.3943653404712677, 0.2150460183620453, 0.03506714105606079, 0.22457878291606903, 0.9598016738891602]], "normalized_score": [0.08592478185892105], "tokens": [["\u0120Palm", "iped", "\u0120is", "\u0120a", "\u0120scientific", "\u0120name", "\u0120for", "\u0120the", "\u0120order", "\u0120of", "\u0120P", "aed", "oph", "ry", "ini", "\u0120of", "\u0120the", "\u0120family", "\u0120Ped", "et", "idae"]], "entropy": [[5.635109901428223, 2.0527167320251465, 4.601513862609863, 3.524110794067383, 6.196258544921875, 2.5301899909973145, 2.296877384185791, 3.1310017108917236, 6.737561225891113, 4.473336219787598, 5.124585151672363, 3.2862582206726074, 4.370324611663818, 2.7279129028320312, 3.2014076709747314, 2.6559977531433105, 3.3341469764709473, 4.489493370056152, 4.047911167144775, 3.3862175941467285, 0.3220108449459076]], "avg_entropy": [3.720235363358543], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Totipalmation", "score_of_biggest_cluster": 0.08592478185892105, "score_of_first_answer": 0.08592478185892105, "max_score_of_first_answer": 0.9598016738891602, "min_score_of_first_answer": 0.0032897256314754486, "avg_score": 0.08592478185892105, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24493035052810352, "std_of_first_answer": 0.24493035052810352, "avg_entropy_of_first_answer": 3.720235363358543, "lex_sim": 1.0}
{"question": "What are the shape-shifting foxes of the Japanese folklore?", "answer": ["Kitayama Shimenjyu"], "scores": [[0.005905479192733765, 0.017330655828118324, 0.6017811894416809, 0.0026383858639746904, 0.008715205825865269, 0.013698315247893333, 0.4787294268608093]], "normalized_score": [0.026545017957687378], "tokens": [["\u0120Kit", "ay", "ama", "\u0120Sh", "imen", "j", "yu"]], "entropy": [[4.492705345153809, 4.467518329620361, 2.4911530017852783, 3.593418836593628, 4.037674903869629, 2.628174304962158, 2.904459238052368]], "avg_entropy": [3.5164434228624617], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kintsu", "score_of_biggest_cluster": 0.026545017957687378, "score_of_first_answer": 0.026545017957687378, "max_score_of_first_answer": 0.6017811894416809, "min_score_of_first_answer": 0.0026383858639746904, "avg_score": 0.026545017957687378, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24198631012931515, "std_of_first_answer": 0.24198631012931515, "avg_entropy_of_first_answer": 3.5164434228624617, "lex_sim": 1.0}
{"question": "\"What movie series features \"\"minions\"\" that resemble Twinkies?\"", "answer": ["Minions"], "scores": [[0.013284304179251194, 0.996325671672821]], "normalized_score": [0.11504559963941574], "tokens": [["\u0120Min", "ions"]], "entropy": [[3.4667885303497314, 0.02805180661380291]], "avg_entropy": [1.7474201684817672], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Despicable Me (single)", "score_of_biggest_cluster": 0.11504559963941574, "score_of_first_answer": 0.11504559963941574, "max_score_of_first_answer": 0.996325671672821, "min_score_of_first_answer": 0.013284304179251194, "avg_score": 0.11504559963941574, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4915206837467849, "std_of_first_answer": 0.4915206837467849, "avg_entropy_of_first_answer": 1.7474201684817672, "lex_sim": 1.0}
{"question": "\"The cruise liner \"\"Costa Concordia\"\" sank in the Tyrhenian sea in January 2012 against the rocks of the coast of what island?\"", "answer": ["Costa Concordia"], "scores": [[0.0001349054218735546, 0.06671988219022751, 0.9975753426551819]], "normalized_score": [0.020784683525562286], "tokens": [["\u0120Costa", "\u0120Concord", "ia"]], "entropy": [[3.400465488433838, 4.749581336975098, 0.02875647321343422]], "avg_entropy": [2.7262677662074566], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Giglio (disambiguation)", "score_of_biggest_cluster": 0.020784683525562286, "score_of_first_answer": 0.020784683525562286, "max_score_of_first_answer": 0.9975753426551819, "min_score_of_first_answer": 0.0001349054218735546, "avg_score": 0.020784683525562286, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4553158699965989, "std_of_first_answer": 0.4553158699965989, "avg_entropy_of_first_answer": 2.7262677662074566, "lex_sim": 1.0}
{"question": "What pope resigned in 2013?", "answer": ["Benedict XVI"], "scores": [[0.3805646002292633, 0.6891771554946899]], "normalized_score": [0.5121293067932129], "tokens": [["\u0120Benedict", "\u0120XVI"]], "entropy": [[2.923511028289795, 1.647298812866211]], "avg_entropy": [2.285404920578003], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pope Benedict XVI", "score_of_biggest_cluster": 0.5121293067932129, "score_of_first_answer": 0.5121293067932129, "max_score_of_first_answer": 0.6891771554946899, "min_score_of_first_answer": 0.3805646002292633, "avg_score": 0.5121293067932129, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.15430627763271332, "std_of_first_answer": 0.15430627763271332, "avg_entropy_of_first_answer": 2.285404920578003, "lex_sim": 1.0}
{"question": "Give any year in the life of the Portuguese prince known as Henry the Navigator.", "answer": ["1419"], "scores": [[0.5176856517791748, 0.27725034952163696]], "normalized_score": [0.37885159254074097], "tokens": [["\u012014", "19"]], "entropy": [[3.0281107425689697, 3.32608699798584]], "avg_entropy": [3.177098870277405], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1394-1460", "score_of_biggest_cluster": 0.37885159254074097, "score_of_first_answer": 0.37885159254074097, "max_score_of_first_answer": 0.5176856517791748, "min_score_of_first_answer": 0.27725034952163696, "avg_score": 0.37885159254074097, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12021765112876892, "std_of_first_answer": 0.12021765112876892, "avg_entropy_of_first_answer": 3.177098870277405, "lex_sim": 1.0}
{"question": "Sue Nicholls who plays Gail?s mother in Coronation Street, played Joan Greengross the secretary of which character in a 1970s sitcom?", "answer": ["Joan  Q:"], "scores": [[0.03434774652123451, 0.006739535368978977, 0.00011734479630831629, 0.005118072032928467]], "normalized_score": [0.0034337975084781647], "tokens": [["\u0120Joan", "\u0120", "\u0120Q", ":"]], "entropy": [[6.137796401977539, 5.828950881958008, 4.870480537414551, 4.087114334106445]], "avg_entropy": [5.231085538864136], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Companies featured in The Fall and Rise of Reginald Perrin", "score_of_biggest_cluster": 0.0034337975084781647, "score_of_first_answer": 0.0034337975084781647, "max_score_of_first_answer": 0.03434774652123451, "min_score_of_first_answer": 0.00011734479630831629, "avg_score": 0.0034337975084781647, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.013369268445290128, "std_of_first_answer": 0.013369268445290128, "avg_entropy_of_first_answer": 5.231085538864136, "lex_sim": 1.0}
{"question": "What is the common name for the species of large cattle, Bos primigenius, once prevalent in Europe, that was hunted to extinction in 1627?", "answer": ["Bull"], "scores": [[0.02474343031644821]], "normalized_score": [0.024743426591157913], "tokens": [["\u0120Bull"]], "entropy": [[5.2800374031066895]], "avg_entropy": [5.2800374031066895], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Indian Auroch", "score_of_biggest_cluster": 0.024743426591157913, "score_of_first_answer": 0.024743426591157913, "max_score_of_first_answer": 0.02474343031644821, "min_score_of_first_answer": 0.02474343031644821, "avg_score": 0.024743426591157913, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.2800374031066895, "lex_sim": 1.0}
{"question": "In which North Yorkshire town is the Green Howards Museum?", "answer": ["Harrogate"], "scores": [[0.03330925107002258, 0.9917978048324585, 0.9999865293502808]], "normalized_score": [0.32086873054504395], "tokens": [["\u0120Har", "rog", "ate"]], "entropy": [[4.341793537139893, 0.06818850338459015, 0.00020073039922863245]], "avg_entropy": [1.4700609236412372], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "RICHMOND", "score_of_biggest_cluster": 0.32086873054504395, "score_of_first_answer": 0.32086873054504395, "max_score_of_first_answer": 0.9999865293502808, "min_score_of_first_answer": 0.03330925107002258, "avg_score": 0.32086873054504395, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.45377825265531063, "std_of_first_answer": 0.45377825265531063, "avg_entropy_of_first_answer": 1.4700609236412372, "lex_sim": 1.0}
{"question": "Which Asian city's international airport is built at the mouth of the Tama-gawa?", "answer": ["Haneda Airport"], "scores": [[0.15490753948688507, 0.991666853427887, 0.16403008997440338]], "normalized_score": [0.2931707501411438], "tokens": [["\u0120Han", "eda", "\u0120Airport"]], "entropy": [[2.6151840686798096, 0.0628499910235405, 1.2705761194229126]], "avg_entropy": [1.3162033930420876], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u6771\u4eac\u90fd", "score_of_biggest_cluster": 0.2931707501411438, "score_of_first_answer": 0.2931707501411438, "max_score_of_first_answer": 0.991666853427887, "min_score_of_first_answer": 0.15490753948688507, "avg_score": 0.2931707501411438, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3923195951452182, "std_of_first_answer": 0.3923195951452182, "avg_entropy_of_first_answer": 1.3162033930420876, "lex_sim": 1.0}
{"question": "Which statesman was known as the Iron Chancellor?", "answer": ["Bismarck"], "scores": [[0.01653711125254631, 0.9617013335227966, 0.945924699306488, 0.9993332028388977]], "normalized_score": [0.3501600921154022], "tokens": [["\u0120B", "ism", "ar", "ck"]], "entropy": [[3.997953176498413, 0.3288363814353943, 0.27145636081695557, 0.007412062492221594]], "avg_entropy": [1.1514144953107461], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Count von Bismarck-Schoenausen", "score_of_biggest_cluster": 0.3501600921154022, "score_of_first_answer": 0.3501600921154022, "max_score_of_first_answer": 0.9993332028388977, "min_score_of_first_answer": 0.01653711125254631, "avg_score": 0.3501600921154022, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4128787943778756, "std_of_first_answer": 0.4128787943778756, "avg_entropy_of_first_answer": 1.1514144953107461, "lex_sim": 1.0}
{"question": "What is the square root of 0.25?", "answer": ["0!"], "scores": [[0.3714989125728607, 0.00017219209985341877]], "normalized_score": [0.007998073473572731], "tokens": [["\u01200", "!"]], "entropy": [[3.7479171752929688, 0.6039336323738098]], "avg_entropy": [2.1759254038333893], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "0.5", "score_of_biggest_cluster": 0.007998073473572731, "score_of_first_answer": 0.007998073473572731, "max_score_of_first_answer": 0.3714989125728607, "min_score_of_first_answer": 0.00017219209985341877, "avg_score": 0.007998073473572731, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18566336023650365, "std_of_first_answer": 0.18566336023650365, "avg_entropy_of_first_answer": 2.1759254038333893, "lex_sim": 1.0}
{"question": "238 AD is known in Roman history as the Year of the ........ (how many) Emperors?", "answer": ["Four"], "scores": [[0.061158858239650726]], "normalized_score": [0.061158861964941025], "tokens": [["\u0120Four"]], "entropy": [[4.8987202644348145]], "avg_entropy": [4.8987202644348145], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "SIX", "score_of_biggest_cluster": 0.061158861964941025, "score_of_first_answer": 0.061158861964941025, "max_score_of_first_answer": 0.061158858239650726, "min_score_of_first_answer": 0.061158858239650726, "avg_score": 0.061158861964941025, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.8987202644348145, "lex_sim": 1.0}
{"question": "Who was the first Pope to be appointed in the 21st century?", "answer": ["John Paul II"], "scores": [[0.16618387401103973, 0.6695928573608398, 0.817350447177887]], "normalized_score": [0.4497135579586029], "tokens": [["\u0120John", "\u0120Paul", "\u0120II"]], "entropy": [[4.103864669799805, 1.9602988958358765, 1.1026721000671387]], "avg_entropy": [2.38894522190094], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pope Benedict XVI", "score_of_biggest_cluster": 0.4497135579586029, "score_of_first_answer": 0.4497135579586029, "max_score_of_first_answer": 0.817350447177887, "min_score_of_first_answer": 0.16618387401103973, "avg_score": 0.4497135579586029, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27874138070892823, "std_of_first_answer": 0.27874138070892823, "avg_entropy_of_first_answer": 2.38894522190094, "lex_sim": 1.0}
{"question": "Levi Stubbs was the lead singer of which 'Motown' group from 1953 until 2000?", "answer": ["The Supremes"], "scores": [[0.9012174010276794, 0.17953099310398102, 0.9997885823249817, 0.9999468326568604]], "normalized_score": [0.6341814398765564], "tokens": [["\u0120The", "\u0120Sup", "rem", "es"]], "entropy": [[0.722588837146759, 3.1884865760803223, 0.002625728026032448, 0.0007698985282331705]], "avg_entropy": [0.9786177599453367], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Roquel Payton", "score_of_biggest_cluster": 0.6341814398765564, "score_of_first_answer": 0.6341814398765564, "max_score_of_first_answer": 0.9999468326568604, "min_score_of_first_answer": 0.17953099310398102, "avg_score": 0.6341814398765564, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3433474677113777, "std_of_first_answer": 0.3433474677113777, "avg_entropy_of_first_answer": 0.9786177599453367, "lex_sim": 1.0}
{"question": "Who played Richard DeVere in the TV sitcom To the Manor Born?", "answer": ["John Rhys-Davies"], "scores": [[0.0619971938431263, 0.009267264977097511, 0.993845522403717, 0.9069265723228455, 0.9917586445808411, 0.99839848279953, 0.9857231974601746]], "normalized_score": [0.3381403982639313], "tokens": [["\u0120John", "\u0120Rh", "ys", "-", "D", "av", "ies"]], "entropy": [[5.475020408630371, 5.502379417419434, 0.054026275873184204, 0.5520436763763428, 0.067901611328125, 0.01724851131439209, 0.0859721451997757]], "avg_entropy": [1.6792274351630891], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Peter Bowles", "score_of_biggest_cluster": 0.3381403982639313, "score_of_first_answer": 0.3381403982639313, "max_score_of_first_answer": 0.99839848279953, "min_score_of_first_answer": 0.009267264977097511, "avg_score": 0.3381403982639313, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4257426439541029, "std_of_first_answer": 0.4257426439541029, "avg_entropy_of_first_answer": 1.6792274351630891, "lex_sim": 1.0}
{"question": "Whose statue, situated in Grafton Street in Dublin is known as 'the tart with the cart'?", "answer": ["Pallas Athena"], "scores": [[0.03146494925022125, 0.003978987690061331, 0.8410743474960327]], "normalized_score": [0.04722202569246292], "tokens": [["\u0120P", "allas", "\u0120Athena"]], "entropy": [[6.199965476989746, 2.368875026702881, 1.033382773399353]], "avg_entropy": [3.200741092363993], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cockles and Mussles", "score_of_biggest_cluster": 0.04722202569246292, "score_of_first_answer": 0.04722202569246292, "max_score_of_first_answer": 0.8410743474960327, "min_score_of_first_answer": 0.003978987690061331, "avg_score": 0.04722202569246292, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38829420363938827, "std_of_first_answer": 0.38829420363938827, "avg_entropy_of_first_answer": 3.200741092363993, "lex_sim": 1.0}
{"question": "The London production of which play by acclaimed Australian playwright, David Williamson, won him the George Devine Award, the first time for a non-UK writer?", "answer": ["Hire me, I'm cheap!"], "scores": [[0.0032082521356642246, 0.006202036514878273, 0.044646795839071274, 0.11151311546564102, 0.3622554838657379, 0.46205446124076843, 0.1621863692998886, 0.26363617181777954]], "normalized_score": [0.07183412462472916], "tokens": [["\u0120H", "ire", "\u0120me", ",", "\u0120I", "'m", "\u0120cheap", "!"]], "entropy": [[6.312830448150635, 4.5606231689453125, 4.236236572265625, 3.023536205291748, 4.544950485229492, 2.4422054290771484, 4.463688850402832, 1.5628232955932617]], "avg_entropy": [3.893361806869507], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Removalists", "score_of_biggest_cluster": 0.07183412462472916, "score_of_first_answer": 0.07183412462472916, "max_score_of_first_answer": 0.46205446124076843, "min_score_of_first_answer": 0.0032082521356642246, "avg_score": 0.07183412462472916, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.15987585719160383, "std_of_first_answer": 0.15987585719160383, "avg_entropy_of_first_answer": 3.893361806869507, "lex_sim": 1.0}
{"question": "What do we call the vegetable called the rutabaga in America?", "answer": ["Brinjal"], "scores": [[0.003245942760258913, 0.034750618040561676, 0.3208393156528473, 0.9607076644897461]], "normalized_score": [0.07678838819265366], "tokens": [["\u0120Br", "in", "j", "al"]], "entropy": [[4.584716796875, 3.261806011199951, 3.8985326290130615, 0.31427207589149475]], "avg_entropy": [3.014831878244877], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Svenskar", "score_of_biggest_cluster": 0.07678838819265366, "score_of_first_answer": 0.07678838819265366, "max_score_of_first_answer": 0.9607076644897461, "min_score_of_first_answer": 0.003245942760258913, "avg_score": 0.07678838819265366, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3846480324807222, "std_of_first_answer": 0.3846480324807222, "avg_entropy_of_first_answer": 3.014831878244877, "lex_sim": 1.0}
{"question": "What is an alternative name for Whit Sunday the time when, according to Christian tradition, the Holy Spirit descended on the Apostles?", "answer": ["Lord's Day"], "scores": [[0.002931802300736308, 0.9035079479217529, 0.7130545973777771]], "normalized_score": [0.12361270934343338], "tokens": [["\u0120Lord", "'s", "\u0120Day"]], "entropy": [[3.8333802223205566, 0.7112365961074829, 0.9431543350219727]], "avg_entropy": [1.8292570511500041], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pentecost Sunday", "score_of_biggest_cluster": 0.12361270934343338, "score_of_first_answer": 0.12361270934343338, "max_score_of_first_answer": 0.9035079479217529, "min_score_of_first_answer": 0.002931802300736308, "avg_score": 0.12361270934343338, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3875255198597409, "std_of_first_answer": 0.3875255198597409, "avg_entropy_of_first_answer": 1.8292570511500041, "lex_sim": 1.0}
{"question": "How was Moose the dog better known when he starred in a popular US sitcom?", "answer": ["Moose"], "scores": [[0.017733046784996986]], "normalized_score": [0.017733048647642136], "tokens": [["\u0120Moose"]], "entropy": [[5.964526176452637]], "avg_entropy": [5.964526176452637], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Quiksilver Big Wave Invitational in Memory of Eddie Aikau", "score_of_biggest_cluster": 0.017733048647642136, "score_of_first_answer": 0.017733048647642136, "max_score_of_first_answer": 0.017733046784996986, "min_score_of_first_answer": 0.017733046784996986, "avg_score": 0.017733048647642136, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.964526176452637, "lex_sim": 1.0}
{"question": "Canberra is the capital of which country?", "answer": ["Australia"], "scores": [[0.9117313623428345]], "normalized_score": [0.9117313623428345], "tokens": [["\u0120Australia"]], "entropy": [[0.7174012064933777]], "avg_entropy": [0.7174012064933777], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Australia (Commonwealth realm)", "score_of_biggest_cluster": 0.9117313623428345, "score_of_first_answer": 0.9117313623428345, "max_score_of_first_answer": 0.9117313623428345, "min_score_of_first_answer": 0.9117313623428345, "avg_score": 0.9117313623428345, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.7174012064933777, "lex_sim": 1.0}
{"question": "Gamophobia is the fear of what?", "answer": ["Not being able to achieve anything in life"], "scores": [[0.0019407363142818213, 0.1380876749753952, 0.5143227577209473, 0.9881515502929688, 0.004510612227022648, 0.030135946348309517, 0.04661209136247635, 0.882871687412262]], "normalized_score": [0.07248317450284958], "tokens": [["\u0120Not", "\u0120being", "\u0120able", "\u0120to", "\u0120achieve", "\u0120anything", "\u0120in", "\u0120life"]], "entropy": [[6.548205852508545, 3.74849534034729, 3.455631732940674, 0.12133291363716125, 4.8312883377075195, 3.7704195976257324, 1.6281383037567139, 0.7876152992248535]], "avg_entropy": [3.111390922218561], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marraige", "score_of_biggest_cluster": 0.07248317450284958, "score_of_first_answer": 0.07248317450284958, "max_score_of_first_answer": 0.9881515502929688, "min_score_of_first_answer": 0.0019407363142818213, "avg_score": 0.07248317450284958, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38621399440093546, "std_of_first_answer": 0.38621399440093546, "avg_entropy_of_first_answer": 3.111390922218561, "lex_sim": 1.0}
{"question": "Which actor played the father of the Waltons children?", "answer": ["The voice of John Wayne"], "scores": [[0.002498229732736945, 0.00215572165325284, 0.5453507304191589, 0.03974989801645279, 0.11648934334516525]], "normalized_score": [0.02671189419925213], "tokens": [["\u0120The", "\u0120voice", "\u0120of", "\u0120John", "\u0120Wayne"]], "entropy": [[5.611176013946533, 5.688654899597168, 2.420452117919922, 5.2672600746154785, 4.895571708679199]], "avg_entropy": [4.77662296295166], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ralph Waite", "score_of_biggest_cluster": 0.02671189419925213, "score_of_first_answer": 0.02671189419925213, "max_score_of_first_answer": 0.5453507304191589, "min_score_of_first_answer": 0.00215572165325284, "avg_score": 0.02671189419925213, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20630658407315217, "std_of_first_answer": 0.20630658407315217, "avg_entropy_of_first_answer": 4.77662296295166, "lex_sim": 1.0}
{"question": "What F1 event is now for vintage and classic cars, run from Brescia to Rome?", "answer": ["Vallelunga"], "scores": [[0.005190479103475809, 0.8741759061813354, 0.8995233774185181, 0.9762958288192749, 0.9937957525253296]], "normalized_score": [0.33078014850616455], "tokens": [["\u0120Val", "le", "l", "ung", "a"]], "entropy": [[5.340719223022461, 0.9344809055328369, 0.788462221622467, 0.21021881699562073, 0.04439862444996834]], "avg_entropy": [1.4636559583246709], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mille Miglia", "score_of_biggest_cluster": 0.33078014850616455, "score_of_first_answer": 0.33078014850616455, "max_score_of_first_answer": 0.9937957525253296, "min_score_of_first_answer": 0.005190479103475809, "avg_score": 0.33078014850616455, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3750104217344129, "std_of_first_answer": 0.3750104217344129, "avg_entropy_of_first_answer": 1.4636559583246709, "lex_sim": 1.0}
{"question": "Where was the Titanic heading for on her maiden voyage", "answer": ["New York"], "scores": [[0.32706624269485474, 0.9721481204032898]], "normalized_score": [0.5638766288757324], "tokens": [["\u0120New", "\u0120York"]], "entropy": [[3.7901694774627686, 0.22018849849700928]], "avg_entropy": [2.005178987979889], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "New Yourk State", "score_of_biggest_cluster": 0.5638766288757324, "score_of_first_answer": 0.5638766288757324, "max_score_of_first_answer": 0.9721481204032898, "min_score_of_first_answer": 0.32706624269485474, "avg_score": 0.5638766288757324, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32254093885421753, "std_of_first_answer": 0.32254093885421753, "avg_entropy_of_first_answer": 2.005178987979889, "lex_sim": 1.0}
{"question": "In politics and economics, \u2018what\u2019 Wednesday refers to 16th September 1992, when the British government was forced to withdraw the pound sterling from the European Rate Mechanism, after they were unable to keep it above its agreed lower limit?", "answer": ["\u32cd"], "scores": [[0.005738829728215933, 2.4137609216268174e-05, 0.03930767998099327, 0.01952863484621048]], "normalized_score": [0.0032111946493387222], "tokens": [["\u0120", "\u00e3", "\u012d", "\u012f"]], "entropy": [[4.0224456787109375, 5.072885513305664, 2.64383864402771, 3.793488025665283]], "avg_entropy": [3.8831644654273987], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "List of Black Wednesdays", "score_of_biggest_cluster": 0.0032111946493387222, "score_of_first_answer": 0.0032111946493387222, "max_score_of_first_answer": 0.03930767998099327, "min_score_of_first_answer": 2.4137609216268174e-05, "avg_score": 0.0032111946493387222, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01513381329731436, "std_of_first_answer": 0.01513381329731436, "avg_entropy_of_first_answer": 3.8831644654273987, "lex_sim": 0.0}
{"question": "Which title, referring to an episode in the legendary history of early Rome, is shared by paintings by Poussin, Rubens and Picasso?", "answer": ["Roman History (TV series)"], "scores": [[0.005335038993507624, 0.1363711655139923, 0.01188516616821289, 3.299774834886193e-05, 0.4690387547016144, 0.8160361051559448]], "normalized_score": [0.021863076835870743], "tokens": [["\u0120Roman", "\u0120History", "\u0120(", "TV", "\u0120series", ")"]], "entropy": [[5.0150604248046875, 5.749509811401367, 1.398655652999878, 5.913805961608887, 2.605344772338867, 1.2552837133407593]], "avg_entropy": [3.6562767227490744], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rape of the Sabine Women (Giambologna)", "score_of_biggest_cluster": 0.021863076835870743, "score_of_first_answer": 0.021863076835870743, "max_score_of_first_answer": 0.8160361051559448, "min_score_of_first_answer": 3.299774834886193e-05, "avg_score": 0.021863076835870743, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30542407197266297, "std_of_first_answer": 0.30542407197266297, "avg_entropy_of_first_answer": 3.6562767227490744, "lex_sim": 1.0}
{"question": "What is the medical description of the loss of hair from the head or body, sometimes to the extent of baldness?", "answer": ["Alopecia"], "scores": [[0.7289233803749084, 0.997991681098938, 0.9910586476325989, 0.9983204007148743]], "normalized_score": [0.9210740327835083], "tokens": [["\u0120Al", "op", "ec", "ia"]], "entropy": [[1.6645599603652954, 0.021117936819791794, 0.09462475776672363, 0.01759495958685875]], "avg_entropy": [0.4494744036346674], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "NEC cicatricial alopecia", "score_of_biggest_cluster": 0.9210740327835083, "score_of_first_answer": 0.9210740327835083, "max_score_of_first_answer": 0.9983204007148743, "min_score_of_first_answer": 0.7289233803749084, "avg_score": 0.9210740327835083, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11559312031601642, "std_of_first_answer": 0.11559312031601642, "avg_entropy_of_first_answer": 0.4494744036346674, "lex_sim": 1.0}
{"question": "Who, in 1434, painted 'The Marriage of Giovanni Arnolfini' which now hangs in the National Gallery, London?", "answer": ["Van Eyck"], "scores": [[0.03325411304831505, 0.6583618521690369, 0.9853731989860535]], "normalized_score": [0.27837926149368286], "tokens": [["\u0120Van", "\u0120Ey", "ck"]], "entropy": [[4.045935153961182, 1.4469797611236572, 0.10726608335971832]], "avg_entropy": [1.866726999481519], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "J. Van Eyck", "score_of_biggest_cluster": 0.27837926149368286, "score_of_first_answer": 0.27837926149368286, "max_score_of_first_answer": 0.9853731989860535, "min_score_of_first_answer": 0.03325411304831505, "avg_score": 0.27837926149368286, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3950002616402247, "std_of_first_answer": 0.3950002616402247, "avg_entropy_of_first_answer": 1.866726999481519, "lex_sim": 1.0}
{"question": "The Council of Trent in the 16th century was held between believers of what religious faith?", "answer": ["Catholic"], "scores": [[0.13151678442955017]], "normalized_score": [0.13151679933071136], "tokens": [["\u0120Catholic"]], "entropy": [[1.7841846942901611]], "avg_entropy": [1.7841846942901611], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nazarethism", "score_of_biggest_cluster": 0.13151679933071136, "score_of_first_answer": 0.13151679933071136, "max_score_of_first_answer": 0.13151678442955017, "min_score_of_first_answer": 0.13151678442955017, "avg_score": 0.13151679933071136, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.7841846942901611, "lex_sim": 1.0}
{"question": "Tahini paste is made by grinding which seeds?", "answer": ["Chickpea"], "scores": [[0.026994910091161728, 0.846619725227356, 0.13685959577560425]], "normalized_score": [0.14624527096748352], "tokens": [["\u0120Chick", "pe", "a"]], "entropy": [[3.863189697265625, 0.7981338500976562, 0.41585785150527954]], "avg_entropy": [1.6923937996228535], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Till (seed)", "score_of_biggest_cluster": 0.14624527096748352, "score_of_first_answer": 0.14624527096748352, "max_score_of_first_answer": 0.846619725227356, "min_score_of_first_answer": 0.026994910091161728, "avg_score": 0.14624527096748352, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36325909457253924, "std_of_first_answer": 0.36325909457253924, "avg_entropy_of_first_answer": 1.6923937996228535, "lex_sim": 1.0}
{"question": "What creature was used as the US Dodge automotive logo in the 1930s-50s, and again 1970s-2000s?", "answer": ["Dodge Charger"], "scores": [[0.04370585083961487, 0.024398958310484886, 0.9889928698539734]], "normalized_score": [0.10178910195827484], "tokens": [["\u0120Dodge", "\u0120Charg", "er"]], "entropy": [[5.511444091796875, 4.959938049316406, 0.07363354414701462]], "avg_entropy": [3.5150052284200988], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ram (disambiguation)", "score_of_biggest_cluster": 0.10178910195827484, "score_of_first_answer": 0.10178910195827484, "max_score_of_first_answer": 0.9889928698539734, "min_score_of_first_answer": 0.024398958310484886, "avg_score": 0.10178910195827484, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45023225100471453, "std_of_first_answer": 0.45023225100471453, "avg_entropy_of_first_answer": 3.5150052284200988, "lex_sim": 1.0}
{"question": "What is the longest time that there can be between General Elections in the UK?", "answer": ["7 years 8 months"], "scores": [[0.015279357321560383, 0.7769972681999207, 0.002280917949974537, 0.9606887102127075]], "normalized_score": [0.0714174285531044], "tokens": [["\u01207", "\u0120years", "\u01208", "\u0120months"]], "entropy": [[5.126547813415527, 1.516462802886963, 1.5308479070663452, 0.30586010217666626]], "avg_entropy": [2.1199296563863754], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "5 years", "score_of_biggest_cluster": 0.0714174285531044, "score_of_first_answer": 0.0714174285531044, "max_score_of_first_answer": 0.9606887102127075, "min_score_of_first_answer": 0.002280917949974537, "avg_score": 0.0714174285531044, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4349321387581852, "std_of_first_answer": 0.4349321387581852, "avg_entropy_of_first_answer": 2.1199296563863754, "lex_sim": 1.0}
{"question": "What homonymic word refers both to a pit and a hunted animal?", "answer": ["Moth"], "scores": [[0.0031034452840685844, 0.00953623466193676]], "normalized_score": [0.005440147127956152], "tokens": [["\u0120M", "oth"]], "entropy": [[7.187878608703613, 5.234463691711426]], "avg_entropy": [6.2111711502075195], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marble quarry", "score_of_biggest_cluster": 0.005440147127956152, "score_of_first_answer": 0.005440147127956152, "max_score_of_first_answer": 0.00953623466193676, "min_score_of_first_answer": 0.0031034452840685844, "avg_score": 0.005440147127956152, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0032163946889340878, "std_of_first_answer": 0.0032163946889340878, "avg_entropy_of_first_answer": 6.2111711502075195, "lex_sim": 1.0}
{"question": "Chim Chim Cheree, I Love to Laugh and Feed the Birds are all songs from which film?", "answer": ["The Awful Truth"], "scores": [[0.19766075909137726, 0.0013376231072470546, 0.846376895904541, 0.956575334072113]], "normalized_score": [0.12095791846513748], "tokens": [["\u0120The", "\u0120Aw", "ful", "\u0120Truth"]], "entropy": [[5.231433868408203, 4.188475131988525, 0.7416254281997681, 0.2618424594402313]], "avg_entropy": [2.605844222009182], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mrs. George Banks", "score_of_biggest_cluster": 0.12095791846513748, "score_of_first_answer": 0.12095791846513748, "max_score_of_first_answer": 0.956575334072113, "min_score_of_first_answer": 0.0013376231072470546, "avg_score": 0.12095791846513748, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40881237175478635, "std_of_first_answer": 0.40881237175478635, "avg_entropy_of_first_answer": 2.605844222009182, "lex_sim": 1.0}
{"question": "Which children's fictional character lives at 32, Windsor Gardens, London?", "answer": ["Mr Rabbit"], "scores": [[0.03465207293629646, 0.0028299898840487003]], "normalized_score": [0.009902780875563622], "tokens": [["\u0120Mr", "\u0120Rabbit"]], "entropy": [[5.791753768920898, 3.059399366378784]], "avg_entropy": [4.425576567649841], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Darkest Peru", "score_of_biggest_cluster": 0.009902780875563622, "score_of_first_answer": 0.009902780875563622, "max_score_of_first_answer": 0.03465207293629646, "min_score_of_first_answer": 0.0028299898840487003, "avg_score": 0.009902780875563622, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01591104152612388, "std_of_first_answer": 0.01591104152612388, "avg_entropy_of_first_answer": 4.425576567649841, "lex_sim": 1.0}
{"question": "Which journalist first told the world about the My Lai massacre?", "answer": ["David Halberstam"], "scores": [[0.013232478871941566, 0.8384683132171631, 0.9981266856193542, 0.9827651381492615, 0.9838770031929016]], "normalized_score": [0.4035904109477997], "tokens": [["\u0120David", "\u0120Hal", "ber", "st", "am"]], "entropy": [[3.2140932083129883, 1.2928411960601807, 0.019558385014533997, 0.12272297590970993, 0.11564594507217407]], "avg_entropy": [0.9529723420739173], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Seymour Hersch", "score_of_biggest_cluster": 0.4035904109477997, "score_of_first_answer": 0.4035904109477997, "max_score_of_first_answer": 0.9981266856193542, "min_score_of_first_answer": 0.013232478871941566, "avg_score": 0.4035904109477997, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3795297866723797, "std_of_first_answer": 0.3795297866723797, "avg_entropy_of_first_answer": 0.9529723420739173, "lex_sim": 1.0}
{"question": "What instrument was originally called the trompette-saicueboute?", "answer": ["Trumpet"], "scores": [[0.05530926212668419, 0.9540749788284302]], "normalized_score": [0.22971542179584503], "tokens": [["\u0120Trump", "et"]], "entropy": [[5.585507869720459, 0.32490432262420654]], "avg_entropy": [2.9552060961723328], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Harmonic glissando", "score_of_biggest_cluster": 0.22971542179584503, "score_of_first_answer": 0.22971542179584503, "max_score_of_first_answer": 0.9540749788284302, "min_score_of_first_answer": 0.05530926212668419, "avg_score": 0.22971542179584503, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.449382858350873, "std_of_first_answer": 0.449382858350873, "avg_entropy_of_first_answer": 2.9552060961723328, "lex_sim": 1.0}
{"question": "In which US state is Green Bay?", "answer": ["US Wist"], "scores": [[0.00015763119154144078, 0.003204830689355731, 0.008595260791480541]], "normalized_score": [0.0016314317472279072], "tokens": [["\u0120US", "\u0120W", "ist"]], "entropy": [[1.1889610290527344, 3.519570827484131, 4.004283905029297]], "avg_entropy": [2.904271920522054], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Badger State", "score_of_biggest_cluster": 0.0016314317472279072, "score_of_first_answer": 0.0016314317472279072, "max_score_of_first_answer": 0.008595260791480541, "min_score_of_first_answer": 0.00015763119154144078, "avg_score": 0.0016314317472279072, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.003488644351975602, "std_of_first_answer": 0.003488644351975602, "avg_entropy_of_first_answer": 2.904271920522054, "lex_sim": 1.0}
{"question": "Which British playwright wrote the screenplay for the 1998 film 'Shakespeare In Love'?", "answer": ["William Shakespeare"], "scores": [[0.3733697831630707, 0.7753703594207764]], "normalized_score": [0.5380519032478333], "tokens": [["\u0120William", "\u0120Shakespeare"]], "entropy": [[3.6121065616607666, 1.3739618062973022]], "avg_entropy": [2.4930341839790344], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tom Straussler", "score_of_biggest_cluster": 0.5380519032478333, "score_of_first_answer": 0.5380519032478333, "max_score_of_first_answer": 0.7753703594207764, "min_score_of_first_answer": 0.3733697831630707, "avg_score": 0.5380519032478333, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20100028812885284, "std_of_first_answer": 0.20100028812885284, "avg_entropy_of_first_answer": 2.4930341839790344, "lex_sim": 1.0}
{"question": "What is the name for a female bear?", "answer": ["She-bear"], "scores": [[0.029633063822984695, 0.8782024383544922, 0.8600183725357056]], "normalized_score": [0.2818121314048767], "tokens": [["\u0120She", "-", "bear"]], "entropy": [[5.22412109375, 0.933933436870575, 0.8001453876495361]], "avg_entropy": [2.319399972756704], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "SOW (disambiguation)", "score_of_biggest_cluster": 0.2818121314048767, "score_of_first_answer": 0.2818121314048767, "max_score_of_first_answer": 0.8782024383544922, "min_score_of_first_answer": 0.029633063822984695, "avg_score": 0.2818121314048767, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39580303807834305, "std_of_first_answer": 0.39580303807834305, "avg_entropy_of_first_answer": 2.319399972756704, "lex_sim": 1.0}
{"question": "Which sculptor's works include St Michael's Victory over the Devil at Coventry Cathedral and Jacob and the Angel?", "answer": ["Giambologna"], "scores": [[0.003625306533649564, 0.24770216643810272, 0.9897984266281128, 0.7873718738555908, 0.9578545093536377]], "normalized_score": [0.23187832534313202], "tokens": [["\u0120G", "iam", "b", "olog", "na"]], "entropy": [[2.9087026119232178, 3.431425094604492, 0.09684423357248306, 0.6168326139450073, 0.31863218545913696]], "avg_entropy": [1.4744873479008676], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kathleen Epstein", "score_of_biggest_cluster": 0.23187832534313202, "score_of_first_answer": 0.23187832534313202, "max_score_of_first_answer": 0.9897984266281128, "min_score_of_first_answer": 0.003625306533649564, "avg_score": 0.23187832534313202, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3987104635901159, "std_of_first_answer": 0.3987104635901159, "avg_entropy_of_first_answer": 1.4744873479008676, "lex_sim": 1.0}
{"question": "One of the oldest known board games involves a grid of thirty squares and sets of pawns, figured in a 1999 video game, and is called what?", "answer": ["Go (game)"], "scores": [[0.046072203665971756, 0.0058754365891218185, 0.060554519295692444, 0.9030932188034058]], "normalized_score": [0.06202824413776398], "tokens": [["\u0120Go", "\u0120(", "game", ")"]], "entropy": [[4.5175371170043945, 1.140784740447998, 4.661879539489746, 0.5774267911911011]], "avg_entropy": [2.72440704703331], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Senet", "score_of_biggest_cluster": 0.06202824413776398, "score_of_first_answer": 0.06202824413776398, "max_score_of_first_answer": 0.9030932188034058, "min_score_of_first_answer": 0.0058754365891218185, "avg_score": 0.06202824413776398, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3753474680972224, "std_of_first_answer": 0.3753474680972224, "avg_entropy_of_first_answer": 2.72440704703331, "lex_sim": 1.0}
{"question": "Buenos Aires is located on the estuary of which river?", "answer": ["R\u00edo de la Plata"], "scores": [[0.19243602454662323, 0.6226566433906555, 0.9911625385284424, 0.9872204065322876, 0.9280493855476379, 0.9932308197021484, 0.9999908208847046]], "normalized_score": [0.7277106642723083], "tokens": [["\u0120R", "\u00c3\u0143", "o", "\u0120de", "\u0120la", "\u0120Pl", "ata"]], "entropy": [[3.302147150039673, 2.535522222518921, 0.0878799632191658, 0.10640007257461548, 0.31041598320007324, 0.061297014355659485, 0.00013973568275105208]], "avg_entropy": [0.9148288773701226], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The River Plate (disambiguation)", "score_of_biggest_cluster": 0.7277106642723083, "score_of_first_answer": 0.7277106642723083, "max_score_of_first_answer": 0.9999908208847046, "min_score_of_first_answer": 0.19243602454662323, "avg_score": 0.7277106642723083, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.28385846580159546, "std_of_first_answer": 0.28385846580159546, "avg_entropy_of_first_answer": 0.9148288773701226, "lex_sim": 1.0}
{"question": "What sporting term, meaning a second chance, comes from the French word for fishing again?", "answer": ["Doubles"], "scores": [[0.003975001163780689, 0.5786523222923279]], "normalized_score": [0.047959811985492706], "tokens": [["\u0120Dou", "bles"]], "entropy": [[6.785356521606445, 1.8587605953216553]], "avg_entropy": [4.32205855846405], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rep\u00eachage", "score_of_biggest_cluster": 0.047959811985492706, "score_of_first_answer": 0.047959811985492706, "max_score_of_first_answer": 0.5786523222923279, "min_score_of_first_answer": 0.003975001163780689, "avg_score": 0.047959811985492706, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2873386605642736, "std_of_first_answer": 0.2873386605642736, "avg_entropy_of_first_answer": 4.32205855846405, "lex_sim": 1.0}
{"question": "Which was nicknamed Old Rowley after a favourite stallion?", "answer": ["Rowley"], "scores": [[0.009669575840234756, 0.8185839056968689]], "normalized_score": [0.08896828442811966], "tokens": [["\u0120Row", "ley"]], "entropy": [[7.381567001342773, 1.0947253704071045]], "avg_entropy": [4.238146185874939], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "King Charles the Second", "score_of_biggest_cluster": 0.08896828442811966, "score_of_first_answer": 0.08896828442811966, "max_score_of_first_answer": 0.8185839056968689, "min_score_of_first_answer": 0.009669575840234756, "avg_score": 0.08896828442811966, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.40445716492831707, "std_of_first_answer": 0.40445716492831707, "avg_entropy_of_first_answer": 4.238146185874939, "lex_sim": 1.0}
{"question": "What geological period, named after an English county, was 416-360 million years ago?", "answer": ["Carboniferous"], "scores": [[0.09377183765172958, 0.9954257607460022, 0.9956602454185486]], "normalized_score": [0.4529644846916199], "tokens": [["\u0120Carbon", "ifer", "ous"]], "entropy": [[3.568859815597534, 0.04777000844478607, 0.03750213608145714]], "avg_entropy": [1.2180439867079258], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Emsian", "score_of_biggest_cluster": 0.4529644846916199, "score_of_first_answer": 0.4529644846916199, "max_score_of_first_answer": 0.9956602454185486, "min_score_of_first_answer": 0.09377183765172958, "avg_score": 0.4529644846916199, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4250990148814381, "std_of_first_answer": 0.4250990148814381, "avg_entropy_of_first_answer": 1.2180439867079258, "lex_sim": 1.0}
{"question": "Who named his jet plane 'Big Bunny'?", "answer": ["\u339c"], "scores": [[0.0017904541455209255, 4.0038539736997336e-05, 0.00810894276946783, 0.09083782136440277]], "normalized_score": [0.00269567733630538], "tokens": [["\u0120", "\u00e3", "\u0130", "\u013e"]], "entropy": [[6.380801677703857, 5.346418857574463, 2.2024221420288086, 3.8040547370910645]], "avg_entropy": [4.433424353599548], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hugh Marston Hefner", "score_of_biggest_cluster": 0.00269567733630538, "score_of_first_answer": 0.00269567733630538, "max_score_of_first_answer": 0.09083782136440277, "min_score_of_first_answer": 4.0038539736997336e-05, "avg_score": 0.00269567733630538, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.03801795036296796, "std_of_first_answer": 0.03801795036296796, "avg_entropy_of_first_answer": 4.433424353599548, "lex_sim": 0.0}
{"question": "What nationality was the tennis player Judy Teggart who lost to Billie Jean King in the 1968 Ladies Single Final at Wimbledon?", "answer": ["U.K."], "scores": [[0.003294598078355193, 0.9459584355354309, 0.16461314260959625, 0.7656174898147583]], "normalized_score": [0.14077892899513245], "tokens": [["\u0120U", ".", "K", "."]], "entropy": [[2.82448148727417, 0.41825246810913086, 0.7138086557388306, 0.7851111888885498]], "avg_entropy": [1.1854134500026703], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Australia (Commonwealth realm)", "score_of_biggest_cluster": 0.14077892899513245, "score_of_first_answer": 0.14077892899513245, "max_score_of_first_answer": 0.9459584355354309, "min_score_of_first_answer": 0.003294598078355193, "avg_score": 0.14077892899513245, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39528506512425765, "std_of_first_answer": 0.39528506512425765, "avg_entropy_of_first_answer": 1.1854134500026703, "lex_sim": 1.0}
{"question": "Who wrote the 1911 opera \u2018Der Rosenkavalier\u2019?", "answer": ["Richard Strauss"], "scores": [[0.7378606796264648, 0.960830569267273]], "normalized_score": [0.8419970870018005], "tokens": [["\u0120Richard", "\u0120Strauss"]], "entropy": [[1.944381833076477, 0.3255235552787781]], "avg_entropy": [1.1349526941776276], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Strauss, Richard", "score_of_biggest_cluster": 0.8419970870018005, "score_of_first_answer": 0.8419970870018005, "max_score_of_first_answer": 0.960830569267273, "min_score_of_first_answer": 0.7378606796264648, "avg_score": 0.8419970870018005, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.11148494482040405, "std_of_first_answer": 0.11148494482040405, "avg_entropy_of_first_answer": 1.1349526941776276, "lex_sim": 1.0}
{"question": "Which American car manufacturer has a logo which features three shields inside a circle?", "answer": ["Chevrolet"], "scores": [[0.12270825356245041]], "normalized_score": [0.12270825356245041], "tokens": [["\u0120Chevrolet"]], "entropy": [[4.385730266571045]], "avg_entropy": [4.385730266571045], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Buicks", "score_of_biggest_cluster": 0.12270825356245041, "score_of_first_answer": 0.12270825356245041, "max_score_of_first_answer": 0.12270825356245041, "min_score_of_first_answer": 0.12270825356245041, "avg_score": 0.12270825356245041, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.385730266571045, "lex_sim": 1.0}
{"question": "Which Major League Baseball team plays its home games at Chase Field, built in 1992?", "answer": ["Los Angeles Dodgers"], "scores": [[0.03448253124952316, 0.9058768153190613, 0.35166847705841064]], "normalized_score": [0.22229714691638947], "tokens": [["\u0120Los", "\u0120Angeles", "\u0120Dodgers"]], "entropy": [[3.1005921363830566, 0.635908305644989, 1.1517516374588013]], "avg_entropy": [1.629417359828949], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The D Backs", "score_of_biggest_cluster": 0.22229714691638947, "score_of_first_answer": 0.22229714691638947, "max_score_of_first_answer": 0.9058768153190613, "min_score_of_first_answer": 0.03448253124952316, "avg_score": 0.22229714691638947, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3601052014073068, "std_of_first_answer": 0.3601052014073068, "avg_entropy_of_first_answer": 1.629417359828949, "lex_sim": 1.0}
{"question": "Which boxer fought Mohammed Ali in the fight known as 'The Thriller In Manilla'?", "answer": ["George Foreman"], "scores": [[0.14224959909915924, 0.8976684808731079, 0.9980731010437012]], "normalized_score": [0.5032415390014648], "tokens": [["\u0120George", "\u0120Fore", "man"]], "entropy": [[4.185568332672119, 0.8844616413116455, 0.021278945729136467]], "avg_entropy": [1.6971029732376337], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joe Speedo Frazier", "score_of_biggest_cluster": 0.5032415390014648, "score_of_first_answer": 0.5032415390014648, "max_score_of_first_answer": 0.9980731010437012, "min_score_of_first_answer": 0.14224959909915924, "avg_score": 0.5032415390014648, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38197915027056756, "std_of_first_answer": 0.38197915027056756, "avg_entropy_of_first_answer": 1.6971029732376337, "lex_sim": 1.0}
{"question": "Edward Stanley served three terms as Prime Minister in the 19th century, he was known by what title?", "answer": ["First Earl of Derby"], "scores": [[0.02739918790757656, 0.5774226188659668, 0.9254449605941772, 0.9866001009941101]], "normalized_score": [0.3466815650463104], "tokens": [["\u0120First", "\u0120Earl", "\u0120of", "\u0120Derby"]], "entropy": [[3.602905511856079, 1.6980130672454834, 0.4490499198436737, 0.11043715476989746]], "avg_entropy": [1.4651014134287834], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Earl of Derby", "score_of_biggest_cluster": 0.3466815650463104, "score_of_first_answer": 0.3466815650463104, "max_score_of_first_answer": 0.9866001009941101, "min_score_of_first_answer": 0.02739918790757656, "avg_score": 0.3466815650463104, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3809005849242032, "std_of_first_answer": 0.3809005849242032, "avg_entropy_of_first_answer": 1.4651014134287834, "lex_sim": 1.0}
{"question": "On a motorway what is the speed limit for a car towing a caravan?", "answer": ["80mph"], "scores": [[0.021571792662143707, 0.06218668818473816]], "normalized_score": [0.036626193672418594], "tokens": [["\u012080", "mph"]], "entropy": [[4.906888961791992, 2.79107666015625]], "avg_entropy": [3.848982810974121], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "60 mph", "score_of_biggest_cluster": 0.036626193672418594, "score_of_first_answer": 0.036626193672418594, "max_score_of_first_answer": 0.06218668818473816, "min_score_of_first_answer": 0.021571792662143707, "avg_score": 0.036626193672418594, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.020307447761297226, "std_of_first_answer": 0.020307447761297226, "avg_entropy_of_first_answer": 3.848982810974121, "lex_sim": 1.0}
{"question": "Who would do anything for love (but not that)?", "answer": ["The love-crazed robot named Alice"], "scores": [[0.03001648746430874, 0.006524914409965277, 0.31229329109191895, 0.023862116038799286, 0.9744778275489807, 0.9907913208007812, 0.014238198287785053, 0.007734009530395269, 0.004436193965375423]], "normalized_score": [0.0445295050740242], "tokens": [["\u0120The", "\u0120love", "-", "c", "raz", "ed", "\u0120robot", "\u0120named", "\u0120Alice"]], "entropy": [[6.4132537841796875, 7.3614325523376465, 4.078007221221924, 4.8253583908081055, 0.19503331184387207, 0.08232666552066803, 6.667417526245117, 4.368705749511719, 6.517281532287598]], "avg_entropy": [4.500979637106259], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Smoked meatloaf", "score_of_biggest_cluster": 0.0445295050740242, "score_of_first_answer": 0.0445295050740242, "max_score_of_first_answer": 0.9907913208007812, "min_score_of_first_answer": 0.004436193965375423, "avg_score": 0.0445295050740242, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39573505223887206, "std_of_first_answer": 0.39573505223887206, "avg_entropy_of_first_answer": 4.500979637106259, "lex_sim": 1.0}
{"question": "What was the name of the ship that took the Pilgrim Fathers to America?", "answer": ["Mayflower"], "scores": [[0.5152093172073364, 0.9980019927024841]], "normalized_score": [0.7170633673667908], "tokens": [["\u0120May", "flower"]], "entropy": [[2.3275821208953857, 0.017292991280555725]], "avg_entropy": [1.1724375560879707], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ship Mayflower", "score_of_biggest_cluster": 0.7170633673667908, "score_of_first_answer": 0.7170633673667908, "max_score_of_first_answer": 0.9980019927024841, "min_score_of_first_answer": 0.5152093172073364, "avg_score": 0.7170633673667908, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.24139633774757385, "std_of_first_answer": 0.24139633774757385, "avg_entropy_of_first_answer": 1.1724375560879707, "lex_sim": 1.0}
{"question": "Whom did Andy Murray defeat in the Wimbledon men's singles final in 2013 ?", "answer": ["WTA Women's Top 10"], "scores": [[0.0010888878023251891, 0.018299924209713936, 0.0023688971996307373, 0.8480420708656311, 0.0012912240345031023, 0.3187803328037262]], "normalized_score": [0.015951991081237793], "tokens": [["\u0120W", "TA", "\u0120Women", "'s", "\u0120Top", "\u012010"]], "entropy": [[3.3893678188323975, 1.2939523458480835, 5.3529052734375, 1.1087441444396973, 2.816159248352051, 2.9674925804138184]], "avg_entropy": [2.821436901887258], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Novak Djokovi\u0107", "score_of_biggest_cluster": 0.015951991081237793, "score_of_first_answer": 0.015951991081237793, "max_score_of_first_answer": 0.8480420708656311, "min_score_of_first_answer": 0.0010888878023251891, "avg_score": 0.015951991081237793, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31229632650946576, "std_of_first_answer": 0.31229632650946576, "avg_entropy_of_first_answer": 2.821436901887258, "lex_sim": 1.0}
{"question": "Sanskrit is an ancient language of which modern day country?", "answer": ["India"], "scores": [[0.8419876098632812]], "normalized_score": [0.8419876098632812], "tokens": [["\u0120India"]], "entropy": [[1.1954069137573242]], "avg_entropy": [1.1954069137573242], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u0d2d\u0d3e\u0d30\u0d24 \u0d2e\u0d39\u0d3e\u0d30\u0d3e\u0d1c\u0d4d\u0d2f\u0d02", "score_of_biggest_cluster": 0.8419876098632812, "score_of_first_answer": 0.8419876098632812, "max_score_of_first_answer": 0.8419876098632812, "min_score_of_first_answer": 0.8419876098632812, "avg_score": 0.8419876098632812, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.1954069137573242, "lex_sim": 1.0}
{"question": "Who is the only Rugby League player to win two consecutive Lance Todd trophies, doing so in 2007 and 2008?", "answer": ["Ricky Walford"], "scores": [[0.005758128594607115, 0.20469693839550018, 0.46636733412742615, 0.9972922205924988]], "normalized_score": [0.15301565825939178], "tokens": [["\u0120Ricky", "\u0120W", "alf", "ord"]], "entropy": [[5.648256301879883, 2.017080783843994, 2.0161099433898926, 0.027511777356266975]], "avg_entropy": [2.427239701617509], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paul Wellens", "score_of_biggest_cluster": 0.15301565825939178, "score_of_first_answer": 0.15301565825939178, "max_score_of_first_answer": 0.9972922205924988, "min_score_of_first_answer": 0.005758128594607115, "avg_score": 0.15301565825939178, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3719406251584424, "std_of_first_answer": 0.3719406251584424, "avg_entropy_of_first_answer": 2.427239701617509, "lex_sim": 1.0}
{"question": "In which city did Field Marshal Freidrich Paulus surrender in January 1943?", "answer": ["Bergen-op-Ilt"], "scores": [[0.0017501900438219309, 0.42307302355766296, 0.8121418952941895, 0.15991300344467163, 0.9723249673843384, 0.025773126631975174, 0.003652190789580345]], "normalized_score": [0.07066773623228073], "tokens": [["\u0120Ber", "gen", "-", "op", "-", "I", "lt"]], "entropy": [[3.222684383392334, 2.2695460319519043, 0.9924888610839844, 0.875656247138977, 0.2769553065299988, 2.860562324523926, 4.596131324768066]], "avg_entropy": [2.156289211341313], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tsaritzyn", "score_of_biggest_cluster": 0.07066773623228073, "score_of_first_answer": 0.07066773623228073, "max_score_of_first_answer": 0.9723249673843384, "min_score_of_first_answer": 0.0017501900438219309, "avg_score": 0.07066773623228073, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.375750111491423, "std_of_first_answer": 0.375750111491423, "avg_entropy_of_first_answer": 2.156289211341313, "lex_sim": 1.0}
{"question": "\"Who wrote the TV series \"\"Pennies From Heaven\"\", and \"\"The Singing Detective\"\"?\"", "answer": ["Tom Cruise and Clint Eastwood"], "scores": [[0.005169873591512442, 0.003615870140492916, 0.22278477251529694, 0.030230585485696793, 0.9159951210021973, 0.9887304902076721]], "normalized_score": [0.06963582336902618], "tokens": [["\u0120Tom", "\u0120Cruise", "\u0120and", "\u0120Clint", "\u0120East", "wood"]], "entropy": [[5.855023384094238, 4.3116044998168945, 1.1894935369491577, 5.057929992675781, 0.6520678997039795, 0.1008467972278595]], "avg_entropy": [2.8611610184113183], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dennis C. G. Potter", "score_of_biggest_cluster": 0.06963582336902618, "score_of_first_answer": 0.06963582336902618, "max_score_of_first_answer": 0.9887304902076721, "min_score_of_first_answer": 0.003615870140492916, "avg_score": 0.06963582336902618, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42522845943033066, "std_of_first_answer": 0.42522845943033066, "avg_entropy_of_first_answer": 2.8611610184113183, "lex_sim": 1.0}
{"question": "Although it ended up being very ineffective, what was the name for the interconnected series of concrete fortifications built by France along its border with Germany between WWI and WWII?", "answer": ["Maginot Line"], "scores": [[0.5606345534324646, 0.9990285634994507, 0.997714638710022, 0.871928870677948]], "normalized_score": [0.8354804515838623], "tokens": [["\u0120Mag", "in", "ot", "\u0120Line"]], "entropy": [[2.0980286598205566, 0.011959392577409744, 0.02371962182223797, 0.6810283660888672]], "avg_entropy": [0.7036840100772679], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Magniot line", "score_of_biggest_cluster": 0.8354804515838623, "score_of_first_answer": 0.8354804515838623, "max_score_of_first_answer": 0.9990285634994507, "min_score_of_first_answer": 0.5606345534324646, "avg_score": 0.8354804515838623, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17890475446296705, "std_of_first_answer": 0.17890475446296705, "avg_entropy_of_first_answer": 0.7036840100772679, "lex_sim": 1.0}
{"question": "The first US space station was launched on May 14, 1973. What was it called?", "answer": ["Skylab, also known as the Tranquility Base"], "scores": [[0.8890495896339417, 0.9886561632156372, 0.008859176188707352, 0.013877354562282562, 0.4298638105392456, 0.9768078327178955, 0.413910448551178, 0.005444696638733149, 0.9482394456863403, 0.7731267809867859, 0.7567472457885742]], "normalized_score": [0.2194095253944397], "tokens": [["\u0120Sky", "lab", ",", "\u0120also", "\u0120known", "\u0120as", "\u0120the", "\u0120Tr", "anqu", "ility", "\u0120Base"]], "entropy": [[0.8879639506340027, 0.07528521120548248, 0.362662672996521, 4.98784065246582, 2.554748296737671, 0.18891891837120056, 3.302638053894043, 4.703475475311279, 0.37086769938468933, 1.0312442779541016, 1.4668779373168945]], "avg_entropy": [1.8120475587519733], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Skylab One", "score_of_biggest_cluster": 0.2194095253944397, "score_of_first_answer": 0.2194095253944397, "max_score_of_first_answer": 0.9886561632156372, "min_score_of_first_answer": 0.005444696638733149, "avg_score": 0.2194095253944397, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3870923672320554, "std_of_first_answer": 0.3870923672320554, "avg_entropy_of_first_answer": 1.8120475587519733, "lex_sim": 1.0}
{"question": "What is the name of the Lord played by Rik Mayall in the UK television series \u2018Blackadder Goes Forth\u2019?", "answer": ["God"], "scores": [[0.025744842365384102]], "normalized_score": [0.02574484422802925], "tokens": [["\u0120God"]], "entropy": [[5.878786563873291]], "avg_entropy": [5.878786563873291], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Flashheart", "score_of_biggest_cluster": 0.02574484422802925, "score_of_first_answer": 0.02574484422802925, "max_score_of_first_answer": 0.025744842365384102, "min_score_of_first_answer": 0.025744842365384102, "avg_score": 0.02574484422802925, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.878786563873291, "lex_sim": 1.0}
{"question": "What product 'brand' from the Gamay is traditionally released the third Thursday of November each year?", "answer": ["La Gamay Noir"], "scores": [[0.00593022583052516, 0.012163403443992138, 0.9012659788131714, 0.007692909333854914]], "normalized_score": [0.026593003422021866], "tokens": [["\u0120La", "\u0120Gam", "ay", "\u0120Noir"]], "entropy": [[3.2513346672058105, 5.515165328979492, 0.7093175649642944, 2.4284722805023193]], "avg_entropy": [2.976072460412979], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Beaujolais Nouveau", "score_of_biggest_cluster": 0.026593003422021866, "score_of_first_answer": 0.026593003422021866, "max_score_of_first_answer": 0.9012659788131714, "min_score_of_first_answer": 0.00593022583052516, "avg_score": 0.026593003422021866, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3865443276221434, "std_of_first_answer": 0.3865443276221434, "avg_entropy_of_first_answer": 2.976072460412979, "lex_sim": 1.0}
{"question": "Which British monarch founded the Yeomen of the Guard, originally as a royal bodyguard?", "answer": ["George II"], "scores": [[0.13992169499397278, 0.2643125355243683]], "normalized_score": [0.19230979681015015], "tokens": [["\u0120George", "\u0120II"]], "entropy": [[2.762312412261963, 2.2560887336730957]], "avg_entropy": [2.5092005729675293], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hendrik VII", "score_of_biggest_cluster": 0.19230979681015015, "score_of_first_answer": 0.19230979681015015, "max_score_of_first_answer": 0.2643125355243683, "min_score_of_first_answer": 0.13992169499397278, "avg_score": 0.19230979681015015, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.062195420265197754, "std_of_first_answer": 0.062195420265197754, "avg_entropy_of_first_answer": 2.5092005729675293, "lex_sim": 1.0}
{"question": "What is the name of the sidekick of cartoon superhero DangerMouse?", "answer": ["Mickey"], "scores": [[0.0033629490062594414]], "normalized_score": [0.0033629483077675104], "tokens": [["\u0120Mickey"]], "entropy": [[7.15597677230835]], "avg_entropy": [7.15597677230835], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Donnie Murdo", "score_of_biggest_cluster": 0.0033629483077675104, "score_of_first_answer": 0.0033629483077675104, "max_score_of_first_answer": 0.0033629490062594414, "min_score_of_first_answer": 0.0033629490062594414, "avg_score": 0.0033629483077675104, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 7.15597677230835, "lex_sim": 1.0}
{"question": "As at 2012 Dame Marjorie Scardino is/was head of what high-performing global media/learning corporation?", "answer": ["Pearson"], "scores": [[0.8306622505187988]], "normalized_score": [0.8306622505187988], "tokens": [["\u0120Pearson"]], "entropy": [[1.3001928329467773]], "avg_entropy": [1.3001928329467773], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Parker Publishing Company, Inc.", "score_of_biggest_cluster": 0.8306622505187988, "score_of_first_answer": 0.8306622505187988, "max_score_of_first_answer": 0.8306622505187988, "min_score_of_first_answer": 0.8306622505187988, "avg_score": 0.8306622505187988, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.3001928329467773, "lex_sim": 1.0}
{"question": "Which snooker player was simply known as 'The Grinder'?", "answer": ["Ronnie O'Sullivan"], "scores": [[0.12450242787599564, 0.9008938074111938, 0.92911696434021, 0.9991036057472229]], "normalized_score": [0.568045437335968], "tokens": [["\u0120Ronnie", "\u0120O", "'", "Sullivan"]], "entropy": [[4.0631794929504395, 0.8329461812973022, 0.6040157079696655, 0.009471971541643143]], "avg_entropy": [1.3774033384397626], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cliff Thorburn", "score_of_biggest_cluster": 0.568045437335968, "score_of_first_answer": 0.568045437335968, "max_score_of_first_answer": 0.9991036057472229, "min_score_of_first_answer": 0.12450242787599564, "avg_score": 0.568045437335968, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35623510388957896, "std_of_first_answer": 0.35623510388957896, "avg_entropy_of_first_answer": 1.3774033384397626, "lex_sim": 1.0}
{"question": "In Verdi's opera 'Rigoletto', what is the name of the daughter of the title character?", "answer": ["Gilda"], "scores": [[0.7288141846656799, 0.9654499292373657]], "normalized_score": [0.8388287425041199], "tokens": [["\u0120G", "ilda"]], "entropy": [[1.8521857261657715, 0.21926768124103546]], "avg_entropy": [1.0357267037034035], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gilda", "score_of_biggest_cluster": 0.8388287425041199, "score_of_first_answer": 0.8388287425041199, "max_score_of_first_answer": 0.9654499292373657, "min_score_of_first_answer": 0.7288141846656799, "avg_score": 0.8388287425041199, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1183178722858429, "std_of_first_answer": 0.1183178722858429, "avg_entropy_of_first_answer": 1.0357267037034035, "lex_sim": 1.0}
{"question": "Who was the President of the USA at the time of the 1929 Stock Market 'Crash'?", "answer": ["President Herbert Hoover"], "scores": [[0.00557094207033515, 0.7168696522712708, 0.9478248357772827]], "normalized_score": [0.15584716200828552], "tokens": [["\u0120President", "\u0120Herbert", "\u0120Hoover"]], "entropy": [[1.0360229015350342, 1.284171223640442, 0.36646631360054016]], "avg_entropy": [0.8955534795920054], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Presidency of Herbert Hoover", "score_of_biggest_cluster": 0.15584716200828552, "score_of_first_answer": 0.15584716200828552, "max_score_of_first_answer": 0.9478248357772827, "min_score_of_first_answer": 0.00557094207033515, "avg_score": 0.15584716200828552, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4009888543931412, "std_of_first_answer": 0.4009888543931412, "avg_entropy_of_first_answer": 0.8955534795920054, "lex_sim": 1.0}
{"question": "In the Commonwealth games in Delhi, which British athlete won the mens 200m gold medal?", "answer": ["Greg Rutherford"], "scores": [[0.011930589564144611, 0.9857490658760071]], "normalized_score": [0.10844617336988449], "tokens": [["\u0120Greg", "\u0120Rutherford"]], "entropy": [[5.2378644943237305, 0.1498931497335434]], "avg_entropy": [2.693878822028637], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Leon Baptiste", "score_of_biggest_cluster": 0.10844617336988449, "score_of_first_answer": 0.10844617336988449, "max_score_of_first_answer": 0.9857490658760071, "min_score_of_first_answer": 0.011930589564144611, "avg_score": 0.10844617336988449, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.48690923815593123, "std_of_first_answer": 0.48690923815593123, "avg_entropy_of_first_answer": 2.693878822028637, "lex_sim": 1.0}
{"question": "Which metal is obtained from the mineral ore haematite?", "answer": ["Iron"], "scores": [[0.589194655418396]], "normalized_score": [0.589194655418396], "tokens": [["\u0120Iron"]], "entropy": [[2.6409811973571777]], "avg_entropy": [2.6409811973571777], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Scunthorpe & Lindsey United F.C.", "score_of_biggest_cluster": 0.589194655418396, "score_of_first_answer": 0.589194655418396, "max_score_of_first_answer": 0.589194655418396, "min_score_of_first_answer": 0.589194655418396, "avg_score": 0.589194655418396, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.6409811973571777, "lex_sim": 1.0}
{"question": "In past times a rabbit was what we\u2019d call a baby bunny. What were adult bunnies called?", "answer": ["Mature bunnies"], "scores": [[0.019242366775870323, 0.18357014656066895, 0.21276147663593292, 0.9824321866035461, 0.997520387172699]], "normalized_score": [0.23628465831279755], "tokens": [["\u0120M", "ature", "\u0120b", "unn", "ies"]], "entropy": [[5.855109691619873, 3.3766188621520996, 2.3405208587646484, 0.11136234551668167, 0.024219268932938576]], "avg_entropy": [2.3415662053972484], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Coney (disambiguation)", "score_of_biggest_cluster": 0.23628465831279755, "score_of_first_answer": 0.23628465831279755, "max_score_of_first_answer": 0.997520387172699, "min_score_of_first_answer": 0.019242366775870323, "avg_score": 0.23628465831279755, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4223378112133447, "std_of_first_answer": 0.4223378112133447, "avg_entropy_of_first_answer": 2.3415662053972484, "lex_sim": 1.0}
{"question": "What was first worn by British soldiers in India in 1845?", "answer": ["The Rifleman"], "scores": [[0.14389458298683167, 0.006655670236796141, 0.09300445765256882]], "normalized_score": [0.044659439474344254], "tokens": [["\u0120The", "\u0120Rifle", "man"]], "entropy": [[5.983355522155762, 6.576613426208496, 3.3757848739624023]], "avg_entropy": [5.311917940775554], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Khaki", "score_of_biggest_cluster": 0.044659439474344254, "score_of_first_answer": 0.044659439474344254, "max_score_of_first_answer": 0.14389458298683167, "min_score_of_first_answer": 0.006655670236796141, "avg_score": 0.044659439474344254, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05664748412332689, "std_of_first_answer": 0.05664748412332689, "avg_entropy_of_first_answer": 5.311917940775554, "lex_sim": 1.0}
{"question": "In which town in Greater Manchester is the TV series set?", "answer": ["Manchester"], "scores": [[0.15752963721752167]], "normalized_score": [0.15752963721752167], "tokens": [["\u0120Manchester"]], "entropy": [[4.643313407897949]], "avg_entropy": [4.643313407897949], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rochdale, England", "score_of_biggest_cluster": 0.15752963721752167, "score_of_first_answer": 0.15752963721752167, "max_score_of_first_answer": 0.15752963721752167, "min_score_of_first_answer": 0.15752963721752167, "avg_score": 0.15752963721752167, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.643313407897949, "lex_sim": 1.0}
{"question": "What does the DSM-IV define as: \u00a0\u00a0\u00a0\u00a0A. Over a period of at least 6 months, recurrent, intense sexually arousing fantasies, sexual urges, or behaviors involving the act of observing an unsuspecting person who is naked, in the process of disrobing, or engaging in sexual activity. B. The person has acted on these urges, or the sexual urges or fantasies cause marked distress or interpersonal difficulty.", "answer": ["Homosex"], "scores": [[0.02201220765709877, 0.013352872803807259, 0.971207857131958]], "normalized_score": [0.0658440887928009], "tokens": [["\u0120Hom", "ose", "x"]], "entropy": [[4.956706523895264, 0.42705851793289185, 0.22200119495391846]], "avg_entropy": [1.8685887455940247], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mixoscopia", "score_of_biggest_cluster": 0.0658440887928009, "score_of_first_answer": 0.0658440887928009, "max_score_of_first_answer": 0.971207857131958, "min_score_of_first_answer": 0.013352872803807259, "avg_score": 0.0658440887928009, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44951004636707675, "std_of_first_answer": 0.44951004636707675, "avg_entropy_of_first_answer": 1.8685887455940247, "lex_sim": 1.0}
{"question": "Milwaukee is the largest city in which American state?", "answer": ["Wisconsin"], "scores": [[0.8309111595153809]], "normalized_score": [0.8309111595153809], "tokens": [["\u0120Wisconsin"]], "entropy": [[1.2256556749343872]], "avg_entropy": [1.2256556749343872], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Badger State", "score_of_biggest_cluster": 0.8309111595153809, "score_of_first_answer": 0.8309111595153809, "max_score_of_first_answer": 0.8309111595153809, "min_score_of_first_answer": 0.8309111595153809, "avg_score": 0.8309111595153809, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.2256556749343872, "lex_sim": 1.0}
{"question": "\"Which comedian said, \"\"Marriage is a wonderful invention, but then again, so is the bicycle repair kit\"\"?\"", "answer": ["Jerry Seinfeld"], "scores": [[0.018925195559859276, 0.1584385186433792, 0.987076461315155]], "normalized_score": [0.1435767263174057], "tokens": [["\u0120Jerry", "\u0120Se", "infeld"]], "entropy": [[5.839668273925781, 2.33415150642395, 0.10298943519592285]], "avg_entropy": [2.7589364051818848], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Billy Conolly", "score_of_biggest_cluster": 0.1435767263174057, "score_of_first_answer": 0.1435767263174057, "max_score_of_first_answer": 0.987076461315155, "min_score_of_first_answer": 0.018925195559859276, "avg_score": 0.1435767263174057, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42732003108998357, "std_of_first_answer": 0.42732003108998357, "avg_entropy_of_first_answer": 2.7589364051818848, "lex_sim": 1.0}
{"question": "Estelle Parsons won a best supporting actress Oscar for her role in which 1967 film?", "answer": ["Cool Hand Luke"], "scores": [[0.01567273959517479, 0.9575812220573425, 0.9979861974716187]], "normalized_score": [0.2464989274740219], "tokens": [["\u0120Cool", "\u0120Hand", "\u0120Luke"]], "entropy": [[4.80143404006958, 0.2985033392906189, 0.022917941212654114]], "avg_entropy": [1.707618440190951], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Prentiss Oakley", "score_of_biggest_cluster": 0.2464989274740219, "score_of_first_answer": 0.2464989274740219, "max_score_of_first_answer": 0.9979861974716187, "min_score_of_first_answer": 0.01567273959517479, "avg_score": 0.2464989274740219, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45384332598835964, "std_of_first_answer": 0.45384332598835964, "avg_entropy_of_first_answer": 1.707618440190951, "lex_sim": 1.0}
{"question": "Which English football league club is based in Burslem?", "answer": ["Burslem"], "scores": [[0.0338265523314476, 0.9221537709236145, 0.9795531630516052]], "normalized_score": [0.3126293420791626], "tokens": [["\u0120B", "urs", "lem"]], "entropy": [[1.299596905708313, 0.48371708393096924, 0.14888322353363037]], "avg_entropy": [0.6440657377243042], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Port Vale F.C", "score_of_biggest_cluster": 0.3126293420791626, "score_of_first_answer": 0.3126293420791626, "max_score_of_first_answer": 0.9795531630516052, "min_score_of_first_answer": 0.0338265523314476, "avg_score": 0.3126293420791626, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.432925289902259, "std_of_first_answer": 0.432925289902259, "avg_entropy_of_first_answer": 0.6440657377243042, "lex_sim": 1.0}
{"question": "Which band released the 1970 album 'In the Wake of Poseidon'?", "answer": ["Electric Eels"], "scores": [[0.006659369450062513, 0.0031532973516732454, 0.719455897808075]], "normalized_score": [0.02472107857465744], "tokens": [["\u0120Electric", "\u0120E", "els"]], "entropy": [[4.751967430114746, 1.2703678607940674, 0.7372918725013733]], "avg_entropy": [2.2532090544700623], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "KingCrimson", "score_of_biggest_cluster": 0.02472107857465744, "score_of_first_answer": 0.02472107857465744, "max_score_of_first_answer": 0.719455897808075, "min_score_of_first_answer": 0.0031532973516732454, "avg_score": 0.02472107857465744, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33684493610255306, "std_of_first_answer": 0.33684493610255306, "avg_entropy_of_first_answer": 2.2532090544700623, "lex_sim": 1.0}
{"question": "How many balls are on a snooker table at the beginning of a game?", "answer": ["16 balls"], "scores": [[0.0015323086408898234, 0.014122751541435719]], "normalized_score": [0.0046519264578819275], "tokens": [["\u012016", "\u0120balls"]], "entropy": [[4.487705230712891, 0.685967206954956]], "avg_entropy": [2.5868362188339233], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "twenty-two", "score_of_biggest_cluster": 0.0046519264578819275, "score_of_first_answer": 0.0046519264578819275, "max_score_of_first_answer": 0.014122751541435719, "min_score_of_first_answer": 0.0015323086408898234, "avg_score": 0.0046519264578819275, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0062952214502729475, "std_of_first_answer": 0.0062952214502729475, "avg_entropy_of_first_answer": 2.5868362188339233, "lex_sim": 1.0}
{"question": "Which device, now a plaything, has a lift end, a dingle end and an elbow?", "answer": ["Nunchucks"], "scores": [[0.0025225249119102955, 0.036709994077682495, 0.05204017460346222]], "normalized_score": [0.016890907660126686], "tokens": [["\u0120N", "unch", "ucks"]], "entropy": [[6.761366844177246, 4.095810890197754, 1.6147539615631104]], "avg_entropy": [4.157310565312703], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Boomerang world cup", "score_of_biggest_cluster": 0.016890907660126686, "score_of_first_answer": 0.016890907660126686, "max_score_of_first_answer": 0.05204017460346222, "min_score_of_first_answer": 0.0025225249119102955, "avg_score": 0.016890907660126686, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.020698349676266124, "std_of_first_answer": 0.020698349676266124, "avg_entropy_of_first_answer": 4.157310565312703, "lex_sim": 1.0}
{"question": "In London in 1788, the \u2018Daily Universal Register\u2019 was relaunched as which newspaper?", "answer": ["The London Daily Advertiser"], "scores": [[0.6659440398216248, 0.033889930695295334, 0.35005828738212585, 0.25238940119743347, 0.9945207834243774, 0.9990115165710449]], "normalized_score": [0.3543921411037445], "tokens": [["\u0120The", "\u0120London", "\u0120Daily", "\u0120Ad", "vertis", "er"]], "entropy": [[2.04007625579834, 3.272740602493286, 2.941744804382324, 2.464989423751831, 0.051920484751462936, 0.011506237089633942]], "avg_entropy": [1.7971629680444796], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Times (United Kingdom)", "score_of_biggest_cluster": 0.3543921411037445, "score_of_first_answer": 0.3543921411037445, "max_score_of_first_answer": 0.9990115165710449, "min_score_of_first_answer": 0.033889930695295334, "avg_score": 0.3543921411037445, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3668715782104961, "std_of_first_answer": 0.3668715782104961, "avg_entropy_of_first_answer": 1.7971629680444796, "lex_sim": 1.0}
{"question": "RAF Lakenheath is situated in which English county?", "answer": ["Suffolk"], "scores": [[0.791318416595459]], "normalized_score": [0.791318416595459], "tokens": [["\u0120Suffolk"]], "entropy": [[0.9550880193710327]], "avg_entropy": [0.9550880193710327], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wyverstone Street", "score_of_biggest_cluster": 0.791318416595459, "score_of_first_answer": 0.791318416595459, "max_score_of_first_answer": 0.791318416595459, "min_score_of_first_answer": 0.791318416595459, "avg_score": 0.791318416595459, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.9550880193710327, "lex_sim": 1.0}
{"question": "In the NATO phonetic alphabet which word represents the letter \u2018C\u2019?", "answer": ["C"], "scores": [[0.16162316501140594]], "normalized_score": [0.16162316501140594], "tokens": [["\u0120C"]], "entropy": [[5.729117393493652]], "avg_entropy": [5.729117393493652], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charlie", "score_of_biggest_cluster": 0.16162316501140594, "score_of_first_answer": 0.16162316501140594, "max_score_of_first_answer": 0.16162316501140594, "min_score_of_first_answer": 0.16162316501140594, "avg_score": 0.16162316501140594, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.729117393493652, "lex_sim": 1.0}
{"question": "What type of creature is a Meadow Brown?", "answer": ["Meadow Brown"], "scores": [[0.11033085733652115, 0.7347502112388611]], "normalized_score": [0.2847202718257904], "tokens": [["\u0120Meadow", "\u0120Brown"]], "entropy": [[5.263294219970703, 1.8975040912628174]], "avg_entropy": [3.5803991556167603], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Butterfly", "score_of_biggest_cluster": 0.2847202718257904, "score_of_first_answer": 0.2847202718257904, "max_score_of_first_answer": 0.7347502112388611, "min_score_of_first_answer": 0.11033085733652115, "avg_score": 0.2847202718257904, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31220967695116997, "std_of_first_answer": 0.31220967695116997, "avg_entropy_of_first_answer": 3.5803991556167603, "lex_sim": 1.0}
{"question": "The Tinta Barroca grape is chiefly used to make which drink?", "answer": ["Brandy Poit"], "scores": [[0.04281802102923393, 0.9824994802474976, 9.242464147973806e-05, 0.18321892619132996]], "normalized_score": [0.029052216559648514], "tokens": [["\u0120Br", "andy", "\u0120Po", "it"]], "entropy": [[3.9630589485168457, 0.14722344279289246, 0.6314793825149536, 2.7375080585479736]], "avg_entropy": [1.8698174580931664], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sea port", "score_of_biggest_cluster": 0.029052216559648514, "score_of_first_answer": 0.029052216559648514, "max_score_of_first_answer": 0.9824994802474976, "min_score_of_first_answer": 9.242464147973806e-05, "avg_score": 0.029052216559648514, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3985949916677875, "std_of_first_answer": 0.3985949916677875, "avg_entropy_of_first_answer": 1.8698174580931664, "lex_sim": 1.0}
{"question": "\"Who published \"\"A Book of Nonsense\"\" (a volume of limericks that went through three editions) in 1846, \"\"The History of the Seven Families of the Lake Pipple-Popple\"\" in 1865, and \"\"The Owl and the Pussycat\"\" in 1867?\"", "answer": ["John Tyndall"], "scores": [[0.024598514661192894, 0.006241264287382364, 0.942092776298523, 0.9075337648391724]], "normalized_score": [0.10703711956739426], "tokens": [["\u0120John", "\u0120Ty", "nd", "all"]], "entropy": [[2.9413719177246094, 5.073371887207031, 0.37052813172340393, 0.40925103425979614]], "avg_entropy": [2.19863074272871], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Edward Lear", "score_of_biggest_cluster": 0.10703711956739426, "score_of_first_answer": 0.10703711956739426, "max_score_of_first_answer": 0.942092776298523, "min_score_of_first_answer": 0.006241264287382364, "avg_score": 0.10703711956739426, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4549071274820177, "std_of_first_answer": 0.4549071274820177, "avg_entropy_of_first_answer": 2.19863074272871, "lex_sim": 1.0}
{"question": "Patusnaya and mallasol are types of what?", "answer": ["Flocculation"], "scores": [[0.0023703339975327253, 0.001772834686562419, 0.4292103946208954]], "normalized_score": [0.012172580696642399], "tokens": [["\u0120Fl", "occ", "ulation"]], "entropy": [[7.461212158203125, 3.381927013397217, 1.7453279495239258]], "avg_entropy": [4.196155707041423], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ikra", "score_of_biggest_cluster": 0.012172580696642399, "score_of_first_answer": 0.012172580696642399, "max_score_of_first_answer": 0.4292103946208954, "min_score_of_first_answer": 0.001772834686562419, "avg_score": 0.012172580696642399, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20135531392182188, "std_of_first_answer": 0.20135531392182188, "avg_entropy_of_first_answer": 4.196155707041423, "lex_sim": 1.0}
{"question": "The city of Baghdad lies on which river?", "answer": ["Tigris"], "scores": [[0.4666961133480072, 0.9975214600563049]], "normalized_score": [0.6823044419288635], "tokens": [["\u0120Tig", "ris"]], "entropy": [[2.4132354259490967, 0.02834932506084442]], "avg_entropy": [1.2207923755049706], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tigris", "score_of_biggest_cluster": 0.6823044419288635, "score_of_first_answer": 0.6823044419288635, "max_score_of_first_answer": 0.9975214600563049, "min_score_of_first_answer": 0.4666961133480072, "avg_score": 0.6823044419288635, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.26541267335414886, "std_of_first_answer": 0.26541267335414886, "avg_entropy_of_first_answer": 1.2207923755049706, "lex_sim": 1.0}
{"question": "If you had a seal point and/or a blue point, what breed of cat do you have?", "answer": ["Seal Point Siamese"], "scores": [[0.002563850488513708, 0.2654356360435486, 0.10885314643383026, 0.9922444224357605, 0.999187171459198]], "normalized_score": [0.1490018516778946], "tokens": [["\u0120Seal", "\u0120Point", "\u0120Si", "ames", "e"]], "entropy": [[5.355224132537842, 2.163471221923828, 3.3841328620910645, 0.0784723311662674, 0.009499052539467812]], "avg_entropy": [2.198159920051694], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Siamese", "score_of_biggest_cluster": 0.1490018516778946, "score_of_first_answer": 0.1490018516778946, "max_score_of_first_answer": 0.999187171459198, "min_score_of_first_answer": 0.002563850488513708, "avg_score": 0.1490018516778946, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4343919012421029, "std_of_first_answer": 0.4343919012421029, "avg_entropy_of_first_answer": 2.198159920051694, "lex_sim": 1.0}
{"question": "\"Which English comedian had a minor hit in 1958 with his cover version of the Bobby Darin song, \"\"Splish Splash\"\"?\"", "answer": ["Terry Jones"], "scores": [[0.026494426652789116, 0.03883935883641243]], "normalized_score": [0.03207844868302345], "tokens": [["\u0120Terry", "\u0120Jones"]], "entropy": [[4.7060770988464355, 3.536691188812256]], "avg_entropy": [4.121384143829346], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "CHARLIE DRAKE", "score_of_biggest_cluster": 0.03207844868302345, "score_of_first_answer": 0.03207844868302345, "max_score_of_first_answer": 0.03883935883641243, "min_score_of_first_answer": 0.026494426652789116, "avg_score": 0.03207844868302345, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.006172466091811657, "std_of_first_answer": 0.006172466091811657, "avg_entropy_of_first_answer": 4.121384143829346, "lex_sim": 1.0}
{"question": "Clifford\u2019s Tower is all that remains of which city\u2019s castle?", "answer": ["Clifford's Tower"], "scores": [[0.004513316787779331, 0.25855979323387146, 0.7907686829566956]], "normalized_score": [0.0973573625087738], "tokens": [["\u0120Clifford", "'s", "\u0120Tower"]], "entropy": [[4.612936019897461, 1.809287667274475, 0.9419469833374023]], "avg_entropy": [2.4547235568364463], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Park Grove (1895)", "score_of_biggest_cluster": 0.0973573625087738, "score_of_first_answer": 0.0973573625087738, "max_score_of_first_answer": 0.7907686829566956, "min_score_of_first_answer": 0.004513316787779331, "avg_score": 0.0973573625087738, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3276148509936109, "std_of_first_answer": 0.3276148509936109, "avg_entropy_of_first_answer": 2.4547235568364463, "lex_sim": 1.0}
{"question": "Darwin is the capital of which Australian state ?", "answer": ["Tasmania"], "scores": [[0.07155370712280273]], "normalized_score": [0.07155370712280273], "tokens": [["\u0120Tasmania"]], "entropy": [[2.422316551208496]], "avg_entropy": [2.422316551208496], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Northern Territory", "score_of_biggest_cluster": 0.07155370712280273, "score_of_first_answer": 0.07155370712280273, "max_score_of_first_answer": 0.07155370712280273, "min_score_of_first_answer": 0.07155370712280273, "avg_score": 0.07155370712280273, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.422316551208496, "lex_sim": 1.0}
{"question": "My bologna has a first name. What is it?", "answer": ["Bologna Fred"], "scores": [[0.0346435010433197, 0.7563790678977966, 0.8970322608947754, 0.0003299227391835302]], "normalized_score": [0.05277100205421448], "tokens": [["\u0120B", "olog", "na", "\u0120Fred"]], "entropy": [[6.9227681159973145, 2.003602981567383, 0.6247802972793579, 1.3358581066131592]], "avg_entropy": [2.7217523753643036], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Oscar (film)", "score_of_biggest_cluster": 0.05277100205421448, "score_of_first_answer": 0.05277100205421448, "max_score_of_first_answer": 0.8970322608947754, "min_score_of_first_answer": 0.0003299227391835302, "avg_score": 0.05277100205421448, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40783442849748625, "std_of_first_answer": 0.40783442849748625, "avg_entropy_of_first_answer": 2.7217523753643036, "lex_sim": 1.0}
{"question": "What sort of cross can be seen on a former flag of Hungary and the current flag of Slovakia?", "answer": ["The Christian cross and the starry cross"], "scores": [[0.04563493654131889, 0.009909371845424175, 0.7059184312820435, 0.0250992551445961, 0.8199646472930908, 0.009270944632589817, 0.015038534998893738, 0.889680027961731]], "normalized_score": [0.07309534400701523], "tokens": [["\u0120The", "\u0120Christian", "\u0120cross", "\u0120and", "\u0120the", "\u0120star", "ry", "\u0120cross"]], "entropy": [[6.314426898956299, 6.097118377685547, 1.3121064901351929, 1.1750195026397705, 1.3042824268341064, 5.818690299987793, 1.6862287521362305, 0.8693054914474487]], "avg_entropy": [3.0721472799777985], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Two-barred cross", "score_of_biggest_cluster": 0.07309534400701523, "score_of_first_answer": 0.07309534400701523, "max_score_of_first_answer": 0.889680027961731, "min_score_of_first_answer": 0.009270944632589817, "avg_score": 0.07309534400701523, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.38262106296858206, "std_of_first_answer": 0.38262106296858206, "avg_entropy_of_first_answer": 3.0721472799777985, "lex_sim": 1.0}
{"question": "What is the name of a bishop's staff that is shaped like a shepherd's crook?", "answer": ["Pastoral Staff"], "scores": [[0.015087826177477837, 0.961230993270874, 0.14244046807289124]], "normalized_score": [0.12735886871814728], "tokens": [["\u0120Past", "oral", "\u0120Staff"]], "entropy": [[6.0477399826049805, 0.29982689023017883, 1.2719295024871826]], "avg_entropy": [2.539832125107447], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pateritsa", "score_of_biggest_cluster": 0.12735886871814728, "score_of_first_answer": 0.12735886871814728, "max_score_of_first_answer": 0.961230993270874, "min_score_of_first_answer": 0.015087826177477837, "avg_score": 0.12735886871814728, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4192352180113546, "std_of_first_answer": 0.4192352180113546, "avg_entropy_of_first_answer": 2.539832125107447, "lex_sim": 1.0}
{"question": "Famous as a presenter on TV-am with Anne Diamond, who is the chairman of Luton Town FC?", "answer": ["John White"], "scores": [[0.03715546429157257, 0.002070946153253317]], "normalized_score": [0.008771942928433418], "tokens": [["\u0120John", "\u0120White"]], "entropy": [[5.7653656005859375, 5.72819709777832]], "avg_entropy": [5.746781349182129], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nick Owen", "score_of_biggest_cluster": 0.008771942928433418, "score_of_first_answer": 0.008771942928433418, "max_score_of_first_answer": 0.03715546429157257, "min_score_of_first_answer": 0.002070946153253317, "avg_score": 0.008771942928433418, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.017542259069159627, "std_of_first_answer": 0.017542259069159627, "avg_entropy_of_first_answer": 5.746781349182129, "lex_sim": 1.0}
{"question": "What is the name of the Sergeant who constantly harasses the goldbricking Private Beetle Bailey in the comic strip of the same name?", "answer": ["Loomis"], "scores": [[0.0026218846905976534, 0.018494432792067528, 0.9906489253044128]], "normalized_score": [0.03635171428322792], "tokens": [["\u0120L", "oom", "is"]], "entropy": [[6.33149528503418, 4.302115440368652, 0.07420139014720917]], "avg_entropy": [3.569270705183347], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Snorkle", "score_of_biggest_cluster": 0.03635171428322792, "score_of_first_answer": 0.03635171428322792, "max_score_of_first_answer": 0.9906489253044128, "min_score_of_first_answer": 0.0026218846905976534, "avg_score": 0.03635171428322792, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46206465734176483, "std_of_first_answer": 0.46206465734176483, "avg_entropy_of_first_answer": 3.569270705183347, "lex_sim": 1.0}
{"question": "Tomorrow marks the anniversary of the debut of what long running children's television series, originally produced by the Children's Television Workshop?", "answer": ["Sesame Street"], "scores": [[0.421199232339859, 0.9824525117874146, 0.9738725423812866]], "normalized_score": [0.7386415600776672], "tokens": [["\u0120S", "esame", "\u0120Street"]], "entropy": [[3.1969876289367676, 0.1643630713224411, 0.23009991645812988]], "avg_entropy": [1.1971502055724461], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Furry Arms", "score_of_biggest_cluster": 0.7386415600776672, "score_of_first_answer": 0.7386415600776672, "max_score_of_first_answer": 0.9824525117874146, "min_score_of_first_answer": 0.421199232339859, "avg_score": 0.7386415600776672, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26257837923979827, "std_of_first_answer": 0.26257837923979827, "avg_entropy_of_first_answer": 1.1971502055724461, "lex_sim": 1.0}
{"question": "Where, specifically, is the junction known as 'the Crossroads of the World'?", "answer": ["London"], "scores": [[0.025427907705307007]], "normalized_score": [0.025427909567952156], "tokens": [["\u0120London"]], "entropy": [[6.71372127532959]], "avg_entropy": [6.71372127532959], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Time Square", "score_of_biggest_cluster": 0.025427909567952156, "score_of_first_answer": 0.025427909567952156, "max_score_of_first_answer": 0.025427907705307007, "min_score_of_first_answer": 0.025427907705307007, "avg_score": 0.025427909567952156, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.71372127532959, "lex_sim": 1.0}
{"question": "Who played Frankenstein in the 1994 film of the same name?", "answer": ["Kevin Spacey"], "scores": [[0.02150893025100231, 0.12395688891410828, 0.9969737529754639]], "normalized_score": [0.13852378726005554], "tokens": [["\u0120Kevin", "\u0120Space", "y"]], "entropy": [[5.2071533203125, 3.1967296600341797, 0.03446871042251587]], "avg_entropy": [2.812783896923065], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kenneth Charles Branagh", "score_of_biggest_cluster": 0.13852378726005554, "score_of_first_answer": 0.13852378726005554, "max_score_of_first_answer": 0.9969737529754639, "min_score_of_first_answer": 0.02150893025100231, "avg_score": 0.13852378726005554, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4376941663793499, "std_of_first_answer": 0.4376941663793499, "avg_entropy_of_first_answer": 2.812783896923065, "lex_sim": 1.0}
{"question": "In computers, the X-Y Position Indicator for a Display System is now known as what?", "answer": ["XY Plotter"], "scores": [[0.0379943773150444, 0.24494440853595734, 0.8634931445121765]], "normalized_score": [0.2003004401922226], "tokens": [["\u0120XY", "\u0120Plot", "ter"]], "entropy": [[4.800116539001465, 4.495399475097656, 0.642874538898468]], "avg_entropy": [3.3127968509991965], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cursor", "score_of_biggest_cluster": 0.2003004401922226, "score_of_first_answer": 0.2003004401922226, "max_score_of_first_answer": 0.8634931445121765, "min_score_of_first_answer": 0.0379943773150444, "avg_score": 0.2003004401922226, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3506944014450473, "std_of_first_answer": 0.3506944014450473, "avg_entropy_of_first_answer": 3.3127968509991965, "lex_sim": 1.0}
{"question": "What is the leather finger protection used by archers called", "answer": ["\u32ca"], "scores": [[0.004161314573138952, 4.2825573473237455e-05, 0.0262551661580801, 0.011257058940827847]], "normalized_score": [0.0026939748786389828], "tokens": [["\u0120", "\u00e3", "\u012d", "\u012c"]], "entropy": [[6.020985126495361, 5.803642272949219, 2.9082252979278564, 3.807555913925171]], "avg_entropy": [4.635102152824402], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tab X-Tra", "score_of_biggest_cluster": 0.0026939748786389828, "score_of_first_answer": 0.0026939748786389828, "max_score_of_first_answer": 0.0262551661580801, "min_score_of_first_answer": 4.2825573473237455e-05, "avg_score": 0.0026939748786389828, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.009978849090296166, "std_of_first_answer": 0.009978849090296166, "avg_entropy_of_first_answer": 4.635102152824402, "lex_sim": 0.0}
{"question": "\"Which author famously upset her publisher by beginning a book with \"\"I am quite tired of making goody goody books about nice people\"\"?\"", "answer": ["Mary Eleanor Coelia"], "scores": [[0.034676384180784225, 0.01950583979487419, 0.008336212486028671, 0.016164161264896393]], "normalized_score": [0.017375214025378227], "tokens": [["\u0120Mary", "\u0120Eleanor", "\u0120Co", "elia"]], "entropy": [[5.218478202819824, 4.43150520324707, 5.284079551696777, 3.3881688117980957]], "avg_entropy": [4.580557942390442], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Beatrix Potter bibliography", "score_of_biggest_cluster": 0.017375214025378227, "score_of_first_answer": 0.017375214025378227, "max_score_of_first_answer": 0.034676384180784225, "min_score_of_first_answer": 0.008336212486028671, "avg_score": 0.017375214025378227, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.00956509484683238, "std_of_first_answer": 0.00956509484683238, "avg_entropy_of_first_answer": 4.580557942390442, "lex_sim": 1.0}
{"question": "Who killed Robert F Kennedy?", "answer": ["Sirhan Sirhan"], "scores": [[0.7742632627487183, 0.927155613899231, 0.9291623830795288, 0.9913045167922974]], "normalized_score": [0.9017475843429565], "tokens": [["\u0120Sir", "han", "\u0120Sir", "han"]], "entropy": [[1.667049527168274, 0.6394392251968384, 0.422171413898468, 0.08019709587097168]], "avg_entropy": [0.702214315533638], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sir Han Sir Han", "score_of_biggest_cluster": 0.9017475843429565, "score_of_first_answer": 0.9017475843429565, "max_score_of_first_answer": 0.9913045167922974, "min_score_of_first_answer": 0.7742632627487183, "avg_score": 0.9017475843429565, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08002244533711414, "std_of_first_answer": 0.08002244533711414, "avg_entropy_of_first_answer": 0.702214315533638, "lex_sim": 1.0}
{"question": "What is the state capital of New South Wales, Australia?", "answer": ["Sydney"], "scores": [[0.94844651222229]], "normalized_score": [0.94844651222229], "tokens": [["\u0120Sydney"]], "entropy": [[0.38517341017723083]], "avg_entropy": [0.38517341017723083], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sydney, New South Wales, Australia", "score_of_biggest_cluster": 0.94844651222229, "score_of_first_answer": 0.94844651222229, "max_score_of_first_answer": 0.94844651222229, "min_score_of_first_answer": 0.94844651222229, "avg_score": 0.94844651222229, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.38517341017723083, "lex_sim": 1.0}
{"question": "What is the official residence of the Lord Mayor of London ?", "answer": ["Blackfriars"], "scores": [[0.0024258585181087255, 0.7588034868240356, 0.9936415553092957, 0.9843010306358337]], "normalized_score": [0.20598621666431427], "tokens": [["\u0120Black", "f", "ri", "ars"]], "entropy": [[2.447340488433838, 1.59775710105896, 0.05661677569150925, 0.08880685269832611]], "avg_entropy": [1.0476303044706583], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mansion House (disambiguation)", "score_of_biggest_cluster": 0.20598621666431427, "score_of_first_answer": 0.20598621666431427, "max_score_of_first_answer": 0.9936415553092957, "min_score_of_first_answer": 0.0024258585181087255, "avg_score": 0.20598621666431427, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40502930710816826, "std_of_first_answer": 0.40502930710816826, "avg_entropy_of_first_answer": 1.0476303044706583, "lex_sim": 1.0}
{"question": "What was the 42nd state to join the Union on Nov 11, 1889?", "answer": ["Dakota"], "scores": [[0.012412693351507187]], "normalized_score": [0.012412693351507187], "tokens": [["\u0120Dakota"]], "entropy": [[4.103262901306152]], "avg_entropy": [4.103262901306152], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Washington (Amtrak station)", "score_of_biggest_cluster": 0.012412693351507187, "score_of_first_answer": 0.012412693351507187, "max_score_of_first_answer": 0.012412693351507187, "min_score_of_first_answer": 0.012412693351507187, "avg_score": 0.012412693351507187, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.103262901306152, "lex_sim": 1.0}
{"question": "What is the title of Tony Blair's autobiography/memoir published in 2010, which covered events from 1994 to his resignation as Prime Minister?", "answer": ["An Unlikely Union: Love Undetected"], "scores": [[0.004241880960762501, 0.0155559116974473, 0.9875574111938477, 0.4869208335876465, 0.08015484362840652, 0.04960952699184418, 0.00010022724745795131, 0.7980649471282959, 0.7152078151702881]], "normalized_score": [0.05781589448451996], "tokens": [["\u0120An", "\u0120Unlike", "ly", "\u0120Union", ":", "\u0120Love", "\u0120Und", "et", "ected"]], "entropy": [[3.53987717628479, 4.590359687805176, 0.08062382787466049, 3.2650821208953857, 0.8740337491035461, 1.3337645530700684, 0.9864086508750916, 1.0533363819122314, 0.9228957891464233]], "avg_entropy": [1.8495979929963748], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A Journey", "score_of_biggest_cluster": 0.05781589448451996, "score_of_first_answer": 0.05781589448451996, "max_score_of_first_answer": 0.9875574111938477, "min_score_of_first_answer": 0.00010022724745795131, "avg_score": 0.05781589448451996, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37656788241284656, "std_of_first_answer": 0.37656788241284656, "avg_entropy_of_first_answer": 1.8495979929963748, "lex_sim": 1.0}
{"question": "Which media mogul married actress Jane Fonda in 1991, the marriage ending in divorce ten years later?", "answer": ["Rupert Murdoch"], "scores": [[0.047518063336610794, 0.9605222940444946]], "normalized_score": [0.2136402428150177], "tokens": [["\u0120Rupert", "\u0120Murdoch"]], "entropy": [[4.534074783325195, 0.3089677095413208]], "avg_entropy": [2.421521246433258], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "TED TURNER", "score_of_biggest_cluster": 0.2136402428150177, "score_of_first_answer": 0.2136402428150177, "max_score_of_first_answer": 0.9605222940444946, "min_score_of_first_answer": 0.047518063336610794, "avg_score": 0.2136402428150177, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4565021153539419, "std_of_first_answer": 0.4565021153539419, "avg_entropy_of_first_answer": 2.421521246433258, "lex_sim": 1.0}
{"question": "Which mediaeval author wrote The Canterbury Tales", "answer": ["Geoffrey Chaucer"], "scores": [[0.7809818387031555, 0.9648078680038452, 0.9971873164176941]], "normalized_score": [0.9091164469718933], "tokens": [["\u0120Geoffrey", "\u0120Chau", "cer"]], "entropy": [[0.9835423231124878, 0.2478901594877243, 0.030136369168758392]], "avg_entropy": [0.42052295058965683], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chausseur", "score_of_biggest_cluster": 0.9091164469718933, "score_of_first_answer": 0.9091164469718933, "max_score_of_first_answer": 0.9971873164176941, "min_score_of_first_answer": 0.7809818387031555, "avg_score": 0.9091164469718933, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09521043729072223, "std_of_first_answer": 0.09521043729072223, "avg_entropy_of_first_answer": 0.42052295058965683, "lex_sim": 1.0}
{"question": "In cooking Florentine means garnished with which vegetable?", "answer": ["Romanesco"], "scores": [[0.005517198238521814, 0.31154975295066833, 0.9867441654205322]], "normalized_score": [0.119256891310215], "tokens": [["\u0120Rom", "anes", "co"]], "entropy": [[5.226475715637207, 1.6510465145111084, 0.10169948637485504]], "avg_entropy": [2.326407238841057], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paalak", "score_of_biggest_cluster": 0.119256891310215, "score_of_first_answer": 0.119256891310215, "max_score_of_first_answer": 0.9867441654205322, "min_score_of_first_answer": 0.005517198238521814, "avg_score": 0.119256891310215, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40992543770508827, "std_of_first_answer": 0.40992543770508827, "avg_entropy_of_first_answer": 2.326407238841057, "lex_sim": 1.0}
{"question": "Which 1960 film is based on the 1954 film \u2018Seven Samurai\u2019?", "answer": ["Kurosawa"], "scores": [[0.007052589673548937, 0.8694995045661926, 0.9981694221496582]], "normalized_score": [0.18292538821697235], "tokens": [["\u0120Kuro", "s", "awa"]], "entropy": [[4.580752372741699, 0.9197878837585449, 0.02037026546895504]], "avg_entropy": [1.8403035073230665], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bernardo O'Reilly (Character)", "score_of_biggest_cluster": 0.18292538821697235, "score_of_first_answer": 0.18292538821697235, "max_score_of_first_answer": 0.9981694221496582, "min_score_of_first_answer": 0.007052589673548937, "avg_score": 0.18292538821697235, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44003575666785316, "std_of_first_answer": 0.44003575666785316, "avg_entropy_of_first_answer": 1.8403035073230665, "lex_sim": 1.0}
{"question": "Which island was home to Odysseus?", "answer": ["Scyros"], "scores": [[0.008716769516468048, 0.5861169695854187]], "normalized_score": [0.07147759199142456], "tokens": [["\u0120Scy", "ros"]], "entropy": [[4.150938510894775, 0.9524968266487122]], "avg_entropy": [2.5517176687717438], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ithake", "score_of_biggest_cluster": 0.07147759199142456, "score_of_first_answer": 0.07147759199142456, "max_score_of_first_answer": 0.5861169695854187, "min_score_of_first_answer": 0.008716769516468048, "avg_score": 0.07147759199142456, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2887001000344753, "std_of_first_answer": 0.2887001000344753, "avg_entropy_of_first_answer": 2.5517176687717438, "lex_sim": 1.0}
{"question": "What railway line is green on the traditional map of the London Underground (Tube)?", "answer": ["Circle"], "scores": [[0.03670195862650871]], "normalized_score": [0.03670195862650871], "tokens": [["\u0120Circle"]], "entropy": [[3.4862842559814453]], "avg_entropy": [3.4862842559814453], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "District line", "score_of_biggest_cluster": 0.03670195862650871, "score_of_first_answer": 0.03670195862650871, "max_score_of_first_answer": 0.03670195862650871, "min_score_of_first_answer": 0.03670195862650871, "avg_score": 0.03670195862650871, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.4862842559814453, "lex_sim": 1.0}
{"question": "Which element is integral to all organic compounds?", "answer": ["Carbon"], "scores": [[0.40777650475502014]], "normalized_score": [0.40777647495269775], "tokens": [["\u0120Carbon"]], "entropy": [[2.530348062515259]], "avg_entropy": [2.530348062515259], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Carbon atom", "score_of_biggest_cluster": 0.40777647495269775, "score_of_first_answer": 0.40777647495269775, "max_score_of_first_answer": 0.40777650475502014, "min_score_of_first_answer": 0.40777650475502014, "avg_score": 0.40777647495269775, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.530348062515259, "lex_sim": 1.0}
{"question": "\"Sportscaster Jack Brickhouse who once said \"\"Any team can have a bad century\"\" provided play-by-play coverage of what American major league team's games from 1948 to 1981?\"", "answer": ["Seattle Seawolves"], "scores": [[0.004926807247102261, 0.005803884472697973, 0.38448360562324524, 0.8737693428993225]], "normalized_score": [0.05567236989736557], "tokens": [["\u0120Seattle", "\u0120Se", "aw", "olves"]], "entropy": [[3.5750582218170166, 1.3663406372070312, 2.367504119873047, 0.7053790092468262]], "avg_entropy": [2.0035704970359802], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chicaco Cubs", "score_of_biggest_cluster": 0.05567236989736557, "score_of_first_answer": 0.05567236989736557, "max_score_of_first_answer": 0.8737693428993225, "min_score_of_first_answer": 0.004926807247102261, "avg_score": 0.05567236989736557, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35664358829498627, "std_of_first_answer": 0.35664358829498627, "avg_entropy_of_first_answer": 2.0035704970359802, "lex_sim": 1.0}
{"question": "April 18, 1942 saw the first retaliation against Japan for the attack on Pearl Harbor when what famed American aviator led a flight of B25 bombers from the deck of the USS Hornet?", "answer": ["George H.W."], "scores": [[0.009298492223024368, 0.1723354309797287, 0.9211705327033997, 0.421639084815979, 0.9860721230506897, 0.9342520236968994]], "normalized_score": [0.28823065757751465], "tokens": [["\u0120George", "\u0120H", ".", "W", ".", "\u0120Bush"]], "entropy": [[4.500839710235596, 4.370244979858398, 0.674175500869751, 2.171034336090088, 0.1342678666114807, 0.5614474415779114]], "avg_entropy": [2.068668305873871], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "James Doolittle", "score_of_biggest_cluster": 0.28823065757751465, "score_of_first_answer": 0.28823065757751465, "max_score_of_first_answer": 0.9860721230506897, "min_score_of_first_answer": 0.009298492223024368, "avg_score": 0.28823065757751465, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3923324064901917, "std_of_first_answer": 0.3923324064901917, "avg_entropy_of_first_answer": 2.068668305873871, "lex_sim": 1.0}
{"question": "The name of which person born in Dublin is connected in English history with the locations of Vimiero, Talavera, Salamanca, Vittoria, Orthes and Waterloo?", "answer": ["George Spencer, 3rd Duke of Marlbro'"], "scores": [[0.03566847741603851, 0.012747613713145256, 0.23715290427207947, 0.1499481499195099, 0.968458890914917, 0.8164739608764648, 0.9874709248542786, 0.9799997806549072, 0.9717492461204529, 0.002884897170588374, 0.46190619468688965]], "normalized_score": [0.19558513164520264], "tokens": [["\u0120George", "\u0120Spencer", ",", "\u01203", "rd", "\u0120Duke", "\u0120of", "\u0120Mar", "l", "bro", "'"]], "entropy": [[4.784104347229004, 5.59599494934082, 2.6478078365325928, 3.379793167114258, 0.23682832717895508, 0.5559999942779541, 0.09289991855621338, 0.16552656888961792, 0.24013733863830566, 0.17543752491474152, 2.4226436614990234]], "avg_entropy": [1.8451976031064987], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "First Duke of Wellington", "score_of_biggest_cluster": 0.19558513164520264, "score_of_first_answer": 0.19558513164520264, "max_score_of_first_answer": 0.9874709248542786, "min_score_of_first_answer": 0.002884897170588374, "avg_score": 0.19558513164520264, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41582277906538134, "std_of_first_answer": 0.41582277906538134, "avg_entropy_of_first_answer": 1.8451976031064987, "lex_sim": 1.0}
{"question": "What is the most populated city in America?", "answer": ["New York City"], "scores": [[0.5741311311721802, 0.8007777333259583, 0.21811014413833618]], "normalized_score": [0.46458619832992554], "tokens": [["\u0120New", "\u0120York", "\u0120City"]], "entropy": [[2.254779577255249, 0.7489240169525146, 1.2228288650512695]], "avg_entropy": [1.4088441530863445], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "New Yourk State", "score_of_biggest_cluster": 0.46458619832992554, "score_of_first_answer": 0.46458619832992554, "max_score_of_first_answer": 0.8007777333259583, "min_score_of_first_answer": 0.21811014413833618, "avg_score": 0.46458619832992554, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23981964171391637, "std_of_first_answer": 0.23981964171391637, "avg_entropy_of_first_answer": 1.4088441530863445, "lex_sim": 1.0}
{"question": "During the Falklands War which ship was the British flagship?", "answer": ["HMS Hermes(Royal Navy)"], "scores": [[0.2901844382286072, 0.6364257335662842, 0.00012616575986612588, 0.009105654433369637, 0.6899076104164124, 0.7769880294799805]], "normalized_score": [0.06960597634315491], "tokens": [["\u0120HMS", "\u0120Hermes", "(", "Royal", "\u0120Navy", ")"]], "entropy": [[4.2025861740112305, 2.4138073921203613, 0.42892563343048096, 6.742175579071045, 1.8704845905303955, 1.6715320348739624]], "avg_entropy": [2.8882519006729126], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "HMS Hermes", "score_of_biggest_cluster": 0.06960597634315491, "score_of_first_answer": 0.06960597634315491, "max_score_of_first_answer": 0.7769880294799805, "min_score_of_first_answer": 0.00012616575986612588, "avg_score": 0.06960597634315491, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3180197073686951, "std_of_first_answer": 0.3180197073686951, "avg_entropy_of_first_answer": 2.8882519006729126, "lex_sim": 1.0}
{"question": "In which year was the Dambuster's raid", "answer": ["1943"], "scores": [[0.15576393902301788]], "normalized_score": [0.15576393902301788], "tokens": [["\u01201943"]], "entropy": [[4.030750751495361]], "avg_entropy": [4.030750751495361], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and forty-three", "score_of_biggest_cluster": 0.15576393902301788, "score_of_first_answer": 0.15576393902301788, "max_score_of_first_answer": 0.15576393902301788, "min_score_of_first_answer": 0.15576393902301788, "avg_score": 0.15576393902301788, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.030750751495361, "lex_sim": 1.0}
{"question": "From which film, for which Chaplin film wrote the music, did Petula Clark have a Number One with 'This Is My Song'?", "answer": ["A-Film (1942, The Kid)"], "scores": [[0.033741045743227005, 0.0221527349203825, 0.0018678064225241542, 0.03132490813732147, 0.25246474146842957, 0.03997553139925003, 0.01783667504787445, 0.051639165729284286, 0.2517242729663849, 0.5959469676017761]], "normalized_score": [0.04770048335194588], "tokens": [["\u0120A", "-", "Film", "\u0120(", "19", "42", ",", "\u0120The", "\u0120Kid", ")"]], "entropy": [[4.545402526855469, 5.958304405212402, 5.979735374450684, 3.7630250453948975, 5.009586334228516, 3.4908711910247803, 0.9496726393699646, 6.083866119384766, 3.266082525253296, 2.1015501022338867]], "avg_entropy": [4.1148096263408664], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A Countess from Hong Kong", "score_of_biggest_cluster": 0.04770048335194588, "score_of_first_answer": 0.04770048335194588, "max_score_of_first_answer": 0.5959469676017761, "min_score_of_first_answer": 0.0018678064225241542, "avg_score": 0.04770048335194588, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17910826354065684, "std_of_first_answer": 0.17910826354065684, "avg_entropy_of_first_answer": 4.1148096263408664, "lex_sim": 1.0}
{"question": "'Agar Agar' is a vegetarian alternative to what?", "answer": ["SUGAR"], "scores": [[0.007668627426028252, 0.001965711824595928, 0.8738336563110352]], "normalized_score": [0.023616863414645195], "tokens": [["\u0120S", "UG", "AR"]], "entropy": [[5.591997146606445, 1.6826756000518799, 0.7956538200378418]], "avg_entropy": [2.690108855565389], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jeletin", "score_of_biggest_cluster": 0.023616863414645195, "score_of_first_answer": 0.023616863414645195, "max_score_of_first_answer": 0.8738336563110352, "min_score_of_first_answer": 0.001965711824595928, "avg_score": 0.023616863414645195, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40966491634189833, "std_of_first_answer": 0.40966491634189833, "avg_entropy_of_first_answer": 2.690108855565389, "lex_sim": 1.0}
{"question": "Which country used to be called Southern Rhodesia?", "answer": ["Rhodesia"], "scores": [[0.14613208174705505, 0.9859835505485535]], "normalized_score": [0.37958377599716187], "tokens": [["\u0120Rhodes", "ia"]], "entropy": [[3.4995689392089844, 0.11775120347738266]], "avg_entropy": [1.8086600713431835], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Zimbobwe", "score_of_biggest_cluster": 0.37958377599716187, "score_of_first_answer": 0.37958377599716187, "max_score_of_first_answer": 0.9859835505485535, "min_score_of_first_answer": 0.14613208174705505, "avg_score": 0.37958377599716187, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4199257344007492, "std_of_first_answer": 0.4199257344007492, "avg_entropy_of_first_answer": 1.8086600713431835, "lex_sim": 1.0}
{"question": "US professional wrestler and actor Terry Gene Bollea is better known by what name?", "answer": ["Ted"], "scores": [[0.0006148765096440911]], "normalized_score": [0.0006148764514364302], "tokens": [["\u0120Ted"]], "entropy": [[2.2741076946258545]], "avg_entropy": [2.2741076946258545], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Terrance Bollea", "score_of_biggest_cluster": 0.0006148764514364302, "score_of_first_answer": 0.0006148764514364302, "max_score_of_first_answer": 0.0006148765096440911, "min_score_of_first_answer": 0.0006148765096440911, "avg_score": 0.0006148764514364302, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.2741076946258545, "lex_sim": 1.0}
{"question": "In \u2018Dad\u2019s Army\u2019, what was the surname of the air-raid warden?", "answer": ["Jones"], "scores": [[0.06758201867341995]], "normalized_score": [0.06758201122283936], "tokens": [["\u0120Jones"]], "entropy": [[6.623560905456543]], "avg_entropy": [6.623560905456543], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hodges (disambiguation)", "score_of_biggest_cluster": 0.06758201122283936, "score_of_first_answer": 0.06758201122283936, "max_score_of_first_answer": 0.06758201867341995, "min_score_of_first_answer": 0.06758201867341995, "avg_score": 0.06758201122283936, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.623560905456543, "lex_sim": 1.0}
{"question": "What singer-songwriter promised to write albums for all 50 states, but only completed Michigan and Illinois?", "answer": ["Bob Dylan"], "scores": [[0.06052326038479805, 0.7332165241241455]], "normalized_score": [0.21065767109394073], "tokens": [["\u0120Bob", "\u0120Dylan"]], "entropy": [[5.8569769859313965, 1.5326206684112549]], "avg_entropy": [3.6947988271713257], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sufjan Stenves", "score_of_biggest_cluster": 0.21065767109394073, "score_of_first_answer": 0.21065767109394073, "max_score_of_first_answer": 0.7332165241241455, "min_score_of_first_answer": 0.06052326038479805, "avg_score": 0.21065767109394073, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33634663186967373, "std_of_first_answer": 0.33634663186967373, "avg_entropy_of_first_answer": 3.6947988271713257, "lex_sim": 1.0}
{"question": "What trophy is played for at the US Superbowl?", "answer": ["The Vince Lombardi Trophy"], "scores": [[0.2854737341403961, 0.7733506560325623, 0.9992831349372864, 0.9752158522605896, 0.6094693541526794]], "normalized_score": [0.6660959124565125], "tokens": [["\u0120The", "\u0120Vince", "\u0120Lomb", "ardi", "\u0120Trophy"]], "entropy": [[3.6180500984191895, 1.4928247928619385, 0.009635856375098228, 0.15017305314540863, 1.2559213638305664]], "avg_entropy": [1.3053210329264402], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "World Championship Game Trophy", "score_of_biggest_cluster": 0.6660959124565125, "score_of_first_answer": 0.6660959124565125, "max_score_of_first_answer": 0.9992831349372864, "min_score_of_first_answer": 0.2854737341403961, "avg_score": 0.6660959124565125, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26330375957197066, "std_of_first_answer": 0.26330375957197066, "avg_entropy_of_first_answer": 1.3053210329264402, "lex_sim": 1.0}
{"question": "Michael J Fox starred as Marty McFly in which series of films?", "answer": ["Back to the Future series"], "scores": [[0.9475810527801514, 0.8673809766769409, 0.9762143492698669, 0.7634408473968506, 0.02564636804163456]], "normalized_score": [0.4357471466064453], "tokens": [["\u0120Back", "\u0120to", "\u0120the", "\u0120Future", "\u0120series"]], "entropy": [[0.4149976074695587, 0.4318332374095917, 0.1505223512649536, 0.6003656387329102, 1.8611239194869995]], "avg_entropy": [0.6917685508728028], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marvin Berry & the Starlighters", "score_of_biggest_cluster": 0.4357471466064453, "score_of_first_answer": 0.4357471466064453, "max_score_of_first_answer": 0.9762143492698669, "min_score_of_first_answer": 0.02564636804163456, "avg_score": 0.4357471466064453, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3530145178135961, "std_of_first_answer": 0.3530145178135961, "avg_entropy_of_first_answer": 0.6917685508728028, "lex_sim": 1.0}
{"question": "Who was the first Briton to hold a world javelin record", "answer": ["Chris Broad"], "scores": [[0.004247906152158976, 0.0029020095244050026]], "normalized_score": [0.003511049784719944], "tokens": [["\u0120Chris", "\u0120Broad"]], "entropy": [[6.074178695678711, 5.828428268432617]], "avg_entropy": [5.951303482055664], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fatima Whitbread", "score_of_biggest_cluster": 0.003511049784719944, "score_of_first_answer": 0.003511049784719944, "max_score_of_first_answer": 0.004247906152158976, "min_score_of_first_answer": 0.0029020095244050026, "avg_score": 0.003511049784719944, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0006729483138769865, "std_of_first_answer": 0.0006729483138769865, "avg_entropy_of_first_answer": 5.951303482055664, "lex_sim": 1.0}
{"question": "Leonidas was the king of what city-state?", "answer": ["Sparta"], "scores": [[0.9667714238166809, 0.995969295501709]], "normalized_score": [0.9812617897987366], "tokens": [["\u0120Spart", "a"]], "entropy": [[0.29405540227890015, 0.045309364795684814]], "avg_entropy": [0.16968238353729248], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lacedaemon", "score_of_biggest_cluster": 0.9812617897987366, "score_of_first_answer": 0.9812617897987366, "max_score_of_first_answer": 0.995969295501709, "min_score_of_first_answer": 0.9667714238166809, "avg_score": 0.9812617897987366, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.014598935842514038, "std_of_first_answer": 0.014598935842514038, "avg_entropy_of_first_answer": 0.16968238353729248, "lex_sim": 1.0}
{"question": "Which actress played The Sculptress on TV?", "answer": ["Charlotte Gainsbourg"], "scores": [[0.00463933777064085, 0.0070146797224879265, 0.9815748929977417, 0.9269777536392212]], "normalized_score": [0.07376735657453537], "tokens": [["\u0120Charlotte", "\u0120G", "ains", "bourg"]], "entropy": [[6.273654460906982, 2.6145753860473633, 0.18232715129852295, 0.318012535572052]], "avg_entropy": [2.34714238345623], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "PAULINE QUIRKE", "score_of_biggest_cluster": 0.07376735657453537, "score_of_first_answer": 0.07376735657453537, "max_score_of_first_answer": 0.9815748929977417, "min_score_of_first_answer": 0.00463933777064085, "avg_score": 0.07376735657453537, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.47461809574901914, "std_of_first_answer": 0.47461809574901914, "avg_entropy_of_first_answer": 2.34714238345623, "lex_sim": 1.0}
{"question": "Which Rugby League team plays home games at Derwent Park?", "answer": ["Parramatta Eels"], "scores": [[0.005595198832452297, 0.9924256801605225, 0.9983308911323547, 0.47569260001182556, 0.9964239001274109]], "normalized_score": [0.30472758412361145], "tokens": [["\u0120Par", "ram", "atta", "\u0120E", "els"]], "entropy": [[5.572507858276367, 0.08351452648639679, 0.016290251165628433, 1.7279644012451172, 0.02919064462184906]], "avg_entropy": [1.4858935363590717], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Workington Town", "score_of_biggest_cluster": 0.30472758412361145, "score_of_first_answer": 0.30472758412361145, "max_score_of_first_answer": 0.9983308911323547, "min_score_of_first_answer": 0.005595198832452297, "avg_score": 0.30472758412361145, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39867134714345576, "std_of_first_answer": 0.39867134714345576, "avg_entropy_of_first_answer": 1.4858935363590717, "lex_sim": 1.0}
{"question": "Feugo, Yangin, Brand, and Pozar are Spanish, Turkish, Dutch and Polish for?", "answer": ["a b c"], "scores": [[0.006388582289218903, 0.0025200850795954466, 0.7728559374809265]], "normalized_score": [0.023172494024038315], "tokens": [["\u0120a", "\u0120b", "\u0120c"]], "entropy": [[8.092988967895508, 7.359739303588867, 1.7663755416870117]], "avg_entropy": [5.739701271057129], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Open flame", "score_of_biggest_cluster": 0.023172494024038315, "score_of_first_answer": 0.023172494024038315, "max_score_of_first_answer": 0.7728559374809265, "min_score_of_first_answer": 0.0025200850795954466, "avg_score": 0.023172494024038315, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3622314326719433, "std_of_first_answer": 0.3622314326719433, "avg_entropy_of_first_answer": 5.739701271057129, "lex_sim": 1.0}
{"question": "According to Greek legend, who was the wife of Odysseus?", "answer": ["Antigone"], "scores": [[0.06700379401445389, 0.03397096320986748, 0.9904984831809998]], "normalized_score": [0.13112546503543854], "tokens": [["\u0120Ant", "ig", "one"]], "entropy": [[2.7921314239501953, 1.1329281330108643, 0.09072597324848175]], "avg_entropy": [1.3385951767365138], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "PENELOPE", "score_of_biggest_cluster": 0.13112546503543854, "score_of_first_answer": 0.13112546503543854, "max_score_of_first_answer": 0.9904984831809998, "min_score_of_first_answer": 0.03397096320986748, "avg_score": 0.13112546503543854, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44333063975270753, "std_of_first_answer": 0.44333063975270753, "avg_entropy_of_first_answer": 1.3385951767365138, "lex_sim": 1.0}
{"question": "Which act has had more Christmas number one singles than any other?", "answer": ["Wham!"], "scores": [[0.011424203403294086, 0.9350223541259766, 0.5257013440132141, 0.07033858448266983, 0.017248567193746567, 0.7082874178886414]], "normalized_score": [0.12999418377876282], "tokens": [["\u0120Wh", "am", "!", "\u0120(", "1", ")"]], "entropy": [[6.241026878356934, 0.3723157048225403, 2.2730627059936523, 2.673708200454712, 5.665499210357666, 1.835727334022522]], "avg_entropy": [3.1768900056680045], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fab 4", "score_of_biggest_cluster": 0.12999418377876282, "score_of_first_answer": 0.12999418377876282, "max_score_of_first_answer": 0.9350223541259766, "min_score_of_first_answer": 0.011424203403294086, "avg_score": 0.12999418377876282, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.365229226165145, "std_of_first_answer": 0.365229226165145, "avg_entropy_of_first_answer": 3.1768900056680045, "lex_sim": 1.0}
{"question": "In which present day British county was the first UK Women\u2019s Institute founded in 1915?", "answer": ["East Herts"], "scores": [[0.016522739082574844, 0.008744914084672928, 0.1926863193511963]], "normalized_score": [0.030308395624160767], "tokens": [["\u0120East", "\u0120Hert", "s"]], "entropy": [[4.837494850158691, 2.1946816444396973, 0.6369815468788147]], "avg_entropy": [2.5563860138257346], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ynys M\u00f4n (Anglesey)", "score_of_biggest_cluster": 0.030308395624160767, "score_of_first_answer": 0.030308395624160767, "max_score_of_first_answer": 0.1926863193511963, "min_score_of_first_answer": 0.008744914084672928, "avg_score": 0.030308395624160767, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08493693225380058, "std_of_first_answer": 0.08493693225380058, "avg_entropy_of_first_answer": 2.5563860138257346, "lex_sim": 1.0}
{"question": "In internet domain names what country is represented by the domain code '.se'?", "answer": ["Sweden"], "scores": [[0.9282985329627991]], "normalized_score": [0.9282985329627991], "tokens": [["\u0120Sweden"]], "entropy": [[0.5830883979797363]], "avg_entropy": [0.5830883979797363], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Swedish sin", "score_of_biggest_cluster": 0.9282985329627991, "score_of_first_answer": 0.9282985329627991, "max_score_of_first_answer": 0.9282985329627991, "min_score_of_first_answer": 0.9282985329627991, "avg_score": 0.9282985329627991, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.5830883979797363, "lex_sim": 1.0}
{"question": "Who wrote the 19th century novel \u2018Anna Karenina\u2019?", "answer": ["Leo Tolstoy"], "scores": [[0.8185557723045349, 0.9518642425537109, 0.9984384179115295, 0.9735924601554871]], "normalized_score": [0.932889997959137], "tokens": [["\u0120Leo", "\u0120Tol", "st", "oy"]], "entropy": [[0.9584028124809265, 0.3795207440853119, 0.01999571919441223, 0.1477125734090805]], "avg_entropy": [0.3764079622924328], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lyof Nikolaivitch Tolstoi", "score_of_biggest_cluster": 0.932889997959137, "score_of_first_answer": 0.932889997959137, "max_score_of_first_answer": 0.9984384179115295, "min_score_of_first_answer": 0.8185557723045349, "avg_score": 0.932889997959137, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06956286719479306, "std_of_first_answer": 0.06956286719479306, "avg_entropy_of_first_answer": 0.3764079622924328, "lex_sim": 1.0}
{"question": "\u2018Loitering With Intent\u2019 is a 1993 autobiography by which British actor?", "answer": ["Richard Hawley"], "scores": [[0.03821175545454025, 0.004046094603836536, 0.9775256514549255]], "normalized_score": [0.05326644703745842], "tokens": [["\u0120Richard", "\u0120Haw", "ley"]], "entropy": [[5.1475934982299805, 3.796677827835083, 0.17920413613319397]], "avg_entropy": [3.0411584873994193], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Peter o toole", "score_of_biggest_cluster": 0.05326644703745842, "score_of_first_answer": 0.05326644703745842, "max_score_of_first_answer": 0.9775256514549255, "min_score_of_first_answer": 0.004046094603836536, "avg_score": 0.05326644703745842, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45106544674283017, "std_of_first_answer": 0.45106544674283017, "avg_entropy_of_first_answer": 3.0411584873994193, "lex_sim": 1.0}
{"question": "A Cuba Libre is based on which spirit", "answer": ["Silver"], "scores": [[0.005692807026207447]], "normalized_score": [0.00569280656054616], "tokens": [["\u0120Silver"]], "entropy": [[2.8220839500427246]], "avg_entropy": [2.8220839500427246], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Spiced Rum", "score_of_biggest_cluster": 0.00569280656054616, "score_of_first_answer": 0.00569280656054616, "max_score_of_first_answer": 0.005692807026207447, "min_score_of_first_answer": 0.005692807026207447, "avg_score": 0.00569280656054616, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.8220839500427246, "lex_sim": 1.0}
{"question": "In 2009, who was convicted of the murder of Lana Clarkson?", "answer": ["Mark Hinton"], "scores": [[0.007953852415084839, 0.028875533491373062, 0.12608200311660767]], "normalized_score": [0.03070813976228237], "tokens": [["\u0120Mark", "\u0120H", "inton"]], "entropy": [[6.166170120239258, 5.522914886474609, 4.164793968200684]], "avg_entropy": [5.284626324971517], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Annette Records", "score_of_biggest_cluster": 0.03070813976228237, "score_of_first_answer": 0.03070813976228237, "max_score_of_first_answer": 0.12608200311660767, "min_score_of_first_answer": 0.007953852415084839, "avg_score": 0.03070813976228237, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05146851730720793, "std_of_first_answer": 0.05146851730720793, "avg_entropy_of_first_answer": 5.284626324971517, "lex_sim": 1.0}
{"question": "With 47 attacks in the year 2010 alone, what body of water is known as Pirate Alley?", "answer": ["Caribbean Sea off the coast of Cuba"], "scores": [[0.078602634370327, 0.6850550770759583, 0.0007346141501329839, 0.4548993706703186, 0.6716260313987732, 0.9919049739837646, 0.025683827698230743]], "normalized_score": [0.11742772161960602], "tokens": [["\u0120Caribbean", "\u0120Sea", "\u0120off", "\u0120the", "\u0120coast", "\u0120of", "\u0120Cuba"]], "entropy": [[4.653768539428711, 1.2217638492584229, 0.2705882489681244, 3.2529382705688477, 2.125551700592041, 0.08684396743774414, 4.10279655456543]], "avg_entropy": [2.2448930186884746], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gulf of Aden", "score_of_biggest_cluster": 0.11742772161960602, "score_of_first_answer": 0.11742772161960602, "max_score_of_first_answer": 0.9919049739837646, "min_score_of_first_answer": 0.0007346141501329839, "avg_score": 0.11742772161960602, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3604644417631365, "std_of_first_answer": 0.3604644417631365, "avg_entropy_of_first_answer": 2.2448930186884746, "lex_sim": 1.0}
{"question": "What was the title given to the daughters of the Kings of Spain and Portugal who were not the heir to the throne?", "answer": ["Basque"], "scores": [[0.0010263818548992276, 0.5949124097824097]], "normalized_score": [0.02471047267317772], "tokens": [["\u0120Bas", "que"]], "entropy": [[2.3986873626708984, 1.8178801536560059]], "avg_entropy": [2.108283758163452], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Infantes", "score_of_biggest_cluster": 0.02471047267317772, "score_of_first_answer": 0.02471047267317772, "max_score_of_first_answer": 0.5949124097824097, "min_score_of_first_answer": 0.0010263818548992276, "avg_score": 0.02471047267317772, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2969430139637552, "std_of_first_answer": 0.2969430139637552, "avg_entropy_of_first_answer": 2.108283758163452, "lex_sim": 1.0}
{"question": "On which island is the holiday resort of Faliraki?", "answer": ["Kos"], "scores": [[0.3407754600048065]], "normalized_score": [0.3407754600048065], "tokens": [["\u0120Kos"]], "entropy": [[3.430701494216919]], "avg_entropy": [3.430701494216919], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "RHODES", "score_of_biggest_cluster": 0.3407754600048065, "score_of_first_answer": 0.3407754600048065, "max_score_of_first_answer": 0.3407754600048065, "min_score_of_first_answer": 0.3407754600048065, "avg_score": 0.3407754600048065, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.430701494216919, "lex_sim": 1.0}
{"question": "In which 1955 film does Boccherini\u2019s String Quartet in E play a significant part?", "answer": ["Les Diaboliques"], "scores": [[0.005212814547121525, 0.40115007758140564, 0.9964882135391235, 0.9410402774810791]], "normalized_score": [0.2104334831237793], "tokens": [["\u0120Les", "\u0120Di", "abol", "iques"]], "entropy": [[5.571450233459473, 3.298511505126953, 0.031172633171081543, 0.2859383821487427]], "avg_entropy": [2.2967681884765625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Ladykillers", "score_of_biggest_cluster": 0.2104334831237793, "score_of_first_answer": 0.2104334831237793, "max_score_of_first_answer": 0.9964882135391235, "min_score_of_first_answer": 0.005212814547121525, "avg_score": 0.2104334831237793, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4080555776311035, "std_of_first_answer": 0.4080555776311035, "avg_entropy_of_first_answer": 2.2967681884765625, "lex_sim": 1.0}
{"question": "What is the first name of Irish singer Van Morrison?", "answer": ["Jim"], "scores": [[0.0029547158628702164]], "normalized_score": [0.00295471609570086], "tokens": [["\u0120Jim"]], "entropy": [[4.586946487426758]], "avg_entropy": [4.586946487426758], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gheevarghese", "score_of_biggest_cluster": 0.00295471609570086, "score_of_first_answer": 0.00295471609570086, "max_score_of_first_answer": 0.0029547158628702164, "min_score_of_first_answer": 0.0029547158628702164, "avg_score": 0.00295471609570086, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.586946487426758, "lex_sim": 1.0}
{"question": "Which English monarch was the son of Edward the Black Prince?", "answer": ["Edward VIII"], "scores": [[0.37539851665496826, 0.0001492045121267438]], "normalized_score": [0.007484059315174818], "tokens": [["\u0120Edward", "\u0120VIII"]], "entropy": [[2.2110750675201416, 1.7690489292144775]], "avg_entropy": [1.9900619983673096], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Richard ii", "score_of_biggest_cluster": 0.007484059315174818, "score_of_first_answer": 0.007484059315174818, "max_score_of_first_answer": 0.37539851665496826, "min_score_of_first_answer": 0.0001492045121267438, "avg_score": 0.007484059315174818, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18762465607142076, "std_of_first_answer": 0.18762465607142076, "avg_entropy_of_first_answer": 1.9900619983673096, "lex_sim": 1.0}
{"question": "Generally the standard steps per minute for a military 'quick march' pace is?", "answer": ["1.2"], "scores": [[0.013495992869138718, 0.572792649269104, 0.04088451340794563]], "normalized_score": [0.06811670958995819], "tokens": [["\u01201", ".", "2"]], "entropy": [[5.395100116729736, 2.3300933837890625, 3.8510546684265137]], "avg_entropy": [3.8587493896484375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "120", "score_of_biggest_cluster": 0.06811670958995819, "score_of_first_answer": 0.06811670958995819, "max_score_of_first_answer": 0.572792649269104, "min_score_of_first_answer": 0.013495992869138718, "avg_score": 0.06811670958995819, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2574423659153373, "std_of_first_answer": 0.2574423659153373, "avg_entropy_of_first_answer": 3.8587493896484375, "lex_sim": 1.0}
{"question": "What was the name of Sebastian's teddy bear in Brideshead Revisited", "answer": ["Neddy"], "scores": [[0.0031141696963459253, 0.12349652498960495]], "normalized_score": [0.019610943272709846], "tokens": [["\u0120N", "eddy"]], "entropy": [[6.735382556915283, 3.722454309463501]], "avg_entropy": [5.228918433189392], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Aloysius (disambiguation)", "score_of_biggest_cluster": 0.019610943272709846, "score_of_first_answer": 0.019610943272709846, "max_score_of_first_answer": 0.12349652498960495, "min_score_of_first_answer": 0.0031141696963459253, "avg_score": 0.019610943272709846, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06019117764662951, "std_of_first_answer": 0.06019117764662951, "avg_entropy_of_first_answer": 5.228918433189392, "lex_sim": 1.0}
{"question": "Which amphibian did the ancients believe could live in fire?", "answer": ["Hydrae"], "scores": [[0.005350679624825716, 0.0011814975878223777]], "normalized_score": [0.002514322055503726], "tokens": [["\u0120Hydra", "e"]], "entropy": [[2.3219618797302246, 0.9411495327949524]], "avg_entropy": [1.6315557062625885], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Urodele amphibians", "score_of_biggest_cluster": 0.002514322055503726, "score_of_first_answer": 0.002514322055503726, "max_score_of_first_answer": 0.005350679624825716, "min_score_of_first_answer": 0.0011814975878223777, "avg_score": 0.002514322055503726, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.002084591018501669, "std_of_first_answer": 0.002084591018501669, "avg_entropy_of_first_answer": 1.6315557062625885, "lex_sim": 1.0}
{"question": "What are the topmost 7 vertebrae of the backbone called?", "answer": ["Cervical vertebrae"], "scores": [[0.23230253159999847, 0.9862285852432251, 0.9554222226142883, 0.5449751615524292, 0.9871887564659119, 0.8751959800720215]], "normalized_score": [0.6847281455993652], "tokens": [["\u0120C", "erv", "ical", "\u0120verte", "bra", "e"]], "entropy": [[3.7475709915161133, 0.12633216381072998, 0.3053678572177887, 2.162712574005127, 0.08676831424236298, 0.4942857027053833]], "avg_entropy": [1.1538396005829175], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cervical", "score_of_biggest_cluster": 0.6847281455993652, "score_of_first_answer": 0.6847281455993652, "max_score_of_first_answer": 0.9871887564659119, "min_score_of_first_answer": 0.23230253159999847, "avg_score": 0.6847281455993652, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.28251213928234414, "std_of_first_answer": 0.28251213928234414, "avg_entropy_of_first_answer": 1.1538396005829175, "lex_sim": 1.0}
{"question": "In which year was Olaf Palme assassinated and the Chernobyl nuclear power station exploded?", "answer": ["1986"], "scores": [[0.6436144113540649]], "normalized_score": [0.6436144113540649], "tokens": [["\u01201986"]], "entropy": [[2.6024656295776367]], "avg_entropy": [2.6024656295776367], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1986", "score_of_biggest_cluster": 0.6436144113540649, "score_of_first_answer": 0.6436144113540649, "max_score_of_first_answer": 0.6436144113540649, "min_score_of_first_answer": 0.6436144113540649, "avg_score": 0.6436144113540649, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.6024656295776367, "lex_sim": 1.0}
{"question": "What was the name of Canada's first woman Prime Minister?", "answer": ["Elizabeth II"], "scores": [[0.07984783500432968, 0.22701790928840637]], "normalized_score": [0.13463613390922546], "tokens": [["\u0120Elizabeth", "\u0120II"]], "entropy": [[4.290157318115234, 4.92780876159668]], "avg_entropy": [4.608983039855957], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kim Campbell", "score_of_biggest_cluster": 0.13463613390922546, "score_of_first_answer": 0.13463613390922546, "max_score_of_first_answer": 0.22701790928840637, "min_score_of_first_answer": 0.07984783500432968, "avg_score": 0.13463613390922546, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07358503714203835, "std_of_first_answer": 0.07358503714203835, "avg_entropy_of_first_answer": 4.608983039855957, "lex_sim": 1.0}
{"question": "Which painting by Thomas Gainsborough is thought to be of Jonathan Buttall, the son of a wealthy hardware merchant?", "answer": ["A Young Man Reading An Old Man's Advice (painting)"], "scores": [[0.03190673142671585, 0.041853100061416626, 0.210049569606781, 0.1124226450920105, 0.0007804391789250076, 0.09505049139261246, 0.12019673734903336, 0.47083890438079834, 0.0025259489193558693, 0.06757566332817078, 0.0005844163824804127, 0.9579196572303772, 0.5139787793159485]], "normalized_score": [0.04763513430953026], "tokens": [["\u0120A", "\u0120Young", "\u0120Man", "\u0120Reading", "\u0120An", "\u0120Old", "\u0120Man", "'s", "\u0120Advice", "\u0120(", "pain", "ting", ")"]], "entropy": [[4.5987958908081055, 6.6873779296875, 3.37646484375, 4.539149761199951, 1.7735774517059326, 3.9832234382629395, 3.012115716934204, 2.1792187690734863, 3.2522497177124023, 1.6725410223007202, 5.183292865753174, 0.21213436126708984, 3.244762897491455]], "avg_entropy": [3.3626849743036122], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Blues boy", "score_of_biggest_cluster": 0.04763513430953026, "score_of_first_answer": 0.04763513430953026, "max_score_of_first_answer": 0.9579196572303772, "min_score_of_first_answer": 0.0005844163824804127, "avg_score": 0.04763513430953026, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27185044005673625, "std_of_first_answer": 0.27185044005673625, "avg_entropy_of_first_answer": 3.3626849743036122, "lex_sim": 1.0}
{"question": "To what office in the USA was Henry Kissinger appointed in 1973?", "answer": ["Secretary of State"], "scores": [[0.30261704325675964, 0.9384614825248718, 0.7951306104660034]], "normalized_score": [0.6089515686035156], "tokens": [["\u0120Secretary", "\u0120of", "\u0120State"]], "entropy": [[4.011420249938965, 0.38238346576690674, 0.8997731804847717]], "avg_entropy": [1.7645256320635478], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Secretary of State", "score_of_biggest_cluster": 0.6089515686035156, "score_of_first_answer": 0.6089515686035156, "max_score_of_first_answer": 0.9384614825248718, "min_score_of_first_answer": 0.30261704325675964, "avg_score": 0.6089515686035156, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2723175237594913, "std_of_first_answer": 0.2723175237594913, "avg_entropy_of_first_answer": 1.7645256320635478, "lex_sim": 1.0}
{"question": "What is the title of Max Bygraves autobiography", "answer": ["\u32d1"], "scores": [[0.011722696013748646, 8.46393686515512e-06, 0.03492411598563194, 0.028444990515708923]], "normalized_score": [0.003150885459035635], "tokens": [["\u0120", "\u00e3", "\u012d", "\u0133"]], "entropy": [[5.6174397468566895, 5.373331069946289, 3.145977735519409, 3.9091644287109375]], "avg_entropy": [4.511478245258331], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "I Wanna Tell You a Story", "score_of_biggest_cluster": 0.003150885459035635, "score_of_first_answer": 0.003150885459035635, "max_score_of_first_answer": 0.03492411598563194, "min_score_of_first_answer": 8.46393686515512e-06, "avg_score": 0.003150885459035635, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01374973209736268, "std_of_first_answer": 0.01374973209736268, "avg_entropy_of_first_answer": 4.511478245258331, "lex_sim": 0.0}
{"question": "To which family of birds does the linnet belong?", "answer": ["Grunth"], "scores": [[0.009956940077245235, 0.12537895143032074, 0.03951290249824524]], "normalized_score": [0.03667440265417099], "tokens": [["\u0120Gr", "un", "th"]], "entropy": [[5.418939113616943, 4.598847389221191, 3.2067861557006836]], "avg_entropy": [4.408190886179606], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fringillidae", "score_of_biggest_cluster": 0.03667440265417099, "score_of_first_answer": 0.03667440265417099, "max_score_of_first_answer": 0.12537895143032074, "min_score_of_first_answer": 0.009956940077245235, "avg_score": 0.03667440265417099, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04895437102108071, "std_of_first_answer": 0.04895437102108071, "avg_entropy_of_first_answer": 4.408190886179606, "lex_sim": 1.0}
{"question": "What word is given to a point in tennis which has to be replayed?", "answer": ["Replay"], "scores": [[0.02273690514266491]], "normalized_score": [0.02273690700531006], "tokens": [["\u0120Replay"]], "entropy": [[5.714526653289795]], "avg_entropy": [5.714526653289795], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Let (disambiguation)", "score_of_biggest_cluster": 0.02273690700531006, "score_of_first_answer": 0.02273690700531006, "max_score_of_first_answer": 0.02273690514266491, "min_score_of_first_answer": 0.02273690514266491, "avg_score": 0.02273690700531006, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.714526653289795, "lex_sim": 1.0}
{"question": "In the 18th century, the capital of Russia was moved from Moscow to where?", "answer": ["St. Petersburg"], "scores": [[0.7999616265296936, 0.5794101357460022, 0.8794562220573425]], "normalized_score": [0.7414635419845581], "tokens": [["\u0120St", ".", "\u0120Petersburg"]], "entropy": [[1.049617052078247, 0.9959737062454224, 0.6503673791885376]], "avg_entropy": [0.898652712504069], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "St Petersb.", "score_of_biggest_cluster": 0.7414635419845581, "score_of_first_answer": 0.7414635419845581, "max_score_of_first_answer": 0.8794562220573425, "min_score_of_first_answer": 0.5794101357460022, "avg_score": 0.7414635419845581, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1269251768719427, "std_of_first_answer": 0.1269251768719427, "avg_entropy_of_first_answer": 0.898652712504069, "lex_sim": 1.0}
{"question": "What gas is necessary for plants to perform photosynthesis?", "answer": ["CO2"], "scores": [[0.056028060615062714, 0.8392363786697388]], "normalized_score": [0.21684275567531586], "tokens": [["\u0120CO", "2"]], "entropy": [[2.650226593017578, 0.8851084113121033]], "avg_entropy": [1.7676675021648407], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Carbonic-acid gas", "score_of_biggest_cluster": 0.21684275567531586, "score_of_first_answer": 0.21684275567531586, "max_score_of_first_answer": 0.8392363786697388, "min_score_of_first_answer": 0.056028060615062714, "avg_score": 0.21684275567531586, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39160415902733803, "std_of_first_answer": 0.39160415902733803, "avg_entropy_of_first_answer": 1.7676675021648407, "lex_sim": 1.0}
{"question": "Which children\u2019s playground game is sometimes called \u2018Fivestones\u2019, \u2018Knucklebones\u2019 or \u2018Snobs\u2019?", "answer": ["Fivestones"], "scores": [[0.017888730391860008, 0.30211976170539856, 0.9269654154777527]], "normalized_score": [0.17110946774482727], "tokens": [["\u0120F", "iv", "estones"]], "entropy": [[6.214615345001221, 2.0658297538757324, 0.474679172039032]], "avg_entropy": [2.918374756971995], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jacks (disambiguation)", "score_of_biggest_cluster": 0.17110946774482727, "score_of_first_answer": 0.17110946774482727, "max_score_of_first_answer": 0.9269654154777527, "min_score_of_first_answer": 0.017888730391860008, "avg_score": 0.17110946774482727, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3797133113820527, "std_of_first_answer": 0.3797133113820527, "avg_entropy_of_first_answer": 2.918374756971995, "lex_sim": 1.0}
{"question": "Round the Clock, Hare and Hounds, Cricket and Shanghai are all games played on which type of board?", "answer": ["Go board"], "scores": [[0.011761615052819252, 0.20420195162296295]], "normalized_score": [0.0490075945854187], "tokens": [["\u0120Go", "\u0120board"]], "entropy": [[5.752838611602783, 3.1791934967041016]], "avg_entropy": [4.466016054153442], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jumpers (Darts)", "score_of_biggest_cluster": 0.0490075945854187, "score_of_first_answer": 0.0490075945854187, "max_score_of_first_answer": 0.20420195162296295, "min_score_of_first_answer": 0.011761615052819252, "avg_score": 0.0490075945854187, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09622016828507185, "std_of_first_answer": 0.09622016828507185, "avg_entropy_of_first_answer": 4.466016054153442, "lex_sim": 1.0}
{"question": "A swanee whistle was used to provide the voices of whom, in an animated tv series?", "answer": ["Snow White and the Seven Dwarfs"], "scores": [[0.004254598636180162, 0.5775296092033386, 0.6055909991264343, 0.8390949368476868, 0.9243682622909546, 0.9816837906837463, 0.9035703539848328]], "normalized_score": [0.37401241064071655], "tokens": [["\u0120Snow", "\u0120White", "\u0120and", "\u0120the", "\u0120Seven", "\u0120Dwar", "fs"]], "entropy": [[6.650636672973633, 2.4069936275482178, 1.206148624420166, 1.023597002029419, 0.4422631859779358, 0.18831467628479004, 0.34589433670043945]], "avg_entropy": [1.751978303704943], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Soup Dragon", "score_of_biggest_cluster": 0.37401241064071655, "score_of_first_answer": 0.37401241064071655, "max_score_of_first_answer": 0.9816837906837463, "min_score_of_first_answer": 0.004254598636180162, "avg_score": 0.37401241064071655, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31574524844259283, "std_of_first_answer": 0.31574524844259283, "avg_entropy_of_first_answer": 1.751978303704943, "lex_sim": 1.0}
{"question": "Mel Gibson (1990) and Kenneth Branagh (1996) have both played which character in movies?", "answer": ["Jack Ryan"], "scores": [[0.01447848416864872, 0.04105467349290848]], "normalized_score": [0.024380512535572052], "tokens": [["\u0120Jack", "\u0120Ryan"]], "entropy": [[5.805351257324219, 3.4041106700897217]], "avg_entropy": [4.60473096370697], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Murder of Gonzago", "score_of_biggest_cluster": 0.024380512535572052, "score_of_first_answer": 0.024380512535572052, "max_score_of_first_answer": 0.04105467349290848, "min_score_of_first_answer": 0.01447848416864872, "avg_score": 0.024380512535572052, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.013288094662129879, "std_of_first_answer": 0.013288094662129879, "avg_entropy_of_first_answer": 4.60473096370697, "lex_sim": 1.0}
{"question": "\"Shakespeare coined which four word phrase in the play \"\"Macbeth\"\" which has become a commonly used cliche for compassion ?\"", "answer": ["\"Thane\""], "scores": [[0.200311541557312, 0.10093968361616135, 0.019827360287308693, 0.0475534051656723]], "normalized_score": [0.06607749313116074], "tokens": [["\u0120\"", "Th", "ane", "\""]], "entropy": [[5.36314058303833, 5.010340690612793, 0.6862689256668091, 1.8079357147216797]], "avg_entropy": [3.216921478509903], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "MILK OF HUMAN KINDNESS", "score_of_biggest_cluster": 0.06607749313116074, "score_of_first_answer": 0.06607749313116074, "max_score_of_first_answer": 0.200311541557312, "min_score_of_first_answer": 0.019827360287308693, "avg_score": 0.06607749313116074, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06891225914179791, "std_of_first_answer": 0.06891225914179791, "avg_entropy_of_first_answer": 3.216921478509903, "lex_sim": 1.0}
{"question": "Statistically the greatest driver of Formula One, Michael Schumacher won his seven championships with which two companies?", "answer": ["Ferrari and Benetton"], "scores": [[0.5717809200286865, 0.703417956829071, 0.8365095257759094, 0.9810154438018799, 0.9997870326042175]], "normalized_score": [0.8011234998703003], "tokens": [["\u0120Ferrari", "\u0120and", "\u0120Ben", "et", "ton"]], "entropy": [[1.4278664588928223, 1.2674591541290283, 0.7857046127319336, 0.15855127573013306, 0.0030118627473711967]], "avg_entropy": [0.7285186728462577], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Benetton and Ferrari", "score_of_biggest_cluster": 0.8011234998703003, "score_of_first_answer": 0.8011234998703003, "max_score_of_first_answer": 0.9997870326042175, "min_score_of_first_answer": 0.5717809200286865, "avg_score": 0.8011234998703003, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.16353273784894204, "std_of_first_answer": 0.16353273784894204, "avg_entropy_of_first_answer": 0.7285186728462577, "lex_sim": 1.0}
{"question": "In Spain which fruit is called a melocoton?", "answer": ["A pome de la Rosa"], "scores": [[0.11713206022977829, 0.016627449542284012, 0.7206445932388306, 0.00029060529777780175, 0.048406947404146194, 0.006833622232079506]], "normalized_score": [0.022647179663181305], "tokens": [["\u0120A", "\u0120p", "ome", "\u0120de", "\u0120la", "\u0120Rosa"]], "entropy": [[4.903814315795898, 4.71753454208374, 1.3037158250808716, 0.49595168232917786, 5.503207206726074, 5.739355564117432]], "avg_entropy": [3.777263189355532], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Peacherine", "score_of_biggest_cluster": 0.022647179663181305, "score_of_first_answer": 0.022647179663181305, "max_score_of_first_answer": 0.7206445932388306, "min_score_of_first_answer": 0.00029060529777780175, "avg_score": 0.022647179663181305, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25746132452902565, "std_of_first_answer": 0.25746132452902565, "avg_entropy_of_first_answer": 3.777263189355532, "lex_sim": 1.0}
{"question": "\"In the story of Jason and the Argonauts, what was the name of the Greek god who rises from the sea and holds the rocks in place in order to let the ship \"\"The Argo\"\" to pass ?\"", "answer": ["Ath\u0113naeus"], "scores": [[0.003616037778556347, 0.014662125147879124, 0.5039347410202026, 0.33738237619400024]], "normalized_score": [0.054793830960989], "tokens": [["\u0120Ath", "\u00c4\u0135", "na", "eus"]], "entropy": [[4.265995502471924, 3.4587700366973877, 1.8768566846847534, 2.332327365875244]], "avg_entropy": [2.9834873974323273], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Triton", "score_of_biggest_cluster": 0.054793830960989, "score_of_first_answer": 0.054793830960989, "max_score_of_first_answer": 0.5039347410202026, "min_score_of_first_answer": 0.003616037778556347, "avg_score": 0.054793830960989, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21405556124897865, "std_of_first_answer": 0.21405556124897865, "avg_entropy_of_first_answer": 2.9834873974323273, "lex_sim": 1.0}
{"question": "Who partnered Jo Durie when she won the Wimbledon mixed doubles title in 1987?", "answer": ["\u32e4"], "scores": [[0.0014746435917913914, 3.2786429073894396e-06, 0.02518715336918831, 0.016419926658272743]], "normalized_score": [0.0011891390895470977], "tokens": [["\u0120", "\u00e3", "\u012d", "\u00a4"]], "entropy": [[6.166134357452393, 6.119452476501465, 2.483308792114258, 3.840869665145874]], "avg_entropy": [4.652441322803497], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jeremy Bates (disambiguation)", "score_of_biggest_cluster": 0.0011891390895470977, "score_of_first_answer": 0.0011891390895470977, "max_score_of_first_answer": 0.02518715336918831, "min_score_of_first_answer": 3.2786429073894396e-06, "avg_score": 0.0011891390895470977, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.010513109860975974, "std_of_first_answer": 0.010513109860975974, "avg_entropy_of_first_answer": 4.652441322803497, "lex_sim": 0.0}
{"question": "Who was the illustrator for most of Roald Dahl\u2019s stories for children?", "answer": ["Quentin Blake"], "scores": [[0.6906583309173584, 0.9982560276985168]], "normalized_score": [0.8303335905075073], "tokens": [["\u0120Quentin", "\u0120Blake"]], "entropy": [[2.3224358558654785, 0.020792348310351372]], "avg_entropy": [1.171614102087915], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Quentin Saxby Blake", "score_of_biggest_cluster": 0.8303335905075073, "score_of_first_answer": 0.8303335905075073, "max_score_of_first_answer": 0.9982560276985168, "min_score_of_first_answer": 0.6906583309173584, "avg_score": 0.8303335905075073, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.15379884839057922, "std_of_first_answer": 0.15379884839057922, "avg_entropy_of_first_answer": 1.171614102087915, "lex_sim": 1.0}
{"question": "What holds the greatest volume of fresh water on Earth?", "answer": ["Lake Titicaca  #include <stdio.h>"], "scores": [[0.5469561815261841, 0.11456543952226639, 0.9819081425666809, 0.9974278807640076, 0.007136187516152859, 0.00033434407669119537, 4.708689448307268e-05, 0.10793508589267731, 0.1449768990278244, 0.7915708422660828, 0.9819960594177246, 0.9928786158561707, 0.9654598236083984]], "normalized_score": [0.09833509474992752], "tokens": [["\u0120Lake", "\u0120Tit", "ic", "aca", "\u0120", "\u0120#", "include", "\u0120<", "std", "io", ".", "h", ">"]], "entropy": [[2.190150260925293, 3.033071517944336, 0.16269537806510925, 0.02728120982646942, 1.1539421081542969, 1.482578158378601, 3.3393616676330566, 4.906165599822998, 4.591499328613281, 1.1467921733856201, 0.12572504580020905, 0.07614578306674957, 0.25325271487236023]], "avg_entropy": [1.7298969958837216], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Glaciar", "score_of_biggest_cluster": 0.09833509474992752, "score_of_first_answer": 0.09833509474992752, "max_score_of_first_answer": 0.9974278807640076, "min_score_of_first_answer": 4.708689448307268e-05, "avg_score": 0.09833509474992752, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43219259030621004, "std_of_first_answer": 0.43219259030621004, "avg_entropy_of_first_answer": 1.7298969958837216, "lex_sim": 1.0}
{"question": "In the US, which state lies to the west of New Hampshire?", "answer": ["California"], "scores": [[0.07278376072645187]], "normalized_score": [0.07278376072645187], "tokens": [["\u0120California"]], "entropy": [[3.512352466583252]], "avg_entropy": [3.512352466583252], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sports in Vermont", "score_of_biggest_cluster": 0.07278376072645187, "score_of_first_answer": 0.07278376072645187, "max_score_of_first_answer": 0.07278376072645187, "min_score_of_first_answer": 0.07278376072645187, "avg_score": 0.07278376072645187, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.512352466583252, "lex_sim": 1.0}
{"question": "Which radio broadcaster was known as the Man In Black", "answer": ["Jethro Tull"], "scores": [[0.00691566476598382, 0.008112787269055843, 0.9951145648956299, 0.7523273825645447, 0.9994186162948608]], "normalized_score": [0.1332312971353531], "tokens": [["\u0120J", "eth", "ro", "\u0120T", "ull"]], "entropy": [[6.087906837463379, 4.7502217292785645, 0.05263598635792732, 1.7729051113128662, 0.0069153811782598495]], "avg_entropy": [2.5341170091181993], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Valentine Dyall", "score_of_biggest_cluster": 0.1332312971353531, "score_of_first_answer": 0.1332312971353531, "max_score_of_first_answer": 0.9994186162948608, "min_score_of_first_answer": 0.00691566476598382, "avg_score": 0.1332312971353531, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4537828809130257, "std_of_first_answer": 0.4537828809130257, "avg_entropy_of_first_answer": 2.5341170091181993, "lex_sim": 1.0}
{"question": "Film musical interpretations of Romeo and Juliet and the memoir of Baroness von Trapp shared what lead character name?", "answer": ["Friedrich"], "scores": [[0.008673840202391148]], "normalized_score": [0.008673840202391148], "tokens": [["\u0120Friedrich"]], "entropy": [[6.0427350997924805]], "avg_entropy": [6.0427350997924805], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mar\u00eda", "score_of_biggest_cluster": 0.008673840202391148, "score_of_first_answer": 0.008673840202391148, "max_score_of_first_answer": 0.008673840202391148, "min_score_of_first_answer": 0.008673840202391148, "avg_score": 0.008673840202391148, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.0427350997924805, "lex_sim": 1.0}
{"question": "What is an orthodox Mahommedan sect whose name derives from a body of traditional teaching which has equal authority with the Koran?", "answer": ["Sufi, Sunni, Shia"], "scores": [[0.13815143704414368, 0.33501365780830383, 0.008629855699837208, 0.026094520464539528, 0.350992351770401, 0.1555882692337036]], "normalized_score": [0.09103481471538544], "tokens": [["\u0120Su", "fi", ",", "\u0120Sunni", ",", "\u0120Shia"]], "entropy": [[4.416262626647949, 0.9755227565765381, 0.9461803436279297, 5.563084602355957, 1.856180191040039, 3.451767683029175]], "avg_entropy": [2.868166367212931], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sunni Arabs", "score_of_biggest_cluster": 0.09103481471538544, "score_of_first_answer": 0.09103481471538544, "max_score_of_first_answer": 0.350992351770401, "min_score_of_first_answer": 0.008629855699837208, "avg_score": 0.09103481471538544, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13413536786485092, "std_of_first_answer": 0.13413536786485092, "avg_entropy_of_first_answer": 2.868166367212931, "lex_sim": 1.0}
{"question": "In which river is the Boulder Dam?", "answer": ["The Colorado River"], "scores": [[0.11990861594676971, 0.5642914772033691, 0.2351602017879486]], "normalized_score": [0.2515200078487396], "tokens": [["\u0120The", "\u0120Colorado", "\u0120River"]], "entropy": [[3.545905113220215, 2.5228054523468018, 1.1764376163482666]], "avg_entropy": [2.415049393971761], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Colorado (State)", "score_of_biggest_cluster": 0.2515200078487396, "score_of_first_answer": 0.2515200078487396, "max_score_of_first_answer": 0.5642914772033691, "min_score_of_first_answer": 0.11990861594676971, "avg_score": 0.2515200078487396, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18829245924420934, "std_of_first_answer": 0.18829245924420934, "avg_entropy_of_first_answer": 2.415049393971761, "lex_sim": 1.0}
{"question": "Who was the conservative candidate beaten by Ken Livingstone in the first two elections for Mayor of London?", "answer": ["Nicholas Parsons"], "scores": [[0.007454692851752043, 0.00047713291132822633]], "normalized_score": [0.0018859690753743052], "tokens": [["\u0120Nicholas", "\u0120Parsons"]], "entropy": [[5.454021453857422, 5.180759429931641]], "avg_entropy": [5.317390441894531], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Steve Norris", "score_of_biggest_cluster": 0.0018859690753743052, "score_of_first_answer": 0.0018859690753743052, "max_score_of_first_answer": 0.007454692851752043, "min_score_of_first_answer": 0.00047713291132822633, "avg_score": 0.0018859690753743052, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0034887799702119082, "std_of_first_answer": 0.0034887799702119082, "avg_entropy_of_first_answer": 5.317390441894531, "lex_sim": 1.0}
{"question": "Who was the first Roman Emperor to sport a beard?", "answer": ["Tiberius (AD 14-37)"], "scores": [[0.06541215628385544, 0.993346095085144, 0.9817596673965454, 0.016130942851305008, 0.022293683141469955, 0.8791628479957581, 0.5611999034881592, 0.9557977914810181, 0.9354449510574341]], "normalized_score": [0.2786247134208679], "tokens": [["\u0120T", "iber", "ius", "\u0120(", "AD", "\u012014", "-", "37", ")"]], "entropy": [[3.737748146057129, 0.0658789724111557, 0.15156611800193787, 1.0733041763305664, 4.539937973022461, 0.7065837383270264, 2.2615480422973633, 0.26627466082572937, 0.44969192147254944]], "avg_entropy": [1.4725037498606577], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Publius Aelius Traianus Hadrianus", "score_of_biggest_cluster": 0.2786247134208679, "score_of_first_answer": 0.2786247134208679, "max_score_of_first_answer": 0.993346095085144, "min_score_of_first_answer": 0.016130942851305008, "avg_score": 0.2786247134208679, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4189184202879159, "std_of_first_answer": 0.4189184202879159, "avg_entropy_of_first_answer": 1.4725037498606577, "lex_sim": 1.0}
{"question": "Which superhero lives beneath the sea and can command all of the marine life via telepathic abilities?", "answer": ["The Sea King"], "scores": [[0.06134207174181938, 0.061745479702949524, 0.36717647314071655]], "normalized_score": [0.11162105947732925], "tokens": [["\u0120The", "\u0120Sea", "\u0120King"]], "entropy": [[2.941977024078369, 4.448760509490967, 3.3153252601623535]], "avg_entropy": [3.568687597910563], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Aqua-Man", "score_of_biggest_cluster": 0.11162105947732925, "score_of_first_answer": 0.11162105947732925, "max_score_of_first_answer": 0.36717647314071655, "min_score_of_first_answer": 0.06134207174181938, "avg_score": 0.11162105947732925, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.14407672939150867, "std_of_first_answer": 0.14407672939150867, "avg_entropy_of_first_answer": 3.568687597910563, "lex_sim": 1.0}
{"question": "When the egg is released from the ovary, what is left behind?", "answer": ["Ovary"], "scores": [[0.049267638474702835, 0.1287609189748764]], "normalized_score": [0.07964764535427094], "tokens": [["\u0120Ov", "ary"]], "entropy": [[4.716113090515137, 2.7331268787384033]], "avg_entropy": [3.72461998462677], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Corpora lutea", "score_of_biggest_cluster": 0.07964764535427094, "score_of_first_answer": 0.07964764535427094, "max_score_of_first_answer": 0.1287609189748764, "min_score_of_first_answer": 0.049267638474702835, "avg_score": 0.07964764535427094, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.039746640250086784, "std_of_first_answer": 0.039746640250086784, "avg_entropy_of_first_answer": 3.72461998462677, "lex_sim": 1.0}
{"question": "\"In a 2007 interview, which actor 'animatedly' bemoaned \"\"I hate that cat! Ever since I did that cat, I disappear. It's all about the cat. It stole everything from me.\"\"?\"", "answer": ["Liam Neeson"], "scores": [[0.004787316545844078, 0.9341784119606018, 0.9938894510269165, 0.9967746138572693]], "normalized_score": [0.25799673795700073], "tokens": [["\u0120Liam", "\u0120N", "ees", "on"]], "entropy": [[5.286199569702148, 0.4937284588813782, 0.06394946575164795, 0.028092771768569946]], "avg_entropy": [1.4679925665259361], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Antonio banderas", "score_of_biggest_cluster": 0.25799673795700073, "score_of_first_answer": 0.25799673795700073, "max_score_of_first_answer": 0.9967746138572693, "min_score_of_first_answer": 0.004787316545844078, "avg_score": 0.25799673795700073, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4208341163348472, "std_of_first_answer": 0.4208341163348472, "avg_entropy_of_first_answer": 1.4679925665259361, "lex_sim": 1.0}
{"question": "In which year did the first episode of Minder take place?", "answer": ["1967 (1)"], "scores": [[0.02404787391424179, 0.001435127924196422, 0.018619678914546967, 0.13602563738822937]], "normalized_score": [0.01719452627003193], "tokens": [["\u01201967", "\u0120(", "1", ")"]], "entropy": [[4.704320430755615, 0.38528144359588623, 6.15017032623291, 2.825850486755371]], "avg_entropy": [3.5164056718349457], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and seventy-nine", "score_of_biggest_cluster": 0.01719452627003193, "score_of_first_answer": 0.01719452627003193, "max_score_of_first_answer": 0.13602563738822937, "min_score_of_first_answer": 0.001435127924196422, "avg_score": 0.01719452627003193, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05319415746257599, "std_of_first_answer": 0.05319415746257599, "avg_entropy_of_first_answer": 3.5164056718349457, "lex_sim": 1.0}
{"question": "Libya was a colony of which European country prior to World War II?", "answer": ["Italy"], "scores": [[0.5137187242507935]], "normalized_score": [0.5137187242507935], "tokens": [["\u0120Italy"]], "entropy": [[2.7354588508605957]], "avg_entropy": [2.7354588508605957], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Environment of Italy", "score_of_biggest_cluster": 0.5137187242507935, "score_of_first_answer": 0.5137187242507935, "max_score_of_first_answer": 0.5137187242507935, "min_score_of_first_answer": 0.5137187242507935, "avg_score": 0.5137187242507935, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.7354588508605957, "lex_sim": 1.0}
{"question": "Who evaded capture disguised as Betty Burke?", "answer": ["Betty Jane Healey"], "scores": [[0.01888742670416832, 0.005740582477301359, 0.003059609327465296, 0.13152870535850525]], "normalized_score": [0.0144528578966856], "tokens": [["\u0120Betty", "\u0120Jane", "\u0120He", "aley"]], "entropy": [[6.207291603088379, 5.461544990539551, 4.286999702453613, 4.240367889404297]], "avg_entropy": [5.04905104637146], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Betty Burke", "score_of_biggest_cluster": 0.0144528578966856, "score_of_first_answer": 0.0144528578966856, "max_score_of_first_answer": 0.13152870535850525, "min_score_of_first_answer": 0.003059609327465296, "avg_score": 0.0144528578966856, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05329491361926536, "std_of_first_answer": 0.05329491361926536, "avg_entropy_of_first_answer": 5.04905104637146, "lex_sim": 1.0}
{"question": "What swirly pattern is named after the administrative central town of Renfrewshire in Scotland?", "answer": ["Loch Lomond pattern"], "scores": [[0.011248601600527763, 0.22927983105182648, 0.9942277669906616, 0.9714605808258057, 0.013009114190936089]], "normalized_score": [0.12650994956493378], "tokens": [["\u0120Loch", "\u0120L", "om", "ond", "\u0120pattern"]], "entropy": [[6.999049663543701, 4.273867607116699, 0.06394013017416, 0.1431485116481781, 1.7348060607910156]], "avg_entropy": [2.642962394654751], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paisley, Renfrewshire", "score_of_biggest_cluster": 0.12650994956493378, "score_of_first_answer": 0.12650994956493378, "max_score_of_first_answer": 0.9942277669906616, "min_score_of_first_answer": 0.011248601600527763, "avg_score": 0.12650994956493378, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4472352417833646, "std_of_first_answer": 0.4472352417833646, "avg_entropy_of_first_answer": 2.642962394654751, "lex_sim": 1.0}
{"question": "The daughter of the extravagant baronet 'Sir Walter Eliot', 'Anne Eliot is the central character in which of Jane Austen's novels?", "answer": ["Pride and Prejudice (novel)"], "scores": [[0.26994696259498596, 0.9596055746078491, 0.8189647197723389, 0.9979349374771118, 0.9983239769935608, 0.017803959548473358, 0.0050596389919519424, 0.9470331072807312, 0.8110021352767944]], "normalized_score": [0.29025617241859436], "tokens": [["\u0120Pride", "\u0120and", "\u0120Pre", "jud", "ice", "\u0120(", "no", "vel", ")"]], "entropy": [[2.769181966781616, 0.23815545439720154, 0.6939603090286255, 0.024971336126327515, 0.01767699420452118, 0.5596165657043457, 2.658812999725342, 0.463459849357605, 1.2323845624923706]], "avg_entropy": [0.9620244486464394], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Inveigling", "score_of_biggest_cluster": 0.29025617241859436, "score_of_first_answer": 0.29025617241859436, "max_score_of_first_answer": 0.9983239769935608, "min_score_of_first_answer": 0.0050596389919519424, "avg_score": 0.29025617241859436, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4001302323362657, "std_of_first_answer": 0.4001302323362657, "avg_entropy_of_first_answer": 0.9620244486464394, "lex_sim": 1.0}
{"question": "Which is the smallest of the Great Lakes in area?", "answer": ["Lake Huron"], "scores": [[0.17816874384880066, 0.18266938626766205, 0.9994792342185974]], "normalized_score": [0.3192201852798462], "tokens": [["\u0120Lake", "\u0120Hur", "on"]], "entropy": [[3.515986204147339, 2.84940242767334, 0.006646052002906799]], "avg_entropy": [2.1240115612745285], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ontario (canada)", "score_of_biggest_cluster": 0.3192201852798462, "score_of_first_answer": 0.3192201852798462, "max_score_of_first_answer": 0.9994792342185974, "min_score_of_first_answer": 0.17816874384880066, "avg_score": 0.3192201852798462, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3861130382965429, "std_of_first_answer": 0.3861130382965429, "avg_entropy_of_first_answer": 2.1240115612745285, "lex_sim": 1.0}
{"question": "What model of Ford was named after a city in Southern Spain", "answer": ["Escort"], "scores": [[0.059780970215797424, 0.8717555403709412]], "normalized_score": [0.2282857894897461], "tokens": [["\u0120Esc", "ort"]], "entropy": [[5.661844730377197, 0.8155211806297302]], "avg_entropy": [3.2386829555034637], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Granada", "score_of_biggest_cluster": 0.2282857894897461, "score_of_first_answer": 0.2282857894897461, "max_score_of_first_answer": 0.8717555403709412, "min_score_of_first_answer": 0.059780970215797424, "avg_score": 0.2282857894897461, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40598728507757187, "std_of_first_answer": 0.40598728507757187, "avg_entropy_of_first_answer": 3.2386829555034637, "lex_sim": 1.0}
{"question": "Raph Koster, Brad McQuaid, Mark Jacobs, Brian Green and J. Todd Coleman are influential in MMORPG design. What is MMORPG?", "answer": ["Multiuser online game"], "scores": [[0.012847666628658772, 0.6812978386878967, 0.9950124621391296, 0.4206388592720032, 0.2765268385410309]], "normalized_score": [0.2518414556980133], "tokens": [["\u0120Mult", "ius", "er", "\u0120online", "\u0120game"]], "entropy": [[2.055837869644165, 1.6504113674163818, 0.04883626103401184, 2.6938202381134033, 1.8270366191864014]], "avg_entropy": [1.6551884710788727], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Massively Multi Player On-line Roleplaying Game", "score_of_biggest_cluster": 0.2518414556980133, "score_of_first_answer": 0.2518414556980133, "max_score_of_first_answer": 0.9950124621391296, "min_score_of_first_answer": 0.012847666628658772, "avg_score": 0.2518414556980133, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33730627703308935, "std_of_first_answer": 0.33730627703308935, "avg_entropy_of_first_answer": 1.6551884710788727, "lex_sim": 1.0}
{"question": "Coleopterology is the study of which insects?", "answer": ["Beetles"], "scores": [[0.22620509564876556, 0.9947293400764465]], "normalized_score": [0.47435519099235535], "tokens": [["\u0120Beet", "les"]], "entropy": [[4.372739791870117, 0.061952874064445496]], "avg_entropy": [2.2173463329672813], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Black-Beetle", "score_of_biggest_cluster": 0.47435519099235535, "score_of_first_answer": 0.47435519099235535, "max_score_of_first_answer": 0.9947293400764465, "min_score_of_first_answer": 0.22620509564876556, "avg_score": 0.47435519099235535, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3842621222138405, "std_of_first_answer": 0.3842621222138405, "avg_entropy_of_first_answer": 2.2173463329672813, "lex_sim": 1.0}
{"question": "Where was Dick Turpin executed by hanging?", "answer": ["Maidstone Gaol, Kent, UK"], "scores": [[0.0037362698931246996, 0.9463660717010498, 0.09234654903411865, 0.9992406368255615, 0.1403704285621643, 0.8615773916244507, 0.2255151867866516, 0.1864774078130722]], "normalized_score": [0.18945015966892242], "tokens": [["\u0120Maid", "stone", "\u0120Ga", "ol", ",", "\u0120Kent", ",", "\u0120UK"]], "entropy": [[5.262541770935059, 0.4118177890777588, 1.7980303764343262, 0.00730753131210804, 0.8488802313804626, 0.9602079391479492, 0.8730591535568237, 1.1161668300628662]], "avg_entropy": [1.4097514527384192], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Park Grove (1895)", "score_of_biggest_cluster": 0.18945015966892242, "score_of_first_answer": 0.18945015966892242, "max_score_of_first_answer": 0.9992406368255615, "min_score_of_first_answer": 0.0037362698931246996, "avg_score": 0.18945015966892242, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39648621218736974, "std_of_first_answer": 0.39648621218736974, "avg_entropy_of_first_answer": 1.4097514527384192, "lex_sim": 1.0}
{"question": "Which BBC TV sitcom about a retired security guard was set at 37 Wyngate Drive in the first series then at 19, Riverbank for the final 5 series?", "answer": ["Wicker Park"], "scores": [[0.007513260934501886, 0.004751847591251135, 0.4723169505596161]], "normalized_score": [0.0256433617323637], "tokens": [["\u0120W", "icker", "\u0120Park"]], "entropy": [[5.448122978210449, 3.0443172454833984, 2.4839272499084473]], "avg_entropy": [3.6587891578674316], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "List of One Foot in the Grave characters", "score_of_biggest_cluster": 0.0256433617323637, "score_of_first_answer": 0.0256433617323637, "max_score_of_first_answer": 0.4723169505596161, "min_score_of_first_answer": 0.004751847591251135, "avg_score": 0.0256433617323637, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21976432346356592, "std_of_first_answer": 0.21976432346356592, "avg_entropy_of_first_answer": 3.6587891578674316, "lex_sim": 1.0}
{"question": "What is the 3rd largest car rental company in the United States?", "answer": ["Enterprise"], "scores": [[0.483949214220047]], "normalized_score": [0.483949214220047], "tokens": [["\u0120Enterprise"]], "entropy": [[2.3351364135742188]], "avg_entropy": [2.3351364135742188], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The alamo", "score_of_biggest_cluster": 0.483949214220047, "score_of_first_answer": 0.483949214220047, "max_score_of_first_answer": 0.483949214220047, "min_score_of_first_answer": 0.483949214220047, "avg_score": 0.483949214220047, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.3351364135742188, "lex_sim": 1.0}
{"question": "In 2006, who was the first football player in World Cup history to win the \u2018Best Young Player Award\u2019?", "answer": ["Lionel Messi"], "scores": [[0.09598685801029205, 0.9886082410812378]], "normalized_score": [0.30804771184921265], "tokens": [["\u0120Lionel", "\u0120Messi"]], "entropy": [[5.80025053024292, 0.12071261554956436]], "avg_entropy": [2.960481572896242], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lukas Podolski", "score_of_biggest_cluster": 0.30804771184921265, "score_of_first_answer": 0.30804771184921265, "max_score_of_first_answer": 0.9886082410812378, "min_score_of_first_answer": 0.09598685801029205, "avg_score": 0.30804771184921265, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44631069153547287, "std_of_first_answer": 0.44631069153547287, "avg_entropy_of_first_answer": 2.960481572896242, "lex_sim": 1.0}
{"question": "\"Which British singer, born Sandra Ann Goodrich, was known as \"\"the barefoot pop princess of the 1960s\"\"?\"", "answer": ["Sandie Shaw"], "scores": [[0.2032722681760788, 0.99413001537323, 0.9338504076004028]], "normalized_score": [0.5735872983932495], "tokens": [["\u0120Sand", "ie", "\u0120Shaw"]], "entropy": [[4.641570568084717, 0.04123089089989662, 0.4018118679523468]], "avg_entropy": [1.6948711089789867], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Just A Disillusion", "score_of_biggest_cluster": 0.5735872983932495, "score_of_first_answer": 0.5735872983932495, "max_score_of_first_answer": 0.99413001537323, "min_score_of_first_answer": 0.2032722681760788, "avg_score": 0.5735872983932495, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35944927400116883, "std_of_first_answer": 0.35944927400116883, "avg_entropy_of_first_answer": 1.6948711089789867, "lex_sim": 1.0}
{"question": "The 1964 film A Shot In The Dark was the sequel to which film of the same year ?", "answer": ["A Shot In The Dark (1964)"], "scores": [[0.08447974920272827, 0.8666033744812012, 0.88701331615448, 0.9694371223449707, 0.8716364502906799, 0.06737244129180908, 0.6116065979003906, 0.9181986451148987]], "normalized_score": [0.4620151221752167], "tokens": [["\u0120A", "\u0120Shot", "\u0120In", "\u0120The", "\u0120Dark", "\u0120(", "1964", ")"]], "entropy": [[5.841103553771973, 1.0555403232574463, 0.5380934476852417, 0.20815002918243408, 0.7013349533081055, 1.2969977855682373, 1.8781342506408691, 0.46989110112190247]], "avg_entropy": [1.4986556805670261], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pink panther", "score_of_biggest_cluster": 0.4620151221752167, "score_of_first_answer": 0.4620151221752167, "max_score_of_first_answer": 0.9694371223449707, "min_score_of_first_answer": 0.06737244129180908, "avg_score": 0.4620151221752167, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3511052364577691, "std_of_first_answer": 0.3511052364577691, "avg_entropy_of_first_answer": 1.4986556805670261, "lex_sim": 1.0}
{"question": "Sandra Bullock won this year's 'Oscar' for Best Actress for her role as 'Leigh Anne Tuohy', in which film?", "answer": ["The Blind Side"], "scores": [[0.3217921257019043, 0.9146788120269775, 0.9789135456085205]], "normalized_score": [0.6604847311973572], "tokens": [["\u0120The", "\u0120Blind", "\u0120Side"]], "entropy": [[3.3394508361816406, 0.7451026439666748, 0.1529178023338318]], "avg_entropy": [1.412490427494049], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Blind Side (film)", "score_of_biggest_cluster": 0.6604847311973572, "score_of_first_answer": 0.6604847311973572, "max_score_of_first_answer": 0.9789135456085205, "min_score_of_first_answer": 0.3217921257019043, "avg_score": 0.6604847311973572, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.29579446398544024, "std_of_first_answer": 0.29579446398544024, "avg_entropy_of_first_answer": 1.412490427494049, "lex_sim": 1.0}
{"question": "What six letter word is used to describe the soft skin that covers a deer's antlers as they develop ?", "answer": ["Chirugid"], "scores": [[0.0026731272228062153, 0.05694848671555519, 0.00250932271592319, 0.033408958464860916]], "normalized_score": [0.01062870305031538], "tokens": [["\u0120Ch", "ir", "ug", "id"]], "entropy": [[6.375143051147461, 5.294135570526123, 4.667773246765137, 3.612617015838623]], "avg_entropy": [4.987417221069336], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Three pile", "score_of_biggest_cluster": 0.01062870305031538, "score_of_first_answer": 0.01062870305031538, "max_score_of_first_answer": 0.05694848671555519, "min_score_of_first_answer": 0.00250932271592319, "avg_score": 0.01062870305031538, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.022862431326830666, "std_of_first_answer": 0.022862431326830666, "avg_entropy_of_first_answer": 4.987417221069336, "lex_sim": 1.0}
{"question": "In 1933, which actress starred in the original 'King Kong' movie?", "answer": ["Fay Wray"], "scores": [[0.8892828822135925, 0.9970875382423401, 0.9981174468994141]], "normalized_score": [0.9601040482521057], "tokens": [["\u0120Fay", "\u0120W", "ray"]], "entropy": [[0.8235492706298828, 0.0290963277220726, 0.0185551717877388]], "avg_entropy": [0.2904002567132314], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Vina Fay Wray", "score_of_biggest_cluster": 0.9601040482521057, "score_of_first_answer": 0.9601040482521057, "max_score_of_first_answer": 0.9981174468994141, "min_score_of_first_answer": 0.8892828822135925, "avg_score": 0.9601040482521057, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.051064085056231896, "std_of_first_answer": 0.051064085056231896, "avg_entropy_of_first_answer": 0.2904002567132314, "lex_sim": 1.0}
{"question": "Complete the name of the MLB (Baseball) team based in Arlington, Texas - 'The Texas....'?", "answer": ["Tejas"], "scores": [[0.000685567909386009, 0.5384379625320435]], "normalized_score": [0.019212907180190086], "tokens": [["\u0120Te", "jas"]], "entropy": [[3.520343780517578, 2.874289035797119]], "avg_entropy": [3.1973164081573486], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rangers (baseball)", "score_of_biggest_cluster": 0.019212907180190086, "score_of_first_answer": 0.019212907180190086, "max_score_of_first_answer": 0.5384379625320435, "min_score_of_first_answer": 0.000685567909386009, "avg_score": 0.019212907180190086, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2688761973113287, "std_of_first_answer": 0.2688761973113287, "avg_entropy_of_first_answer": 3.1973164081573486, "lex_sim": 1.0}
{"question": "Where did the first battle of the English Civil War take place?", "answer": ["Bosworth Field"], "scores": [[0.011743471026420593, 0.9594910740852356, 0.5859671235084534]], "normalized_score": [0.18760176002979279], "tokens": [["\u0120Bos", "worth", "\u0120Field"]], "entropy": [[4.673663139343262, 0.2595449984073639, 1.596350908279419]], "avg_entropy": [2.176519682010015], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Edghill", "score_of_biggest_cluster": 0.18760176002979279, "score_of_first_answer": 0.18760176002979279, "max_score_of_first_answer": 0.9594910740852356, "min_score_of_first_answer": 0.011743471026420593, "avg_score": 0.18760176002979279, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3897974493713971, "std_of_first_answer": 0.3897974493713971, "avg_entropy_of_first_answer": 2.176519682010015, "lex_sim": 1.0}
{"question": "Name the capital city of the Canadian Province of British Columbia?", "answer": ["Victoria"], "scores": [[0.599195122718811]], "normalized_score": [0.599195122718811], "tokens": [["\u0120Victoria"]], "entropy": [[1.4679163694381714]], "avg_entropy": [1.4679163694381714], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "CIVI-TV", "score_of_biggest_cluster": 0.599195122718811, "score_of_first_answer": 0.599195122718811, "max_score_of_first_answer": 0.599195122718811, "min_score_of_first_answer": 0.599195122718811, "avg_score": 0.599195122718811, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.4679163694381714, "lex_sim": 1.0}
{"question": "\"The name of which animal means \"\"man of the forest\"\"?\"", "answer": ["Red pandas"], "scores": [[0.029408490285277367, 0.0008445564308203757, 0.7365763187408447]], "normalized_score": [0.026349548250436783], "tokens": [["\u0120Red", "\u0120pand", "as"]], "entropy": [[5.315135955810547, 2.88295841217041, 1.5637257099151611]], "avg_entropy": [3.2539400259653726], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pongini", "score_of_biggest_cluster": 0.026349548250436783, "score_of_first_answer": 0.026349548250436783, "max_score_of_first_answer": 0.7365763187408447, "min_score_of_first_answer": 0.0008445564308203757, "avg_score": 0.026349548250436783, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.340294555739194, "std_of_first_answer": 0.340294555739194, "avg_entropy_of_first_answer": 3.2539400259653726, "lex_sim": 1.0}
{"question": "'Wilson, the Wonder Athlete' who was still breaking world records at the age of 128, first appeared in 1933 in which boy's comic?", "answer": ["Tom, Tommy or Tony the Wonder Athlete"], "scores": [[0.0012450801441445947, 0.04866832122206688, 0.10731099545955658, 0.08285947889089584, 0.008060579188168049, 0.046760428696870804, 0.09906214475631714, 0.028920384123921394, 0.9998917579650879]], "normalized_score": [0.04370453581213951], "tokens": [["\u0120Tom", ",", "\u0120Tommy", "\u0120or", "\u0120Tony", "\u0120the", "\u0120Wonder", "\u0120Ath", "lete"]], "entropy": [[5.040016174316406, 2.8940911293029785, 2.491419553756714, 1.8698935508728027, 3.4549810886383057, 2.541879653930664, 4.754446506500244, 2.7456870079040527, 0.001399644068442285]], "avg_entropy": [2.865979367698957], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tottenham Hotspurs F.C.", "score_of_biggest_cluster": 0.04370453581213951, "score_of_first_answer": 0.04370453581213951, "max_score_of_first_answer": 0.9998917579650879, "min_score_of_first_answer": 0.0012450801441445947, "avg_score": 0.04370453581213951, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2997246789978091, "std_of_first_answer": 0.2997246789978091, "avg_entropy_of_first_answer": 2.865979367698957, "lex_sim": 1.0}
{"question": "Which famous British actor is/was married to Anouk Aimee?", "answer": ["Ian McKellen"], "scores": [[0.03428756445646286, 0.2659985423088074, 0.8762434720993042]], "normalized_score": [0.1999310553073883], "tokens": [["\u0120Ian", "\u0120McK", "ellen"]], "entropy": [[5.071582794189453, 2.9038357734680176, 0.4201549291610718]], "avg_entropy": [2.798524498939514], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Albert Finney", "score_of_biggest_cluster": 0.1999310553073883, "score_of_first_answer": 0.1999310553073883, "max_score_of_first_answer": 0.8762434720993042, "min_score_of_first_answer": 0.03428756445646286, "avg_score": 0.1999310553073883, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35511791500335627, "std_of_first_answer": 0.35511791500335627, "avg_entropy_of_first_answer": 2.798524498939514, "lex_sim": 1.0}
{"question": "What was painter Rubens first name?", "answer": ["Anthonie"], "scores": [[0.009803893975913525, 0.28322264552116394, 0.4930762052536011]], "normalized_score": [0.11104021221399307], "tokens": [["\u0120Anth", "on", "ie"]], "entropy": [[2.8786661624908447, 2.902489185333252, 1.7131541967391968]], "avg_entropy": [2.4981031815210977], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Peter (consul)", "score_of_biggest_cluster": 0.11104021221399307, "score_of_first_answer": 0.11104021221399307, "max_score_of_first_answer": 0.4930762052536011, "min_score_of_first_answer": 0.009803893975913525, "avg_score": 0.11104021221399307, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1978631561561388, "std_of_first_answer": 0.1978631561561388, "avg_entropy_of_first_answer": 2.4981031815210977, "lex_sim": 1.0}
{"question": "What port city lies at the mouth of the River Seine?", "answer": ["Rouen"], "scores": [[0.37655264139175415, 0.9979637861251831]], "normalized_score": [0.6130138039588928], "tokens": [["\u0120Rou", "en"]], "entropy": [[2.155651330947876, 0.020905379205942154]], "avg_entropy": [1.088278355076909], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Le Havre, the City Rebuilt by Auguste Perret", "score_of_biggest_cluster": 0.6130138039588928, "score_of_first_answer": 0.6130138039588928, "max_score_of_first_answer": 0.9979637861251831, "min_score_of_first_answer": 0.37655264139175415, "avg_score": 0.6130138039588928, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3107055723667145, "std_of_first_answer": 0.3107055723667145, "avg_entropy_of_first_answer": 1.088278355076909, "lex_sim": 1.0}
{"question": "A member of which profession would reside in a manse", "answer": ["\u32d0"], "scores": [[0.0030044387094676495, 4.14941314375028e-06, 0.03714914247393608, 0.05861258506774902]], "normalized_score": [0.002282561268657446], "tokens": [["\u0120", "\u00e3", "\u012d", "\u0132"]], "entropy": [[4.850002288818359, 3.996924638748169, 2.8267457485198975, 3.887713670730591]], "avg_entropy": [3.890346586704254], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Clergyman", "score_of_biggest_cluster": 0.002282561268657446, "score_of_first_answer": 0.002282561268657446, "max_score_of_first_answer": 0.05861258506774902, "min_score_of_first_answer": 4.14941314375028e-06, "avg_score": 0.002282561268657446, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02442143919767681, "std_of_first_answer": 0.02442143919767681, "avg_entropy_of_first_answer": 3.890346586704254, "lex_sim": 0.0}
{"question": "By what name was world champion boxer Walker Smith better known?", "answer": ["Jack Walker"], "scores": [[0.13610827922821045, 0.10122083872556686]], "normalized_score": [0.1173754408955574], "tokens": [["\u0120Jack", "\u0120Walker"]], "entropy": [[4.829547882080078, 4.523525238037109]], "avg_entropy": [4.676536560058594], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Walker Smith Junior", "score_of_biggest_cluster": 0.1173754408955574, "score_of_first_answer": 0.1173754408955574, "max_score_of_first_answer": 0.13610827922821045, "min_score_of_first_answer": 0.10122083872556686, "avg_score": 0.1173754408955574, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.017443720251321793, "std_of_first_answer": 0.017443720251321793, "avg_entropy_of_first_answer": 4.676536560058594, "lex_sim": 1.0}
{"question": "The TV characters Jack Ford, Trevor Chaplin and Jack Halford link which actor?", "answer": ["Ben Kingsley"], "scores": [[0.01083353441208601, 0.2923324406147003, 0.9968980550765991]], "normalized_score": [0.1467008739709854], "tokens": [["\u0120Ben", "\u0120Kings", "ley"]], "entropy": [[5.401915550231934, 3.275242328643799, 0.02997530624270439]], "avg_entropy": [2.9023777283728123], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "James Christopher Bolam", "score_of_biggest_cluster": 0.1467008739709854, "score_of_first_answer": 0.1467008739709854, "max_score_of_first_answer": 0.9968980550765991, "min_score_of_first_answer": 0.01083353441208601, "avg_score": 0.1467008739709854, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4147258235119086, "std_of_first_answer": 0.4147258235119086, "avg_entropy_of_first_answer": 2.9023777283728123, "lex_sim": 1.0}
{"question": "What American city has suffered two (separate) outcomes of what are now called 'The Move' and 'The Decision' in sports lore?", "answer": ["Kansas City - The Kansas City Royals moved to Kansas City, Missouri on December 31, 1985 and, a month later, they won the 1995 Major League Baseball world series"], "scores": [[0.03088640235364437, 0.9760347604751587, 0.00785259623080492, 0.07026206701993942, 0.4142122268676758, 0.9735286235809326, 0.18371565639972687, 0.011998280882835388, 0.5250995755195618, 0.4895038604736328, 0.9750535488128662, 0.1843864619731903, 0.6972782611846924, 0.014660036191344261, 0.08695477992296219, 0.4523215591907501, 0.8811784982681274, 0.08894731104373932, 0.17918027937412262, 0.005834805313497782, 0.04914945363998413, 0.05997812747955322, 0.9142464399337769, 0.9189945459365845, 0.08533503115177155, 0.3690148591995239, 0.6621062159538269, 0.0006293668993748724, 0.013456995598971844, 0.9744267463684082, 0.9064432382583618, 0.013538452796638012, 0.448042094707489]], "normalized_score": [0.1390260010957718], "tokens": [["\u0120Kansas", "\u0120City", "\u0120-", "\u0120The", "\u0120Kansas", "\u0120City", "\u0120Royals", "\u0120moved", "\u0120to", "\u0120Kansas", "\u0120City", ",", "\u0120Missouri", "\u0120on", "\u0120December", "\u012031", ",", "\u01201985", "\u0120and", ",", "\u0120a", "\u0120month", "\u0120later", ",", "\u0120they", "\u0120won", "\u0120the", "\u01201995", "\u0120Major", "\u0120League", "\u0120Baseball", "\u0120world", "\u0120series"]], "entropy": [[4.839450836181641, 0.16804492473602295, 1.629420280456543, 5.660305023193359, 4.221524238586426, 0.24634359776973724, 2.641772747039795, 3.5018017292022705, 1.625293254852295, 2.5996904373168945, 0.18751674890518188, 2.4658074378967285, 1.5620390176773071, 2.4077038764953613, 3.432244300842285, 2.4086055755615234, 0.5806838870048523, 3.386157989501953, 2.2451283931732178, 3.506502389907837, 3.7219011783599854, 1.691463828086853, 0.4707130193710327, 0.4906403720378876, 3.159419059753418, 2.7764973640441895, 1.1356189250946045, 1.708930253982544, 0.5845954418182373, 0.22066108882427216, 0.5997976660728455, 0.8754571080207825, 1.4110558032989502]], "avg_entropy": [2.0655390240929346], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Rock 'n' Roll Capital of the World", "score_of_biggest_cluster": 0.1390260010957718, "score_of_first_answer": 0.1390260010957718, "max_score_of_first_answer": 0.9760347604751587, "min_score_of_first_answer": 0.0006293668993748724, "avg_score": 0.1390260010957718, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36957761500290337, "std_of_first_answer": 0.36957761500290337, "avg_entropy_of_first_answer": 2.0655390240929346, "lex_sim": 1.0}
{"question": "The \u2018Mahlabatari Declaration of Faith\u2019 was a statement of core principles laid down by the political leaders of which country in 1974?", "answer": ["Pakistan"], "scores": [[0.010217149741947651]], "normalized_score": [0.010217147879302502], "tokens": [["\u0120Pakistan"]], "entropy": [[4.712155818939209]], "avg_entropy": [4.712155818939209], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "South africa", "score_of_biggest_cluster": 0.010217147879302502, "score_of_first_answer": 0.010217147879302502, "max_score_of_first_answer": 0.010217149741947651, "min_score_of_first_answer": 0.010217149741947651, "avg_score": 0.010217147879302502, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.712155818939209, "lex_sim": 1.0}
{"question": "Which American songwriter wrote the music and lyrics for the musical 'Annie get Your Gun'?", "answer": ["Charles W. Morgan"], "scores": [[0.01363736018538475, 0.016231315210461617, 0.7951322793960571, 0.004254775587469339]], "normalized_score": [0.02941710874438286], "tokens": [["\u0120Charles", "\u0120W", ".", "\u0120Morgan"]], "entropy": [[2.102288246154785, 4.786367416381836, 1.81925368309021, 5.599975109100342]], "avg_entropy": [3.576971113681793], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ellin Mackay", "score_of_biggest_cluster": 0.02941710874438286, "score_of_first_answer": 0.02941710874438286, "max_score_of_first_answer": 0.7951322793960571, "min_score_of_first_answer": 0.004254775587469339, "avg_score": 0.02941710874438286, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3394063241335522, "std_of_first_answer": 0.3394063241335522, "avg_entropy_of_first_answer": 3.576971113681793, "lex_sim": 1.0}
{"question": "Who has dated Tony Romo, John Mayer, and Nick Lachey?", "answer": ["The Man Who Married an Computer"], "scores": [[0.01209291536360979, 0.001995768863707781, 0.07109354436397552, 0.004748732317239046, 0.006264053285121918, 0.0017574775265529752]], "normalized_score": [0.006690611131489277], "tokens": [["\u0120The", "\u0120Man", "\u0120Who", "\u0120Married", "\u0120an", "\u0120Computer"]], "entropy": [[6.682052135467529, 7.495473861694336, 3.6785106658935547, 4.699670791625977, 5.173028469085693, 3.850209951400757]], "avg_entropy": [5.263157645861308], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joe Simpson (manager)", "score_of_biggest_cluster": 0.006690611131489277, "score_of_first_answer": 0.006690611131489277, "max_score_of_first_answer": 0.07109354436397552, "min_score_of_first_answer": 0.0017574775265529752, "avg_score": 0.006690611131489277, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.024732826328982868, "std_of_first_answer": 0.024732826328982868, "avg_entropy_of_first_answer": 5.263157645861308, "lex_sim": 1.0}
{"question": "Which artist has a daughter and two sons with Jane Asher, whom he married in 1981?", "answer": ["David Bailey"], "scores": [[0.012009002268314362, 0.03330707177519798]], "normalized_score": [0.019999617710709572], "tokens": [["\u0120David", "\u0120Bailey"]], "entropy": [[4.348873615264893, 5.851086616516113]], "avg_entropy": [5.099980115890503], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gerald Anthony Scarfe", "score_of_biggest_cluster": 0.019999617710709572, "score_of_first_answer": 0.019999617710709572, "max_score_of_first_answer": 0.03330707177519798, "min_score_of_first_answer": 0.012009002268314362, "avg_score": 0.019999617710709572, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01064903475344181, "std_of_first_answer": 0.01064903475344181, "avg_entropy_of_first_answer": 5.099980115890503, "lex_sim": 1.0}
{"question": "\"The former Tory Home Secretary Reginald Maudling is quoted as saying, \"\"For God's sake bring me a large scotch. What a bloody awful country\"\". To where was he referring ?\"", "answer": ["England (for the large scotch)"], "scores": [[0.05340009927749634, 0.0025100938510149717, 0.0054693035781383514, 0.1353076547384262, 0.05820676311850548, 0.7088953852653503, 0.995583713054657, 0.9952758550643921, 0.8244802355766296]], "normalized_score": [0.11435341089963913], "tokens": [["\u0120England", "\u0120(", "for", "\u0120the", "\u0120large", "\u0120sc", "ot", "ch", ")"]], "entropy": [[5.619878768920898, 0.513308048248291, 6.31026554107666, 4.982247352600098, 6.332156181335449, 1.8989300727844238, 0.03512287884950638, 0.04299427941441536, 1.1241827011108398]], "avg_entropy": [2.9843428693711758], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Norn Iron", "score_of_biggest_cluster": 0.11435341089963913, "score_of_first_answer": 0.11435341089963913, "max_score_of_first_answer": 0.995583713054657, "min_score_of_first_answer": 0.0025100938510149717, "avg_score": 0.11435341089963913, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4218710605288912, "std_of_first_answer": 0.4218710605288912, "avg_entropy_of_first_answer": 2.9843428693711758, "lex_sim": 1.0}
{"question": "During World War II in which country did the Battle of the Kasserine Pass take place?", "answer": ["Tunisian campaign of North Africa"], "scores": [[0.01675148867070675, 0.11523717641830444, 0.0170265082269907, 0.06935451924800873, 0.07117307186126709, 0.8564503788948059]], "normalized_score": [0.07196876406669617], "tokens": [["\u0120Tunis", "ian", "\u0120campaign", "\u0120of", "\u0120North", "\u0120Africa"]], "entropy": [[3.017007827758789, 1.3376195430755615, 2.901998519897461, 1.0353105068206787, 2.4439034461975098, 0.46310919523239136]], "avg_entropy": [1.8664915064970653], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ISO 3166-1:TN", "score_of_biggest_cluster": 0.07196876406669617, "score_of_first_answer": 0.07196876406669617, "max_score_of_first_answer": 0.8564503788948059, "min_score_of_first_answer": 0.01675148867070675, "avg_score": 0.07196876406669617, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29954121793035215, "std_of_first_answer": 0.29954121793035215, "avg_entropy_of_first_answer": 1.8664915064970653, "lex_sim": 1.0}
{"question": "Which Italian demonstrated the first modern electric battery?", "answer": ["Alessandro Volta"], "scores": [[0.7698763608932495, 0.9544941186904907, 0.9906954765319824, 0.9995032548904419]], "normalized_score": [0.9235909581184387], "tokens": [["\u0120Aless", "andro", "\u0120Vol", "ta"]], "entropy": [[1.3621549606323242, 0.2652146816253662, 0.09099322557449341, 0.0058758314698934555]], "avg_entropy": [0.4310596748255193], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alessandro, Count Volta", "score_of_biggest_cluster": 0.9235909581184387, "score_of_first_answer": 0.9235909581184387, "max_score_of_first_answer": 0.9995032548904419, "min_score_of_first_answer": 0.7698763608932495, "avg_score": 0.9235909581184387, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.09320248333031288, "std_of_first_answer": 0.09320248333031288, "avg_entropy_of_first_answer": 0.4310596748255193, "lex_sim": 1.0}
{"question": "\"Which American vocalist (described by the British newspaper \"\"The Guardian\"\" as \"\"one of the greatest voices of her generation\"\") whose repertoire included jazz, blues, folk, gospel and pop, was unknown outside Washington, D.C., until after she died of melanoma in 1996?\"", "answer": ["Aretha Franklin"], "scores": [[0.1559777706861496, 0.9991412162780762, 0.9572805166244507]], "normalized_score": [0.5303667187690735], "tokens": [["\u0120Are", "tha", "\u0120Franklin"]], "entropy": [[4.20274019241333, 0.008635473437607288, 0.33276256918907166]], "avg_entropy": [1.5147127450133364], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eva Cassidy", "score_of_biggest_cluster": 0.5303667187690735, "score_of_first_answer": 0.5303667187690735, "max_score_of_first_answer": 0.9991412162780762, "min_score_of_first_answer": 0.1559777706861496, "avg_score": 0.5303667187690735, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3879809566927798, "std_of_first_answer": 0.3879809566927798, "avg_entropy_of_first_answer": 1.5147127450133364, "lex_sim": 1.0}
{"question": "Which of the United States of America is known as the Old Colony State?", "answer": ["Massachusetts"], "scores": [[0.6066626906394958]], "normalized_score": [0.6066626906394958], "tokens": [["\u0120Massachusetts"]], "entropy": [[1.8219801187515259]], "avg_entropy": [1.8219801187515259], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Masachusetts", "score_of_biggest_cluster": 0.6066626906394958, "score_of_first_answer": 0.6066626906394958, "max_score_of_first_answer": 0.6066626906394958, "min_score_of_first_answer": 0.6066626906394958, "avg_score": 0.6066626906394958, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.8219801187515259, "lex_sim": 1.0}
{"question": "Which former West Indian fast bowler was known as 'Whispering Death'?", "answer": ["Michael Holding"], "scores": [[0.06437913328409195, 0.9889216423034668]], "normalized_score": [0.2523210644721985], "tokens": [["\u0120Michael", "\u0120Holding"]], "entropy": [[4.7802324295043945, 0.11968612670898438]], "avg_entropy": [2.4499592781066895], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Michael Holding", "score_of_biggest_cluster": 0.2523210644721985, "score_of_first_answer": 0.2523210644721985, "max_score_of_first_answer": 0.9889216423034668, "min_score_of_first_answer": 0.06437913328409195, "avg_score": 0.2523210644721985, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4622712545096874, "std_of_first_answer": 0.4622712545096874, "avg_entropy_of_first_answer": 2.4499592781066895, "lex_sim": 1.0}
{"question": "The youngest gold medalist for team GB at the 2012 Olympics, Jade Jones, won in which sport?", "answer": ["Shooting"], "scores": [[0.020927442237734795]], "normalized_score": [0.020927442237734795], "tokens": [["\u0120Shooting"]], "entropy": [[4.950249195098877]], "avg_entropy": [4.950249195098877], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Punch (Taekwondo)", "score_of_biggest_cluster": 0.020927442237734795, "score_of_first_answer": 0.020927442237734795, "max_score_of_first_answer": 0.020927442237734795, "min_score_of_first_answer": 0.020927442237734795, "avg_score": 0.020927442237734795, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.950249195098877, "lex_sim": 1.0}
{"question": "\"Which product was promoted with the slogan \"\"Full of Eastern Promise\"\"?\"", "answer": ["Nescaf\u00e9 Moccaf\u00e9"], "scores": [[0.005549334920942783, 0.33661699295043945, 0.8663694858551025, 0.8942258954048157, 0.0008358729537576437, 0.0066979206167161465, 0.02042084001004696, 0.62065190076828]], "normalized_score": [0.05642106384038925], "tokens": [["\u0120N", "esc", "af", "\u00c3\u00a9", "\u0120M", "occ", "af", "\u00c3\u00a9"]], "entropy": [[7.138175010681152, 4.120205402374268, 0.41662079095840454, 0.6537514328956604, 1.351672649383545, 4.7777910232543945, 2.137056827545166, 2.3465166091918945]], "avg_entropy": [2.8677237182855606], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fry's Turkish Delight", "score_of_biggest_cluster": 0.05642106384038925, "score_of_first_answer": 0.05642106384038925, "max_score_of_first_answer": 0.8942258954048157, "min_score_of_first_answer": 0.0008358729537576437, "avg_score": 0.05642106384038925, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3713145205127269, "std_of_first_answer": 0.3713145205127269, "avg_entropy_of_first_answer": 2.8677237182855606, "lex_sim": 1.0}
{"question": "Who, after marrying in 1791, and living in Naples, became the mistress of Lord Nelson, having his child, Horatia, in 1803?", "answer": ["Mary Rose Nelson"], "scores": [[0.017320595681667328, 0.0014602590817958117, 0.01885654777288437]], "normalized_score": [0.00781300663948059], "tokens": [["\u0120Mary", "\u0120Rose", "\u0120Nelson"]], "entropy": [[4.450169563293457, 4.490207672119141, 4.0837507247924805]], "avg_entropy": [4.341375986735026], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lady Hamilton Emma", "score_of_biggest_cluster": 0.00781300663948059, "score_of_first_answer": 0.00781300663948059, "max_score_of_first_answer": 0.01885654777288437, "min_score_of_first_answer": 0.0014602590817958117, "avg_score": 0.00781300663948059, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.00786370202690419, "std_of_first_answer": 0.00786370202690419, "avg_entropy_of_first_answer": 4.341375986735026, "lex_sim": 1.0}
{"question": "Taphephobia is the abnormal fear of being what?", "answer": ["Snakes"], "scores": [[0.0014449249720200896, 0.7677225470542908]], "normalized_score": [0.03330617770552635], "tokens": [["\u0120Sn", "akes"]], "entropy": [[6.193840026855469, 0.88432776927948]], "avg_entropy": [3.5390838980674744], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Texan funeral", "score_of_biggest_cluster": 0.03330617770552635, "score_of_first_answer": 0.03330617770552635, "max_score_of_first_answer": 0.7677225470542908, "min_score_of_first_answer": 0.0014449249720200896, "avg_score": 0.03330617770552635, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38313881104113534, "std_of_first_answer": 0.38313881104113534, "avg_entropy_of_first_answer": 3.5390838980674744, "lex_sim": 1.0}
{"question": "What caused a 34 minute suspension of play in the 2013 Superbowl?", "answer": ["Foul language"], "scores": [[0.0028675768990069628, 0.3097142279148102, 0.2981698215007782]], "normalized_score": [0.0642165020108223], "tokens": [["\u0120F", "oul", "\u0120language"]], "entropy": [[5.95094633102417, 1.6485986709594727, 3.584048271179199]], "avg_entropy": [3.727864424387614], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Power failure", "score_of_biggest_cluster": 0.0642165020108223, "score_of_first_answer": 0.0642165020108223, "max_score_of_first_answer": 0.3097142279148102, "min_score_of_first_answer": 0.0028675768990069628, "avg_score": 0.0642165020108223, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1420060860299503, "std_of_first_answer": 0.1420060860299503, "avg_entropy_of_first_answer": 3.727864424387614, "lex_sim": 1.0}
{"question": "What is the Japanese word for the sash worn as a belt in traditional dress (kimonos and martial arts uniforms, etc)?", "answer": ["Obi"], "scores": [[0.2754442095756531]], "normalized_score": [0.2754442095756531], "tokens": [["\u0120Obi"]], "entropy": [[2.863069534301758]], "avg_entropy": [2.863069534301758], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Obi", "score_of_biggest_cluster": 0.2754442095756531, "score_of_first_answer": 0.2754442095756531, "max_score_of_first_answer": 0.2754442095756531, "min_score_of_first_answer": 0.2754442095756531, "avg_score": 0.2754442095756531, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.863069534301758, "lex_sim": 1.0}
{"question": "Italy is divided into how many regions?", "answer": ["\u318d"], "scores": [[0.008190557360649109, 3.1807812774786726e-06, 0.03837878629565239, 0.5389508605003357]], "normalized_score": [0.0048180557787418365], "tokens": [["\u0120", "\u00e3", "\u0128", "\u012f"]], "entropy": [[3.6748929023742676, 4.40274715423584, 2.2937614917755127, 2.2549796104431152]], "avg_entropy": [3.156595289707184], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "twenty", "score_of_biggest_cluster": 0.0048180557787418365, "score_of_first_answer": 0.0048180557787418365, "max_score_of_first_answer": 0.5389508605003357, "min_score_of_first_answer": 3.1807812774786726e-06, "avg_score": 0.0048180557787418365, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22710054928064724, "std_of_first_answer": 0.22710054928064724, "avg_entropy_of_first_answer": 3.156595289707184, "lex_sim": 0.0}
{"question": "The technical Hawaiian-English word 'aa', commonly the first two-letter word in a dictionary, refers to?", "answer": ["Dictionary (novel)"], "scores": [[0.01655958592891693, 0.004715573042631149, 0.002854578662663698, 0.017612775787711143, 0.6932427883148193]], "normalized_score": [0.019362786784768105], "tokens": [["\u0120Dictionary", "\u0120(", "no", "vel", ")"]], "entropy": [[6.30280876159668, 1.0493264198303223, 6.102202892303467, 5.865048408508301, 1.6604920625686646]], "avg_entropy": [4.195975708961487], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u02bba\u02bba", "score_of_biggest_cluster": 0.019362786784768105, "score_of_first_answer": 0.019362786784768105, "max_score_of_first_answer": 0.6932427883148193, "min_score_of_first_answer": 0.002854578662663698, "avg_score": 0.019362786784768105, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2731884696696559, "std_of_first_answer": 0.2731884696696559, "avg_entropy_of_first_answer": 4.195975708961487, "lex_sim": 1.0}
{"question": "Which UK international airport has the code PIK?", "answer": ["Oxford Airport"], "scores": [[0.010338257998228073, 0.07594438642263412]], "normalized_score": [0.02802021987736225], "tokens": [["\u0120Oxford", "\u0120Airport"]], "entropy": [[4.998248100280762, 1.9724218845367432]], "avg_entropy": [3.4853349924087524], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Prestwick", "score_of_biggest_cluster": 0.02802021987736225, "score_of_first_answer": 0.02802021987736225, "max_score_of_first_answer": 0.07594438642263412, "min_score_of_first_answer": 0.010338257998228073, "avg_score": 0.02802021987736225, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.032803064212203026, "std_of_first_answer": 0.032803064212203026, "avg_entropy_of_first_answer": 3.4853349924087524, "lex_sim": 1.0}
{"question": "Which country is the world's largest producer of copper?", "answer": ["China"], "scores": [[0.2819100618362427]], "normalized_score": [0.2819100618362427], "tokens": [["\u0120China"]], "entropy": [[3.2871487140655518]], "avg_entropy": [3.2871487140655518], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ISO 3166-1:CL", "score_of_biggest_cluster": 0.2819100618362427, "score_of_first_answer": 0.2819100618362427, "max_score_of_first_answer": 0.2819100618362427, "min_score_of_first_answer": 0.2819100618362427, "avg_score": 0.2819100618362427, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.2871487140655518, "lex_sim": 1.0}
{"question": "In the children\u2019s television series \u2018Ivor the Engine\u2019 what is the name of the dragon?", "answer": ["Whirlwind"], "scores": [[0.007269465364515781, 0.013019068166613579]], "normalized_score": [0.009728392586112022], "tokens": [["\u0120Wh", "irlwind"]], "entropy": [[6.887057304382324, 4.949657440185547]], "avg_entropy": [5.9183573722839355], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Idris", "score_of_biggest_cluster": 0.009728392586112022, "score_of_first_answer": 0.009728392586112022, "max_score_of_first_answer": 0.013019068166613579, "min_score_of_first_answer": 0.007269465364515781, "avg_score": 0.009728392586112022, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0028748014010488987, "std_of_first_answer": 0.0028748014010488987, "avg_entropy_of_first_answer": 5.9183573722839355, "lex_sim": 1.0}
{"question": "Which celebrity first found fame as the Joan Collins Fan Club", "answer": ["Gossamer"], "scores": [[0.004605667199939489, 0.005734250880777836, 0.1705976277589798]], "normalized_score": [0.016516350209712982], "tokens": [["\u0120G", "oss", "amer"]], "entropy": [[6.629924774169922, 3.819450855255127, 4.626921653747559]], "avg_entropy": [5.025432427724202], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joan Collins Fanclub", "score_of_biggest_cluster": 0.016516350209712982, "score_of_first_answer": 0.016516350209712982, "max_score_of_first_answer": 0.1705976277589798, "min_score_of_first_answer": 0.004605667199939489, "avg_score": 0.016516350209712982, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07798471196832443, "std_of_first_answer": 0.07798471196832443, "avg_entropy_of_first_answer": 5.025432427724202, "lex_sim": 1.0}
{"question": "What was the name of Matilda\u2019s teacher, who adopted her at the end of the book?", "answer": ["Aba"], "scores": [[0.0021559777669608593, 0.010646154172718525]], "normalized_score": [0.004790914710611105], "tokens": [["\u0120Ab", "a"]], "entropy": [[5.292547225952148, 2.6907076835632324]], "avg_entropy": [3.9916274547576904], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mr. Wormwood", "score_of_biggest_cluster": 0.004790914710611105, "score_of_first_answer": 0.004790914710611105, "max_score_of_first_answer": 0.010646154172718525, "min_score_of_first_answer": 0.0021559777669608593, "avg_score": 0.004790914710611105, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.004245088202878833, "std_of_first_answer": 0.004245088202878833, "avg_entropy_of_first_answer": 3.9916274547576904, "lex_sim": 1.0}
{"question": "What is the capital of the Navarre region of northern Spain", "answer": ["Pamplona"], "scores": [[0.6526359915733337, 0.969629168510437, 0.9992119073867798]], "normalized_score": [0.8583111763000488], "tokens": [["\u0120Pam", "pl", "ona"]], "entropy": [[2.1563751697540283, 0.259208619594574, 0.009488618932664394]], "avg_entropy": [0.8083574694270889], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pamplona", "score_of_biggest_cluster": 0.8583111763000488, "score_of_first_answer": 0.8583111763000488, "max_score_of_first_answer": 0.9992119073867798, "min_score_of_first_answer": 0.6526359915733337, "avg_score": 0.8583111763000488, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.15687032085488545, "std_of_first_answer": 0.15687032085488545, "avg_entropy_of_first_answer": 0.8083574694270889, "lex_sim": 1.0}
{"question": "Which country is known as The Hashemite Kingdom?", "answer": ["Jordan"], "scores": [[0.8648442625999451]], "normalized_score": [0.8648442625999451], "tokens": [["\u0120Jordan"]], "entropy": [[1.0109930038452148]], "avg_entropy": [1.0109930038452148], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jordan (country)", "score_of_biggest_cluster": 0.8648442625999451, "score_of_first_answer": 0.8648442625999451, "max_score_of_first_answer": 0.8648442625999451, "min_score_of_first_answer": 0.8648442625999451, "avg_score": 0.8648442625999451, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.0109930038452148, "lex_sim": 1.0}
{"question": "Give a year in the life of St Ignatius Loyola, founder of the Jesuits.", "answer": ["16th Century"], "scores": [[0.05034363642334938, 0.27324315905570984, 0.06302376091480255]], "normalized_score": [0.0953526422381401], "tokens": [["\u012016", "th", "\u0120Century"]], "entropy": [[2.2289836406707764, 3.682394027709961, 1.8509390354156494]], "avg_entropy": [2.5874389012654624], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1491-1556", "score_of_biggest_cluster": 0.0953526422381401, "score_of_first_answer": 0.0953526422381401, "max_score_of_first_answer": 0.27324315905570984, "min_score_of_first_answer": 0.05034363642334938, "avg_score": 0.0953526422381401, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10221827304655368, "std_of_first_answer": 0.10221827304655368, "avg_entropy_of_first_answer": 2.5874389012654624, "lex_sim": 1.0}
{"question": "In which country were democratic elections held in May 1989, and though Guillermo Endara was elected president, the results were annulled by the existing government?", "answer": ["Haiti"], "scores": [[0.05664632096886635]], "normalized_score": [0.05664631724357605], "tokens": [["\u0120Haiti"]], "entropy": [[3.6866042613983154]], "avg_entropy": [3.6866042613983154], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sport in Panama", "score_of_biggest_cluster": 0.05664631724357605, "score_of_first_answer": 0.05664631724357605, "max_score_of_first_answer": 0.05664632096886635, "min_score_of_first_answer": 0.05664632096886635, "avg_score": 0.05664631724357605, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.6866042613983154, "lex_sim": 1.0}
{"question": "In what card game do you need the queen of spades to shoot the moon?", "answer": ["Moon card, Rummy"], "scores": [[0.007606524042785168, 0.047871217131614685, 0.008006623946130276, 0.02085959166288376, 0.9010894298553467]], "normalized_score": [0.03529854491353035], "tokens": [["\u0120Moon", "\u0120card", ",", "\u0120R", "ummy"]], "entropy": [[5.250547885894775, 4.84617805480957, 1.7829898595809937, 6.25522518157959, 0.7267985343933105]], "avg_entropy": [3.772347903251648], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rickety Kate", "score_of_biggest_cluster": 0.03529854491353035, "score_of_first_answer": 0.03529854491353035, "max_score_of_first_answer": 0.9010894298553467, "min_score_of_first_answer": 0.007606524042785168, "avg_score": 0.03529854491353035, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35230529656126347, "std_of_first_answer": 0.35230529656126347, "avg_entropy_of_first_answer": 3.772347903251648, "lex_sim": 1.0}
{"question": "We know it as Myanmar today. By what name was it known before 1989?", "answer": ["Burma"], "scores": [[0.7152446508407593]], "normalized_score": [0.7152446508407593], "tokens": [["\u0120Burma"]], "entropy": [[1.807762622833252]], "avg_entropy": [1.807762622833252], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Economic sanctions against Myanmar", "score_of_biggest_cluster": 0.7152446508407593, "score_of_first_answer": 0.7152446508407593, "max_score_of_first_answer": 0.7152446508407593, "min_score_of_first_answer": 0.7152446508407593, "avg_score": 0.7152446508407593, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.807762622833252, "lex_sim": 1.0}
{"question": "Who wrote the 1982 novel, \u2018The Prodigal Daughter\u2019?", "answer": ["Lillian Hellman"], "scores": [[0.003858677577227354, 0.1751447468996048, 0.4308393597602844, 0.9983144998550415]], "normalized_score": [0.1305733621120453], "tokens": [["\u0120L", "illian", "\u0120Hell", "man"]], "entropy": [[6.296627044677734, 3.7719757556915283, 4.047924518585205, 0.015199942514300346]], "avg_entropy": [3.532931815367192], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "JEFFREY ARCHER", "score_of_biggest_cluster": 0.1305733621120453, "score_of_first_answer": 0.1305733621120453, "max_score_of_first_answer": 0.9983144998550415, "min_score_of_first_answer": 0.003858677577227354, "avg_score": 0.1305733621120453, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3762985861333984, "std_of_first_answer": 0.3762985861333984, "avg_entropy_of_first_answer": 3.532931815367192, "lex_sim": 1.0}
{"question": "A Gay Girl in Damascus is the title of a hoax blog by fictional feminist/freedom campaigner Amina Abdallah Alaf al Omari, who in June 2011 was said to be based and abducted in which country?", "answer": ["US"], "scores": [[0.0004246708413120359]], "normalized_score": [0.00042467075400054455], "tokens": [["\u0120US"]], "entropy": [[2.832094430923462]], "avg_entropy": [2.832094430923462], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u0627\u0644\u062c\u0645\u0647\u0648\u0631\u064a\u0629 \u0627\u0644\u0639\u0631\u0628\u064a\u0629 \u0627\u0644\u0633\u0648\u0631\u064a\u0629", "score_of_biggest_cluster": 0.00042467075400054455, "score_of_first_answer": 0.00042467075400054455, "max_score_of_first_answer": 0.0004246708413120359, "min_score_of_first_answer": 0.0004246708413120359, "avg_score": 0.00042467075400054455, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.832094430923462, "lex_sim": 1.0}
{"question": "What was the name of the Argentine Navy cruiser which was torpedoed and sunk during the 1982 Falklands War?", "answer": ["General Belgrano"], "scores": [[0.3512076437473297, 0.9791943430900574, 0.9997124075889587, 0.9999189376831055]], "normalized_score": [0.7657167911529541], "tokens": [["\u0120General", "\u0120Bel", "gr", "ano"]], "entropy": [[3.531571865081787, 0.1803407371044159, 0.003497839905321598, 0.001013085711747408]], "avg_entropy": [0.929105881950818], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Manuel Belgrano", "score_of_biggest_cluster": 0.7657167911529541, "score_of_first_answer": 0.7657167911529541, "max_score_of_first_answer": 0.9999189376831055, "min_score_of_first_answer": 0.3512076437473297, "avg_score": 0.7657167911529541, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2780065881192157, "std_of_first_answer": 0.2780065881192157, "avg_entropy_of_first_answer": 0.929105881950818, "lex_sim": 1.0}
{"question": "A novel by General Lew Wallace published in 1880 was made into which films in 1926 (a silent movie) and 1959 (produced by William Wyler)?", "answer": ["Ben-Hur"], "scores": [[0.9743020534515381, 0.5071370005607605, 0.9927881360054016]], "normalized_score": [0.7886636853218079], "tokens": [["\u0120Ben", "-", "Hur"]], "entropy": [[0.20614764094352722, 0.8109610080718994, 0.06512399017810822]], "avg_entropy": [0.3607442130645116], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ben hur", "score_of_biggest_cluster": 0.7886636853218079, "score_of_first_answer": 0.7886636853218079, "max_score_of_first_answer": 0.9927881360054016, "min_score_of_first_answer": 0.5071370005607605, "avg_score": 0.7886636853218079, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.22470769834246157, "std_of_first_answer": 0.22470769834246157, "avg_entropy_of_first_answer": 0.3607442130645116, "lex_sim": 1.0}
{"question": "What was the confrontation between the United States and the Soviet Union that was at its peak in October 1962?", "answer": ["Space Race"], "scores": [[0.005655376240611076, 0.5425236225128174]], "normalized_score": [0.05539111793041229], "tokens": [["\u0120Space", "\u0120Race"]], "entropy": [[1.996250033378601, 2.042402505874634]], "avg_entropy": [2.0193262696266174], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The cuban missile crisis", "score_of_biggest_cluster": 0.05539111793041229, "score_of_first_answer": 0.05539111793041229, "max_score_of_first_answer": 0.5425236225128174, "min_score_of_first_answer": 0.005655376240611076, "avg_score": 0.05539111793041229, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26843412313610315, "std_of_first_answer": 0.26843412313610315, "avg_entropy_of_first_answer": 2.0193262696266174, "lex_sim": 1.0}
{"question": "Which American city has districts called Financial, Mission, Japantown and Richmond?", "answer": ["San Francisco"], "scores": [[0.650049090385437, 0.9159100651741028]], "normalized_score": [0.7716129422187805], "tokens": [["\u0120San", "\u0120Francisco"]], "entropy": [[2.3442230224609375, 0.5546709299087524]], "avg_entropy": [1.449446976184845], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "San Fransciso", "score_of_biggest_cluster": 0.7716129422187805, "score_of_first_answer": 0.7716129422187805, "max_score_of_first_answer": 0.9159100651741028, "min_score_of_first_answer": 0.650049090385437, "avg_score": 0.7716129422187805, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.13293048739433289, "std_of_first_answer": 0.13293048739433289, "avg_entropy_of_first_answer": 1.449446976184845, "lex_sim": 1.0}
{"question": "During the 1970s which car maker produced the Triumph TR7 and the Jaguar XJS?", "answer": ["Triumph and Jaguar"], "scores": [[0.1456705778837204, 0.02700919657945633, 0.6703790426254272]], "normalized_score": [0.13816605508327484], "tokens": [["\u0120Triumph", "\u0120and", "\u0120Jaguar"]], "entropy": [[4.738574028015137, 1.3721563816070557, 2.246156930923462]], "avg_entropy": [2.7856291135152182], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "British Leyland Motor Corporation", "score_of_biggest_cluster": 0.13816605508327484, "score_of_first_answer": 0.13816605508327484, "max_score_of_first_answer": 0.6703790426254272, "min_score_of_first_answer": 0.02700919657945633, "avg_score": 0.13816605508327484, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2795480986612046, "std_of_first_answer": 0.2795480986612046, "avg_entropy_of_first_answer": 2.7856291135152182, "lex_sim": 1.0}
{"question": "Which composer was married to Clara Wieck from 1840 until his death in 1856?", "answer": ["Fr\u00e9d\u00e9ric Chopin"], "scores": [[0.01914108358323574, 0.8480348587036133, 0.9816384315490723, 0.9836534857749939, 0.9922600388526917, 0.9609869718551636, 0.9986440539360046]], "normalized_score": [0.5484442710876465], "tokens": [["\u0120Fr", "\u00c3\u00a9", "d", "\u00c3\u00a9", "ric", "\u0120Chop", "in"]], "entropy": [[4.095168113708496, 1.0424766540527344, 0.1286058872938156, 0.14491358399391174, 0.056960660964250565, 0.2771657109260559, 0.014413261786103249]], "avg_entropy": [0.8228148389607668], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Schumann, Robert Alexander", "score_of_biggest_cluster": 0.5484442710876465, "score_of_first_answer": 0.5484442710876465, "max_score_of_first_answer": 0.9986440539360046, "min_score_of_first_answer": 0.01914108358323574, "avg_score": 0.5484442710876465, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3330066165084454, "std_of_first_answer": 0.3330066165084454, "avg_entropy_of_first_answer": 0.8228148389607668, "lex_sim": 1.0}
{"question": "For which TV show does the Dave Arch orchestra regularly provide the musical accompaniment?", "answer": ["It's Okay, Ma"], "scores": [[0.005453350488096476, 0.8351940512657166, 0.001523312064819038, 0.34070849418640137, 0.1958570033311844]], "normalized_score": [0.05408942326903343], "tokens": [["\u0120It", "'s", "\u0120Okay", ",", "\u0120Ma"]], "entropy": [[5.881535530090332, 1.2061668634414673, 4.022830486297607, 2.388363838195801, 2.7363767623901367]], "avg_entropy": [3.247054696083069], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "People's Strictly", "score_of_biggest_cluster": 0.05408942326903343, "score_of_first_answer": 0.05408942326903343, "max_score_of_first_answer": 0.8351940512657166, "min_score_of_first_answer": 0.001523312064819038, "avg_score": 0.05408942326903343, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3071939773027851, "std_of_first_answer": 0.3071939773027851, "avg_entropy_of_first_answer": 3.247054696083069, "lex_sim": 1.0}
{"question": "The usual graph shape for product failure rate, starting and ending high with a long flat bottom in between, is known as what sort of curve?", "answer": ["Erlang"], "scores": [[0.003396320156753063, 0.21059922873973846]], "normalized_score": [0.026744388043880463], "tokens": [["\u0120Er", "lang"]], "entropy": [[5.469933986663818, 4.006582736968994]], "avg_entropy": [4.738258361816406], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\ud83d\udec1", "score_of_biggest_cluster": 0.026744388043880463, "score_of_first_answer": 0.026744388043880463, "max_score_of_first_answer": 0.21059922873973846, "min_score_of_first_answer": 0.003396320156753063, "avg_score": 0.026744388043880463, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1036014542914927, "std_of_first_answer": 0.1036014542914927, "avg_entropy_of_first_answer": 4.738258361816406, "lex_sim": 1.0}
{"question": "The name of which month is used for a letter in the phonetic alphabet?", "answer": ["\u4e59"], "scores": [[0.013244042173027992, 0.00016308976046275347, 0.1331566572189331]], "normalized_score": [0.006600906141102314], "tokens": [["\u0120", "\u00e4\u00b9", "\u013b"]], "entropy": [[6.452680587768555, 5.01623010635376, 3.4374067783355713]], "avg_entropy": [4.968772490819295], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The month of November", "score_of_biggest_cluster": 0.006600906141102314, "score_of_first_answer": 0.006600906141102314, "max_score_of_first_answer": 0.1331566572189331, "min_score_of_first_answer": 0.00016308976046275347, "avg_score": 0.006600906141102314, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05984928807568589, "std_of_first_answer": 0.05984928807568589, "avg_entropy_of_first_answer": 4.968772490819295, "lex_sim": 0.0}
{"question": "Which was the first commercial jet airliner to go into service in the UK?", "answer": ["British European Airways, Bristol Fighter"], "scores": [[0.0009773974306881428, 0.2435728758573532, 0.9456653594970703, 0.026701677590608597, 0.023174459114670753, 0.0005469547468237579]], "normalized_score": [0.02058999426662922], "tokens": [["\u0120British", "\u0120European", "\u0120Airways", ",", "\u0120Bristol", "\u0120Fighter"]], "entropy": [[0.6436604261398315, 2.5304040908813477, 0.41964852809906006, 3.1538634300231934, 5.278627395629883, 3.31974458694458]], "avg_entropy": [2.557658076286316], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cometary nuclei", "score_of_biggest_cluster": 0.02058999426662922, "score_of_first_answer": 0.02058999426662922, "max_score_of_first_answer": 0.9456653594970703, "min_score_of_first_answer": 0.0005469547468237579, "avg_score": 0.02058999426662922, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34115765342269966, "std_of_first_answer": 0.34115765342269966, "avg_entropy_of_first_answer": 2.557658076286316, "lex_sim": 1.0}
{"question": "What country does Waterford crystal come from?", "answer": ["Ireland"], "scores": [[0.8527661561965942]], "normalized_score": [0.8527661561965942], "tokens": [["\u0120Ireland"]], "entropy": [[1.1012201309204102]], "avg_entropy": [1.1012201309204102], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Irlanda", "score_of_biggest_cluster": 0.8527661561965942, "score_of_first_answer": 0.8527661561965942, "max_score_of_first_answer": 0.8527661561965942, "min_score_of_first_answer": 0.8527661561965942, "avg_score": 0.8527661561965942, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.1012201309204102, "lex_sim": 1.0}
{"question": "Denis Gabor won the 1971 Nobel Prize in Physics for which invention?", "answer": ["The Laser"], "scores": [[0.12601563334465027, 0.18693877756595612]], "normalized_score": [0.1534835696220398], "tokens": [["\u0120The", "\u0120Laser"]], "entropy": [[5.478570938110352, 3.6067628860473633]], "avg_entropy": [4.542666912078857], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hologrammatic", "score_of_biggest_cluster": 0.1534835696220398, "score_of_first_answer": 0.1534835696220398, "max_score_of_first_answer": 0.18693877756595612, "min_score_of_first_answer": 0.12601563334465027, "avg_score": 0.1534835696220398, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.030461572110652924, "std_of_first_answer": 0.030461572110652924, "avg_entropy_of_first_answer": 4.542666912078857, "lex_sim": 1.0}
{"question": "The ESA was 35 years old in 2010. For what does the S stand?", "answer": ["The Space Agency"], "scores": [[0.035627786070108414, 0.17241932451725006, 0.4371071457862854]], "normalized_score": [0.13899126648902893], "tokens": [["\u0120The", "\u0120Space", "\u0120Agency"]], "entropy": [[5.248388290405273, 4.92746114730835, 3.142963409423828]], "avg_entropy": [4.43960428237915], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Space", "score_of_biggest_cluster": 0.13899126648902893, "score_of_first_answer": 0.13899126648902893, "max_score_of_first_answer": 0.4371071457862854, "min_score_of_first_answer": 0.035627786070108414, "avg_score": 0.13899126648902893, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.16665241405625464, "std_of_first_answer": 0.16665241405625464, "avg_entropy_of_first_answer": 4.43960428237915, "lex_sim": 1.0}
{"question": "Bjrn Ulvaeus and Benny Andersson wrote most of the songs for which pop group?", "answer": ["ABBA"], "scores": [[0.8431099057197571, 0.9990511536598206]], "normalized_score": [0.9177744388580322], "tokens": [["\u0120AB", "BA"]], "entropy": [[0.5038471221923828, 0.010282475501298904]], "avg_entropy": [0.25706479884684086], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Infina AB", "score_of_biggest_cluster": 0.9177744388580322, "score_of_first_answer": 0.9177744388580322, "max_score_of_first_answer": 0.9990511536598206, "min_score_of_first_answer": 0.8431099057197571, "avg_score": 0.9177744388580322, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07797062397003174, "std_of_first_answer": 0.07797062397003174, "avg_entropy_of_first_answer": 0.25706479884684086, "lex_sim": 1.0}
{"question": "What's the cube root of 27,000?", "answer": ["3,000"], "scores": [[0.09341863542795181, 0.2239353507757187, 0.27616724371910095]], "normalized_score": [0.17943596839904785], "tokens": [["\u01203", ",", "000"]], "entropy": [[5.072752952575684, 1.6173291206359863, 4.669723987579346]], "avg_entropy": [3.786602020263672], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "30", "score_of_biggest_cluster": 0.17943596839904785, "score_of_first_answer": 0.17943596839904785, "max_score_of_first_answer": 0.27616724371910095, "min_score_of_first_answer": 0.09341863542795181, "avg_score": 0.17943596839904785, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07685472403464344, "std_of_first_answer": 0.07685472403464344, "avg_entropy_of_first_answer": 3.786602020263672, "lex_sim": 1.0}
{"question": "What colour is the cross on the national flag of Denmark?", "answer": ["Red"], "scores": [[0.109039805829525]], "normalized_score": [0.109039805829525], "tokens": [["\u0120Red"]], "entropy": [[3.533627986907959]], "avg_entropy": [3.533627986907959], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "White (Colour)", "score_of_biggest_cluster": 0.109039805829525, "score_of_first_answer": 0.109039805829525, "max_score_of_first_answer": 0.109039805829525, "min_score_of_first_answer": 0.109039805829525, "avg_score": 0.109039805829525, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.533627986907959, "lex_sim": 1.0}
{"question": "Who made Victor Krum's wand?", "answer": ["Harry Potter"], "scores": [[0.12032138556241989, 0.8237536549568176]], "normalized_score": [0.3148256242275238], "tokens": [["\u0120Harry", "\u0120Potter"]], "entropy": [[5.372446060180664, 1.4753004312515259]], "avg_entropy": [3.423873245716095], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sanguini", "score_of_biggest_cluster": 0.3148256242275238, "score_of_first_answer": 0.3148256242275238, "max_score_of_first_answer": 0.8237536549568176, "min_score_of_first_answer": 0.12032138556241989, "avg_score": 0.3148256242275238, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35171613469719887, "std_of_first_answer": 0.35171613469719887, "avg_entropy_of_first_answer": 3.423873245716095, "lex_sim": 1.0}
{"question": "What is the chief river of Burma ?", "answer": ["Tumpin"], "scores": [[0.0038345179054886103, 0.0021280141081660986, 0.0452401228249073]], "normalized_score": [0.007173584774136543], "tokens": [["\u0120T", "ump", "in"]], "entropy": [[2.695344924926758, 3.9768078327178955, 3.9645206928253174]], "avg_entropy": [3.5455578168233237], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "IRRAWADDY", "score_of_biggest_cluster": 0.007173584774136543, "score_of_first_answer": 0.007173584774136543, "max_score_of_first_answer": 0.0452401228249073, "min_score_of_first_answer": 0.0021280141081660986, "avg_score": 0.007173584774136543, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.019933194514448, "std_of_first_answer": 0.019933194514448, "avg_entropy_of_first_answer": 3.5455578168233237, "lex_sim": 1.0}
{"question": "What is the only country in the southern hemisphere to have won a gold medal in the Winter Olympics?", "answer": ["Australia"], "scores": [[0.3758455216884613]], "normalized_score": [0.3758455216884613], "tokens": [["\u0120Australia"]], "entropy": [[3.1636741161346436]], "avg_entropy": [3.1636741161346436], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Australia (Commonwealth realm)", "score_of_biggest_cluster": 0.3758455216884613, "score_of_first_answer": 0.3758455216884613, "max_score_of_first_answer": 0.3758455216884613, "min_score_of_first_answer": 0.3758455216884613, "avg_score": 0.3758455216884613, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.1636741161346436, "lex_sim": 1.0}
{"question": "Who originated the 'Wall of Sound' record production technique?", "answer": ["Eddie Van Halen"], "scores": [[0.0037837657146155834, 0.09885754436254501, 0.9522046446800232, 0.997902512550354]], "normalized_score": [0.13730555772781372], "tokens": [["\u0120Eddie", "\u0120Van", "\u0120Hal", "en"]], "entropy": [[5.514939308166504, 3.7873003482818604, 0.45048847794532776, 0.02307504415512085]], "avg_entropy": [2.443950794637203], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Annette Records", "score_of_biggest_cluster": 0.13730555772781372, "score_of_first_answer": 0.13730555772781372, "max_score_of_first_answer": 0.997902512550354, "min_score_of_first_answer": 0.0037837657146155834, "avg_score": 0.13730555772781372, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4633697695568122, "std_of_first_answer": 0.4633697695568122, "avg_entropy_of_first_answer": 2.443950794637203, "lex_sim": 1.0}
{"question": "What is the darkly common name for the plague which killed 30-50% of Europe between 1347-52?", "answer": ["Black Death"], "scores": [[0.4684300124645233, 0.8664183616638184]], "normalized_score": [0.6370685696601868], "tokens": [["\u0120Black", "\u0120Death"]], "entropy": [[2.2394707202911377, 0.5701512694358826]], "avg_entropy": [1.4048109948635101], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Black death", "score_of_biggest_cluster": 0.6370685696601868, "score_of_first_answer": 0.6370685696601868, "max_score_of_first_answer": 0.8664183616638184, "min_score_of_first_answer": 0.4684300124645233, "avg_score": 0.6370685696601868, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.19899417459964752, "std_of_first_answer": 0.19899417459964752, "avg_entropy_of_first_answer": 1.4048109948635101, "lex_sim": 1.0}
{"question": "What nationality was the pointillist artist MONDRIAN?", "answer": ["French"], "scores": [[0.23281466960906982]], "normalized_score": [0.23281468451023102], "tokens": [["\u0120French"]], "entropy": [[3.685490131378174]], "avg_entropy": [3.685490131378174], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Autochtones", "score_of_biggest_cluster": 0.23281468451023102, "score_of_first_answer": 0.23281468451023102, "max_score_of_first_answer": 0.23281466960906982, "min_score_of_first_answer": 0.23281466960906982, "avg_score": 0.23281468451023102, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.685490131378174, "lex_sim": 1.0}
{"question": "Which European town gave its name to a Treaty which symbolizes closer economic links between European countries?", "answer": ["Berwick"], "scores": [[0.009844053536653519, 0.0632672905921936]], "normalized_score": [0.024956097826361656], "tokens": [["\u0120Ber", "wick"]], "entropy": [[5.481531143188477, 1.8799817562103271]], "avg_entropy": [3.680756449699402], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Maastricht, the Netherlands", "score_of_biggest_cluster": 0.024956097826361656, "score_of_first_answer": 0.024956097826361656, "max_score_of_first_answer": 0.0632672905921936, "min_score_of_first_answer": 0.009844053536653519, "avg_score": 0.024956097826361656, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.026711618527770042, "std_of_first_answer": 0.026711618527770042, "avg_entropy_of_first_answer": 3.680756449699402, "lex_sim": 1.0}
{"question": "\"Which composer wrote \"\"Night and Day\"\", \"\"I Get a Kick out of You\"\", \"\"Well, Did You Evah!\"\" and \"\"I've Got You Under My Skin\"\"?\"", "answer": ["Gershwin"], "scores": [[0.00544973136857152, 0.8860009908676147, 0.8409439921379089, 0.9995964169502258]], "normalized_score": [0.25240635871887207], "tokens": [["\u0120G", "ers", "h", "win"]], "entropy": [[2.16324782371521, 0.735920786857605, 0.6669090986251831, 0.005251970607787371]], "avg_entropy": [0.8928324199514464], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cole Porter", "score_of_biggest_cluster": 0.25240635871887207, "score_of_first_answer": 0.25240635871887207, "max_score_of_first_answer": 0.9995964169502258, "min_score_of_first_answer": 0.00544973136857152, "avg_score": 0.25240635871887207, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39543120914840296, "std_of_first_answer": 0.39543120914840296, "avg_entropy_of_first_answer": 0.8928324199514464, "lex_sim": 1.0}
{"question": "Which British Prime Minister married Gladys Mary Baldwin on January 1st 1940?", "answer": ["David Cecil"], "scores": [[0.042374443262815475, 0.003409977536648512]], "normalized_score": [0.01202064473181963], "tokens": [["\u0120David", "\u0120Cecil"]], "entropy": [[3.6224277019500732, 2.1372759342193604]], "avg_entropy": [2.879851818084717], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A week is a long time in politics", "score_of_biggest_cluster": 0.01202064473181963, "score_of_first_answer": 0.01202064473181963, "max_score_of_first_answer": 0.042374443262815475, "min_score_of_first_answer": 0.003409977536648512, "avg_score": 0.01202064473181963, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.019482232863083482, "std_of_first_answer": 0.019482232863083482, "avg_entropy_of_first_answer": 2.879851818084717, "lex_sim": 1.0}
{"question": "In which town or city in Wales is the baked bean museum of excellence?", "answer": ["Llanhilleth"], "scores": [[0.10149108618497849, 0.700273334980011, 0.012400000356137753, 0.24797604978084564, 0.9960021376609802]], "normalized_score": [0.18516436219215393], "tokens": [["\u0120L", "lan", "h", "ille", "th"]], "entropy": [[4.415872097015381, 0.8691195249557495, 3.0051498413085938, 3.617274284362793, 0.029444197192788124]], "avg_entropy": [2.387371988967061], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Port Talbot, Neath Port Talbot", "score_of_biggest_cluster": 0.18516436219215393, "score_of_first_answer": 0.18516436219215393, "max_score_of_first_answer": 0.9960021376609802, "min_score_of_first_answer": 0.012400000356137753, "avg_score": 0.18516436219215393, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3760742277775105, "std_of_first_answer": 0.3760742277775105, "avg_entropy_of_first_answer": 2.387371988967061, "lex_sim": 1.0}
{"question": "In which county will you find Fountains abbey and Studley Water Gardens?", "answer": ["Wiltshire"], "scores": [[0.03122570365667343, 0.9856610298156738, 0.966273307800293]], "normalized_score": [0.3098227381706238], "tokens": [["\u0120Wil", "ts", "hire"]], "entropy": [[3.4275050163269043, 0.11151108890771866, 0.20372189581394196]], "avg_entropy": [1.247579333682855], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wrench Green", "score_of_biggest_cluster": 0.3098227381706238, "score_of_first_answer": 0.3098227381706238, "max_score_of_first_answer": 0.9856610298156738, "min_score_of_first_answer": 0.03122570365667343, "avg_score": 0.3098227381706238, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4454257261442144, "std_of_first_answer": 0.4454257261442144, "avg_entropy_of_first_answer": 1.247579333682855, "lex_sim": 1.0}
{"question": "The documentary Lousy Little Sixpence (1983) and the feature-film Rabbit-Proof Fence (2002) focus on 'Stolen Generations' in which country?", "answer": ["Australia"], "scores": [[0.8858058452606201]], "normalized_score": [0.8858058452606201], "tokens": [["\u0120Australia"]], "entropy": [[0.8400994539260864]], "avg_entropy": [0.8400994539260864], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Australia (Commonwealth realm)", "score_of_biggest_cluster": 0.8858058452606201, "score_of_first_answer": 0.8858058452606201, "max_score_of_first_answer": 0.8858058452606201, "min_score_of_first_answer": 0.8858058452606201, "avg_score": 0.8858058452606201, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.8400994539260864, "lex_sim": 1.0}
{"question": "Scottish artist, Sir Henry Raeburn was knighted by which British monarch in 1822?", "answer": ["George IV"], "scores": [[0.5285729169845581, 0.7468345165252686]], "normalized_score": [0.6282964944839478], "tokens": [["\u0120George", "\u0120IV"]], "entropy": [[1.9321460723876953, 1.0056326389312744]], "avg_entropy": [1.4688893556594849], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George, Prince Regent", "score_of_biggest_cluster": 0.6282964944839478, "score_of_first_answer": 0.6282964944839478, "max_score_of_first_answer": 0.7468345165252686, "min_score_of_first_answer": 0.5285729169845581, "avg_score": 0.6282964944839478, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.10913079977035522, "std_of_first_answer": 0.10913079977035522, "avg_entropy_of_first_answer": 1.4688893556594849, "lex_sim": 1.0}
{"question": "Juba is the capital of which country?", "answer": ["South Sudan"], "scores": [[0.3028199374675751, 0.930231511592865]], "normalized_score": [0.5307472348213196], "tokens": [["\u0120South", "\u0120Sudan"]], "entropy": [[3.350177764892578, 0.31912147998809814]], "avg_entropy": [1.8346496224403381], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Al-Istiw\u0101\u02be\u012byah", "score_of_biggest_cluster": 0.5307472348213196, "score_of_first_answer": 0.5307472348213196, "max_score_of_first_answer": 0.930231511592865, "min_score_of_first_answer": 0.3028199374675751, "avg_score": 0.5307472348213196, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31370578706264496, "std_of_first_answer": 0.31370578706264496, "avg_entropy_of_first_answer": 1.8346496224403381, "lex_sim": 1.0}
{"question": "\"Who was known in the 1991 Gulf War as \"\"Stormin' Norman\"\"?\"", "answer": ["Norman Storm"], "scores": [[0.506910502910614, 4.480755524127744e-05]], "normalized_score": [0.004765860270708799], "tokens": [["\u0120Norman", "\u0120Storm"]], "entropy": [[3.194859743118286, 0.0733172744512558]], "avg_entropy": [1.634088508784771], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "General H. Norman Schwarzkopf Jr.", "score_of_biggest_cluster": 0.004765860270708799, "score_of_first_answer": 0.004765860270708799, "max_score_of_first_answer": 0.506910502910614, "min_score_of_first_answer": 4.480755524127744e-05, "avg_score": 0.004765860270708799, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25343284767768637, "std_of_first_answer": 0.25343284767768637, "avg_entropy_of_first_answer": 1.634088508784771, "lex_sim": 1.0}
{"question": "Theon Greyjoy, a character in the television series \u2018Game of Thrones\u2019, is played by the younger brother of which English singer?", "answer": ["Justin Tomlinson"], "scores": [[0.005082888528704643, 0.004263287875801325, 0.9530484676361084, 0.9999549388885498]], "normalized_score": [0.06741207838058472], "tokens": [["\u0120Justin", "\u0120Tom", "l", "inson"]], "entropy": [[4.610013008117676, 3.4440574645996094, 0.31778380274772644, 0.0005757446051575243]], "avg_entropy": [2.0931075050175423], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lily Loves", "score_of_biggest_cluster": 0.06741207838058472, "score_of_first_answer": 0.06741207838058472, "max_score_of_first_answer": 0.9999549388885498, "min_score_of_first_answer": 0.004263287875801325, "avg_score": 0.06741207838058472, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.486197311141495, "std_of_first_answer": 0.486197311141495, "avg_entropy_of_first_answer": 2.0931075050175423, "lex_sim": 1.0}
{"question": "Which author's second husband was archaeologist Max Mallowan?", "answer": ["J Harmsworth Mallowan"], "scores": [[0.020957963541150093, 0.0006552103441208601, 0.023634349927306175, 0.9400082230567932, 0.0012089282972738147, 0.7631365656852722, 0.9632026553153992]], "normalized_score": [0.04298369213938713], "tokens": [["\u0120J", "\u0120Har", "ms", "worth", "\u0120M", "allow", "an"]], "entropy": [[5.880926609039307, 2.2221899032592773, 3.0341739654541016, 0.45843505859375, 1.4012408256530762, 1.8157343864440918, 0.24942967295646667]], "avg_entropy": [2.151732917342867], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Christie Estate", "score_of_biggest_cluster": 0.04298369213938713, "score_of_first_answer": 0.04298369213938713, "max_score_of_first_answer": 0.9632026553153992, "min_score_of_first_answer": 0.0006552103441208601, "avg_score": 0.04298369213938713, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4380839220522056, "std_of_first_answer": 0.4380839220522056, "avg_entropy_of_first_answer": 2.151732917342867, "lex_sim": 1.0}
{"question": "Who played Beverley Hills hairdresser George Roundy in the 1975 film \u2018Shampoo\u2019?", "answer": ["George Raft"], "scores": [[0.1598905473947525, 0.02855728566646576, 0.9656900763511658]], "normalized_score": [0.1639806479215622], "tokens": [["\u0120George", "\u0120Ra", "ft"]], "entropy": [[4.939273834228516, 2.8964319229125977, 0.2762726843357086]], "avg_entropy": [2.7039928138256073], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Warren Beatty", "score_of_biggest_cluster": 0.1639806479215622, "score_of_first_answer": 0.1639806479215622, "max_score_of_first_answer": 0.9656900763511658, "min_score_of_first_answer": 0.02855728566646576, "avg_score": 0.1639806479215622, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41429715233272707, "std_of_first_answer": 0.41429715233272707, "avg_entropy_of_first_answer": 2.7039928138256073, "lex_sim": 1.0}
{"question": "What video game series features an archaeologist named Lara Croft?", "answer": ["Tomb Raider"], "scores": [[0.7876698970794678, 0.947653591632843]], "normalized_score": [0.8639665842056274], "tokens": [["\u0120Tomb", "\u0120Raider"]], "entropy": [[1.5370923280715942, 0.3691707253456116]], "avg_entropy": [0.9531315267086029], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tomb Raider Series", "score_of_biggest_cluster": 0.8639665842056274, "score_of_first_answer": 0.8639665842056274, "max_score_of_first_answer": 0.947653591632843, "min_score_of_first_answer": 0.7876698970794678, "avg_score": 0.8639665842056274, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.07999184727668762, "std_of_first_answer": 0.07999184727668762, "avg_entropy_of_first_answer": 0.9531315267086029, "lex_sim": 1.0}
{"question": "In the 2005 remake of the film 'King Kong' who played the part of Ann Darrow, originally played by Fay Wray?", "answer": ["Naomi Watts"], "scores": [[0.40895360708236694, 0.9262701272964478]], "normalized_score": [0.6154685020446777], "tokens": [["\u0120Naomi", "\u0120Watts"]], "entropy": [[3.890519380569458, 0.5637678503990173]], "avg_entropy": [2.2271436154842377], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Naomi Watts", "score_of_biggest_cluster": 0.6154685020446777, "score_of_first_answer": 0.6154685020446777, "max_score_of_first_answer": 0.9262701272964478, "min_score_of_first_answer": 0.40895360708236694, "avg_score": 0.6154685020446777, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2586582601070404, "std_of_first_answer": 0.2586582601070404, "avg_entropy_of_first_answer": 2.2271436154842377, "lex_sim": 1.0}
{"question": "Herb of Grace is the name given to which strong- smelling herb?", "answer": ["Bayberry"], "scores": [[0.006258203648030758, 0.14989878237247467]], "normalized_score": [0.030628375709056854], "tokens": [["\u0120Bay", "berry"]], "entropy": [[5.305863380432129, 3.0032355785369873]], "avg_entropy": [4.154549479484558], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rue", "score_of_biggest_cluster": 0.030628375709056854, "score_of_first_answer": 0.030628375709056854, "max_score_of_first_answer": 0.14989878237247467, "min_score_of_first_answer": 0.006258203648030758, "avg_score": 0.030628375709056854, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07182028936222196, "std_of_first_answer": 0.07182028936222196, "avg_entropy_of_first_answer": 4.154549479484558, "lex_sim": 1.0}
{"question": "Who played the sadistic sheriff in the Clint Eastwood directed film Unforgiven?", "answer": ["\u0e19\u0e48\u0e32\u0e02\u0e19\u0e25\u0e38\u0e22"], "scores": [[0.0009424056042917073, 0.00018983305199071765, 0.046104345470666885, 0.17260560393333435, 0.17539721727371216, 0.9495640397071838, 0.9727169871330261, 0.6790838241577148, 0.02238035388290882, 0.9627748131752014, 0.0893704742193222, 0.9209844470024109, 0.8520219922065735, 0.9934031367301941, 0.995522141456604, 0.9793353080749512, 0.2615905702114105]], "normalized_score": [0.16681241989135742], "tokens": [["\u0120", "\u00e0\u00b8", "\u013b", "\u00e0\u00b9", "\u012a", "\u00e0\u00b8", "\u00b2", "\u00e0\u00b8", "\u0124", "\u00e0\u00b8", "\u013b", "\u00e0\u00b8", "\u00a5", "\u00e0\u00b8", "\u00b8", "\u00e0\u00b8", "\u00a2"]], "entropy": [[4.31059455871582, 5.490172386169434, 3.6753029823303223, 1.2709797620773315, 1.9003266096115112, 0.350208044052124, 0.19252724945545197, 1.0350675582885742, 2.8866450786590576, 0.18576686084270477, 2.567586660385132, 0.37328124046325684, 0.8056116104125977, 0.04090230539441109, 0.03883149102330208, 0.10097108781337738, 0.8195522427558899]], "avg_entropy": [1.5320192781441353], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gene hackman", "score_of_biggest_cluster": 0.16681241989135742, "score_of_first_answer": 0.16681241989135742, "max_score_of_first_answer": 0.995522141456604, "min_score_of_first_answer": 0.00018983305199071765, "avg_score": 0.16681241989135742, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42310750920896584, "std_of_first_answer": 0.42310750920896584, "avg_entropy_of_first_answer": 1.5320192781441353, "lex_sim": 0.0}
{"question": "In what decade was the Scottish National Party founded?", "answer": ["1960s"], "scores": [[0.034958723932504654, 0.3501757085323334]], "normalized_score": [0.1106422021985054], "tokens": [["\u01201960", "s"]], "entropy": [[5.205141067504883, 1.4379581212997437]], "avg_entropy": [3.3215495944023132], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1930\u2019s", "score_of_biggest_cluster": 0.1106422021985054, "score_of_first_answer": 0.1106422021985054, "max_score_of_first_answer": 0.3501757085323334, "min_score_of_first_answer": 0.034958723932504654, "avg_score": 0.1106422021985054, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.15760849229991436, "std_of_first_answer": 0.15760849229991436, "avg_entropy_of_first_answer": 3.3215495944023132, "lex_sim": 1.0}
{"question": "\"What is another name for \"\"mother of pearl\"\"?\"", "answer": ["\"mother-of-pearl\""], "scores": [[0.01954626850783825, 0.14691488444805145, 0.269113689661026, 0.9609676003456116, 0.9567931294441223, 0.9130169749259949, 0.989940345287323, 0.7550534009933472]], "normalized_score": [0.3852185904979706], "tokens": [["\u0120\"", "mother", "-", "of", "-", "pe", "arl", "\""]], "entropy": [[5.107641220092773, 5.052488803863525, 1.8735005855560303, 0.376024454832077, 0.36326298117637634, 0.7975449562072754, 0.06049060449004173, 1.3962993621826172]], "avg_entropy": [1.8784066210500896], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mother of pearl", "score_of_biggest_cluster": 0.3852185904979706, "score_of_first_answer": 0.3852185904979706, "max_score_of_first_answer": 0.989940345287323, "min_score_of_first_answer": 0.01954626850783825, "avg_score": 0.3852185904979706, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3836972442697574, "std_of_first_answer": 0.3836972442697574, "avg_entropy_of_first_answer": 1.8784066210500896, "lex_sim": 1.0}
{"question": "\"What Canadian province's name is Latin for \"\"New Scotland\"\"?\"", "answer": ["ABQ"], "scores": [[0.00013209872122388333, 0.05525926873087883]], "normalized_score": [0.002701791236177087], "tokens": [["\u0120AB", "Q"]], "entropy": [[3.1036555767059326, 4.366410255432129]], "avg_entropy": [3.7350329160690308], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Colonial Nova Scotia", "score_of_biggest_cluster": 0.002701791236177087, "score_of_first_answer": 0.002701791236177087, "max_score_of_first_answer": 0.05525926873087883, "min_score_of_first_answer": 0.00013209872122388333, "avg_score": 0.002701791236177087, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.027563585004827473, "std_of_first_answer": 0.027563585004827473, "avg_entropy_of_first_answer": 3.7350329160690308, "lex_sim": 1.0}
{"question": "The Isle of Capri is close to which Italian resort?", "answer": ["Taormina"], "scores": [[0.025182271376252174, 0.9934908747673035, 0.9070824980735779]], "normalized_score": [0.28311869502067566], "tokens": [["\u0120Ta", "orm", "ina"]], "entropy": [[4.0144734382629395, 0.07235545665025711, 0.47727203369140625]], "avg_entropy": [1.5213669762015343], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sorrento", "score_of_biggest_cluster": 0.28311869502067566, "score_of_first_answer": 0.28311869502067566, "max_score_of_first_answer": 0.9934908747673035, "min_score_of_first_answer": 0.025182271376252174, "avg_score": 0.28311869502067566, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4375228208413524, "std_of_first_answer": 0.4375228208413524, "avg_entropy_of_first_answer": 1.5213669762015343, "lex_sim": 1.0}
{"question": "Which Commonwealth country was the first outside the UK to broadcast Dr Who?", "answer": ["Australia"], "scores": [[0.46802955865859985]], "normalized_score": [0.46802955865859985], "tokens": [["\u0120Australia"]], "entropy": [[2.5285754203796387]], "avg_entropy": [2.5285754203796387], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "N Z", "score_of_biggest_cluster": 0.46802955865859985, "score_of_first_answer": 0.46802955865859985, "max_score_of_first_answer": 0.46802955865859985, "min_score_of_first_answer": 0.46802955865859985, "avg_score": 0.46802955865859985, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.5285754203796387, "lex_sim": 1.0}
{"question": "Whose second 'breadfruit voyage' as captain was on the ship HMS Providence, 1791-94?", "answer": ["Capt."], "scores": [[0.007603313308209181, 0.6309405565261841, 0.0323253832757473, 0.2572387456893921]], "normalized_score": [0.07947266101837158], "tokens": [["\u0120Capt", ".", "\u0120Samuel", "\u0120Hood"]], "entropy": [[3.861598014831543, 2.395613670349121, 3.75759220123291, 2.2779459953308105]], "avg_entropy": [3.073187470436096], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Governor Bligh", "score_of_biggest_cluster": 0.07947266101837158, "score_of_first_answer": 0.07947266101837158, "max_score_of_first_answer": 0.6309405565261841, "min_score_of_first_answer": 0.007603313308209181, "avg_score": 0.07947266101837158, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25000719218239054, "std_of_first_answer": 0.25000719218239054, "avg_entropy_of_first_answer": 3.073187470436096, "lex_sim": 1.0}
{"question": "Technically lugumes/beans, cereals and nuts are defined as what category of foodstuff?", "answer": ["Grains"], "scores": [[0.06372393667697906, 0.994109034538269]], "normalized_score": [0.2516913414001465], "tokens": [["\u0120Gr", "ains"]], "entropy": [[5.631701469421387, 0.055521730333566666]], "avg_entropy": [2.8436115998774767], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Edible seed", "score_of_biggest_cluster": 0.2516913414001465, "score_of_first_answer": 0.2516913414001465, "max_score_of_first_answer": 0.994109034538269, "min_score_of_first_answer": 0.06372393667697906, "avg_score": 0.2516913414001465, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.465192548930645, "std_of_first_answer": 0.465192548930645, "avg_entropy_of_first_answer": 2.8436115998774767, "lex_sim": 1.0}
{"question": "What is identified by an I. S. B. N.?", "answer": ["IDS BODIES, UNREFERRED INFORMATION"], "scores": [[0.0008076251251623034, 0.03355685994029045, 0.09352296590805054, 0.000987717299722135, 0.09200963377952576, 0.011480158194899559, 0.00301789166405797, 0.0032648651394993067, 0.04295288026332855, 0.14728334546089172, 0.010810241103172302]], "normalized_score": [0.012993190437555313], "tokens": [["\u0120ID", "S", "\u0120B", "OD", "IES", ",", "\u0120UN", "RE", "FER", "RED", "\u0120INFORMATION"]], "entropy": [[4.18062686920166, 4.164878845214844, 4.700348854064941, 4.022638320922852, 3.89513897895813, 2.6101608276367188, 5.122574329376221, 3.610654354095459, 2.9474987983703613, 1.7089273929595947, 3.7606616020202637]], "avg_entropy": [3.7021917429837314], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Booke", "score_of_biggest_cluster": 0.012993190437555313, "score_of_first_answer": 0.012993190437555313, "max_score_of_first_answer": 0.14728334546089172, "min_score_of_first_answer": 0.0008076251251623034, "avg_score": 0.012993190437555313, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.047261575276085065, "std_of_first_answer": 0.047261575276085065, "avg_entropy_of_first_answer": 3.7021917429837314, "lex_sim": 1.0}
{"question": "By what score did GB beat Belgium to win the Davis Cup in November 2015?", "answer": ["2 Sets and 5/1"], "scores": [[0.025035377591848373, 0.0009255311451852322, 0.12117335200309753, 0.2196928858757019, 0.022655732929706573, 0.03119092807173729]], "normalized_score": [0.027535676956176758], "tokens": [["\u01202", "\u0120Sets", "\u0120and", "\u01205", "/", "1"]], "entropy": [[4.6513166427612305, 2.7074532508850098, 2.988654613494873, 2.9936561584472656, 3.704667568206787, 2.4783496856689453]], "avg_entropy": [3.254016319910685], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "3-1", "score_of_biggest_cluster": 0.027535676956176758, "score_of_first_answer": 0.027535676956176758, "max_score_of_first_answer": 0.2196928858757019, "min_score_of_first_answer": 0.0009255311451852322, "avg_score": 0.027535676956176758, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07699276975991183, "std_of_first_answer": 0.07699276975991183, "avg_entropy_of_first_answer": 3.254016319910685, "lex_sim": 1.0}
{"question": "Andy Schleck competes in what sport at an international level?", "answer": ["Giro d'Italia"], "scores": [[0.0027146644424647093, 0.371665894985199, 0.6110313534736633, 0.8799826502799988, 0.9526219367980957, 0.9901171326637268]], "normalized_score": [0.2828148901462555], "tokens": [["\u0120G", "iro", "\u0120d", "'", "It", "alia"]], "entropy": [[4.1771464347839355, 4.239575386047363, 1.8472847938537598, 0.9802371263504028, 0.265363872051239, 0.08642169088125229]], "avg_entropy": [1.9326715506613255], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cyclist", "score_of_biggest_cluster": 0.2828148901462555, "score_of_first_answer": 0.2828148901462555, "max_score_of_first_answer": 0.9901171326637268, "min_score_of_first_answer": 0.0027146644424647093, "avg_score": 0.2828148901462555, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3551318730349169, "std_of_first_answer": 0.3551318730349169, "avg_entropy_of_first_answer": 1.9326715506613255, "lex_sim": 1.0}
{"question": "Who is the father of the twin boys born to Brooke Mueller on 14 March 2009?", "answer": ["Scott"], "scores": [[0.015288307331502438]], "normalized_score": [0.015288308262825012], "tokens": [["\u0120Scott"]], "entropy": [[6.326981544494629]], "avg_entropy": [6.326981544494629], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bi-winning", "score_of_biggest_cluster": 0.015288308262825012, "score_of_first_answer": 0.015288308262825012, "max_score_of_first_answer": 0.015288307331502438, "min_score_of_first_answer": 0.015288307331502438, "avg_score": 0.015288308262825012, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.326981544494629, "lex_sim": 1.0}
{"question": "The Salt Flat of Uyuni, the world's largest salt flat, is in which country?", "answer": ["Bolivia"], "scores": [[0.8494355082511902]], "normalized_score": [0.8494355082511902], "tokens": [["\u0120Bolivia"]], "entropy": [[0.8110199570655823]], "avg_entropy": [0.8110199570655823], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bolivia, Plurinational State of", "score_of_biggest_cluster": 0.8494355082511902, "score_of_first_answer": 0.8494355082511902, "max_score_of_first_answer": 0.8494355082511902, "min_score_of_first_answer": 0.8494355082511902, "avg_score": 0.8494355082511902, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.8110199570655823, "lex_sim": 1.0}
{"question": "What was Shakespeare's shortest play - written in 1594?", "answer": ["MIDSUMMER"], "scores": [[0.006162088830024004, 0.0019706827588379383, 0.9317237734794617, 0.9731788635253906]], "normalized_score": [0.05760447308421135], "tokens": [["\u0120M", "IDS", "UM", "MER"]], "entropy": [[4.456356048583984, 0.8379549980163574, 0.4733690023422241, 0.1961570531129837]], "avg_entropy": [1.4909592755138874], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Comedie of Errors", "score_of_biggest_cluster": 0.05760447308421135, "score_of_first_answer": 0.05760447308421135, "max_score_of_first_answer": 0.9731788635253906, "min_score_of_first_answer": 0.0019706827588379383, "avg_score": 0.05760447308421135, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.47442123339268166, "std_of_first_answer": 0.47442123339268166, "avg_entropy_of_first_answer": 1.4909592755138874, "lex_sim": 1.0}
{"question": "If Aman handles military intelligence and Shin Bet handles internal security, which agency is responsible for the overseas intelligence work of its country?", "answer": ["Mossad"], "scores": [[0.6628711819648743, 0.9998804330825806]], "normalized_score": [0.814120352268219], "tokens": [["\u0120Moss", "ad"]], "entropy": [[2.1883974075317383, 0.001774464501067996]], "avg_entropy": [1.0950859360164031], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mosaad", "score_of_biggest_cluster": 0.814120352268219, "score_of_first_answer": 0.814120352268219, "max_score_of_first_answer": 0.9998804330825806, "min_score_of_first_answer": 0.6628711819648743, "avg_score": 0.814120352268219, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.16850462555885315, "std_of_first_answer": 0.16850462555885315, "avg_entropy_of_first_answer": 1.0950859360164031, "lex_sim": 1.0}
{"question": "The River Jordan flows into which sea?", "answer": ["Mediterranean"], "scores": [[0.2052486687898636]], "normalized_score": [0.2052486687898636], "tokens": [["\u0120Mediterranean"]], "entropy": [[2.1708872318267822]], "avg_entropy": [2.1708872318267822], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Life in the Dead Sea", "score_of_biggest_cluster": 0.2052486687898636, "score_of_first_answer": 0.2052486687898636, "max_score_of_first_answer": 0.2052486687898636, "min_score_of_first_answer": 0.2052486687898636, "avg_score": 0.2052486687898636, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.1708872318267822, "lex_sim": 1.0}
{"question": "Which writer and politician became Lord Tweedsmuir?", "answer": ["JK Rowling"], "scores": [[0.010848112404346466, 0.0240317415446043, 0.5532995462417603]], "normalized_score": [0.052444491535425186], "tokens": [["\u0120J", "K", "\u0120Rowling"]], "entropy": [[4.7424421310424805, 2.027285099029541, 3.0610337257385254]], "avg_entropy": [3.2769203186035156], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Scantlebury Blenkiron", "score_of_biggest_cluster": 0.052444491535425186, "score_of_first_answer": 0.052444491535425186, "max_score_of_first_answer": 0.5532995462417603, "min_score_of_first_answer": 0.010848112404346466, "avg_score": 0.052444491535425186, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25266397871009116, "std_of_first_answer": 0.25266397871009116, "avg_entropy_of_first_answer": 3.2769203186035156, "lex_sim": 1.0}
{"question": "Which character created by Keith Waterhouse was played on film by Tom Courtneyand on TV by Jeff Rawle?", "answer": ["Florrie Fisher"], "scores": [[0.002459301147609949, 0.36914122104644775, 0.03381827101111412]], "normalized_score": [0.031312551349401474], "tokens": [["\u0120Flor", "rie", "\u0120Fisher"]], "entropy": [[6.330960750579834, 1.509985327720642, 5.2439494132995605]], "avg_entropy": [4.361631830533345], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "'BILLY LIAR'", "score_of_biggest_cluster": 0.031312551349401474, "score_of_first_answer": 0.031312551349401474, "max_score_of_first_answer": 0.36914122104644775, "min_score_of_first_answer": 0.002459301147609949, "avg_score": 0.031312551349401474, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1659586615837399, "std_of_first_answer": 0.1659586615837399, "avg_entropy_of_first_answer": 4.361631830533345, "lex_sim": 1.0}
{"question": "Who did US Forbes magazine rank in 2010 as the most powerful woman in the world?", "answer": ["Angela Merkel"], "scores": [[0.04741288349032402, 0.9334610104560852]], "normalized_score": [0.2103760540485382], "tokens": [["\u0120Angela", "\u0120Merkel"]], "entropy": [[2.982394218444824, 0.6673967838287354]], "avg_entropy": [1.8248955011367798], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "First Lady Obama", "score_of_biggest_cluster": 0.2103760540485382, "score_of_first_answer": 0.2103760540485382, "max_score_of_first_answer": 0.9334610104560852, "min_score_of_first_answer": 0.04741288349032402, "avg_score": 0.2103760540485382, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4430240634828806, "std_of_first_answer": 0.4430240634828806, "avg_entropy_of_first_answer": 1.8248955011367798, "lex_sim": 1.0}
{"question": "In cricket what is the term used for a ball bowled as an off break but designed to make the batsman think it's a leg break?", "answer": ["No ball"], "scores": [[0.013147908262908459, 0.28631478548049927]], "normalized_score": [0.061355046927928925], "tokens": [["\u0120No", "\u0120ball"]], "entropy": [[6.695001602172852, 3.1710658073425293]], "avg_entropy": [4.93303370475769], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wrong 'un", "score_of_biggest_cluster": 0.061355046927928925, "score_of_first_answer": 0.061355046927928925, "max_score_of_first_answer": 0.28631478548049927, "min_score_of_first_answer": 0.013147908262908459, "avg_score": 0.061355046927928925, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1365834386087954, "std_of_first_answer": 0.1365834386087954, "avg_entropy_of_first_answer": 4.93303370475769, "lex_sim": 1.0}
{"question": "\"Who according to the Book of Genesis was described as \"\"A mighty hunter before the Lord\"\"?\"", "answer": ["A great hunter among the mountains"], "scores": [[0.009393573738634586, 0.0033056975807994604, 0.8125689029693604, 0.0011289023095741868, 0.6492517590522766, 0.002123965648934245]], "normalized_score": [0.018437210470438004], "tokens": [["\u0120A", "\u0120great", "\u0120hunter", "\u0120among", "\u0120the", "\u0120mountains"]], "entropy": [[3.9096455574035645, 4.475287437438965, 1.4295876026153564, 1.9553704261779785, 1.8197524547576904, 4.773984909057617]], "avg_entropy": [3.060604731241862], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nimrod (computing)", "score_of_biggest_cluster": 0.018437210470438004, "score_of_first_answer": 0.018437210470438004, "max_score_of_first_answer": 0.8125689029693604, "min_score_of_first_answer": 0.0011289023095741868, "avg_score": 0.018437210470438004, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3459123875531729, "std_of_first_answer": 0.3459123875531729, "avg_entropy_of_first_answer": 3.060604731241862, "lex_sim": 1.0}
{"question": "Who wrote the 1984 Booker Prize winning novel 'Hotel Du Lac'?", "answer": ["Michel Duran (novelist)"], "scores": [[0.0020555059891194105, 0.004510466009378433, 0.563894510269165, 0.001423861482180655, 0.019107991829514503, 0.990276575088501, 0.7543448209762573, 0.8407908082008362]], "normalized_score": [0.05544715374708176], "tokens": [["\u0120Michel", "\u0120Dur", "an", "\u0120(", "no", "vel", "ist", ")"]], "entropy": [[4.883781433105469, 3.4612386226654053, 1.7548457384109497, 2.0232887268066406, 6.216324329376221, 0.10443481057882309, 1.0497716665267944, 0.9800373315811157]], "avg_entropy": [2.5592153323814273], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Anita Brookner", "score_of_biggest_cluster": 0.05544715374708176, "score_of_first_answer": 0.05544715374708176, "max_score_of_first_answer": 0.990276575088501, "min_score_of_first_answer": 0.001423861482180655, "avg_score": 0.05544715374708176, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4052450636679223, "std_of_first_answer": 0.4052450636679223, "avg_entropy_of_first_answer": 2.5592153323814273, "lex_sim": 1.0}
{"question": "What is the common name for Hydrocephalus, a pathological condition whereby fluid is retained within the skull?", "answer": ["Aqueductal"], "scores": [[0.0320524126291275, 0.7875921726226807, 0.9831675887107849, 0.5011810660362244]], "normalized_score": [0.33396127820014954], "tokens": [["\u0120Aqu", "ed", "uct", "al"]], "entropy": [[5.002780914306641, 1.4059967994689941, 0.17512154579162598, 1.9454010725021362]], "avg_entropy": [2.1323250830173492], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hydrocephalic", "score_of_biggest_cluster": 0.33396127820014954, "score_of_first_answer": 0.33396127820014954, "max_score_of_first_answer": 0.9831675887107849, "min_score_of_first_answer": 0.0320524126291275, "avg_score": 0.33396127820014954, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3577825958198587, "std_of_first_answer": 0.3577825958198587, "avg_entropy_of_first_answer": 2.1323250830173492, "lex_sim": 1.0}
{"question": "What is the name of the first digital library founded by Michael Hart in 1971?", "answer": ["World Wide Web Consortium"], "scores": [[0.014289402402937412, 0.20146113634109497, 0.8731421828269958, 0.009533509612083435]], "normalized_score": [0.06996579468250275], "tokens": [["\u0120World", "\u0120Wide", "\u0120Web", "\u0120Consortium"]], "entropy": [[6.1402082443237305, 3.574566602706909, 1.0115020275115967, 0.6874468326568604]], "avg_entropy": [2.853430926799774], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gutenburg Project", "score_of_biggest_cluster": 0.06996579468250275, "score_of_first_answer": 0.06996579468250275, "max_score_of_first_answer": 0.8731421828269958, "min_score_of_first_answer": 0.009533509612083435, "avg_score": 0.06996579468250275, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3541270567806523, "std_of_first_answer": 0.3541270567806523, "avg_entropy_of_first_answer": 2.853430926799774, "lex_sim": 1.0}
{"question": "Who is the valet to 'Phileas Fogg' in the novel 'Around The World In Eighty Days'?", "answer": ["Phileas Fogg"], "scores": [[0.0032171907369047403, 0.8771473169326782, 0.9951289892196655, 0.810598611831665, 0.9993401169776917]], "normalized_score": [0.2960667014122009], "tokens": [["\u0120Ph", "ile", "as", "\u0120F", "ogg"]], "entropy": [[4.149331569671631, 0.7451825737953186, 0.05037418752908707, 1.3884179592132568, 0.009164135903120041]], "avg_entropy": [1.2684940852224826], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jean Passepartout", "score_of_biggest_cluster": 0.2960667014122009, "score_of_first_answer": 0.2960667014122009, "max_score_of_first_answer": 0.9993401169776917, "min_score_of_first_answer": 0.0032171907369047403, "avg_score": 0.2960667014122009, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3738845040823813, "std_of_first_answer": 0.3738845040823813, "avg_entropy_of_first_answer": 1.2684940852224826, "lex_sim": 1.0}
{"question": "On the banks of which river does the town of Kidderminster stand?", "answer": ["The Wye"], "scores": [[0.41673240065574646, 0.03286813199520111, 0.5866081714630127]], "normalized_score": [0.20029041171073914], "tokens": [["\u0120The", "\u0120W", "ye"]], "entropy": [[3.1303906440734863, 2.8353042602539062, 1.5027178525924683]], "avg_entropy": [2.489470918973287], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "River Stour", "score_of_biggest_cluster": 0.20029041171073914, "score_of_first_answer": 0.20029041171073914, "max_score_of_first_answer": 0.5866081714630127, "min_score_of_first_answer": 0.03286813199520111, "avg_score": 0.20029041171073914, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23162171879824314, "std_of_first_answer": 0.23162171879824314, "avg_entropy_of_first_answer": 2.489470918973287, "lex_sim": 1.0}
{"question": "The documentary about the making of which 1970s groundbreaking film was titled Hearts of Darkness alluding not just to the novel that inspired it but also to it's chaotic production?", "answer": ["Apocalypse Now"], "scores": [[0.19071261584758759, 0.9324511885643005]], "normalized_score": [0.421699196100235], "tokens": [["\u0120Apocalypse", "\u0120Now"]], "entropy": [[5.103397846221924, 0.4008621573448181]], "avg_entropy": [2.752130001783371], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charlie don't surf!", "score_of_biggest_cluster": 0.421699196100235, "score_of_first_answer": 0.421699196100235, "max_score_of_first_answer": 0.9324511885643005, "min_score_of_first_answer": 0.19071261584758759, "avg_score": 0.421699196100235, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3708692863583565, "std_of_first_answer": 0.3708692863583565, "avg_entropy_of_first_answer": 2.752130001783371, "lex_sim": 1.0}
{"question": "What type of fish is used in the production of Worcestershire sauce?", "answer": ["Red Basso"], "scores": [[0.009892646223306656, 0.0030727279372513294, 0.00022429130331147462]], "normalized_score": [0.0018961947644129395], "tokens": [["\u0120Red", "\u0120Bass", "o"]], "entropy": [[4.223984718322754, 4.2505364418029785, 0.40006083250045776]], "avg_entropy": [2.9581939975420632], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Anchovy", "score_of_biggest_cluster": 0.0018961947644129395, "score_of_first_answer": 0.0018961947644129395, "max_score_of_first_answer": 0.009892646223306656, "min_score_of_first_answer": 0.00022429130331147462, "avg_score": 0.0018961947644129395, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0040565716716491505, "std_of_first_answer": 0.0040565716716491505, "avg_entropy_of_first_answer": 2.9581939975420632, "lex_sim": 1.0}
{"question": "What is the stage name of American singer/songwriter Katheryn Elizabeth Hudson?", "answer": ["Katheryn Elizabeth Hudson"], "scores": [[0.2712175250053406, 0.8801316022872925, 0.9897581338882446, 0.39434078335762024, 0.7246817946434021]], "normalized_score": [0.5832875370979309], "tokens": [["\u0120K", "ather", "yn", "\u0120Elizabeth", "\u0120Hudson"]], "entropy": [[4.77423095703125, 0.8877906799316406, 0.07579343020915985, 3.137942314147949, 2.2515993118286133]], "avg_entropy": [2.2254713386297227], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Katheryn Perry", "score_of_biggest_cluster": 0.5832875370979309, "score_of_first_answer": 0.5832875370979309, "max_score_of_first_answer": 0.9897581338882446, "min_score_of_first_answer": 0.2712175250053406, "avg_score": 0.5832875370979309, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27669144911059435, "std_of_first_answer": 0.27669144911059435, "avg_entropy_of_first_answer": 2.2254713386297227, "lex_sim": 1.0}
{"question": "Which geological period is named after an ancient tribe which once inhabited South Wales?", "answer": ["Cambrian"], "scores": [[0.2206139713525772, 0.9896864295005798]], "normalized_score": [0.4672672152519226], "tokens": [["\u0120Camb", "rian"]], "entropy": [[4.126986026763916, 0.07733036577701569]], "avg_entropy": [2.102158196270466], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Silurian period", "score_of_biggest_cluster": 0.4672672152519226, "score_of_first_answer": 0.4672672152519226, "max_score_of_first_answer": 0.9896864295005798, "min_score_of_first_answer": 0.2206139713525772, "avg_score": 0.4672672152519226, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3845362290740013, "std_of_first_answer": 0.3845362290740013, "avg_entropy_of_first_answer": 2.102158196270466, "lex_sim": 1.0}
{"question": "What type of scenes was Dame Laura Knight noted for painting", "answer": ["The Italian Renaissance"], "scores": [[0.023132437840104103, 0.005992115009576082, 0.14967292547225952]], "normalized_score": [0.027477776631712914], "tokens": [["\u0120The", "\u0120Italian", "\u0120Renaissance"]], "entropy": [[6.901566505432129, 7.4196367263793945, 4.0407867431640625]], "avg_entropy": [6.120663324991862], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Circus ring", "score_of_biggest_cluster": 0.027477776631712914, "score_of_first_answer": 0.027477776631712914, "max_score_of_first_answer": 0.14967292547225952, "min_score_of_first_answer": 0.005992115009576082, "avg_score": 0.027477776631712914, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06407500897137143, "std_of_first_answer": 0.06407500897137143, "avg_entropy_of_first_answer": 6.120663324991862, "lex_sim": 1.0}
{"question": "Who taught Eliza Dolittle to be a lady ?", "answer": ["John Hunter"], "scores": [[0.04538939520716667, 0.004922300111502409]], "normalized_score": [0.014947249554097652], "tokens": [["\u0120John", "\u0120Hunter"]], "entropy": [[5.134434700012207, 5.368830680847168]], "avg_entropy": [5.2516326904296875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Professor Henry Higgins", "score_of_biggest_cluster": 0.014947249554097652, "score_of_first_answer": 0.014947249554097652, "max_score_of_first_answer": 0.04538939520716667, "min_score_of_first_answer": 0.004922300111502409, "avg_score": 0.014947249554097652, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02023354754783213, "std_of_first_answer": 0.02023354754783213, "avg_entropy_of_first_answer": 5.2516326904296875, "lex_sim": 1.0}
{"question": "By what name is singer 'Anthony Dominic Benevetto' better known?", "answer": ["Anthony Beneditto"], "scores": [[0.2502670884132385, 0.057765811681747437, 0.004679536446928978]], "normalized_score": [0.04074670374393463], "tokens": [["\u0120Anthony", "\u0120Bened", "itto"]], "entropy": [[5.0743937492370605, 5.787699222564697, 0.945841908454895]], "avg_entropy": [3.9359782934188843], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Anthony Benedetto", "score_of_biggest_cluster": 0.04074670374393463, "score_of_first_answer": 0.04074670374393463, "max_score_of_first_answer": 0.2502670884132385, "min_score_of_first_answer": 0.004679536446928978, "avg_score": 0.04074670374393463, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1055083671470687, "std_of_first_answer": 0.1055083671470687, "avg_entropy_of_first_answer": 3.9359782934188843, "lex_sim": 1.0}
{"question": "Which stand-up comedian presents 8 Out Of 10 Cats and Distraction?", "answer": ["Jack Dee"], "scores": [[0.017106156796216965, 0.9590790867805481]], "normalized_score": [0.12808650732040405], "tokens": [["\u0120Jack", "\u0120Dee"]], "entropy": [[5.610354900360107, 0.3127000331878662]], "avg_entropy": [2.961527466773987], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jimmy Carr", "score_of_biggest_cluster": 0.12808650732040405, "score_of_first_answer": 0.12808650732040405, "max_score_of_first_answer": 0.9590790867805481, "min_score_of_first_answer": 0.017106156796216965, "avg_score": 0.12808650732040405, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.47098646499216557, "std_of_first_answer": 0.47098646499216557, "avg_entropy_of_first_answer": 2.961527466773987, "lex_sim": 1.0}
{"question": "In which year was the \u00a32 coin issued and introduced in the UK, although minted in the previous year?", "answer": ["2010"], "scores": [[0.0007805756758898497]], "normalized_score": [0.0007805756176821887], "tokens": [["\u01202010"]], "entropy": [[4.771306991577148]], "avg_entropy": [4.771306991577148], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1998", "score_of_biggest_cluster": 0.0007805756176821887, "score_of_first_answer": 0.0007805756176821887, "max_score_of_first_answer": 0.0007805756758898497, "min_score_of_first_answer": 0.0007805756758898497, "avg_score": 0.0007805756176821887, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.771306991577148, "lex_sim": 1.0}
{"question": "Cursed to sail around the Cape of Good Hope, which ghost ship is the theme of an 1841 opera by Richard Wagner?", "answer": ["Flying Dutchman"], "scores": [[0.29085931181907654, 0.9835264086723328, 0.9975364208221436]], "normalized_score": [0.6583638191223145], "tokens": [["\u0120Flying", "\u0120Dutch", "man"]], "entropy": [[3.958635091781616, 0.1670864373445511, 0.021974381059408188]], "avg_entropy": [1.382565303395192], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Flying Dutchman", "score_of_biggest_cluster": 0.6583638191223145, "score_of_first_answer": 0.6583638191223145, "max_score_of_first_answer": 0.9975364208221436, "min_score_of_first_answer": 0.29085931181907654, "avg_score": 0.6583638191223145, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3298781801905547, "std_of_first_answer": 0.3298781801905547, "avg_entropy_of_first_answer": 1.382565303395192, "lex_sim": 1.0}
{"question": "\"In \"\"The Hitchhikers Guide to the Galaxy\"\" by Douglas Adams, what was the name of the computer that determined that the answer to the ultimate question of Life, the Universe and Everything was 42?\"", "answer": ["Galaxy V.1.1"], "scores": [[0.002131163142621517, 0.0015599059406667948, 0.0637580007314682, 0.06334725767374039, 0.3072109818458557, 0.19456444680690765]], "normalized_score": [0.03048454411327839], "tokens": [["\u0120Galaxy", "\u0120V", ".", "1", ".", "1"]], "entropy": [[4.508397102355957, 3.998242139816284, 2.999420166015625, 3.451545238494873, 1.4679253101348877, 2.8645379543304443]], "avg_entropy": [3.2150113185246787], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Deep Thought (disambiguation)", "score_of_biggest_cluster": 0.03048454411327839, "score_of_first_answer": 0.03048454411327839, "max_score_of_first_answer": 0.3072109818458557, "min_score_of_first_answer": 0.0015599059406667948, "avg_score": 0.03048454411327839, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11077580443673989, "std_of_first_answer": 0.11077580443673989, "avg_entropy_of_first_answer": 3.2150113185246787, "lex_sim": 1.0}
{"question": "Who was European Footballer of the Year for 1973 and 1974?", "answer": ["Pele"], "scores": [[0.011551890522241592, 0.9879269003868103]], "normalized_score": [0.1068289577960968], "tokens": [["\u0120Pe", "le"]], "entropy": [[4.924848556518555, 0.11115661263465881]], "avg_entropy": [2.5180025845766068], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hendrik Johannes Cruijff", "score_of_biggest_cluster": 0.1068289577960968, "score_of_first_answer": 0.1068289577960968, "max_score_of_first_answer": 0.9879269003868103, "min_score_of_first_answer": 0.011551890522241592, "avg_score": 0.1068289577960968, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.48818750493228436, "std_of_first_answer": 0.48818750493228436, "avg_entropy_of_first_answer": 2.5180025845766068, "lex_sim": 1.0}
{"question": "Where on the body would a Hindu woman wear a bindi?", "answer": ["Forehead"], "scores": [[0.028873169794678688, 0.987848162651062]], "normalized_score": [0.1688854843378067], "tokens": [["\u0120Fore", "head"]], "entropy": [[4.428485870361328, 0.09094132483005524]], "avg_entropy": [2.2597135975956917], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hairline", "score_of_biggest_cluster": 0.1688854843378067, "score_of_first_answer": 0.1688854843378067, "max_score_of_first_answer": 0.987848162651062, "min_score_of_first_answer": 0.028873169794678688, "avg_score": 0.1688854843378067, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.47948749642819166, "std_of_first_answer": 0.47948749642819166, "avg_entropy_of_first_answer": 2.2597135975956917, "lex_sim": 1.0}
{"question": "\"Who painted \"\"Girl before a Mirror\"\" ?\"", "answer": ["Mary Cassat"], "scores": [[0.024054422974586487, 0.7276379466056824, 0.11400847136974335]], "normalized_score": [0.1258971244096756], "tokens": [["\u0120Mary", "\u0120Cass", "at"]], "entropy": [[5.183555603027344, 2.1693599224090576, 0.783119797706604]], "avg_entropy": [2.712011774381002], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pablo Diego Jose Francisco de Paula Juan Nepomuceno Maria de los Remedios Cipriano de la Santisima Trinidad Clito Ruiz y Picasso", "score_of_biggest_cluster": 0.1258971244096756, "score_of_first_answer": 0.1258971244096756, "max_score_of_first_answer": 0.7276379466056824, "min_score_of_first_answer": 0.024054422974586487, "avg_score": 0.1258971244096756, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.31263444013489994, "std_of_first_answer": 0.31263444013489994, "avg_entropy_of_first_answer": 2.712011774381002, "lex_sim": 1.0}
{"question": "Which character was played by Linda Thorson in The Avengers?", "answer": ["Linda Thorson"], "scores": [[0.0059004356153309345, 0.5475867986679077, 0.9566107988357544, 0.9062547087669373]], "normalized_score": [0.23005445301532745], "tokens": [["\u0120Linda", "\u0120Th", "ors", "on"]], "entropy": [[6.355212211608887, 3.067497491836548, 0.22951900959014893, 0.39510422945022583]], "avg_entropy": [2.5118332356214523], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tara King", "score_of_biggest_cluster": 0.23005445301532745, "score_of_first_answer": 0.23005445301532745, "max_score_of_first_answer": 0.9566107988357544, "min_score_of_first_answer": 0.0059004356153309345, "avg_score": 0.23005445301532745, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37967018302808936, "std_of_first_answer": 0.37967018302808936, "avg_entropy_of_first_answer": 2.5118332356214523, "lex_sim": 1.0}
{"question": "Which former Soviet republic is the largest land-locked county in the world?", "answer": ["Kyrgyzstan"], "scores": [[0.05971544608473778, 0.9401044249534607, 0.952078104019165, 0.9485979080200195]], "normalized_score": [0.474519819021225], "tokens": [["\u0120Kyr", "gy", "z", "stan"]], "entropy": [[5.180010795593262, 0.3931664824485779, 0.23294144868850708, 0.3814617097377777]], "avg_entropy": [1.546895109117031], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Name of Kazakhstan", "score_of_biggest_cluster": 0.474519819021225, "score_of_first_answer": 0.474519819021225, "max_score_of_first_answer": 0.952078104019165, "min_score_of_first_answer": 0.05971544608473778, "avg_score": 0.474519819021225, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38419847723333367, "std_of_first_answer": 0.38419847723333367, "avg_entropy_of_first_answer": 1.546895109117031, "lex_sim": 1.0}
{"question": "\"Ravel's \"\"Bolero\"\", pop music riffs and (often) suspense music in film soundtracks are examples of what?\"", "answer": ["(Novel)"], "scores": [[0.003325418569147587, 0.007170254364609718, 0.03751247003674507, 0.07331865280866623]], "normalized_score": [0.016002677381038666], "tokens": [["\u0120(", "No", "vel", ")"]], "entropy": [[7.070932865142822, 6.809417724609375, 4.8439130783081055, 1.0203508138656616]], "avg_entropy": [4.936153620481491], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Riff driven", "score_of_biggest_cluster": 0.016002677381038666, "score_of_first_answer": 0.016002677381038666, "max_score_of_first_answer": 0.07331865280866623, "min_score_of_first_answer": 0.003325418569147587, "avg_score": 0.016002677381038666, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.028130203388611733, "std_of_first_answer": 0.028130203388611733, "avg_entropy_of_first_answer": 4.936153620481491, "lex_sim": 1.0}
{"question": "January 17, 1929 saw the pride of Chester, Il, Elzie Segar debut what iconic cartoon character in a Thimble Theater strip?", "answer": ["Popeye"], "scores": [[0.3265823721885681, 0.9973880648612976]], "normalized_score": [0.5707270503044128], "tokens": [["\u0120Pop", "eye"]], "entropy": [[4.119229316711426, 0.028333978727459908]], "avg_entropy": [2.073781647719443], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Popeye Cigarettes", "score_of_biggest_cluster": 0.5707270503044128, "score_of_first_answer": 0.5707270503044128, "max_score_of_first_answer": 0.9973880648612976, "min_score_of_first_answer": 0.3265823721885681, "avg_score": 0.5707270503044128, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33540284633636475, "std_of_first_answer": 0.33540284633636475, "avg_entropy_of_first_answer": 2.073781647719443, "lex_sim": 1.0}
{"question": "In 2009, a special design was used on the reverse of the 50p coin to commemorate the 250th anniversary of which establishment?", "answer": ["A&I Insurance"], "scores": [[0.0015474724350497127, 0.13142095506191254, 0.0016380333108827472, 0.012599864974617958]], "normalized_score": [0.008049038238823414], "tokens": [["\u0120A", "&", "I", "\u0120Insurance"]], "entropy": [[3.8928415775299072, 6.118151664733887, 2.9029128551483154, 4.584188461303711]], "avg_entropy": [4.374523639678955], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kew Botanic Gardens", "score_of_biggest_cluster": 0.008049038238823414, "score_of_first_answer": 0.008049038238823414, "max_score_of_first_answer": 0.13142095506191254, "min_score_of_first_answer": 0.0015474724350497127, "avg_score": 0.008049038238823414, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.054813037370764914, "std_of_first_answer": 0.054813037370764914, "avg_entropy_of_first_answer": 4.374523639678955, "lex_sim": 1.0}
{"question": "Which playwright and poet became President of his country in 1989?", "answer": ["Hugo Chavez"], "scores": [[0.002572887111455202, 0.2123628556728363]], "normalized_score": [0.02337489277124405], "tokens": [["\u0120Hugo", "\u0120Chavez"]], "entropy": [[5.760484218597412, 4.8764824867248535]], "avg_entropy": [5.318483352661133], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hollar, V\u00e1clav", "score_of_biggest_cluster": 0.02337489277124405, "score_of_first_answer": 0.02337489277124405, "max_score_of_first_answer": 0.2123628556728363, "min_score_of_first_answer": 0.002572887111455202, "avg_score": 0.02337489277124405, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10489498428069055, "std_of_first_answer": 0.10489498428069055, "avg_entropy_of_first_answer": 5.318483352661133, "lex_sim": 1.0}
{"question": "Which US No 1 single came from Diana Ross's platinum album Diana?", "answer": ["'Love's in Need of Love Today'"], "scores": [[0.001581914140842855, 0.03812430426478386, 0.014941616915166378, 0.22310878336429596, 0.8456467986106873, 0.9790331721305847, 0.9922980666160583, 0.940076470375061, 0.8236455917358398]], "normalized_score": [0.17143219709396362], "tokens": [["\u0120'", "Love", "'s", "\u0120in", "\u0120Need", "\u0120of", "\u0120Love", "\u0120Today", "'"]], "entropy": [[3.1908164024353027, 2.3250677585601807, 2.54526686668396, 3.2017524242401123, 0.9760839939117432, 0.154359370470047, 0.07168266177177429, 0.3713589906692505, 0.8625178933143616]], "avg_entropy": [1.5221007068951924], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Upside Down (Diana Ross song)", "score_of_biggest_cluster": 0.17143219709396362, "score_of_first_answer": 0.17143219709396362, "max_score_of_first_answer": 0.9922980666160583, "min_score_of_first_answer": 0.001581914140842855, "avg_score": 0.17143219709396362, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4280626862191779, "std_of_first_answer": 0.4280626862191779, "avg_entropy_of_first_answer": 1.5221007068951924, "lex_sim": 1.0}
{"question": "Who built the original Palace of Versailles that was enlarged by Louis XIV?", "answer": ["Louis XII"], "scores": [[0.09049775451421738, 0.017320243641734123]], "normalized_score": [0.03959095478057861], "tokens": [["\u0120Louis", "\u0120XII"]], "entropy": [[4.5809221267700195, 2.0603461265563965]], "avg_entropy": [3.320634126663208], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lewis XIII", "score_of_biggest_cluster": 0.03959095478057861, "score_of_first_answer": 0.03959095478057861, "max_score_of_first_answer": 0.09049775451421738, "min_score_of_first_answer": 0.017320243641734123, "avg_score": 0.03959095478057861, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.03658875543624163, "std_of_first_answer": 0.03658875543624163, "avg_entropy_of_first_answer": 3.320634126663208, "lex_sim": 1.0}
{"question": "What colour ink was traditionally used by Roman emperors to sign important documents?", "answer": ["Imperial Purple"], "scores": [[0.008517357520759106, 0.17591406404972076]], "normalized_score": [0.03870817646384239], "tokens": [["\u0120Imperial", "\u0120Purple"]], "entropy": [[3.578073024749756, 2.615795135498047]], "avg_entropy": [3.0969340801239014], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Purplingly", "score_of_biggest_cluster": 0.03870817646384239, "score_of_first_answer": 0.03870817646384239, "max_score_of_first_answer": 0.17591406404972076, "min_score_of_first_answer": 0.008517357520759106, "avg_score": 0.03870817646384239, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08369835326448083, "std_of_first_answer": 0.08369835326448083, "avg_entropy_of_first_answer": 3.0969340801239014, "lex_sim": 1.0}
{"question": "In what year in the 20th century was the Miss World contest held for the first time?", "answer": ["Miss World was founded in the 20th century"], "scores": [[0.004820231813937426, 0.8346760869026184, 0.19331714510917664, 0.06316422671079636, 0.8527233600616455, 0.056307077407836914, 0.09961686283349991, 0.9594143033027649, 0.9096802473068237]], "normalized_score": [0.18066778779029846], "tokens": [["\u0120Miss", "\u0120World", "\u0120was", "\u0120founded", "\u0120in", "\u0120the", "\u012020", "th", "\u0120century"]], "entropy": [[4.845660209655762, 1.2358849048614502, 4.406543731689453, 2.6818408966064453, 1.0061986446380615, 4.337431907653809, 2.884859561920166, 0.32023048400878906, 0.5084676146507263]], "avg_entropy": [2.4696797728538513], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and fifty-one", "score_of_biggest_cluster": 0.18066778779029846, "score_of_first_answer": 0.18066778779029846, "max_score_of_first_answer": 0.9594143033027649, "min_score_of_first_answer": 0.004820231813937426, "avg_score": 0.18066778779029846, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40439249707293606, "std_of_first_answer": 0.40439249707293606, "avg_entropy_of_first_answer": 2.4696797728538513, "lex_sim": 1.0}
{"question": "Cathay is a poetic name for which far eastern country?", "answer": ["China"], "scores": [[0.7940260171890259]], "normalized_score": [0.7940260171890259], "tokens": [["\u0120China"]], "entropy": [[1.6450538635253906]], "avg_entropy": [1.6450538635253906], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chinese People's Republic", "score_of_biggest_cluster": 0.7940260171890259, "score_of_first_answer": 0.7940260171890259, "max_score_of_first_answer": 0.7940260171890259, "min_score_of_first_answer": 0.7940260171890259, "avg_score": 0.7940260171890259, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.6450538635253906, "lex_sim": 1.0}
{"question": "\"\"\"You're Going To Lose That Girl\"\" was from which Beatles film in 1965?\"", "answer": ["Help!"], "scores": [[0.523686945438385, 0.6803246736526489]], "normalized_score": [0.5968895554542542], "tokens": [["\u0120Help", "!"]], "entropy": [[1.9075143337249756, 1.0412731170654297]], "avg_entropy": [1.4743937253952026], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "HELP!!", "score_of_biggest_cluster": 0.5968895554542542, "score_of_first_answer": 0.5968895554542542, "max_score_of_first_answer": 0.6803246736526489, "min_score_of_first_answer": 0.523686945438385, "avg_score": 0.5968895554542542, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.07831886410713196, "std_of_first_answer": 0.07831886410713196, "avg_entropy_of_first_answer": 1.4743937253952026, "lex_sim": 1.0}
{"question": "Queen Joan of Navarre, who was imprisoned for witchcraft, was the stepmother of which English monarch?", "answer": ["King Henry IV"], "scores": [[0.13252970576286316, 0.3900695741176605, 0.2700789272785187]], "normalized_score": [0.24079567193984985], "tokens": [["\u0120King", "\u0120Henry", "\u0120IV"]], "entropy": [[2.742863655090332, 1.6588634252548218, 2.178757429122925]], "avg_entropy": [2.1934948364893594], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Henry 5", "score_of_biggest_cluster": 0.24079567193984985, "score_of_first_answer": 0.24079567193984985, "max_score_of_first_answer": 0.3900695741176605, "min_score_of_first_answer": 0.13252970576286316, "avg_score": 0.24079567193984985, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10522163247199529, "std_of_first_answer": 0.10522163247199529, "avg_entropy_of_first_answer": 2.1934948364893594, "lex_sim": 1.0}
{"question": "Which Cricket county's ' Twenty / 20 ' team are known as the ' Foxes ' ?", "answer": ["Gloucestershire"], "scores": [[0.13591182231903076, 0.9290342330932617, 0.9987823367118835, 0.9961835741996765]], "normalized_score": [0.595353364944458], "tokens": [["\u0120Glou", "ces", "ters", "hire"]], "entropy": [[3.341240406036377, 0.3941844403743744, 0.013326208107173443, 0.03287861496210098]], "avg_entropy": [0.9454074173700064], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Leics", "score_of_biggest_cluster": 0.595353364944458, "score_of_first_answer": 0.595353364944458, "max_score_of_first_answer": 0.9987823367118835, "min_score_of_first_answer": 0.13591182231903076, "avg_score": 0.595353364944458, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.364266107717003, "std_of_first_answer": 0.364266107717003, "avg_entropy_of_first_answer": 0.9454074173700064, "lex_sim": 1.0}
{"question": "What is the only bird that can wink like a human", "answer": ["The American bittern"], "scores": [[0.1274934858083725, 0.002127865795046091, 0.007475381717085838, 0.9998517036437988]], "normalized_score": [0.03773551061749458], "tokens": [["\u0120The", "\u0120American", "\u0120bit", "tern"]], "entropy": [[4.77149772644043, 5.548936367034912, 4.924779891967773, 0.0021830375771969557]], "avg_entropy": [3.811849255755078], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "OWL", "score_of_biggest_cluster": 0.03773551061749458, "score_of_first_answer": 0.03773551061749458, "max_score_of_first_answer": 0.9998517036437988, "min_score_of_first_answer": 0.002127865795046091, "avg_score": 0.03773551061749458, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41618970742581485, "std_of_first_answer": 0.41618970742581485, "avg_entropy_of_first_answer": 3.811849255755078, "lex_sim": 1.0}
{"question": "Who were the Greek equivalents of the Norns of Norse mythology?", "answer": ["\u0386, \u03ac\u03c4\u03b9\u03c2\u03ac, \u03ac\u03c4\u03c1\u03b5\u03c2\u03ac\u03bd\u03bd\u03c5\u03b2\u03b7"], "scores": [[0.0006802096031606197, 0.024477319791913033, 0.01265699416399002, 0.7672916054725647, 0.0007723833550699055, 0.022001255303621292, 0.05366266146302223, 0.5439167022705078, 0.000518993241712451, 0.4550061523914337, 0.48967427015304565, 0.5794976353645325, 0.7133315205574036, 0.46964478492736816, 0.009201941080391407, 0.13887572288513184, 0.41151300072669983, 0.2963186800479889, 0.8256123661994934, 0.032566919922828674, 0.021964970976114273, 0.07772979140281677, 0.978488564491272, 0.01650424860417843, 0.360495388507843, 0.3286148011684418]], "normalized_score": [0.07937400788068771], "tokens": [["\u0120\u00ce", "\u0128", ",", "\u0120\u00ce", "\u00ac", "\u00cf\u0126", "\u00ce\u00b9", "\u00cf\u0124", "\u00ce", "\u00ac", ",", "\u0120\u00ce", "\u00ac", "\u00cf\u0126", "\u00cf\u0123", "\u00ce\u00b5", "\u00cf\u0124", "\u00ce", "\u00ac", "\u00ce\u00bd", "\u00ce\u00bd", "\u00cf", "\u0127", "\u00ce\u00b2", "\u00ce", "\u00b7"]], "entropy": [[5.028301239013672, 3.2758185863494873, 3.019570827484131, 1.4234874248504639, 1.2993742227554321, 2.4553847312927246, 2.5532238483428955, 1.7558879852294922, 1.4290351867675781, 2.2527334690093994, 2.5485334396362305, 1.8092827796936035, 1.531583547592163, 2.1774420738220215, 1.2692078351974487, 2.029109001159668, 2.2581722736358643, 2.6230692863464355, 0.8223446011543274, 2.391864538192749, 2.538821220397949, 2.4021992683410645, 0.15288934111595154, 2.2925333976745605, 1.3553998470306396, 1.447853684425354]], "avg_entropy": [2.0824278329427424], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fates (disambiguation)", "score_of_biggest_cluster": 0.07937400788068771, "score_of_first_answer": 0.07937400788068771, "max_score_of_first_answer": 0.978488564491272, "min_score_of_first_answer": 0.000518993241712451, "avg_score": 0.07937400788068771, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29887160714743427, "std_of_first_answer": 0.29887160714743427, "avg_entropy_of_first_answer": 2.0824278329427424, "lex_sim": 0.0}
{"question": "What were the names of the founder of British company JCB?", "answer": ["Henry and Joseph Chapman"], "scores": [[0.047234926372766495, 0.0762777328491211, 0.5229394435882568, 0.01502589974552393]], "normalized_score": [0.072943776845932], "tokens": [["\u0120Henry", "\u0120and", "\u0120Joseph", "\u0120Chapman"]], "entropy": [[4.063895225524902, 4.47953462600708, 2.517085075378418, 5.278790473937988]], "avg_entropy": [4.084826350212097], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "J C Bamford (person)", "score_of_biggest_cluster": 0.072943776845932, "score_of_first_answer": 0.072943776845932, "max_score_of_first_answer": 0.5229394435882568, "min_score_of_first_answer": 0.01502589974552393, "avg_score": 0.072943776845932, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.207576843093914, "std_of_first_answer": 0.207576843093914, "avg_entropy_of_first_answer": 4.084826350212097, "lex_sim": 1.0}
{"question": "Who did Johnnie Cochran defend in a famous 1990s trial?", "answer": ["Michael Jackson"], "scores": [[0.034421611577272415, 0.5460778474807739]], "normalized_score": [0.1371016949415207], "tokens": [["\u0120Michael", "\u0120Jackson"]], "entropy": [[3.620073080062866, 3.1649932861328125]], "avg_entropy": [3.3925331830978394], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Orenthal James", "score_of_biggest_cluster": 0.1371016949415207, "score_of_first_answer": 0.1371016949415207, "max_score_of_first_answer": 0.5460778474807739, "min_score_of_first_answer": 0.034421611577272415, "avg_score": 0.1371016949415207, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25582811795175076, "std_of_first_answer": 0.25582811795175076, "avg_entropy_of_first_answer": 3.3925331830978394, "lex_sim": 1.0}
{"question": "Wayne Rooney signed for Manchester United from which other Premiership Club?", "answer": ["Man Utd & Everton"], "scores": [[0.023517245426774025, 0.2637278139591217, 0.9972655773162842, 0.00032783084316179156, 0.4419148564338684]], "normalized_score": [0.06172602251172066], "tokens": [["\u0120Man", "\u0120Ut", "d", "\u0120&", "\u0120Everton"]], "entropy": [[2.18721079826355, 2.572244644165039, 0.023570671677589417, 0.6574950218200684, 2.9396274089813232]], "avg_entropy": [1.676029708981514], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Everton", "score_of_biggest_cluster": 0.06172602251172066, "score_of_first_answer": 0.06172602251172066, "max_score_of_first_answer": 0.9972655773162842, "min_score_of_first_answer": 0.00032783084316179156, "avg_score": 0.06172602251172066, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.36430687389163546, "std_of_first_answer": 0.36430687389163546, "avg_entropy_of_first_answer": 1.676029708981514, "lex_sim": 1.0}
{"question": "JSC. What phrase was trending worldwide on Twitter after the events of 7th Jan. 2015?", "answer": ["JSC"], "scores": [[0.02271278388798237, 0.5234387516975403]], "normalized_score": [0.10903556644916534], "tokens": [["\u0120J", "SC"]], "entropy": [[6.296127796173096, 3.555302619934082]], "avg_entropy": [4.925715208053589], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Je suis Charlie", "score_of_biggest_cluster": 0.10903556644916534, "score_of_first_answer": 0.10903556644916534, "max_score_of_first_answer": 0.5234387516975403, "min_score_of_first_answer": 0.02271278388798237, "avg_score": 0.10903556644916534, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.25036298390477896, "std_of_first_answer": 0.25036298390477896, "avg_entropy_of_first_answer": 4.925715208053589, "lex_sim": 1.0}
{"question": "\"Who wrote the piece of music recognised in much of the Western world as \"\"The Wedding March\"\" (or as \"\"Here Comes The Bride\"\")?\"", "answer": ["Charles Gounod"], "scores": [[0.014034711755812168, 0.27373620867729187, 0.996408998966217, 0.997947633266449]], "normalized_score": [0.2486109733581543], "tokens": [["\u0120Charles", "\u0120G", "oun", "od"]], "entropy": [[4.747117519378662, 4.660967826843262, 0.0362558551132679, 0.020899098366498947]], "avg_entropy": [2.3663100749254227], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jacob Ludwig Felix Mendelssohn Bartholdy", "score_of_biggest_cluster": 0.2486109733581543, "score_of_first_answer": 0.2486109733581543, "max_score_of_first_answer": 0.997947633266449, "min_score_of_first_answer": 0.014034711755812168, "avg_score": 0.2486109733581543, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43641503049820246, "std_of_first_answer": 0.43641503049820246, "avg_entropy_of_first_answer": 2.3663100749254227, "lex_sim": 1.0}
{"question": "In sport, what is the next country in the sequence China, Sweden, USA, USA, China, Germany, Canada ...?", "answer": ["\u32c3"], "scores": [[0.004018557723611593, 0.00014671370445284992, 0.02103918045759201, 0.002869637683033943]], "normalized_score": [0.0024425822775810957], "tokens": [["\u0120", "\u00e3", "\u012d", "\u0125"]], "entropy": [[4.761051177978516, 4.278787612915039, 2.230903148651123, 3.9029693603515625]], "avg_entropy": [3.79342782497406], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "La Republique francaise", "score_of_biggest_cluster": 0.0024425822775810957, "score_of_first_answer": 0.0024425822775810957, "max_score_of_first_answer": 0.02103918045759201, "min_score_of_first_answer": 0.00014671370445284992, "avg_score": 0.0024425822775810957, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.008216045879411365, "std_of_first_answer": 0.008216045879411365, "avg_entropy_of_first_answer": 3.79342782497406, "lex_sim": 0.0}
{"question": "In Vladimir Nabokov's book \"Lolita\", what is Lolita's real name?", "answer": ["Victoria"], "scores": [[0.007366674952208996]], "normalized_score": [0.007366673555225134], "tokens": [["\u0120Victoria"]], "entropy": [[2.369493007659912]], "avg_entropy": [2.369493007659912], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Hunted Enchanters", "score_of_biggest_cluster": 0.007366673555225134, "score_of_first_answer": 0.007366673555225134, "max_score_of_first_answer": 0.007366674952208996, "min_score_of_first_answer": 0.007366674952208996, "avg_score": 0.007366673555225134, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.369493007659912, "lex_sim": 1.0}
{"question": "What is the name of the fragrant essential oil obtained chiefly from the Damask Rose?", "answer": ["Rose Otto oil"], "scores": [[0.5447220802307129, 0.10199642181396484, 0.02527507022023201]], "normalized_score": [0.11198264360427856], "tokens": [["\u0120Rose", "\u0120Otto", "\u0120oil"]], "entropy": [[2.684494972229004, 3.401859760284424, 0.5879195928573608]], "avg_entropy": [2.2247581084569297], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Attar", "score_of_biggest_cluster": 0.11198264360427856, "score_of_first_answer": 0.11198264360427856, "max_score_of_first_answer": 0.5447220802307129, "min_score_of_first_answer": 0.02527507022023201, "avg_score": 0.11198264360427856, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.22893894640268442, "std_of_first_answer": 0.22893894640268442, "avg_entropy_of_first_answer": 2.2247581084569297, "lex_sim": 1.0}
{"question": "JB Who wrote the novel \u2018Greenmantle\u2019?", "answer": ["John Buchan"], "scores": [[0.41891413927078247, 0.9878508448600769, 0.9936150312423706]], "normalized_score": [0.7436093688011169], "tokens": [["\u0120John", "\u0120Buch", "an"]], "entropy": [[3.682257652282715, 0.1350298821926117, 0.05906730890274048]], "avg_entropy": [1.2921182811260223], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Scantlebury Blenkiron", "score_of_biggest_cluster": 0.7436093688011169, "score_of_first_answer": 0.7436093688011169, "max_score_of_first_answer": 0.9936150312423706, "min_score_of_first_answer": 0.41891413927078247, "avg_score": 0.7436093688011169, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26956823832945, "std_of_first_answer": 0.26956823832945, "avg_entropy_of_first_answer": 1.2921182811260223, "lex_sim": 1.0}
{"question": "What is the currency of Lithuania?", "answer": ["Litas"], "scores": [[0.16767750680446625, 0.36478888988494873]], "normalized_score": [0.24731941521167755], "tokens": [["\u0120L", "itas"]], "entropy": [[3.619313955307007, 2.564244270324707]], "avg_entropy": [3.091779112815857], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Litai", "score_of_biggest_cluster": 0.24731941521167755, "score_of_first_answer": 0.24731941521167755, "max_score_of_first_answer": 0.36478888988494873, "min_score_of_first_answer": 0.16767750680446625, "avg_score": 0.24731941521167755, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09855569154024124, "std_of_first_answer": 0.09855569154024124, "avg_entropy_of_first_answer": 3.091779112815857, "lex_sim": 1.0}
{"question": "Which European flag consists of three horizontal stripes, red at the top and bottom, white in the centre?", "answer": ["Portuguese Flag"], "scores": [[0.003559458302333951, 0.04263489693403244]], "normalized_score": [0.012318974360823631], "tokens": [["\u0120Portuguese", "\u0120Flag"]], "entropy": [[5.259340286254883, 1.4136276245117188]], "avg_entropy": [3.336483955383301], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Republic of Austria", "score_of_biggest_cluster": 0.012318974360823631, "score_of_first_answer": 0.012318974360823631, "max_score_of_first_answer": 0.04263489693403244, "min_score_of_first_answer": 0.003559458302333951, "avg_score": 0.012318974360823631, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.019537719315849245, "std_of_first_answer": 0.019537719315849245, "avg_entropy_of_first_answer": 3.336483955383301, "lex_sim": 1.0}
{"question": "What word best describes a triangle where all sides are equal?", "answer": ["Isosceles"], "scores": [[0.16362632811069489, 0.9897955060005188, 0.9848327040672302, 0.9927343130111694]], "normalized_score": [0.6308099627494812], "tokens": [["\u0120Is", "os", "ce", "les"]], "entropy": [[3.5111286640167236, 0.09100914746522903, 0.14822080731391907, 0.06350512057542801]], "avg_entropy": [0.9534659348428249], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Regular Triangle", "score_of_biggest_cluster": 0.6308099627494812, "score_of_first_answer": 0.6308099627494812, "max_score_of_first_answer": 0.9927343130111694, "min_score_of_first_answer": 0.16362632811069489, "avg_score": 0.6308099627494812, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3574607648440348, "std_of_first_answer": 0.3574607648440348, "avg_entropy_of_first_answer": 0.9534659348428249, "lex_sim": 1.0}
{"question": "Which is the highest prime number less than 100?", "answer": ["23"], "scores": [[0.019504200667142868]], "normalized_score": [0.019504200667142868], "tokens": [["\u012023"]], "entropy": [[5.557150840759277]], "avg_entropy": [5.557150840759277], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ninety-seven", "score_of_biggest_cluster": 0.019504200667142868, "score_of_first_answer": 0.019504200667142868, "max_score_of_first_answer": 0.019504200667142868, "min_score_of_first_answer": 0.019504200667142868, "avg_score": 0.019504200667142868, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.557150840759277, "lex_sim": 1.0}
{"question": "\"What is a \"\"white dwarf\"\"?\"", "answer": ["A carbon or oxygen white dwarf"], "scores": [[0.4974774420261383, 0.0015964997000992298, 0.018792716786265373, 0.454303503036499, 0.34593626856803894, 0.961944580078125]], "normalized_score": [0.11452598124742508], "tokens": [["\u0120A", "\u0120carbon", "\u0120or", "\u0120oxygen", "\u0120white", "\u0120dwarf"]], "entropy": [[3.1862130165100098, 4.139430046081543, 2.9149584770202637, 2.993964195251465, 2.6890501976013184, 0.2529545724391937]], "avg_entropy": [2.6960950841506324], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stellar diameter", "score_of_biggest_cluster": 0.11452598124742508, "score_of_first_answer": 0.11452598124742508, "max_score_of_first_answer": 0.961944580078125, "min_score_of_first_answer": 0.0015964997000992298, "avg_score": 0.11452598124742508, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3247567556603107, "std_of_first_answer": 0.3247567556603107, "avg_entropy_of_first_answer": 2.6960950841506324, "lex_sim": 1.0}
{"question": "Which Briton won the Nobel Prize for Literature in the 1980s?", "answer": ["C.S."], "scores": [[0.015530216507613659, 0.6671708226203918, 0.4491737484931946, 0.9610463976860046, 0.8567220568656921]], "normalized_score": [0.32861143350601196], "tokens": [["\u0120C", ".", "S", ".", "\u0120Lewis"]], "entropy": [[5.162593841552734, 2.0853261947631836, 2.3630664348602295, 0.29246824979782104, 0.7767387628555298]], "avg_entropy": [2.1360386967658997], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Golding, William", "score_of_biggest_cluster": 0.32861143350601196, "score_of_first_answer": 0.32861143350601196, "max_score_of_first_answer": 0.9610463976860046, "min_score_of_first_answer": 0.015530216507613659, "avg_score": 0.32861143350601196, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3360413547533553, "std_of_first_answer": 0.3360413547533553, "avg_entropy_of_first_answer": 2.1360386967658997, "lex_sim": 1.0}
{"question": "What was American footballer William Perry's nickname?", "answer": ["Dixie"], "scores": [[0.005534446332603693, 0.18780925869941711]], "normalized_score": [0.03224003687500954], "tokens": [["\u0120D", "ixie"]], "entropy": [[6.197319984436035, 3.1854841709136963]], "avg_entropy": [4.691402077674866], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Fridge", "score_of_biggest_cluster": 0.03224003687500954, "score_of_first_answer": 0.03224003687500954, "max_score_of_first_answer": 0.18780925869941711, "min_score_of_first_answer": 0.005534446332603693, "avg_score": 0.03224003687500954, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09113740618340671, "std_of_first_answer": 0.09113740618340671, "avg_entropy_of_first_answer": 4.691402077674866, "lex_sim": 1.0}
{"question": "Who wrote the novel The Naked and the Dead?", "answer": ["Ernest Hemingway"], "scores": [[0.15677635371685028, 0.8056373000144958, 0.9751521348953247, 0.9980859756469727]], "normalized_score": [0.5921273231506348], "tokens": [["\u0120Ernest", "\u0120Hem", "ing", "way"]], "entropy": [[3.3822221755981445, 1.295578122138977, 0.1321805864572525, 0.02315618097782135]], "avg_entropy": [1.2082842662930489], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Maileresque", "score_of_biggest_cluster": 0.5921273231506348, "score_of_first_answer": 0.5921273231506348, "max_score_of_first_answer": 0.9980859756469727, "min_score_of_first_answer": 0.15677635371685028, "avg_score": 0.5921273231506348, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34139960638624706, "std_of_first_answer": 0.34139960638624706, "avg_entropy_of_first_answer": 1.2082842662930489, "lex_sim": 1.0}
{"question": "Which Liverpool player scored two goals in both the semi-final and final of the FA Cup in 1986?", "answer": ["Craig Johnston"], "scores": [[0.0030554411932826042, 0.9239621758460999]], "normalized_score": [0.053132977336645126], "tokens": [["\u0120Craig", "\u0120Johnston"]], "entropy": [[3.9222707748413086, 0.6751829385757446]], "avg_entropy": [2.2987268567085266], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "IAN RUSH", "score_of_biggest_cluster": 0.053132977336645126, "score_of_first_answer": 0.053132977336645126, "max_score_of_first_answer": 0.9239621758460999, "min_score_of_first_answer": 0.0030554411932826042, "avg_score": 0.053132977336645126, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4604533673264086, "std_of_first_answer": 0.4604533673264086, "avg_entropy_of_first_answer": 2.2987268567085266, "lex_sim": 1.0}
{"question": "The term 'campesino' refers in the Spanish-speaking world to a?", "answer": ["Small farmer"], "scores": [[0.0047883098013699055, 0.5443655848503113]], "normalized_score": [0.051054779440164566], "tokens": [["\u0120Small", "\u0120farmer"]], "entropy": [[5.3171610832214355, 2.2108817100524902]], "avg_entropy": [3.764021396636963], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Estate of Swedish Peasants", "score_of_biggest_cluster": 0.051054779440164566, "score_of_first_answer": 0.051054779440164566, "max_score_of_first_answer": 0.5443655848503113, "min_score_of_first_answer": 0.0047883098013699055, "avg_score": 0.051054779440164566, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2697886375244707, "std_of_first_answer": 0.2697886375244707, "avg_entropy_of_first_answer": 3.764021396636963, "lex_sim": 1.0}
{"question": "Which semi-aquatic carnivorous mammal of the weasel family has only two living species: the American and the European?", "answer": ["Walrus"], "scores": [[0.006335689686238766, 0.9907802939414978]], "normalized_score": [0.07922925055027008], "tokens": [["\u0120Wal", "rus"]], "entropy": [[5.017738342285156, 0.09347141534090042]], "avg_entropy": [2.5556048788130283], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mustela vison evergladensis", "score_of_biggest_cluster": 0.07922925055027008, "score_of_first_answer": 0.07922925055027008, "max_score_of_first_answer": 0.9907802939414978, "min_score_of_first_answer": 0.006335689686238766, "avg_score": 0.07922925055027008, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4922223021276295, "std_of_first_answer": 0.4922223021276295, "avg_entropy_of_first_answer": 2.5556048788130283, "lex_sim": 1.0}
{"question": "Which cult film, starring Harrison Ford, was based on the story Do Androids Dream of Electric Sheep?", "answer": ["Blade Runner"], "scores": [[0.9385112524032593, 0.9886294603347778]], "normalized_score": [0.9632444381713867], "tokens": [["\u0120Blade", "\u0120Runner"]], "entropy": [[0.5018295049667358, 0.08641372621059418]], "avg_entropy": [0.294121615588665], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "V-K", "score_of_biggest_cluster": 0.9632444381713867, "score_of_first_answer": 0.9632444381713867, "max_score_of_first_answer": 0.9886294603347778, "min_score_of_first_answer": 0.9385112524032593, "avg_score": 0.9632444381713867, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.025059103965759277, "std_of_first_answer": 0.025059103965759277, "avg_entropy_of_first_answer": 0.294121615588665, "lex_sim": 1.0}
{"question": "Who was the first of these to become US President?", "answer": ["Rutherford Hayes"], "scores": [[0.010706692934036255, 0.06665954738855362]], "normalized_score": [0.026715224608778954], "tokens": [["\u0120Rutherford", "\u0120Hayes"]], "entropy": [[3.9001259803771973, 0.7621835470199585]], "avg_entropy": [2.331154763698578], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Father of the Constitution", "score_of_biggest_cluster": 0.026715224608778954, "score_of_first_answer": 0.026715224608778954, "max_score_of_first_answer": 0.06665954738855362, "min_score_of_first_answer": 0.010706692934036255, "avg_score": 0.026715224608778954, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.027976427227258682, "std_of_first_answer": 0.027976427227258682, "avg_entropy_of_first_answer": 2.331154763698578, "lex_sim": 1.0}
{"question": "Which Norwegian explorer was the founder of the Nordic settlement in Greenland in the 10th century?", "answer": ["Erik the Red"], "scores": [[0.5502465963363647, 0.9476714134216309, 0.9970574378967285]], "normalized_score": [0.8041031956672668], "tokens": [["\u0120Erik", "\u0120the", "\u0120Red"]], "entropy": [[2.2023603916168213, 0.467711478471756, 0.02799069508910179]], "avg_entropy": [0.8993541883925597], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eystribyggd", "score_of_biggest_cluster": 0.8041031956672668, "score_of_first_answer": 0.8041031956672668, "max_score_of_first_answer": 0.9970574378967285, "min_score_of_first_answer": 0.5502465963363647, "avg_score": 0.8041031956672668, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20000705342689865, "std_of_first_answer": 0.20000705342689865, "avg_entropy_of_first_answer": 0.8993541883925597, "lex_sim": 1.0}
{"question": "Who had an 80s No 1 hit with Hold On To The Nights?", "answer": ["Steve Winwood"], "scores": [[0.003510798327624798, 0.3254755735397339, 0.9993274211883545]], "normalized_score": [0.1045227199792862], "tokens": [["\u0120Steve", "\u0120Win", "wood"]], "entropy": [[6.630871772766113, 3.6811165809631348, 0.007232872769236565]], "avg_entropy": [3.439740408832828], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Richard Noel Marx", "score_of_biggest_cluster": 0.1045227199792862, "score_of_first_answer": 0.1045227199792862, "max_score_of_first_answer": 0.9993274211883545, "min_score_of_first_answer": 0.003510798327624798, "avg_score": 0.1045227199792862, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4149147670737252, "std_of_first_answer": 0.4149147670737252, "avg_entropy_of_first_answer": 3.439740408832828, "lex_sim": 1.0}
{"question": "What is the Latin for wolf spider?", "answer": ["Loxelycta"], "scores": [[0.1035415381193161, 0.5553521513938904, 0.0031657754443585873, 0.06669384986162186, 0.0786517858505249, 0.5846452116966248]], "normalized_score": [0.09074199944734573], "tokens": [["\u0120L", "ox", "e", "ly", "ct", "a"]], "entropy": [[2.6589128971099854, 2.3703765869140625, 3.710413932800293, 4.659971237182617, 4.607674598693848, 1.5727845430374146]], "avg_entropy": [3.263355632623037], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Www.lycos.com", "score_of_biggest_cluster": 0.09074199944734573, "score_of_first_answer": 0.09074199944734573, "max_score_of_first_answer": 0.5846452116966248, "min_score_of_first_answer": 0.0031657754443585873, "avg_score": 0.09074199944734573, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24104762169014923, "std_of_first_answer": 0.24104762169014923, "avg_entropy_of_first_answer": 3.263355632623037, "lex_sim": 1.0}
{"question": "The River Durance flows into the Rhone near to which French town or city?", "answer": ["Sainte Victoire"], "scores": [[0.02230626717209816, 0.022798048332333565, 0.9916640520095825, 0.09113143384456635, 0.9738765954971313]], "normalized_score": [0.13494980335235596], "tokens": [["\u0120S", "ain", "te", "\u0120Vict", "oire"]], "entropy": [[4.7057037353515625, 2.9244985580444336, 0.07973887771368027, 2.2981841564178467, 0.22107702493667603]], "avg_entropy": [2.04584047049284], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Avignon, France", "score_of_biggest_cluster": 0.13494980335235596, "score_of_first_answer": 0.13494980335235596, "max_score_of_first_answer": 0.9916640520095825, "min_score_of_first_answer": 0.02230626717209816, "avg_score": 0.13494980335235596, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45992666146390654, "std_of_first_answer": 0.45992666146390654, "avg_entropy_of_first_answer": 2.04584047049284, "lex_sim": 1.0}
{"question": "In which country is Lake Bala", "answer": ["Turkey"], "scores": [[0.00962241180241108]], "normalized_score": [0.00962240993976593], "tokens": [["\u0120Turkey"]], "entropy": [[5.416332721710205]], "avg_entropy": [5.416332721710205], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Welsh nation", "score_of_biggest_cluster": 0.00962240993976593, "score_of_first_answer": 0.00962240993976593, "max_score_of_first_answer": 0.00962241180241108, "min_score_of_first_answer": 0.00962241180241108, "avg_score": 0.00962240993976593, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.416332721710205, "lex_sim": 1.0}
{"question": "Michael Fassbender was nominated as Best Actor at the 88th Academy Awards for his role in which biographical drama?", "answer": ["Steve Jobs"], "scores": [[0.3545283377170563, 0.995968222618103]], "normalized_score": [0.594221293926239], "tokens": [["\u0120Steve", "\u0120Jobs"]], "entropy": [[3.1687726974487305, 0.03631051257252693]], "avg_entropy": [1.6025416050106287], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jobs, Steven Paul", "score_of_biggest_cluster": 0.594221293926239, "score_of_first_answer": 0.594221293926239, "max_score_of_first_answer": 0.995968222618103, "min_score_of_first_answer": 0.3545283377170563, "avg_score": 0.594221293926239, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3207199424505234, "std_of_first_answer": 0.3207199424505234, "avg_entropy_of_first_answer": 1.6025416050106287, "lex_sim": 1.0}
{"question": "What Christian holiday is celebrated immediately after Halloween", "answer": ["Beelzebub-day"], "scores": [[0.0005844571860507131, 0.07080938667058945, 0.5298316478729248, 0.9676497578620911, 0.02398398332297802, 0.17021439969539642]], "normalized_score": [0.06651753187179565], "tokens": [["\u0120Be", "el", "zeb", "ub", "-", "day"]], "entropy": [[2.4684910774230957, 4.967523574829102, 3.3125457763671875, 0.29935768246650696, 2.6354360580444336, 5.000517845153809]], "avg_entropy": [3.1139786690473557], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Solemnity of All Saints", "score_of_biggest_cluster": 0.06651753187179565, "score_of_first_answer": 0.06651753187179565, "max_score_of_first_answer": 0.9676497578620911, "min_score_of_first_answer": 0.0005844571860507131, "avg_score": 0.06651753187179565, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34965665876878005, "std_of_first_answer": 0.34965665876878005, "avg_entropy_of_first_answer": 3.1139786690473557, "lex_sim": 1.0}
{"question": "What country is Rhodes a part of?", "answer": ["Greece"], "scores": [[0.10569241642951965]], "normalized_score": [0.10569240897893906], "tokens": [["\u0120Greece"]], "entropy": [[4.586112022399902]], "avg_entropy": [4.586112022399902], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yunanistan", "score_of_biggest_cluster": 0.10569240897893906, "score_of_first_answer": 0.10569240897893906, "max_score_of_first_answer": 0.10569241642951965, "min_score_of_first_answer": 0.10569241642951965, "avg_score": 0.10569240897893906, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.586112022399902, "lex_sim": 1.0}
{"question": "In what month is Hogmanay celebrated?", "answer": ["\u15cc\u15fe\u15cc"], "scores": [[1.5941685660436633e-07, 0.00578086543828249, 0.007011959794908762, 0.8645524382591248, 0.3923391103744507, 0.005511211231350899, 0.6807320713996887, 0.8109645843505859, 0.10514239966869354]], "normalized_score": [0.02071106620132923], "tokens": [["\u0120\u00e1", "\u0139", "\u012e", "\u00e1", "\u0139", "\u00be", "\u00e1", "\u0139", "\u012e"]], "entropy": [[1.9216599464416504, 2.8605048656463623, 3.814901828765869, 1.1641671657562256, 2.399538993835449, 3.447018623352051, 1.5198894739151, 1.0081396102905273, 3.6131339073181152]], "avg_entropy": [2.416550490591261], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "December", "score_of_biggest_cluster": 0.02071106620132923, "score_of_first_answer": 0.02071106620132923, "max_score_of_first_answer": 0.8645524382591248, "min_score_of_first_answer": 1.5941685660436633e-07, "avg_score": 0.02071106620132923, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3523097072555043, "std_of_first_answer": 0.3523097072555043, "avg_entropy_of_first_answer": 2.416550490591261, "lex_sim": 0.0}
{"question": "The stomach of which animal is traditionally stuffed for a haggis?", "answer": ["Rump"], "scores": [[0.007466232404112816, 0.21632367372512817]], "normalized_score": [0.04018859192728996], "tokens": [["\u0120R", "ump"]], "entropy": [[4.268167972564697, 4.25861930847168]], "avg_entropy": [4.2633936405181885], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sheep", "score_of_biggest_cluster": 0.04018859192728996, "score_of_first_answer": 0.04018859192728996, "max_score_of_first_answer": 0.21632367372512817, "min_score_of_first_answer": 0.007466232404112816, "avg_score": 0.04018859192728996, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10442872066050768, "std_of_first_answer": 0.10442872066050768, "avg_entropy_of_first_answer": 4.2633936405181885, "lex_sim": 1.0}
{"question": "What acid is found in tea?", "answer": ["EGC"], "scores": [[0.0071653579361736774, 0.25614842772483826]], "normalized_score": [0.04284152016043663], "tokens": [["\u0120E", "GC"]], "entropy": [[4.331760406494141, 4.201168060302734]], "avg_entropy": [4.2664642333984375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pseudotannin", "score_of_biggest_cluster": 0.04284152016043663, "score_of_first_answer": 0.04284152016043663, "max_score_of_first_answer": 0.25614842772483826, "min_score_of_first_answer": 0.0071653579361736774, "avg_score": 0.04284152016043663, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12449153489433229, "std_of_first_answer": 0.12449153489433229, "avg_entropy_of_first_answer": 4.2664642333984375, "lex_sim": 1.0}
{"question": "How is the prince described in the title of the 1911 ballet music composed by Bela Bartok?", "answer": ["Hungarian Skiffle"], "scores": [[0.006520065478980541, 0.0004780651943292469, 0.04497600719332695, 0.3104499876499176]], "normalized_score": [0.01444368064403534], "tokens": [["\u0120Hungarian", "\u0120Sk", "iff", "le"]], "entropy": [[5.290424823760986, 3.71956205368042, 3.834627628326416, 1.8563728332519531]], "avg_entropy": [3.675246834754944], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Diffuse-porous wood", "score_of_biggest_cluster": 0.01444368064403534, "score_of_first_answer": 0.01444368064403534, "max_score_of_first_answer": 0.3104499876499176, "min_score_of_first_answer": 0.0004780651943292469, "avg_score": 0.01444368064403534, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12806928219323185, "std_of_first_answer": 0.12806928219323185, "avg_entropy_of_first_answer": 3.675246834754944, "lex_sim": 1.0}
{"question": "What U.S. state is closest to Africa (geographically)?", "answer": ["Delaware"], "scores": [[0.017261521890759468]], "normalized_score": [0.01726151816546917], "tokens": [["\u0120Delaware"]], "entropy": [[4.2382097244262695]], "avg_entropy": [4.2382097244262695], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Religion in Maine", "score_of_biggest_cluster": 0.01726151816546917, "score_of_first_answer": 0.01726151816546917, "max_score_of_first_answer": 0.017261521890759468, "min_score_of_first_answer": 0.017261521890759468, "avg_score": 0.01726151816546917, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.2382097244262695, "lex_sim": 1.0}
{"question": "Which NATO member country has the smallest population?", "answer": ["Luxembourg"], "scores": [[0.1262654960155487]], "normalized_score": [0.1262655109167099], "tokens": [["\u0120Luxembourg"]], "entropy": [[4.544803142547607]], "avg_entropy": [4.544803142547607], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "IJsland", "score_of_biggest_cluster": 0.1262655109167099, "score_of_first_answer": 0.1262655109167099, "max_score_of_first_answer": 0.1262654960155487, "min_score_of_first_answer": 0.1262654960155487, "avg_score": 0.1262655109167099, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.544803142547607, "lex_sim": 1.0}
{"question": "Which Charles Dickens novel, his last completed work, features' John Harmon' and the title character 'John Rokesmith'?", "answer": ["Great Expectations"], "scores": [[0.089725062251091, 0.9239538311958313, 0.9987943172454834]], "normalized_score": [0.43585970997810364], "tokens": [["\u0120Great", "\u0120Expect", "ations"]], "entropy": [[3.2082529067993164, 0.3903868794441223, 0.011151885613799095]], "avg_entropy": [1.2032638906190793], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Harmon (character)", "score_of_biggest_cluster": 0.43585970997810364, "score_of_first_answer": 0.43585970997810364, "max_score_of_first_answer": 0.9987943172454834, "min_score_of_first_answer": 0.089725062251091, "avg_score": 0.43585970997810364, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41203366200985525, "std_of_first_answer": 0.41203366200985525, "avg_entropy_of_first_answer": 1.2032638906190793, "lex_sim": 1.0}
{"question": "Under the Treaty of Breda (31 July 1667), negotiated to end the Second Anglo-Dutch War, the Dutch kept the colony of Suriname conquered from the English, and left the English with what area of land?", "answer": ["New Netherland"], "scores": [[0.054148148745298386, 0.1706654131412506, 0.9890435934066772]], "normalized_score": [0.20908114314079285], "tokens": [["\u0120New", "\u0120Nether", "land"]], "entropy": [[6.15777587890625, 2.9560670852661133, 0.09389975666999817]], "avg_entropy": [3.0692475736141205], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fort Amsterdam (New Amsterdam)", "score_of_biggest_cluster": 0.20908114314079285, "score_of_first_answer": 0.20908114314079285, "max_score_of_first_answer": 0.9890435934066772, "min_score_of_first_answer": 0.054148148745298386, "avg_score": 0.20908114314079285, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4159792477843983, "std_of_first_answer": 0.4159792477843983, "avg_entropy_of_first_answer": 3.0692475736141205, "lex_sim": 1.0}
{"question": "A drupe is a type of what?", "answer": ["Berry"], "scores": [[0.013781974092125893]], "normalized_score": [0.013781974092125893], "tokens": [["\u0120Berry"]], "entropy": [[3.80688214302063]], "avg_entropy": [3.80688214302063], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fruiting", "score_of_biggest_cluster": 0.013781974092125893, "score_of_first_answer": 0.013781974092125893, "max_score_of_first_answer": 0.013781974092125893, "min_score_of_first_answer": 0.013781974092125893, "avg_score": 0.013781974092125893, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.80688214302063, "lex_sim": 1.0}
{"question": "What are the major suits in the card game Bridge?", "answer": ["Spades:"], "scores": [[0.3335036337375641, 0.9739924073219299, 0.00512870354577899]], "normalized_score": [0.118546262383461], "tokens": [["\u0120Sp", "ades", ":"]], "entropy": [[4.666460037231445, 0.12895432114601135, 0.8623568415641785]], "avg_entropy": [1.8859237333138783], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hearts and Spades", "score_of_biggest_cluster": 0.118546262383461, "score_of_first_answer": 0.118546262383461, "max_score_of_first_answer": 0.9739924073219299, "min_score_of_first_answer": 0.00512870354577899, "avg_score": 0.118546262383461, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40232005506408375, "std_of_first_answer": 0.40232005506408375, "avg_entropy_of_first_answer": 1.8859237333138783, "lex_sim": 1.0}
{"question": "Which actress singer who charted with Santa Baby and starred as Catwoman died on Christmas day 2008?", "answer": ["Julie Andrews"], "scores": [[0.01552275288850069, 0.4904371201992035]], "normalized_score": [0.08725214004516602], "tokens": [["\u0120Julie", "\u0120Andrews"]], "entropy": [[6.215307235717773, 2.8917012214660645]], "avg_entropy": [4.553504228591919], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eartha Kitt", "score_of_biggest_cluster": 0.08725214004516602, "score_of_first_answer": 0.08725214004516602, "max_score_of_first_answer": 0.4904371201992035, "min_score_of_first_answer": 0.01552275288850069, "avg_score": 0.08725214004516602, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2374571836553514, "std_of_first_answer": 0.2374571836553514, "avg_entropy_of_first_answer": 4.553504228591919, "lex_sim": 1.0}
{"question": "\"In the book by L. M. Montgomery, in which Canadian province is \"\"Anne of Green Gables\"\" set?\"", "answer": ["BC"], "scores": [[9.978901653084904e-05]], "normalized_score": [9.978897287510335e-05], "tokens": [["\u0120BC"]], "entropy": [[0.9681837558746338]], "avg_entropy": [0.9681837558746338], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "P.E.I", "score_of_biggest_cluster": 9.978897287510335e-05, "score_of_first_answer": 9.978897287510335e-05, "max_score_of_first_answer": 9.978901653084904e-05, "min_score_of_first_answer": 9.978901653084904e-05, "avg_score": 9.978897287510335e-05, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.9681837558746338, "lex_sim": 1.0}
{"question": "Which US TV series is set in the Princeton-Plainsboro Teaching Hospital?", "answer": ["ER"], "scores": [[0.3083361089229584]], "normalized_score": [0.3083361089229584], "tokens": [["\u0120ER"]], "entropy": [[3.776196002960205]], "avg_entropy": [3.776196002960205], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\ud83c\udfe0", "score_of_biggest_cluster": 0.3083361089229584, "score_of_first_answer": 0.3083361089229584, "max_score_of_first_answer": 0.3083361089229584, "min_score_of_first_answer": 0.3083361089229584, "avg_score": 0.3083361089229584, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.776196002960205, "lex_sim": 1.0}
{"question": "Which English town's Anglo Saxon name was Gipeswic pronounced Gipperswick", "answer": ["Ipswich"], "scores": [[0.10748110711574554, 0.9882638454437256]], "normalized_score": [0.3259136378765106], "tokens": [["\u0120Ips", "wich"]], "entropy": [[5.437656402587891, 0.10441634058952332]], "avg_entropy": [2.771036371588707], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ipswich, UK", "score_of_biggest_cluster": 0.3259136378765106, "score_of_first_answer": 0.3259136378765106, "max_score_of_first_answer": 0.9882638454437256, "min_score_of_first_answer": 0.10748110711574554, "avg_score": 0.3259136378765106, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.44039136916399, "std_of_first_answer": 0.44039136916399, "avg_entropy_of_first_answer": 2.771036371588707, "lex_sim": 1.0}
{"question": "What song did Wayne and Garth headbang to in the car?", "answer": ["It's Alright, Ma!"], "scores": [[0.009035944938659668, 0.7173875570297241, 0.006553913466632366, 0.27772173285484314, 0.2878590226173401, 0.005802694242447615]], "normalized_score": [0.05197262391448021], "tokens": [["\u0120It", "'s", "\u0120Alright", ",", "\u0120Ma", "!"]], "entropy": [[6.31107234954834, 1.761975884437561, 4.568853378295898, 3.173452854156494, 2.2142670154571533, 1.7905402183532715]], "avg_entropy": [3.3033602833747864], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bohamian Rhapsody", "score_of_biggest_cluster": 0.05197262391448021, "score_of_first_answer": 0.05197262391448021, "max_score_of_first_answer": 0.7173875570297241, "min_score_of_first_answer": 0.005802694242447615, "avg_score": 0.05197262391448021, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2553545945360406, "std_of_first_answer": 0.2553545945360406, "avg_entropy_of_first_answer": 3.3033602833747864, "lex_sim": 1.0}
{"question": "At which sport could you win the Victor Barna Award?", "answer": ["Hockey"], "scores": [[0.02165188267827034]], "normalized_score": [0.02165188454091549], "tokens": [["\u0120Hockey"]], "entropy": [[6.272363662719727]], "avg_entropy": [6.272363662719727], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ping-pang Ch'iu", "score_of_biggest_cluster": 0.02165188454091549, "score_of_first_answer": 0.02165188454091549, "max_score_of_first_answer": 0.02165188267827034, "min_score_of_first_answer": 0.02165188267827034, "avg_score": 0.02165188454091549, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.272363662719727, "lex_sim": 1.0}
{"question": "Who was the first winner of 'I'm A Celebrity, Get Me Out Of Here'?", "answer": ["Steve Wozniak"], "scores": [[0.00796582829207182, 0.020073745399713516, 0.1827576458454132, 0.9570594429969788, 0.8183242678642273]], "normalized_score": [0.1180102676153183], "tokens": [["\u0120Steve", "\u0120W", "oz", "ni", "ak"]], "entropy": [[6.809013366699219, 5.301271438598633, 4.209290981292725, 0.3497648239135742, 0.5596912503242493]], "avg_entropy": [3.4458063721656798], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "TONY BLACKBURN", "score_of_biggest_cluster": 0.1180102676153183, "score_of_first_answer": 0.1180102676153183, "max_score_of_first_answer": 0.9570594429969788, "min_score_of_first_answer": 0.00796582829207182, "avg_score": 0.1180102676153183, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4075539753061587, "std_of_first_answer": 0.4075539753061587, "avg_entropy_of_first_answer": 3.4458063721656798, "lex_sim": 1.0}
{"question": "Introduced in 1908, for what does the B in the degree-equivalent course MBA stand?", "answer": ["Bachelor of Science"], "scores": [[0.3512424826622009, 0.38970908522605896, 0.11787282675504684]], "normalized_score": [0.2526894211769104], "tokens": [["\u0120Bachelor", "\u0120of", "\u0120Science"]], "entropy": [[4.174007892608643, 1.283719778060913, 2.671330451965332]], "avg_entropy": [2.709686040878296], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The business", "score_of_biggest_cluster": 0.2526894211769104, "score_of_first_answer": 0.2526894211769104, "max_score_of_first_answer": 0.38970908522605896, "min_score_of_first_answer": 0.11787282675504684, "avg_score": 0.2526894211769104, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12010922215982706, "std_of_first_answer": 0.12010922215982706, "avg_entropy_of_first_answer": 2.709686040878296, "lex_sim": 1.0}
{"question": "What's the name of the intelligent talking horses in Swift's Gulliver's Travels ?", "answer": ["Glubbdubs"], "scores": [[0.001601343508809805, 0.31189635396003723, 0.7313927412033081, 0.075737863779068]], "normalized_score": [0.07252531498670578], "tokens": [["\u0120Gl", "ub", "bd", "ubs"]], "entropy": [[4.791146755218506, 0.9983223676681519, 1.5167858600616455, 0.7316949367523193]], "avg_entropy": [2.0094874799251556], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Houyhnhm", "score_of_biggest_cluster": 0.07252531498670578, "score_of_first_answer": 0.07252531498670578, "max_score_of_first_answer": 0.7313927412033081, "min_score_of_first_answer": 0.001601343508809805, "avg_score": 0.07252531498670578, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2846056560161913, "std_of_first_answer": 0.2846056560161913, "avg_entropy_of_first_answer": 2.0094874799251556, "lex_sim": 1.0}
{"question": "Who was the driver of the limousine at the time of Diana Princess of Wales' death?", "answer": ["Philippe Dijon"], "scores": [[0.002100057899951935, 0.007931850850582123, 0.028937963768839836, 0.8638362884521484]], "normalized_score": [0.02540249563753605], "tokens": [["\u0120Philippe", "\u0120D", "ij", "on"]], "entropy": [[5.408727645874023, 3.5183544158935547, 4.475781440734863, 0.7646850347518921]], "avg_entropy": [3.5418871343135834], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "HENRI PAUL", "score_of_biggest_cluster": 0.02540249563753605, "score_of_first_answer": 0.02540249563753605, "max_score_of_first_answer": 0.8638362884521484, "min_score_of_first_answer": 0.002100057899951935, "avg_score": 0.02540249563753605, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36856245143222827, "std_of_first_answer": 0.36856245143222827, "avg_entropy_of_first_answer": 3.5418871343135834, "lex_sim": 1.0}
{"question": "Which British band released their debut album \u2018Definitely Maybe\u2019 in 1994?", "answer": ["The Stone Roses"], "scores": [[0.18962958455085754, 0.2361118197441101, 0.9982873797416687]], "normalized_score": [0.35488948225975037], "tokens": [["\u0120The", "\u0120Stone", "\u0120Roses"]], "entropy": [[2.929516315460205, 3.3450589179992676, 0.019263695925474167]], "avg_entropy": [2.0979463097949824], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Oasis (Geography)", "score_of_biggest_cluster": 0.35488948225975037, "score_of_first_answer": 0.35488948225975037, "max_score_of_first_answer": 0.9982873797416687, "min_score_of_first_answer": 0.18962958455085754, "avg_score": 0.35488948225975037, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37073494757321296, "std_of_first_answer": 0.37073494757321296, "avg_entropy_of_first_answer": 2.0979463097949824, "lex_sim": 1.0}
{"question": "\"What has been described as the \"\"fifth\"\" taste, after sweet, sour, bitter, and salty?\"", "answer": ["Umami"], "scores": [[0.8039460182189941, 0.9995585083961487]], "normalized_score": [0.8964323997497559], "tokens": [["\u0120Um", "ami"]], "entropy": [[1.3806744813919067, 0.005341885145753622]], "avg_entropy": [0.6930081832688302], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Savory taste", "score_of_biggest_cluster": 0.8964323997497559, "score_of_first_answer": 0.8964323997497559, "max_score_of_first_answer": 0.9995585083961487, "min_score_of_first_answer": 0.8039460182189941, "avg_score": 0.8964323997497559, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.09780624508857727, "std_of_first_answer": 0.09780624508857727, "avg_entropy_of_first_answer": 0.6930081832688302, "lex_sim": 1.0}
{"question": "Long Island Sound separates Long Island from which American state?", "answer": ["CT"], "scores": [[0.0016946472460404038]], "normalized_score": [0.0016946475952863693], "tokens": [["\u0120CT"]], "entropy": [[1.1168239116668701]], "avg_entropy": [1.1168239116668701], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Economy of Connecticut", "score_of_biggest_cluster": 0.0016946475952863693, "score_of_first_answer": 0.0016946475952863693, "max_score_of_first_answer": 0.0016946472460404038, "min_score_of_first_answer": 0.0016946472460404038, "avg_score": 0.0016946475952863693, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.1168239116668701, "lex_sim": 1.0}
{"question": "What instrument has a long neck and a round body of parchment stretched over a metal frame?", "answer": ["Ondioline"], "scores": [[0.010820157825946808, 0.024684391915798187, 0.05049886554479599, 0.7309452295303345]], "normalized_score": [0.056034523993730545], "tokens": [["\u0120O", "nd", "iol", "ine"]], "entropy": [[5.032370090484619, 3.228472948074341, 1.8166886568069458, 1.2604756355285645]], "avg_entropy": [2.8345018327236176], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Open back banjo", "score_of_biggest_cluster": 0.056034523993730545, "score_of_first_answer": 0.056034523993730545, "max_score_of_first_answer": 0.7309452295303345, "min_score_of_first_answer": 0.010820157825946808, "avg_score": 0.056034523993730545, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3044282292047928, "std_of_first_answer": 0.3044282292047928, "avg_entropy_of_first_answer": 2.8345018327236176, "lex_sim": 1.0}
{"question": "Who plays the bounty hunter Carson Wells in the 2007 film \u2018No Country For Old Men\u2019?", "answer": ["Matt Damon"], "scores": [[0.03196491301059723, 0.9173089861869812]], "normalized_score": [0.1712358146905899], "tokens": [["\u0120Matt", "\u0120Damon"]], "entropy": [[3.677407741546631, 0.5867326855659485]], "avg_entropy": [2.1320702135562897], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Woody Harelson", "score_of_biggest_cluster": 0.1712358146905899, "score_of_first_answer": 0.1712358146905899, "max_score_of_first_answer": 0.9173089861869812, "min_score_of_first_answer": 0.03196491301059723, "avg_score": 0.1712358146905899, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.442672036588192, "std_of_first_answer": 0.442672036588192, "avg_entropy_of_first_answer": 2.1320702135562897, "lex_sim": 1.0}
{"question": "Robert Walden Cassotto achieved fame under what name?", "answer": ["Robert Cassotto was a jazz trombonist and composer."], "scores": [[0.062476545572280884, 0.13720807433128357, 0.9581896066665649, 0.000829919648822397, 0.3359278738498688, 0.0031755641102790833, 0.013769561424851418, 0.9986183643341064, 0.8477476835250854, 0.9933422207832336, 0.13510975241661072, 0.20632892847061157, 0.31774309277534485, 0.038879334926605225, 0.09179449081420898, 0.4875497817993164, 0.974634051322937, 0.33255285024642944, 0.0011278087040409446, 0.3059869110584259, 0.027183692902326584, 0.07448659092187881, 0.25425487756729126, 0.23983126878738403]], "normalized_score": [0.11277832835912704], "tokens": [["\u0120Robert", "\u0120Cass", "otto", "\u0120was", "\u0120a", "\u0120jazz", "\u0120t", "rom", "bon", "ist", "\u0120and", "\u0120composer", ".", "\u0120His", "\u0120best", "-", "known", "\u0120composition", "\u0120includes", "\u0120\"", "I", "\u0120Love", "\u0120Paris", "\"."]], "entropy": [[6.8846282958984375, 4.569704055786133, 0.384027361869812, 0.658300518989563, 3.0008692741394043, 5.859391689300537, 3.1150383949279785, 0.018274784088134766, 0.48026737570762634, 0.062252916395664215, 2.4691195487976074, 4.183071136474609, 2.261035203933716, 1.2853288650512695, 5.011051654815674, 1.0318794250488281, 0.1674143224954605, 2.7701382637023926, 1.1015121936798096, 3.388223886489868, 6.123613357543945, 4.030871391296387, 3.025698661804199, 2.1649320125579834]], "avg_entropy": [2.668610191283127], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bobby Darrin", "score_of_biggest_cluster": 0.11277832835912704, "score_of_first_answer": 0.11277832835912704, "max_score_of_first_answer": 0.9986183643341064, "min_score_of_first_answer": 0.000829919648822397, "avg_score": 0.11277832835912704, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34690971331279485, "std_of_first_answer": 0.34690971331279485, "avg_entropy_of_first_answer": 2.668610191283127, "lex_sim": 1.0}
{"question": "Which Parliamentary Act was abbreviated to DORA?", "answer": ["House of Lords (Obstruction of Reference) Act 1939"], "scores": [[0.0011775370221585035, 0.9077045917510986, 0.39938604831695557, 0.029892273247241974, 0.043115902692079544, 0.2401455044746399, 0.8478948473930359, 0.040233444422483444, 0.2659126818180084, 0.856827974319458, 0.004094414878636599]], "normalized_score": [0.09242602437734604], "tokens": [["\u0120House", "\u0120of", "\u0120Lords", "\u0120(", "Ob", "struction", "\u0120of", "\u0120Reference", ")", "\u0120Act", "\u01201939"]], "entropy": [[5.222114562988281, 0.7595220804214478, 2.8871331214904785, 2.279129981994629, 4.507753372192383, 2.0894479751586914, 0.7689469456672668, 4.186905860900879, 2.3553626537323, 0.7310278415679932, 3.1061205863952637]], "avg_entropy": [2.6266786347736013], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Defense of the realm act", "score_of_biggest_cluster": 0.09242602437734604, "score_of_first_answer": 0.09242602437734604, "max_score_of_first_answer": 0.9077045917510986, "min_score_of_first_answer": 0.0011775370221585035, "avg_score": 0.09242602437734604, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.352544976759409, "std_of_first_answer": 0.352544976759409, "avg_entropy_of_first_answer": 2.6266786347736013, "lex_sim": 1.0}
{"question": "\u2018A crutch and a duck\u2019 is the term for which bingo number?", "answer": ["13"], "scores": [[0.014644739218056202]], "normalized_score": [0.014644736424088478], "tokens": [["\u012013"]], "entropy": [[5.709725379943848]], "avg_entropy": [5.709725379943848], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "seventy-two", "score_of_biggest_cluster": 0.014644736424088478, "score_of_first_answer": 0.014644736424088478, "max_score_of_first_answer": 0.014644739218056202, "min_score_of_first_answer": 0.014644739218056202, "avg_score": 0.014644736424088478, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.709725379943848, "lex_sim": 1.0}
{"question": "A 'Comedo' is the medical name for what?", "answer": ["A facial tear trough"], "scores": [[0.23203833401203156, 0.002226148033514619, 0.019004663452506065, 0.21555188298225403]], "normalized_score": [0.03814006596803665], "tokens": [["\u0120A", "\u0120facial", "\u0120tear", "\u0120trough"]], "entropy": [[5.2992730140686035, 6.9937744140625, 5.017854690551758, 2.2055633068084717]], "avg_entropy": [4.879116356372833], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Comedone", "score_of_biggest_cluster": 0.03814006596803665, "score_of_first_answer": 0.03814006596803665, "max_score_of_first_answer": 0.23203833401203156, "min_score_of_first_answer": 0.002226148033514619, "avg_score": 0.03814006596803665, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1069138046394753, "std_of_first_answer": 0.1069138046394753, "avg_entropy_of_first_answer": 4.879116356372833, "lex_sim": 1.0}
{"question": "Which song was a UK number one for Terry Jacks in 1974 and for Westlife in 1999?", "answer": ["She Blinded Me with Science"], "scores": [[0.0023659011349081993, 0.0018812157213687897, 0.9960875511169434, 0.9683425426483154, 0.2857237756252289, 0.9811228513717651]], "normalized_score": [0.10313481092453003], "tokens": [["\u0120She", "\u0120Blind", "ed", "\u0120Me", "\u0120with", "\u0120Science"]], "entropy": [[5.986924171447754, 3.66451358795166, 0.03262275829911232, 0.15104594826698303, 0.6136974096298218, 0.13960057497024536]], "avg_entropy": [1.7647340750942628], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Seasons in the Sun", "score_of_biggest_cluster": 0.10313481092453003, "score_of_first_answer": 0.10313481092453003, "max_score_of_first_answer": 0.9960875511169434, "min_score_of_first_answer": 0.0018812157213687897, "avg_score": 0.10313481092453003, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45265105532640215, "std_of_first_answer": 0.45265105532640215, "avg_entropy_of_first_answer": 1.7647340750942628, "lex_sim": 1.0}
{"question": "The Erythraean Sea, lying between Africa and Asia is more commonly called the (What?) Sea?", "answer": ["THE BOTTOMLESS SEA"], "scores": [[0.0002614989352878183, 0.008874421939253807, 0.036893755197525024, 0.8711264729499817, 0.4052927792072296, 0.5582402944564819]], "normalized_score": [0.05064528062939644], "tokens": [["\u0120THE", "\u0120B", "OTT", "OM", "LESS", "\u0120SEA"]], "entropy": [[3.6565561294555664, 4.547672271728516, 4.7191314697265625, 0.4846079647541046, 1.781362533569336, 2.010098934173584]], "avg_entropy": [2.866571550567945], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Arabicus Sinus", "score_of_biggest_cluster": 0.05064528062939644, "score_of_first_answer": 0.05064528062939644, "max_score_of_first_answer": 0.8711264729499817, "min_score_of_first_answer": 0.0002614989352878183, "avg_score": 0.05064528062939644, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32830298742285247, "std_of_first_answer": 0.32830298742285247, "avg_entropy_of_first_answer": 2.866571550567945, "lex_sim": 1.0}
{"question": "With reference to interest rates for what does the T stand in TIBOR?", "answer": ["The target interest rate"], "scores": [[0.05659782513976097, 0.010331286117434502, 0.10017682611942291, 0.7986300587654114]], "normalized_score": [0.08270213752985], "tokens": [["\u0120The", "\u0120target", "\u0120interest", "\u0120rate"]], "entropy": [[4.766425132751465, 5.667396545410156, 2.4303321838378906, 1.2103685140609741]], "avg_entropy": [3.5186305940151215], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u6771\u4eac\u90fd", "score_of_biggest_cluster": 0.08270213752985, "score_of_first_answer": 0.08270213752985, "max_score_of_first_answer": 0.7986300587654114, "min_score_of_first_answer": 0.010331286117434502, "avg_score": 0.08270213752985, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32326224376552515, "std_of_first_answer": 0.32326224376552515, "avg_entropy_of_first_answer": 3.5186305940151215, "lex_sim": 1.0}
{"question": "Who is the protagonist and narrator of Robert Louis Stevenson's novels 'Kidnapped' and 'Catriona'?", "answer": ["David Balfour"], "scores": [[0.5806359052658081, 0.9864596128463745, 0.9959153532981873, 0.9929393529891968]], "normalized_score": [0.8675251603126526], "tokens": [["\u0120David", "\u0120B", "alf", "our"]], "entropy": [[2.8033716678619385, 0.1319480538368225, 0.036161214113235474, 0.052760425955057144]], "avg_entropy": [0.7560603404417634], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "David Balfour", "score_of_biggest_cluster": 0.8675251603126526, "score_of_first_answer": 0.8675251603126526, "max_score_of_first_answer": 0.9959153532981873, "min_score_of_first_answer": 0.5806359052658081, "avg_score": 0.8675251603126526, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.17805973176766365, "std_of_first_answer": 0.17805973176766365, "avg_entropy_of_first_answer": 0.7560603404417634, "lex_sim": 1.0}
{"question": "Which English football league team are known as The Posh'?", "answer": ["Manchester United"], "scores": [[0.16745592653751373, 0.7282790541648865]], "normalized_score": [0.34922003746032715], "tokens": [["\u0120Manchester", "\u0120United"]], "entropy": [[4.340703964233398, 1.1827424764633179]], "avg_entropy": [2.761723220348358], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Posh", "score_of_biggest_cluster": 0.34922003746032715, "score_of_first_answer": 0.34922003746032715, "max_score_of_first_answer": 0.7282790541648865, "min_score_of_first_answer": 0.16745592653751373, "avg_score": 0.34922003746032715, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28041156381368637, "std_of_first_answer": 0.28041156381368637, "avg_entropy_of_first_answer": 2.761723220348358, "lex_sim": 1.0}
{"question": "The Goya Gate is located at which European museum?", "answer": ["National Museum of Goya and Etching"], "scores": [[0.04392078518867493, 0.2132217139005661, 0.8196927905082703, 0.035973384976387024, 0.675775408744812, 0.027628403156995773, 0.0030862942803651094, 0.2031199336051941]], "normalized_score": [0.08683308959007263], "tokens": [["\u0120National", "\u0120Museum", "\u0120of", "\u0120G", "oya", "\u0120and", "\u0120Et", "ching"]], "entropy": [[3.4955105781555176, 2.5409514904022217, 0.9392548203468323, 3.8151543140411377, 1.8005907535552979, 1.6054037809371948, 3.69022798538208, 2.392324686050415]], "avg_entropy": [2.534927301108837], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Prado", "score_of_biggest_cluster": 0.08683308959007263, "score_of_first_answer": 0.08683308959007263, "max_score_of_first_answer": 0.8196927905082703, "min_score_of_first_answer": 0.0030862942803651094, "avg_score": 0.08683308959007263, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2974923680173854, "std_of_first_answer": 0.2974923680173854, "avg_entropy_of_first_answer": 2.534927301108837, "lex_sim": 1.0}
{"question": "The song with theshortest title to have reached number one in the UK charts was released by Telly Savalas in 1975. What was it called?", "answer": ["\"I'll be Glad When You're Dead, You Rascal, You\""], "scores": [[0.25323086977005005, 0.04016938805580139, 0.11931131035089493, 0.1507105976343155, 0.002722603501752019, 0.9397386312484741, 0.9288825392723083, 0.949310302734375, 0.989521861076355, 0.5908410549163818, 0.9674025774002075, 0.9934746623039246, 0.8883379101753235, 0.18152210116386414, 0.9837877750396729, 0.7537866830825806]], "normalized_score": [0.33704057335853577], "tokens": [["\u0120\"", "I", "'ll", "\u0120be", "\u0120Glad", "\u0120When", "\u0120You", "'re", "\u0120Dead", ",", "\u0120You", "\u0120R", "ascal", ",", "\u0120You", "\""]], "entropy": [[5.289212226867676, 5.47007942199707, 4.035220146179199, 3.3932080268859863, 3.9597959518432617, 0.34645456075668335, 0.4854508638381958, 0.33791881799697876, 0.08220545947551727, 1.8545831441879272, 0.2678576707839966, 0.0642322450876236, 0.7035015821456909, 0.9975541830062866, 0.14073538780212402, 1.2135342359542847]], "avg_entropy": [1.7900964953005314], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "I.F.", "score_of_biggest_cluster": 0.33704057335853577, "score_of_first_answer": 0.33704057335853577, "max_score_of_first_answer": 0.9934746623039246, "min_score_of_first_answer": 0.002722603501752019, "avg_score": 0.33704057335853577, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3904043075376845, "std_of_first_answer": 0.3904043075376845, "avg_entropy_of_first_answer": 1.7900964953005314, "lex_sim": 1.0}
{"question": "In what US state is Mount Rushmore?", "answer": ["Black Rushmore"], "scores": [[0.000764071533922106, 9.835495438892394e-05, 0.5187674760818481]], "normalized_score": [0.0033907887991517782], "tokens": [["\u0120Black", "\u0120Rush", "more"]], "entropy": [[1.5306669473648071, 1.9162911176681519, 2.061440944671631]], "avg_entropy": [1.8361330032348633], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "South dakota", "score_of_biggest_cluster": 0.0033907887991517782, "score_of_first_answer": 0.0033907887991517782, "max_score_of_first_answer": 0.5187674760818481, "min_score_of_first_answer": 9.835495438892394e-05, "avg_score": 0.0033907887991517782, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24434620873600585, "std_of_first_answer": 0.24434620873600585, "avg_entropy_of_first_answer": 1.8361330032348633, "lex_sim": 1.0}
{"question": "Which monarch said 'Will no-one rid me of this turbulent priest'?", "answer": ["Thomas More"], "scores": [[0.0014906281139701605, 0.08967149257659912]], "normalized_score": [0.011561436578631401], "tokens": [["\u0120Thomas", "\u0120More"]], "entropy": [[3.4663162231445312, 2.733396291732788]], "avg_entropy": [3.0998562574386597], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Henry II", "score_of_biggest_cluster": 0.011561436578631401, "score_of_first_answer": 0.011561436578631401, "max_score_of_first_answer": 0.08967149257659912, "min_score_of_first_answer": 0.0014906281139701605, "avg_score": 0.011561436578631401, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04409043223131448, "std_of_first_answer": 0.04409043223131448, "avg_entropy_of_first_answer": 3.0998562574386597, "lex_sim": 1.0}
{"question": "The Sinai Peninsula is part of which country?", "answer": ["Egypt"], "scores": [[0.8874732255935669]], "normalized_score": [0.8874732255935669], "tokens": [["\u0120Egypt"]], "entropy": [[0.8238672018051147]], "avg_entropy": [0.8238672018051147], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "EGY", "score_of_biggest_cluster": 0.8874732255935669, "score_of_first_answer": 0.8874732255935669, "max_score_of_first_answer": 0.8874732255935669, "min_score_of_first_answer": 0.8874732255935669, "avg_score": 0.8874732255935669, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.8238672018051147, "lex_sim": 1.0}
{"question": "What ancient monument and tourist attraction would you see at Carnac in northern France?", "answer": ["Long Barrow"], "scores": [[0.0019733018707484007, 0.06690884381532669, 0.571708083152771]], "normalized_score": [0.042262036353349686], "tokens": [["\u0120Long", "\u0120Bar", "row"]], "entropy": [[3.9125120639801025, 3.773149251937866, 0.696122407913208]], "avg_entropy": [2.7939279079437256], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bautasten", "score_of_biggest_cluster": 0.042262036353349686, "score_of_first_answer": 0.042262036353349686, "max_score_of_first_answer": 0.571708083152771, "min_score_of_first_answer": 0.0019733018707484007, "avg_score": 0.042262036353349686, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2546537120714177, "std_of_first_answer": 0.2546537120714177, "avg_entropy_of_first_answer": 2.7939279079437256, "lex_sim": 1.0}
{"question": "Who was Richard Nixon's running mate in the 1960 US presidential election?", "answer": ["Henry Wallace"], "scores": [[0.6570815443992615, 0.014797253534197807]], "normalized_score": [0.09860529005527496], "tokens": [["\u0120Henry", "\u0120Wallace"]], "entropy": [[2.0649259090423584, 1.5038280487060547]], "avg_entropy": [1.7843769788742065], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lodge, Henry Cabot", "score_of_biggest_cluster": 0.09860529005527496, "score_of_first_answer": 0.09860529005527496, "max_score_of_first_answer": 0.6570815443992615, "min_score_of_first_answer": 0.014797253534197807, "avg_score": 0.09860529005527496, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32114214543253183, "std_of_first_answer": 0.32114214543253183, "avg_entropy_of_first_answer": 1.7843769788742065, "lex_sim": 1.0}
{"question": "\"Who developed the men's scent \"\"I Am King\"\", which was first marketed in February 2009?\"", "answer": ["Tom Ford"], "scores": [[0.017110317945480347, 0.8711951971054077]], "normalized_score": [0.1220918595790863], "tokens": [["\u0120Tom", "\u0120Ford"]], "entropy": [[6.0106401443481445, 1.174019455909729]], "avg_entropy": [3.5923298001289368], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sean Comb", "score_of_biggest_cluster": 0.1220918595790863, "score_of_first_answer": 0.1220918595790863, "max_score_of_first_answer": 0.8711951971054077, "min_score_of_first_answer": 0.017110317945480347, "avg_score": 0.1220918595790863, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4270424395799637, "std_of_first_answer": 0.4270424395799637, "avg_entropy_of_first_answer": 3.5923298001289368, "lex_sim": 1.0}
{"question": "What element is the main constituent of anthracite?", "answer": ["Carbon"], "scores": [[0.6460923552513123]], "normalized_score": [0.6460923552513123], "tokens": [["\u0120Carbon"]], "entropy": [[1.4269347190856934]], "avg_entropy": [1.4269347190856934], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Carbon atom", "score_of_biggest_cluster": 0.6460923552513123, "score_of_first_answer": 0.6460923552513123, "max_score_of_first_answer": 0.6460923552513123, "min_score_of_first_answer": 0.6460923552513123, "avg_score": 0.6460923552513123, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.4269347190856934, "lex_sim": 1.0}
{"question": "Which Gilbert and Sullivan opera has the alternate title 'Bunthome's Bride'?", "answer": ["The Pirates of Penzance"], "scores": [[0.17688457667827606, 0.14904610812664032, 0.8066497445106506, 0.9805424213409424, 0.9503019452095032, 0.9969237446784973]], "normalized_score": [0.51993328332901], "tokens": [["\u0120The", "\u0120Pirates", "\u0120of", "\u0120Pen", "z", "ance"]], "entropy": [[3.0009427070617676, 2.554976463317871, 0.8248085975646973, 0.130386620759964, 0.420263409614563, 0.03711985796689987]], "avg_entropy": [1.1614162760476272], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Patience", "score_of_biggest_cluster": 0.51993328332901, "score_of_first_answer": 0.51993328332901, "max_score_of_first_answer": 0.9969237446784973, "min_score_of_first_answer": 0.14904610812664032, "avg_score": 0.51993328332901, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3685201277870379, "std_of_first_answer": 0.3685201277870379, "avg_entropy_of_first_answer": 1.1614162760476272, "lex_sim": 1.0}
{"question": "What is the name of Tokyo's seaport ?", "answer": ["Yurio"], "scores": [[0.035914141684770584, 0.38728925585746765, 0.01402121689170599]], "normalized_score": [0.05799122154712677], "tokens": [["\u0120Y", "ur", "io"]], "entropy": [[3.8011374473571777, 2.8735263347625732, 3.415325403213501]], "avg_entropy": [3.3633297284444175], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u6a2a\u6d5c", "score_of_biggest_cluster": 0.05799122154712677, "score_of_first_answer": 0.05799122154712677, "max_score_of_first_answer": 0.38728925585746765, "min_score_of_first_answer": 0.01402121689170599, "avg_score": 0.05799122154712677, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17103371984924037, "std_of_first_answer": 0.17103371984924037, "avg_entropy_of_first_answer": 3.3633297284444175, "lex_sim": 1.0}
{"question": "In imperial measure, how many gills are there in a gallon?", "answer": ["24"], "scores": [[0.004232455510646105]], "normalized_score": [0.004232455510646105], "tokens": [["\u012024"]], "entropy": [[4.686004161834717]], "avg_entropy": [4.686004161834717], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "32", "score_of_biggest_cluster": 0.004232455510646105, "score_of_first_answer": 0.004232455510646105, "max_score_of_first_answer": 0.004232455510646105, "min_score_of_first_answer": 0.004232455510646105, "avg_score": 0.004232455510646105, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.686004161834717, "lex_sim": 1.0}
{"question": "\"John Wayne appeared as a centurion in the film \"\"The Greatest Story Ever Told\"\" (1965). What was his famous line?\"", "answer": ["We did it!"], "scores": [[0.004759401082992554, 0.011108111590147018, 0.6086999773979187, 0.23964715003967285, 0.0367722325026989, 0.19555552303791046, 0.02050701528787613, 0.8827071785926819]], "normalized_score": [0.07502564787864685], "tokens": [["\u0120We", "\u0120did", "\u0120it", "!", "\u0120We", "'re", "\u0120saved", "!"]], "entropy": [[3.8967411518096924, 4.267714023590088, 1.7944813966751099, 1.8751404285430908, 0.9671212434768677, 3.334822654724121, 3.8815178871154785, 0.6310760974884033]], "avg_entropy": [2.5810768604278564], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Truly this man was the son of God", "score_of_biggest_cluster": 0.07502564787864685, "score_of_first_answer": 0.07502564787864685, "max_score_of_first_answer": 0.8827071785926819, "min_score_of_first_answer": 0.004759401082992554, "avg_score": 0.07502564787864685, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3056552018543595, "std_of_first_answer": 0.3056552018543595, "avg_entropy_of_first_answer": 2.5810768604278564, "lex_sim": 1.0}
{"question": "What can be a medieval weapon, a spice, or a Jedi knight (or a pepper spray)", "answer": ["Pepper spray"], "scores": [[0.05205657333135605, 0.3896338939666748]], "normalized_score": [0.14241839945316315], "tokens": [["\u0120Pepper", "\u0120spray"]], "entropy": [[5.472212791442871, 2.4020771980285645]], "avg_entropy": [3.9371449947357178], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "MACE", "score_of_biggest_cluster": 0.14241839945316315, "score_of_first_answer": 0.14241839945316315, "max_score_of_first_answer": 0.3896338939666748, "min_score_of_first_answer": 0.05205657333135605, "avg_score": 0.14241839945316315, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.16878866031765938, "std_of_first_answer": 0.16878866031765938, "avg_entropy_of_first_answer": 3.9371449947357178, "lex_sim": 1.0}
{"question": "Which is the only English football team, in the top four divisions, to have a Latin word in its name?", "answer": ["St. Helens"], "scores": [[0.006237622816115618, 0.41105517745018005, 0.08251912146806717, 0.9970153570175171]], "normalized_score": [0.12051580101251602], "tokens": [["\u0120St", ".", "\u0120Hel", "ens"]], "entropy": [[5.416668891906738, 3.314180374145508, 4.77001953125, 0.033149752765893936]], "avg_entropy": [3.383504637517035], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Aston Villa", "score_of_biggest_cluster": 0.12051580101251602, "score_of_first_answer": 0.12051580101251602, "max_score_of_first_answer": 0.9970153570175171, "min_score_of_first_answer": 0.006237622816115618, "avg_score": 0.12051580101251602, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39042647156016597, "std_of_first_answer": 0.39042647156016597, "avg_entropy_of_first_answer": 3.383504637517035, "lex_sim": 1.0}
{"question": "Who played Thelma in the UK television series \u2018The Likely Lads\u2019?", "answer": ["Thelma Barlow"], "scores": [[0.012398106046020985, 0.7957831025123596, 0.9995757937431335, 0.09241713583469391, 0.969436526298523]], "normalized_score": [0.2450459599494934], "tokens": [["\u0120The", "l", "ma", "\u0120Bar", "low"]], "entropy": [[6.237229347229004, 1.8971138000488281, 0.006205370649695396, 5.821479797363281, 0.26558923721313477]], "avg_entropy": [2.8455235105007888], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brigit Forsyth", "score_of_biggest_cluster": 0.2450459599494934, "score_of_first_answer": 0.2450459599494934, "max_score_of_first_answer": 0.9995757937431335, "min_score_of_first_answer": 0.012398106046020985, "avg_score": 0.2450459599494934, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43220163330211236, "std_of_first_answer": 0.43220163330211236, "avg_entropy_of_first_answer": 2.8455235105007888, "lex_sim": 1.0}
{"question": "Peter Schmeiche\u2019s son is currently the regular goalkeeper at which English football league team?", "answer": ["Man City"], "scores": [[0.006922882981598377, 0.26760441064834595]], "normalized_score": [0.04304177686572075], "tokens": [["\u0120Man", "\u0120City"]], "entropy": [[4.506833076477051, 3.092895984649658]], "avg_entropy": [3.7998645305633545], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Leicester Fosse FC", "score_of_biggest_cluster": 0.04304177686572075, "score_of_first_answer": 0.04304177686572075, "max_score_of_first_answer": 0.26760441064834595, "min_score_of_first_answer": 0.006922882981598377, "avg_score": 0.04304177686572075, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13034076383337379, "std_of_first_answer": 0.13034076383337379, "avg_entropy_of_first_answer": 3.7998645305633545, "lex_sim": 1.0}
{"question": "In cockney rhyming slang what is a dog and bone", "answer": ["A good, good bone"], "scores": [[0.12533226609230042, 0.003466683207079768, 0.01532154530286789, 0.02106781117618084, 0.034732747822999954]], "normalized_score": [0.021753419190645218], "tokens": [["\u0120A", "\u0120good", ",", "\u0120good", "\u0120bone"]], "entropy": [[5.971169948577881, 6.3848772048950195, 5.43278169631958, 4.537349224090576, 4.3499650955200195]], "avg_entropy": [5.335228633880615], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u2121", "score_of_biggest_cluster": 0.021753419190645218, "score_of_first_answer": 0.021753419190645218, "max_score_of_first_answer": 0.12533226609230042, "min_score_of_first_answer": 0.003466683207079768, "avg_score": 0.021753419190645218, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0438439858389438, "std_of_first_answer": 0.0438439858389438, "avg_entropy_of_first_answer": 5.335228633880615, "lex_sim": 1.0}
{"question": "In 2006 Monty Panesar became the first person of what religion to play test cricket for England?", "answer": ["Mormon"], "scores": [[0.0017603207379579544]], "normalized_score": [0.0017603205051273108], "tokens": [["\u0120Mormon"]], "entropy": [[3.5450408458709717]], "avg_entropy": [3.5450408458709717], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sikhni", "score_of_biggest_cluster": 0.0017603205051273108, "score_of_first_answer": 0.0017603205051273108, "max_score_of_first_answer": 0.0017603207379579544, "min_score_of_first_answer": 0.0017603207379579544, "avg_score": 0.0017603205051273108, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.5450408458709717, "lex_sim": 1.0}
{"question": "Who, in 1964 became the first cricketer to take 300 wickets?", "answer": ["Sir Garfield Sobers"], "scores": [[0.01646476797759533, 0.19142678380012512, 0.9338383078575134, 0.999643087387085]], "normalized_score": [0.23289959132671356], "tokens": [["\u0120Sir", "\u0120Garfield", "\u0120Sob", "ers"]], "entropy": [[4.875253677368164, 3.4779791831970215, 0.6299861669540405, 0.004463626071810722]], "avg_entropy": [2.246920663397759], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fiery Fred", "score_of_biggest_cluster": 0.23289959132671356, "score_of_first_answer": 0.23289959132671356, "max_score_of_first_answer": 0.999643087387085, "min_score_of_first_answer": 0.01646476797759533, "avg_score": 0.23289959132671356, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4364304254299362, "std_of_first_answer": 0.4364304254299362, "avg_entropy_of_first_answer": 2.246920663397759, "lex_sim": 1.0}
{"question": "Found in the garden what type of insect is a looper", "answer": ["A lady's slipper hawk moth"], "scores": [[0.08770144730806351, 0.006825166288763285, 0.002128471154719591, 0.41840922832489014, 0.9967747330665588, 0.00019938441982958466, 0.4514169991016388]], "normalized_score": [0.03354787081480026], "tokens": [["\u0120A", "\u0120lady", "'s", "\u0120sl", "ipper", "\u0120hawk", "\u0120moth"]], "entropy": [[5.7965407371521, 5.635309219360352, 1.308070182800293, 3.0769882202148438, 0.02496042475104332, 0.9303358793258667, 1.4846034049987793]], "avg_entropy": [2.6081154383718967], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Catepillars", "score_of_biggest_cluster": 0.03354787081480026, "score_of_first_answer": 0.03354787081480026, "max_score_of_first_answer": 0.9967747330665588, "min_score_of_first_answer": 0.00019938441982958466, "avg_score": 0.03354787081480026, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34421949153703785, "std_of_first_answer": 0.34421949153703785, "avg_entropy_of_first_answer": 2.6081154383718967, "lex_sim": 1.0}
{"question": "Which famous novelist also wrote under the pseudonym Mary Westmacott?", "answer": ["The author"], "scores": [[0.004443824756890535, 0.04622409865260124]], "normalized_score": [0.014332194812595844], "tokens": [["\u0120The", "\u0120author"]], "entropy": [[4.624571800231934, 5.7176713943481445]], "avg_entropy": [5.171121597290039], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Christie Estate", "score_of_biggest_cluster": 0.014332194812595844, "score_of_first_answer": 0.014332194812595844, "max_score_of_first_answer": 0.04622409865260124, "min_score_of_first_answer": 0.004443824756890535, "avg_score": 0.014332194812595844, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.020890136947855353, "std_of_first_answer": 0.020890136947855353, "avg_entropy_of_first_answer": 5.171121597290039, "lex_sim": 1.0}
{"question": "Which 1986 Peter Weir movie and based on a Paul Theroux novel starred Harrison Ford, Helen Mirren and River Phoenix?", "answer": ["The Mosquito Coast"], "scores": [[0.13106387853622437, 0.10161367058753967, 0.9527356624603271, 0.9990026354789734, 0.9878259301185608]], "normalized_score": [0.4164193868637085], "tokens": [["\u0120The", "\u0120Mos", "qu", "ito", "\u0120Coast"]], "entropy": [[2.2996840476989746, 3.9427592754364014, 0.25756391882896423, 0.010942334309220314, 0.10839949548244476]], "avg_entropy": [1.323869814351201], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Mosquito Coast", "score_of_biggest_cluster": 0.4164193868637085, "score_of_first_answer": 0.4164193868637085, "max_score_of_first_answer": 0.9990026354789734, "min_score_of_first_answer": 0.10161367058753967, "avg_score": 0.4164193868637085, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.42341258275119703, "std_of_first_answer": 0.42341258275119703, "avg_entropy_of_first_answer": 1.323869814351201, "lex_sim": 1.0}
{"question": "Regarded as its Nobel Prize, in what field is the annual Pritzer Prize awarded?", "answer": ["Chemical Engineering"], "scores": [[0.008710919879376888, 0.11789807677268982]], "normalized_score": [0.0320468544960022], "tokens": [["\u0120Chemical", "\u0120Engineering"]], "entropy": [[5.235123634338379, 3.268523931503296]], "avg_entropy": [4.251823782920837], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Architecturally", "score_of_biggest_cluster": 0.0320468544960022, "score_of_first_answer": 0.0320468544960022, "max_score_of_first_answer": 0.11789807677268982, "min_score_of_first_answer": 0.008710919879376888, "avg_score": 0.0320468544960022, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.054593578446656466, "std_of_first_answer": 0.054593578446656466, "avg_entropy_of_first_answer": 4.251823782920837, "lex_sim": 1.0}
{"question": "The initial parts of which London building were built as a fortress by the Normans, had additions to become a palace and then was used primarily as a prison?", "answer": ["Tower of London"], "scores": [[0.23039555549621582, 0.7989586591720581, 0.9891545176506042]], "normalized_score": [0.5667882561683655], "tokens": [["\u0120Tower", "\u0120of", "\u0120London"]], "entropy": [[3.868960380554199, 1.1430119276046753, 0.12135014683008194]], "avg_entropy": [1.7111074849963188], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "London Tower", "score_of_biggest_cluster": 0.5667882561683655, "score_of_first_answer": 0.5667882561683655, "max_score_of_first_answer": 0.9891545176506042, "min_score_of_first_answer": 0.23039555549621582, "avg_score": 0.5667882561683655, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32234447241930186, "std_of_first_answer": 0.32234447241930186, "avg_entropy_of_first_answer": 1.7111074849963188, "lex_sim": 1.0}
{"question": "\"Damon Runyon's two stories \"\"The Idyll of Miss Sarah Brown\"\" and \"\"Blood Pressure\"\" were the basis for which musical?\"", "answer": ["Cabaret"], "scores": [[0.0036126216873526573, 0.9867880344390869]], "normalized_score": [0.05970671772956848], "tokens": [["\u0120Cab", "aret"]], "entropy": [[3.566756248474121, 0.0992220863699913]], "avg_entropy": [1.8329891674220562], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Arvide Abernathy", "score_of_biggest_cluster": 0.05970671772956848, "score_of_first_answer": 0.05970671772956848, "max_score_of_first_answer": 0.9867880344390869, "min_score_of_first_answer": 0.0036126216873526573, "avg_score": 0.05970671772956848, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.49158770637586713, "std_of_first_answer": 0.49158770637586713, "avg_entropy_of_first_answer": 1.8329891674220562, "lex_sim": 1.0}
{"question": "Which children's TV show used cabbages as the booby prize?", "answer": ["It's a long \u3450"], "scores": [[0.004391751252114773, 0.793410062789917, 0.33948710560798645, 0.039791807532310486, 0.006095480173826218, 3.387670585652813e-05, 0.012436965480446815, 0.015371222048997879]], "normalized_score": [0.014409058727324009], "tokens": [["\u0120It", "'s", "\u0120a", "\u0120long", "\u0120", "\u00e3", "\u0133", "\u0132"]], "entropy": [[5.600200653076172, 1.2611814737319946, 3.840007781982422, 5.297874927520752, 2.161010980606079, 3.547224521636963, 1.5358084440231323, 4.053325653076172]], "avg_entropy": [3.412079304456711], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cracker-jack", "score_of_biggest_cluster": 0.014409058727324009, "score_of_first_answer": 0.014409058727324009, "max_score_of_first_answer": 0.793410062789917, "min_score_of_first_answer": 3.387670585652813e-05, "avg_score": 0.014409058727324009, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2653914914503464, "std_of_first_answer": 0.2653914914503464, "avg_entropy_of_first_answer": 3.412079304456711, "lex_sim": 1.0}
{"question": "What dance craze was named after a city in South Carolina?", "answer": ["Charleston"], "scores": [[0.5686970949172974]], "normalized_score": [0.5686970949172974], "tokens": [["\u0120Charleston"]], "entropy": [[2.84944224357605]], "avg_entropy": [2.84944224357605], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charleston rhythm", "score_of_biggest_cluster": 0.5686970949172974, "score_of_first_answer": 0.5686970949172974, "max_score_of_first_answer": 0.5686970949172974, "min_score_of_first_answer": 0.5686970949172974, "avg_score": 0.5686970949172974, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.84944224357605, "lex_sim": 1.0}
{"question": "What type of fruit is a Blenheim Orange?", "answer": ["A cross between a pomegranate and a plum."], "scores": [[0.19728314876556396, 0.09302512556314468, 0.9028700590133667, 0.6499093174934387, 0.0491003692150116, 0.3556612730026245, 0.902748703956604, 0.9950898885726929, 0.9456058740615845, 0.6086483597755432, 0.2515312135219574, 0.04240911453962326]], "normalized_score": [0.3171907961368561], "tokens": [["\u0120A", "\u0120cross", "\u0120between", "\u0120a", "\u0120p", "ome", "gran", "ate", "\u0120and", "\u0120a", "\u0120plum", "."]], "entropy": [[4.736422538757324, 5.396060943603516, 0.6112680435180664, 1.2339781522750854, 5.107662200927734, 1.3220335245132446, 0.5738484859466553, 0.044110968708992004, 0.3304869830608368, 1.2580437660217285, 3.83561635017395, 0.6160500049591064]], "avg_entropy": [2.0887984968721867], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Apple Blossom", "score_of_biggest_cluster": 0.3171907961368561, "score_of_first_answer": 0.3171907961368561, "max_score_of_first_answer": 0.9950898885726929, "min_score_of_first_answer": 0.04240911453962326, "avg_score": 0.3171907961368561, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35997683820284354, "std_of_first_answer": 0.35997683820284354, "avg_entropy_of_first_answer": 2.0887984968721867, "lex_sim": 1.0}
{"question": "Which is the largest city on the Caspian Sea?", "answer": ["Astara"], "scores": [[0.2740599513053894, 0.10918913036584854]], "normalized_score": [0.17298659682273865], "tokens": [["\u0120Ast", "ara"]], "entropy": [[3.7003774642944336, 0.7486156225204468]], "avg_entropy": [2.22449654340744], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Benzite/old version", "score_of_biggest_cluster": 0.17298659682273865, "score_of_first_answer": 0.17298659682273865, "max_score_of_first_answer": 0.2740599513053894, "min_score_of_first_answer": 0.10918913036584854, "avg_score": 0.17298659682273865, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08243541046977043, "std_of_first_answer": 0.08243541046977043, "avg_entropy_of_first_answer": 2.22449654340744, "lex_sim": 1.0}
{"question": "The islands called the Moluccas were previously known as what?", "answer": ["Portuguese territory"], "scores": [[0.018406802788376808, 0.004337560851126909]], "normalized_score": [0.008935357443988323], "tokens": [["\u0120Portuguese", "\u0120territory"]], "entropy": [[5.452188491821289, 4.018094062805176]], "avg_entropy": [4.735141277313232], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Spice Islands", "score_of_biggest_cluster": 0.008935357443988323, "score_of_first_answer": 0.008935357443988323, "max_score_of_first_answer": 0.018406802788376808, "min_score_of_first_answer": 0.004337560851126909, "avg_score": 0.008935357443988323, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0070346209686249495, "std_of_first_answer": 0.0070346209686249495, "avg_entropy_of_first_answer": 4.735141277313232, "lex_sim": 1.0}
{"question": "Louis Reard created which two-piece item of clothing in 1946?", "answer": ["Reard was born in 1899, two years after Coco Chanel."], "scores": [[0.004598531872034073, 0.9400292634963989, 0.0025097981560975313, 0.03842402249574661, 0.7370143532752991, 0.01169542595744133, 0.12181634455919266, 0.012440997175872326, 0.6887216567993164, 0.361255407333374, 0.0026675958652049303, 0.9883488416671753, 0.9976227879524231, 0.23796705901622772]], "normalized_score": [0.08384151756763458], "tokens": [["\u0120Re", "ard", "\u0120was", "\u0120born", "\u0120in", "\u01201899", ",", "\u0120two", "\u0120years", "\u0120after", "\u0120Coco", "\u0120Chan", "el", "."]], "entropy": [[5.8861002922058105, 0.5404245257377625, 5.975299835205078, 3.481793165206909, 1.8699779510498047, 5.166393280029297, 2.018993854522705, 4.231045722961426, 1.7280614376068115, 1.2937633991241455, 4.235772132873535, 0.11483100056648254, 0.025176184251904488, 3.252187967300415]], "avg_entropy": [2.844272910617292], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bikinis", "score_of_biggest_cluster": 0.08384151756763458, "score_of_first_answer": 0.08384151756763458, "max_score_of_first_answer": 0.9976227879524231, "min_score_of_first_answer": 0.0025097981560975313, "avg_score": 0.08384151756763458, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3951647947922733, "std_of_first_answer": 0.3951647947922733, "avg_entropy_of_first_answer": 2.844272910617292, "lex_sim": 1.0}
{"question": "Which shipyard built the 'Titanic'?", "answer": ["Messina"], "scores": [[0.0025942246429622173, 0.04853970929980278]], "normalized_score": [0.011221534572541714], "tokens": [["\u0120Mess", "ina"]], "entropy": [[2.628849506378174, 0.9883407354354858]], "avg_entropy": [1.8085951209068298], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Harland & Wolf", "score_of_biggest_cluster": 0.011221534572541714, "score_of_first_answer": 0.011221534572541714, "max_score_of_first_answer": 0.04853970929980278, "min_score_of_first_answer": 0.0025942246429622173, "avg_score": 0.011221534572541714, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02297274232842028, "std_of_first_answer": 0.02297274232842028, "avg_entropy_of_first_answer": 1.8085951209068298, "lex_sim": 1.0}
{"question": "Which author has been nominated for the Booker Prize for the novels 'Briefing For A Descent Into Hell', 'The Sirian Experiments' and 'The Good Terrorist' but is yet to win the award?", "answer": ["C J Sansom"], "scores": [[0.003496363526210189, 0.0363890677690506, 0.754429042339325, 0.8533172011375427]], "normalized_score": [0.09513255953788757], "tokens": [["\u0120C", "\u0120J", "\u0120Sans", "om"]], "entropy": [[5.770482063293457, 3.3358712196350098, 1.898971676826477, 0.7495753169059753]], "avg_entropy": [2.93872506916523], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jane Somers", "score_of_biggest_cluster": 0.09513255953788757, "score_of_first_answer": 0.09513255953788757, "max_score_of_first_answer": 0.8533172011375427, "min_score_of_first_answer": 0.003496363526210189, "avg_score": 0.09513255953788757, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3936931802067272, "std_of_first_answer": 0.3936931802067272, "avg_entropy_of_first_answer": 2.93872506916523, "lex_sim": 1.0}
{"question": "In which city are Argentine, Bel-Air, Bolivar, Liege, Rome and Stalingrad all found within a few miles of each other?", "answer": ["LA"], "scores": [[0.0008266644435934722]], "normalized_score": [0.0008266645018011332], "tokens": [["\u0120LA"]], "entropy": [[4.744344234466553]], "avg_entropy": [4.744344234466553], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pa\u0159\u00ed\u017e", "score_of_biggest_cluster": 0.0008266645018011332, "score_of_first_answer": 0.0008266645018011332, "max_score_of_first_answer": 0.0008266644435934722, "min_score_of_first_answer": 0.0008266644435934722, "avg_score": 0.0008266645018011332, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.744344234466553, "lex_sim": 1.0}
{"question": "Terry Molloy, David Gooderson and Julian Bleach have all portrayed which villain in the UK television series Dr Who?", "answer": ["The Master"], "scores": [[0.4157029688358307, 0.5122864246368408]], "normalized_score": [0.46147480607032776], "tokens": [["\u0120The", "\u0120Master"]], "entropy": [[3.391312599182129, 2.520285129547119]], "avg_entropy": [2.955798864364624], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Davros", "score_of_biggest_cluster": 0.46147480607032776, "score_of_first_answer": 0.46147480607032776, "max_score_of_first_answer": 0.5122864246368408, "min_score_of_first_answer": 0.4157029688358307, "avg_score": 0.46147480607032776, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.048291727900505066, "std_of_first_answer": 0.048291727900505066, "avg_entropy_of_first_answer": 2.955798864364624, "lex_sim": 1.0}
{"question": "What was developed at the Bell Telephone Company under the direction of Dr William Shockley?", "answer": ["The electronic transistor"], "scores": [[0.30928266048431396, 0.004588346462696791, 0.37361302971839905]], "normalized_score": [0.08093654364347458], "tokens": [["\u0120The", "\u0120electronic", "\u0120transistor"]], "entropy": [[4.323030471801758, 2.4949917793273926, 3.0881803035736084]], "avg_entropy": [3.302067518234253], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Collector (electronics)", "score_of_biggest_cluster": 0.08093654364347458, "score_of_first_answer": 0.08093654364347458, "max_score_of_first_answer": 0.37361302971839905, "min_score_of_first_answer": 0.004588346462696791, "avg_score": 0.08093654364347458, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1609541819742048, "std_of_first_answer": 0.1609541819742048, "avg_entropy_of_first_answer": 3.302067518234253, "lex_sim": 1.0}
{"question": "\"Who was the villain in the movie \"\"Thor\"\"?\"", "answer": ["Loki"], "scores": [[0.46181154251098633]], "normalized_score": [0.46181154251098633], "tokens": [["\u0120Loki"]], "entropy": [[3.6892590522766113]], "avg_entropy": [3.6892590522766113], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Loptr", "score_of_biggest_cluster": 0.46181154251098633, "score_of_first_answer": 0.46181154251098633, "max_score_of_first_answer": 0.46181154251098633, "min_score_of_first_answer": 0.46181154251098633, "avg_score": 0.46181154251098633, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.6892590522766113, "lex_sim": 1.0}
{"question": "In 2009 to what did the United Kingdom\u2019s largest insurance company, Norwich Union, change its name?", "answer": ["\u3157"], "scores": [[0.005011436063796282, 2.7883059374289587e-05, 0.32494959235191345, 0.053766198456287384]], "normalized_score": [0.0070292153395712376], "tokens": [["\u0120", "\u00e3", "\u0127", "\u0139"]], "entropy": [[6.291722297668457, 5.74244499206543, 2.7259514331817627, 3.333542585372925]], "avg_entropy": [4.5234153270721436], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Aviva Plc", "score_of_biggest_cluster": 0.0070292153395712376, "score_of_first_answer": 0.0070292153395712376, "max_score_of_first_answer": 0.32494959235191345, "min_score_of_first_answer": 2.7883059374289587e-05, "avg_score": 0.0070292153395712376, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1338760255193143, "std_of_first_answer": 0.1338760255193143, "avg_entropy_of_first_answer": 4.5234153270721436, "lex_sim": 0.0}
{"question": "Who is currently the Supreme Governor of the Church of England?", "answer": ["The LORD Jesus Christ"], "scores": [[0.5025656223297119, 0.0001703513989923522, 0.009467053227126598, 0.8775504231452942]], "normalized_score": [0.02904065139591694], "tokens": [["\u0120The", "\u0120LORD", "\u0120Jesus", "\u0120Christ"]], "entropy": [[2.72149658203125, 2.57551646232605, 3.630124568939209, 0.6270059943199158]], "avg_entropy": [2.388535901904106], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Queen Elizabeth II of Great Britain and Northern Ireland", "score_of_biggest_cluster": 0.02904065139591694, "score_of_first_answer": 0.02904065139591694, "max_score_of_first_answer": 0.8775504231452942, "min_score_of_first_answer": 0.0001703513989923522, "avg_score": 0.02904065139591694, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3673904397944865, "std_of_first_answer": 0.3673904397944865, "avg_entropy_of_first_answer": 2.388535901904106, "lex_sim": 1.0}
{"question": "Which was the first year that the Queen's Christmas speech was televised?", "answer": ["1986"], "scores": [[0.016088789328932762]], "normalized_score": [0.01608879119157791], "tokens": [["\u01201986"]], "entropy": [[4.8522748947143555]], "avg_entropy": [4.8522748947143555], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and fifty-seven", "score_of_biggest_cluster": 0.01608879119157791, "score_of_first_answer": 0.01608879119157791, "max_score_of_first_answer": 0.016088789328932762, "min_score_of_first_answer": 0.016088789328932762, "avg_score": 0.01608879119157791, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.8522748947143555, "lex_sim": 1.0}
{"question": "What traditional sport was banned in Britain in 2004?", "answer": ["Fencing"], "scores": [[0.008275138214230537, 0.8205347061157227]], "normalized_score": [0.08240167051553726], "tokens": [["\u0120F", "encing"]], "entropy": [[6.234906196594238, 1.3219530582427979]], "avg_entropy": [3.778429627418518], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fox-hunter", "score_of_biggest_cluster": 0.08240167051553726, "score_of_first_answer": 0.08240167051553726, "max_score_of_first_answer": 0.8205347061157227, "min_score_of_first_answer": 0.008275138214230537, "avg_score": 0.08240167051553726, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40612978395074606, "std_of_first_answer": 0.40612978395074606, "avg_entropy_of_first_answer": 3.778429627418518, "lex_sim": 1.0}
{"question": "What is a Teeter-Totter more commonly called?", "answer": ["The Rocker"], "scores": [[0.01671481505036354, 0.010011170990765095, 0.27452611923217773]], "normalized_score": [0.035814318805933], "tokens": [["\u0120The", "\u0120Rock", "er"]], "entropy": [[5.998586654663086, 6.792219638824463, 1.7797935009002686]], "avg_entropy": [4.8568665981292725], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "C-saw", "score_of_biggest_cluster": 0.035814318805933, "score_of_first_answer": 0.035814318805933, "max_score_of_first_answer": 0.27452611923217773, "min_score_of_first_answer": 0.010011170990765095, "avg_score": 0.035814318805933, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12314389291067537, "std_of_first_answer": 0.12314389291067537, "avg_entropy_of_first_answer": 4.8568665981292725, "lex_sim": 1.0}
{"question": "Toothpaste, turtle, oxblood, oilie/oily, ade, and swirly are types of what?", "answer": ["Waxes"], "scores": [[0.003499604295939207, 0.015303615480661392]], "normalized_score": [0.007318235468119383], "tokens": [["\u0120Wax", "es"]], "entropy": [[7.036433219909668, 1.60538649559021]], "avg_entropy": [4.320909857749939], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marbles (game)", "score_of_biggest_cluster": 0.007318235468119383, "score_of_first_answer": 0.007318235468119383, "max_score_of_first_answer": 0.015303615480661392, "min_score_of_first_answer": 0.003499604295939207, "avg_score": 0.007318235468119383, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.005902005592361093, "std_of_first_answer": 0.005902005592361093, "avg_entropy_of_first_answer": 4.320909857749939, "lex_sim": 1.0}
{"question": "Michaelmas Day is celebrated on the 29th of which month?", "answer": ["\u32c1"], "scores": [[0.0030672752764075994, 5.925913683313411e-06, 0.03499941527843475, 0.01785440742969513]], "normalized_score": [0.0018358132801949978], "tokens": [["\u0120", "\u00e3", "\u012d", "\u0123"]], "entropy": [[1.6253736019134521, 3.4632372856140137, 2.8255443572998047, 3.8833186626434326]], "avg_entropy": [2.949368476867676], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Setember", "score_of_biggest_cluster": 0.0018358132801949978, "score_of_first_answer": 0.0018358132801949978, "max_score_of_first_answer": 0.03499941527843475, "min_score_of_first_answer": 5.925913683313411e-06, "avg_score": 0.0018358132801949978, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.013885146260550868, "std_of_first_answer": 0.013885146260550868, "avg_entropy_of_first_answer": 2.949368476867676, "lex_sim": 0.0}
{"question": "What is 1009 when expressed as Roman Numerals?", "answer": ["CCXLIII"], "scores": [[0.008277197368443012, 0.06950518488883972, 0.0664927288889885]], "normalized_score": [0.03369443491101265], "tokens": [["\u0120CC", "XL", "III"]], "entropy": [[4.672911643981934, 2.4639530181884766, 1.864905595779419]], "avg_entropy": [3.0005900859832764], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "MIX", "score_of_biggest_cluster": 0.03369443491101265, "score_of_first_answer": 0.03369443491101265, "max_score_of_first_answer": 0.06950518488883972, "min_score_of_first_answer": 0.008277197368443012, "avg_score": 0.03369443491101265, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.028179956346406767, "std_of_first_answer": 0.028179956346406767, "avg_entropy_of_first_answer": 3.0005900859832764, "lex_sim": 1.0}
{"question": "\"From which Beatles song do the following lyrics come, \"\"Nothing's going to change my world\"\"?\"", "answer": ["\"Yesterday\", \"Here Comes The Sun\", \"Come Together'\""], "scores": [[0.292798787355423, 0.036638934165239334, 0.08844121545553207, 0.7241067886352539, 0.025331180542707443, 0.7607505321502686, 0.2017407864332199, 0.9945776462554932, 0.606925368309021, 0.8338008522987366, 0.01457580178976059, 0.9578946232795715, 0.00022678163077216595]], "normalized_score": [0.13344794511795044], "tokens": [["\u0120\"", "Yesterday", "\",", "\u0120\"", "Here", "\u0120Comes", "\u0120The", "\u0120Sun", "\",", "\u0120\"", "Come", "\u0120Together", "'\""]], "entropy": [[4.333587646484375, 4.802269458770752, 1.5906468629837036, 2.1298727989196777, 4.5888214111328125, 0.9493995308876038, 0.6361747980117798, 0.05284533277153969, 0.9323320388793945, 0.6699728965759277, 4.705881118774414, 0.28141623735427856, 0.8107333183288574]], "avg_entropy": [2.037227188451932], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Across The Universe", "score_of_biggest_cluster": 0.13344794511795044, "score_of_first_answer": 0.13344794511795044, "max_score_of_first_answer": 0.9945776462554932, "min_score_of_first_answer": 0.00022678163077216595, "avg_score": 0.13344794511795044, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3773209677659418, "std_of_first_answer": 0.3773209677659418, "avg_entropy_of_first_answer": 2.037227188451932, "lex_sim": 1.0}
{"question": "What is the second lightest chemical element?", "answer": ["Iodine"], "scores": [[0.016283638775348663, 0.9123866558074951, 0.9955098628997803]], "normalized_score": [0.24546635150909424], "tokens": [["\u0120I", "od", "ine"]], "entropy": [[5.497256278991699, 0.6979219913482666, 0.03959278762340546]], "avg_entropy": [2.0782570193211236], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Solid Helium", "score_of_biggest_cluster": 0.24546635150909424, "score_of_first_answer": 0.24546635150909424, "max_score_of_first_answer": 0.9955098628997803, "min_score_of_first_answer": 0.016283638775348663, "avg_score": 0.24546635150909424, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4433200602905828, "std_of_first_answer": 0.4433200602905828, "avg_entropy_of_first_answer": 2.0782570193211236, "lex_sim": 1.0}
{"question": "What type of creature is a Dog Face?", "answer": ["dog face"], "scores": [[0.011178791522979736, 0.2907737195491791]], "normalized_score": [0.05701315030455589], "tokens": [["\u0120dog", "\u0120face"]], "entropy": [[5.669909477233887, 2.803950309753418]], "avg_entropy": [4.236929893493652], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Butterfly", "score_of_biggest_cluster": 0.05701315030455589, "score_of_first_answer": 0.05701315030455589, "max_score_of_first_answer": 0.2907737195491791, "min_score_of_first_answer": 0.011178791522979736, "avg_score": 0.05701315030455589, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13979746401309967, "std_of_first_answer": 0.13979746401309967, "avg_entropy_of_first_answer": 4.236929893493652, "lex_sim": 1.0}
{"question": "Near which Chinese city is the famous Terracotta Army to be found?", "answer": ["\u3258"], "scores": [[0.0004562321410048753, 6.227203994058073e-05, 0.010697459802031517, 0.02264419011771679]], "normalized_score": [0.0016196800861507654], "tokens": [["\u0120", "\u00e3", "\u012b", "\u013a"]], "entropy": [[2.0755105018615723, 4.1070146560668945, 2.3945224285125732, 3.9657459259033203]], "avg_entropy": [3.13569837808609], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Xian, China", "score_of_biggest_cluster": 0.0016196800861507654, "score_of_first_answer": 0.0016196800861507654, "max_score_of_first_answer": 0.02264419011771679, "min_score_of_first_answer": 6.227203994058073e-05, "avg_score": 0.0016196800861507654, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.009230107108616685, "std_of_first_answer": 0.009230107108616685, "avg_entropy_of_first_answer": 3.13569837808609, "lex_sim": 0.0}
{"question": "Which late English singer was born Ronald William Wycherley in April 1940?", "answer": ["Adele"], "scores": [[0.001398278633132577, 0.5677323341369629]], "normalized_score": [0.02817530184984207], "tokens": [["\u0120Ad", "ele"]], "entropy": [[6.013339996337891, 2.259571075439453]], "avg_entropy": [4.136455535888672], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ronald Wycherley", "score_of_biggest_cluster": 0.02817530184984207, "score_of_first_answer": 0.02817530184984207, "max_score_of_first_answer": 0.5677323341369629, "min_score_of_first_answer": 0.001398278633132577, "avg_score": 0.02817530184984207, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28316702775191516, "std_of_first_answer": 0.28316702775191516, "avg_entropy_of_first_answer": 4.136455535888672, "lex_sim": 1.0}
{"question": "In Egyptian myth who was the God of chaos and disorder?", "answer": ["Typhon"], "scores": [[0.028673525899648666, 0.9656100869178772]], "normalized_score": [0.16639547049999237], "tokens": [["\u0120Typh", "on"]], "entropy": [[4.3730292320251465, 0.2353433072566986]], "avg_entropy": [2.3041862696409225], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Seth (Bible)", "score_of_biggest_cluster": 0.16639547049999237, "score_of_first_answer": 0.16639547049999237, "max_score_of_first_answer": 0.9656100869178772, "min_score_of_first_answer": 0.028673525899648666, "avg_score": 0.16639547049999237, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46846828050911427, "std_of_first_answer": 0.46846828050911427, "avg_entropy_of_first_answer": 2.3041862696409225, "lex_sim": 1.0}
{"question": "\"Which car company, in 1986, comissioned \"\"Cars\"\", a series of artworks to track the evolution of its designs, from Andy Warhol?\"", "answer": ["Porsche"], "scores": [[0.015063779428601265]], "normalized_score": [0.015063777565956116], "tokens": [["\u0120Porsche"]], "entropy": [[4.99868106842041]], "avg_entropy": [4.99868106842041], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mercedes benz", "score_of_biggest_cluster": 0.015063777565956116, "score_of_first_answer": 0.015063777565956116, "max_score_of_first_answer": 0.015063779428601265, "min_score_of_first_answer": 0.015063779428601265, "avg_score": 0.015063777565956116, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.99868106842041, "lex_sim": 1.0}
{"question": "Which fruit is also known as the alligator pear?", "answer": ["Mandarin Dragon Fruit"], "scores": [[0.004160825628787279, 0.00026686806813813746, 0.15653583407402039]], "normalized_score": [0.005580802448093891], "tokens": [["\u0120Mandarin", "\u0120Dragon", "\u0120Fruit"]], "entropy": [[6.203614711761475, 1.1715781688690186, 2.2969894409179688]], "avg_entropy": [3.2240607738494873], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Avocado pear", "score_of_biggest_cluster": 0.005580802448093891, "score_of_first_answer": 0.005580802448093891, "max_score_of_first_answer": 0.15653583407402039, "min_score_of_first_answer": 0.00026686806813813746, "avg_score": 0.005580802448093891, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07276544955518731, "std_of_first_answer": 0.07276544955518731, "avg_entropy_of_first_answer": 3.2240607738494873, "lex_sim": 1.0}
{"question": "The syndrome when hostages develop positive feelings for their captors is named after what city?", "answer": ["Stockholm Syndrome"], "scores": [[0.5543331503868103, 0.05110272765159607]], "normalized_score": [0.1683090478181839], "tokens": [["\u0120Stockholm", "\u0120Syndrome"]], "entropy": [[3.4811959266662598, 0.8876367807388306]], "avg_entropy": [2.184416353702545], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stockholm, Sweden", "score_of_biggest_cluster": 0.1683090478181839, "score_of_first_answer": 0.1683090478181839, "max_score_of_first_answer": 0.5543331503868103, "min_score_of_first_answer": 0.05110272765159607, "avg_score": 0.1683090478181839, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2516152113676071, "std_of_first_answer": 0.2516152113676071, "avg_entropy_of_first_answer": 2.184416353702545, "lex_sim": 1.0}
{"question": "Mitt Romney was Governor of which state 2003-7?", "answer": ["GA"], "scores": [[2.9607541364384815e-05]], "normalized_score": [2.9607535907416604e-05], "tokens": [["\u0120GA"]], "entropy": [[2.4673333168029785]], "avg_entropy": [2.4673333168029785], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Masachusetts", "score_of_biggest_cluster": 2.9607535907416604e-05, "score_of_first_answer": 2.9607535907416604e-05, "max_score_of_first_answer": 2.9607541364384815e-05, "min_score_of_first_answer": 2.9607541364384815e-05, "avg_score": 2.9607535907416604e-05, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.4673333168029785, "lex_sim": 1.0}
{"question": "During which year was Washington DC declared the capital of the USA?", "answer": ["1800"], "scores": [[0.049113932996988297]], "normalized_score": [0.049113932996988297], "tokens": [["\u01201800"]], "entropy": [[2.4199185371398926]], "avg_entropy": [2.4199185371398926], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1790", "score_of_biggest_cluster": 0.049113932996988297, "score_of_first_answer": 0.049113932996988297, "max_score_of_first_answer": 0.049113932996988297, "min_score_of_first_answer": 0.049113932996988297, "avg_score": 0.049113932996988297, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.4199185371398926, "lex_sim": 1.0}
{"question": "What courtesy title is held by the heir to the Marquis of Bath?", "answer": ["\u4ea1\u6c0e\u9808\u5225\u7684\u8fb2\u7522\u5152\u7ae5\u5973\u82f1\u53ca\u4fda\u65af\u5766"], "scores": [[0.002785887336358428, 2.3961254555615596e-05, 0.023846335709095, 0.0016471948474645615, 0.0012415861710906029, 0.010306384414434433, 0.11276770383119583, 0.037478744983673096, 0.011669888161122799, 0.28086593747138977, 0.017885910347104073, 0.06392126530408859, 0.009719379246234894, 0.406475692987442, 0.036796603351831436, 0.9427986741065979, 0.01779182441532612, 0.23200717568397522, 0.011474640108644962, 0.9951131939888, 0.0045128376223146915, 0.014358715154230595, 0.06857801973819733, 0.9807332754135132, 0.0074320463463664055, 0.22523951530456543, 0.005035225301980972, 0.00445895828306675, 0.033461954444646835, 0.6034850478172302, 0.044779323041439056, 0.12822477519512177, 0.9852403402328491]], "normalized_score": [0.032309189438819885], "tokens": [["\u0120", "\u00e4\u00ba", "\u00a1", "\u00e6\u00b0", "\u0130", "\u00e9", "\u0142", "\u012a", "\u00e5\u012a", "\u00a5", "\u00e7\u013c\u0126", "\u00e8", "\u00be", "\u00b2", "\u00e7\u0136", "\u00a2", "\u00e5\u0127", "\u0134", "\u00e7\u00ab", "\u00a5", "\u00e5\u00a5\u00b3", "\u00e8", "\u012d", "\u00b1", "\u00e5\u0131", "\u012c", "\u00e4\u00bf", "\u013c", "\u00e6\u0138", "\u00af", "\u00e5", "\u013f", "\u00a6"]], "entropy": [[3.836909532546997, 4.291530609130859, 3.031724214553833, 4.501537322998047, 2.1956331729888916, 4.256880283355713, 3.586684465408325, 2.6835620403289795, 4.420784950256348, 2.4616801738739014, 4.351100921630859, 4.426308631896973, 3.2261555194854736, 1.798852562904358, 3.469998359680176, 0.30012384057044983, 2.6317198276519775, 1.8727881908416748, 3.4293746948242188, 0.043204303830862045, 4.315113067626953, 3.123073101043701, 3.2082362174987793, 0.12402306497097015, 3.048827886581421, 2.3924834728240967, 4.034215450286865, 2.6270527839660645, 4.047333717346191, 0.9128047823905945, 4.299742698669434, 2.519144296646118, 0.09262008219957352]], "avg_entropy": [2.8957946739639295], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Viscount Weymouth", "score_of_biggest_cluster": 0.032309189438819885, "score_of_first_answer": 0.032309189438819885, "max_score_of_first_answer": 0.9951131939888, "min_score_of_first_answer": 2.3961254555615596e-05, "avg_score": 0.032309189438819885, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31904968609958856, "std_of_first_answer": 0.31904968609958856, "avg_entropy_of_first_answer": 2.8957946739639295, "lex_sim": 0.0}
{"question": "In the 'Jungle Book' what type of creature was 'Bagheera'?", "answer": ["African Lioness"], "scores": [[0.0059730070643126965, 0.2007119208574295, 0.0035562515258789062]], "normalized_score": [0.01621510088443756], "tokens": [["\u0120African", "\u0120Lion", "ess"]], "entropy": [[3.963224411010742, 4.211031436920166, 0.4658378064632416]], "avg_entropy": [2.8800312181313834], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Panther athletics", "score_of_biggest_cluster": 0.01621510088443756, "score_of_first_answer": 0.01621510088443756, "max_score_of_first_answer": 0.2007119208574295, "min_score_of_first_answer": 0.0035562515258789062, "avg_score": 0.01621510088443756, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09237570820791655, "std_of_first_answer": 0.09237570820791655, "avg_entropy_of_first_answer": 2.8800312181313834, "lex_sim": 1.0}
{"question": "On a standard English QWERTY keyboard the % (percent) sign is above the number?", "answer": ["Yes"], "scores": [[0.0444643497467041]], "normalized_score": [0.0444643460214138], "tokens": [["\u0120Yes"]], "entropy": [[6.116320610046387]], "avg_entropy": [6.116320610046387], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "five", "score_of_biggest_cluster": 0.0444643460214138, "score_of_first_answer": 0.0444643460214138, "max_score_of_first_answer": 0.0444643497467041, "min_score_of_first_answer": 0.0444643497467041, "avg_score": 0.0444643460214138, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.116320610046387, "lex_sim": 1.0}
{"question": "What was the first name of the father of Isambard Kingdom Brunel, a French engineer who moved to England and constructed the Thames Tunnel?", "answer": ["Isamblard"], "scores": [[0.08382918685674667, 0.15823736786842346, 0.035476382821798325, 0.6798389554023743]], "normalized_score": [0.13374033570289612], "tokens": [["\u0120Is", "am", "bl", "ard"]], "entropy": [[5.4073686599731445, 1.5271270275115967, 3.1043176651000977, 1.9028480052947998]], "avg_entropy": [2.9854153394699097], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marc (disambiguation)", "score_of_biggest_cluster": 0.13374033570289612, "score_of_first_answer": 0.13374033570289612, "max_score_of_first_answer": 0.6798389554023743, "min_score_of_first_answer": 0.035476382821798325, "avg_score": 0.13374033570289612, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2580508447684698, "std_of_first_answer": 0.2580508447684698, "avg_entropy_of_first_answer": 2.9854153394699097, "lex_sim": 1.0}
{"question": "\"Pop music - the group called \"\"The Revolution\"\" was the backing roup for which pop singer?\"", "answer": ["\"The Beatles\""], "scores": [[0.0033454368822276592, 0.37313351035118103, 0.45557841658592224, 0.7717097997665405]], "normalized_score": [0.1447383463382721], "tokens": [["\u0120\"", "The", "\u0120Beatles", "\""]], "entropy": [[3.2139463424682617, 4.4820427894592285, 3.8867788314819336, 1.102544903755188]], "avg_entropy": [3.171328216791153], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "PRINCE2:Close a project", "score_of_biggest_cluster": 0.1447383463382721, "score_of_first_answer": 0.1447383463382721, "max_score_of_first_answer": 0.7717097997665405, "min_score_of_first_answer": 0.0033454368822276592, "avg_score": 0.1447383463382721, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2735462659498893, "std_of_first_answer": 0.2735462659498893, "avg_entropy_of_first_answer": 3.171328216791153, "lex_sim": 1.0}
{"question": "Roman Emperor Vespasian came to power in 69AD - the Year of the Four Emperors. Name any of the other three Emperors that year.", "answer": ["Galba \u32d1 Septimius Severus - the Year of Four Emperors"], "scores": [[0.1587013453245163, 0.9181404709815979, 0.010099700652062893, 8.495676411257591e-06, 0.022039854899048805, 0.028493709862232208, 0.009757193736732006, 0.5793682932853699, 0.9103474020957947, 0.5512463450431824, 0.003913656808435917, 0.015175106935203075, 0.07723674923181534, 0.9758215546607971, 0.14147649705410004, 0.984915018081665, 0.9979904890060425]], "normalized_score": [0.06790110468864441], "tokens": [["\u0120Gal", "ba", "\u0120", "\u00e3", "\u012d", "\u0133", "\u0120Sept", "im", "ius", "\u0120Severus", "\u0120-", "\u0120the", "\u0120Year", "\u0120of", "\u0120Four", "\u0120Em", "perors"]], "entropy": [[3.230865955352783, 0.5010998249053955, 1.6671642065048218, 2.319096565246582, 2.7075233459472656, 3.7717137336730957, 4.731837749481201, 1.1932339668273926, 0.6247487664222717, 2.589188575744629, 2.330038547515869, 6.278918743133545, 5.3540449142456055, 0.2326744794845581, 1.0559619665145874, 0.14976532757282257, 0.02242174744606018]], "avg_entropy": [2.2800175538834404], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Galba, Otho, Vitellius", "score_of_biggest_cluster": 0.06790110468864441, "score_of_first_answer": 0.06790110468864441, "max_score_of_first_answer": 0.9979904890060425, "min_score_of_first_answer": 8.495676411257591e-06, "avg_score": 0.06790110468864441, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4119182519428777, "std_of_first_answer": 0.4119182519428777, "avg_entropy_of_first_answer": 2.2800175538834404, "lex_sim": 1.0}
{"question": "Who invaded Europe from Mongolia and Turkey over 300 years, beginning in the 13th century?", "answer": ["\ud68c\ub2c8\ud0c0\uc784(Heitanimitasi)"], "scores": [[0.0010675245430320501, 2.3847907868912444e-05, 0.035003047436475754, 0.5132951140403748, 0.030807137489318848, 0.1637791246175766, 0.06821730732917786, 0.10660280287265778, 0.8036984801292419, 0.19939900934696198, 0.21161247789859772, 0.5113906264305115, 0.012523854151368141, 0.011798272840678692, 0.003651620354503393, 0.00047401728807017207, 0.09689635038375854, 0.0005479821120388806, 0.777107298374176]], "normalized_score": [0.027970748022198677], "tokens": [["\u0120", "\u00ed", "\u013c", "\u012e", "\u00eb\u012d", "\u012a", "\u00ed", "\u0125", "\u0122", "\u00ec", "\u0140", "\u0126", "(", "He", "it", "anim", "it", "asi", ")"]], "entropy": [[3.399857521057129, 4.343497276306152, 3.137660264968872, 1.786428451538086, 2.1470937728881836, 2.3334133625030518, 2.6325454711914062, 2.9956953525543213, 0.9385619759559631, 2.508561611175537, 2.5388355255126953, 1.545353651046753, 2.6096596717834473, 4.578624248504639, 4.054426193237305, 3.1809592247009277, 3.8343076705932617, 2.734219551086426, 1.4609198570251465]], "avg_entropy": [2.7768747712436475], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tatar (people)", "score_of_biggest_cluster": 0.027970748022198677, "score_of_first_answer": 0.027970748022198677, "max_score_of_first_answer": 0.8036984801292419, "min_score_of_first_answer": 2.3847907868912444e-05, "avg_score": 0.027970748022198677, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25665574099930344, "std_of_first_answer": 0.25665574099930344, "avg_entropy_of_first_answer": 2.7768747712436475, "lex_sim": 1.0}
{"question": "What number is Hurricane on the Beaufort Scale?", "answer": ["IX"], "scores": [[0.0005181403248570859]], "normalized_score": [0.0005181404412724078], "tokens": [["\u0120IX"]], "entropy": [[4.953108787536621]], "avg_entropy": [4.953108787536621], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "twelve", "score_of_biggest_cluster": 0.0005181404412724078, "score_of_first_answer": 0.0005181404412724078, "max_score_of_first_answer": 0.0005181403248570859, "min_score_of_first_answer": 0.0005181403248570859, "avg_score": 0.0005181404412724078, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.953108787536621, "lex_sim": 1.0}
{"question": "\"Who played the part of the tragic Jennifer Cavalleri in the 1970 film, \"\"Love Story\"\"?\"", "answer": ["George Peppard"], "scores": [[0.0007415759610012174, 0.7082051634788513, 0.9956620335578918]], "normalized_score": [0.08056424558162689], "tokens": [["\u0120George", "\u0120Pe", "ppard"]], "entropy": [[3.774223804473877, 1.1626882553100586, 0.04796145111322403]], "avg_entropy": [1.6616245036323865], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ali McGraw", "score_of_biggest_cluster": 0.08056424558162689, "score_of_first_answer": 0.08056424558162689, "max_score_of_first_answer": 0.9956620335578918, "min_score_of_first_answer": 0.0007415759610012174, "avg_score": 0.08056424558162689, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4180647074801368, "std_of_first_answer": 0.4180647074801368, "avg_entropy_of_first_answer": 1.6616245036323865, "lex_sim": 1.0}
{"question": "In which American state is the Isabella Stewart Gardner Museum?", "answer": ["Massachusetts"], "scores": [[0.8525063395500183]], "normalized_score": [0.8525063395500183], "tokens": [["\u0120Massachusetts"]], "entropy": [[0.9686089754104614]], "avg_entropy": [0.9686089754104614], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Masachusetts", "score_of_biggest_cluster": 0.8525063395500183, "score_of_first_answer": 0.8525063395500183, "max_score_of_first_answer": 0.8525063395500183, "min_score_of_first_answer": 0.8525063395500183, "avg_score": 0.8525063395500183, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.9686089754104614, "lex_sim": 1.0}
{"question": "\"Whose last recorded words in the House of Commons were: \"\"I wish everyone, friend or foe, well. And that is that. The end \"\" ?\"", "answer": ["John Bercow"], "scores": [[0.04820061847567558, 0.014132675714790821, 0.3609767258167267, 0.9855630397796631]], "normalized_score": [0.12476998567581177], "tokens": [["\u0120John", "\u0120B", "erc", "ow"]], "entropy": [[5.425736904144287, 4.628652572631836, 3.189791202545166, 0.1359196901321411]], "avg_entropy": [3.3450250923633575], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rt Hon Anthony Blair MP", "score_of_biggest_cluster": 0.12476998567581177, "score_of_first_answer": 0.12476998567581177, "max_score_of_first_answer": 0.9855630397796631, "min_score_of_first_answer": 0.014132675714790821, "avg_score": 0.12476998567581177, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3898496214750941, "std_of_first_answer": 0.3898496214750941, "avg_entropy_of_first_answer": 3.3450250923633575, "lex_sim": 1.0}
{"question": "Which of the Noble Gases takes its name from the Greek for 'hidden' ?", "answer": ["Hypo"], "scores": [[0.002043035114184022, 0.1408952921628952]], "normalized_score": [0.016966260969638824], "tokens": [["\u0120Hyp", "o"]], "entropy": [[4.1923699378967285, 2.971501588821411]], "avg_entropy": [3.58193576335907], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "KRYPTON", "score_of_biggest_cluster": 0.016966260969638824, "score_of_first_answer": 0.016966260969638824, "max_score_of_first_answer": 0.1408952921628952, "min_score_of_first_answer": 0.002043035114184022, "avg_score": 0.016966260969638824, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06942612852435559, "std_of_first_answer": 0.06942612852435559, "avg_entropy_of_first_answer": 3.58193576335907, "lex_sim": 1.0}
{"question": "The Alfred Hitchcock films Rebecca and The Birds were based on novels by which author?", "answer": ["Rebecca"], "scores": [[0.011167865246534348]], "normalized_score": [0.011167866177856922], "tokens": [["\u0120Rebecca"]], "entropy": [[0.7494828104972839]], "avg_entropy": [0.7494828104972839], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lady Browning", "score_of_biggest_cluster": 0.011167866177856922, "score_of_first_answer": 0.011167866177856922, "max_score_of_first_answer": 0.011167865246534348, "min_score_of_first_answer": 0.011167865246534348, "avg_score": 0.011167866177856922, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.7494828104972839, "lex_sim": 1.0}
{"question": "In the Crimean War, what did Florence Nightingale begin to be called?", "answer": ["The Lady"], "scores": [[0.1902463138103485, 0.6474959850311279]], "normalized_score": [0.3509753942489624], "tokens": [["\u0120The", "\u0120Lady"]], "entropy": [[3.5526535511016846, 2.363884925842285]], "avg_entropy": [2.958269238471985], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Lady with the Lamp", "score_of_biggest_cluster": 0.3509753942489624, "score_of_first_answer": 0.3509753942489624, "max_score_of_first_answer": 0.6474959850311279, "min_score_of_first_answer": 0.1902463138103485, "avg_score": 0.3509753942489624, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2286248356103897, "std_of_first_answer": 0.2286248356103897, "avg_entropy_of_first_answer": 2.958269238471985, "lex_sim": 1.0}
{"question": "\"Give the name of the character played by James Stewart in the 1946 film \"\"It's a Wonderful Life\"\"\"", "answer": ["George Bailey"], "scores": [[0.2887742221355438, 0.972947895526886]], "normalized_score": [0.530058741569519], "tokens": [["\u0120George", "\u0120Bailey"]], "entropy": [[3.5389022827148438, 0.22436779737472534]], "avg_entropy": [1.8816350400447845], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George Bailey (disambiguation)", "score_of_biggest_cluster": 0.530058741569519, "score_of_first_answer": 0.530058741569519, "max_score_of_first_answer": 0.972947895526886, "min_score_of_first_answer": 0.2887742221355438, "avg_score": 0.530058741569519, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3420868366956711, "std_of_first_answer": 0.3420868366956711, "avg_entropy_of_first_answer": 1.8816350400447845, "lex_sim": 1.0}
{"question": "What was the Mississippi State Penitentiary formerly called?", "answer": ["The Mississippi State Penitentiary was known as Alcorn State Corrections Center until 2012."], "scores": [[0.29574400186538696, 0.07943738996982574, 0.7553889751434326, 0.3588935434818268, 0.8915397524833679, 0.9985184073448181, 0.08815772086381912, 0.058923058211803436, 0.8162397146224976, 0.005544885527342558, 0.3440462648868561, 0.5842722654342651, 0.0014735186705365777, 0.1193723976612091, 0.264249324798584, 0.014211788773536682, 0.5788957476615906]], "normalized_score": [0.15034647285938263], "tokens": [["\u0120The", "\u0120Mississippi", "\u0120State", "\u0120Pen", "itent", "iary", "\u0120was", "\u0120known", "\u0120as", "\u0120Al", "corn", "\u0120State", "\u0120Corrections", "\u0120Center", "\u0120until", "\u01202012", "."]], "entropy": [[4.595694541931152, 5.3879194259643555, 1.6798285245895386, 2.699709892272949, 0.6412670612335205, 0.01762698031961918, 2.429461717605591, 3.6829371452331543, 1.108058214187622, 2.8360209465026855, 1.791522741317749, 2.5711965560913086, 2.0539298057556152, 3.0831408500671387, 2.8295018672943115, 4.506814002990723, 1.5332555770874023]], "avg_entropy": [2.555757991202614], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "MSP Fire Department", "score_of_biggest_cluster": 0.15034647285938263, "score_of_first_answer": 0.15034647285938263, "max_score_of_first_answer": 0.9985184073448181, "min_score_of_first_answer": 0.0014735186705365777, "avg_score": 0.15034647285938263, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32902124603770244, "std_of_first_answer": 0.32902124603770244, "avg_entropy_of_first_answer": 2.555757991202614, "lex_sim": 1.0}
{"question": "In which US city is the 1974 film Towering Inferno set?", "answer": ["\u338f"], "scores": [[0.001149338437244296, 2.3190654246718623e-05, 0.020574579015374184, 0.01843755505979061]], "normalized_score": [0.0017831941368058324], "tokens": [["\u0120", "\u00e3", "\u0130", "\u0131"]], "entropy": [[3.990817070007324, 4.5548295974731445, 2.8337435722351074, 3.6829090118408203]], "avg_entropy": [3.765574812889099], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "San Fransciso", "score_of_biggest_cluster": 0.0017831941368058324, "score_of_first_answer": 0.0017831941368058324, "max_score_of_first_answer": 0.020574579015374184, "min_score_of_first_answer": 2.3190654246718623e-05, "avg_score": 0.0017831941368058324, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.009498374419315938, "std_of_first_answer": 0.009498374419315938, "avg_entropy_of_first_answer": 3.765574812889099, "lex_sim": 0.0}
{"question": "Two of Peter O'Toole's nominations for Best Actor 'Oscar' were for playing which English king in two different films?", "answer": ["Edmund II of England"], "scores": [[0.002677440643310547, 0.1930006593465805, 0.1019546389579773, 0.8159099221229553]], "normalized_score": [0.08097145706415176], "tokens": [["\u0120Edmund", "\u0120II", "\u0120of", "\u0120England"]], "entropy": [[2.729396343231201, 3.6576244831085205, 1.8938355445861816, 1.0561825037002563]], "avg_entropy": [2.33425971865654], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Henry II", "score_of_biggest_cluster": 0.08097145706415176, "score_of_first_answer": 0.08097145706415176, "max_score_of_first_answer": 0.8159099221229553, "min_score_of_first_answer": 0.002677440643310547, "avg_score": 0.08097145706415176, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3175554511082194, "std_of_first_answer": 0.3175554511082194, "avg_entropy_of_first_answer": 2.33425971865654, "lex_sim": 1.0}
{"question": "The London restaurant Le Gavroche is run by which chef?", "answer": ["Monsieur Bijou"], "scores": [[0.009914392605423927, 0.9699658751487732, 0.9970753192901611, 0.029540305957198143, 0.005893177818506956, 0.43192213773727417]], "normalized_score": [0.09469348937273026], "tokens": [["\u0120Mons", "ie", "ur", "\u0120B", "ij", "ou"]], "entropy": [[4.984482765197754, 0.2965506315231323, 0.02520984783768654, 4.685917377471924, 2.773843288421631, 2.3147528171539307]], "avg_entropy": [2.51345945460101], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Michel (Albert) Roux", "score_of_biggest_cluster": 0.09469348937273026, "score_of_first_answer": 0.09469348937273026, "max_score_of_first_answer": 0.9970753192901611, "min_score_of_first_answer": 0.005893177818506956, "avg_score": 0.09469348937273026, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4333549670330536, "std_of_first_answer": 0.4333549670330536, "avg_entropy_of_first_answer": 2.51345945460101, "lex_sim": 1.0}
{"question": "Satya Nadella, boss of which vast corporation, apologised in 2014 for suggesting female workers should rely on faith and karma instead of asking for a pay rise?", "answer": ["Microsoft"], "scores": [[0.784758448600769]], "normalized_score": [0.784758448600769], "tokens": [["\u0120Microsoft"]], "entropy": [[1.7976887226104736]], "avg_entropy": [1.7976887226104736], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "'Microsoft Corp", "score_of_biggest_cluster": 0.784758448600769, "score_of_first_answer": 0.784758448600769, "max_score_of_first_answer": 0.784758448600769, "min_score_of_first_answer": 0.784758448600769, "avg_score": 0.784758448600769, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.7976887226104736, "lex_sim": 1.0}
{"question": "Which city is the destination of the yacht race which leaves Sydney every Boxing Day?", "answer": ["The Cup to Melbourne"], "scores": [[0.019669631496071815, 0.006827654782682657, 0.0003017743583768606, 0.029985113069415092]], "normalized_score": [0.0059042382054030895], "tokens": [["\u0120The", "\u0120Cup", "\u0120to", "\u0120Melbourne"]], "entropy": [[4.109369277954102, 5.880572319030762, 1.131059169769287, 5.0859880447387695]], "avg_entropy": [4.05174720287323], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "UN/LOCODE:AUHBA", "score_of_biggest_cluster": 0.0059042382054030895, "score_of_first_answer": 0.0059042382054030895, "max_score_of_first_answer": 0.029985113069415092, "min_score_of_first_answer": 0.0003017743583768606, "avg_score": 0.0059042382054030895, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.011473869777541508, "std_of_first_answer": 0.011473869777541508, "avg_entropy_of_first_answer": 4.05174720287323, "lex_sim": 1.0}
{"question": "Which Scotsman became the first European to reach the River Niger in 1796?", "answer": ["George Abercrombie"], "scores": [[0.023729583248496056, 0.020776862278580666, 0.774582028388977, 0.9993958473205566, 0.679465115070343]], "normalized_score": [0.19176459312438965], "tokens": [["\u0120George", "\u0120Aber", "c", "rom", "bie"]], "entropy": [[3.875824213027954, 4.760698318481445, 0.8213635683059692, 0.007147969212383032, 0.6734868884086609]], "avg_entropy": [2.0277041914872824], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mungo Park (disambiguation)", "score_of_biggest_cluster": 0.19176459312438965, "score_of_first_answer": 0.19176459312438965, "max_score_of_first_answer": 0.9993958473205566, "min_score_of_first_answer": 0.020776862278580666, "avg_score": 0.19176459312438965, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40335758696232676, "std_of_first_answer": 0.40335758696232676, "avg_entropy_of_first_answer": 2.0277041914872824, "lex_sim": 1.0}
{"question": "What is the capital of Colombia?", "answer": ["Bogot\u00e1"], "scores": [[0.8082115650177002, 0.42367759346961975, 0.7669987678527832]], "normalized_score": [0.6404005885124207], "tokens": [["\u0120Bog", "ot", "\u00c3\u00a1"]], "entropy": [[1.2989871501922607, 0.8496378064155579, 1.2023932933807373]], "avg_entropy": [1.1170060833295186], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "City of Bogot\u00e1", "score_of_biggest_cluster": 0.6404005885124207, "score_of_first_answer": 0.6404005885124207, "max_score_of_first_answer": 0.8082115650177002, "min_score_of_first_answer": 0.42367759346961975, "avg_score": 0.6404005885124207, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.17238016729972513, "std_of_first_answer": 0.17238016729972513, "avg_entropy_of_first_answer": 1.1170060833295186, "lex_sim": 1.0}
{"question": "Which country was invaded by Iraq in 1990?", "answer": ["Iraq"], "scores": [[0.20559988915920258]], "normalized_score": [0.20559987425804138], "tokens": [["\u0120Iraq"]], "entropy": [[3.844554901123047]], "avg_entropy": [3.844554901123047], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ISO 3166-1:KW", "score_of_biggest_cluster": 0.20559987425804138, "score_of_first_answer": 0.20559987425804138, "max_score_of_first_answer": 0.20559988915920258, "min_score_of_first_answer": 0.20559988915920258, "avg_score": 0.20559987425804138, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.844554901123047, "lex_sim": 1.0}
{"question": "The 'Gowk' is an old dialect word for which bird?", "answer": ["A wren or jackdaw"], "scores": [[0.03852638974785805, 0.006640356499701738, 0.7830828428268433, 0.009062739089131355, 0.025784878060221672, 0.9487441778182983, 0.995037853717804]], "normalized_score": [0.08898948132991791], "tokens": [["\u0120A", "\u0120w", "ren", "\u0120or", "\u0120jack", "d", "aw"]], "entropy": [[5.50029182434082, 5.9179887771606445, 1.0483132600784302, 0.6638501286506653, 3.715209484100342, 0.3630415201187134, 0.045749131590127945]], "avg_entropy": [2.464920589434249], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The cuckoo", "score_of_biggest_cluster": 0.08898948132991791, "score_of_first_answer": 0.08898948132991791, "max_score_of_first_answer": 0.995037853717804, "min_score_of_first_answer": 0.006640356499701738, "avg_score": 0.08898948132991791, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4440398636313575, "std_of_first_answer": 0.4440398636313575, "avg_entropy_of_first_answer": 2.464920589434249, "lex_sim": 1.0}
{"question": "\"Which country set up the \"\"Committee on Public Information\"\" from April 1917 to June 1919 to run a propaganda campaign using newsprint, posters, radio, telegraph, cable and movies to broadcast its message, including fabrications (e.g. images of enemy soldiers killing babies and hoisting them on bayonets), and told citizens to seek out spies, which led to the creation of \"\"patriotic organizations,\"\" which spied, tapped telephones, and opened mail in an effort to ferret out \"\"spies and traitors\"\" (i.e. anyone who called for peace, questioned the war's progress, or criticized government policies)?\"", "answer": ["The United States of America"], "scores": [[0.02213458903133869, 0.47847747802734375, 0.9459302425384521, 0.180392786860466, 0.9695398211479187]], "normalized_score": [0.281005859375], "tokens": [["\u0120The", "\u0120United", "\u0120States", "\u0120of", "\u0120America"]], "entropy": [[3.87314510345459, 2.8542940616607666, 0.25842219591140747, 0.7894185781478882, 0.2766357362270355]], "avg_entropy": [1.6103831350803375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The United States of America", "score_of_biggest_cluster": 0.281005859375, "score_of_first_answer": 0.281005859375, "max_score_of_first_answer": 0.9695398211479187, "min_score_of_first_answer": 0.02213458903133869, "avg_score": 0.281005859375, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.38689204787686043, "std_of_first_answer": 0.38689204787686043, "avg_entropy_of_first_answer": 1.6103831350803375, "lex_sim": 1.0}
{"question": "Which politician was born Goldie Mabovich?", "answer": ["Goldie M. Biden"], "scores": [[0.10090770572423935, 0.8958643674850464, 0.6536895036697388, 0.0025842103641480207, 0.0008284167852252722]], "normalized_score": [0.04172755777835846], "tokens": [["\u0120Gold", "ie", "\u0120M", ".", "\u0120Biden"]], "entropy": [[6.23162841796875, 0.6217412352561951, 2.741100311279297, 0.44769880175590515, 5.031147003173828]], "avg_entropy": [3.014663153886795], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Golda Meir", "score_of_biggest_cluster": 0.04172755777835846, "score_of_first_answer": 0.04172755777835846, "max_score_of_first_answer": 0.8958643674850464, "min_score_of_first_answer": 0.0008284167852252722, "avg_score": 0.04172755777835846, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3722936166735509, "std_of_first_answer": 0.3722936166735509, "avg_entropy_of_first_answer": 3.014663153886795, "lex_sim": 1.0}
{"question": "Used to describe US citizens who attempted to foment insurrections in Latin America in the mid-19th century, what is the term for someone who engages in an unauthorized military expedition into a foreign country to support a revolution?", "answer": ["A maverick"], "scores": [[0.005087018944323063, 0.014272631146013737, 0.4443533420562744, 0.9973153471946716]], "normalized_score": [0.07531510293483734], "tokens": [["\u0120A", "\u0120m", "aver", "ick"]], "entropy": [[6.604964256286621, 6.843411445617676, 3.5043978691101074, 0.026056984439492226]], "avg_entropy": [4.244707638863474], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Filibusting", "score_of_biggest_cluster": 0.07531510293483734, "score_of_first_answer": 0.07531510293483734, "max_score_of_first_answer": 0.9973153471946716, "min_score_of_first_answer": 0.005087018944323063, "avg_score": 0.07531510293483734, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40579133431438386, "std_of_first_answer": 0.40579133431438386, "avg_entropy_of_first_answer": 4.244707638863474, "lex_sim": 1.0}
{"question": "Which mythical hero wielded a magical sword called 'Gram'?", "answer": ["Grammaton"], "scores": [[0.6856080293655396, 0.0019344688626006246, 0.6256951093673706]], "normalized_score": [0.09397237002849579], "tokens": [["\u0120Gram", "mat", "on"]], "entropy": [[2.4302549362182617, 4.167699813842773, 2.057633876800537]], "avg_entropy": [2.8851962089538574], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Siegfried (name)", "score_of_biggest_cluster": 0.09397237002849579, "score_of_first_answer": 0.09397237002849579, "max_score_of_first_answer": 0.6856080293655396, "min_score_of_first_answer": 0.0019344688626006246, "avg_score": 0.09397237002849579, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3091343526610313, "std_of_first_answer": 0.3091343526610313, "avg_entropy_of_first_answer": 2.8851962089538574, "lex_sim": 1.0}
{"question": "What item of clothing do you associate with a 1979 hit song from Neil Diamond?", "answer": ["Glitterati"], "scores": [[0.0052503482438623905, 0.5366717576980591, 0.0048951455391943455]], "normalized_score": [0.023982111364603043], "tokens": [["\u0120Gl", "itter", "ati"]], "entropy": [[5.758788108825684, 2.212092161178589, 3.3591456413269043]], "avg_entropy": [3.776675303777059], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Riveted jeans", "score_of_biggest_cluster": 0.023982111364603043, "score_of_first_answer": 0.023982111364603043, "max_score_of_first_answer": 0.5366717576980591, "min_score_of_first_answer": 0.0048951455391943455, "avg_score": 0.023982111364603043, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25059821889816775, "std_of_first_answer": 0.25059821889816775, "avg_entropy_of_first_answer": 3.776675303777059, "lex_sim": 1.0}
{"question": "In fashion, Hobble, Pencil and Dirndl are all types of what?", "answer": ["Underwear"], "scores": [[0.026626845821738243, 0.4264397919178009]], "normalized_score": [0.106558658182621], "tokens": [["\u0120Under", "wear"]], "entropy": [[5.455394268035889, 1.8718321323394775]], "avg_entropy": [3.663613200187683], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Broomstick skirt", "score_of_biggest_cluster": 0.106558658182621, "score_of_first_answer": 0.106558658182621, "max_score_of_first_answer": 0.4264397919178009, "min_score_of_first_answer": 0.026626845821738243, "avg_score": 0.106558658182621, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19990647304803133, "std_of_first_answer": 0.19990647304803133, "avg_entropy_of_first_answer": 3.663613200187683, "lex_sim": 1.0}
{"question": "How many times did Actress Zsa Zsa Gabor marry?", "answer": ["Eighteen times"], "scores": [[0.02049732208251953, 0.10392612218856812, 0.23315706849098206]], "normalized_score": [0.079193614423275], "tokens": [["\u0120Eight", "een", "\u0120times"]], "entropy": [[4.348217487335205, 1.580688714981079, 1.0856618881225586]], "avg_entropy": [2.3381893634796143], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "9", "score_of_biggest_cluster": 0.079193614423275, "score_of_first_answer": 0.079193614423275, "max_score_of_first_answer": 0.23315706849098206, "min_score_of_first_answer": 0.02049732208251953, "avg_score": 0.079193614423275, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0874866147932636, "std_of_first_answer": 0.0874866147932636, "avg_entropy_of_first_answer": 2.3381893634796143, "lex_sim": 1.0}
{"question": "Which peninsula has the Mediterranean Sea to the south, the Aegean Sea to the west and the Black Sea to the north, and is separated from Europe by the Dardanelles and the Bosphorus?", "answer": ["Anatolia"], "scores": [[0.26306360960006714, 0.953490674495697]], "normalized_score": [0.500827968120575], "tokens": [["\u0120Anat", "olia"]], "entropy": [[3.7449076175689697, 0.24316155910491943]], "avg_entropy": [1.9940345883369446], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Asian Minor", "score_of_biggest_cluster": 0.500827968120575, "score_of_first_answer": 0.500827968120575, "max_score_of_first_answer": 0.953490674495697, "min_score_of_first_answer": 0.26306360960006714, "avg_score": 0.500827968120575, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.34521353244781494, "std_of_first_answer": 0.34521353244781494, "avg_entropy_of_first_answer": 1.9940345883369446, "lex_sim": 1.0}
{"question": "In the culinary world, what word is used for the edible offal of a fowl, typically including the heart, gizzard and liver?", "answer": ["Poultry"], "scores": [[0.014398515224456787, 0.24339509010314941]], "normalized_score": [0.059199053794145584], "tokens": [["\u0120P", "oultry"]], "entropy": [[6.178822994232178, 3.7510974407196045]], "avg_entropy": [4.964960217475891], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Giblets", "score_of_biggest_cluster": 0.059199053794145584, "score_of_first_answer": 0.059199053794145584, "max_score_of_first_answer": 0.24339509010314941, "min_score_of_first_answer": 0.014398515224456787, "avg_score": 0.059199053794145584, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11449828743934631, "std_of_first_answer": 0.11449828743934631, "avg_entropy_of_first_answer": 4.964960217475891, "lex_sim": 1.0}
{"question": "What type of whale was Moby Dick?", "answer": ["Right whale"], "scores": [[0.06451523303985596, 0.5597890615463257]], "normalized_score": [0.19003926217556], "tokens": [["\u0120Right", "\u0120whale"]], "entropy": [[4.801243782043457, 1.994087815284729]], "avg_entropy": [3.397665798664093], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sperm whales", "score_of_biggest_cluster": 0.19003926217556, "score_of_first_answer": 0.19003926217556, "max_score_of_first_answer": 0.5597890615463257, "min_score_of_first_answer": 0.06451523303985596, "avg_score": 0.19003926217556, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24763691425323486, "std_of_first_answer": 0.24763691425323486, "avg_entropy_of_first_answer": 3.397665798664093, "lex_sim": 1.0}
{"question": "Which band released a 2001 album entitled \u2018Amnesiac\u2019?", "answer": ["Slipknot"], "scores": [[0.006233606953173876, 0.9945723414421082, 0.9987120628356934]], "normalized_score": [0.1836278885602951], "tokens": [["\u0120Slip", "k", "not"]], "entropy": [[5.188972473144531, 0.054106395691633224, 0.01208680123090744]], "avg_entropy": [1.7517218900223572], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dead Air Space", "score_of_biggest_cluster": 0.1836278885602951, "score_of_first_answer": 0.1836278885602951, "max_score_of_first_answer": 0.9987120628356934, "min_score_of_first_answer": 0.006233606953173876, "avg_score": 0.1836278885602951, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46688614800319533, "std_of_first_answer": 0.46688614800319533, "avg_entropy_of_first_answer": 1.7517218900223572, "lex_sim": 1.0}
{"question": "The flag of which country prominently features 'The Eagle of Saladin' holding a scroll bearing the name of the country in Arabic?", "answer": ["UAE"], "scores": [[0.0025683899875730276]], "normalized_score": [0.002568389754742384], "tokens": [["\u0120UAE"]], "entropy": [[4.391465663909912]], "avg_entropy": [4.391465663909912], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "EGY", "score_of_biggest_cluster": 0.002568389754742384, "score_of_first_answer": 0.002568389754742384, "max_score_of_first_answer": 0.0025683899875730276, "min_score_of_first_answer": 0.0025683899875730276, "avg_score": 0.002568389754742384, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.391465663909912, "lex_sim": 1.0}
{"question": "On which record label did Elvis Presley launch his career?", "answer": ["RCA Music"], "scores": [[0.47436144948005676, 0.9831209182739258, 0.0011301313061267138]], "normalized_score": [0.08077588677406311], "tokens": [["\u0120R", "CA", "\u0120Music"]], "entropy": [[2.450357437133789, 0.18099498748779297, 1.2796621322631836]], "avg_entropy": [1.3036715189615886], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The SUN", "score_of_biggest_cluster": 0.08077588677406311, "score_of_first_answer": 0.08077588677406311, "max_score_of_first_answer": 0.9831209182739258, "min_score_of_first_answer": 0.0011301313061267138, "avg_score": 0.08077588677406311, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40098351078285455, "std_of_first_answer": 0.40098351078285455, "avg_entropy_of_first_answer": 1.3036715189615886, "lex_sim": 1.0}
{"question": "Whose alter ego is Sasha Fierce?", "answer": ["Kanye West"], "scores": [[0.006960010156035423, 0.8968504071235657]], "normalized_score": [0.07900689542293549], "tokens": [["\u0120Kanye", "\u0120West"]], "entropy": [[4.008486747741699, 0.6921050548553467]], "avg_entropy": [2.350295901298523], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Beyonc", "score_of_biggest_cluster": 0.07900689542293549, "score_of_first_answer": 0.07900689542293549, "max_score_of_first_answer": 0.8968504071235657, "min_score_of_first_answer": 0.006960010156035423, "avg_score": 0.07900689542293549, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4449451984837651, "std_of_first_answer": 0.4449451984837651, "avg_entropy_of_first_answer": 2.350295901298523, "lex_sim": 1.0}
{"question": "What claimed the life of singer Kathleen Ferrier?", "answer": ["Anaphylactic Shock"], "scores": [[0.03625813499093056, 0.02733554132282734, 0.9936031699180603, 0.8573660850524902, 0.01017171423882246]], "normalized_score": [0.09700214862823486], "tokens": [["\u0120An", "aph", "yl", "actic", "\u0120Shock"]], "entropy": [[5.979419708251953, 3.9778826236724854, 0.06145276874303818, 0.5188257694244385, 0.3876485228538513]], "avg_entropy": [2.185045878589153], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cancer pathology", "score_of_biggest_cluster": 0.09700214862823486, "score_of_first_answer": 0.09700214862823486, "max_score_of_first_answer": 0.9936031699180603, "min_score_of_first_answer": 0.01017171423882246, "avg_score": 0.09700214862823486, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4435241864604523, "std_of_first_answer": 0.4435241864604523, "avg_entropy_of_first_answer": 2.185045878589153, "lex_sim": 1.0}
{"question": "The world cup opens on Friday. Which country is hosting this year?", "answer": ["South Korea"], "scores": [[0.03665314242243767, 0.1601976752281189]], "normalized_score": [0.07662733644247055], "tokens": [["\u0120South", "\u0120Korea"]], "entropy": [[4.018655776977539, 0.7687243223190308]], "avg_entropy": [2.393690049648285], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "South africa", "score_of_biggest_cluster": 0.07662733644247055, "score_of_first_answer": 0.07662733644247055, "max_score_of_first_answer": 0.1601976752281189, "min_score_of_first_answer": 0.03665314242243767, "avg_score": 0.07662733644247055, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.061772266402840614, "std_of_first_answer": 0.061772266402840614, "avg_entropy_of_first_answer": 2.393690049648285, "lex_sim": 1.0}
{"question": "In medicine, chromotopsia is the term for abnormally coloured what?", "answer": ["eyes"], "scores": [[0.0366789810359478]], "normalized_score": [0.0366789773106575], "tokens": [["\u0120eyes"]], "entropy": [[5.413187503814697]], "avg_entropy": [5.413187503814697], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Vision (disambiguation)", "score_of_biggest_cluster": 0.0366789773106575, "score_of_first_answer": 0.0366789773106575, "max_score_of_first_answer": 0.0366789810359478, "min_score_of_first_answer": 0.0366789810359478, "avg_score": 0.0366789773106575, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.413187503814697, "lex_sim": 1.0}
{"question": "The largest ice fields in the southern hemisphere outside Antarctica are located where?", "answer": ["Erebos Icefield"], "scores": [[0.003229553811252117, 0.003068298567086458, 0.5257814526557922, 0.02112906612455845, 0.5104542970657349]], "normalized_score": [0.03547615185379982], "tokens": [["\u0120E", "reb", "os", "\u0120Ice", "field"]], "entropy": [[5.039002418518066, 4.53173828125, 3.238166570663452, 3.4028382301330566, 1.8941731452941895]], "avg_entropy": [3.621183729171753], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Patagonia (Argentina and Chile)", "score_of_biggest_cluster": 0.03547615185379982, "score_of_first_answer": 0.03547615185379982, "max_score_of_first_answer": 0.5257814526557922, "min_score_of_first_answer": 0.003068298567086458, "avg_score": 0.03547615185379982, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24947959952832252, "std_of_first_answer": 0.24947959952832252, "avg_entropy_of_first_answer": 3.621183729171753, "lex_sim": 1.0}
{"question": "Canaletto is famous for his landscapes of Venice and which other city?", "answer": ["Venice"], "scores": [[0.030021248385310173]], "normalized_score": [0.030021250247955322], "tokens": [["\u0120Venice"]], "entropy": [[4.780898094177246]], "avg_entropy": [4.780898094177246], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "London (UK)", "score_of_biggest_cluster": 0.030021250247955322, "score_of_first_answer": 0.030021250247955322, "max_score_of_first_answer": 0.030021248385310173, "min_score_of_first_answer": 0.030021248385310173, "avg_score": 0.030021250247955322, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.780898094177246, "lex_sim": 1.0}
{"question": "What type of creature is a discus?", "answer": ["Snapper"], "scores": [[0.0013859332539141178, 0.0466606505215168]], "normalized_score": [0.008041676133871078], "tokens": [["\u0120Sn", "apper"]], "entropy": [[5.841976165771484, 2.0180113315582275]], "avg_entropy": [3.929993748664856], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fish proteins", "score_of_biggest_cluster": 0.008041676133871078, "score_of_first_answer": 0.008041676133871078, "max_score_of_first_answer": 0.0466606505215168, "min_score_of_first_answer": 0.0013859332539141178, "avg_score": 0.008041676133871078, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02263735863380134, "std_of_first_answer": 0.02263735863380134, "avg_entropy_of_first_answer": 3.929993748664856, "lex_sim": 1.0}
{"question": "Which fictional UK television doctor lives in Tannochbrae?", "answer": ["Tannochbrae"], "scores": [[0.002693560905754566, 0.7915432453155518, 0.9196388125419617, 0.8746304512023926, 0.9953413009643555]], "normalized_score": [0.27953943610191345], "tokens": [["\u0120T", "ann", "och", "bra", "e"]], "entropy": [[5.171365737915039, 1.6027882099151611, 0.5545461773872375, 1.0317225456237793, 0.05027517303824425]], "avg_entropy": [1.6821395687758922], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dr Finlay", "score_of_biggest_cluster": 0.27953943610191345, "score_of_first_answer": 0.27953943610191345, "max_score_of_first_answer": 0.9953413009643555, "min_score_of_first_answer": 0.002693560905754566, "avg_score": 0.27953943610191345, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3630905798723335, "std_of_first_answer": 0.3630905798723335, "avg_entropy_of_first_answer": 1.6821395687758922, "lex_sim": 1.0}
{"question": "Who was Margaret Thatcher's first Chancellor of the Exchequer?", "answer": ["Peter Brooke"], "scores": [[0.009341198951005936, 0.048227787017822266]], "normalized_score": [0.021225111559033394], "tokens": [["\u0120Peter", "\u0120Brooke"]], "entropy": [[3.79085636138916, 3.9972681999206543]], "avg_entropy": [3.8940622806549072], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lord Howe of Aberavon", "score_of_biggest_cluster": 0.021225111559033394, "score_of_first_answer": 0.021225111559033394, "max_score_of_first_answer": 0.048227787017822266, "min_score_of_first_answer": 0.009341198951005936, "avg_score": 0.021225111559033394, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.019443294033408165, "std_of_first_answer": 0.019443294033408165, "avg_entropy_of_first_answer": 3.8940622806549072, "lex_sim": 1.0}
{"question": "Which island known as Ynys Gybi in Welsh, lies just off the west coast of Anglesey?", "answer": ["Eifionydd"], "scores": [[0.003960411995649338, 0.3557957708835602, 0.5113211274147034, 0.9847580194473267, 0.9996945858001709]], "normalized_score": [0.23451268672943115], "tokens": [["\u0120E", "if", "ion", "y", "dd"]], "entropy": [[4.392647743225098, 3.370962142944336, 2.161980152130127, 0.12731555104255676, 0.004103172570466995]], "avg_entropy": [2.011401752382517], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Holy Island (disambiguation)", "score_of_biggest_cluster": 0.23451268672943115, "score_of_first_answer": 0.23451268672943115, "max_score_of_first_answer": 0.9996945858001709, "min_score_of_first_answer": 0.003960411995649338, "avg_score": 0.23451268672943115, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3811516792081743, "std_of_first_answer": 0.3811516792081743, "avg_entropy_of_first_answer": 2.011401752382517, "lex_sim": 1.0}
{"question": "Which precious gem is the birthstone for May?", "answer": ["Emerald"], "scores": [[0.14261455833911896]], "normalized_score": [0.14261455833911896], "tokens": [["\u0120Emerald"]], "entropy": [[3.105894088745117]], "avg_entropy": [3.105894088745117], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Synthetic emerald", "score_of_biggest_cluster": 0.14261455833911896, "score_of_first_answer": 0.14261455833911896, "max_score_of_first_answer": 0.14261455833911896, "min_score_of_first_answer": 0.14261455833911896, "avg_score": 0.14261455833911896, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.105894088745117, "lex_sim": 1.0}
{"question": "Historically, what type of wood was used to make black piano keys?", "answer": ["Ivory"], "scores": [[0.09438376128673553]], "normalized_score": [0.09438374638557434], "tokens": [["\u0120Ivory"]], "entropy": [[4.036343574523926]], "avg_entropy": [4.036343574523926], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "St Helena Ebony", "score_of_biggest_cluster": 0.09438374638557434, "score_of_first_answer": 0.09438374638557434, "max_score_of_first_answer": 0.09438376128673553, "min_score_of_first_answer": 0.09438376128673553, "avg_score": 0.09438374638557434, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.036343574523926, "lex_sim": 1.0}
{"question": "Although both her parents are Australian, in which US state was Nicole Kidman born?", "answer": ["Florida"], "scores": [[0.00740015646442771]], "normalized_score": [0.007400155533105135], "tokens": [["\u0120Florida"]], "entropy": [[3.5672690868377686]], "avg_entropy": [3.5672690868377686], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Haiwii", "score_of_biggest_cluster": 0.007400155533105135, "score_of_first_answer": 0.007400155533105135, "max_score_of_first_answer": 0.00740015646442771, "min_score_of_first_answer": 0.00740015646442771, "avg_score": 0.007400155533105135, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.5672690868377686, "lex_sim": 1.0}
{"question": "Which British city has the most canals ?", "answer": ["Norwich"], "scores": [[0.08270402252674103]], "normalized_score": [0.08270402997732162], "tokens": [["\u0120Norwich"]], "entropy": [[4.6472296714782715]], "avg_entropy": [4.6472296714782715], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "County Borough of Birmingham", "score_of_biggest_cluster": 0.08270402997732162, "score_of_first_answer": 0.08270402997732162, "max_score_of_first_answer": 0.08270402252674103, "min_score_of_first_answer": 0.08270402252674103, "avg_score": 0.08270402997732162, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.6472296714782715, "lex_sim": 1.0}
{"question": "What is the surname of the film director whose first names are Francis Ford?", "answer": ["Ford"], "scores": [[0.08031399548053741]], "normalized_score": [0.08031398802995682], "tokens": [["\u0120Ford"]], "entropy": [[5.724441051483154]], "avg_entropy": [5.724441051483154], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Coppola (disambiguation)", "score_of_biggest_cluster": 0.08031398802995682, "score_of_first_answer": 0.08031398802995682, "max_score_of_first_answer": 0.08031399548053741, "min_score_of_first_answer": 0.08031399548053741, "avg_score": 0.08031398802995682, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.724441051483154, "lex_sim": 1.0}
{"question": "\"Which composer wrote the musical for the 18th-century opera, \"\"Idomeneo\"\"?\"", "answer": ["Handel"], "scores": [[0.05109501630067825, 0.9982909560203552]], "normalized_score": [0.22584882378578186], "tokens": [["\u0120Hand", "el"]], "entropy": [[4.036046981811523, 0.021105531603097916]], "avg_entropy": [2.0285762567073107], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joannes Chrysostomus Wolfgangus Theophilus Mozart", "score_of_biggest_cluster": 0.22584882378578186, "score_of_first_answer": 0.22584882378578186, "max_score_of_first_answer": 0.9982909560203552, "min_score_of_first_answer": 0.05109501630067825, "avg_score": 0.22584882378578186, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4735979698598385, "std_of_first_answer": 0.4735979698598385, "avg_entropy_of_first_answer": 2.0285762567073107, "lex_sim": 1.0}
{"question": "What movie franchise has been set in L.A., Miami, Tokyo, Rio, London, and Dubai?", "answer": ["The Matrix"], "scores": [[0.15880192816257477, 0.20608946681022644]], "normalized_score": [0.18090718984603882], "tokens": [["\u0120The", "\u0120Matrix"]], "entropy": [[4.957069396972656, 3.9773125648498535]], "avg_entropy": [4.467190980911255], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fast 8", "score_of_biggest_cluster": 0.18090718984603882, "score_of_first_answer": 0.18090718984603882, "max_score_of_first_answer": 0.20608946681022644, "min_score_of_first_answer": 0.15880192816257477, "avg_score": 0.18090718984603882, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.023643769323825836, "std_of_first_answer": 0.023643769323825836, "avg_entropy_of_first_answer": 4.467190980911255, "lex_sim": 1.0}
{"question": "Meaning Fox in Spanish, what was the secret identity of Don Diego De La Vega?", "answer": ["The Falcon"], "scores": [[0.055095016956329346, 0.011685763485729694]], "normalized_score": [0.025373756885528564], "tokens": [["\u0120The", "\u0120Falcon"]], "entropy": [[6.542198181152344, 6.5704665184021]], "avg_entropy": [6.556332349777222], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Don Diego Vega", "score_of_biggest_cluster": 0.025373756885528564, "score_of_first_answer": 0.025373756885528564, "max_score_of_first_answer": 0.055095016956329346, "min_score_of_first_answer": 0.011685763485729694, "avg_score": 0.025373756885528564, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.021704626735299826, "std_of_first_answer": 0.021704626735299826, "avg_entropy_of_first_answer": 6.556332349777222, "lex_sim": 1.0}
{"question": "Which drink is known as \u2018The Wine of Islam\u2019?", "answer": ["Muscatel"], "scores": [[0.008378681726753712, 0.3503396809101105, 0.49530795216560364]], "normalized_score": [0.11328699439764023], "tokens": [["\u0120Mus", "c", "atel"]], "entropy": [[6.011392593383789, 0.8742383122444153, 1.069650411605835]], "avg_entropy": [2.651760439078013], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kawha", "score_of_biggest_cluster": 0.11328699439764023, "score_of_first_answer": 0.11328699439764023, "max_score_of_first_answer": 0.49530795216560364, "min_score_of_first_answer": 0.008378681726753712, "avg_score": 0.11328699439764023, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20413863454998654, "std_of_first_answer": 0.20413863454998654, "avg_entropy_of_first_answer": 2.651760439078013, "lex_sim": 1.0}
{"question": "What is the international telephone dialling code for Germany?", "answer": ["+49"], "scores": [[0.045903291553258896, 0.9177348613739014]], "normalized_score": [0.20524875819683075], "tokens": [["\u0120+", "49"]], "entropy": [[1.4824798107147217, 0.6246048212051392]], "avg_entropy": [1.0535423159599304], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "forty-nine", "score_of_biggest_cluster": 0.20524875819683075, "score_of_first_answer": 0.20524875819683075, "max_score_of_first_answer": 0.9177348613739014, "min_score_of_first_answer": 0.045903291553258896, "avg_score": 0.20524875819683075, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.43591578491032124, "std_of_first_answer": 0.43591578491032124, "avg_entropy_of_first_answer": 1.0535423159599304, "lex_sim": 1.0}
{"question": "Whose portrait is to appear on the new \u00a310 note due to be issued in 2017?", "answer": ["David Cameron"], "scores": [[0.01816536672413349, 0.3571604788303375]], "normalized_score": [0.08054782450199127], "tokens": [["\u0120David", "\u0120Cameron"]], "entropy": [[4.81001091003418, 3.3526482582092285]], "avg_entropy": [4.081329584121704], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jane Austen", "score_of_biggest_cluster": 0.08054782450199127, "score_of_first_answer": 0.08054782450199127, "max_score_of_first_answer": 0.3571604788303375, "min_score_of_first_answer": 0.01816536672413349, "avg_score": 0.08054782450199127, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.16949755605310202, "std_of_first_answer": 0.16949755605310202, "avg_entropy_of_first_answer": 4.081329584121704, "lex_sim": 1.0}
{"question": "Which song begins with the line The taxman's taken all my dough?", "answer": ["Taxman (The Wall)"], "scores": [[0.14086046814918518, 0.9226043820381165, 0.10743535310029984, 0.015504793263971806, 0.0007577179349027574, 0.7349918484687805]], "normalized_score": [0.0702858716249466], "tokens": [["\u0120Tax", "man", "\u0120(", "The", "\u0120Wall", ")"]], "entropy": [[5.340821743011475, 0.5726146101951599, 3.129528045654297, 5.676977157592773, 4.869721412658691, 1.477954387664795]], "avg_entropy": [3.5112695594628653], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sunny Afternoon", "score_of_biggest_cluster": 0.0702858716249466, "score_of_first_answer": 0.0702858716249466, "max_score_of_first_answer": 0.9226043820381165, "min_score_of_first_answer": 0.0007577179349027574, "avg_score": 0.0702858716249466, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3668008470599752, "std_of_first_answer": 0.3668008470599752, "avg_entropy_of_first_answer": 3.5112695594628653, "lex_sim": 1.0}
{"question": "Who was the only Prime Minister of Britain to be assassinated?", "answer": ["John Coltrane"], "scores": [[0.076172836124897, 0.0050688982009887695, 0.17270387709140778, 0.9668322801589966]], "normalized_score": [0.08960696309804916], "tokens": [["\u0120John", "\u0120Col", "tr", "ane"]], "entropy": [[4.049619674682617, 4.498344421386719, 3.5953128337860107, 0.23928281664848328]], "avg_entropy": [3.0956399366259575], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Spencer perceval", "score_of_biggest_cluster": 0.08960696309804916, "score_of_first_answer": 0.08960696309804916, "max_score_of_first_answer": 0.9668322801589966, "min_score_of_first_answer": 0.0050688982009887695, "avg_score": 0.08960696309804916, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3866020617740443, "std_of_first_answer": 0.3866020617740443, "avg_entropy_of_first_answer": 3.0956399366259575, "lex_sim": 1.0}
{"question": "\"Which word completes each of the novels in a series by Enid Blyton - including \"\"The ??? of the Burnt Cottage\"\", \"\"The ??? of the Vanished Prince\"\" and \"\"The ??? of the Missing Necklace\"\"?\"", "answer": ["(nothing happens)"], "scores": [[0.0019794260151684284, 0.003204190172255039, 0.004521985072642565, 0.720363438129425]], "normalized_score": [0.011989046819508076], "tokens": [["\u0120(", "nothing", "\u0120happens", ")"]], "entropy": [[7.122708320617676, 6.197718143463135, 1.9910881519317627, 1.6103804111480713]], "avg_entropy": [4.230473756790161], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mystery", "score_of_biggest_cluster": 0.011989046819508076, "score_of_first_answer": 0.011989046819508076, "max_score_of_first_answer": 0.720363438129425, "min_score_of_first_answer": 0.0019794260151684284, "avg_score": 0.011989046819508076, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31052693753355726, "std_of_first_answer": 0.31052693753355726, "avg_entropy_of_first_answer": 4.230473756790161, "lex_sim": 1.0}
{"question": "The fictional character \u2018Disco Stu\u2019 appears in which US cartoon tv Series?", "answer": ["Muppet Babies"], "scores": [[0.0047705513425171375, 0.15760256350040436, 0.8268845081329346, 0.9908644556999207]], "normalized_score": [0.15754249691963196], "tokens": [["\u0120M", "uppet", "\u0120Bab", "ies"]], "entropy": [[5.0921430587768555, 2.8630073070526123, 0.9891666769981384, 0.06466276943683624]], "avg_entropy": [2.2522449530661106], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Promulent", "score_of_biggest_cluster": 0.15754249691963196, "score_of_first_answer": 0.15754249691963196, "max_score_of_first_answer": 0.9908644556999207, "min_score_of_first_answer": 0.0047705513425171375, "avg_score": 0.15754249691963196, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42136410143928255, "std_of_first_answer": 0.42136410143928255, "avg_entropy_of_first_answer": 2.2522449530661106, "lex_sim": 1.0}
{"question": "What is the family home of Cavendish's (Dukes of Devonshire)?", "answer": ["Chatsworth"], "scores": [[0.4877132773399353, 0.996461808681488, 0.9977083206176758]], "normalized_score": [0.7856147289276123], "tokens": [["\u0120Ch", "ats", "worth"]], "entropy": [[3.6479296684265137, 0.0364040806889534, 0.02097150683403015]], "avg_entropy": [1.2351017519831657], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chatsworth", "score_of_biggest_cluster": 0.7856147289276123, "score_of_first_answer": 0.7856147289276123, "max_score_of_first_answer": 0.9977083206176758, "min_score_of_first_answer": 0.4877132773399353, "avg_score": 0.7856147289276123, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.24012070254169587, "std_of_first_answer": 0.24012070254169587, "avg_entropy_of_first_answer": 1.2351017519831657, "lex_sim": 1.0}
{"question": "Which Roman Emperor was allegedy killed by his wife, Aggripina, in AD54?", "answer": ["Nero"], "scores": [[0.16662585735321045]], "normalized_score": [0.16662585735321045], "tokens": [["\u0120Nero"]], "entropy": [[2.5048842430114746]], "avg_entropy": [2.5048842430114746], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Claudius", "score_of_biggest_cluster": 0.16662585735321045, "score_of_first_answer": 0.16662585735321045, "max_score_of_first_answer": 0.16662585735321045, "min_score_of_first_answer": 0.16662585735321045, "avg_score": 0.16662585735321045, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.5048842430114746, "lex_sim": 1.0}
{"question": "Indians used wampum for money, what was wampum", "answer": ["\u32cb"], "scores": [[0.004329878371208906, 3.669468787848018e-05, 0.01801450550556183, 0.009828451089560986]], "normalized_score": [0.0023030133452266455], "tokens": [["\u0120", "\u00e3", "\u012d", "\u012d"]], "entropy": [[5.510768413543701, 4.423194885253906, 3.0192129611968994, 3.881551742553711]], "avg_entropy": [4.208682000637054], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Beads or shells", "score_of_biggest_cluster": 0.0023030133452266455, "score_of_first_answer": 0.0023030133452266455, "max_score_of_first_answer": 0.01801450550556183, "min_score_of_first_answer": 3.669468787848018e-05, "avg_score": 0.0023030133452266455, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.006717637280008306, "std_of_first_answer": 0.006717637280008306, "avg_entropy_of_first_answer": 4.208682000637054, "lex_sim": 0.0}
{"question": "Which is the largest in area - The Sahara Desert or Australia?", "answer": ["The Sahara Desert"], "scores": [[0.19509413838386536, 0.8587213158607483, 0.7353955507278442]], "normalized_score": [0.49759095907211304], "tokens": [["\u0120The", "\u0120Sahara", "\u0120Desert"]], "entropy": [[2.2781991958618164, 1.087843418121338, 0.9019958972930908]], "avg_entropy": [1.4226795037587483], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Grasslands of the Sahara", "score_of_biggest_cluster": 0.49759095907211304, "score_of_first_answer": 0.49759095907211304, "max_score_of_first_answer": 0.8587213158607483, "min_score_of_first_answer": 0.19509413838386536, "avg_score": 0.49759095907211304, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28820052648815525, "std_of_first_answer": 0.28820052648815525, "avg_entropy_of_first_answer": 1.4226795037587483, "lex_sim": 1.0}
{"question": "Willow, Reed, Grasshopper, and Dartford are species of which bird?", "answer": ["Robin"], "scores": [[0.04047761484980583]], "normalized_score": [0.04047761484980583], "tokens": [["\u0120Robin"]], "entropy": [[5.530372142791748]], "avg_entropy": [5.530372142791748], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Warblers", "score_of_biggest_cluster": 0.04047761484980583, "score_of_first_answer": 0.04047761484980583, "max_score_of_first_answer": 0.04047761484980583, "min_score_of_first_answer": 0.04047761484980583, "avg_score": 0.04047761484980583, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.530372142791748, "lex_sim": 1.0}
{"question": "What is the title of the 1996 Oscar-winning documentary about the Muhammad Ali and George Foreman 1974 \u2018Rumble in the Jungle\u2019 fight in Zaire?", "answer": ["Ali, Foreman and Johnson"], "scores": [[0.08932294696569443, 0.1034165769815445, 0.27434277534484863, 0.9987329840660095, 0.30532729625701904, 0.007559238467365503]], "normalized_score": [0.1342012882232666], "tokens": [["\u0120Ali", ",", "\u0120Fore", "man", "\u0120and", "\u0120Johnson"]], "entropy": [[4.0916571617126465, 3.6333606243133545, 4.073857307434082, 0.014183619059622288, 1.6676201820373535, 3.0248866081237793]], "avg_entropy": [2.7509275837801397], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "When We Were Kings", "score_of_biggest_cluster": 0.1342012882232666, "score_of_first_answer": 0.1342012882232666, "max_score_of_first_answer": 0.9987329840660095, "min_score_of_first_answer": 0.007559238467365503, "avg_score": 0.1342012882232666, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3310061218161732, "std_of_first_answer": 0.3310061218161732, "avg_entropy_of_first_answer": 2.7509275837801397, "lex_sim": 1.0}
{"question": "The announcement came in November 1978 that which member of the 'Royal Family' was to join the armed forces?", "answer": ["Prince Andrew"], "scores": [[0.4286175072193146, 0.2736831605434418]], "normalized_score": [0.3424987494945526], "tokens": [["\u0120Prince", "\u0120Andrew"]], "entropy": [[2.7364697456359863, 2.5465519428253174]], "avg_entropy": [2.641510844230652], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Prince Andrew, Duke of York", "score_of_biggest_cluster": 0.3424987494945526, "score_of_first_answer": 0.3424987494945526, "max_score_of_first_answer": 0.4286175072193146, "min_score_of_first_answer": 0.2736831605434418, "avg_score": 0.3424987494945526, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0774671733379364, "std_of_first_answer": 0.0774671733379364, "avg_entropy_of_first_answer": 2.641510844230652, "lex_sim": 1.0}
{"question": "Which TV detective drove a car with the registration plate 248RPA?", "answer": ["Colombo"], "scores": [[0.03331565856933594, 0.3952019214630127]], "normalized_score": [0.11474498361349106], "tokens": [["\u0120Col", "ombo"]], "entropy": [[6.14553165435791, 0.8563472032546997]], "avg_entropy": [3.500939428806305], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "E. Morse", "score_of_biggest_cluster": 0.11474498361349106, "score_of_first_answer": 0.11474498361349106, "max_score_of_first_answer": 0.3952019214630127, "min_score_of_first_answer": 0.03331565856933594, "avg_score": 0.11474498361349106, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18094313144683838, "std_of_first_answer": 0.18094313144683838, "avg_entropy_of_first_answer": 3.500939428806305, "lex_sim": 1.0}
{"question": "Who was Queen of France as the wife of Henry II (who reigned for 15 years from 1547) and remained the dominant person in France after his death during the reign of 3 of her 4 sons until 1589?", "answer": ["Catherine Mon ong"], "scores": [[0.18121688067913055, 0.0009041487355716527, 0.0031696672085672617, 0.04869438707828522]], "normalized_score": [0.01261051557958126], "tokens": [["\u0120Catherine", "\u0120Mon", "\u0120on", "g"]], "entropy": [[3.3307642936706543, 0.8305861949920654, 4.771772384643555, 5.099413871765137]], "avg_entropy": [3.508134186267853], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Catherine de M\u00e9dicis", "score_of_biggest_cluster": 0.01261051557958126, "score_of_first_answer": 0.01261051557958126, "max_score_of_first_answer": 0.18121688067913055, "min_score_of_first_answer": 0.0009041487355716527, "avg_score": 0.01261051557958126, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.07337286726448566, "std_of_first_answer": 0.07337286726448566, "avg_entropy_of_first_answer": 3.508134186267853, "lex_sim": 1.0}
{"question": "\"Who played Vyvyan, a psychotic punk metal medical student with orange-dyed & spiked hair and four metal stars embedded in his forehead, in the UK TV series \"\"The Young Ones\"\"?\"", "answer": ["Vyvyan"], "scores": [[0.0035033037420362234, 0.9160041809082031, 0.9857811331748962, 0.9967125654220581]], "normalized_score": [0.23696348071098328], "tokens": [["\u0120V", "y", "v", "yan"]], "entropy": [[5.681411266326904, 0.6712682247161865, 0.09915844351053238, 0.039186492562294006]], "avg_entropy": [1.6227561067789793], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ade Edmundson", "score_of_biggest_cluster": 0.23696348071098328, "score_of_first_answer": 0.23696348071098328, "max_score_of_first_answer": 0.9967125654220581, "min_score_of_first_answer": 0.0035033037420362234, "avg_score": 0.23696348071098328, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41799330026933207, "std_of_first_answer": 0.41799330026933207, "avg_entropy_of_first_answer": 1.6227561067789793, "lex_sim": 1.0}
{"question": "Which English clergyman/chemist discovered oxygen?", "answer": ["Priest"], "scores": [[0.03898393735289574]], "normalized_score": [0.03898393735289574], "tokens": [["\u0120Priest"]], "entropy": [[3.418229103088379]], "avg_entropy": [3.418229103088379], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Priestley, Joseph", "score_of_biggest_cluster": 0.03898393735289574, "score_of_first_answer": 0.03898393735289574, "max_score_of_first_answer": 0.03898393735289574, "min_score_of_first_answer": 0.03898393735289574, "avg_score": 0.03898393735289574, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.418229103088379, "lex_sim": 1.0}
{"question": "What colour is the bottom stripe on the U.S. flag?", "answer": ["Red"], "scores": [[0.08307906985282898]], "normalized_score": [0.08307906985282898], "tokens": [["\u0120Red"]], "entropy": [[3.6127145290374756]], "avg_entropy": [3.6127145290374756], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Red (colour)", "score_of_biggest_cluster": 0.08307906985282898, "score_of_first_answer": 0.08307906985282898, "max_score_of_first_answer": 0.08307906985282898, "min_score_of_first_answer": 0.08307906985282898, "avg_score": 0.08307906985282898, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.6127145290374756, "lex_sim": 1.0}
{"question": "What was the surname of the 19th president of the USA who had the first christian name Rutherford ?", "answer": ["Rutherford B. Hayes"], "scores": [[0.11243878304958344, 0.06159768998622894, 0.7171071171760559, 0.7662002444267273]], "normalized_score": [0.24837149679660797], "tokens": [["\u0120Rutherford", "\u0120B", ".", "\u0120Hayes"]], "entropy": [[5.608881950378418, 1.408862829208374, 1.6416687965393066, 1.2715930938720703]], "avg_entropy": [2.4827516674995422], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Hayes", "score_of_biggest_cluster": 0.24837149679660797, "score_of_first_answer": 0.24837149679660797, "max_score_of_first_answer": 0.7662002444267273, "min_score_of_first_answer": 0.06159768998622894, "avg_score": 0.24837149679660797, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32827010252831434, "std_of_first_answer": 0.32827010252831434, "avg_entropy_of_first_answer": 2.4827516674995422, "lex_sim": 1.0}
{"question": "In the TV series 'Friends', what is 'rachel's' surname?", "answer": ["Swanson, it is"], "scores": [[0.0028383417520672083, 0.006689922884106636, 0.02247234247624874, 0.3962174952030182]], "normalized_score": [0.020277615636587143], "tokens": [["\u0120Swanson", ",", "\u0120it", "\u0120is"]], "entropy": [[6.636967658996582, 0.30285143852233887, 5.608877182006836, 2.2746729850769043]], "avg_entropy": [3.7058423161506653], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Green (crater)", "score_of_biggest_cluster": 0.020277615636587143, "score_of_first_answer": 0.020277615636587143, "max_score_of_first_answer": 0.3962174952030182, "min_score_of_first_answer": 0.0028383417520672083, "avg_score": 0.020277615636587143, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.16711031920991673, "std_of_first_answer": 0.16711031920991673, "avg_entropy_of_first_answer": 3.7058423161506653, "lex_sim": 1.0}
{"question": "Which top German football club play home games at the Allianz Stadion?", "answer": ["Kaiserslautern"], "scores": [[0.0116632841527462, 0.9870477318763733, 0.9939343333244324, 0.9918102622032166, 0.9615305662155151]], "normalized_score": [0.4051181375980377], "tokens": [["\u0120Ka", "isers", "l", "aut", "ern"]], "entropy": [[3.3122055530548096, 0.0989844799041748, 0.07108977437019348, 0.07729057967662811, 0.2350982129573822]], "avg_entropy": [0.7589337199926376], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "FC Bayern M\u00fcnchen", "score_of_biggest_cluster": 0.4051181375980377, "score_of_first_answer": 0.4051181375980377, "max_score_of_first_answer": 0.9939343333244324, "min_score_of_first_answer": 0.0116632841527462, "avg_score": 0.4051181375980377, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3889400877011686, "std_of_first_answer": 0.3889400877011686, "avg_entropy_of_first_answer": 0.7589337199926376, "lex_sim": 1.0}
{"question": "What is the common name of the garden weed urtica dioica?", "answer": ["Garlic Urtica"], "scores": [[0.0029368612449616194, 0.9832740426063538, 0.11308644711971283, 0.8126396536827087, 0.9762606024742126]], "normalized_score": [0.1917283982038498], "tokens": [["\u0120Gar", "lic", "\u0120Ur", "t", "ica"]], "entropy": [[3.8192098140716553, 0.14175263047218323, 2.6425163745880127, 1.1403142213821411, 0.19576019048690796]], "avg_entropy": [1.58791064620018], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nettle", "score_of_biggest_cluster": 0.1917283982038498, "score_of_first_answer": 0.1917283982038498, "max_score_of_first_answer": 0.9832740426063538, "min_score_of_first_answer": 0.0029368612449616194, "avg_score": 0.1917283982038498, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43005950743042903, "std_of_first_answer": 0.43005950743042903, "avg_entropy_of_first_answer": 1.58791064620018, "lex_sim": 1.0}
{"question": "Which giant hunter of Greek mythology was shot dead with an arrow fired by the goddess Artemis?", "answer": ["Orion"], "scores": [[0.16077040135860443]], "normalized_score": [0.16077040135860443], "tokens": [["\u0120Orion"]], "entropy": [[4.739707946777344]], "avg_entropy": [4.739707946777344], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Orion (music)", "score_of_biggest_cluster": 0.16077040135860443, "score_of_first_answer": 0.16077040135860443, "max_score_of_first_answer": 0.16077040135860443, "min_score_of_first_answer": 0.16077040135860443, "avg_score": 0.16077040135860443, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.739707946777344, "lex_sim": 1.0}
{"question": "What is the name of the fictional motor racing circuit in the children\u2019s television series \u2018Roary the Racing Car\u2019?", "answer": ["Autodrome"], "scores": [[0.001937136403284967, 0.5503029823303223, 0.9124815464019775]], "normalized_score": [0.09908214956521988], "tokens": [["\u0120Aut", "od", "rome"]], "entropy": [[6.919008255004883, 2.2642712593078613, 0.4338875412940979]], "avg_entropy": [3.2057223518689475], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Silver Hatch", "score_of_biggest_cluster": 0.09908214956521988, "score_of_first_answer": 0.09908214956521988, "max_score_of_first_answer": 0.9124815464019775, "min_score_of_first_answer": 0.001937136403284967, "avg_score": 0.09908214956521988, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3743096661904045, "std_of_first_answer": 0.3743096661904045, "avg_entropy_of_first_answer": 3.2057223518689475, "lex_sim": 1.0}
{"question": "Which American singer was born Eleanora Fagan?", "answer": ["Fagan"], "scores": [[0.0163526963442564, 0.540804922580719]], "normalized_score": [0.09404052793979645], "tokens": [["\u0120F", "agan"]], "entropy": [[5.80567741394043, 2.091118574142456]], "avg_entropy": [3.948397994041443], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Billie halliday", "score_of_biggest_cluster": 0.09404052793979645, "score_of_first_answer": 0.09404052793979645, "max_score_of_first_answer": 0.540804922580719, "min_score_of_first_answer": 0.0163526963442564, "avg_score": 0.09404052793979645, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2622261131182313, "std_of_first_answer": 0.2622261131182313, "avg_entropy_of_first_answer": 3.948397994041443, "lex_sim": 1.0}
{"question": "'Call me Ishmael' are the opening words of which work of literature?", "answer": ["Moby Dick (novel)"], "scores": [[0.6943310499191284, 0.9981510043144226, 0.8622560501098633, 0.05043550580739975, 0.01711086742579937, 0.9972892999649048, 0.8502806425094604]], "normalized_score": [0.3312172591686249], "tokens": [["\u0120Mob", "y", "\u0120Dick", "\u0120(", "no", "vel", ")"]], "entropy": [[1.847792387008667, 0.022491106763482094, 0.5415993332862854, 1.377348780632019, 4.464338302612305, 0.031777799129486084, 0.8499696254730225]], "avg_entropy": [1.3050453335578953], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fedallah", "score_of_biggest_cluster": 0.3312172591686249, "score_of_first_answer": 0.3312172591686249, "max_score_of_first_answer": 0.9981510043144226, "min_score_of_first_answer": 0.01711086742579937, "avg_score": 0.3312172591686249, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3942547778092158, "std_of_first_answer": 0.3942547778092158, "avg_entropy_of_first_answer": 1.3050453335578953, "lex_sim": 1.0}
{"question": "What is the secret identity of Penrod \u2018Penry\u2019 Pooch?", "answer": ["Penry II"], "scores": [[0.04941616207361221, 0.4869980216026306, 0.0004340674786362797]], "normalized_score": [0.021860048174858093], "tokens": [["\u0120Pen", "ry", "\u0120II"]], "entropy": [[6.761447906494141, 2.392120361328125, 2.498016834259033]], "avg_entropy": [3.883861700693766], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hong Kong Phooie", "score_of_biggest_cluster": 0.021860048174858093, "score_of_first_answer": 0.021860048174858093, "max_score_of_first_answer": 0.4869980216026306, "min_score_of_first_answer": 0.0004340674786362797, "avg_score": 0.021860048174858093, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21873921835844243, "std_of_first_answer": 0.21873921835844243, "avg_entropy_of_first_answer": 3.883861700693766, "lex_sim": 1.0}
{"question": "In which year of the nineteenth century was the most notable eruption of Krakatoa", "answer": ["1883"], "scores": [[0.6675148606300354, 0.8804062008857727]], "normalized_score": [0.7666056156158447], "tokens": [["\u012018", "83"]], "entropy": [[2.4839065074920654, 0.7549076080322266]], "avg_entropy": [1.619407057762146], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, eight hundred and eighty-three", "score_of_biggest_cluster": 0.7666056156158447, "score_of_first_answer": 0.7666056156158447, "max_score_of_first_answer": 0.8804062008857727, "min_score_of_first_answer": 0.6675148606300354, "avg_score": 0.7666056156158447, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.10644567012786865, "std_of_first_answer": 0.10644567012786865, "avg_entropy_of_first_answer": 1.619407057762146, "lex_sim": 1.0}
{"question": "Whose single I saw mommy kissing Santa Claus sold three million copies in 1952 ?", "answer": ["Elvis"], "scores": [[0.1583549529314041]], "normalized_score": [0.1583549529314041], "tokens": [["\u0120Elvis"]], "entropy": [[5.497625350952148]], "avg_entropy": [5.497625350952148], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bwana Jimmy", "score_of_biggest_cluster": 0.1583549529314041, "score_of_first_answer": 0.1583549529314041, "max_score_of_first_answer": 0.1583549529314041, "min_score_of_first_answer": 0.1583549529314041, "avg_score": 0.1583549529314041, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.497625350952148, "lex_sim": 1.0}
{"question": "In which city\u2019s harbour was the ship Queen Elizabeth ravaged by fire in 1972?", "answer": ["Cardiff Bay"], "scores": [[0.01963173784315586, 0.021632039919495583]], "normalized_score": [0.020607629790902138], "tokens": [["\u0120Cardiff", "\u0120Bay"]], "entropy": [[4.971090316772461, 0.8687480092048645]], "avg_entropy": [2.9199191629886627], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kong Hong", "score_of_biggest_cluster": 0.020607629790902138, "score_of_first_answer": 0.020607629790902138, "max_score_of_first_answer": 0.021632039919495583, "min_score_of_first_answer": 0.01963173784315586, "avg_score": 0.020607629790902138, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0010001510381698608, "std_of_first_answer": 0.0010001510381698608, "avg_entropy_of_first_answer": 2.9199191629886627, "lex_sim": 1.0}
{"question": "\"What children's TV show featured the song \"\"Won't You Be My Neighbor\"\"?\"", "answer": ["Sesame Street"], "scores": [[0.4259002208709717, 0.9988512992858887, 0.9471986293792725]], "normalized_score": [0.7386124134063721], "tokens": [["\u0120S", "esame", "\u0120Street"]], "entropy": [[2.9989514350891113, 0.01477634347975254, 0.4245481491088867]], "avg_entropy": [1.1460919758925836], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mr. Rogers' Neighborhood", "score_of_biggest_cluster": 0.7386124134063721, "score_of_first_answer": 0.7386124134063721, "max_score_of_first_answer": 0.9988512992858887, "min_score_of_first_answer": 0.4259002208709717, "avg_score": 0.7386124134063721, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25877767533768475, "std_of_first_answer": 0.25877767533768475, "avg_entropy_of_first_answer": 1.1460919758925836, "lex_sim": 1.0}
{"question": "Who played 'Jonathan Hart' in the TV series 'Hart to Hart'?", "answer": ["John Wayne"], "scores": [[0.043351057916879654, 0.0068105836398899555]], "normalized_score": [0.017182722687721252], "tokens": [["\u0120John", "\u0120Wayne"]], "entropy": [[5.399692535400391, 5.387537956237793]], "avg_entropy": [5.393615245819092], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Robert Wagner", "score_of_biggest_cluster": 0.017182722687721252, "score_of_first_answer": 0.017182722687721252, "max_score_of_first_answer": 0.043351057916879654, "min_score_of_first_answer": 0.0068105836398899555, "avg_score": 0.017182722687721252, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01827023713849485, "std_of_first_answer": 0.01827023713849485, "avg_entropy_of_first_answer": 5.393615245819092, "lex_sim": 1.0}
{"question": "Which rugby league team will play at Langtree Park in 2012?", "answer": ["Salford City Reds"], "scores": [[0.024394601583480835, 0.9655331373214722, 0.9992769360542297, 0.1567791849374771, 0.7269488573074341]], "normalized_score": [0.30599039793014526], "tokens": [["\u0120S", "alf", "ord", "\u0120City", "\u0120Reds"]], "entropy": [[5.323390960693359, 0.31716853380203247, 0.007933180779218674, 2.0373826026916504, 1.0005619525909424]], "avg_entropy": [1.7372874461114407], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "St.helens", "score_of_biggest_cluster": 0.30599039793014526, "score_of_first_answer": 0.30599039793014526, "max_score_of_first_answer": 0.9992769360542297, "min_score_of_first_answer": 0.024394601583480835, "avg_score": 0.30599039793014526, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40833554957561435, "std_of_first_answer": 0.40833554957561435, "avg_entropy_of_first_answer": 1.7372874461114407, "lex_sim": 1.0}
{"question": "Which economist wrote The Wealth Of Nations?", "answer": ["Smith, Adam, 1723-1790"], "scores": [[0.0016860305331647396, 0.09604483097791672, 0.8633307814598083, 0.010369854979217052, 0.3840949833393097, 0.5208134651184082, 0.5371178388595581, 0.9612787961959839, 0.9928151369094849]], "normalized_score": [0.1743232160806656], "tokens": [["\u0120Smith", ",", "\u0120Adam", ",", "\u012017", "23", "-", "17", "90"]], "entropy": [[0.48667240142822266, 1.0227566957473755, 1.1427147388458252, 0.5527033805847168, 3.8516845703125, 2.1217260360717773, 1.7175090312957764, 0.2572574019432068, 0.06274557113647461]], "avg_entropy": [1.2461966474850972], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Vile maxim", "score_of_biggest_cluster": 0.1743232160806656, "score_of_first_answer": 0.1743232160806656, "max_score_of_first_answer": 0.9928151369094849, "min_score_of_first_answer": 0.0016860305331647396, "avg_score": 0.1743232160806656, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.373003447803237, "std_of_first_answer": 0.373003447803237, "avg_entropy_of_first_answer": 1.2461966474850972, "lex_sim": 1.0}
{"question": "In the church calendar what name is given to the three days before Ascension Day?", "answer": ["Lent"], "scores": [[0.0214567668735981]], "normalized_score": [0.02145676501095295], "tokens": [["\u0120Lent"]], "entropy": [[5.2216901779174805]], "avg_entropy": [5.2216901779174805], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gang-Day", "score_of_biggest_cluster": 0.02145676501095295, "score_of_first_answer": 0.02145676501095295, "max_score_of_first_answer": 0.0214567668735981, "min_score_of_first_answer": 0.0214567668735981, "avg_score": 0.02145676501095295, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.2216901779174805, "lex_sim": 1.0}
{"question": "In which Thomas Hardy novel does Michael Henchard appear?", "answer": ["Far from the Madding Crowd (novel)"], "scores": [[0.3956345021724701, 0.8992028832435608, 0.96314537525177, 0.9953442215919495, 0.9991853833198547, 0.9352606534957886, 0.0266713984310627, 0.0018627977697178721, 0.951507031917572, 0.7391507625579834]], "normalized_score": [0.3196503520011902], "tokens": [["\u0120Far", "\u0120from", "\u0120the", "\u0120Madd", "ing", "\u0120Crowd", "\u0120(", "no", "vel", ")"]], "entropy": [[2.6320152282714844, 0.3650946319103241, 0.28769445419311523, 0.04644697904586792, 0.007895184680819511, 0.4785357713699341, 0.5578263401985168, 1.3259482383728027, 0.46674495935440063, 1.6014777421951294]], "avg_entropy": [0.7769679529592395], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mayor of Casterbridge", "score_of_biggest_cluster": 0.3196503520011902, "score_of_first_answer": 0.3196503520011902, "max_score_of_first_answer": 0.9991853833198547, "min_score_of_first_answer": 0.0018627977697178721, "avg_score": 0.3196503520011902, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37925496550217064, "std_of_first_answer": 0.37925496550217064, "avg_entropy_of_first_answer": 0.7769679529592395, "lex_sim": 1.0}
{"question": "The world was first aware of the Chernobyl disaster after detectors were triggered at a nuclear plant in which country?", "answer": ["RCA"], "scores": [[0.0036407727748155594, 0.006766466423869133]], "normalized_score": [0.00496338214725256], "tokens": [["\u0120R", "CA"]], "entropy": [[4.255765914916992, 3.58137845993042]], "avg_entropy": [3.918572187423706], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Swedish sin", "score_of_biggest_cluster": 0.00496338214725256, "score_of_first_answer": 0.00496338214725256, "max_score_of_first_answer": 0.006766466423869133, "min_score_of_first_answer": 0.0036407727748155594, "avg_score": 0.00496338214725256, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0015628468245267868, "std_of_first_answer": 0.0015628468245267868, "avg_entropy_of_first_answer": 3.918572187423706, "lex_sim": 1.0}
{"question": "In the human body, what is the common name for the 'Talus bone'?", "answer": ["The Calcaneus"], "scores": [[0.02542220801115036, 0.12986290454864502, 0.8750166893005371, 0.965561032295227, 0.8040560483932495]], "normalized_score": [0.2952270209789276], "tokens": [["\u0120The", "\u0120Cal", "c", "ane", "us"]], "entropy": [[4.216332912445068, 4.591590881347656, 0.7966039180755615, 0.2194693386554718, 0.7355917096138]], "avg_entropy": [2.1119177520275114], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ancle", "score_of_biggest_cluster": 0.2952270209789276, "score_of_first_answer": 0.2952270209789276, "max_score_of_first_answer": 0.965561032295227, "min_score_of_first_answer": 0.02542220801115036, "avg_score": 0.2952270209789276, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3985147313378925, "std_of_first_answer": 0.3985147313378925, "avg_entropy_of_first_answer": 2.1119177520275114, "lex_sim": 1.0}
{"question": "What hourglass-shaped women's garment was made using whalebone?", "answer": ["Corset"], "scores": [[0.20634342730045319, 0.9049378037452698]], "normalized_score": [0.43212032318115234], "tokens": [["\u0120Cors", "et"]], "entropy": [[4.158961296081543, 0.43105852603912354]], "avg_entropy": [2.2950099110603333], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mechanical corset", "score_of_biggest_cluster": 0.43212032318115234, "score_of_first_answer": 0.43212032318115234, "max_score_of_first_answer": 0.9049378037452698, "min_score_of_first_answer": 0.20634342730045319, "avg_score": 0.43212032318115234, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3492971882224083, "std_of_first_answer": 0.3492971882224083, "avg_entropy_of_first_answer": 2.2950099110603333, "lex_sim": 1.0}
{"question": "James Esdaile, John Elliotson, Jean-Martin Charcot, Hippolyte Bernheim, Pierre Janet, mile Cou, Morton Prince, Clark L. Hull, Andrew Salter, Theodore R. Sarbin, Ernest Hilgard, Martin Theodore Orne, and Nicholas Spanos are associated with what?", "answer": ["Neurological Association"], "scores": [[0.010033595375716686, 0.09709728509187698, 0.0008287482196465135]], "normalized_score": [0.009311695583164692], "tokens": [["\u0120Neurolog", "ical", "\u0120Association"]], "entropy": [[5.812406063079834, 0.43221181631088257, 4.626680374145508]], "avg_entropy": [3.623766084512075], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hypnosis for fertility", "score_of_biggest_cluster": 0.009311695583164692, "score_of_first_answer": 0.009311695583164692, "max_score_of_first_answer": 0.09709728509187698, "min_score_of_first_answer": 0.0008287482196465135, "avg_score": 0.009311695583164692, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.043374911424409655, "std_of_first_answer": 0.043374911424409655, "avg_entropy_of_first_answer": 3.623766084512075, "lex_sim": 1.0}
{"question": "What is the capital of the U S state of Pennsylvania?", "answer": ["PHILADELPHIA, Pennsylvania"], "scores": [[0.0022699753753840923, 0.7902081608772278, 0.9770564436912537, 0.9924484491348267, 0.9955788254737854, 0.9972749352455139, 0.030524635687470436, 0.2632378339767456]], "normalized_score": [0.24705003201961517], "tokens": [["\u0120PH", "IL", "AD", "EL", "PH", "IA", ",", "\u0120Pennsylvania"]], "entropy": [[3.2281148433685303, 1.4436728954315186, 0.19390258193016052, 0.07669050991535187, 0.03853509947657585, 0.02589283138513565, 0.4820491075515747, 3.2464473247528076]], "avg_entropy": [1.0919131492264569], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Harrisburg (PA)", "score_of_biggest_cluster": 0.24705003201961517, "score_of_first_answer": 0.24705003201961517, "max_score_of_first_answer": 0.9972749352455139, "min_score_of_first_answer": 0.0022699753753840923, "avg_score": 0.24705003201961517, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42337090758945223, "std_of_first_answer": 0.42337090758945223, "avg_entropy_of_first_answer": 1.0919131492264569, "lex_sim": 1.0}
{"question": "ln the Bible Old Testament, who was the first High Priest of the Hebrews?", "answer": ["Samuel"], "scores": [[0.029938209801912308]], "normalized_score": [0.02993820793926716], "tokens": [["\u0120Samuel"]], "entropy": [[3.636906147003174]], "avg_entropy": [3.636906147003174], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "AARON", "score_of_biggest_cluster": 0.02993820793926716, "score_of_first_answer": 0.02993820793926716, "max_score_of_first_answer": 0.029938209801912308, "min_score_of_first_answer": 0.029938209801912308, "avg_score": 0.02993820793926716, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.636906147003174, "lex_sim": 1.0}
{"question": "Which is the second largest bay in Britain?", "answer": ["Torbay"], "scores": [[0.01944175362586975, 0.2809380888938904]], "normalized_score": [0.0739048644900322], "tokens": [["\u0120Tor", "bay"]], "entropy": [[4.5662431716918945, 1.860378384590149]], "avg_entropy": [3.2133107781410217], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Morecambe", "score_of_biggest_cluster": 0.0739048644900322, "score_of_first_answer": 0.0739048644900322, "max_score_of_first_answer": 0.2809380888938904, "min_score_of_first_answer": 0.01944175362586975, "avg_score": 0.0739048644900322, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13074816763401031, "std_of_first_answer": 0.13074816763401031, "avg_entropy_of_first_answer": 3.2133107781410217, "lex_sim": 1.0}
{"question": "Also known as the Blue Mosque, the Sultan Ahmed Mosque is in which city?", "answer": ["Istanbul"], "scores": [[0.7705278396606445]], "normalized_score": [0.7705278396606445], "tokens": [["\u0120Istanbul"]], "entropy": [[1.5962023735046387]], "avg_entropy": [1.5962023735046387], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stambulu", "score_of_biggest_cluster": 0.7705278396606445, "score_of_first_answer": 0.7705278396606445, "max_score_of_first_answer": 0.7705278396606445, "min_score_of_first_answer": 0.7705278396606445, "avg_score": 0.7705278396606445, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.5962023735046387, "lex_sim": 1.0}
{"question": "What colour are the berries of the mistletoe plant?", "answer": ["Blue"], "scores": [[0.06029748171567917]], "normalized_score": [0.06029747799038887], "tokens": [["\u0120Blue"]], "entropy": [[3.8496947288513184]], "avg_entropy": [3.8496947288513184], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "White (Colour)", "score_of_biggest_cluster": 0.06029747799038887, "score_of_first_answer": 0.06029747799038887, "max_score_of_first_answer": 0.06029748171567917, "min_score_of_first_answer": 0.06029748171567917, "avg_score": 0.06029747799038887, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.8496947288513184, "lex_sim": 1.0}
{"question": "\"Which English playwright born in 1572 wrote the plays \"\"Il Volpone\"\", \"\"The Alchemist\"\" & \"\"Every Man in his Humour\"\"?\"", "answer": ["John Fletcher"], "scores": [[0.03543437272310257, 0.44850799441337585]], "normalized_score": [0.12606586515903473], "tokens": [["\u0120John", "\u0120Fletcher"]], "entropy": [[1.3260042667388916, 1.8389654159545898]], "avg_entropy": [1.5824848413467407], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "(Ben)Jonson", "score_of_biggest_cluster": 0.12606586515903473, "score_of_first_answer": 0.12606586515903473, "max_score_of_first_answer": 0.44850799441337585, "min_score_of_first_answer": 0.03543437272310257, "avg_score": 0.12606586515903473, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20653681084513664, "std_of_first_answer": 0.20653681084513664, "avg_entropy_of_first_answer": 1.5824848413467407, "lex_sim": 1.0}
{"question": "In transport what is classified using the Whyte Notation or the UIC Classification?", "answer": ["LPG, Kerosene, Narrow Gauge Steam, High Speed Diesel Passenger, Natural Gas, Nuclear Steam Generator, Oil & Gas, Natural Gas Liquids, Petroleum, Propane, LPG, Gas Tied Logistics, Railway,"], "scores": [[0.004460613243281841, 0.04842006415128708, 0.09607695043087006, 0.0037566442042589188, 0.9840684533119202, 0.9758516550064087, 0.6701465845108032, 0.0069151706993579865, 0.000625887478236109, 0.20951205492019653, 0.9993758797645569, 0.05364242568612099, 0.37488073110580444, 0.02327340841293335, 0.4973680078983307, 0.21627405285835266, 0.0008350605494342744, 0.32223543524742126, 0.002546037081629038, 0.9211695194244385, 0.4956328570842743, 0.005269897636026144, 0.055904146283864975, 0.0018291022861376405, 0.5561835169792175, 0.008945563808083534, 0.008733929134905338, 0.5619320273399353, 0.36644718050956726, 0.007636468857526779, 0.8762209415435791, 0.08196591585874557, 0.7802635431289673, 0.7590736746788025, 0.03145550191402435, 0.44137537479400635, 0.19901934266090393, 0.9817797541618347, 0.7474450469017029, 0.011795989237725735, 0.8612089157104492, 0.6604995131492615, 0.010664847679436207, 0.022394485771656036, 0.012768003158271313, 0.001128005445934832, 0.6935041546821594, 0.6518859267234802, 0.013710160739719868, 0.351318895816803]], "normalized_score": [0.0796450674533844], "tokens": [["\u0120L", "PG", ",", "\u0120Ker", "os", "ene", ",", "\u0120N", "arrow", "\u0120Gau", "ge", "\u0120Steam", ",", "\u0120High", "\u0120Speed", "\u0120Diesel", "\u0120Passenger", ",", "\u0120Natural", "\u0120Gas", ",", "\u0120Nuclear", "\u0120Steam", "\u0120Generator", ",", "\u0120Oil", "\u0120&", "\u0120Gas", ",", "\u0120Natural", "\u0120Gas", "\u0120Liqu", "ids", ",", "\u0120Petroleum", ",", "\u0120Prop", "ane", ",", "\u0120L", "PG", ",", "\u0120Gas", "\u0120T", "ied", "\u0120Log", "istics", ",", "\u0120Railway", ","]], "entropy": [[6.401832580566406, 4.266661643981934, 3.5349788665771484, 5.073482036590576, 0.13797743618488312, 0.14430654048919678, 1.4306459426879883, 4.044592380523682, 1.4077799320220947, 3.0041065216064453, 0.006110232323408127, 2.6990506649017334, 2.4628686904907227, 4.951175689697266, 2.7007265090942383, 2.9621224403381348, 1.5862393379211426, 2.6812987327575684, 4.282744407653809, 0.5367316007614136, 2.5538322925567627, 5.218996047973633, 3.0146546363830566, 2.3764913082122803, 1.7982444763183594, 5.275310516357422, 4.045272350311279, 2.5468990802764893, 3.8352603912353516, 5.33960485458374, 0.9743416905403137, 3.1773645877838135, 0.8889133930206299, 1.2322368621826172, 5.336183547973633, 2.5752158164978027, 4.764965057373047, 0.1295558512210846, 1.2967978715896606, 5.175302505493164, 0.6706835031509399, 2.2460598945617676, 5.246888637542725, 3.4013898372650146, 4.296404838562012, 3.595092296600342, 1.4948244094848633, 1.4350239038467407, 5.521973133087158, 3.6587557792663574]], "avg_entropy": [2.948759431168437], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "WHEEL ARRANGEMENTS", "score_of_biggest_cluster": 0.0796450674533844, "score_of_first_answer": 0.0796450674533844, "max_score_of_first_answer": 0.9993758797645569, "min_score_of_first_answer": 0.000625887478236109, "avg_score": 0.0796450674533844, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3518929872554576, "std_of_first_answer": 0.3518929872554576, "avg_entropy_of_first_answer": 2.948759431168437, "lex_sim": 1.0}
{"question": "Which French vessel was the first ocean-going liner to exceed 1,000 feet in length ?", "answer": ["CSS Angamola"], "scores": [[0.0012293667532503605, 0.0032409420236945152, 0.13586516678333282, 0.010636298917233944]], "normalized_score": [0.008710899390280247], "tokens": [["\u0120CSS", "\u0120Ang", "am", "ola"]], "entropy": [[4.884847640991211, 5.103342056274414, 2.684673309326172, 2.253688335418701]], "avg_entropy": [3.7316378355026245], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Normandie (disambiguation)", "score_of_biggest_cluster": 0.008710899390280247, "score_of_first_answer": 0.008710899390280247, "max_score_of_first_answer": 0.13586516678333282, "min_score_of_first_answer": 0.0012293667532503605, "avg_score": 0.008710899390280247, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05675907477071923, "std_of_first_answer": 0.05675907477071923, "avg_entropy_of_first_answer": 3.7316378355026245, "lex_sim": 1.0}
{"question": "What was the first name of Mr. Ferrari, the founder of the car manufacturing company?", "answer": ["Enzo"], "scores": [[0.7085592746734619, 0.8658452033996582]], "normalized_score": [0.7832641005516052], "tokens": [["\u0120En", "zo"]], "entropy": [[2.155205488204956, 0.4117453992366791]], "avg_entropy": [1.2834754437208176], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Enzo", "score_of_biggest_cluster": 0.7832641005516052, "score_of_first_answer": 0.7832641005516052, "max_score_of_first_answer": 0.8658452033996582, "min_score_of_first_answer": 0.7085592746734619, "avg_score": 0.7832641005516052, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.07864296436309814, "std_of_first_answer": 0.07864296436309814, "avg_entropy_of_first_answer": 1.2834754437208176, "lex_sim": 1.0}
{"question": "What is referred to as earth's sister planet due to its similar size and mass?", "answer": ["Venus"], "scores": [[0.3338196873664856]], "normalized_score": [0.3338196873664856], "tokens": [["\u0120Venus"]], "entropy": [[3.223752975463867]], "avg_entropy": [3.223752975463867], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sol 2", "score_of_biggest_cluster": 0.3338196873664856, "score_of_first_answer": 0.3338196873664856, "max_score_of_first_answer": 0.3338196873664856, "min_score_of_first_answer": 0.3338196873664856, "avg_score": 0.3338196873664856, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.223752975463867, "lex_sim": 1.0}
{"question": "In the books by the Rev. W. V. Awdry and the UK TV series first broadcast in 1984 with Ringo Starr as storyteller, how many wheels does Thomas the Tank Engine have?", "answer": ["One hundred and eighty (four, two and four)"], "scores": [[0.03331317380070686, 0.040749188512563705, 0.7621487975120544, 0.04614837095141411, 0.0032063608523458242, 0.00627804733812809, 0.023426318541169167, 0.038042500615119934, 0.203841894865036, 0.09049013257026672, 0.7553588151931763]], "normalized_score": [0.05423174798488617], "tokens": [["\u0120One", "\u0120hundred", "\u0120and", "\u0120eighty", "\u0120(", "four", ",", "\u0120two", "\u0120and", "\u0120four", ")"]], "entropy": [[3.7789084911346436, 1.2658681869506836, 1.2119741439819336, 3.122342586517334, 1.6242338418960571, 4.4018754959106445, 3.1728029251098633, 4.409575939178467, 2.385374069213867, 1.9718133211135864, 1.6270320415496826]], "avg_entropy": [2.6338000947778877], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "six", "score_of_biggest_cluster": 0.05423174798488617, "score_of_first_answer": 0.05423174798488617, "max_score_of_first_answer": 0.7621487975120544, "min_score_of_first_answer": 0.0032063608523458242, "avg_score": 0.05423174798488617, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27689741522341743, "std_of_first_answer": 0.27689741522341743, "avg_entropy_of_first_answer": 2.6338000947778877, "lex_sim": 1.0}
{"question": "The Four Horsemen of the Apocalypse are described in which book of the New Testament?", "answer": ["\u314f"], "scores": [[0.0027501278091222048, 1.9073462453889078e-06, 0.3011852502822876, 0.024835115298628807]], "normalized_score": [0.002502766903489828], "tokens": [["\u0120", "\u00e3", "\u0127", "\u0131"]], "entropy": [[2.7986764907836914, 3.2285497188568115, 2.6931846141815186, 3.167829990386963]], "avg_entropy": [2.972060203552246], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Revelation (game)", "score_of_biggest_cluster": 0.002502766903489828, "score_of_first_answer": 0.002502766903489828, "max_score_of_first_answer": 0.3011852502822876, "min_score_of_first_answer": 1.9073462453889078e-06, "avg_score": 0.002502766903489828, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12680110239527687, "std_of_first_answer": 0.12680110239527687, "avg_entropy_of_first_answer": 2.972060203552246, "lex_sim": 0.0}
{"question": "In which country is the most northerly point on mainland Africa?", "answer": ["Cameroon"], "scores": [[0.012118548154830933]], "normalized_score": [0.012118546292185783], "tokens": [["\u0120Cameroon"]], "entropy": [[5.358208656311035]], "avg_entropy": [5.358208656311035], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ISO 3166-1:TN", "score_of_biggest_cluster": 0.012118546292185783, "score_of_first_answer": 0.012118546292185783, "max_score_of_first_answer": 0.012118548154830933, "min_score_of_first_answer": 0.012118548154830933, "avg_score": 0.012118546292185783, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.358208656311035, "lex_sim": 1.0}
{"question": "What is the main ingredient of borscht soup?", "answer": ["Turnip"], "scores": [[0.003927253652364016, 0.6130136251449585]], "normalized_score": [0.04906587675213814], "tokens": [["\u0120Turn", "ip"]], "entropy": [[4.712804317474365, 0.7037135362625122]], "avg_entropy": [2.7082589268684387], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Beetroot", "score_of_biggest_cluster": 0.04906587675213814, "score_of_first_answer": 0.04906587675213814, "max_score_of_first_answer": 0.6130136251449585, "min_score_of_first_answer": 0.003927253652364016, "avg_score": 0.04906587675213814, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30454318574629724, "std_of_first_answer": 0.30454318574629724, "avg_entropy_of_first_answer": 2.7082589268684387, "lex_sim": 1.0}
{"question": "Who was the winning owner of the Ascot Gold Cup in 2013?", "answer": ["John Magnier"], "scores": [[0.05585567280650139, 0.03658803924918175, 0.999524712562561]], "normalized_score": [0.12688200175762177], "tokens": [["\u0120John", "\u0120Magn", "ier"]], "entropy": [[6.366786956787109, 2.6008591651916504, 0.005959156434983015]], "avg_entropy": [2.9912017594712474], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Q.U.E.E.N.", "score_of_biggest_cluster": 0.12688200175762177, "score_of_first_answer": 0.12688200175762177, "max_score_of_first_answer": 0.999524712562561, "min_score_of_first_answer": 0.03658803924918175, "avg_score": 0.12688200175762177, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44946011256826046, "std_of_first_answer": 0.44946011256826046, "avg_entropy_of_first_answer": 2.9912017594712474, "lex_sim": 1.0}
{"question": "Which river flows through Cork City where it splits in two for a short distance, creating an island on which Cork's city centre is built?", "answer": ["River Lee"], "scores": [[0.12048249691724777, 0.9387565851211548]], "normalized_score": [0.3363089859485626], "tokens": [["\u0120River", "\u0120Lee"]], "entropy": [[3.057180166244507, 0.4734748601913452]], "avg_entropy": [1.765327513217926], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "River Lee, Ireland", "score_of_biggest_cluster": 0.3363089859485626, "score_of_first_answer": 0.3363089859485626, "max_score_of_first_answer": 0.9387565851211548, "min_score_of_first_answer": 0.12048249691724777, "avg_score": 0.3363089859485626, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4091370441019535, "std_of_first_answer": 0.4091370441019535, "avg_entropy_of_first_answer": 1.765327513217926, "lex_sim": 1.0}
{"question": "What is the minimum number of points needed to win a tiebreak in tennis, such as in the Hopman Cup for mixed doubles, on the ATP and WTA tours for doubles and sometmes in USTA league play?", "answer": ["3 points"], "scores": [[0.0311230830848217, 0.0502813421189785]], "normalized_score": [0.03955895081162453], "tokens": [["\u01203", "\u0120points"]], "entropy": [[3.640397071838379, 1.122475504875183]], "avg_entropy": [2.381436288356781], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "10", "score_of_biggest_cluster": 0.03955895081162453, "score_of_first_answer": 0.03955895081162453, "max_score_of_first_answer": 0.0502813421189785, "min_score_of_first_answer": 0.0311230830848217, "avg_score": 0.03955895081162453, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0095791295170784, "std_of_first_answer": 0.0095791295170784, "avg_entropy_of_first_answer": 2.381436288356781, "lex_sim": 1.0}
{"question": "What name for a temple derives from the ancient Greek meaning 'to all gods' ?", "answer": ["Daeum"], "scores": [[0.0077123516239225864, 0.0009660348296165466, 0.3774344325065613]], "normalized_score": [0.01411476545035839], "tokens": [["\u0120D", "ae", "um"]], "entropy": [[3.6797244548797607, 2.985349178314209, 3.645376682281494]], "avg_entropy": [3.4368167718251548], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pantheon (comics)", "score_of_biggest_cluster": 0.01411476545035839, "score_of_first_answer": 0.01411476545035839, "max_score_of_first_answer": 0.3774344325065613, "min_score_of_first_answer": 0.0009660348296165466, "avg_score": 0.01411476545035839, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17590034562909754, "std_of_first_answer": 0.17590034562909754, "avg_entropy_of_first_answer": 3.4368167718251548, "lex_sim": 1.0}
{"question": "Who became the new French president in 2007?", "answer": ["Fran\u00e7ois Hollande"], "scores": [[0.0026273615658283234, 0.19651131331920624]], "normalized_score": [0.02272237278521061], "tokens": [["\u0120Fran\u00c3\u00a7ois", "\u0120Hollande"]], "entropy": [[0.8228359222412109, 1.6927400827407837]], "avg_entropy": [1.2577880024909973], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hyperpresident", "score_of_biggest_cluster": 0.02272237278521061, "score_of_first_answer": 0.02272237278521061, "max_score_of_first_answer": 0.19651131331920624, "min_score_of_first_answer": 0.0026273615658283234, "avg_score": 0.02272237278521061, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09694197587668896, "std_of_first_answer": 0.09694197587668896, "avg_entropy_of_first_answer": 1.2577880024909973, "lex_sim": 1.0}
{"question": "Which group released an album in 1986 entitled \u2018The Queen is Dead\u2019?", "answer": ["Genesis"], "scores": [[0.003863166319206357]], "normalized_score": [0.0038631660863757133], "tokens": [["\u0120Genesis"]], "entropy": [[3.2068443298339844]], "avg_entropy": [3.2068443298339844], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dale Hibbert", "score_of_biggest_cluster": 0.0038631660863757133, "score_of_first_answer": 0.0038631660863757133, "max_score_of_first_answer": 0.003863166319206357, "min_score_of_first_answer": 0.003863166319206357, "avg_score": 0.0038631660863757133, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.2068443298339844, "lex_sim": 1.0}
{"question": "What technical word is given usually to the right-side odd-numbered page of a book?", "answer": ["Quiescent"], "scores": [[0.0023324231151491404, 0.08092569559812546, 0.8359298706054688]], "normalized_score": [0.05403658747673035], "tokens": [["\u0120Qu", "ies", "cent"]], "entropy": [[5.629452705383301, 3.390193462371826, 0.7638845443725586]], "avg_entropy": [3.261176904042562], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Verso and recto", "score_of_biggest_cluster": 0.05403658747673035, "score_of_first_answer": 0.05403658747673035, "max_score_of_first_answer": 0.8359298706054688, "min_score_of_first_answer": 0.0023324231151491404, "avg_score": 0.05403658747673035, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3758091878748633, "std_of_first_answer": 0.3758091878748633, "avg_entropy_of_first_answer": 3.261176904042562, "lex_sim": 1.0}
{"question": "\"Who wrote the opera \"\"Gloriana\"\" for the coronation of Queen Elizabeth II?\"", "answer": ["Thomas Ad\u00e8s"], "scores": [[0.019803954288363457, 0.15487581491470337, 0.6209336519241333, 0.9938667416572571]], "normalized_score": [0.20858217775821686], "tokens": [["\u0120Thomas", "\u0120Ad", "\u00c3\u00a8", "s"]], "entropy": [[4.71121883392334, 3.7309927940368652, 1.507451057434082, 0.04954982548952103]], "avg_entropy": [2.499803127720952], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Britten, Benjamin", "score_of_biggest_cluster": 0.20858217775821686, "score_of_first_answer": 0.20858217775821686, "max_score_of_first_answer": 0.9938667416572571, "min_score_of_first_answer": 0.019803954288363457, "avg_score": 0.20858217775821686, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3863769537852351, "std_of_first_answer": 0.3863769537852351, "avg_entropy_of_first_answer": 2.499803127720952, "lex_sim": 1.0}
{"question": "Luzon, Visayas and Mindanao form the three major island groups of which country?", "answer": ["\u32c2"], "scores": [[0.0038737428840249777, 1.0866722732316703e-05, 0.024073241278529167, 0.014719867147505283]], "normalized_score": [0.001965246396139264], "tokens": [["\u0120", "\u00e3", "\u012d", "\u0124"]], "entropy": [[2.177016019821167, 2.698286771774292, 2.5009589195251465, 3.8707237243652344]], "avg_entropy": [2.81174635887146], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Islas Filipinas", "score_of_biggest_cluster": 0.001965246396139264, "score_of_first_answer": 0.001965246396139264, "max_score_of_first_answer": 0.024073241278529167, "min_score_of_first_answer": 1.0866722732316703e-05, "avg_score": 0.001965246396139264, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.009432053493143256, "std_of_first_answer": 0.009432053493143256, "avg_entropy_of_first_answer": 2.81174635887146, "lex_sim": 0.0}
{"question": "\"What profession had been followed by Yorick, a character in Shakespeare's \"\"Hamlet\"\"?\"", "answer": ["Dentist"], "scores": [[0.002558094682171941, 0.7221620678901672]], "normalized_score": [0.04298091307282448], "tokens": [["\u0120Dent", "ist"]], "entropy": [[5.650730133056641, 0.8271929025650024]], "avg_entropy": [3.2389615178108215], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Folly And Fool", "score_of_biggest_cluster": 0.04298091307282448, "score_of_first_answer": 0.04298091307282448, "max_score_of_first_answer": 0.7221620678901672, "min_score_of_first_answer": 0.002558094682171941, "avg_score": 0.04298091307282448, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35980198660399765, "std_of_first_answer": 0.35980198660399765, "avg_entropy_of_first_answer": 3.2389615178108215, "lex_sim": 1.0}
{"question": "By what name was the American short-story writer William Sydney Porter better known?", "answer": ["O. Henry"], "scores": [[0.29787376523017883, 0.591861367225647, 0.9542969465255737]], "normalized_score": [0.5520502328872681], "tokens": [["\u0120O", ".", "\u0120Henry"]], "entropy": [[2.504243850708008, 2.273808002471924, 0.2735871970653534]], "avg_entropy": [1.683879683415095], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "O.henry books", "score_of_biggest_cluster": 0.5520502328872681, "score_of_first_answer": 0.5520502328872681, "max_score_of_first_answer": 0.9542969465255737, "min_score_of_first_answer": 0.29787376523017883, "avg_score": 0.5520502328872681, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2684688378795337, "std_of_first_answer": 0.2684688378795337, "avg_entropy_of_first_answer": 1.683879683415095, "lex_sim": 1.0}
{"question": "In which 1956 film did actress Gina Lollobrigida make her debut in American films?", "answer": ["Bell, Book and Candle"], "scores": [[0.006268953904509544, 0.2801133990287781, 0.896935760974884, 0.7610662579536438, 0.9913910627365112]], "normalized_score": [0.26001089811325073], "tokens": [["\u0120Bell", ",", "\u0120Book", "\u0120and", "\u0120Candle"]], "entropy": [[5.737237930297852, 3.2553248405456543, 0.6287482976913452, 0.8538671731948853, 0.09224900603294373]], "avg_entropy": [2.113485449552536], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Trapese", "score_of_biggest_cluster": 0.26001089811325073, "score_of_first_answer": 0.26001089811325073, "max_score_of_first_answer": 0.9913910627365112, "min_score_of_first_answer": 0.006268953904509544, "avg_score": 0.26001089811325073, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37982079542997627, "std_of_first_answer": 0.37982079542997627, "avg_entropy_of_first_answer": 2.113485449552536, "lex_sim": 1.0}
{"question": "On the 11th of February 1990 Mike Tyson (and the world!) was stunned when he lost his heavyweight crown in the tenth round to which boxer?", "answer": ["Larry Holmes"], "scores": [[0.0491027869284153, 0.9946152567863464]], "normalized_score": [0.22099408507347107], "tokens": [["\u0120Larry", "\u0120Holmes"]], "entropy": [[4.3122687339782715, 0.06043636053800583]], "avg_entropy": [2.1863525472581387], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "James %22Buster%22 Douglas", "score_of_biggest_cluster": 0.22099408507347107, "score_of_first_answer": 0.22099408507347107, "max_score_of_first_answer": 0.9946152567863464, "min_score_of_first_answer": 0.0491027869284153, "avg_score": 0.22099408507347107, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.47275623492896557, "std_of_first_answer": 0.47275623492896557, "avg_entropy_of_first_answer": 2.1863525472581387, "lex_sim": 1.0}
{"question": "In which modern-day country is the birthplace of Buddha (Siddhartha Gautama)?", "answer": ["India"], "scores": [[0.24000120162963867]], "normalized_score": [0.24000118672847748], "tokens": [["\u0120India"]], "entropy": [[2.7835559844970703]], "avg_entropy": [2.7835559844970703], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nepol", "score_of_biggest_cluster": 0.24000118672847748, "score_of_first_answer": 0.24000118672847748, "max_score_of_first_answer": 0.24000120162963867, "min_score_of_first_answer": 0.24000120162963867, "avg_score": 0.24000118672847748, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.7835559844970703, "lex_sim": 1.0}
{"question": "\"Who was known as \"\"The Waltz King\"\"?\"", "answer": ["Johann Strauss II"], "scores": [[0.4263570308685303, 0.9201794266700745, 0.2927546203136444]], "normalized_score": [0.486089825630188], "tokens": [["\u0120Johann", "\u0120Strauss", "\u0120II"]], "entropy": [[3.6631124019622803, 0.6272565126419067, 1.8681154251098633]], "avg_entropy": [2.0528281132380166], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Strauss jnr", "score_of_biggest_cluster": 0.486089825630188, "score_of_first_answer": 0.486089825630188, "max_score_of_first_answer": 0.9201794266700745, "min_score_of_first_answer": 0.2927546203136444, "avg_score": 0.486089825630188, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.26985017454807875, "std_of_first_answer": 0.26985017454807875, "avg_entropy_of_first_answer": 2.0528281132380166, "lex_sim": 1.0}
{"question": "According to Greek myth, who was the wife of Odysseus, celebrated for her faithfulness to him during his twenty year absence?", "answer": ["Circe"], "scores": [[0.07538794726133347, 0.9882579445838928]], "normalized_score": [0.2729519009590149], "tokens": [["\u0120Cir", "ce"]], "entropy": [[2.5435495376586914, 0.11501223593950272]], "avg_entropy": [1.329280886799097], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "PENELOPE", "score_of_biggest_cluster": 0.2729519009590149, "score_of_first_answer": 0.2729519009590149, "max_score_of_first_answer": 0.9882579445838928, "min_score_of_first_answer": 0.07538794726133347, "avg_score": 0.2729519009590149, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4564349986612797, "std_of_first_answer": 0.4564349986612797, "avg_entropy_of_first_answer": 1.329280886799097, "lex_sim": 1.0}
{"question": "\"What movie featured the song \"\"You've Got a Friend in Me\"\"?\"", "answer": ["Toy Story"], "scores": [[0.20603998005390167, 0.9643719792366028]], "normalized_score": [0.4457568824291229], "tokens": [["\u0120Toy", "\u0120Story"]], "entropy": [[4.392693996429443, 0.2763991057872772]], "avg_entropy": [2.3345465511083603], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "To Infinity and Beyond (catchphrase)", "score_of_biggest_cluster": 0.4457568824291229, "score_of_first_answer": 0.4457568824291229, "max_score_of_first_answer": 0.9643719792366028, "min_score_of_first_answer": 0.20603998005390167, "avg_score": 0.4457568824291229, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37916599959135056, "std_of_first_answer": 0.37916599959135056, "avg_entropy_of_first_answer": 2.3345465511083603, "lex_sim": 1.0}
{"question": "Which entertainer had a gospel minister character called Brother Lee Love?", "answer": ["Brother Lee Lee"], "scores": [[0.24458245933055878, 0.10678468644618988, 0.003913901746273041]], "normalized_score": [0.046757154166698456], "tokens": [["\u0120Brother", "\u0120Lee", "\u0120Lee"]], "entropy": [[5.09062385559082, 4.053107738494873, 3.0428247451782227]], "avg_entropy": [4.062185446421306], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kenny Everett", "score_of_biggest_cluster": 0.046757154166698456, "score_of_first_answer": 0.046757154166698456, "max_score_of_first_answer": 0.24458245933055878, "min_score_of_first_answer": 0.003913901746273041, "avg_score": 0.046757154166698456, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09859681038697697, "std_of_first_answer": 0.09859681038697697, "avg_entropy_of_first_answer": 4.062185446421306, "lex_sim": 1.0}
{"question": "Which acid is found in rhubarb leaves ?", "answer": ["Hydrochloric acid"], "scores": [[0.007269368041306734, 0.31418082118034363, 0.9780851602554321, 0.7737108469009399]], "normalized_score": [0.20389561355113983], "tokens": [["\u0120Hydro", "chlor", "ic", "\u0120acid"]], "entropy": [[5.352079391479492, 1.2017714977264404, 0.18261757493019104, 0.8651169538497925]], "avg_entropy": [1.900396354496479], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "OXALIC", "score_of_biggest_cluster": 0.20389561355113983, "score_of_first_answer": 0.20389561355113983, "max_score_of_first_answer": 0.9780851602554321, "min_score_of_first_answer": 0.007269368041306734, "avg_score": 0.20389561355113983, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38060939286043466, "std_of_first_answer": 0.38060939286043466, "avg_entropy_of_first_answer": 1.900396354496479, "lex_sim": 1.0}
{"question": "The longest running television series ever, November 6, 1947 saw the debut of what now weekly news program, now hosted by David Gregory, following the unfortunante sudden death of host Tim Russert?", "answer": ["Meet the Press"], "scores": [[0.5612610578536987, 0.9234265685081482, 0.9956848621368408]], "normalized_score": [0.802102267742157], "tokens": [["\u0120Meet", "\u0120the", "\u0120Press"]], "entropy": [[2.4993250370025635, 0.3400818705558777, 0.03373195230960846]], "avg_entropy": [0.9577129532893499], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Meet the Press with David Gregory", "score_of_biggest_cluster": 0.802102267742157, "score_of_first_answer": 0.802102267742157, "max_score_of_first_answer": 0.9956848621368408, "min_score_of_first_answer": 0.5612610578536987, "avg_score": 0.802102267742157, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.19006114800639123, "std_of_first_answer": 0.19006114800639123, "avg_entropy_of_first_answer": 0.9577129532893499, "lex_sim": 1.0}
{"question": "George W Bush was given which diminutive herbaceous nickname?", "answer": ["Dubya"], "scores": [[0.1205253005027771, 0.35553282499313354]], "normalized_score": [0.207004114985466], "tokens": [["\u0120Dub", "ya"]], "entropy": [[5.672613143920898, 2.5274858474731445]], "avg_entropy": [4.1000494956970215], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bushes", "score_of_biggest_cluster": 0.207004114985466, "score_of_first_answer": 0.207004114985466, "max_score_of_first_answer": 0.35553282499313354, "min_score_of_first_answer": 0.1205253005027771, "avg_score": 0.207004114985466, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11750376224517822, "std_of_first_answer": 0.11750376224517822, "avg_entropy_of_first_answer": 4.1000494956970215, "lex_sim": 1.0}
{"question": "Who won the Oscar for Best Film Director for the film Silence of the Lambs?", "answer": ["Christopher Nolan"], "scores": [[0.009125134907662868, 0.4540117084980011]], "normalized_score": [0.06436550617218018], "tokens": [["\u0120Christopher", "\u0120Nolan"]], "entropy": [[4.684165954589844, 3.7535297870635986]], "avg_entropy": [4.218847870826721], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Demme, Jonathan", "score_of_biggest_cluster": 0.06436550617218018, "score_of_first_answer": 0.06436550617218018, "max_score_of_first_answer": 0.4540117084980011, "min_score_of_first_answer": 0.009125134907662868, "avg_score": 0.06436550617218018, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22244328679516912, "std_of_first_answer": 0.22244328679516912, "avg_entropy_of_first_answer": 4.218847870826721, "lex_sim": 1.0}
{"question": "In which country is the annual International Alphorn Festival held?", "answer": ["UK"], "scores": [[0.0012846948811784387]], "normalized_score": [0.001284694648347795], "tokens": [["\u0120UK"]], "entropy": [[4.760822296142578]], "avg_entropy": [4.760822296142578], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ISO 3166-1:CH", "score_of_biggest_cluster": 0.001284694648347795, "score_of_first_answer": 0.001284694648347795, "max_score_of_first_answer": 0.0012846948811784387, "min_score_of_first_answer": 0.0012846948811784387, "avg_score": 0.001284694648347795, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.760822296142578, "lex_sim": 1.0}
{"question": "According to the bible, who helped Christ with the cross?", "answer": ["Simon of Cyrene"], "scores": [[0.38284751772880554, 0.40691518783569336, 0.9941546320915222, 0.9806771278381348]], "normalized_score": [0.6242771148681641], "tokens": [["\u0120Simon", "\u0120of", "\u0120Cy", "rene"]], "entropy": [[3.6659274101257324, 2.1823387145996094, 0.06761927157640457, 0.12242142111063004]], "avg_entropy": [1.509576704353094], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Simon of cyrene", "score_of_biggest_cluster": 0.6242771148681641, "score_of_first_answer": 0.6242771148681641, "max_score_of_first_answer": 0.9941546320915222, "min_score_of_first_answer": 0.38284751772880554, "avg_score": 0.6242771148681641, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.296427737330753, "std_of_first_answer": 0.296427737330753, "avg_entropy_of_first_answer": 1.509576704353094, "lex_sim": 1.0}
{"question": "The Levant, (deriving from 15th century French, lever, 'rising', relating to the Eastern sunrise) refers to what part of the world?", "answer": ["Middle Eastern countries"], "scores": [[0.049028221517801285, 0.027348298579454422, 0.13938330113887787]], "normalized_score": [0.05717362463474274], "tokens": [["\u0120Middle", "\u0120Eastern", "\u0120countries"]], "entropy": [[4.54832649230957, 1.0751628875732422, 2.6755497455596924]], "avg_entropy": [2.7663463751475015], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eastern Mediterranean", "score_of_biggest_cluster": 0.05717362463474274, "score_of_first_answer": 0.05717362463474274, "max_score_of_first_answer": 0.13938330113887787, "min_score_of_first_answer": 0.027348298579454422, "avg_score": 0.05717362463474274, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.048517924832459564, "std_of_first_answer": 0.048517924832459564, "avg_entropy_of_first_answer": 2.7663463751475015, "lex_sim": 1.0}
{"question": "Tanganyika and Zanzibar merged in 1964 to become which East African Republic?", "answer": ["TANGAZINC"], "scores": [[0.09506385028362274, 0.24340546131134033, 0.012305364944040775, 0.36314794421195984, 0.009318294934928417]], "normalized_score": [0.06262849271297455], "tokens": [["\u0120T", "AN", "GA", "Z", "INC"]], "entropy": [[2.996647596359253, 3.4059317111968994, 1.1433923244476318, 2.1324055194854736, 3.9052352905273438]], "avg_entropy": [2.7167224884033203], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tanzanian", "score_of_biggest_cluster": 0.06262849271297455, "score_of_first_answer": 0.06262849271297455, "max_score_of_first_answer": 0.36314794421195984, "min_score_of_first_answer": 0.009318294934928417, "avg_score": 0.06262849271297455, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13840704592958622, "std_of_first_answer": 0.13840704592958622, "avg_entropy_of_first_answer": 2.7167224884033203, "lex_sim": 1.0}
{"question": "How many \u2018E\u2019 tiles are provided in a Scrabble game?", "answer": ["Two"], "scores": [[0.01907879300415516]], "normalized_score": [0.01907879114151001], "tokens": [["\u0120Two"]], "entropy": [[5.106527328491211]], "avg_entropy": [5.106527328491211], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "twelve", "score_of_biggest_cluster": 0.01907879114151001, "score_of_first_answer": 0.01907879114151001, "max_score_of_first_answer": 0.01907879300415516, "min_score_of_first_answer": 0.01907879300415516, "avg_score": 0.01907879114151001, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.106527328491211, "lex_sim": 1.0}
{"question": "The 1965 film \u2018The Heroes of Telemark\u2019 is set in which European country?", "answer": ["Nordland"], "scores": [[0.0007086780969984829, 0.2502588629722595]], "normalized_score": [0.013317395932972431], "tokens": [["\u0120Nord", "land"]], "entropy": [[2.1074695587158203, 2.7619080543518066]], "avg_entropy": [2.4346888065338135], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Norv\u00e8ge", "score_of_biggest_cluster": 0.013317395932972431, "score_of_first_answer": 0.013317395932972431, "max_score_of_first_answer": 0.2502588629722595, "min_score_of_first_answer": 0.0007086780969984829, "avg_score": 0.013317395932972431, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.12477509243763052, "std_of_first_answer": 0.12477509243763052, "avg_entropy_of_first_answer": 2.4346888065338135, "lex_sim": 1.0}
{"question": "Glen Moy and Autumn Bliss are varieties of which fruit?", "answer": ["Apple"], "scores": [[0.09391041100025177]], "normalized_score": [0.09391040354967117], "tokens": [["\u0120Apple"]], "entropy": [[4.8959197998046875]], "avg_entropy": [4.8959197998046875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hindberries", "score_of_biggest_cluster": 0.09391040354967117, "score_of_first_answer": 0.09391040354967117, "max_score_of_first_answer": 0.09391041100025177, "min_score_of_first_answer": 0.09391041100025177, "avg_score": 0.09391040354967117, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.8959197998046875, "lex_sim": 1.0}
{"question": "Which river is spanned by Abraham Darby's Iron Bridge?", "answer": ["River Tees"], "scores": [[0.15637359023094177, 0.0343545638024807, 0.9740007519721985]], "normalized_score": [0.1736077219247818], "tokens": [["\u0120River", "\u0120Te", "es"]], "entropy": [[3.5271105766296387, 3.0828213691711426, 0.17390543222427368]], "avg_entropy": [2.261279126008352], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Seven Boar", "score_of_biggest_cluster": 0.1736077219247818, "score_of_first_answer": 0.1736077219247818, "max_score_of_first_answer": 0.9740007519721985, "min_score_of_first_answer": 0.0343545638024807, "avg_score": 0.1736077219247818, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4171780564924677, "std_of_first_answer": 0.4171780564924677, "avg_entropy_of_first_answer": 2.261279126008352, "lex_sim": 1.0}
{"question": "Which continent has the world's highest waterfall, largest river by volume, longest mountain range, driest place, largest rainforest, highest capital city, and highest commercially navigable lake?", "answer": ["Asia, Australia, Earth, South America, North America, North America, Western Europe"], "scores": [[0.05699516460299492, 0.030001068487763405, 0.06815358251333237, 0.9535072445869446, 0.007170480210334063, 0.9353911876678467, 0.03287830576300621, 0.8702710866928101, 0.9593137502670288, 0.36670106649398804, 0.9884374737739563, 0.9429803490638733, 0.04662928730249405, 0.799688994884491, 0.6280103921890259, 0.0012349835596978664, 0.7056357860565186]], "normalized_score": [0.17554031312465668], "tokens": [["\u0120Asia", ",", "\u0120Australia", ",", "\u0120Earth", ",", "\u0120South", "\u0120America", ",", "\u0120North", "\u0120America", ",", "\u0120North", "\u0120America", ",", "\u0120Western", "\u0120Europe"]], "entropy": [[3.6713297367095947, 0.6648950576782227, 3.1435508728027344, 0.3081734776496887, 3.5921835899353027, 0.3965643346309662, 3.272592544555664, 0.7895209789276123, 0.24552369117736816, 2.9269912242889404, 0.1025671511888504, 0.3151731491088867, 2.73970103263855, 1.0932796001434326, 0.98146653175354, 3.4108712673187256, 1.706775188446045]], "avg_entropy": [1.7271270252325956], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "South amerika", "score_of_biggest_cluster": 0.17554031312465668, "score_of_first_answer": 0.17554031312465668, "max_score_of_first_answer": 0.9884374737739563, "min_score_of_first_answer": 0.0012349835596978664, "avg_score": 0.17554031312465668, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4103604581519031, "std_of_first_answer": 0.4103604581519031, "avg_entropy_of_first_answer": 1.7271270252325956, "lex_sim": 1.0}
{"question": "Livorno, Pisa and Siena are three of the largest cities in which Italian region?", "answer": ["Liguria"], "scores": [[0.07656940817832947, 0.9475890398025513, 0.9497589468955994]], "normalized_score": [0.4099802076816559], "tokens": [["\u0120L", "ig", "uria"]], "entropy": [[2.1250550746917725, 0.41715773940086365, 0.2683800458908081]], "avg_entropy": [0.9368642866611481], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Toscana", "score_of_biggest_cluster": 0.4099802076816559, "score_of_first_answer": 0.4099802076816559, "max_score_of_first_answer": 0.9497589468955994, "min_score_of_first_answer": 0.07656940817832947, "avg_score": 0.4099802076816559, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41111499846918553, "std_of_first_answer": 0.41111499846918553, "avg_entropy_of_first_answer": 0.9368642866611481, "lex_sim": 1.0}
{"question": "\"What \"\"C\"\" is a name given to any straight line that joins two places on a curve?\"", "answer": ["\u3141"], "scores": [[0.002051440766081214, 0.00011409440776333213, 0.2480279952287674, 0.04802136495709419]], "normalized_score": [0.007266320753842592], "tokens": [["\u0120", "\u00e3", "\u0127", "\u0123"]], "entropy": [[5.9536333084106445, 5.067945957183838, 2.4586851596832275, 3.4636940956115723]], "avg_entropy": [4.235989630222321], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chord", "score_of_biggest_cluster": 0.007266320753842592, "score_of_first_answer": 0.007266320753842592, "max_score_of_first_answer": 0.2480279952287674, "min_score_of_first_answer": 0.00011409440776333213, "avg_score": 0.007266320753842592, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10197441882979083, "std_of_first_answer": 0.10197441882979083, "avg_entropy_of_first_answer": 4.235989630222321, "lex_sim": 0.0}
{"question": "Which type of bird is named after Thomas Bewick, an English engraver and ornithologist, who specialised in illustrations of birds?", "answer": ["Woodpecker"], "scores": [[0.15947064757347107, 0.5586379766464233, 0.9542790651321411]], "normalized_score": [0.43970581889152527], "tokens": [["\u0120Wood", "pe", "cker"]], "entropy": [[4.530577659606934, 1.680153727531433, 0.19140592217445374]], "avg_entropy": [2.1340457697709403], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Swan", "score_of_biggest_cluster": 0.43970581889152527, "score_of_first_answer": 0.43970581889152527, "max_score_of_first_answer": 0.9542790651321411, "min_score_of_first_answer": 0.15947064757347107, "avg_score": 0.43970581889152527, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3244802421856145, "std_of_first_answer": 0.3244802421856145, "avg_entropy_of_first_answer": 2.1340457697709403, "lex_sim": 1.0}
{"question": "\"In the 1956 film \"\"The Ten Commandments\"\", who played Moses?\"", "answer": ["Moses"], "scores": [[0.0019784439355134964]], "normalized_score": [0.0019784437026828527], "tokens": [["\u0120Moses"]], "entropy": [[3.50015926361084]], "avg_entropy": [3.50015926361084], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Holly Heston Rochell", "score_of_biggest_cluster": 0.0019784437026828527, "score_of_first_answer": 0.0019784437026828527, "max_score_of_first_answer": 0.0019784439355134964, "min_score_of_first_answer": 0.0019784439355134964, "avg_score": 0.0019784437026828527, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.50015926361084, "lex_sim": 1.0}
{"question": "Born in 1861, which famous opera singer's real name was Helen Mitchell?", "answer": ["Lilli B\u00e9al"], "scores": [[0.055575646460056305, 0.15123672783374786, 0.001025665202178061, 0.023619426414370537, 0.010629285126924515]], "normalized_score": [0.01849542371928692], "tokens": [["\u0120L", "illi", "\u0120B", "\u00c3\u00a9", "al"]], "entropy": [[5.2786455154418945, 2.5879693031311035, 0.7815032005310059, 3.3777871131896973, 3.7005529403686523]], "avg_entropy": [3.1452916145324705], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Helen Porter Mitchell", "score_of_biggest_cluster": 0.01849542371928692, "score_of_first_answer": 0.01849542371928692, "max_score_of_first_answer": 0.15123672783374786, "min_score_of_first_answer": 0.001025665202178061, "avg_score": 0.01849542371928692, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.054611193569296106, "std_of_first_answer": 0.054611193569296106, "avg_entropy_of_first_answer": 3.1452916145324705, "lex_sim": 1.0}
{"question": "Barra island, Outer Hebrides, has at 2012 the world's only official beach-based what?", "answer": ["Wiccan"], "scores": [[0.0025585449766367674, 0.021765293553471565, 0.4865794777870178]], "normalized_score": [0.030035654082894325], "tokens": [["\u0120W", "ic", "can"]], "entropy": [[7.937355995178223, 4.619164943695068, 2.2803359031677246]], "avg_entropy": [4.945618947347005], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Airline destinations", "score_of_biggest_cluster": 0.030035654082894325, "score_of_first_answer": 0.030035654082894325, "max_score_of_first_answer": 0.4865794777870178, "min_score_of_first_answer": 0.0025585449766367674, "avg_score": 0.030035654082894325, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22377999822599134, "std_of_first_answer": 0.22377999822599134, "avg_entropy_of_first_answer": 4.945618947347005, "lex_sim": 1.0}
{"question": "Which large American bird (Gymnogyps) has been reintroduced to the wild after breeding in captivity in 1991?", "answer": ["American Robin"], "scores": [[0.018544794991612434, 0.029905691742897034]], "normalized_score": [0.023549839854240417], "tokens": [["\u0120American", "\u0120Robin"]], "entropy": [[4.855430603027344, 4.315885543823242]], "avg_entropy": [4.585658073425293], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "(California) Condor", "score_of_biggest_cluster": 0.023549839854240417, "score_of_first_answer": 0.023549839854240417, "max_score_of_first_answer": 0.029905691742897034, "min_score_of_first_answer": 0.018544794991612434, "avg_score": 0.023549839854240417, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0056804483756423, "std_of_first_answer": 0.0056804483756423, "avg_entropy_of_first_answer": 4.585658073425293, "lex_sim": 1.0}
{"question": "How many triangles are on a backgammon board?", "answer": ["8"], "scores": [[0.0047814613208174706]], "normalized_score": [0.004781461786478758], "tokens": [["\u01208"]], "entropy": [[4.693367004394531]], "avg_entropy": [4.693367004394531], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "24", "score_of_biggest_cluster": 0.004781461786478758, "score_of_first_answer": 0.004781461786478758, "max_score_of_first_answer": 0.0047814613208174706, "min_score_of_first_answer": 0.0047814613208174706, "avg_score": 0.004781461786478758, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.693367004394531, "lex_sim": 1.0}
{"question": "Mohenjo-daro and Harappa in present-day Pakistan were the most important city settlements of a Bronze Age civilization that is named after what major geographic feature?", "answer": ["Indus valley"], "scores": [[0.6224105954170227, 0.9990783929824829, 0.04145324230194092]], "normalized_score": [0.29540079832077026], "tokens": [["\u0120Ind", "us", "\u0120valley"]], "entropy": [[2.2138805389404297, 0.01062360405921936, 1.8182848691940308]], "avg_entropy": [1.3475963373978932], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lion River", "score_of_biggest_cluster": 0.29540079832077026, "score_of_first_answer": 0.29540079832077026, "max_score_of_first_answer": 0.9990783929824829, "min_score_of_first_answer": 0.04145324230194092, "avg_score": 0.29540079832077026, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3939029780538501, "std_of_first_answer": 0.3939029780538501, "avg_entropy_of_first_answer": 1.3475963373978932, "lex_sim": 1.0}
{"question": "What was the name of two tyrants who ruled Syracuse in the 2nd century BC?", "answer": ["Dionysius the Elder"], "scores": [[0.3433390259742737, 0.7970770001411438, 0.8879930377006531, 0.24905933439731598, 0.7598304748535156]], "normalized_score": [0.5401700139045715], "tokens": [["\u0120Dion", "ys", "ius", "\u0120the", "\u0120Elder"]], "entropy": [[3.6268701553344727, 0.7734485864639282, 0.4577102065086365, 1.943476676940918, 1.2929272651672363]], "avg_entropy": [1.6188865780830384], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dionysios", "score_of_biggest_cluster": 0.5401700139045715, "score_of_first_answer": 0.5401700139045715, "max_score_of_first_answer": 0.8879930377006531, "min_score_of_first_answer": 0.24905933439731598, "avg_score": 0.5401700139045715, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25926090319045786, "std_of_first_answer": 0.25926090319045786, "avg_entropy_of_first_answer": 1.6188865780830384, "lex_sim": 1.0}
{"question": "Which city hosted the 1980 Summer Olympics, which were boycotted by over 60 nations?", "answer": ["Moscow"], "scores": [[0.6058526039123535]], "normalized_score": [0.6058526039123535], "tokens": [["\u0120Moscow"]], "entropy": [[2.2679738998413086]], "avg_entropy": [2.2679738998413086], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Moskou", "score_of_biggest_cluster": 0.6058526039123535, "score_of_first_answer": 0.6058526039123535, "max_score_of_first_answer": 0.6058526039123535, "min_score_of_first_answer": 0.6058526039123535, "avg_score": 0.6058526039123535, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.2679738998413086, "lex_sim": 1.0}
{"question": "In what year was the Magna Carta signed", "answer": ["1215"], "scores": [[0.823344886302948, 0.7688871622085571]], "normalized_score": [0.7956502437591553], "tokens": [["\u012012", "15"]], "entropy": [[1.3027982711791992, 1.1858201026916504]], "avg_entropy": [1.2443091869354248], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, two hundred and fifteen", "score_of_biggest_cluster": 0.7956502437591553, "score_of_first_answer": 0.7956502437591553, "max_score_of_first_answer": 0.823344886302948, "min_score_of_first_answer": 0.7688871622085571, "avg_score": 0.7956502437591553, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.027228862047195435, "std_of_first_answer": 0.027228862047195435, "avg_entropy_of_first_answer": 1.2443091869354248, "lex_sim": 1.0}
{"question": "Covering an area in excess of 30,000 square miles, in which country can you find the 'Dasht  e Kavir Desert' the largest to lie wholly in it's own country?", "answer": ["Iran"], "scores": [[0.6296843886375427]], "normalized_score": [0.6296843886375427], "tokens": [["\u0120Iran"]], "entropy": [[2.1917757987976074]], "avg_entropy": [2.1917757987976074], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ir\u0101n", "score_of_biggest_cluster": 0.6296843886375427, "score_of_first_answer": 0.6296843886375427, "max_score_of_first_answer": 0.6296843886375427, "min_score_of_first_answer": 0.6296843886375427, "avg_score": 0.6296843886375427, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.1917757987976074, "lex_sim": 1.0}
{"question": "'Paralympics' is a combination of Olympics and what other word?", "answer": ["Par-Al-li-em-pics-ics"], "scores": [[0.095869280397892, 0.023293564096093178, 0.025905577465891838, 0.21266202628612518, 0.01065121404826641, 0.919835090637207, 0.04819708317518234, 0.7849075198173523, 0.1979731172323227, 0.921172559261322, 0.002327338792383671, 0.2909352779388428]], "normalized_score": [0.09532627463340759], "tokens": [["\u0120Par", "-", "Al", "-", "li", "-", "em", "-", "p", "ics", "-", "ics"]], "entropy": [[3.2777764797210693, 3.7541794776916504, 3.4119300842285156, 3.3701648712158203, 4.740754127502441, 0.547380805015564, 3.840395450592041, 1.1333688497543335, 2.1423251628875732, 0.5088106989860535, 0.4167039394378662, 4.121394157409668]], "avg_entropy": [2.60543200870355], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paralel", "score_of_biggest_cluster": 0.09532627463340759, "score_of_first_answer": 0.09532627463340759, "max_score_of_first_answer": 0.921172559261322, "min_score_of_first_answer": 0.002327338792383671, "avg_score": 0.09532627463340759, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3478553079633599, "std_of_first_answer": 0.3478553079633599, "avg_entropy_of_first_answer": 2.60543200870355, "lex_sim": 1.0}
{"question": "\"Which country did Tsar Nicholas I of Russia call \"\"the sick man of Europe\"\"?\"", "answer": ["Austria"], "scores": [[0.09667526185512543]], "normalized_score": [0.09667526930570602], "tokens": [["\u0120Austria"]], "entropy": [[3.413727283477783]], "avg_entropy": [3.413727283477783], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Turkey", "score_of_biggest_cluster": 0.09667526930570602, "score_of_first_answer": 0.09667526930570602, "max_score_of_first_answer": 0.09667526185512543, "min_score_of_first_answer": 0.09667526185512543, "avg_score": 0.09667526930570602, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.413727283477783, "lex_sim": 1.0}
{"question": "What product invented by Frederick Walton in 1864 became so widely used and 'stepped on' that the name became generic just 14 years after its invention?", "answer": ["The telephone"], "scores": [[0.19698140025138855, 0.035121120512485504]], "normalized_score": [0.08317576348781586], "tokens": [["\u0120The", "\u0120telephone"]], "entropy": [[5.880217552185059, 6.698986053466797]], "avg_entropy": [6.289601802825928], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lynolium", "score_of_biggest_cluster": 0.08317576348781586, "score_of_first_answer": 0.08317576348781586, "max_score_of_first_answer": 0.19698140025138855, "min_score_of_first_answer": 0.035121120512485504, "avg_score": 0.08317576348781586, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08093013986945152, "std_of_first_answer": 0.08093013986945152, "avg_entropy_of_first_answer": 6.289601802825928, "lex_sim": 1.0}
{"question": "Which musical features the songs If I loved you and You\u2019ll never walk alone?", "answer": ["The Musical (album)"], "scores": [[0.09076308459043503, 0.007211332209408283, 0.023570429533720016, 0.006119670812040567, 0.922826886177063]], "normalized_score": [0.03872828185558319], "tokens": [["\u0120The", "\u0120Musical", "\u0120(", "album", ")"]], "entropy": [[6.23079776763916, 5.839756488800049, 2.7530055046081543, 5.737584114074707, 0.5454792976379395]], "avg_entropy": [4.221324634552002], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Carousels", "score_of_biggest_cluster": 0.03872828185558319, "score_of_first_answer": 0.03872828185558319, "max_score_of_first_answer": 0.922826886177063, "min_score_of_first_answer": 0.006119670812040567, "avg_score": 0.03872828185558319, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35771105113377344, "std_of_first_answer": 0.35771105113377344, "avg_entropy_of_first_answer": 4.221324634552002, "lex_sim": 1.0}
{"question": "From which 1968 film did English band Duran Duran take their name?", "answer": ["A Hard Day's Night"], "scores": [[0.18385165929794312, 0.0497804619371891, 0.8823155760765076, 0.9630550146102905, 0.9924181699752808]], "normalized_score": [0.3780064582824707], "tokens": [["\u0120A", "\u0120Hard", "\u0120Day", "'s", "\u0120Night"]], "entropy": [[4.6274213790893555, 3.1357486248016357, 0.6877872943878174, 0.3010949492454529, 0.06729304790496826]], "avg_entropy": [1.763869059085846], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Barbarella (disambiguation)", "score_of_biggest_cluster": 0.3780064582824707, "score_of_first_answer": 0.3780064582824707, "max_score_of_first_answer": 0.9924181699752808, "min_score_of_first_answer": 0.0497804619371891, "avg_score": 0.3780064582824707, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40997654599178474, "std_of_first_answer": 0.40997654599178474, "avg_entropy_of_first_answer": 1.763869059085846, "lex_sim": 1.0}
{"question": "Who wrote the book 'Friends, Voters, Countrymen' about his election as MP for Henley-on-Thames in 2001?", "answer": ["The Queen"], "scores": [[0.0075161815620958805, 0.0028907416854053736]], "normalized_score": [0.004661261104047298], "tokens": [["\u0120The", "\u0120Queen"]], "entropy": [[5.085503578186035, 5.867232322692871]], "avg_entropy": [5.476367950439453], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Boris Jonson", "score_of_biggest_cluster": 0.004661261104047298, "score_of_first_answer": 0.004661261104047298, "max_score_of_first_answer": 0.0075161815620958805, "min_score_of_first_answer": 0.0028907416854053736, "avg_score": 0.004661261104047298, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0023127199383452535, "std_of_first_answer": 0.0023127199383452535, "avg_entropy_of_first_answer": 5.476367950439453, "lex_sim": 1.0}
{"question": "Michael Morpurgo, author of the children's book War Horse, on which the 2012 Spielberg film (of the same name) is based, held what UK position from 2003-5?", "answer": ["Secretary of State for Education"], "scores": [[0.05039051175117493, 0.9341158866882324, 0.9172790050506592, 0.9599561095237732, 0.4657060205936432]], "normalized_score": [0.45407000184059143], "tokens": [["\u0120Secretary", "\u0120of", "\u0120State", "\u0120for", "\u0120Education"]], "entropy": [[5.354432582855225, 0.39389535784721375, 0.4845186471939087, 0.24532189965248108, 2.1400249004364014]], "avg_entropy": [1.7236386775970458], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Children's Laureate", "score_of_biggest_cluster": 0.45407000184059143, "score_of_first_answer": 0.45407000184059143, "max_score_of_first_answer": 0.9599561095237732, "min_score_of_first_answer": 0.05039051175117493, "avg_score": 0.45407000184059143, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3579186719330793, "std_of_first_answer": 0.3579186719330793, "avg_entropy_of_first_answer": 1.7236386775970458, "lex_sim": 1.0}
{"question": "\"Which Shakespeare play features the line: \"\" The play's the thing / Wherein I'll catch the conscience of the king. \"\" ?\"", "answer": ["King Lear"], "scores": [[0.09588827192783356, 0.7053618431091309]], "normalized_score": [0.2600691020488739], "tokens": [["\u0120King", "\u0120Lear"]], "entropy": [[2.970038414001465, 1.098753809928894]], "avg_entropy": [2.0343961119651794], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Hamlet", "score_of_biggest_cluster": 0.2600691020488739, "score_of_first_answer": 0.2600691020488739, "max_score_of_first_answer": 0.7053618431091309, "min_score_of_first_answer": 0.09588827192783356, "avg_score": 0.2600691020488739, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30473678559064865, "std_of_first_answer": 0.30473678559064865, "avg_entropy_of_first_answer": 2.0343961119651794, "lex_sim": 1.0}
{"question": "Mordant is the general term for a chemical which allows what to work properly?", "answer": ["Accelerator"], "scores": [[0.00025077330064959824, 0.021595634520053864]], "normalized_score": [0.0023271457757800817], "tokens": [["\u0120Acceler", "ator"]], "entropy": [[6.455362319946289, 0.8917551040649414]], "avg_entropy": [3.6735587120056152], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "DYE", "score_of_biggest_cluster": 0.0023271457757800817, "score_of_first_answer": 0.0023271457757800817, "max_score_of_first_answer": 0.021595634520053864, "min_score_of_first_answer": 0.00025077330064959824, "avg_score": 0.0023271457757800817, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.010672430609702133, "std_of_first_answer": 0.010672430609702133, "avg_entropy_of_first_answer": 3.6735587120056152, "lex_sim": 1.0}
{"question": "The moons of which planet are named after Shakespearian characters?", "answer": ["Moon - Diana (Shakespeare)"], "scores": [[0.002816860331222415, 0.014977136626839638, 0.010085190646350384, 0.031540460884571075, 0.1820077747106552, 0.6665665507316589, 0.3646945059299469]], "normalized_score": [0.04807775467634201], "tokens": [["\u0120Moon", "\u0120-", "\u0120Diana", "\u0120(", "Sh", "akespeare", ")"]], "entropy": [[4.344423770904541, 4.283855438232422, 6.246824264526367, 1.9070687294006348, 5.057037353515625, 1.3694015741348267, 2.1193671226501465]], "avg_entropy": [3.618282607623509], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Magnetosphere of Uranus", "score_of_biggest_cluster": 0.04807775467634201, "score_of_first_answer": 0.04807775467634201, "max_score_of_first_answer": 0.6665665507316589, "min_score_of_first_answer": 0.002816860331222415, "avg_score": 0.04807775467634201, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23310873855527803, "std_of_first_answer": 0.23310873855527803, "avg_entropy_of_first_answer": 3.618282607623509, "lex_sim": 1.0}
{"question": "Which Aztec ruler was deposed by Cortez?", "answer": ["Cuauht\u00e9moc"], "scores": [[0.03378425911068916, 0.8388185501098633, 0.9799003005027771, 0.0702449157834053, 0.9824640154838562, 0.9968752861022949]], "normalized_score": [0.35225412249565125], "tokens": [["\u0120Cu", "au", "ht", "\u00c3\u00a9", "m", "oc"]], "entropy": [[1.603740930557251, 1.0126172304153442, 0.14296722412109375, 1.8757201433181763, 0.15855799615383148, 0.03189697116613388]], "avg_entropy": [0.8042500826219717], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Montesuma", "score_of_biggest_cluster": 0.35225412249565125, "score_of_first_answer": 0.35225412249565125, "max_score_of_first_answer": 0.9968752861022949, "min_score_of_first_answer": 0.03378425911068916, "avg_score": 0.35225412249565125, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4264540830454148, "std_of_first_answer": 0.4264540830454148, "avg_entropy_of_first_answer": 0.8042500826219717, "lex_sim": 1.0}
{"question": "Which wedding anniversary is celebrated with paper", "answer": ["Marriage anniversary?"], "scores": [[0.0041197980754077435, 0.054439201951026917, 0.0038894612807780504]], "normalized_score": [0.009554891847074032], "tokens": [["\u0120Marriage", "\u0120anniversary", "?"]], "entropy": [[5.71863317489624, 1.6875394582748413, 0.720552921295166]], "avg_entropy": [2.708908518155416], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "First (track)", "score_of_biggest_cluster": 0.009554891847074032, "score_of_first_answer": 0.009554891847074032, "max_score_of_first_answer": 0.054439201951026917, "min_score_of_first_answer": 0.0038894612807780504, "avg_score": 0.009554891847074032, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02377527133450975, "std_of_first_answer": 0.02377527133450975, "avg_entropy_of_first_answer": 2.708908518155416, "lex_sim": 1.0}
{"question": "Which fire-breathing creature of Greek mythology has the head of a lion, body of a goat and the rear of a dragon?", "answer": ["Gorgon"], "scores": [[0.02365013211965561, 0.400706022977829, 0.7887021899223328]], "normalized_score": [0.19551981985569], "tokens": [["\u0120G", "org", "on"]], "entropy": [[4.60418701171875, 0.9914881587028503, 1.0818569660186768]], "avg_entropy": [2.2258440454800925], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chimaeras", "score_of_biggest_cluster": 0.19551981985569, "score_of_first_answer": 0.19551981985569, "max_score_of_first_answer": 0.7887021899223328, "min_score_of_first_answer": 0.02365013211965561, "avg_score": 0.19551981985569, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3123418393616891, "std_of_first_answer": 0.3123418393616891, "avg_entropy_of_first_answer": 2.2258440454800925, "lex_sim": 1.0}
{"question": "With what area of manufacturing are the names 'Audemars Piguet and Cie','Zenith' and 'Ulysse Nardin' associated?", "answer": ["Watches"], "scores": [[0.153790682554245, 0.9961192607879639]], "normalized_score": [0.39139989018440247], "tokens": [["\u0120Wat", "ches"]], "entropy": [[3.954868793487549, 0.03631807863712311]], "avg_entropy": [1.995593436062336], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Watch-maker", "score_of_biggest_cluster": 0.39139989018440247, "score_of_first_answer": 0.39139989018440247, "max_score_of_first_answer": 0.9961192607879639, "min_score_of_first_answer": 0.153790682554245, "avg_score": 0.39139989018440247, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.42116428911685944, "std_of_first_answer": 0.42116428911685944, "avg_entropy_of_first_answer": 1.995593436062336, "lex_sim": 1.0}
{"question": "August 2, 1990 saw Iraq invading what country, sparking a series of events that culminated in the 1st Gulf War?", "answer": ["Iraq"], "scores": [[0.07842638343572617]], "normalized_score": [0.07842638343572617], "tokens": [["\u0120Iraq"]], "entropy": [[2.9996087551116943]], "avg_entropy": [2.9996087551116943], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ISO 3166-1:KW", "score_of_biggest_cluster": 0.07842638343572617, "score_of_first_answer": 0.07842638343572617, "max_score_of_first_answer": 0.07842638343572617, "min_score_of_first_answer": 0.07842638343572617, "avg_score": 0.07842638343572617, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.9996087551116943, "lex_sim": 1.0}
{"question": "Who killed Jean Paul Marat?", "answer": ["Jean Marat."], "scores": [[0.010928229428827763, 0.0017649559304118156, 0.9814984202384949, 0.010910558514297009]], "normalized_score": [0.021318422630429268], "tokens": [["\u0120Jean", "\u0120Mar", "at", "."]], "entropy": [[5.341796398162842, 2.0971269607543945, 0.17257092893123627, 0.8849688768386841]], "avg_entropy": [2.124115791171789], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Corday, Charlotte", "score_of_biggest_cluster": 0.021318422630429268, "score_of_first_answer": 0.021318422630429268, "max_score_of_first_answer": 0.9814984202384949, "min_score_of_first_answer": 0.0017649559304118156, "avg_score": 0.021318422630429268, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42161094044449565, "std_of_first_answer": 0.42161094044449565, "avg_entropy_of_first_answer": 2.124115791171789, "lex_sim": 1.0}
{"question": "In 1483, who was appointed the first grand inquisitor of the Spanish Inquisition?", "answer": ["The Spanish Grand Inquisitor, Rodrigo de Soto, in 1550"], "scores": [[0.0022657006047666073, 0.008282407186925411, 0.019208639860153198, 0.8189884424209595, 0.09308915585279465, 0.02319285087287426, 0.6640285849571228, 0.013686524704098701, 0.5810372233390808, 0.09253805130720139, 0.047199904918670654, 0.13409754633903503, 0.006663359701633453]], "normalized_score": [0.050282735377550125], "tokens": [["\u0120The", "\u0120Spanish", "\u0120Grand", "\u0120Inquisitor", ",", "\u0120Rodrigo", "\u0120de", "\u0120S", "oto", ",", "\u0120in", "\u012015", "50"]], "entropy": [[4.940042972564697, 4.685288429260254, 3.5566675662994385, 1.1864566802978516, 3.0093493461608887, 4.110291957855225, 2.033719301223755, 3.4237241744995117, 2.4214792251586914, 0.9862221479415894, 4.667347431182861, 1.9837253093719482, 4.2363739013671875]], "avg_entropy": [3.1723606494756846], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Torquemada (disambiguation)", "score_of_biggest_cluster": 0.050282735377550125, "score_of_first_answer": 0.050282735377550125, "max_score_of_first_answer": 0.8189884424209595, "min_score_of_first_answer": 0.0022657006047666073, "avg_score": 0.050282735377550125, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27811388219967625, "std_of_first_answer": 0.27811388219967625, "avg_entropy_of_first_answer": 3.1723606494756846, "lex_sim": 1.0}
{"question": "Ur was a major city of which ancient civilization?", "answer": ["Ur"], "scores": [[0.0424799919128418]], "normalized_score": [0.0424799881875515], "tokens": [["\u0120Ur"]], "entropy": [[2.934333086013794]], "avg_entropy": [2.934333086013794], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "SUMER", "score_of_biggest_cluster": 0.0424799881875515, "score_of_first_answer": 0.0424799881875515, "max_score_of_first_answer": 0.0424799919128418, "min_score_of_first_answer": 0.0424799919128418, "avg_score": 0.0424799881875515, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.934333086013794, "lex_sim": 1.0}
{"question": "What is added to white wine to make a Kir?", "answer": ["A pinch of sweetness"], "scores": [[0.02780803292989731, 0.033473290503025055, 0.9548206925392151, 0.0006081579485908151]], "normalized_score": [0.02711450308561325], "tokens": [["\u0120A", "\u0120pinch", "\u0120of", "\u0120sweetness"]], "entropy": [[5.903018474578857, 5.850841522216797, 0.31029999256134033, 3.881901741027832]], "avg_entropy": [3.9865154325962067], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cassis", "score_of_biggest_cluster": 0.02711450308561325, "score_of_first_answer": 0.02711450308561325, "max_score_of_first_answer": 0.9548206925392151, "min_score_of_first_answer": 0.0006081579485908151, "avg_score": 0.02711450308561325, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4047072331300762, "std_of_first_answer": 0.4047072331300762, "avg_entropy_of_first_answer": 3.9865154325962067, "lex_sim": 1.0}
{"question": "\"A 1993 film starring Tom Hanks was \"\"Sleepless in ... \"\" where?\"", "answer": ["Sleepless in Seattle"], "scores": [[0.01792508363723755, 0.8237628936767578, 0.9986214637756348, 0.9082736372947693, 0.9833582043647766]], "normalized_score": [0.4206477701663971], "tokens": [["\u0120S", "lee", "pless", "\u0120in", "\u0120Seattle"]], "entropy": [[5.533078193664551, 1.4176180362701416, 0.016901051625609398, 0.526321291923523, 0.1684519648551941]], "avg_entropy": [1.5324741076678037], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Seattle, Washington.", "score_of_biggest_cluster": 0.4206477701663971, "score_of_first_answer": 0.4206477701663971, "max_score_of_first_answer": 0.9986214637756348, "min_score_of_first_answer": 0.01792508363723755, "avg_score": 0.4206477701663971, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3694938841859696, "std_of_first_answer": 0.3694938841859696, "avg_entropy_of_first_answer": 1.5324741076678037, "lex_sim": 1.0}
{"question": "Rincewind and 'The Luggage' appear in which series of books?", "answer": ["The Fifth Elephant"], "scores": [[0.470471054315567, 0.01352228969335556, 0.5992804765701294]], "normalized_score": [0.1562204211950302], "tokens": [["\u0120The", "\u0120Fifth", "\u0120Elephant"]], "entropy": [[3.432394027709961, 4.465976238250732, 2.6747395992279053]], "avg_entropy": [3.524369955062866], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Discworld books", "score_of_biggest_cluster": 0.1562204211950302, "score_of_first_answer": 0.1562204211950302, "max_score_of_first_answer": 0.5992804765701294, "min_score_of_first_answer": 0.01352228969335556, "avg_score": 0.1562204211950302, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25133127634977986, "std_of_first_answer": 0.25133127634977986, "avg_entropy_of_first_answer": 3.524369955062866, "lex_sim": 1.0}
{"question": "In which street would you find 'Smiffy', 'Wilfred', 'Danny' and 'Plug'?", "answer": ["Rope Alley"], "scores": [[0.0017109408508986235, 0.06657549738883972, 0.11506131291389465]], "normalized_score": [0.023577239364385605], "tokens": [["\u0120R", "ope", "\u0120Alley"]], "entropy": [[7.000085830688477, 5.290380001068115, 3.2652428150177]], "avg_entropy": [5.185236215591431], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "When the Bell Rings", "score_of_biggest_cluster": 0.023577239364385605, "score_of_first_answer": 0.023577239364385605, "max_score_of_first_answer": 0.11506131291389465, "min_score_of_first_answer": 0.0017109408508986235, "avg_score": 0.023577239364385605, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04643584804008052, "std_of_first_answer": 0.04643584804008052, "avg_entropy_of_first_answer": 5.185236215591431, "lex_sim": 1.0}
{"question": "La Brabanconne is the national anthem of which country?", "answer": ["La Vache Qui Rit"], "scores": [[0.005437824875116348, 0.009390419349074364, 0.13280321657657623, 0.5888015627861023, 0.9116501212120056, 0.994633674621582, 0.9982948899269104]], "normalized_score": [0.16694633662700653], "tokens": [["\u0120La", "\u0120V", "ache", "\u0120Qu", "i", "\u0120R", "it"]], "entropy": [[3.7138962745666504, 4.871490001678467, 4.384025573730469, 1.5456953048706055, 0.5572097301483154, 0.05371514707803726, 0.018457356840372086]], "avg_entropy": [2.163498484130417], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Belguim", "score_of_biggest_cluster": 0.16694633662700653, "score_of_first_answer": 0.16694633662700653, "max_score_of_first_answer": 0.9982948899269104, "min_score_of_first_answer": 0.005437824875116348, "avg_score": 0.16694633662700653, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4288829640714724, "std_of_first_answer": 0.4288829640714724, "avg_entropy_of_first_answer": 2.163498484130417, "lex_sim": 1.0}
{"question": "How many male strippers perform on stage together in the 1997 film \u2018The Full Monty\u2019?", "answer": ["Seven"], "scores": [[0.023340707644820213]], "normalized_score": [0.023340705782175064], "tokens": [["\u0120Seven"]], "entropy": [[4.825142860412598]], "avg_entropy": [4.825142860412598], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Six", "score_of_biggest_cluster": 0.023340705782175064, "score_of_first_answer": 0.023340705782175064, "max_score_of_first_answer": 0.023340707644820213, "min_score_of_first_answer": 0.023340707644820213, "avg_score": 0.023340705782175064, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.825142860412598, "lex_sim": 1.0}
{"question": "What is said to be the UK's loudest bird?", "answer": ["Great Scott"], "scores": [[0.06482458114624023, 0.00156634661834687]], "normalized_score": [0.010076594538986683], "tokens": [["\u0120Great", "\u0120Scott"]], "entropy": [[5.121244430541992, 3.7268452644348145]], "avg_entropy": [4.424044847488403], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bitterns", "score_of_biggest_cluster": 0.010076594538986683, "score_of_first_answer": 0.010076594538986683, "max_score_of_first_answer": 0.06482458114624023, "min_score_of_first_answer": 0.00156634661834687, "avg_score": 0.010076594538986683, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.03162911726394668, "std_of_first_answer": 0.03162911726394668, "avg_entropy_of_first_answer": 4.424044847488403, "lex_sim": 1.0}
{"question": "What is the traditional technical term for a road/paving granite block, alternatively called a Belgian block, and incorrectly a cobble or cobblestone?", "answer": ["Rubble"], "scores": [[0.0045854076743125916, 0.9260725975036621]], "normalized_score": [0.06516455858945847], "tokens": [["\u0120Rub", "ble"]], "entropy": [[6.387303352355957, 0.3931853473186493]], "avg_entropy": [3.390244349837303], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sett", "score_of_biggest_cluster": 0.06516455858945847, "score_of_first_answer": 0.06516455858945847, "max_score_of_first_answer": 0.9260725975036621, "min_score_of_first_answer": 0.0045854076743125916, "avg_score": 0.06516455858945847, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46074359491467476, "std_of_first_answer": 0.46074359491467476, "avg_entropy_of_first_answer": 3.390244349837303, "lex_sim": 1.0}
{"question": "If you are standing at Tanjung Piai, the southernmost point of mainland Asia, you can see the skyline of what country across a strait?", "answer": ["Bangladesh"], "scores": [[0.0027591309044510126]], "normalized_score": [0.0027591311372816563], "tokens": [["\u0120Bangladesh"]], "entropy": [[3.8327698707580566]], "avg_entropy": [3.8327698707580566], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Singapore City (disambiguation)", "score_of_biggest_cluster": 0.0027591311372816563, "score_of_first_answer": 0.0027591311372816563, "max_score_of_first_answer": 0.0027591309044510126, "min_score_of_first_answer": 0.0027591309044510126, "avg_score": 0.0027591311372816563, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.8327698707580566, "lex_sim": 1.0}
{"question": "What company is the world's largest auction house?", "answer": ["Sothebys"], "scores": [[0.7539975643157959, 0.9901590943336487, 0.0842457264661789]], "normalized_score": [0.39768657088279724], "tokens": [["\u0120S", "othe", "bys"]], "entropy": [[1.3730617761611938, 0.10623081773519516, 0.37163132429122925]], "avg_entropy": [0.6169746393958727], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Christy's", "score_of_biggest_cluster": 0.39768657088279724, "score_of_first_answer": 0.39768657088279724, "max_score_of_first_answer": 0.9901590943336487, "min_score_of_first_answer": 0.0842457264661789, "avg_score": 0.39768657088279724, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3836982064424381, "std_of_first_answer": 0.3836982064424381, "avg_entropy_of_first_answer": 0.6169746393958727, "lex_sim": 1.0}
{"question": "The greatest confirmed age for any living creature is for a species of quahog (pronounced ko-hog) at over 500 years old. What is the common name for this species of creature?", "answer": ["The quahog \u32e0"], "scores": [[0.01597655937075615, 0.004102867096662521, 0.9281497597694397, 0.975943922996521, 0.01773841679096222, 1.4112309145275503e-05, 0.018987027928233147, 0.029296312481164932]], "normalized_score": [0.017364993691444397], "tokens": [["\u0120The", "\u0120qu", "ah", "og", "\u0120", "\u00e3", "\u012d", "\u0142"]], "entropy": [[6.378324508666992, 6.429092884063721, 0.625376284122467, 0.21351081132888794, 2.566563129425049, 1.8810242414474487, 2.8890130519866943, 3.8078012466430664]], "avg_entropy": [3.0988382697105408], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Clams", "score_of_biggest_cluster": 0.017364993691444397, "score_of_first_answer": 0.017364993691444397, "max_score_of_first_answer": 0.975943922996521, "min_score_of_first_answer": 1.4112309145275503e-05, "avg_score": 0.017364993691444397, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4062969298681072, "std_of_first_answer": 0.4062969298681072, "avg_entropy_of_first_answer": 3.0988382697105408, "lex_sim": 1.0}
{"question": "Which type of roof is pitched at two different angles, the lower part being steeper than the upper?", "answer": ["\u33ba"], "scores": [[0.003337071742862463, 1.798910670913756e-05, 0.020740434527397156, 0.008204863406717777]], "normalized_score": [0.0017877882346510887], "tokens": [["\u0120", "\u00e3", "\u0130", "\u00ba"]], "entropy": [[6.129919528961182, 4.838571548461914, 2.668144702911377, 3.5770719051361084]], "avg_entropy": [4.303426921367645], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mansard Roof", "score_of_biggest_cluster": 0.0017877882346510887, "score_of_first_answer": 0.0017877882346510887, "max_score_of_first_answer": 0.020740434527397156, "min_score_of_first_answer": 1.798910670913756e-05, "avg_score": 0.0017877882346510887, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.007870728761436213, "std_of_first_answer": 0.007870728761436213, "avg_entropy_of_first_answer": 4.303426921367645, "lex_sim": 0.0}
{"question": "Also called The Roodee, which, officially, is Britain\u2019s oldest and shortest racecourse?", "answer": ["Cheltenham"], "scores": [[0.06711110472679138, 0.9439883232116699, 0.9990608096122742]], "normalized_score": [0.3985207974910736], "tokens": [["\u0120Chel", "ten", "ham"]], "entropy": [[5.475724220275879, 0.3165305554866791, 0.011212913319468498]], "avg_entropy": [1.9344892296940088], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "County Borough of Chester", "score_of_biggest_cluster": 0.3985207974910736, "score_of_first_answer": 0.3985207974910736, "max_score_of_first_answer": 0.9990608096122742, "min_score_of_first_answer": 0.06711110472679138, "avg_score": 0.3985207974910736, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.426937009079047, "std_of_first_answer": 0.426937009079047, "avg_entropy_of_first_answer": 1.9344892296940088, "lex_sim": 1.0}
{"question": "The bones of King Richard III have been discovered under a car park in which English city?", "answer": ["Leicester"], "scores": [[0.9463391900062561]], "normalized_score": [0.9463391900062561], "tokens": [["\u0120Leicester"]], "entropy": [[0.369823694229126]], "avg_entropy": [0.369823694229126], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "County Borough of Leicester", "score_of_biggest_cluster": 0.9463391900062561, "score_of_first_answer": 0.9463391900062561, "max_score_of_first_answer": 0.9463391900062561, "min_score_of_first_answer": 0.9463391900062561, "avg_score": 0.9463391900062561, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.369823694229126, "lex_sim": 1.0}
