{"question": "Which US city had the first subway system, being opened in 1895?", "answer": ["Boston"], "scores": [[0.7145098447799683]], "normalized_score": [0.7145098447799683], "tokens": [["\u2581Boston"]], "entropy": [[1.3199143409729004]], "avg_entropy": [1.3199143409729004], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "City of Boston", "score_of_biggest_cluster": 0.7145098447799683, "score_of_first_answer": 0.7145098447799683, "max_score_of_first_answer": 0.7145098447799683, "min_score_of_first_answer": 0.7145098447799683, "avg_score": 0.7145098447799683, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.3199143409729004, "lex_sim": 1.0}
{"question": "To the nearest two, how many tennis Grand Slam titles did Jimmy Connors win?", "answer": ["8"], "scores": [[0.12733709812164307, 0.3389028012752533]], "normalized_score": [0.20773756504058838], "tokens": [["\u2581", "8"]], "entropy": [[3.510300636291504, 1.990177869796753]], "avg_entropy": [2.7502392530441284], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "10", "score_of_biggest_cluster": 0.20773756504058838, "score_of_first_answer": 0.20773756504058838, "max_score_of_first_answer": 0.3389028012752533, "min_score_of_first_answer": 0.12733709812164307, "avg_score": 0.20773756504058838, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10578285157680511, "std_of_first_answer": 0.10578285157680511, "avg_entropy_of_first_answer": 2.7502392530441284, "lex_sim": 1.0}
{"question": "Replacing the unofficial E. Pluribis Unum, what phrase did the US adopt as its' national motto on July 30, 1956?", "answer": ["In God We Trust"], "scores": [[0.49114951491355896, 0.9608667492866516, 0.8285923600196838, 0.9930301308631897]], "normalized_score": [0.7893962264060974], "tokens": [["\u2581In", "\u2581God", "\u2581We", "\u2581Trust"]], "entropy": [[2.1093645095825195, 0.26734572649002075, 0.5272833704948425, 0.06182119995355606]], "avg_entropy": [0.7414537016302347], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "In God We Trust", "score_of_biggest_cluster": 0.7893962264060974, "score_of_first_answer": 0.7893962264060974, "max_score_of_first_answer": 0.9930301308631897, "min_score_of_first_answer": 0.49114951491355896, "avg_score": 0.7893962264060974, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.19873928969518906, "std_of_first_answer": 0.19873928969518906, "avg_entropy_of_first_answer": 0.7414537016302347, "lex_sim": 1.0}
{"question": "In which year of the 1940's did Al Capone die?", "answer": ["1947"], "scores": [[0.6456137299537659, 0.9728602170944214, 0.9577642679214478, 0.9576852917671204, 0.8501753807067871]], "normalized_score": [0.8669671416282654], "tokens": [["\u2581", "1", "9", "4", "7"]], "entropy": [[2.092472791671753, 0.20065420866012573, 0.2899685204029083, 0.29300153255462646, 0.7423994541168213]], "avg_entropy": [0.723699301481247], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and forty-seven", "score_of_biggest_cluster": 0.8669671416282654, "score_of_first_answer": 0.8669671416282654, "max_score_of_first_answer": 0.9728602170944214, "min_score_of_first_answer": 0.6456137299537659, "avg_score": 0.8669671416282654, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.12367794327784486, "std_of_first_answer": 0.12367794327784486, "avg_entropy_of_first_answer": 0.723699301481247, "lex_sim": 1.0}
{"question": "A swanee whistle was used to provide the voices of whom, in an animated tv series?", "answer": ["Roger Rabbit"], "scores": [[0.002562297973781824, 0.5802386999130249, 0.9941996335983276]], "normalized_score": [0.11391212046146393], "tokens": [["\u2581Roger", "\u2581Rab", "bit"]], "entropy": [[4.458903789520264, 1.7313745021820068, 0.05954531207680702]], "avg_entropy": [2.0832745345930257], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Soup Dragon", "score_of_biggest_cluster": 0.11391212046146393, "score_of_first_answer": 0.11391212046146393, "max_score_of_first_answer": 0.9941996335983276, "min_score_of_first_answer": 0.002562297973781824, "avg_score": 0.11391212046146393, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4066691646616757, "std_of_first_answer": 0.4066691646616757, "avg_entropy_of_first_answer": 2.0832745345930257, "lex_sim": 1.0}
{"question": "Where on the body would a Hindu woman wear a bindi?", "answer": ["On the forehead"], "scores": [[0.29182568192481995, 0.7145913243293762, 0.9000787734985352, 0.9981628060340881]], "normalized_score": [0.6579089164733887], "tokens": [["\u2581On", "\u2581the", "\u2581fore", "head"]], "entropy": [[3.2527379989624023, 0.9076942801475525, 0.6333602070808411, 0.017278362065553665]], "avg_entropy": [1.2027677120640874], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hairline", "score_of_biggest_cluster": 0.6579089164733887, "score_of_first_answer": 0.6579089164733887, "max_score_of_first_answer": 0.9981628060340881, "min_score_of_first_answer": 0.29182568192481995, "avg_score": 0.6579089164733887, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.270653560984286, "std_of_first_answer": 0.270653560984286, "avg_entropy_of_first_answer": 1.2027677120640874, "lex_sim": 1.0}
{"question": "Which Prime Minister created the first Life Peerages?", "answer": ["David Lloyd George"], "scores": [[0.05891275033354759, 0.8576284646987915, 0.9462833404541016]], "normalized_score": [0.362947016954422], "tokens": [["\u2581David", "\u2581Lloyd", "\u2581George"]], "entropy": [[3.7225425243377686, 0.9052298069000244, 0.2924489676952362]], "avg_entropy": [1.6400737663110097], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Harold Macmillan, Earl of Stockton", "score_of_biggest_cluster": 0.362947016954422, "score_of_first_answer": 0.362947016954422, "max_score_of_first_answer": 0.9462833404541016, "min_score_of_first_answer": 0.05891275033354759, "avg_score": 0.362947016954422, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3990590381802438, "std_of_first_answer": 0.3990590381802438, "avg_entropy_of_first_answer": 1.6400737663110097, "lex_sim": 1.0}
{"question": "Which English producer and screenwriter is responsible for the creation of the fictional school Grange Hill ?", "answer": ["Neville Teller"], "scores": [[0.003002478741109371, 0.989159882068634, 0.08133941143751144, 0.5200842618942261]], "normalized_score": [0.10587180405855179], "tokens": [["\u2581Ne", "ville", "\u2581T", "eller"]], "entropy": [[4.031001567840576, 0.08229173719882965, 4.8220977783203125, 1.7384696006774902]], "avg_entropy": [2.668465171009302], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Phil REDMOND", "score_of_biggest_cluster": 0.10587180405855179, "score_of_first_answer": 0.10587180405855179, "max_score_of_first_answer": 0.989159882068634, "min_score_of_first_answer": 0.003002478741109371, "avg_score": 0.10587180405855179, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3939133516533154, "std_of_first_answer": 0.3939133516533154, "avg_entropy_of_first_answer": 2.668465171009302, "lex_sim": 1.0}
{"question": "Who wrote the 1951 novel \u2018The Caine Mutiny\u2019?", "answer": ["Herman Wouk"], "scores": [[0.8702728748321533, 0.998842179775238, 0.9183179140090942, 0.9891362190246582, 0.9948750138282776]], "normalized_score": [0.952870786190033], "tokens": [["\u2581H", "erman", "\u2581W", "ou", "k"]], "entropy": [[1.0303086042404175, 0.012963959947228432, 0.6082401275634766, 0.10005446523427963, 0.05051514133810997]], "avg_entropy": [0.3604164596647024], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "HERMAN WOUK", "score_of_biggest_cluster": 0.952870786190033, "score_of_first_answer": 0.952870786190033, "max_score_of_first_answer": 0.998842179775238, "min_score_of_first_answer": 0.8702728748321533, "avg_score": 0.952870786190033, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0513793097185301, "std_of_first_answer": 0.0513793097185301, "avg_entropy_of_first_answer": 0.3604164596647024, "lex_sim": 1.0}
{"question": "What is the title of Max Bygraves autobiography", "answer": ["A Funny Kind of Life"], "scores": [[0.037543896585702896, 0.0442589670419693, 0.9894204139709473, 0.06604501605033875, 0.7101508975028992, 0.6334534287452698]], "normalized_score": [0.1911926418542862], "tokens": [["\u2581A", "\u2581Fun", "ny", "\u2581Kind", "\u2581of", "\u2581Life"]], "entropy": [[5.058980941772461, 5.852437973022461, 0.10383474826812744, 2.5696310997009277, 0.6386138796806335, 2.09013032913208]], "avg_entropy": [2.7189381619294486], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "I Wanna Tell You a Story", "score_of_biggest_cluster": 0.1911926418542862, "score_of_first_answer": 0.1911926418542862, "max_score_of_first_answer": 0.9894204139709473, "min_score_of_first_answer": 0.037543896585702896, "avg_score": 0.1911926418542862, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38001589535090413, "std_of_first_answer": 0.38001589535090413, "avg_entropy_of_first_answer": 2.7189381619294486, "lex_sim": 1.0}
{"question": "In the novel \u2018Watership Down\u2019, Yona is what type of creature?", "answer": ["Wastrel"], "scores": [[0.010183009319007397, 0.010237314738333225, 0.7555911540985107]], "normalized_score": [0.042866338044404984], "tokens": [["\u2581W", "ast", "rel"]], "entropy": [[4.320527076721191, 4.066754341125488, 1.1412606239318848]], "avg_entropy": [3.176180680592855], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Domestic Hedgehog", "score_of_biggest_cluster": 0.042866338044404984, "score_of_first_answer": 0.042866338044404984, "max_score_of_first_answer": 0.7555911540985107, "min_score_of_first_answer": 0.010183009319007397, "avg_score": 0.042866338044404984, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35137597007283883, "std_of_first_answer": 0.35137597007283883, "avg_entropy_of_first_answer": 3.176180680592855, "lex_sim": 1.0}
{"question": "In which prison was 'Amy' born in the novel 'Little Dorrit'?", "answer": ["Marshalsea"], "scores": [[0.5929522514343262, 0.9933070540428162, 0.9978524446487427]], "normalized_score": [0.8376383185386658], "tokens": [["\u2581Marsh", "alse", "a"]], "entropy": [[2.3465723991394043, 0.0649939775466919, 0.021126508712768555]], "avg_entropy": [0.8108976284662882], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chummage", "score_of_biggest_cluster": 0.8376383185386658, "score_of_first_answer": 0.8376383185386658, "max_score_of_first_answer": 0.9978524446487427, "min_score_of_first_answer": 0.5929522514343262, "avg_score": 0.8376383185386658, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1898094936779493, "std_of_first_answer": 0.1898094936779493, "avg_entropy_of_first_answer": 0.8108976284662882, "lex_sim": 1.0}
{"question": "Who published a 1754 book of his designs entitled \u2018The Gentleman and Cabinet-Maker\u2019s Director\u2019?", "answer": ["George Hepplewhite"], "scores": [[0.021303722634911537, 0.9036496877670288, 0.9880995154380798, 0.9996743202209473, 0.9413371086120605]], "normalized_score": [0.44727230072021484], "tokens": [["\u2581George", "\u2581He", "pp", "le", "white"]], "entropy": [[1.3854484558105469, 0.7027572989463806, 0.0879935771226883, 0.0039590331725776196, 0.36029908061027527]], "avg_entropy": [0.5080914891324937], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Thomas Chippendale", "score_of_biggest_cluster": 0.44727230072021484, "score_of_first_answer": 0.44727230072021484, "max_score_of_first_answer": 0.9996743202209473, "min_score_of_first_answer": 0.021303722634911537, "avg_score": 0.44727230072021484, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3763187871407976, "std_of_first_answer": 0.3763187871407976, "avg_entropy_of_first_answer": 0.5080914891324937, "lex_sim": 1.0}
{"question": "Which constituency did Harold Macmillan represent when he was Prime Minister?", "answer": ["Stoke-on-Trent"], "scores": [[0.03859274461865425, 0.9835631847381592, 0.6367159485816956, 0.8856766819953918, 0.9886614084243774, 0.9105810523033142, 0.998337984085083]], "normalized_score": [0.5686981678009033], "tokens": [["\u2581Sto", "ke", "-", "on", "-", "Tr", "ent"]], "entropy": [[2.0366017818450928, 0.13498735427856445, 1.551732063293457, 0.5430349707603455, 0.09167966991662979, 0.39146688580513, 0.018852274864912033]], "avg_entropy": [0.6811935715377331], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Valley Primary School", "score_of_biggest_cluster": 0.5686981678009033, "score_of_first_answer": 0.5686981678009033, "max_score_of_first_answer": 0.998337984085083, "min_score_of_first_answer": 0.03859274461865425, "avg_score": 0.5686981678009033, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32316236835884854, "std_of_first_answer": 0.32316236835884854, "avg_entropy_of_first_answer": 0.6811935715377331, "lex_sim": 1.0}
{"question": "Who won the most of the first ten tournaments called the Women's Hockey World Cup (the field hockey World Cup competition for women), which was inaugurated in 1974?", "answer": ["Pakistan"], "scores": [[0.010746383108198643]], "normalized_score": [0.010746384039521217], "tokens": [["\u2581Pakistan"]], "entropy": [[3.9078755378723145]], "avg_entropy": [3.9078755378723145], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "NETHERLANDS", "score_of_biggest_cluster": 0.010746384039521217, "score_of_first_answer": 0.010746384039521217, "max_score_of_first_answer": 0.010746383108198643, "min_score_of_first_answer": 0.010746383108198643, "avg_score": 0.010746384039521217, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.9078755378723145, "lex_sim": 1.0}
{"question": "O UGLY NINE is an anagram of which singer songwriter?", "answer": ["Pink Floyd"], "scores": [[0.015775728970766068, 0.13145889341831207, 0.8087490797042847, 0.9885595440864563]], "normalized_score": [0.2017897069454193], "tokens": [["\u2581P", "ink", "\u2581F", "loyd"]], "entropy": [[5.072788238525391, 1.9692496061325073, 0.8537540435791016, 0.09781897813081741]], "avg_entropy": [1.9984027165919542], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Godfather of Grunge", "score_of_biggest_cluster": 0.2017897069454193, "score_of_first_answer": 0.2017897069454193, "max_score_of_first_answer": 0.9885595440864563, "min_score_of_first_answer": 0.015775728970766068, "avg_score": 0.2017897069454193, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4193874257009826, "std_of_first_answer": 0.4193874257009826, "avg_entropy_of_first_answer": 1.9984027165919542, "lex_sim": 1.0}
{"question": "Goodge Street tube station is on which London Underground line?", "answer": ["Northern Line"], "scores": [[0.3408592641353607, 0.2598645091056824]], "normalized_score": [0.2976192533969879], "tokens": [["\u2581Northern", "\u2581Line"]], "entropy": [[2.7437710762023926, 1.2801198959350586]], "avg_entropy": [2.0119454860687256], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Northern (disambiguation)", "score_of_biggest_cluster": 0.2976192533969879, "score_of_first_answer": 0.2976192533969879, "max_score_of_first_answer": 0.3408592641353607, "min_score_of_first_answer": 0.2598645091056824, "avg_score": 0.2976192533969879, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04049737751483917, "std_of_first_answer": 0.04049737751483917, "avg_entropy_of_first_answer": 2.0119454860687256, "lex_sim": 1.0}
{"question": "The rotator cuff is a group of muscles and tendons that act to stabilize which part of the body?", "answer": ["Shoulder"], "scores": [[0.248755544424057, 0.8929774761199951]], "normalized_score": [0.47130998969078064], "tokens": [["\u2581Should", "er"]], "entropy": [[2.4929909706115723, 0.3655807077884674]], "avg_entropy": [1.4292858392000198], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Shoulder", "score_of_biggest_cluster": 0.47130998969078064, "score_of_first_answer": 0.47130998969078064, "max_score_of_first_answer": 0.8929774761199951, "min_score_of_first_answer": 0.248755544424057, "avg_score": 0.47130998969078064, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.32211096584796906, "std_of_first_answer": 0.32211096584796906, "avg_entropy_of_first_answer": 1.4292858392000198, "lex_sim": 1.0}
{"question": "What is the capital of Indonesia?", "answer": ["Yo"], "scores": [[0.0051015098579227924, 0.006577394437044859]], "normalized_score": [0.0057926359586417675], "tokens": [["\u2581Y", "o"]], "entropy": [[1.3810203075408936, 1.4136641025543213]], "avg_entropy": [1.3973422050476074], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ID-JK", "score_of_biggest_cluster": 0.0057926359586417675, "score_of_first_answer": 0.0057926359586417675, "max_score_of_first_answer": 0.006577394437044859, "min_score_of_first_answer": 0.0051015098579227924, "avg_score": 0.0057926359586417675, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0007379422895610332, "std_of_first_answer": 0.0007379422895610332, "avg_entropy_of_first_answer": 1.3973422050476074, "lex_sim": 1.0}
{"question": "What decoration, a Cross, was first awarded in 1995 to Corporal Wayne Mills for his actions in Bosnia?", "answer": ["Military Cross"], "scores": [[0.11069019883871078, 0.854629635810852]], "normalized_score": [0.3075697124004364], "tokens": [["\u2581Military", "\u2581Cross"]], "entropy": [[2.576082229614258, 0.8053896427154541]], "avg_entropy": [1.690735936164856], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "CONSPICUOUS GALLANTRY", "score_of_biggest_cluster": 0.3075697124004364, "score_of_first_answer": 0.3075697124004364, "max_score_of_first_answer": 0.854629635810852, "min_score_of_first_answer": 0.11069019883871078, "avg_score": 0.3075697124004364, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37196971848607063, "std_of_first_answer": 0.37196971848607063, "avg_entropy_of_first_answer": 1.690735936164856, "lex_sim": 1.0}
{"question": "Which rock concert held on December 6, 1969 was speculated to be a 'Woodstock West' but became infamous after it was marred by considerable violence?", "answer": ["Altamont Music Festival"], "scores": [[0.8146921992301941, 0.9917740225791931, 0.9973293542861938, 0.02029426582157612, 0.9374781847000122]], "normalized_score": [0.43362656235694885], "tokens": [["\u2581Alt", "am", "ont", "\u2581Music", "\u2581Festival"]], "entropy": [[1.033693790435791, 0.07416683435440063, 0.024870311841368675, 1.7708406448364258, 0.46287673711776733]], "avg_entropy": [0.6732896637171507], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Altamont Festival", "score_of_biggest_cluster": 0.43362656235694885, "score_of_first_answer": 0.43362656235694885, "max_score_of_first_answer": 0.9973293542861938, "min_score_of_first_answer": 0.02029426582157612, "avg_score": 0.43362656235694885, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.37186108495251563, "std_of_first_answer": 0.37186108495251563, "avg_entropy_of_first_answer": 0.6732896637171507, "lex_sim": 1.0}
{"question": "In which decade did stereo records first go on sale?", "answer": ["1960s"], "scores": [[0.4564359486103058, 0.8952988982200623, 0.9855160117149353, 0.21614786982536316, 0.9152372479438782, 0.6046041250228882]], "normalized_score": [0.60319983959198], "tokens": [["\u2581", "1", "9", "6", "0", "s"]], "entropy": [[2.576810359954834, 0.5816015005111694, 0.11629417538642883, 1.3059922456741333, 0.4847566783428192, 1.267714262008667]], "avg_entropy": [1.055528203646342], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1930\u2019s", "score_of_biggest_cluster": 0.60319983959198, "score_of_first_answer": 0.60319983959198, "max_score_of_first_answer": 0.9855160117149353, "min_score_of_first_answer": 0.21614786982536316, "avg_score": 0.60319983959198, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2786415910196429, "std_of_first_answer": 0.2786415910196429, "avg_entropy_of_first_answer": 1.055528203646342, "lex_sim": 1.0}
{"question": "Who is the current Australian prime minister ?", "answer": ["MalcolmTurnbull"], "scores": [[0.030279511585831642, 0.8722882866859436, 0.9984387755393982, 0.00020902750839013606, 0.9623119235038757, 0.9967120885848999]], "normalized_score": [0.13198864459991455], "tokens": [["\u2581Mal", "col", "m", "Turn", "b", "ull"]], "entropy": [[2.3543171882629395, 0.45396435260772705, 0.017796503379940987, 1.7234594821929932, 0.2572573721408844, 0.029886219650506973]], "avg_entropy": [0.806113519705832], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Julia gillard", "score_of_biggest_cluster": 0.13198864459991455, "score_of_first_answer": 0.13198864459991455, "max_score_of_first_answer": 0.9984387755393982, "min_score_of_first_answer": 0.00020902750839013606, "avg_score": 0.13198864459991455, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4462039860370907, "std_of_first_answer": 0.4462039860370907, "avg_entropy_of_first_answer": 0.806113519705832, "lex_sim": 1.0}
{"question": "What film production company was named after brothers Harry (born Hirsz), Albert (born Aaron), Sam (born Szmul), and Jack (born Itzhak)?", "answer": ["A&B Pictures"], "scores": [[0.010271540842950344, 0.06181874871253967, 0.16203425824642181, 0.17511719465255737]], "normalized_score": [0.06515126675367355], "tokens": [["\u2581A", "&", "B", "\u2581Pictures"]], "entropy": [[3.247188091278076, 4.647963523864746, 2.254575490951538, 3.5192747116088867]], "avg_entropy": [3.4172504544258118], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Warner Bros. Inc.", "score_of_biggest_cluster": 0.06515126675367355, "score_of_first_answer": 0.06515126675367355, "max_score_of_first_answer": 0.17511719465255737, "min_score_of_first_answer": 0.010271540842950344, "avg_score": 0.06515126675367355, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06888122750490475, "std_of_first_answer": 0.06888122750490475, "avg_entropy_of_first_answer": 3.4172504544258118, "lex_sim": 1.0}
{"question": "Marc Bolland has announced that he is to stand down as Chief Executive of which Major High Street Store?", "answer": ["Marks and Spencer"], "scores": [[0.8203150629997253, 0.9826856851577759, 0.5635330677032471, 0.971947193145752, 0.8148582577705383]], "normalized_score": [0.8150943517684937], "tokens": [["\u2581Mark", "s", "\u2581and", "\u2581Sp", "encer"]], "entropy": [[1.0063209533691406, 0.14358961582183838, 0.8927181959152222, 0.20362375676631927, 0.5409842729568481]], "avg_entropy": [0.5574473589658737], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mands", "score_of_biggest_cluster": 0.8150943517684937, "score_of_first_answer": 0.8150943517684937, "max_score_of_first_answer": 0.9826856851577759, "min_score_of_first_answer": 0.5635330677032471, "avg_score": 0.8150943517684937, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1515172533565866, "std_of_first_answer": 0.1515172533565866, "avg_entropy_of_first_answer": 0.5574473589658737, "lex_sim": 1.0}
{"question": "Who won the 'Oscar' for Best Actress in 2000 for her part in'Boys Don't Cry'?", "answer": ["Hilary Swank"], "scores": [[0.9381545186042786, 0.9962729215621948, 0.9940603375434875, 0.9862996935844421]], "normalized_score": [0.9784048199653625], "tokens": [["\u2581Hil", "ary", "\u2581Sw", "ank"]], "entropy": [[0.40437549352645874, 0.0416080504655838, 0.06608910113573074, 0.11816322803497314]], "avg_entropy": [0.1575589682906866], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hillary Swank", "score_of_biggest_cluster": 0.9784048199653625, "score_of_first_answer": 0.9784048199653625, "max_score_of_first_answer": 0.9962729215621948, "min_score_of_first_answer": 0.9381545186042786, "avg_score": 0.9784048199653625, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.023698306409040144, "std_of_first_answer": 0.023698306409040144, "avg_entropy_of_first_answer": 0.1575589682906866, "lex_sim": 1.0}
{"question": "What are the tetrahedron, cube, octahedron, dodecahedron and icosahedron known as?", "answer": ["Platonian solids"], "scores": [[0.026539331302046776, 0.35438838601112366, 0.31716471910476685, 0.7431107759475708, 0.9988899827003479]], "normalized_score": [0.29447272419929504], "tokens": [["\u2581Pl", "aton", "ian", "\u2581sol", "ids"]], "entropy": [[3.3692140579223633, 2.7302589416503906, 2.7230122089385986, 1.2791744470596313, 0.012069141492247581]], "avg_entropy": [2.022745759412646], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pythagorean solids", "score_of_biggest_cluster": 0.29447272419929504, "score_of_first_answer": 0.29447272419929504, "max_score_of_first_answer": 0.9988899827003479, "min_score_of_first_answer": 0.026539331302046776, "avg_score": 0.29447272419929504, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34236611574088266, "std_of_first_answer": 0.34236611574088266, "avg_entropy_of_first_answer": 2.022745759412646, "lex_sim": 1.0}
{"question": "What is Robin Williams character called in Good Morning Vietnam?", "answer": ["Adrian Cronauer"], "scores": [[0.9491090178489685, 0.9731459021568298, 0.9251165390014648, 0.997806966304779, 0.9863684773445129]], "normalized_score": [0.9659513235092163], "tokens": [["\u2581Ad", "rian", "\u2581C", "ron", "auer"]], "entropy": [[0.47051578760147095, 0.20495092868804932, 0.5982125401496887, 0.022610558196902275, 0.11834801733493805]], "avg_entropy": [0.28292756639420985], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Adrian", "score_of_biggest_cluster": 0.9659513235092163, "score_of_first_answer": 0.9659513235092163, "max_score_of_first_answer": 0.997806966304779, "min_score_of_first_answer": 0.9251165390014648, "avg_score": 0.9659513235092163, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.02620651053521514, "std_of_first_answer": 0.02620651053521514, "avg_entropy_of_first_answer": 0.28292756639420985, "lex_sim": 1.0}
{"question": "In which English town or city is Her Majesty's Prison Foston Hall?", "answer": ["Foston Hall"], "scores": [[0.2538956105709076, 0.9773226976394653, 0.27872908115386963]], "normalized_score": [0.4104798138141632], "tokens": [["\u2581F", "oston", "\u2581Hall"]], "entropy": [[3.8306703567504883, 0.21397733688354492, 1.8012967109680176]], "avg_entropy": [1.9486481348673503], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Derby, Derbyshire", "score_of_biggest_cluster": 0.4104798138141632, "score_of_first_answer": 0.4104798138141632, "max_score_of_first_answer": 0.9773226976394653, "min_score_of_first_answer": 0.2538956105709076, "avg_score": 0.4104798138141632, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3353267879679491, "std_of_first_answer": 0.3353267879679491, "avg_entropy_of_first_answer": 1.9486481348673503, "lex_sim": 1.0}
{"question": "What U.S. state holds the first presidential primary?", "answer": ["New Hampshire"], "scores": [[0.5578069090843201, 0.9380362629890442, 0.989141583442688]], "normalized_score": [0.8028861880302429], "tokens": [["\u2581New", "\u2581Ham", "pshire"]], "entropy": [[1.3273768424987793, 0.328705757856369, 0.08243817090988159]], "avg_entropy": [0.57950692375501], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "State of New Hampshire", "score_of_biggest_cluster": 0.8028861880302429, "score_of_first_answer": 0.8028861880302429, "max_score_of_first_answer": 0.989141583442688, "min_score_of_first_answer": 0.5578069090843201, "avg_score": 0.8028861880302429, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.19242190815823695, "std_of_first_answer": 0.19242190815823695, "avg_entropy_of_first_answer": 0.57950692375501, "lex_sim": 1.0}
{"question": "\"Who duetted with Bryan Adams on the 1985 hit \"\"It's Only Love\"\"?\"", "answer": ["Kim Carnes"], "scores": [[0.017497044056653976, 0.5094258785247803, 0.9830319881439209]], "normalized_score": [0.20616000890731812], "tokens": [["\u2581Kim", "\u2581Car", "nes"]], "entropy": [[2.9553847312927246, 2.4463272094726562, 0.11976988613605499]], "avg_entropy": [1.8404939423004787], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Anna Bullock", "score_of_biggest_cluster": 0.20616000890731812, "score_of_first_answer": 0.20616000890731812, "max_score_of_first_answer": 0.9830319881439209, "min_score_of_first_answer": 0.017497044056653976, "avg_score": 0.20616000890731812, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3942016479901496, "std_of_first_answer": 0.3942016479901496, "avg_entropy_of_first_answer": 1.8404939423004787, "lex_sim": 1.0}
{"question": "\u201cDreams from my father, a story of race and heritance\u201d is about which politician?", "answer": ["Barack Obama"], "scores": [[0.8747588992118835, 0.8346830606460571, 0.9382529258728027, 0.9969673752784729]], "normalized_score": [0.9090812802314758], "tokens": [["\u2581Bar", "ack", "\u2581Ob", "ama"]], "entropy": [[0.8702782988548279, 0.6007354855537415, 0.3612511157989502, 0.028228819370269775]], "avg_entropy": [0.4651234298944473], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Burack obama", "score_of_biggest_cluster": 0.9090812802314758, "score_of_first_answer": 0.9090812802314758, "max_score_of_first_answer": 0.9969673752784729, "min_score_of_first_answer": 0.8346830606460571, "avg_score": 0.9090812802314758, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06178732586901419, "std_of_first_answer": 0.06178732586901419, "avg_entropy_of_first_answer": 0.4651234298944473, "lex_sim": 1.0}
{"question": "The existance of which planet was mathematically", "answer": ["Pluto"], "scores": [[0.02092098444700241, 0.6319641470909119]], "normalized_score": [0.11498398333787918], "tokens": [["\u2581Pl", "uto"]], "entropy": [[4.405209541320801, 1.7675755023956299]], "avg_entropy": [3.0863925218582153], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "NEPTUNE", "score_of_biggest_cluster": 0.11498398333787918, "score_of_first_answer": 0.11498398333787918, "max_score_of_first_answer": 0.6319641470909119, "min_score_of_first_answer": 0.02092098444700241, "avg_score": 0.11498398333787918, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3055215813219547, "std_of_first_answer": 0.3055215813219547, "avg_entropy_of_first_answer": 3.0863925218582153, "lex_sim": 1.0}
{"question": "Given by the British tabloid press, what is the nickname of Nick Freeman, a solicitor famous for helping his clients escape punishment?", "answer": ["Mr Loophole"], "scores": [[0.22986319661140442, 0.9354094862937927, 0.9994856119155884]], "normalized_score": [0.598984956741333], "tokens": [["\u2581Mr", "\u2581Loop", "hole"]], "entropy": [[3.28536319732666, 0.43547600507736206, 0.0062449052929878235]], "avg_entropy": [1.2423613692323368], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nick Freeman", "score_of_biggest_cluster": 0.598984956741333, "score_of_first_answer": 0.598984956741333, "max_score_of_first_answer": 0.9994856119155884, "min_score_of_first_answer": 0.22986319661140442, "avg_score": 0.598984956741333, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3486832339750875, "std_of_first_answer": 0.3486832339750875, "avg_entropy_of_first_answer": 1.2423613692323368, "lex_sim": 1.0}
{"question": "What was the title of the musical based on the TV programme 'X Factor?", "answer": ["I Dreamed a Dream"], "scores": [[0.4893123209476471, 0.22253815829753876, 0.9534810185432434, 0.5849502086639404, 0.9856380820274353]], "normalized_score": [0.5694136023521423], "tokens": [["\u2581I", "\u2581Dream", "ed", "\u2581a", "\u2581Dream"]], "entropy": [[3.1309900283813477, 2.928077220916748, 0.2703496813774109, 0.802756130695343, 0.12455545365810394]], "avg_entropy": [1.4513457030057908], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The X Factor Musical", "score_of_biggest_cluster": 0.5694136023521423, "score_of_first_answer": 0.5694136023521423, "max_score_of_first_answer": 0.9856380820274353, "min_score_of_first_answer": 0.22253815829753876, "avg_score": 0.5694136023521423, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2889600774901495, "std_of_first_answer": 0.2889600774901495, "avg_entropy_of_first_answer": 1.4513457030057908, "lex_sim": 1.0}
{"question": "What was the first name of the Dutch explorer, Tasman, after whom Tasmania is named?", "answer": ["Abel"], "scores": [[0.2537005841732025, 0.9841961860656738]], "normalized_score": [0.49969103932380676], "tokens": [["\u2581Ab", "el"]], "entropy": [[3.7628302574157715, 0.15602442622184753]], "avg_entropy": [1.9594273418188095], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ABEL", "score_of_biggest_cluster": 0.49969103932380676, "score_of_first_answer": 0.49969103932380676, "max_score_of_first_answer": 0.9841961860656738, "min_score_of_first_answer": 0.2537005841732025, "avg_score": 0.49969103932380676, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.36524780094623566, "std_of_first_answer": 0.36524780094623566, "avg_entropy_of_first_answer": 1.9594273418188095, "lex_sim": 1.0}
{"question": "The port of Immingham is to be found on which estuary?", "answer": ["Humber, estuary"], "scores": [[0.5003129839897156, 0.9913095235824585, 0.008725814521312714, 0.03736763074994087, 0.9907222986221313, 0.992590069770813]], "normalized_score": [0.2327621728181839], "tokens": [["\u2581H", "umber", ",", "\u2581est", "u", "ary"]], "entropy": [[1.7125811576843262, 0.07352994382381439, 0.6387054920196533, 3.076594829559326, 0.08541607856750488, 0.05235026031732559]], "avg_entropy": [0.9398629603286585], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Humber Estuary", "score_of_biggest_cluster": 0.2327621728181839, "score_of_first_answer": 0.2327621728181839, "max_score_of_first_answer": 0.992590069770813, "min_score_of_first_answer": 0.008725814521312714, "avg_score": 0.2327621728181839, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.43492766568404, "std_of_first_answer": 0.43492766568404, "avg_entropy_of_first_answer": 0.9398629603286585, "lex_sim": 1.0}
{"question": "Facebook's new music sharing/streaming feature launched in 2015 was called 'Music... ?", "answer": ["Streaming"], "scores": [[0.005045614670962095, 0.47120732069015503]], "normalized_score": [0.048759933561086655], "tokens": [["\u2581Stream", "ing"]], "entropy": [[5.480469703674316, 1.7204022407531738]], "avg_entropy": [3.600435972213745], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Story", "score_of_biggest_cluster": 0.048759933561086655, "score_of_first_answer": 0.048759933561086655, "max_score_of_first_answer": 0.47120732069015503, "min_score_of_first_answer": 0.005045614670962095, "avg_score": 0.048759933561086655, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23308085300959647, "std_of_first_answer": 0.23308085300959647, "avg_entropy_of_first_answer": 3.600435972213745, "lex_sim": 1.0}
{"question": "What technical word is given usually to the right-side odd-numbered page of a book?", "answer": ["Title Page"], "scores": [[0.043690986931324005, 0.02402275986969471]], "normalized_score": [0.03239719569683075], "tokens": [["\u2581Title", "\u2581Page"]], "entropy": [[4.578688621520996, 1.3782567977905273]], "avg_entropy": [2.9784727096557617], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Verso and recto", "score_of_biggest_cluster": 0.03239719569683075, "score_of_first_answer": 0.03239719569683075, "max_score_of_first_answer": 0.043690986931324005, "min_score_of_first_answer": 0.02402275986969471, "avg_score": 0.03239719569683075, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.009834113530814648, "std_of_first_answer": 0.009834113530814648, "avg_entropy_of_first_answer": 2.9784727096557617, "lex_sim": 1.0}
{"question": "Who is the only man to win a best actor Oscar playing brothers?", "answer": ["Tom Hanks"], "scores": [[0.08319134265184402, 0.9224936366081238, 0.9517949819564819]], "normalized_score": [0.4180179834365845], "tokens": [["\u2581Tom", "\u2581H", "anks"]], "entropy": [[4.3738813400268555, 0.5247327089309692, 0.2667075991630554]], "avg_entropy": [1.72177388270696], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marvin, Lee", "score_of_biggest_cluster": 0.4180179834365845, "score_of_first_answer": 0.4180179834365845, "max_score_of_first_answer": 0.9517949819564819, "min_score_of_first_answer": 0.08319134265184402, "avg_score": 0.4180179834365845, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40273498202122926, "std_of_first_answer": 0.40273498202122926, "avg_entropy_of_first_answer": 1.72177388270696, "lex_sim": 1.0}
{"question": "According to the proverb, what does a stitch in time save?", "answer": ["A hole"], "scores": [[0.11922186613082886, 0.10221562534570694]], "normalized_score": [0.11039175093173981], "tokens": [["\u2581A", "\u2581hole"]], "entropy": [[2.649592399597168, 5.5107035636901855]], "avg_entropy": [4.080147981643677], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "9", "score_of_biggest_cluster": 0.11039175093173981, "score_of_first_answer": 0.11039175093173981, "max_score_of_first_answer": 0.11922186613082886, "min_score_of_first_answer": 0.10221562534570694, "avg_score": 0.11039175093173981, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.008503120392560959, "std_of_first_answer": 0.008503120392560959, "avg_entropy_of_first_answer": 4.080147981643677, "lex_sim": 1.0}
{"question": "In the Norrmalmstorg bank robbery in 1973, employees were held hostage for a few days and became emotionally attached to their captors, and even defended them after they were freed from their six-day ordeal. In which city did this incident take place?", "answer": ["Stockholm"], "scores": [[0.8702073097229004]], "normalized_score": [0.8702073097229004], "tokens": [["\u2581Stockholm"]], "entropy": [[1.013785481452942]], "avg_entropy": [1.013785481452942], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stockholm, Sweden", "score_of_biggest_cluster": 0.8702073097229004, "score_of_first_answer": 0.8702073097229004, "max_score_of_first_answer": 0.8702073097229004, "min_score_of_first_answer": 0.8702073097229004, "avg_score": 0.8702073097229004, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.013785481452942, "lex_sim": 1.0}
{"question": "Who was the first leader of the Russian Federation ?", "answer": ["Boris Yeltsin"], "scores": [[0.3048071563243866, 0.9958885312080383, 0.7194510102272034, 0.967122495174408, 0.9816600680351257, 0.995726466178894]], "normalized_score": [0.7687821388244629], "tokens": [["\u2581Bor", "is", "\u2581Y", "el", "ts", "in"]], "entropy": [[1.9067151546478271, 0.04186239838600159, 1.36600661277771, 0.2617589235305786, 0.11992350220680237, 0.03939124941825867]], "avg_entropy": [0.6226096401611964], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Boris El\u2019cin", "score_of_biggest_cluster": 0.7687821388244629, "score_of_first_answer": 0.7687821388244629, "max_score_of_first_answer": 0.9958885312080383, "min_score_of_first_answer": 0.3048071563243866, "avg_score": 0.7687821388244629, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2532446058548323, "std_of_first_answer": 0.2532446058548323, "avg_entropy_of_first_answer": 0.6226096401611964, "lex_sim": 1.0}
{"question": "How many balls are used in bar billiards?", "answer": ["Ten balls"], "scores": [[0.020350607112050056, 0.009910348802804947]], "normalized_score": [0.01420146320015192], "tokens": [["\u2581Ten", "\u2581balls"]], "entropy": [[3.511013984680176, 0.38699477910995483]], "avg_entropy": [1.9490043818950653], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "8", "score_of_biggest_cluster": 0.01420146320015192, "score_of_first_answer": 0.01420146320015192, "max_score_of_first_answer": 0.020350607112050056, "min_score_of_first_answer": 0.009910348802804947, "avg_score": 0.01420146320015192, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.005220129154622555, "std_of_first_answer": 0.005220129154622555, "avg_entropy_of_first_answer": 1.9490043818950653, "lex_sim": 1.0}
{"question": "What is the name of the structure designed by Anish Kapoor and Cecil Balmond for the 2012 London Olympics?", "answer": ["Olympic orbital ring (South Bank)"], "scores": [[0.07508156448602676, 0.00821696687489748, 0.13835890591144562, 0.021657267585396767, 0.009846065193414688, 0.00974008347839117, 0.0927431583404541, 0.14865201711654663, 0.5810251235961914]], "normalized_score": [0.04826051741838455], "tokens": [["\u2581Olympic", "\u2581orb", "ital", "\u2581ring", "\u2581(", "S", "outh", "\u2581Bank", ")"]], "entropy": [[1.948051929473877, 3.3540773391723633, 1.3961114883422852, 3.7686448097229004, 0.6702603101730347, 4.666686058044434, 3.4719173908233643, 3.258798360824585, 2.423299789428711]], "avg_entropy": [2.773094164000617], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Olympic Orbit", "score_of_biggest_cluster": 0.04826051741838455, "score_of_first_answer": 0.04826051741838455, "max_score_of_first_answer": 0.5810251235961914, "min_score_of_first_answer": 0.00821696687489748, "avg_score": 0.04826051741838455, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17091826680497454, "std_of_first_answer": 0.17091826680497454, "avg_entropy_of_first_answer": 2.773094164000617, "lex_sim": 1.0}
{"question": "In 1973, which Chicago building became the tallest in the world?", "answer": ["Sears Tower"], "scores": [[0.6345372796058655, 0.9922677874565125, 0.8803461194038391]], "normalized_score": [0.8214475512504578], "tokens": [["\u2581S", "ears", "\u2581Tower"]], "entropy": [[1.813647985458374, 0.07307209074497223, 0.70467209815979]], "avg_entropy": [0.8637973914543787], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sears Tower (Willis Tower)", "score_of_biggest_cluster": 0.8214475512504578, "score_of_first_answer": 0.8214475512504578, "max_score_of_first_answer": 0.9922677874565125, "min_score_of_first_answer": 0.6345372796058655, "avg_score": 0.8214475512504578, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.14941350591548144, "std_of_first_answer": 0.14941350591548144, "avg_entropy_of_first_answer": 0.8637973914543787, "lex_sim": 1.0}
{"question": "Which Verdi opera is based on La Dame aux Camellias by Alexandre Dumas fils?", "answer": ["La Traviata"], "scores": [[0.6262860894203186, 0.9540204405784607, 0.9788990616798401, 0.9920404553413391]], "normalized_score": [0.8727692365646362], "tokens": [["\u2581La", "\u2581Tra", "vi", "ata"]], "entropy": [[1.8476057052612305, 0.26959389448165894, 0.16554148495197296, 0.06421473622322083]], "avg_entropy": [0.5867389552295208], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Addio del passato", "score_of_biggest_cluster": 0.8727692365646362, "score_of_first_answer": 0.8727692365646362, "max_score_of_first_answer": 0.9920404553413391, "min_score_of_first_answer": 0.6262860894203186, "avg_score": 0.8727692365646362, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.15160786854226765, "std_of_first_answer": 0.15160786854226765, "avg_entropy_of_first_answer": 0.5867389552295208, "lex_sim": 1.0}
{"question": "Who wrote Of Mice and Men?", "answer": ["John Steinbeck"], "scores": [[0.8270918726921082, 0.7701976299285889, 0.970964252948761]], "normalized_score": [0.8520264029502869], "tokens": [["\u2581John", "\u2581Stein", "beck"]], "entropy": [[1.2701849937438965, 1.5198099613189697, 0.1865934282541275]], "avg_entropy": [0.9921961277723312], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Steinbeck III", "score_of_biggest_cluster": 0.8520264029502869, "score_of_first_answer": 0.8520264029502869, "max_score_of_first_answer": 0.970964252948761, "min_score_of_first_answer": 0.7701976299285889, "avg_score": 0.8520264029502869, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08448764116016094, "std_of_first_answer": 0.08448764116016094, "avg_entropy_of_first_answer": 0.9921961277723312, "lex_sim": 1.0}
{"question": "The first name of which science fiction hero created by Alex Raymond was retitled as 'Speed' in Australia to avoid a negative connotation of the word by which we know him better?", "answer": ["Flash Gordon"], "scores": [[0.6250092387199402, 0.2379567176103592]], "normalized_score": [0.3856489956378937], "tokens": [["\u2581Flash", "\u2581Gordon"]], "entropy": [[2.6002471446990967, 0.9697657227516174]], "avg_entropy": [1.785006433725357], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Speed Gordon", "score_of_biggest_cluster": 0.3856489956378937, "score_of_first_answer": 0.3856489956378937, "max_score_of_first_answer": 0.6250092387199402, "min_score_of_first_answer": 0.2379567176103592, "avg_score": 0.3856489956378937, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1935262605547905, "std_of_first_answer": 0.1935262605547905, "avg_entropy_of_first_answer": 1.785006433725357, "lex_sim": 1.0}
{"question": "What name was given to the English Civil war in the 15th century between the rival houses of Lancaster and York?", "answer": ["War of the Roses (Rose War, White Rose War)"], "scores": [[0.32753971219062805, 0.940784215927124, 0.8544959425926208, 0.806358277797699, 0.9986147880554199, 0.0029011769220232964, 0.11558128893375397, 0.350121408700943, 0.09190710633993149, 0.04982253164052963, 0.02964743971824646, 0.6663110852241516, 0.5530692934989929, 0.6375709772109985]], "normalized_score": [0.22386537492275238], "tokens": [["\u2581War", "\u2581of", "\u2581the", "\u2581Ros", "es", "\u2581(", "R", "ose", "\u2581War", ",", "\u2581White", "\u2581Rose", "\u2581War", ")"]], "entropy": [[2.1331465244293213, 0.36091887950897217, 0.5707970857620239, 0.6090119481086731, 0.015582840889692307, 0.17553715407848358, 4.602887153625488, 2.8304781913757324, 4.033324241638184, 0.8612282276153564, 3.3686320781707764, 1.0156002044677734, 1.4992139339447021, 0.9691429734230042]], "avg_entropy": [1.6461072455027275], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "War of the roses", "score_of_biggest_cluster": 0.22386537492275238, "score_of_first_answer": 0.22386537492275238, "max_score_of_first_answer": 0.9986147880554199, "min_score_of_first_answer": 0.0029011769220232964, "avg_score": 0.22386537492275238, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.35109169299692117, "std_of_first_answer": 0.35109169299692117, "avg_entropy_of_first_answer": 1.6461072455027275, "lex_sim": 1.0}
{"question": "The Cesarewitch horse race is run at Newmarket during which month of the year?", "answer": ["October"], "scores": [[0.45769909024238586]], "normalized_score": [0.45769909024238586], "tokens": [["\u2581October"]], "entropy": [[2.272101402282715]], "avg_entropy": [2.272101402282715], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ocotber", "score_of_biggest_cluster": 0.45769909024238586, "score_of_first_answer": 0.45769909024238586, "max_score_of_first_answer": 0.45769909024238586, "min_score_of_first_answer": 0.45769909024238586, "avg_score": 0.45769909024238586, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.272101402282715, "lex_sim": 1.0}
{"question": "The Mekong River rises in the Tibetan plateau and runs through or between six countries - China, Burma, Thailand, Cambodia, Vietnam and which other?", "answer": ["Laos"], "scores": [[0.8593339323997498, 0.9807611703872681]], "normalized_score": [0.9180421233177185], "tokens": [["\u2581La", "os"]], "entropy": [[1.0313079357147217, 0.12461046874523163]], "avg_entropy": [0.5779592022299767], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Saathiaranagroat Prachhathippatay Prachhachhon Lao", "score_of_biggest_cluster": 0.9180421233177185, "score_of_first_answer": 0.9180421233177185, "max_score_of_first_answer": 0.9807611703872681, "min_score_of_first_answer": 0.8593339323997498, "avg_score": 0.9180421233177185, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.060713618993759155, "std_of_first_answer": 0.060713618993759155, "avg_entropy_of_first_answer": 0.5779592022299767, "lex_sim": 1.0}
{"question": "Who was Wolfe\u2019s opposing French general who died with him on the Plains of Abraham?", "answer": ["Louis-Joseph, vicomte de Montcalm"], "scores": [[0.4049876630306244, 0.6290151476860046, 0.9850044250488281, 0.9917795062065125, 0.09907235950231552, 0.002811545506119728, 0.9730489253997803, 0.9985184073448181, 0.9496075510978699, 0.9847256541252136, 0.9973745346069336, 0.9963679313659668]], "normalized_score": [0.44638216495513916], "tokens": [["\u2581Louis", "-", "Jose", "ph", ",", "\u2581vic", "om", "te", "\u2581de", "\u2581Mont", "cal", "m"]], "entropy": [[3.044278144836426, 1.7070627212524414, 0.12059429287910461, 0.07970139384269714, 1.3173692226409912, 1.704020380973816, 0.20301738381385803, 0.01580622047185898, 0.3481352925300598, 0.12912079691886902, 0.02822720631957054, 0.036847904324531555]], "avg_entropy": [0.727848413400352], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Montcalm", "score_of_biggest_cluster": 0.44638216495513916, "score_of_first_answer": 0.44638216495513916, "max_score_of_first_answer": 0.9985184073448181, "min_score_of_first_answer": 0.002811545506119728, "avg_score": 0.44638216495513916, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3602872520417986, "std_of_first_answer": 0.3602872520417986, "avg_entropy_of_first_answer": 0.727848413400352, "lex_sim": 1.0}
{"question": "Saloth Sar was the real name of which infamous dictator?", "answer": ["Pol Pot"], "scores": [[0.8728177547454834, 0.9721605181694031]], "normalized_score": [0.9211508631706238], "tokens": [["\u2581Pol", "\u2581Pot"]], "entropy": [[1.0483450889587402, 0.20938986539840698]], "avg_entropy": [0.6288674771785736], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Polpot", "score_of_biggest_cluster": 0.9211508631706238, "score_of_first_answer": 0.9211508631706238, "max_score_of_first_answer": 0.9721605181694031, "min_score_of_first_answer": 0.8728177547454834, "avg_score": 0.9211508631706238, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.04967138171195984, "std_of_first_answer": 0.04967138171195984, "avg_entropy_of_first_answer": 0.6288674771785736, "lex_sim": 1.0}
{"question": "Whose portrait is to appear on the new \u00a310 note due to be issued in 2017?", "answer": ["Jane Austen"], "scores": [[0.40705665946006775, 0.9584477543830872, 0.997524082660675]], "normalized_score": [0.7300997972488403], "tokens": [["\u2581Jane", "\u2581Aust", "en"]], "entropy": [[3.287198066711426, 0.3208591341972351, 0.02715967781841755]], "avg_entropy": [1.2117389595756929], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jane Austen", "score_of_biggest_cluster": 0.7300997972488403, "score_of_first_answer": 0.7300997972488403, "max_score_of_first_answer": 0.997524082660675, "min_score_of_first_answer": 0.40705665946006775, "avg_score": 0.7300997972488403, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.26961101132582954, "std_of_first_answer": 0.26961101132582954, "avg_entropy_of_first_answer": 1.2117389595756929, "lex_sim": 1.0}
{"question": "Which Major League Baseball team plays its home games at Chase Field, built in 1992?", "answer": ["Arizona Diamondbacks"], "scores": [[0.7633967995643616, 0.8900286555290222, 0.9844141006469727, 0.981274425983429, 0.9942409992218018]], "normalized_score": [0.9181694984436035], "tokens": [["\u2581Arizona", "\u2581Diam", "ond", "back", "s"]], "entropy": [[1.231236457824707, 0.6251189708709717, 0.10753250867128372, 0.12416371703147888, 0.04205581545829773]], "avg_entropy": [0.4260214939713478], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The D Backs", "score_of_biggest_cluster": 0.9181694984436035, "score_of_first_answer": 0.9181694984436035, "max_score_of_first_answer": 0.9942409992218018, "min_score_of_first_answer": 0.7633967995643616, "avg_score": 0.9181694984436035, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08809384811674417, "std_of_first_answer": 0.08809384811674417, "avg_entropy_of_first_answer": 0.4260214939713478, "lex_sim": 1.0}
{"question": "Which notorious murderer lived at 10 Rillington Place?", "answer": ["John Christie"], "scores": [[0.826043963432312, 0.16744938492774963, 0.966482937335968]], "normalized_score": [0.5113210082054138], "tokens": [["\u2581John", "\u2581Christ", "ie"]], "entropy": [[1.0081777572631836, 1.1115425825119019, 0.282985657453537]], "avg_entropy": [0.8009019990762075], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Christie (footballer)", "score_of_biggest_cluster": 0.5113210082054138, "score_of_first_answer": 0.5113210082054138, "max_score_of_first_answer": 0.966482937335968, "min_score_of_first_answer": 0.16744938492774963, "avg_score": 0.5113210082054138, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34831731097135027, "std_of_first_answer": 0.34831731097135027, "avg_entropy_of_first_answer": 0.8009019990762075, "lex_sim": 1.0}
{"question": "What world leader introduced Glasnost and Perestroika?", "answer": ["Boris Yeltsin"], "scores": [[0.0073424396105110645, 0.991654634475708, 0.7068836092948914, 0.8767071962356567, 0.9800741076469421, 0.9948333501815796]], "normalized_score": [0.40479525923728943], "tokens": [["\u2581Bor", "is", "\u2581Y", "el", "ts", "in"]], "entropy": [[1.2507816553115845, 0.08623373508453369, 1.611839771270752, 0.8683927059173584, 0.12498296052217484, 0.04683496803045273]], "avg_entropy": [0.6648442993561426], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "M. Gorbachev", "score_of_biggest_cluster": 0.40479525923728943, "score_of_first_answer": 0.40479525923728943, "max_score_of_first_answer": 0.9948333501815796, "min_score_of_first_answer": 0.0073424396105110645, "avg_score": 0.40479525923728943, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35123056604805125, "std_of_first_answer": 0.35123056604805125, "avg_entropy_of_first_answer": 0.6648442993561426, "lex_sim": 1.0}
{"question": "What was the codename for the German invasion of Russia in 1941?", "answer": ["Operation Barbarossa"], "scores": [[0.5867002010345459, 0.9120025634765625, 0.9976861476898193, 0.9553689956665039]], "normalized_score": [0.8450732231140137], "tokens": [["\u2581Operation", "\u2581Bar", "bar", "ossa"]], "entropy": [[1.6080243587493896, 0.7109818458557129, 0.024487964808940887, 0.22252659499645233]], "avg_entropy": [0.6415051911026239], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Barbarossa (disambiguation)", "score_of_biggest_cluster": 0.8450732231140137, "score_of_first_answer": 0.8450732231140137, "max_score_of_first_answer": 0.9976861476898193, "min_score_of_first_answer": 0.5867002010345459, "avg_score": 0.8450732231140137, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.16233853918850968, "std_of_first_answer": 0.16233853918850968, "avg_entropy_of_first_answer": 0.6415051911026239, "lex_sim": 1.0}
{"question": "The 1965 film \u2018The Heroes of Telemark\u2019 is set in which European country?", "answer": ["Norway"], "scores": [[0.9419417381286621]], "normalized_score": [0.9419417381286621], "tokens": [["\u2581Norway"]], "entropy": [[0.479745090007782]], "avg_entropy": [0.479745090007782], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Norv\u00e8ge", "score_of_biggest_cluster": 0.9419417381286621, "score_of_first_answer": 0.9419417381286621, "max_score_of_first_answer": 0.9419417381286621, "min_score_of_first_answer": 0.9419417381286621, "avg_score": 0.9419417381286621, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.479745090007782, "lex_sim": 1.0}
{"question": "Bandar Seri Begawan International airport is in which country?", "answer": ["Brunei"], "scores": [[0.889384388923645, 0.99247145652771, 0.9953600764274597]], "normalized_score": [0.957772970199585], "tokens": [["\u2581Br", "une", "i"]], "entropy": [[0.8728581666946411, 0.06960450112819672, 0.048027846962213516]], "avg_entropy": [0.33016350492835045], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Abode of Peace", "score_of_biggest_cluster": 0.957772970199585, "score_of_first_answer": 0.957772970199585, "max_score_of_first_answer": 0.9953600764274597, "min_score_of_first_answer": 0.889384388923645, "avg_score": 0.957772970199585, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04929067296720149, "std_of_first_answer": 0.04929067296720149, "avg_entropy_of_first_answer": 0.33016350492835045, "lex_sim": 1.0}
{"question": "Where, in 2011 and since, is a major gathering place in Egypt for expressing protest?", "answer": ["Tahrir Square"], "scores": [[0.7667704820632935, 0.9976341724395752, 0.9640555381774902, 0.8820263743400574]], "normalized_score": [0.8980594277381897], "tokens": [["\u2581T", "ah", "rir", "\u2581Square"]], "entropy": [[1.570817470550537, 0.029015470296144485, 0.1860138177871704, 0.6188810467720032]], "avg_entropy": [0.6011819513514638], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tahrir Square", "score_of_biggest_cluster": 0.8980594277381897, "score_of_first_answer": 0.8980594277381897, "max_score_of_first_answer": 0.9976341724395752, "min_score_of_first_answer": 0.7667704820632935, "avg_score": 0.8980594277381897, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08899607750655524, "std_of_first_answer": 0.08899607750655524, "avg_entropy_of_first_answer": 0.6011819513514638, "lex_sim": 1.0}
{"question": "What is the name of Tokyo's seaport ?", "answer": ["Yokohoma"], "scores": [[0.3571632504463196, 0.9188741445541382, 0.934190571308136, 0.028888549655675888]], "normalized_score": [0.3067757487297058], "tokens": [["\u2581Y", "ok", "oh", "oma"]], "entropy": [[3.181366443634033, 0.6224415302276611, 0.41290679574012756, 0.23961450159549713]], "avg_entropy": [1.1140823177993298], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u6a2a\u6d5c", "score_of_biggest_cluster": 0.3067757487297058, "score_of_first_answer": 0.3067757487297058, "max_score_of_first_answer": 0.934190571308136, "min_score_of_first_answer": 0.028888549655675888, "avg_score": 0.3067757487297058, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.38471780560747293, "std_of_first_answer": 0.38471780560747293, "avg_entropy_of_first_answer": 1.1140823177993298, "lex_sim": 1.0}
{"question": "In 2006, who became the UK's first woman Secretary of State for Foreign and Commonwealth Affairs?", "answer": ["Margaret Beckett"], "scores": [[0.9201131463050842, 0.8101517558097839, 0.9952762126922607]], "normalized_score": [0.9052817225456238], "tokens": [["\u2581Margaret", "\u2581Beck", "ett"]], "entropy": [[0.5786315202713013, 1.228475570678711, 0.04937385022640228]], "avg_entropy": [0.6188269803921381], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Margaret Becket", "score_of_biggest_cluster": 0.9052817225456238, "score_of_first_answer": 0.9052817225456238, "max_score_of_first_answer": 0.9952762126922607, "min_score_of_first_answer": 0.8101517558097839, "avg_score": 0.9052817225456238, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07602050782111001, "std_of_first_answer": 0.07602050782111001, "avg_entropy_of_first_answer": 0.6188269803921381, "lex_sim": 1.0}
{"question": "\"In the TV series \"\"Thunderbirds\"\", Parker was chauffeur to whom?\"", "answer": ["Lady Penelope"], "scores": [[0.14339567720890045, 0.9911652207374573, 0.9987906813621521, 0.999077558517456]], "normalized_score": [0.6136757135391235], "tokens": [["\u2581Lady", "\u2581Pen", "el", "ope"]], "entropy": [[3.417814254760742, 0.08647066354751587, 0.013131328858435154, 0.008556579239666462]], "avg_entropy": [0.8814932066015899], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Penelon", "score_of_biggest_cluster": 0.6136757135391235, "score_of_first_answer": 0.6136757135391235, "max_score_of_first_answer": 0.999077558517456, "min_score_of_first_answer": 0.14339567720890045, "avg_score": 0.6136757135391235, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3693513003481656, "std_of_first_answer": 0.3693513003481656, "avg_entropy_of_first_answer": 0.8814932066015899, "lex_sim": 1.0}
{"question": "In old measurement 1 rod = 1 pole = 1 perch = what fraction of 1 chain?", "answer": ["20.255777777778f"], "scores": [[0.6392328143119812, 0.0913299098610878, 0.36950165033340454, 0.0715341717004776, 0.2544744610786438, 0.2458152025938034, 0.011617256328463554, 0.05059761926531792, 0.06409527361392975, 0.08964869379997253, 0.2751193344593048, 0.73715740442276, 0.8206431269645691, 0.8595239520072937, 0.8608425855636597, 0.2598319947719574, 0.005116404965519905]], "normalized_score": [0.16567423939704895], "tokens": [["\u2581", "2", "0", ".", "2", "5", "5", "7", "7", "7", "7", "7", "7", "7", "7", "8", "f"]], "entropy": [[2.105855703353882, 1.6978737115859985, 2.8533661365509033, 3.191545009613037, 1.6641438007354736, 2.512460708618164, 3.160891056060791, 3.2358665466308594, 2.63240909576416, 2.639982223510742, 2.486942768096924, 1.130636215209961, 0.7817695140838623, 0.6499878168106079, 0.6114875078201294, 0.8336468935012817, 3.481691360473633]], "avg_entropy": [2.09826800402473], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A QUARTER", "score_of_biggest_cluster": 0.16567423939704895, "score_of_first_answer": 0.16567423939704895, "max_score_of_first_answer": 0.8608425855636597, "min_score_of_first_answer": 0.005116404965519905, "avg_score": 0.16567423939704895, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.30901083872468593, "std_of_first_answer": 0.30901083872468593, "avg_entropy_of_first_answer": 2.09826800402473, "lex_sim": 1.0}
{"question": "Where was the Fiddler in the musical's title?", "answer": ["Anatevka"], "scores": [[0.21880251169204712, 0.6195314526557922, 0.9848595261573792, 0.995435893535614]], "normalized_score": [0.6037759780883789], "tokens": [["\u2581An", "ate", "v", "ka"]], "entropy": [[4.624607086181641, 1.9176278114318848, 0.1393338441848755, 0.043501146137714386]], "avg_entropy": [1.6812674719840288], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sur les toits (cambrioleurs et gendarmes)", "score_of_biggest_cluster": 0.6037759780883789, "score_of_first_answer": 0.6037759780883789, "max_score_of_first_answer": 0.995435893535614, "min_score_of_first_answer": 0.21880251169204712, "avg_score": 0.6037759780883789, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31873451460813257, "std_of_first_answer": 0.31873451460813257, "avg_entropy_of_first_answer": 1.6812674719840288, "lex_sim": 1.0}
{"question": "The main feature of the human illness diabetes is the body's inability to produce what hormone?", "answer": ["Insulin"], "scores": [[0.513317346572876, 0.9951428174972534, 0.981289267539978]], "normalized_score": [0.7943699359893799], "tokens": [["\u2581Ins", "ul", "in"]], "entropy": [[1.532264232635498, 0.046724818646907806, 0.12757715582847595]], "avg_entropy": [0.568855402370294], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Insulin antagonists", "score_of_biggest_cluster": 0.7943699359893799, "score_of_first_answer": 0.7943699359893799, "max_score_of_first_answer": 0.9951428174972534, "min_score_of_first_answer": 0.513317346572876, "avg_score": 0.7943699359893799, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22394082157062944, "std_of_first_answer": 0.22394082157062944, "avg_entropy_of_first_answer": 0.568855402370294, "lex_sim": 1.0}
{"question": "Which vegetable do the French call Chou-Fleur?", "answer": ["Cauliflower"], "scores": [[0.21436437964439392, 0.9927278757095337, 0.9959549903869629, 0.9822661876678467]], "normalized_score": [0.6754806637763977], "tokens": [["\u2581Ca", "ul", "if", "lower"]], "entropy": [[3.7485055923461914, 0.05860032141208649, 0.04400760680437088, 0.11064768582582474]], "avg_entropy": [0.9904403015971184], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Purple cauliflower", "score_of_biggest_cluster": 0.6754806637763977, "score_of_first_answer": 0.6754806637763977, "max_score_of_first_answer": 0.9959549903869629, "min_score_of_first_answer": 0.21436437964439392, "avg_score": 0.6754806637763977, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3360351586961439, "std_of_first_answer": 0.3360351586961439, "avg_entropy_of_first_answer": 0.9904403015971184, "lex_sim": 1.0}
{"question": "Named for its chairman, The Warren Commission published its final report on Sept 27, 1965. What event was it seated to investigate?", "answer": ["Killing of John F. Kennedy"], "scores": [[0.0014285648940131068, 0.6981502771377563, 0.96528160572052, 0.2757045030593872, 0.9212107062339783, 0.7773727774620056, 0.9750219583511353]], "normalized_score": [0.2929880917072296], "tokens": [["\u2581K", "illing", "\u2581of", "\u2581John", "\u2581F", ".", "\u2581Kennedy"]], "entropy": [[2.5976881980895996, 2.172137975692749, 0.26354900002479553, 1.7841260433197021, 0.40178126096725464, 0.6000149250030518, 0.17272427678108215]], "avg_entropy": [1.1417173828397478], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Shooting of John F. Kennedy", "score_of_biggest_cluster": 0.2929880917072296, "score_of_first_answer": 0.2929880917072296, "max_score_of_first_answer": 0.9750219583511353, "min_score_of_first_answer": 0.0014285648940131068, "avg_score": 0.2929880917072296, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3499577623135168, "std_of_first_answer": 0.3499577623135168, "avg_entropy_of_first_answer": 1.1417173828397478, "lex_sim": 1.0}
{"question": "What is the name of the first digital library founded by Michael Hart in 1971?", "answer": ["World Computer Library"], "scores": [[0.017811717465519905, 0.014177845790982246, 0.5201025009155273]], "normalized_score": [0.05083173140883446], "tokens": [["\u2581World", "\u2581Computer", "\u2581Library"]], "entropy": [[2.7744674682617188, 3.3918652534484863, 1.7391462326049805]], "avg_entropy": [2.635159651438395], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gutenburg Project", "score_of_biggest_cluster": 0.05083173140883446, "score_of_first_answer": 0.05083173140883446, "max_score_of_first_answer": 0.5201025009155273, "min_score_of_first_answer": 0.014177845790982246, "avg_score": 0.05083173140883446, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23764328843061755, "std_of_first_answer": 0.23764328843061755, "avg_entropy_of_first_answer": 2.635159651438395, "lex_sim": 1.0}
{"question": "Which word is missing from this music hall song \u2018When father papered the ......... You couldn\u2019t see pa for paste\u2019?", "answer": ["paper"], "scores": [[0.00821603462100029]], "normalized_score": [0.008216033689677715], "tokens": [["\u2581paper"]], "entropy": [[6.0154337882995605]], "avg_entropy": [6.0154337882995605], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "PARLOUR", "score_of_biggest_cluster": 0.008216033689677715, "score_of_first_answer": 0.008216033689677715, "max_score_of_first_answer": 0.00821603462100029, "min_score_of_first_answer": 0.00821603462100029, "avg_score": 0.008216033689677715, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.0154337882995605, "lex_sim": 1.0}
{"question": "Five docks built on The Manchester Ship Canal in what is now Salford Quays were named after which Roman goddess? The area is now served by a Metrolink stop bearing the same name.", "answer": ["The Waters"], "scores": [[0.027505123987793922, 0.0054897526279091835, 0.30158936977386475]], "normalized_score": [0.03571035712957382], "tokens": [["\u2581The", "\u2581W", "aters"]], "entropy": [[3.8195371627807617, 5.577159404754639, 3.0332908630371094]], "avg_entropy": [4.14332914352417], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pomona Station", "score_of_biggest_cluster": 0.03571035712957382, "score_of_first_answer": 0.03571035712957382, "max_score_of_first_answer": 0.30158936977386475, "min_score_of_first_answer": 0.0054897526279091835, "avg_score": 0.03571035712957382, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13469382302021723, "std_of_first_answer": 0.13469382302021723, "avg_entropy_of_first_answer": 4.14332914352417, "lex_sim": 1.0}
{"question": "Who narrated the BBC television production of Paddington?", "answer": ["Jim Broadbent"], "scores": [[0.019167684018611908, 0.9929094314575195, 0.9968773126602173, 0.9990200996398926]], "normalized_score": [0.3710426390171051], "tokens": [["\u2581Jim", "\u2581Broad", "b", "ent"]], "entropy": [[2.842996120452881, 0.07546904683113098, 0.033343929797410965, 0.010708717629313469]], "avg_entropy": [0.7406294536776841], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Michael Hordon", "score_of_biggest_cluster": 0.3710426390171051, "score_of_first_answer": 0.3710426390171051, "max_score_of_first_answer": 0.9990200996398926, "min_score_of_first_answer": 0.019167684018611908, "avg_score": 0.3710426390171051, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42310293819335915, "std_of_first_answer": 0.42310293819335915, "avg_entropy_of_first_answer": 0.7406294536776841, "lex_sim": 1.0}
{"question": "\"The opera \"\"Duke Bluebeard's Castle\"\" and the ballet \"\"The Wooden Prince\"\" are the works of which composer?\"", "answer": ["Maurice Ravel"], "scores": [[0.0026300332974642515, 0.6940420269966125, 0.9993816614151001]], "normalized_score": [0.12218731641769409], "tokens": [["\u2581Maurice", "\u2581R", "avel"]], "entropy": [[1.9557340145111084, 2.1137843132019043, 0.0076174382120370865]], "avg_entropy": [1.3590452553083499], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bela Bart\u00f3k", "score_of_biggest_cluster": 0.12218731641769409, "score_of_first_answer": 0.12218731641769409, "max_score_of_first_answer": 0.9993816614151001, "min_score_of_first_answer": 0.0026300332974642515, "avg_score": 0.12218731641769409, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4169727736293002, "std_of_first_answer": 0.4169727736293002, "avg_entropy_of_first_answer": 1.3590452553083499, "lex_sim": 1.0}
{"question": "What is a name for a type of private club in city outskirts or rural areas that usually offers a variety of recreational sports facilities such as golf, tennis, swimming and polo?", "answer": ["An exclusive golfing community"], "scores": [[0.010810785926878452, 0.05986643582582474, 0.0556945726275444, 0.03641791641712189, 0.04577973484992981]], "normalized_score": [0.035955753177404404], "tokens": [["\u2581An", "\u2581exclusive", "\u2581golf", "ing", "\u2581community"]], "entropy": [[3.1122756004333496, 4.2833781242370605, 2.6207666397094727, 1.3569610118865967, 1.7553131580352783]], "avg_entropy": [2.6257389068603514], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Golf club (institution)", "score_of_biggest_cluster": 0.035955753177404404, "score_of_first_answer": 0.035955753177404404, "max_score_of_first_answer": 0.05986643582582474, "min_score_of_first_answer": 0.010810785926878452, "avg_score": 0.035955753177404404, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.017461699107356135, "std_of_first_answer": 0.017461699107356135, "avg_entropy_of_first_answer": 2.6257389068603514, "lex_sim": 1.0}
{"question": "What controversial relic went on display in 2010 in the Italian city whose name it bears?", "answer": ["Shroud"], "scores": [[0.1869547814130783, 0.969912588596344, 0.9977800250053406]], "normalized_score": [0.5655894875526428], "tokens": [["\u2581S", "hr", "oud"]], "entropy": [[4.246117115020752, 0.2528572380542755, 0.018324075266718864]], "avg_entropy": [1.5057661427805822], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Shroud of Turin", "score_of_biggest_cluster": 0.5655894875526428, "score_of_first_answer": 0.5655894875526428, "max_score_of_first_answer": 0.9977800250053406, "min_score_of_first_answer": 0.1869547814130783, "avg_score": 0.5655894875526428, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3758305022048117, "std_of_first_answer": 0.3758305022048117, "avg_entropy_of_first_answer": 1.5057661427805822, "lex_sim": 1.0}
{"question": "What colour are Hydrangea flowers in acidic soil ?", "answer": ["Magenta"], "scores": [[0.0012982964981347322, 0.9845660328865051]], "normalized_score": [0.03575273975729942], "tokens": [["\u2581Mag", "enta"]], "entropy": [[2.8287410736083984, 0.10122174024581909]], "avg_entropy": [1.4649814069271088], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "BLUE", "score_of_biggest_cluster": 0.03575273975729942, "score_of_first_answer": 0.03575273975729942, "max_score_of_first_answer": 0.9845660328865051, "min_score_of_first_answer": 0.0012982964981347322, "avg_score": 0.03575273975729942, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4916338681941852, "std_of_first_answer": 0.4916338681941852, "avg_entropy_of_first_answer": 1.4649814069271088, "lex_sim": 1.0}
{"question": "Orphelia, Autumn Leaves and Bubbles are 3 of the famous works of which artist?", "answer": ["Van Gogh"], "scores": [[0.016323482617735863, 0.93571537733078, 0.9646830558776855, 0.9984875917434692]], "normalized_score": [0.3482739329338074], "tokens": [["\u2581Van", "\u2581G", "og", "h"]], "entropy": [[5.165282249450684, 0.5329078435897827, 0.19635701179504395, 0.017120828852057457]], "avg_entropy": [1.477916983421892], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Millais", "score_of_biggest_cluster": 0.3482739329338074, "score_of_first_answer": 0.3482739329338074, "max_score_of_first_answer": 0.9984875917434692, "min_score_of_first_answer": 0.016323482617735863, "avg_score": 0.3482739329338074, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41194932162982373, "std_of_first_answer": 0.41194932162982373, "avg_entropy_of_first_answer": 1.477916983421892, "lex_sim": 1.0}
{"question": "Which bird was the emblem of Prussia?", "answer": ["Cock of Prussia"], "scores": [[0.038117341697216034, 0.04514016583561897, 0.022270215675234795, 0.4029948115348816, 0.996915340423584, 0.997802197933197]], "normalized_score": [0.15766505897045135], "tokens": [["\u2581C", "ock", "\u2581of", "\u2581Pr", "uss", "ia"]], "entropy": [[3.9057908058166504, 1.364038109779358, 1.4243117570877075, 2.4212563037872314, 0.02550872415304184, 0.01945355348289013]], "avg_entropy": [1.5267265423511465], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "EAGLE", "score_of_biggest_cluster": 0.15766505897045135, "score_of_first_answer": 0.15766505897045135, "max_score_of_first_answer": 0.997802197933197, "min_score_of_first_answer": 0.022270215675234795, "avg_score": 0.15766505897045135, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4304014195783041, "std_of_first_answer": 0.4304014195783041, "avg_entropy_of_first_answer": 1.5267265423511465, "lex_sim": 1.0}
{"question": "Mrs. O\u2019Leary\u2019s cow is credited with starting what?", "answer": ["Chicago Fire"], "scores": [[0.2968426048755646, 0.45630067586898804]], "normalized_score": [0.36803463101387024], "tokens": [["\u2581Chicago", "\u2581Fire"]], "entropy": [[1.9313082695007324, 1.1232895851135254]], "avg_entropy": [1.527298927307129], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The great Chicago fire", "score_of_biggest_cluster": 0.36803463101387024, "score_of_first_answer": 0.36803463101387024, "max_score_of_first_answer": 0.45630067586898804, "min_score_of_first_answer": 0.2968426048755646, "avg_score": 0.36803463101387024, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.07972903549671173, "std_of_first_answer": 0.07972903549671173, "avg_entropy_of_first_answer": 1.527298927307129, "lex_sim": 1.0}
{"question": "Who was the Beano\u2019s Redskin Chum?", "answer": ["Dennis the Menace"], "scores": [[0.08058793842792511, 0.7552124857902527, 0.979160487651825, 0.9964534044265747]], "normalized_score": [0.4936424791812897], "tokens": [["\u2581Dennis", "\u2581the", "\u2581Men", "ace"]], "entropy": [[5.416746139526367, 1.074018120765686, 0.1551198959350586, 0.03287893161177635]], "avg_entropy": [1.669690771959722], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Little Plum", "score_of_biggest_cluster": 0.4936424791812897, "score_of_first_answer": 0.4936424791812897, "max_score_of_first_answer": 0.9964534044265747, "min_score_of_first_answer": 0.08058793842792511, "avg_score": 0.4936424791812897, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37165250833812763, "std_of_first_answer": 0.37165250833812763, "avg_entropy_of_first_answer": 1.669690771959722, "lex_sim": 1.0}
{"question": "True or false a bird had more neck bones than a giraffe", "answer": ["True"], "scores": [[0.29716959595680237]], "normalized_score": [0.29716956615448], "tokens": [["\u2581True"]], "entropy": [[1.4283100366592407]], "avg_entropy": [1.4283100366592407], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "True (disambiguation)", "score_of_biggest_cluster": 0.29716956615448, "score_of_first_answer": 0.29716956615448, "max_score_of_first_answer": 0.29716959595680237, "min_score_of_first_answer": 0.29716959595680237, "avg_score": 0.29716956615448, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.4283100366592407, "lex_sim": 1.0}
{"question": "American singer Katheryn Hudson is better known by what name?", "answer": ["Katy Perry"], "scores": [[0.6559335589408875, 0.941351056098938, 0.9679163694381714]], "normalized_score": [0.84233158826828], "tokens": [["\u2581K", "aty", "\u2581Perry"]], "entropy": [[1.9769765138626099, 0.4030202031135559, 0.23481622338294983]], "avg_entropy": [0.8716043134530386], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Katheryn Perry", "score_of_biggest_cluster": 0.84233158826828, "score_of_first_answer": 0.84233158826828, "max_score_of_first_answer": 0.9679163694381714, "min_score_of_first_answer": 0.6559335589408875, "avg_score": 0.84233158826828, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1412256419912481, "std_of_first_answer": 0.1412256419912481, "avg_entropy_of_first_answer": 0.8716043134530386, "lex_sim": 1.0}
{"question": "What was the name of the building destroyed in the Oklahoma bombing in March 95", "answer": ["Alfred P Murrah Federal Building"], "scores": [[0.645437479019165, 0.9524235129356384, 0.1658274382352829, 0.9467796683311462, 0.5532655119895935, 0.8776105642318726]], "normalized_score": [0.6004416942596436], "tokens": [["\u2581Alfred", "\u2581P", "\u2581Mur", "rah", "\u2581Federal", "\u2581Building"]], "entropy": [[1.7107627391815186, 0.386635422706604, 1.1580768823623657, 0.38369905948638916, 1.6538811922073364, 0.6428099274635315]], "avg_entropy": [0.9893108705679575], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Murrah Federal Building", "score_of_biggest_cluster": 0.6004416942596436, "score_of_first_answer": 0.6004416942596436, "max_score_of_first_answer": 0.9524235129356384, "min_score_of_first_answer": 0.1658274382352829, "avg_score": 0.6004416942596436, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.27851939904953, "std_of_first_answer": 0.27851939904953, "avg_entropy_of_first_answer": 0.9893108705679575, "lex_sim": 1.0}
{"question": "The Leverhulme Stand and 1539 Restaurant and Bar are facilities at which racecourse?", "answer": ["Ascot Racecourse"], "scores": [[0.1172124519944191, 0.9985975623130798, 0.19450592994689941, 0.9751944541931152]], "normalized_score": [0.38600870966911316], "tokens": [["\u2581As", "cot", "\u2581Race", "course"]], "entropy": [[3.319373369216919, 0.01531586516648531, 0.968544065952301, 0.15923629701137543]], "avg_entropy": [1.1156173993367702], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "County Borough of Chester", "score_of_biggest_cluster": 0.38600870966911316, "score_of_first_answer": 0.38600870966911316, "max_score_of_first_answer": 0.9985975623130798, "min_score_of_first_answer": 0.1172124519944191, "avg_score": 0.38600870966911316, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4164982548825257, "std_of_first_answer": 0.4164982548825257, "avg_entropy_of_first_answer": 1.1156173993367702, "lex_sim": 1.0}
{"question": "\"Who wrote the 1999 novel \"\"Chocolat\"\" set in the French village of Lansquenet-sous-Tannes?\"", "answer": ["Joanne Harris"], "scores": [[0.9753826856613159, 0.9289419651031494, 0.7076553702354431]], "normalized_score": [0.8623067736625671], "tokens": [["\u2581Jo", "anne", "\u2581Harris"]], "entropy": [[0.24398769438266754, 0.48076513409614563, 1.7731224298477173]], "avg_entropy": [0.8326250861088434], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joanne Harris", "score_of_biggest_cluster": 0.8623067736625671, "score_of_first_answer": 0.8623067736625671, "max_score_of_first_answer": 0.9753826856613159, "min_score_of_first_answer": 0.7076553702354431, "avg_score": 0.8623067736625671, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.11681058409962061, "std_of_first_answer": 0.11681058409962061, "avg_entropy_of_first_answer": 0.8326250861088434, "lex_sim": 1.0}
{"question": "When is the original version of the statue Mannekin Pis thought to have been built?", "answer": ["1619"], "scores": [[0.61539626121521, 0.9733663201332092, 0.6625852584838867, 0.9236400723457336, 0.6867660880088806]], "normalized_score": [0.7589216232299805], "tokens": [["\u2581", "1", "6", "1", "9"]], "entropy": [[2.5709428787231445, 0.18394145369529724, 1.2237305641174316, 0.4564262926578522, 1.0749213695526123]], "avg_entropy": [1.1019925117492675], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, three hundred and eighty-eight", "score_of_biggest_cluster": 0.7589216232299805, "score_of_first_answer": 0.7589216232299805, "max_score_of_first_answer": 0.9733663201332092, "min_score_of_first_answer": 0.61539626121521, "avg_score": 0.7589216232299805, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14649479686145908, "std_of_first_answer": 0.14649479686145908, "avg_entropy_of_first_answer": 1.1019925117492675, "lex_sim": 1.0}
{"question": "In which state of the US is it illegal to mis-pronounce the name of the state?", "answer": ["Florida"], "scores": [[0.007506350055336952]], "normalized_score": [0.007506350055336952], "tokens": [["\u2581Florida"]], "entropy": [[4.105286598205566]], "avg_entropy": [4.105286598205566], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Arkansas, United States", "score_of_biggest_cluster": 0.007506350055336952, "score_of_first_answer": 0.007506350055336952, "max_score_of_first_answer": 0.007506350055336952, "min_score_of_first_answer": 0.007506350055336952, "avg_score": 0.007506350055336952, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.105286598205566, "lex_sim": 1.0}
{"question": "Michael J Fox starred as Marty McFly in which series of films?", "answer": ["Back to the Future"], "scores": [[0.7665390372276306, 0.8440123796463013, 0.9825404286384583, 0.8357292413711548]], "normalized_score": [0.8537383079528809], "tokens": [["\u2581Back", "\u2581to", "\u2581the", "\u2581Future"]], "entropy": [[1.513117790222168, 0.5144445896148682, 0.1257503777742386, 0.5297724008560181]], "avg_entropy": [0.6707712896168232], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marvin Berry & the Starlighters", "score_of_biggest_cluster": 0.8537383079528809, "score_of_first_answer": 0.8537383079528809, "max_score_of_first_answer": 0.9825404286384583, "min_score_of_first_answer": 0.7665390372276306, "avg_score": 0.8537383079528809, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07836539401676713, "std_of_first_answer": 0.07836539401676713, "avg_entropy_of_first_answer": 0.6707712896168232, "lex_sim": 1.0}
{"question": "Which poet is buried in Grasmere churchyard?", "answer": ["William Wordsworth"], "scores": [[0.8602607250213623, 0.9467258453369141, 0.983677864074707, 0.9985883831977844]], "normalized_score": [0.9457436800003052], "tokens": [["\u2581William", "\u2581Word", "sw", "orth"]], "entropy": [[0.9473466277122498, 0.4875105023384094, 0.10201530903577805, 0.015615183860063553]], "avg_entropy": [0.3881219057366252], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wordsworth, William", "score_of_biggest_cluster": 0.9457436800003052, "score_of_first_answer": 0.9457436800003052, "max_score_of_first_answer": 0.9985883831977844, "min_score_of_first_answer": 0.8602607250213623, "avg_score": 0.9457436800003052, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0536889457678188, "std_of_first_answer": 0.0536889457678188, "avg_entropy_of_first_answer": 0.3881219057366252, "lex_sim": 1.0}
{"question": "Which animal has the varieties Arctic, Bat-Eared and Fennec?", "answer": ["Vole"], "scores": [[0.009800394997000694, 0.19728566706180573]], "normalized_score": [0.043971315026283264], "tokens": [["\u2581V", "ole"]], "entropy": [[2.5897397994995117, 2.0519235134124756]], "avg_entropy": [2.3208316564559937], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fox (network)", "score_of_biggest_cluster": 0.043971315026283264, "score_of_first_answer": 0.043971315026283264, "max_score_of_first_answer": 0.19728566706180573, "min_score_of_first_answer": 0.009800394997000694, "avg_score": 0.043971315026283264, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09374263603240252, "std_of_first_answer": 0.09374263603240252, "avg_entropy_of_first_answer": 2.3208316564559937, "lex_sim": 1.0}
{"question": "Which of the founders of United Artists had the last name nearest the beginning of the alphabet?", "answer": ["Charlie Chaplin"], "scores": [[0.028050530701875687, 0.9900963306427002, 0.9833726286888123]], "normalized_score": [0.3011472225189209], "tokens": [["\u2581Charlie", "\u2581Chap", "lin"]], "entropy": [[4.5633649826049805, 0.09230545908212662, 0.12049467861652374]], "avg_entropy": [1.5920550401012104], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chaplin", "score_of_biggest_cluster": 0.3011472225189209, "score_of_first_answer": 0.3011472225189209, "max_score_of_first_answer": 0.9900963306427002, "min_score_of_first_answer": 0.028050530701875687, "avg_score": 0.3011472225189209, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4519362836425688, "std_of_first_answer": 0.4519362836425688, "avg_entropy_of_first_answer": 1.5920550401012104, "lex_sim": 1.0}
{"question": "Paul McCartney and Heather Mills became parents together, Michael Howard became leader of the Conservative Party and Saddam Hussein was captured. Which year?", "answer": ["2003"], "scores": [[0.6177588701248169, 0.944585919380188, 0.968635618686676, 0.9931681156158447, 0.9366994500160217]], "normalized_score": [0.8793641924858093], "tokens": [["\u2581", "2", "0", "0", "3"]], "entropy": [[2.1497578620910645, 0.3171282410621643, 0.2161519080400467, 0.06740744411945343, 0.3326111435890198]], "avg_entropy": [0.6166113197803498], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2003", "score_of_biggest_cluster": 0.8793641924858093, "score_of_first_answer": 0.8793641924858093, "max_score_of_first_answer": 0.9931681156158447, "min_score_of_first_answer": 0.6177588701248169, "avg_score": 0.8793641924858093, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1386213395699529, "std_of_first_answer": 0.1386213395699529, "avg_entropy_of_first_answer": 0.6166113197803498, "lex_sim": 1.0}
{"question": "Mark David Chapman was scheduled to appear before the New York State parole board next week for his 6th attempt at getting out of prison for the 1980 murder of whom?", "answer": ["John Lennon"], "scores": [[0.9188542366027832, 0.9656491875648499, 0.9963749051094055, 0.9982494115829468]], "normalized_score": [0.9692414402961731], "tokens": [["\u2581John", "\u2581L", "enn", "on"]], "entropy": [[0.7015705108642578, 0.3221963346004486, 0.03836456686258316, 0.01803639531135559]], "avg_entropy": [0.2700419519096613], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Winston Ono Lennon, MBE", "score_of_biggest_cluster": 0.9692414402961731, "score_of_first_answer": 0.9692414402961731, "max_score_of_first_answer": 0.9982494115829468, "min_score_of_first_answer": 0.9188542366027832, "avg_score": 0.9692414402961731, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03212589822729077, "std_of_first_answer": 0.03212589822729077, "avg_entropy_of_first_answer": 0.2700419519096613, "lex_sim": 1.0}
{"question": "Which musical instrument can have 21, 22, or 23 strings?", "answer": ["Harpsichord"], "scores": [[0.12636947631835938, 0.17430643737316132, 0.8619812726974487, 0.9879834651947021]], "normalized_score": [0.37008440494537354], "tokens": [["\u2581Har", "ps", "ich", "ord"]], "entropy": [[4.068380832672119, 0.6109934449195862, 0.6718347668647766, 0.0828314870595932]], "avg_entropy": [1.3585101328790188], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sitar", "score_of_biggest_cluster": 0.37008440494537354, "score_of_first_answer": 0.37008440494537354, "max_score_of_first_answer": 0.9879834651947021, "min_score_of_first_answer": 0.12636947631835938, "avg_score": 0.37008440494537354, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3902439037233269, "std_of_first_answer": 0.3902439037233269, "avg_entropy_of_first_answer": 1.3585101328790188, "lex_sim": 1.0}
{"question": "Elvis Presley\u2019s manager, Andreas Cornelis van Kujik, was better known by what name?", "answer": ["Colonel Tom Parker"], "scores": [[0.6820330023765564, 0.9104278087615967, 0.9839787483215332]], "normalized_score": [0.8485528230667114], "tokens": [["\u2581Colonel", "\u2581Tom", "\u2581Parker"]], "entropy": [[0.9203568696975708, 0.36007389426231384, 0.10556842386722565]], "avg_entropy": [0.46199972927570343], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Colonel Parker", "score_of_biggest_cluster": 0.8485528230667114, "score_of_first_answer": 0.8485528230667114, "max_score_of_first_answer": 0.9839787483215332, "min_score_of_first_answer": 0.6820330023765564, "avg_score": 0.8485528230667114, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1285583143895226, "std_of_first_answer": 0.1285583143895226, "avg_entropy_of_first_answer": 0.46199972927570343, "lex_sim": 1.0}
{"question": "Blinky, Pinky, Inky, and Clyde are four ghosts in which arcade game?", "answer": ["Pac-Man"], "scores": [[0.8914325833320618, 0.6146180033683777, 0.7381613254547119]], "normalized_score": [0.739517331123352], "tokens": [["\u2581Pac", "-", "Man"]], "entropy": [[0.8458094000816345, 1.229947805404663, 0.7156150341033936]], "avg_entropy": [0.9304574131965637], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pakman", "score_of_biggest_cluster": 0.739517331123352, "score_of_first_answer": 0.739517331123352, "max_score_of_first_answer": 0.8914325833320618, "min_score_of_first_answer": 0.6146180033683777, "avg_score": 0.739517331123352, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11322609760214963, "std_of_first_answer": 0.11322609760214963, "avg_entropy_of_first_answer": 0.9304574131965637, "lex_sim": 1.0}
{"question": "Which is the highest position in the Metropolitan Police?", "answer": ["Police Superintendent"], "scores": [[0.01093214564025402, 0.006879401858896017, 0.969438910484314, 0.921413242816925]], "normalized_score": [0.09053315967321396], "tokens": [["\u2581Police", "\u2581Super", "int", "endent"]], "entropy": [[2.008378505706787, 1.2320404052734375, 0.23400676250457764, 0.33858078718185425]], "avg_entropy": [0.9532516151666641], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Public Business Officials", "score_of_biggest_cluster": 0.09053315967321396, "score_of_first_answer": 0.09053315967321396, "max_score_of_first_answer": 0.969438910484314, "min_score_of_first_answer": 0.006879401858896017, "avg_score": 0.09053315967321396, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4685700914722877, "std_of_first_answer": 0.4685700914722877, "avg_entropy_of_first_answer": 0.9532516151666641, "lex_sim": 1.0}
{"question": "Arctic King, Saladin and Tom Thumb are which types of vegetable?", "answer": ["Carrots"], "scores": [[0.15131638944149017, 0.7580248713493347, 0.9996132254600525]], "normalized_score": [0.4858107268810272], "tokens": [["\u2581Car", "ro", "ts"]], "entropy": [[4.360139846801758, 0.6794404983520508, 0.0047846632078289986]], "avg_entropy": [1.6814550027872126], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "L. sativa", "score_of_biggest_cluster": 0.4858107268810272, "score_of_first_answer": 0.4858107268810272, "max_score_of_first_answer": 0.9996132254600525, "min_score_of_first_answer": 0.15131638944149017, "avg_score": 0.4858107268810272, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35684849521890166, "std_of_first_answer": 0.35684849521890166, "avg_entropy_of_first_answer": 1.6814550027872126, "lex_sim": 1.0}
{"question": "\"\"\"Because I'm worth it\"\" was the advertising slogan for which product on TV?\"", "answer": ["L'Or\u00e9al"], "scores": [[0.22146806120872498, 0.9041833281517029, 0.16194187104701996, 0.6190505623817444]], "normalized_score": [0.3764117956161499], "tokens": [["\u2581L", "'", "Or", "\u00e9al"]], "entropy": [[4.676980972290039, 0.6300021409988403, 1.173119068145752, 1.3827146291732788]], "avg_entropy": [1.9657042026519775], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "L'oreal", "score_of_biggest_cluster": 0.3764117956161499, "score_of_first_answer": 0.3764117956161499, "max_score_of_first_answer": 0.9041833281517029, "min_score_of_first_answer": 0.16194187104701996, "avg_score": 0.3764117956161499, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.30299409840843394, "std_of_first_answer": 0.30299409840843394, "avg_entropy_of_first_answer": 1.9657042026519775, "lex_sim": 1.0}
{"question": "\"Used to describe insects such as the Mayfly that only live for a short time, what word derives from the Greek for \"\"living a day\"\"?\"", "answer": ["One-separator"], "scores": [[0.008907313458621502, 0.5408439040184021, 0.0014985974412411451, 0.0008676422294229269]], "normalized_score": [0.00889633409678936], "tokens": [["\u2581One", "-", "se", "parator"]], "entropy": [[3.659383773803711, 1.6570954322814941, 1.0476417541503906, 3.733491897583008]], "avg_entropy": [2.524403214454651], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ephemeralities", "score_of_biggest_cluster": 0.00889633409678936, "score_of_first_answer": 0.00889633409678936, "max_score_of_first_answer": 0.5408439040184021, "min_score_of_first_answer": 0.0008676422294229269, "avg_score": 0.00889633409678936, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23258656760571894, "std_of_first_answer": 0.23258656760571894, "avg_entropy_of_first_answer": 2.524403214454651, "lex_sim": 1.0}
{"question": "The TV serie Mad Men is set in which decade?", "answer": ["1960s"], "scores": [[0.4801563024520874, 0.9081155061721802, 0.9950889945030212, 0.8869770765304565, 0.9865891933441162, 0.6399932503700256]], "normalized_score": [0.7899537086486816], "tokens": [["\u2581", "1", "9", "6", "0", "s"]], "entropy": [[2.2040038108825684, 0.4372173547744751, 0.046487655490636826, 0.42154592275619507, 0.09990610927343369, 1.254012942314148]], "avg_entropy": [0.7438622992485762], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sixties Revolution", "score_of_biggest_cluster": 0.7899537086486816, "score_of_first_answer": 0.7899537086486816, "max_score_of_first_answer": 0.9950889945030212, "min_score_of_first_answer": 0.4801563024520874, "avg_score": 0.7899537086486816, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19081656839321612, "std_of_first_answer": 0.19081656839321612, "avg_entropy_of_first_answer": 0.7438622992485762, "lex_sim": 1.0}
{"question": "Which root vegetable pie was named after a Minister of Food during World War II?", "answer": ["Jammy Dodger Pie HAPPY VALENTINE'S DAY, EVERYONE :) Thanks for coming to my blog and leaving your sweet comments :) As I said before, I am really busy with work"], "scores": [[0.000656417163554579, 0.07088001072406769, 0.033064909279346466, 0.9389065504074097, 0.929844319820404, 0.019140252843499184, 4.234453626850154e-06, 0.002274211496114731, 0.9774932861328125, 0.008078660815954208, 0.8180170655250549, 0.9983780384063721, 0.9931311011314392, 0.5681211948394775, 0.832499086856842, 0.9706888794898987, 0.967791736125946, 0.9947333335876465, 0.02453891560435295, 0.10719545930624008, 0.9059218168258667, 0.9942345023155212, 0.785829484462738, 0.008233917877078056, 0.0016673875506967306, 0.5804449319839478, 0.026624925434589386, 0.17079810798168182, 0.13701988756656647, 0.008423959836363792, 0.10060326755046844, 0.02202276512980461, 0.24843689799308777, 0.01702551357448101, 0.6245158314704895, 0.06296300143003464, 0.0033939999993890524, 0.09321682155132294, 0.19586947560310364, 0.2133948653936386, 0.8506574034690857, 0.5289260745048523, 0.09958662837743759, 0.020550750195980072, 0.06554292142391205, 0.25424519181251526, 0.10932499170303345]], "normalized_score": [0.09359773248434067], "tokens": [["\u2581Jam", "my", "\u2581D", "od", "ger", "\u2581Pie", "\u2581H", "AP", "PY", "\u2581V", "A", "LE", "NT", "INE", "'", "S", "\u2581DA", "Y", ",", "\u2581E", "VER", "Y", "ONE", "\u2581:)", "\u2581Thanks", "\u2581for", "\u2581coming", "\u2581to", "\u2581my", "\u2581blog", "\u2581and", "\u2581leaving", "\u2581your", "\u2581sweet", "\u2581comments", "\u2581:)", "\u2581As", "\u2581I", "\u2581said", "\u2581before", ",", "\u2581I", "\u2581am", "\u2581really", "\u2581busy", "\u2581with", "\u2581work"]], "entropy": [[4.0415849685668945, 3.6692590713500977, 2.3206331729888916, 0.44866102933883667, 0.2793472111225128, 0.638452410697937, 0.24154528975486755, 5.107066631317139, 0.19493228197097778, 3.468047857284546, 1.1553995609283447, 0.017951540648937225, 0.053338926285505295, 0.8337076902389526, 1.0176689624786377, 0.15872317552566528, 0.2622106075286865, 0.0444384403526783, 2.7690045833587646, 4.974369049072266, 0.670735239982605, 0.05645694583654404, 0.6082762479782104, 1.9037898778915405, 2.332423210144043, 2.1816258430480957, 3.2989580631256104, 2.9441077709198, 4.06883430480957, 4.955405235290527, 2.8227829933166504, 4.627525806427002, 2.3686792850494385, 3.1596691608428955, 1.9760292768478394, 2.4151558876037598, 3.935568332672119, 2.951530933380127, 3.318892002105713, 2.2683138847351074, 0.9388154149055481, 2.7561118602752686, 3.2244369983673096, 4.582921981811523, 4.336111068725586, 3.1468682289123535, 3.04579496383667]], "avg_entropy": [2.267918367652183], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "WOOLTON (Pie)", "score_of_biggest_cluster": 0.09359773248434067, "score_of_first_answer": 0.09359773248434067, "max_score_of_first_answer": 0.9983780384063721, "min_score_of_first_answer": 4.234453626850154e-06, "avg_score": 0.09359773248434067, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3939003037990804, "std_of_first_answer": 0.3939003037990804, "avg_entropy_of_first_answer": 2.267918367652183, "lex_sim": 1.0}
{"question": "The Royal Armouries has three locations, London, Leeds and which other city?", "answer": ["Sheffield"], "scores": [[0.0054193465039134026, 0.985313355922699, 0.9963175058364868]], "normalized_score": [0.17457135021686554], "tokens": [["\u2581She", "ff", "ield"]], "entropy": [[4.593767166137695, 0.1405605673789978, 0.0346139520406723]], "avg_entropy": [1.5896472285191219], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Portsmouth UA", "score_of_biggest_cluster": 0.17457135021686554, "score_of_first_answer": 0.17457135021686554, "max_score_of_first_answer": 0.9963175058364868, "min_score_of_first_answer": 0.0054193465039134026, "avg_score": 0.17457135021686554, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46454189180534866, "std_of_first_answer": 0.46454189180534866, "avg_entropy_of_first_answer": 1.5896472285191219, "lex_sim": 1.0}
{"question": "Who was the first presenter of 'Top of the Pops'?", "answer": ["Brian Matthew"], "scores": [[0.023834777995944023, 0.7934969067573547]], "normalized_score": [0.13752390444278717], "tokens": [["\u2581Brian", "\u2581Matthew"]], "entropy": [[2.4883294105529785, 0.8833048343658447]], "avg_entropy": [1.6858171224594116], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sir Jimmy Savile", "score_of_biggest_cluster": 0.13752390444278717, "score_of_first_answer": 0.13752390444278717, "max_score_of_first_answer": 0.7934969067573547, "min_score_of_first_answer": 0.023834777995944023, "avg_score": 0.13752390444278717, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38483106438070536, "std_of_first_answer": 0.38483106438070536, "avg_entropy_of_first_answer": 1.6858171224594116, "lex_sim": 1.0}
{"question": "Which British comedy-drama series (1979 to 1994) starred George Cole as Arthur Daley, an unscrupulous importer-exporter, wholesaler, and used-car salesman and Dennis Waterman as Terry McCann, his bodyguard?", "answer": ["Minder"], "scores": [[0.8685713410377502, 0.9977771639823914]], "normalized_score": [0.9309353828430176], "tokens": [["\u2581M", "inder"]], "entropy": [[0.9314942955970764, 0.022955570369958878]], "avg_entropy": [0.47722493298351765], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Minder", "score_of_biggest_cluster": 0.9309353828430176, "score_of_first_answer": 0.9309353828430176, "max_score_of_first_answer": 0.9977771639823914, "min_score_of_first_answer": 0.8685713410377502, "avg_score": 0.9309353828430176, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06460291147232056, "std_of_first_answer": 0.06460291147232056, "avg_entropy_of_first_answer": 0.47722493298351765, "lex_sim": 1.0}
{"question": "Near Field Communication technology (NFC) is a set of standards for?", "answer": ["Data over short distances"], "scores": [[0.03098062239587307, 0.00030534950201399624, 0.43572771549224854, 0.8277008533477783]], "normalized_score": [0.04297778010368347], "tokens": [["\u2581Data", "\u2581over", "\u2581short", "\u2581distances"]], "entropy": [[4.728802680969238, 2.644655704498291, 3.069502353668213, 0.7840903997421265]], "avg_entropy": [2.806762784719467], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "SmartPhone", "score_of_biggest_cluster": 0.04297778010368347, "score_of_first_answer": 0.04297778010368347, "max_score_of_first_answer": 0.8277008533477783, "min_score_of_first_answer": 0.00030534950201399624, "avg_score": 0.04297778010368347, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3379481474218869, "std_of_first_answer": 0.3379481474218869, "avg_entropy_of_first_answer": 2.806762784719467, "lex_sim": 1.0}
{"question": "In what year in the 20th century was the Miss World contest held for the first time?", "answer": ["1951"], "scores": [[0.5370876789093018, 0.9842216968536377, 0.9877749681472778, 0.7741351127624512, 0.9686052799224854]], "normalized_score": [0.8289950489997864], "tokens": [["\u2581", "1", "9", "5", "1"]], "entropy": [[2.3081789016723633, 0.1218683123588562, 0.1005825623869896, 0.7187756896018982, 0.1912417709827423]], "avg_entropy": [0.6881294474005699], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and fifty-one", "score_of_biggest_cluster": 0.8289950489997864, "score_of_first_answer": 0.8289950489997864, "max_score_of_first_answer": 0.9877749681472778, "min_score_of_first_answer": 0.5370876789093018, "avg_score": 0.8289950489997864, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.17591677239703452, "std_of_first_answer": 0.17591677239703452, "avg_entropy_of_first_answer": 0.6881294474005699, "lex_sim": 1.0}
{"question": "Which \u2018A\u2019 means in the open air?", "answer": ["Astral"], "scores": [[0.1782795935869217, 0.006235873326659203, 0.6615633964538574]], "normalized_score": [0.09026584029197693], "tokens": [["\u2581A", "str", "al"]], "entropy": [[5.028244495391846, 5.354006767272949, 1.8406273126602173]], "avg_entropy": [4.07429285844167], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Al-fresco", "score_of_biggest_cluster": 0.09026584029197693, "score_of_first_answer": 0.09026584029197693, "max_score_of_first_answer": 0.6615633964538574, "min_score_of_first_answer": 0.006235873326659203, "avg_score": 0.09026584029197693, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27741193564856403, "std_of_first_answer": 0.27741193564856403, "avg_entropy_of_first_answer": 4.07429285844167, "lex_sim": 1.0}
{"question": "Levi Stubbs was the lead singer of which 'Motown' group from 1953 until 2000?", "answer": ["Incredible String Band"], "scores": [[0.0004040751082357019, 0.06116202101111412, 0.6259753704071045, 0.06263047456741333, 0.9647971391677856]], "normalized_score": [0.06225075572729111], "tokens": [["\u2581In", "cred", "ible", "\u2581String", "\u2581Band"]], "entropy": [[1.122053623199463, 4.4452595710754395, 1.1032105684280396, 4.150943756103516, 0.2383546233177185]], "avg_entropy": [2.211964428424835], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Roquel Payton", "score_of_biggest_cluster": 0.06225075572729111, "score_of_first_answer": 0.06225075572729111, "max_score_of_first_answer": 0.9647971391677856, "min_score_of_first_answer": 0.0004040751082357019, "avg_score": 0.06225075572729111, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3852579466506437, "std_of_first_answer": 0.3852579466506437, "avg_entropy_of_first_answer": 2.211964428424835, "lex_sim": 1.0}
{"question": "Which hymn traditionally closes The Last Night of the Proms?", "answer": ["Land Of Hope And Glory"], "scores": [[0.12048473209142685, 0.03021334856748581, 0.9826233983039856, 0.7547327280044556, 0.9969612956047058, 0.9979100823402405]], "normalized_score": [0.3728322684764862], "tokens": [["\u2581Land", "\u2581Of", "\u2581Hope", "\u2581And", "\u2581Gl", "ory"]], "entropy": [[1.4553380012512207, 0.18802502751350403, 0.11929626762866974, 0.7415875196456909, 0.03208089619874954, 0.01952562853693962]], "avg_entropy": [0.42597555679579574], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Neighbourhoods of Jerusalem", "score_of_biggest_cluster": 0.3728322684764862, "score_of_first_answer": 0.3728322684764862, "max_score_of_first_answer": 0.9979100823402405, "min_score_of_first_answer": 0.03021334856748581, "avg_score": 0.3728322684764862, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.413824471328798, "std_of_first_answer": 0.413824471328798, "avg_entropy_of_first_answer": 0.42597555679579574, "lex_sim": 1.0}
{"question": "Which Hollywood film star (1899 -1957) had the middle name De Forest?", "answer": ["John De Forest"], "scores": [[0.15696348249912262, 0.05536545440554619, 0.553770124912262]], "normalized_score": [0.16883234679698944], "tokens": [["\u2581John", "\u2581De", "\u2581Forest"]], "entropy": [[4.326404571533203, 2.9865286350250244, 2.469564437866211]], "avg_entropy": [3.2608325481414795], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Humphrey boggart", "score_of_biggest_cluster": 0.16883234679698944, "score_of_first_answer": 0.16883234679698944, "max_score_of_first_answer": 0.553770124912262, "min_score_of_first_answer": 0.05536545440554619, "avg_score": 0.16883234679698944, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2150413103358412, "std_of_first_answer": 0.2150413103358412, "avg_entropy_of_first_answer": 3.2608325481414795, "lex_sim": 1.0}
{"question": "Which country has the greatest number of ancient pyramids? (hint: They are called the Nubian Pyramids)", "answer": ["Sudan"], "scores": [[0.32331979274749756, 0.9955236911773682]], "normalized_score": [0.5673381090164185], "tokens": [["\u2581Sud", "an"]], "entropy": [[1.7690589427947998, 0.05066836625337601]], "avg_entropy": [0.9098636545240879], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sudan", "score_of_biggest_cluster": 0.5673381090164185, "score_of_first_answer": 0.5673381090164185, "max_score_of_first_answer": 0.9955236911773682, "min_score_of_first_answer": 0.32331979274749756, "avg_score": 0.5673381090164185, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3361019492149353, "std_of_first_answer": 0.3361019492149353, "avg_entropy_of_first_answer": 0.9098636545240879, "lex_sim": 1.0}
{"question": "What used to be known as the Blessed or Fortunate Isles?", "answer": ["Azores"], "scores": [[0.026223376393318176, 0.9742701053619385]], "normalized_score": [0.1598394513130188], "tokens": [["\u2581Az", "ores"]], "entropy": [[3.7753090858459473, 0.21403050422668457]], "avg_entropy": [1.994669795036316], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Canary islands", "score_of_biggest_cluster": 0.1598394513130188, "score_of_first_answer": 0.1598394513130188, "max_score_of_first_answer": 0.9742701053619385, "min_score_of_first_answer": 0.026223376393318176, "avg_score": 0.1598394513130188, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.47402336448431015, "std_of_first_answer": 0.47402336448431015, "avg_entropy_of_first_answer": 1.994669795036316, "lex_sim": 1.0}
{"question": "The town of Keswick in Cumbria lies just north of which body of water?", "answer": ["Windermere"], "scores": [[0.14286468923091888, 0.9942405223846436, 0.9902133345603943]], "normalized_score": [0.5200539231300354], "tokens": [["\u2581Wind", "erm", "ere"]], "entropy": [[2.8843164443969727, 0.05898374319076538, 0.06955625861883163]], "avg_entropy": [1.0042854820688565], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Derwentwater", "score_of_biggest_cluster": 0.5200539231300354, "score_of_first_answer": 0.5200539231300354, "max_score_of_first_answer": 0.9942405223846436, "min_score_of_first_answer": 0.14286468923091888, "avg_score": 0.5200539231300354, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4003965748357312, "std_of_first_answer": 0.4003965748357312, "avg_entropy_of_first_answer": 1.0042854820688565, "lex_sim": 1.0}
{"question": "The hamburger is named after?", "answer": ["Hamburger, Germany"], "scores": [[0.03486132249236107, 0.7005670666694641, 0.1760179102420807, 0.7232509255409241]], "normalized_score": [0.23613470792770386], "tokens": [["\u2581Hamb", "urger", ",", "\u2581Germany"]], "entropy": [[4.873186111450195, 1.7260934114456177, 1.8397924900054932, 1.8311916589736938]], "avg_entropy": [2.56756591796875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "DEHAM", "score_of_biggest_cluster": 0.23613470792770386, "score_of_first_answer": 0.23613470792770386, "max_score_of_first_answer": 0.7232509255409241, "min_score_of_first_answer": 0.03486132249236107, "avg_score": 0.23613470792770386, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30741867966137987, "std_of_first_answer": 0.30741867966137987, "avg_entropy_of_first_answer": 2.56756591796875, "lex_sim": 1.0}
{"question": "\"What 1998 film diected by Peter Weir , was advertised with the tag line \"\"On The Air, Unaware\"\"?\"", "answer": ["The Truman Show You are sitting in a bar enjoying a glass of wine."], "scores": [[0.8334778547286987, 0.996385931968689, 0.991705060005188, 0.9700873494148254, 9.868795132206287e-07, 0.22883421182632446, 0.0006595713784918189, 0.5355148911476135, 0.5174471139907837, 0.04866977035999298, 0.008901010267436504, 0.9982130527496338, 0.7521535158157349, 0.023308157920837402, 0.9844304919242859, 0.3395426869392395, 0.3762379586696625, 0.1337757259607315, 0.051397062838077545, 0.01901407726109028, 0.9439375996589661, 0.004906628746539354, 0.891875684261322, 0.723501980304718, 0.29535728693008423, 0.10335858166217804, 0.03850999101996422, 0.25971871614456177, 0.1658138930797577, 0.3197513520717621, 0.6125819683074951, 0.01058582216501236, 0.002440399955958128, 0.014761673286557198, 0.0430731326341629, 0.03835511580109596, 0.38119766116142273, 0.0004587004368659109, 0.2595326602458954, 0.1072588860988617, 0.11654126644134521, 0.7267414331436157, 0.2167188674211502, 0.022204730659723282, 0.06169533729553223, 0.23777836561203003, 0.005753968842327595]], "normalized_score": [0.08679374307394028], "tokens": [["\u2581The", "\u2581Tr", "uman", "\u2581Show", "\u2581You", "\u2581are", "\u2581sitting", "\u2581in", "\u2581a", "\u2581bar", "\u2581enjo", "ying", "\u2581a", "\u2581glass", "\u2581of", "\u2581wine", ".", "\u2581A", "\u2581stranger", "\u2581joins", "\u2581you", "\u2581for", "\u2581a", "\u2581drink", "\u2581and", "\u2581starts", "\u2581asking", "\u2581questions", "\u2581about", "\u2581your", "\u2581life", ":", "\u2581a", "\u2581very", "\u2581unusual", "\u2581conversation", ".", "\u2581Assuming", "\u2581that", "\u2581this", "\u2581stranger", "\u2581is", "\u2581a", "\u2581real", "\u2581human", ",", "\u2581are"]], "entropy": [[1.2556687593460083, 0.042298804968595505, 0.060147494077682495, 0.1972377598285675, 0.248533695936203, 3.954239845275879, 2.0835747718811035, 1.957821011543274, 1.9286997318267822, 5.3028459548950195, 3.1496171951293945, 0.019888482987880707, 1.0933926105499268, 2.7106075286865234, 0.13388490676879883, 3.5749783515930176, 2.043914794921875, 3.2750353813171387, 4.511667728424072, 2.9145524501800537, 0.3446834683418274, 1.448664903640747, 0.6634852886199951, 1.468775987625122, 1.266916275024414, 4.00556755065918, 2.4400253295898438, 2.045574426651001, 1.5354398488998413, 2.871082305908203, 2.494215250015259, 1.3025702238082886, 3.4352893829345703, 5.964341163635254, 4.410845756530762, 4.087503433227539, 2.943681478500366, 3.8850417137145996, 2.1393227577209473, 2.2223992347717285, 2.463409900665283, 1.649425745010376, 2.9782238006591797, 5.26521110534668, 1.657468557357788, 1.6035377979278564, 3.088277816772461]], "avg_entropy": [2.3433954843339766], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "In case I don't see you... good afternoon, good evening, and good night", "score_of_biggest_cluster": 0.08679374307394028, "score_of_first_answer": 0.08679374307394028, "max_score_of_first_answer": 0.9982130527496338, "min_score_of_first_answer": 9.868795132206287e-07, "avg_score": 0.08679374307394028, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34931129425796775, "std_of_first_answer": 0.34931129425796775, "avg_entropy_of_first_answer": 2.3433954843339766, "lex_sim": 1.0}
{"question": "Orange juice, galliano and which spirit are mixed together to make a Harvey Wallbanger cocktail?", "answer": ["Vodka"], "scores": [[0.4060906767845154, 0.9928559064865112, 0.9926570057868958]], "normalized_score": [0.7369468212127686], "tokens": [["\u2581V", "od", "ka"]], "entropy": [[3.194000244140625, 0.0749906599521637, 0.07310125231742859]], "avg_entropy": [1.1140307188034058], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eric Harris & Dylan Klebold", "score_of_biggest_cluster": 0.7369468212127686, "score_of_first_answer": 0.7369468212127686, "max_score_of_first_answer": 0.9928559064865112, "min_score_of_first_answer": 0.4060906767845154, "avg_score": 0.7369468212127686, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2765569125008556, "std_of_first_answer": 0.2765569125008556, "avg_entropy_of_first_answer": 1.1140307188034058, "lex_sim": 1.0}
{"question": "What is the traditional Maori greeting in which people press noses together?", "answer": ["M\u0101ori customs"], "scores": [[0.025727393105626106, 0.016678929328918457, 0.6672210693359375, 0.013510243035852909, 0.16615267097949982]], "normalized_score": [0.05775650218129158], "tokens": [["\u2581M", "\u0101", "ori", "\u2581custom", "s"]], "entropy": [[3.3716330528259277, 3.51127552986145, 1.6249102354049683, 3.3885140419006348, 1.7746808528900146]], "avg_entropy": [2.734202742576599], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Maori greeting", "score_of_biggest_cluster": 0.05775650218129158, "score_of_first_answer": 0.05775650218129158, "max_score_of_first_answer": 0.6672210693359375, "min_score_of_first_answer": 0.013510243035852909, "avg_score": 0.05775650218129158, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2512949772490248, "std_of_first_answer": 0.2512949772490248, "avg_entropy_of_first_answer": 2.734202742576599, "lex_sim": 1.0}
{"question": "Originating in early central American culture, where on the body would a huarache be worn?", "answer": ["The sole of a moccasin-type shoe"], "scores": [[0.09425412118434906, 0.09312029927968979, 0.3182823657989502, 0.02603662759065628, 0.04280243068933487, 0.7297841906547546, 0.9539161324501038, 0.9894192218780518, 0.041319139301776886, 0.487959086894989, 0.6650702357292175, 0.999548614025116]], "normalized_score": [0.235256627202034], "tokens": [["\u2581The", "\u2581sole", "\u2581of", "\u2581a", "\u2581m", "oc", "cas", "in", "-", "type", "\u2581sh", "oe"]], "entropy": [[3.96224308013916, 3.7186529636383057, 0.882550835609436, 0.3572797477245331, 1.9429800510406494, 0.752799391746521, 0.22359022498130798, 0.0792946070432663, 1.0380371809005737, 1.4104812145233154, 1.261379361152649, 0.005682617891579866]], "avg_entropy": [1.3029142730326082], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Foot fracture", "score_of_biggest_cluster": 0.235256627202034, "score_of_first_answer": 0.235256627202034, "max_score_of_first_answer": 0.999548614025116, "min_score_of_first_answer": 0.02603662759065628, "avg_score": 0.235256627202034, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3825988503249045, "std_of_first_answer": 0.3825988503249045, "avg_entropy_of_first_answer": 1.3029142730326082, "lex_sim": 1.0}
{"question": "Which of Robin Hood's Merry Men was he forced to fight with quarterstaffs on a narrow bridge?", "answer": ["Much"], "scores": [[0.1919875591993332]], "normalized_score": [0.1919875591993332], "tokens": [["\u2581Much"]], "entropy": [[2.3573460578918457]], "avg_entropy": [2.3573460578918457], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Little John", "score_of_biggest_cluster": 0.1919875591993332, "score_of_first_answer": 0.1919875591993332, "max_score_of_first_answer": 0.1919875591993332, "min_score_of_first_answer": 0.1919875591993332, "avg_score": 0.1919875591993332, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.3573460578918457, "lex_sim": 1.0}
{"question": "What was J. D. Salinger's most famous novel?", "answer": ["The Great Gatsby"], "scores": [[0.5290569067001343, 0.0010920437052845955, 0.9120854735374451, 0.9725512862205505, 0.9970284104347229]], "normalized_score": [0.2196238934993744], "tokens": [["\u2581The", "\u2581Great", "\u2581G", "ats", "by"]], "entropy": [[1.5599777698516846, 0.4666345715522766, 0.7789425849914551, 0.2273731529712677, 0.028410425409674644]], "avg_entropy": [0.6122677009552717], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mr Antolini", "score_of_biggest_cluster": 0.2196238934993744, "score_of_first_answer": 0.2196238934993744, "max_score_of_first_answer": 0.9970284104347229, "min_score_of_first_answer": 0.0010920437052845955, "avg_score": 0.2196238934993744, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.380428403504592, "std_of_first_answer": 0.380428403504592, "avg_entropy_of_first_answer": 0.6122677009552717, "lex_sim": 1.0}
{"question": "Which actor and producer married Anne Bancroft?", "answer": ["Mel Brooks"], "scores": [[0.7555280327796936, 0.6952360272407532, 0.993353009223938]], "normalized_score": [0.8050610423088074], "tokens": [["\u2581Mel", "\u2581Bro", "oks"]], "entropy": [[1.437010645866394, 1.4137048721313477, 0.06238514930009842]], "avg_entropy": [0.9710335557659467], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mel Brookes", "score_of_biggest_cluster": 0.8050610423088074, "score_of_first_answer": 0.8050610423088074, "max_score_of_first_answer": 0.993353009223938, "min_score_of_first_answer": 0.6952360272407532, "avg_score": 0.8050610423088074, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1286984331681261, "std_of_first_answer": 0.1286984331681261, "avg_entropy_of_first_answer": 0.9710335557659467, "lex_sim": 1.0}
{"question": "In the 1972 film Cabaret, Sally Bowles is working in which club?", "answer": ["The Kit Kat Club"], "scores": [[0.32868555188179016, 0.9487850666046143, 0.7772532105445862, 0.5500679016113281]], "normalized_score": [0.6042711138725281], "tokens": [["\u2581The", "\u2581Kit", "\u2581Kat", "\u2581Club"]], "entropy": [[2.8898956775665283, 0.4929613173007965, 0.770352840423584, 1.130524754524231]], "avg_entropy": [1.320933647453785], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kit-Kat", "score_of_biggest_cluster": 0.6042711138725281, "score_of_first_answer": 0.6042711138725281, "max_score_of_first_answer": 0.9487850666046143, "min_score_of_first_answer": 0.32868555188179016, "avg_score": 0.6042711138725281, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23382127485916696, "std_of_first_answer": 0.23382127485916696, "avg_entropy_of_first_answer": 1.320933647453785, "lex_sim": 1.0}
{"question": "Who is featured on Puff Daddy's Can't Hold Me Down?", "answer": ["J."], "scores": [[0.007985809817910194, 0.07941292226314545, 0.276712030172348]], "normalized_score": [0.055986009538173676], "tokens": [["\u2581J", ".", "\u2581Lo"]], "entropy": [[4.3271484375, 3.4142696857452393, 1.8792104721069336]], "avg_entropy": [3.206876198450724], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mase", "score_of_biggest_cluster": 0.055986009538173676, "score_of_first_answer": 0.055986009538173676, "max_score_of_first_answer": 0.276712030172348, "min_score_of_first_answer": 0.007985809817910194, "avg_score": 0.055986009538173676, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11364787326553567, "std_of_first_answer": 0.11364787326553567, "avg_entropy_of_first_answer": 3.206876198450724, "lex_sim": 1.0}
{"question": "Which country suspended its annual Antarctic Whale Hunt in February 2011, after activists from a US-based environmental group chased the mother ship?", "answer": ["Chile"], "scores": [[0.010382014326751232]], "normalized_score": [0.010382014326751232], "tokens": [["\u2581Chile"]], "entropy": [[2.28108286857605]], "avg_entropy": [2.28108286857605], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u65e5\u672c\u570b", "score_of_biggest_cluster": 0.010382014326751232, "score_of_first_answer": 0.010382014326751232, "max_score_of_first_answer": 0.010382014326751232, "min_score_of_first_answer": 0.010382014326751232, "avg_score": 0.010382014326751232, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.28108286857605, "lex_sim": 1.0}
{"question": "Who was the only female candidate in the 2010 Labour leadership contest?", "answer": ["Ed Miliband"], "scores": [[0.2014804482460022, 0.4055883586406708, 0.9919096827507019, 0.9961718916893005]], "normalized_score": [0.5330662727355957], "tokens": [["\u2581Ed", "\u2581Mil", "ib", "and"]], "entropy": [[2.7714462280273438, 2.3286242485046387, 0.07301943004131317, 0.04238155484199524]], "avg_entropy": [1.3038678653538227], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Diane Julie Abbott", "score_of_biggest_cluster": 0.5330662727355957, "score_of_first_answer": 0.5330662727355957, "max_score_of_first_answer": 0.9961718916893005, "min_score_of_first_answer": 0.2014804482460022, "avg_score": 0.5330662727355957, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3527173687216451, "std_of_first_answer": 0.3527173687216451, "avg_entropy_of_first_answer": 1.3038678653538227, "lex_sim": 1.0}
{"question": "Thomas Edison, who invented the lightbulb, was afraid of the dark?", "answer": ["Thomas Alva Edison"], "scores": [[0.013479369692504406, 0.06458420306444168, 0.9902503490447998, 0.9334802031517029, 0.9932979941368103]], "normalized_score": [0.24018460512161255], "tokens": [["\u2581Thomas", "\u2581Al", "va", "\u2581Ed", "ison"]], "entropy": [[3.2637739181518555, 1.1446729898452759, 0.09162120521068573, 0.3839716911315918, 0.05472276732325554]], "avg_entropy": [0.9877525143325329], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "True (disambiguation)", "score_of_biggest_cluster": 0.24018460512161255, "score_of_first_answer": 0.24018460512161255, "max_score_of_first_answer": 0.9932979941368103, "min_score_of_first_answer": 0.013479369692504406, "avg_score": 0.24018460512161255, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4580086027447877, "std_of_first_answer": 0.4580086027447877, "avg_entropy_of_first_answer": 0.9877525143325329, "lex_sim": 1.0}
{"question": "Which mythical beings were said to inhabit the slopes of Mount Etna?", "answer": ["Elysian Mountains"], "scores": [[0.06037691980600357, 0.08154641091823578, 0.6192598938941956, 0.9650217890739441, 0.0016628130106255412]], "normalized_score": [0.0866774171590805], "tokens": [["\u2581E", "ly", "s", "ian", "\u2581Mountains"]], "entropy": [[4.281213283538818, 3.783050060272217, 1.988851547241211, 0.270865261554718, 3.1893577575683594]], "avg_entropy": [2.7026675820350645], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cyclops (film)", "score_of_biggest_cluster": 0.0866774171590805, "score_of_first_answer": 0.0866774171590805, "max_score_of_first_answer": 0.9650217890739441, "min_score_of_first_answer": 0.0016628130106255412, "avg_score": 0.0866774171590805, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.381560579407325, "std_of_first_answer": 0.381560579407325, "avg_entropy_of_first_answer": 2.7026675820350645, "lex_sim": 1.0}
{"question": "Peter Parker alias Spiderman worked for which newspaper?", "answer": ["Daily Sun"], "scores": [[0.5908371210098267, 0.0024693028535693884]], "normalized_score": [0.03819627687335014], "tokens": [["\u2581Daily", "\u2581Sun"]], "entropy": [[1.3578598499298096, 0.9619710445404053]], "avg_entropy": [1.1599154472351074], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "DB (comics)", "score_of_biggest_cluster": 0.03819627687335014, "score_of_first_answer": 0.03819627687335014, "max_score_of_first_answer": 0.5908371210098267, "min_score_of_first_answer": 0.0024693028535693884, "avg_score": 0.03819627687335014, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29418390907812864, "std_of_first_answer": 0.29418390907812864, "avg_entropy_of_first_answer": 1.1599154472351074, "lex_sim": 1.0}
{"question": "Who played boxer Jim Braddock in the 2005 film \u2018Cinderella man\u2019?", "answer": ["Gosling Ryan"], "scores": [[0.00018501248268876225, 0.05436217039823532, 0.630096435546875, 0.05370442941784859]], "normalized_score": [0.02415342628955841], "tokens": [["\u2581G", "os", "ling", "\u2581Ryan"]], "entropy": [[0.6956560611724854, 5.256562232971191, 2.273791790008545, 1.7230923175811768]], "avg_entropy": [2.4872756004333496], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Russel crow", "score_of_biggest_cluster": 0.02415342628955841, "score_of_first_answer": 0.02415342628955841, "max_score_of_first_answer": 0.630096435546875, "min_score_of_first_answer": 0.00018501248268876225, "avg_score": 0.02415342628955841, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25815281505758836, "std_of_first_answer": 0.25815281505758836, "avg_entropy_of_first_answer": 2.4872756004333496, "lex_sim": 1.0}
{"question": "Which chain of European retailers began with a store near a crossroads in Annecy, France in 1958?", "answer": ["Auchan"], "scores": [[0.04077813774347305, 0.9767128825187683]], "normalized_score": [0.19957087934017181], "tokens": [["\u2581Auch", "an"]], "entropy": [[2.6292479038238525, 0.19409683346748352]], "avg_entropy": [1.411672368645668], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1Mobile", "score_of_biggest_cluster": 0.19957087934017181, "score_of_first_answer": 0.19957087934017181, "max_score_of_first_answer": 0.9767128825187683, "min_score_of_first_answer": 0.04077813774347305, "avg_score": 0.19957087934017181, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46796737238764763, "std_of_first_answer": 0.46796737238764763, "avg_entropy_of_first_answer": 1.411672368645668, "lex_sim": 1.0}
{"question": "Who wrote the novel \u2018The Prime of Miss Jean Brodie\u2019?", "answer": ["Muriel Spark"], "scores": [[0.9363448619842529, 0.9837759137153625, 0.9277705550193787]], "normalized_score": [0.9489810466766357], "tokens": [["\u2581Mur", "iel", "\u2581Spark"]], "entropy": [[0.561130166053772, 0.11514753103256226, 0.47143805027008057]], "avg_entropy": [0.3825719157854716], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Muriel Sarah Maud Spark", "score_of_biggest_cluster": 0.9489810466766357, "score_of_first_answer": 0.9489810466766357, "max_score_of_first_answer": 0.9837759137153625, "min_score_of_first_answer": 0.9277705550193787, "avg_score": 0.9489810466766357, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.024630206411624157, "std_of_first_answer": 0.024630206411624157, "avg_entropy_of_first_answer": 0.3825719157854716, "lex_sim": 1.0}
{"question": "Which unit of measurement is equal to one-tenth of a nautical mile?", "answer": ["fathom"], "scores": [[0.0523572601377964, 0.8770730495452881, 0.9184334874153137]], "normalized_score": [0.3480862081050873], "tokens": [["\u2581f", "ath", "om"]], "entropy": [[4.276027679443359, 0.4069806933403015, 0.3916440010070801]], "avg_entropy": [1.6915507912635803], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Data Cable", "score_of_biggest_cluster": 0.3480862081050873, "score_of_first_answer": 0.3480862081050873, "max_score_of_first_answer": 0.9184334874153137, "min_score_of_first_answer": 0.0523572601377964, "avg_score": 0.3480862081050873, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3988810530825338, "std_of_first_answer": 0.3988810530825338, "avg_entropy_of_first_answer": 1.6915507912635803, "lex_sim": 1.0}
{"question": "What is the more familiar name for the aromatic gum resin olibanum?", "answer": ["Frankincense"], "scores": [[0.8684864640235901, 0.9930156469345093, 0.9887883067131042]], "normalized_score": [0.9482892155647278], "tokens": [["\u2581Frank", "inc", "ense"]], "entropy": [[0.8437093496322632, 0.07358314096927643, 0.08787471055984497]], "avg_entropy": [0.33505573372046155], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Frankinsense", "score_of_biggest_cluster": 0.9482892155647278, "score_of_first_answer": 0.9482892155647278, "max_score_of_first_answer": 0.9930156469345093, "min_score_of_first_answer": 0.8684864640235901, "avg_score": 0.9482892155647278, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05773302651602584, "std_of_first_answer": 0.05773302651602584, "avg_entropy_of_first_answer": 0.33505573372046155, "lex_sim": 1.0}
{"question": "What British ship was sunk by Gunther Prien, captain of submarine U47, on the night of 13 and 14 October 1939?", "answer": ["HMS Royal Oak"], "scores": [[0.2640818953514099, 0.9074395298957825, 0.9967278242111206]], "normalized_score": [0.6204559803009033], "tokens": [["\u2581HMS", "\u2581Royal", "\u2581Oak"]], "entropy": [[3.1292405128479004, 0.7367027401924133, 0.035281047224998474]], "avg_entropy": [1.3004081000884373], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Royal Oak", "score_of_biggest_cluster": 0.6204559803009033, "score_of_first_answer": 0.6204559803009033, "max_score_of_first_answer": 0.9967278242111206, "min_score_of_first_answer": 0.2640818953514099, "avg_score": 0.6204559803009033, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3263691676789374, "std_of_first_answer": 0.3263691676789374, "avg_entropy_of_first_answer": 1.3004081000884373, "lex_sim": 1.0}
{"question": "What was the capital of the United States between 1785 and 1790??", "answer": ["New York"], "scores": [[0.4319147765636444, 0.9257779121398926]], "normalized_score": [0.6323425769805908], "tokens": [["\u2581New", "\u2581York"]], "entropy": [[2.5585153102874756, 0.46216171979904175]], "avg_entropy": [1.5103385150432587], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "New Yourk State", "score_of_biggest_cluster": 0.6323425769805908, "score_of_first_answer": 0.6323425769805908, "max_score_of_first_answer": 0.9257779121398926, "min_score_of_first_answer": 0.4319147765636444, "avg_score": 0.6323425769805908, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24693156778812408, "std_of_first_answer": 0.24693156778812408, "avg_entropy_of_first_answer": 1.5103385150432587, "lex_sim": 1.0}
{"question": "Which Pacific nation consists of over 800 islands and islets, the largest being Viti Levu?", "answer": ["Fijian islands"], "scores": [[0.8929992318153381, 0.028577590361237526, 0.20770147442817688, 0.05127444863319397]], "normalized_score": [0.12839683890342712], "tokens": [["\u2581F", "ij", "ian", "\u2581islands"]], "entropy": [[0.8115530610084534, 0.2833872139453888, 3.557342052459717, 1.6331095695495605]], "avg_entropy": [1.5713479742407799], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fiji (disambiguation)", "score_of_biggest_cluster": 0.12839683890342712, "score_of_first_answer": 0.12839683890342712, "max_score_of_first_answer": 0.8929992318153381, "min_score_of_first_answer": 0.028577590361237526, "avg_score": 0.12839683890342712, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35199683314758046, "std_of_first_answer": 0.35199683314758046, "avg_entropy_of_first_answer": 1.5713479742407799, "lex_sim": 1.0}
{"question": "Which English town/city did the Romans call Dubris?", "answer": ["Dover"], "scores": [[0.3367786407470703, 0.9818337559700012]], "normalized_score": [0.5750309824943542], "tokens": [["\u2581D", "over"]], "entropy": [[2.9135711193084717, 0.17257268726825714]], "avg_entropy": [1.5430719032883644], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dover RFC", "score_of_biggest_cluster": 0.5750309824943542, "score_of_first_answer": 0.5750309824943542, "max_score_of_first_answer": 0.9818337559700012, "min_score_of_first_answer": 0.3367786407470703, "avg_score": 0.5750309824943542, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32252755761146545, "std_of_first_answer": 0.32252755761146545, "avg_entropy_of_first_answer": 1.5430719032883644, "lex_sim": 1.0}
{"question": "Name the French artist (1839-1906) whose painting 'The Card Players' is one of the most expensive ever sold?", "answer": ["Jean-Fran\u00e7ois Millet"], "scores": [[0.024406958371400833, 0.7805604934692383, 0.34921959042549133, 0.05251046642661095, 0.9479941725730896, 0.9990682005882263]], "normalized_score": [0.2629929780960083], "tokens": [["\u2581Jean", "-", "Fran", "\u00e7ois", "\u2581Mil", "let"]], "entropy": [[1.7551355361938477, 1.3032684326171875, 2.161611557006836, 1.0711400508880615, 0.388064980506897, 0.011772418394684792]], "avg_entropy": [1.1151654959345858], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paul C\u00e9zanne", "score_of_biggest_cluster": 0.2629929780960083, "score_of_first_answer": 0.2629929780960083, "max_score_of_first_answer": 0.9990682005882263, "min_score_of_first_answer": 0.024406958371400833, "avg_score": 0.2629929780960083, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4028465472105875, "std_of_first_answer": 0.4028465472105875, "avg_entropy_of_first_answer": 1.1151654959345858, "lex_sim": 1.0}
{"question": "\u2018Swifter, Higher, Stronger\u2019 is the motto for the modern \u2018what\u2019?", "answer": ["The Olympic Games"], "scores": [[0.032070666551589966, 0.23629458248615265, 0.8545381426811218]], "normalized_score": [0.1863936483860016], "tokens": [["\u2581The", "\u2581Olympic", "\u2581Games"]], "entropy": [[2.642404556274414, 2.010284423828125, 0.6982420682907104]], "avg_entropy": [1.7836436827977498], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "History of the Modern Olympics", "score_of_biggest_cluster": 0.1863936483860016, "score_of_first_answer": 0.1863936483860016, "max_score_of_first_answer": 0.8545381426811218, "min_score_of_first_answer": 0.032070666551589966, "avg_score": 0.1863936483860016, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34966416536332556, "std_of_first_answer": 0.34966416536332556, "avg_entropy_of_first_answer": 1.7836436827977498, "lex_sim": 1.0}
{"question": "Whose debut novel was the 1961 book Call of the Dead?", "answer": ["Ian Fleming"], "scores": [[0.048976123332977295, 0.7486250996589661, 0.9958821535110474, 0.9708977341651917]], "normalized_score": [0.43391790986061096], "tokens": [["\u2581Ian", "\u2581F", "lem", "ing"]], "entropy": [[4.2980451583862305, 1.556894063949585, 0.03481154516339302, 0.14795751869678497]], "avg_entropy": [1.5094270715489984], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John le Carre", "score_of_biggest_cluster": 0.43391790986061096, "score_of_first_answer": 0.43391790986061096, "max_score_of_first_answer": 0.9958821535110474, "min_score_of_first_answer": 0.048976123332977295, "avg_score": 0.43391790986061096, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38301799668235725, "std_of_first_answer": 0.38301799668235725, "avg_entropy_of_first_answer": 1.5094270715489984, "lex_sim": 1.0}
{"question": "In the Lake District which is the only body of water to actually be called lake?", "answer": ["Ullswater"], "scores": [[0.04963231831789017, 0.97065269947052, 0.9843749403953552, 0.996464729309082]], "normalized_score": [0.4662434458732605], "tokens": [["\u2581U", "ll", "sw", "ater"]], "entropy": [[2.0347230434417725, 0.1771235466003418, 0.1260320544242859, 0.034732334315776825]], "avg_entropy": [0.5931527446955442], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lake Bassenthwaite", "score_of_biggest_cluster": 0.4662434458732605, "score_of_first_answer": 0.4662434458732605, "max_score_of_first_answer": 0.996464729309082, "min_score_of_first_answer": 0.04963231831789017, "avg_score": 0.4662434458732605, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4046228684147242, "std_of_first_answer": 0.4046228684147242, "avg_entropy_of_first_answer": 0.5931527446955442, "lex_sim": 1.0}
{"question": "What is the general term for heat energy that has been drawnfrom under the ground?", "answer": ["Geothermal energy"], "scores": [[0.7644224166870117, 0.9151411056518555, 0.9937463998794556, 0.9707539081573486, 0.09255298227071762]], "normalized_score": [0.5742741823196411], "tokens": [["\u2581Ge", "oth", "erm", "al", "\u2581energy"]], "entropy": [[1.3016304969787598, 0.37477701902389526, 0.06474976241588593, 0.1712445318698883, 0.8110437989234924]], "avg_entropy": [0.5446891218423844], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Geothermic gradient", "score_of_biggest_cluster": 0.5742741823196411, "score_of_first_answer": 0.5742741823196411, "max_score_of_first_answer": 0.9937463998794556, "min_score_of_first_answer": 0.09255298227071762, "avg_score": 0.5742741823196411, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33699404905141234, "std_of_first_answer": 0.33699404905141234, "avg_entropy_of_first_answer": 0.5446891218423844, "lex_sim": 1.0}
{"question": "\"Who produces the perfumes \"\"Opium\"\" and \"\"Rive Gauche\"\"?\"", "answer": ["Chanel"], "scores": [[0.2694860100746155, 0.9545485377311707]], "normalized_score": [0.5071858763694763], "tokens": [["\u2581Ch", "anel"]], "entropy": [[3.3734045028686523, 0.39965325593948364]], "avg_entropy": [1.886528879404068], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yves Saint-Laurent", "score_of_biggest_cluster": 0.5071858763694763, "score_of_first_answer": 0.5071858763694763, "max_score_of_first_answer": 0.9545485377311707, "min_score_of_first_answer": 0.2694860100746155, "avg_score": 0.5071858763694763, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3425312638282776, "std_of_first_answer": 0.3425312638282776, "avg_entropy_of_first_answer": 1.886528879404068, "lex_sim": 1.0}
{"question": "What is the official color of the UPS trucks?", "answer": ["Brown"], "scores": [[0.45299720764160156]], "normalized_score": [0.45299720764160156], "tokens": [["\u2581Brown"]], "entropy": [[3.0504908561706543]], "avg_entropy": [3.0504908561706543], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Orangeish blacks", "score_of_biggest_cluster": 0.45299720764160156, "score_of_first_answer": 0.45299720764160156, "max_score_of_first_answer": 0.45299720764160156, "min_score_of_first_answer": 0.45299720764160156, "avg_score": 0.45299720764160156, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.0504908561706543, "lex_sim": 1.0}
{"question": "\"Pop music - the group called \"\"The Revolution\"\" was the backing roup for which pop singer?\"", "answer": ["Madonna"], "scores": [[0.16145536303520203]], "normalized_score": [0.16145534813404083], "tokens": [["\u2581Madonna"]], "entropy": [[4.126847267150879]], "avg_entropy": [4.126847267150879], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "PRINCE2:Close a project", "score_of_biggest_cluster": 0.16145534813404083, "score_of_first_answer": 0.16145534813404083, "max_score_of_first_answer": 0.16145536303520203, "min_score_of_first_answer": 0.16145536303520203, "avg_score": 0.16145534813404083, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.126847267150879, "lex_sim": 1.0}
{"question": "Which villain, played by Richard Kiel, appeared in two James Bond movies, 'The Spy Who Loved Me' and 'Moonraker'?", "answer": ["Jaws"], "scores": [[0.8545541763305664, 0.981173574924469]], "normalized_score": [0.9156779050827026], "tokens": [["\u2581J", "aws"]], "entropy": [[1.1213655471801758, 0.14426149427890778]], "avg_entropy": [0.6328135207295418], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jaws (software)", "score_of_biggest_cluster": 0.9156779050827026, "score_of_first_answer": 0.9156779050827026, "max_score_of_first_answer": 0.981173574924469, "min_score_of_first_answer": 0.8545541763305664, "avg_score": 0.9156779050827026, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0633096992969513, "std_of_first_answer": 0.0633096992969513, "avg_entropy_of_first_answer": 0.6328135207295418, "lex_sim": 1.0}
{"question": "What was the occupation of St Peter, the leader of the Apostles?", "answer": ["Fisherman"], "scores": [[0.7022532224655151, 0.9438068270683289]], "normalized_score": [0.8141199946403503], "tokens": [["\u2581Fish", "erman"]], "entropy": [[1.8844515085220337, 0.3053845167160034]], "avg_entropy": [1.0949180126190186], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fisher girls", "score_of_biggest_cluster": 0.8141199946403503, "score_of_first_answer": 0.8141199946403503, "max_score_of_first_answer": 0.9438068270683289, "min_score_of_first_answer": 0.7022532224655151, "avg_score": 0.8141199946403503, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12077680230140686, "std_of_first_answer": 0.12077680230140686, "avg_entropy_of_first_answer": 1.0949180126190186, "lex_sim": 1.0}
{"question": "\"What did Shylock ask for a pound of in \"\"The Merchant of Venice\"\"?\"", "answer": ["A pound of flesh"], "scores": [[0.3500591218471527, 0.7360575199127197, 0.9929934144020081, 0.9839841723442078, 0.8002434372901917]], "normalized_score": [0.725841760635376], "tokens": [["\u2581A", "\u2581p", "ound", "\u2581of", "\u2581flesh"]], "entropy": [[3.6239633560180664, 2.07509183883667, 0.06512142717838287, 0.14124372601509094, 1.4191365242004395]], "avg_entropy": [1.46491137444973], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Human flesh", "score_of_biggest_cluster": 0.725841760635376, "score_of_first_answer": 0.725841760635376, "max_score_of_first_answer": 0.9929934144020081, "min_score_of_first_answer": 0.3500591218471527, "avg_score": 0.725841760635376, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2340499129724719, "std_of_first_answer": 0.2340499129724719, "avg_entropy_of_first_answer": 1.46491137444973, "lex_sim": 1.0}
{"question": "Which company produces the 'Fireblade' motorcycle?", "answer": ["The Fireblade"], "scores": [[0.007482160814106464, 0.03417491540312767, 0.7331607937812805, 0.9887712597846985]], "normalized_score": [0.11668293923139572], "tokens": [["\u2581The", "\u2581Fire", "bla", "de"]], "entropy": [[2.5282535552978516, 3.974576234817505, 1.8383994102478027, 0.0818796455860138]], "avg_entropy": [2.1057772114872932], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Honda Technologies", "score_of_biggest_cluster": 0.11668293923139572, "score_of_first_answer": 0.11668293923139572, "max_score_of_first_answer": 0.9887712597846985, "min_score_of_first_answer": 0.007482160814106464, "avg_score": 0.11668293923139572, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4297835520977677, "std_of_first_answer": 0.4297835520977677, "avg_entropy_of_first_answer": 2.1057772114872932, "lex_sim": 1.0}
{"question": "Who taught Eliza Dolittle to be a lady ?", "answer": ["Charles Frederick Darnay"], "scores": [[0.005464070476591587, 0.023162608966231346, 0.22585009038448334, 0.06073234975337982, 0.7258927226066589]], "normalized_score": [0.06608201563358307], "tokens": [["\u2581Charles", "\u2581Frederick", "\u2581D", "arn", "ay"]], "entropy": [[2.8005027770996094, 4.390066146850586, 4.11016845703125, 1.6704331636428833, 1.154401183128357]], "avg_entropy": [2.825114345550537], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Professor Henry Higgins", "score_of_biggest_cluster": 0.06608201563358307, "score_of_first_answer": 0.06608201563358307, "max_score_of_first_answer": 0.7258927226066589, "min_score_of_first_answer": 0.005464070476591587, "avg_score": 0.06608201563358307, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2703348589160018, "std_of_first_answer": 0.2703348589160018, "avg_entropy_of_first_answer": 2.825114345550537, "lex_sim": 1.0}
{"question": "The adjective vulpine refers to which animal?", "answer": ["Fox"], "scores": [[0.8558189272880554]], "normalized_score": [0.8558189272880554], "tokens": [["\u2581Fox"]], "entropy": [[0.8911738991737366]], "avg_entropy": [0.8911738991737366], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fox (network)", "score_of_biggest_cluster": 0.8558189272880554, "score_of_first_answer": 0.8558189272880554, "max_score_of_first_answer": 0.8558189272880554, "min_score_of_first_answer": 0.8558189272880554, "avg_score": 0.8558189272880554, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.8911738991737366, "lex_sim": 1.0}
{"question": "What Swedish city is connected to Copenhagen via a bridge?", "answer": ["Malm\u00f6"], "scores": [[0.6558713912963867, 0.1428482085466385, 0.18570007383823395]], "normalized_score": [0.25912052392959595], "tokens": [["\u2581Mal", "m", "\u00f6"]], "entropy": [[1.9103636741638184, 0.544418215751648, 2.359304428100586]], "avg_entropy": [1.604695439338684], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Municipality of Malmoe", "score_of_biggest_cluster": 0.25912052392959595, "score_of_first_answer": 0.25912052392959595, "max_score_of_first_answer": 0.6558713912963867, "min_score_of_first_answer": 0.1428482085466385, "avg_score": 0.25912052392959595, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23240054889923362, "std_of_first_answer": 0.23240054889923362, "avg_entropy_of_first_answer": 1.604695439338684, "lex_sim": 1.0}
{"question": "In which US state was singer Bruno Mars born?", "answer": ["Alaska"], "scores": [[0.0014150196220725775, 0.8015726208686829]], "normalized_score": [0.03367850184440613], "tokens": [["\u2581Al", "aska"]], "entropy": [[1.9832264184951782, 1.4295954704284668]], "avg_entropy": [1.7064109444618225], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Haiwii", "score_of_biggest_cluster": 0.03367850184440613, "score_of_first_answer": 0.03367850184440613, "max_score_of_first_answer": 0.8015726208686829, "min_score_of_first_answer": 0.0014150196220725775, "avg_score": 0.03367850184440613, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40007880062330514, "std_of_first_answer": 0.40007880062330514, "avg_entropy_of_first_answer": 1.7064109444618225, "lex_sim": 1.0}
{"question": "Who was the landlady of the Rovers Return when Coronation Street was first broadcast in 1960?", "answer": ["Annie Walker"], "scores": [[0.3277592957019806, 0.998484194278717, 0.9944993257522583]], "normalized_score": [0.6878603100776672], "tokens": [["\u2581An", "nie", "\u2581Walker"]], "entropy": [[3.2877554893493652, 0.014669829048216343, 0.05944057181477547]], "avg_entropy": [1.120621963404119], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Annie Walker", "score_of_biggest_cluster": 0.6878603100776672, "score_of_first_answer": 0.6878603100776672, "max_score_of_first_answer": 0.998484194278717, "min_score_of_first_answer": 0.3277592957019806, "avg_score": 0.6878603100776672, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.31524770444479083, "std_of_first_answer": 0.31524770444479083, "avg_entropy_of_first_answer": 1.120621963404119, "lex_sim": 1.0}
{"question": "The 'Master Cutler' was a regular train service from Sheffield to which London Mainline rail terminus?", "answer": ["King Cross St Pancras"], "scores": [[0.09377076476812363, 0.03499197959899902, 0.0615837499499321, 0.6538624167442322, 0.9961252808570862, 0.9927963614463806]], "normalized_score": [0.2252650260925293], "tokens": [["\u2581King", "\u2581Cross", "\u2581St", "\u2581P", "anc", "ras"]], "entropy": [[3.4401416778564453, 0.5396043062210083, 1.215303659439087, 0.9249905347824097, 0.042812686413526535, 0.06578496098518372]], "avg_entropy": [1.0381063042829435], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "St. Pancras", "score_of_biggest_cluster": 0.2252650260925293, "score_of_first_answer": 0.2252650260925293, "max_score_of_first_answer": 0.9961252808570862, "min_score_of_first_answer": 0.03499197959899902, "avg_score": 0.2252650260925293, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42455560066031817, "std_of_first_answer": 0.42455560066031817, "avg_entropy_of_first_answer": 1.0381063042829435, "lex_sim": 1.0}
{"question": "What was Queen Victoria's first Christian name?", "answer": ["Alexandria"], "scores": [[0.783130943775177, 0.42836371064186096]], "normalized_score": [0.5791932940483093], "tokens": [["\u2581Alexand", "ria"]], "entropy": [[1.0846631526947021, 1.1971328258514404]], "avg_entropy": [1.1408979892730713], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alexandrina", "score_of_biggest_cluster": 0.5791932940483093, "score_of_first_answer": 0.5791932940483093, "max_score_of_first_answer": 0.783130943775177, "min_score_of_first_answer": 0.42836371064186096, "avg_score": 0.5791932940483093, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17738361656665802, "std_of_first_answer": 0.17738361656665802, "avg_entropy_of_first_answer": 1.1408979892730713, "lex_sim": 1.0}
{"question": "Ridden by jockey Brian Fletcher, which horse won the 1968 Grand National at odds of 100 / 7 ?", "answer": ["Foinavon"], "scores": [[0.35626673698425293, 0.9532540440559387, 0.9822147488594055, 0.9954598546028137]], "normalized_score": [0.7591079473495483], "tokens": [["\u2581F", "oin", "av", "on"]], "entropy": [[3.802910566329956, 0.33566397428512573, 0.09347817301750183, 0.03488681837916374]], "avg_entropy": [1.0667348830029368], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Red Alligator", "score_of_biggest_cluster": 0.7591079473495483, "score_of_first_answer": 0.7591079473495483, "max_score_of_first_answer": 0.9954598546028137, "min_score_of_first_answer": 0.35626673698425293, "avg_score": 0.7591079473495483, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2692081078922588, "std_of_first_answer": 0.2692081078922588, "avg_entropy_of_first_answer": 1.0667348830029368, "lex_sim": 1.0}
{"question": "Who played the character Mrs. Merton on TV?", "answer": ["Vic Reeves"], "scores": [[0.0006099666934460402, 0.6648241281509399, 0.9986857771873474, 0.9966688752174377]], "normalized_score": [0.14174185693264008], "tokens": [["\u2581Vic", "\u2581Re", "e", "ves"]], "entropy": [[4.286494255065918, 1.8039953708648682, 0.014225093647837639, 0.0255324374884367]], "avg_entropy": [1.5325617892667651], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Caroline Hook", "score_of_biggest_cluster": 0.14174185693264008, "score_of_first_answer": 0.14174185693264008, "max_score_of_first_answer": 0.9986857771873474, "min_score_of_first_answer": 0.0006099666934460402, "avg_score": 0.14174185693264008, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4070517265786253, "std_of_first_answer": 0.4070517265786253, "avg_entropy_of_first_answer": 1.5325617892667651, "lex_sim": 1.0}
{"question": "What ice cream company produces flavors such as Everything But The.., Neapolitan Dynamite, and Stephen Colbert's AmeriCone Dream?", "answer": ["Ben & Jerry's Ice Cream"], "scores": [[0.8066608905792236, 0.6032388210296631, 0.9608161449432373, 0.8939095139503479, 0.9985820055007935, 0.019087590277194977, 0.9109195470809937, 0.9996415376663208]], "normalized_score": [0.5402212142944336], "tokens": [["\u2581Ben", "\u2581&", "\u2581Jerry", "'", "s", "\u2581Ice", "\u2581C", "ream"]], "entropy": [[1.2313271760940552, 0.95732182264328, 0.28007861971855164, 0.43969351053237915, 0.016240838915109634, 0.3841388523578644, 0.41664624214172363, 0.003995081875473261]], "avg_entropy": [0.4661802680348046], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Free Cone Day", "score_of_biggest_cluster": 0.5402212142944336, "score_of_first_answer": 0.5402212142944336, "max_score_of_first_answer": 0.9996415376663208, "min_score_of_first_answer": 0.019087590277194977, "avg_score": 0.5402212142944336, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31014018511848906, "std_of_first_answer": 0.31014018511848906, "avg_entropy_of_first_answer": 0.4661802680348046, "lex_sim": 1.0}
{"question": "Subtitled \u2018What You Will\u2019, which play begins: \u2018If music be the food of love, play on\u2019?", "answer": ["Twelfth Night"], "scores": [[0.35444048047065735, 0.9767861366271973, 0.9972967505455017, 0.9333662986755371]], "normalized_score": [0.7534506916999817], "tokens": [["\u2581Tw", "elf", "th", "\u2581Night"]], "entropy": [[3.034594774246216, 0.15879566967487335, 0.02989218756556511, 0.3144654929637909]], "avg_entropy": [0.8844370311126113], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "TWELFTH NIGHT", "score_of_biggest_cluster": 0.7534506916999817, "score_of_first_answer": 0.7534506916999817, "max_score_of_first_answer": 0.9972967505455017, "min_score_of_first_answer": 0.35444048047065735, "avg_score": 0.7534506916999817, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.26717579336728337, "std_of_first_answer": 0.26717579336728337, "avg_entropy_of_first_answer": 0.8844370311126113, "lex_sim": 1.0}
{"question": "'Stane Street' was a Roman road that linked London to which city?", "answer": ["Chichester"], "scores": [[0.4719751179218292, 0.9265929460525513, 0.9854896068572998]], "normalized_score": [0.7553589940071106], "tokens": [["\u2581Ch", "ich", "ester"]], "entropy": [[2.8570804595947266, 0.3515697121620178, 0.13162541389465332]], "avg_entropy": [1.1134251952171326], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chichester Festivities", "score_of_biggest_cluster": 0.7553589940071106, "score_of_first_answer": 0.7553589940071106, "max_score_of_first_answer": 0.9854896068572998, "min_score_of_first_answer": 0.4719751179218292, "avg_score": 0.7553589940071106, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22945425994324056, "std_of_first_answer": 0.22945425994324056, "avg_entropy_of_first_answer": 1.1134251952171326, "lex_sim": 1.0}
{"question": "Which US singer/musician played Leather Tuscadero in the television show \u2018Happy Days\u2019?", "answer": ["Stephanie \"Steph\" Erin Richards"], "scores": [[0.010282023809850216, 0.9916341304779053, 0.012762964703142643, 0.8258814215660095, 0.17539969086647034, 0.8037866950035095, 0.009745663031935692, 0.8048862814903259, 0.036685261875391006, 0.9988815188407898]], "normalized_score": [0.14584851264953613], "tokens": [["\u2581Steph", "anie", "\u2581\"", "Ste", "ph", "\"", "\u2581Er", "in", "\u2581Rich", "ards"]], "entropy": [[4.514013290405273, 0.07567322254180908, 2.9701929092407227, 1.0061120986938477, 1.390709638595581, 0.6958562135696411, 4.982166767120361, 1.2240333557128906, 4.449800491333008, 0.011906159110367298]], "avg_entropy": [2.13204641463235], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Suzie Quatro", "score_of_biggest_cluster": 0.14584851264953613, "score_of_first_answer": 0.14584851264953613, "max_score_of_first_answer": 0.9988815188407898, "min_score_of_first_answer": 0.009745663031935692, "avg_score": 0.14584851264953613, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4252919996153155, "std_of_first_answer": 0.4252919996153155, "avg_entropy_of_first_answer": 2.13204641463235, "lex_sim": 1.0}
{"question": "Splenitis is the inflammation of which part of the body?", "answer": ["Abdomen"], "scores": [[0.0007766096969135106, 0.997648298740387]], "normalized_score": [0.027834929525852203], "tokens": [["\u2581Abd", "omen"]], "entropy": [[2.182960033416748, 0.02586328238248825]], "avg_entropy": [1.1044116578996181], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lienal", "score_of_biggest_cluster": 0.027834929525852203, "score_of_first_answer": 0.027834929525852203, "max_score_of_first_answer": 0.997648298740387, "min_score_of_first_answer": 0.0007766096969135106, "avg_score": 0.027834929525852203, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4984358445217367, "std_of_first_answer": 0.4984358445217367, "avg_entropy_of_first_answer": 1.1044116578996181, "lex_sim": 1.0}
{"question": "Which Canadian sang Miss Chatelaine in 1993", "answer": ["k.d."], "scores": [[0.034573886543512344, 0.6664639711380005, 0.9378688931465149, 0.9584289193153381, 0.7327105402946472]], "normalized_score": [0.4327448010444641], "tokens": [["\u2581k", ".", "d", ".", "\u2581lang"]], "entropy": [[4.547019004821777, 1.242753505706787, 0.30258259177207947, 0.2644408941268921, 1.0643646717071533]], "avg_entropy": [1.484232133626938], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "KD Lang", "score_of_biggest_cluster": 0.4327448010444641, "score_of_first_answer": 0.4327448010444641, "max_score_of_first_answer": 0.9584289193153381, "min_score_of_first_answer": 0.034573886543512344, "avg_score": 0.4327448010444641, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.33543309785885844, "std_of_first_answer": 0.33543309785885844, "avg_entropy_of_first_answer": 1.484232133626938, "lex_sim": 1.0}
{"question": "The peacock belongs to which family of birds?", "answer": ["Phoenicopteridae"], "scores": [[0.0014840041985735297, 0.938426673412323, 0.7389852404594421, 0.8411944508552551, 0.9183717966079712]], "normalized_score": [0.23992593586444855], "tokens": [["\u2581Pho", "en", "ic", "opter", "idae"]], "entropy": [[3.9024462699890137, 0.4517383575439453, 1.505690097808838, 0.7322811484336853, 0.55556321144104]], "avg_entropy": [1.4295438170433044], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pheasant meat", "score_of_biggest_cluster": 0.23992593586444855, "score_of_first_answer": 0.23992593586444855, "max_score_of_first_answer": 0.938426673412323, "min_score_of_first_answer": 0.0014840041985735297, "avg_score": 0.23992593586444855, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3501875090435272, "std_of_first_answer": 0.3501875090435272, "avg_entropy_of_first_answer": 1.4295438170433044, "lex_sim": 1.0}
{"question": "Which is the largest borough in New York City?", "answer": ["New York County"], "scores": [[0.013271603733301163, 0.9546968340873718, 0.08287500590085983]], "normalized_score": [0.10164143890142441], "tokens": [["\u2581New", "\u2581York", "\u2581County"]], "entropy": [[2.082631826400757, 0.31794947385787964, 1.6253626346588135]], "avg_entropy": [1.34198131163915], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Queens, NYC, NY", "score_of_biggest_cluster": 0.10164143890142441, "score_of_first_answer": 0.10164143890142441, "max_score_of_first_answer": 0.9546968340873718, "min_score_of_first_answer": 0.013271603733301163, "avg_score": 0.10164143890142441, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42833001270346904, "std_of_first_answer": 0.42833001270346904, "avg_entropy_of_first_answer": 1.34198131163915, "lex_sim": 1.0}
{"question": "The bitter 19th century rivalry between paleontologists Edward Drinker Cope and Othniel Charles Marsh during their search for dinosaur fossils is usually described with what 2-words?", "answer": ["Bone wars"], "scores": [[0.6507550477981567, 0.9785232543945312, 0.3031221926212311]], "normalized_score": [0.577921450138092], "tokens": [["\u2581B", "one", "\u2581wars"]], "entropy": [[2.3295536041259766, 0.1500324308872223, 1.4377087354660034]], "avg_entropy": [1.3057649234930675], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bone Wars", "score_of_biggest_cluster": 0.577921450138092, "score_of_first_answer": 0.577921450138092, "max_score_of_first_answer": 0.9785232543945312, "min_score_of_first_answer": 0.3031221926212311, "avg_score": 0.577921450138092, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2757710792786023, "std_of_first_answer": 0.2757710792786023, "avg_entropy_of_first_answer": 1.3057649234930675, "lex_sim": 1.0}
{"question": "Who wrote 'The Owl and the Pussycat'?", "answer": ["Edward Lear"], "scores": [[0.9666392803192139, 0.9906541705131531]], "normalized_score": [0.9785730838775635], "tokens": [["\u2581Edward", "\u2581Lear"]], "entropy": [[0.26461368799209595, 0.09895658493041992]], "avg_entropy": [0.18178513646125793], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Edward Lear", "score_of_biggest_cluster": 0.9785730838775635, "score_of_first_answer": 0.9785730838775635, "max_score_of_first_answer": 0.9906541705131531, "min_score_of_first_answer": 0.9666392803192139, "avg_score": 0.9785730838775635, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.012007445096969604, "std_of_first_answer": 0.012007445096969604, "avg_entropy_of_first_answer": 0.18178513646125793, "lex_sim": 1.0}
{"question": "Who was the first female artist to have a #1 album in the USA and the UK simultaneously?", "answer": ["Pink"], "scores": [[0.026508288457989693, 0.6764837503433228]], "normalized_score": [0.13391199707984924], "tokens": [["\u2581P", "ink"]], "entropy": [[3.4991366863250732, 1.1014883518218994]], "avg_entropy": [2.3003125190734863], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Susan Magdalane Boyle", "score_of_biggest_cluster": 0.13391199707984924, "score_of_first_answer": 0.13391199707984924, "max_score_of_first_answer": 0.6764837503433228, "min_score_of_first_answer": 0.026508288457989693, "avg_score": 0.13391199707984924, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32498773094266653, "std_of_first_answer": 0.32498773094266653, "avg_entropy_of_first_answer": 2.3003125190734863, "lex_sim": 1.0}
{"question": "The Asian Tsunami or Boxing day tsunami that hit Thailand, Indonesia and many other countries happened in what year?", "answer": ["26 December 2004"], "scores": [[0.510070264339447, 0.9547914862632751, 0.1486300528049469, 0.4443681538105011, 0.8791241645812988, 0.9977722764015198, 0.9744632244110107, 0.997560977935791, 0.9864687919616699]], "normalized_score": [0.6695861220359802], "tokens": [["\u2581", "2", "6", "\u2581December", "\u2581", "2", "0", "0", "4"]], "entropy": [[2.4083292484283447, 0.2812105417251587, 0.5874236226081848, 1.8762398958206177, 0.4892636835575104, 0.019959546625614166, 0.13172464072704315, 0.02000129222869873, 0.09885527193546295]], "avg_entropy": [0.6570008604062928], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "two thousand and four", "score_of_biggest_cluster": 0.6695861220359802, "score_of_first_answer": 0.6695861220359802, "max_score_of_first_answer": 0.9977722764015198, "min_score_of_first_answer": 0.1486300528049469, "avg_score": 0.6695861220359802, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2977572172548912, "std_of_first_answer": 0.2977572172548912, "avg_entropy_of_first_answer": 0.6570008604062928, "lex_sim": 1.0}
{"question": "What name is used in the Roman Catholic Church for evensong?", "answer": ["Vespers"], "scores": [[0.5943363904953003, 0.92066490650177, 0.9916419982910156]], "normalized_score": [0.8156358003616333], "tokens": [["\u2581V", "es", "pers"]], "entropy": [[2.079037666320801, 0.36261898279190063, 0.05952617526054382]], "avg_entropy": [0.833727608124415], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lucernarium", "score_of_biggest_cluster": 0.8156358003616333, "score_of_first_answer": 0.8156358003616333, "max_score_of_first_answer": 0.9916419982910156, "min_score_of_first_answer": 0.5943363904953003, "avg_score": 0.8156358003616333, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17300603518816798, "std_of_first_answer": 0.17300603518816798, "avg_entropy_of_first_answer": 0.833727608124415, "lex_sim": 1.0}
{"question": "In which year was the Juilliard School founded in New York?", "answer": ["1905"], "scores": [[0.5941164493560791, 0.9855496287345886, 0.8307651877403259, 0.24740464985370636, 0.8576960563659668]], "normalized_score": [0.634971022605896], "tokens": [["\u2581", "1", "9", "0", "5"]], "entropy": [[2.0577683448791504, 0.11277467012405396, 0.5403978824615479, 0.8884624242782593, 0.6688004732131958]], "avg_entropy": [0.8536407589912415], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1905", "score_of_biggest_cluster": 0.634971022605896, "score_of_first_answer": 0.634971022605896, "max_score_of_first_answer": 0.9855496287345886, "min_score_of_first_answer": 0.24740464985370636, "avg_score": 0.634971022605896, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.26058081273511124, "std_of_first_answer": 0.26058081273511124, "avg_entropy_of_first_answer": 0.8536407589912415, "lex_sim": 1.0}
{"question": "\"In the book of Genesis who asked the question,\"\"Am I my brotherskeeper\"\"?\"", "answer": ["Cain and Abel"], "scores": [[0.778765857219696, 0.9810219407081604, 0.009880306199193, 0.8796764612197876, 0.9952316880226135]], "normalized_score": [0.36645498871803284], "tokens": [["\u2581C", "ain", "\u2581and", "\u2581Ab", "el"]], "entropy": [[1.578369379043579, 0.1310102939605713, 0.45552557706832886, 0.7343803644180298, 0.044079508632421494]], "avg_entropy": [0.5886730246245861], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Conflict Archive on the INternet", "score_of_biggest_cluster": 0.36645498871803284, "score_of_first_answer": 0.36645498871803284, "max_score_of_first_answer": 0.9952316880226135, "min_score_of_first_answer": 0.009880306199193, "avg_score": 0.36645498871803284, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3678878918681973, "std_of_first_answer": 0.3678878918681973, "avg_entropy_of_first_answer": 0.5886730246245861, "lex_sim": 1.0}
{"question": "Which river forms almost all the boundary between Devon and Cornwall", "answer": ["Tamar"], "scores": [[0.3438642919063568, 0.8395498394966125]], "normalized_score": [0.5372999310493469], "tokens": [["\u2581T", "amar"]], "entropy": [[2.343191623687744, 0.7837299704551697]], "avg_entropy": [1.563460797071457], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tamar (Bible) (disambiguation)", "score_of_biggest_cluster": 0.5372999310493469, "score_of_first_answer": 0.5372999310493469, "max_score_of_first_answer": 0.8395498394966125, "min_score_of_first_answer": 0.3438642919063568, "avg_score": 0.5372999310493469, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24784277379512787, "std_of_first_answer": 0.24784277379512787, "avg_entropy_of_first_answer": 1.563460797071457, "lex_sim": 1.0}
{"question": "Which boy band consists of Niall Horan, Zayn Malik, Liam Payne, Harry Stiles and Louis Tomlinson?", "answer": ["One Direction"], "scores": [[0.812574565410614, 0.8629773855209351, 0.9926515221595764]], "normalized_score": [0.8862437009811401], "tokens": [["\u2581One", "\u2581D", "irection"]], "entropy": [[1.2949963808059692, 0.7493870258331299, 0.08162709325551987]], "avg_entropy": [0.7086701666315397], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "List of One Direction concert tours", "score_of_biggest_cluster": 0.8862437009811401, "score_of_first_answer": 0.8862437009811401, "max_score_of_first_answer": 0.9926515221595764, "min_score_of_first_answer": 0.812574565410614, "avg_score": 0.8862437009811401, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07585332069199087, "std_of_first_answer": 0.07585332069199087, "avg_entropy_of_first_answer": 0.7086701666315397, "lex_sim": 1.0}
{"question": "What island chain includes the islands of Fuerteventura, Lanzarote, and La Palma?", "answer": ["Canary Islands"], "scores": [[0.7612177133560181, 0.600047767162323, 0.6276568174362183]], "normalized_score": [0.6593849062919617], "tokens": [["\u2581Can", "ary", "\u2581Islands"]], "entropy": [[1.3763790130615234, 0.9660519361495972, 0.9525940418243408]], "avg_entropy": [1.098341663678487], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Canary islands", "score_of_biggest_cluster": 0.6593849062919617, "score_of_first_answer": 0.6593849062919617, "max_score_of_first_answer": 0.7612177133560181, "min_score_of_first_answer": 0.600047767162323, "avg_score": 0.6593849062919617, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.07037717759173276, "std_of_first_answer": 0.07037717759173276, "avg_entropy_of_first_answer": 1.098341663678487, "lex_sim": 1.0}
{"question": "In 1986, whilst facing England which West Indian batsman scored the fastest ever Test century in just 56 balls?", "answer": ["Michael Holding"], "scores": [[0.003048674203455448, 0.40034300088882446, 0.9966729879379272]], "normalized_score": [0.10674937069416046], "tokens": [["\u2581Michael", "\u2581Hold", "ing"]], "entropy": [[2.644036293029785, 3.9318761825561523, 0.029867304489016533]], "avg_entropy": [2.201926593358318], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sir Vivian Richards", "score_of_biggest_cluster": 0.10674937069416046, "score_of_first_answer": 0.10674937069416046, "max_score_of_first_answer": 0.9966729879379272, "min_score_of_first_answer": 0.003048674203455448, "avg_score": 0.10674937069416046, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4083491850412482, "std_of_first_answer": 0.4083491850412482, "avg_entropy_of_first_answer": 2.201926593358318, "lex_sim": 1.0}
{"question": "What waltz by Johann Strauss II was named after a European river?", "answer": ["The Blue Danube"], "scores": [[0.23517060279846191, 0.8128114342689514, 0.9524694681167603, 0.9933966398239136]], "normalized_score": [0.6521337032318115], "tokens": [["\u2581The", "\u2581Blue", "\u2581Dan", "ube"]], "entropy": [[3.032200574874878, 1.223100185394287, 0.3673532009124756, 0.046392716467380524]], "avg_entropy": [1.1672616694122553], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "An Der Schoenen Blau Donau", "score_of_biggest_cluster": 0.6521337032318115, "score_of_first_answer": 0.6521337032318115, "max_score_of_first_answer": 0.9933966398239136, "min_score_of_first_answer": 0.23517060279846191, "avg_score": 0.6521337032318115, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.30381776320885073, "std_of_first_answer": 0.30381776320885073, "avg_entropy_of_first_answer": 1.1672616694122553, "lex_sim": 1.0}
{"question": "Sep 21, 1981 saw the confirmation of which associate justice, the first female Supreme Court justice?", "answer": ["Sandra Day O'Connor"], "scores": [[0.8814866542816162, 0.9982670545578003, 0.9686411023139954, 0.9361302852630615, 0.8588997721672058, 0.9738169312477112, 0.9309930801391602]], "normalized_score": [0.9342764616012573], "tokens": [["\u2581Sand", "ra", "\u2581Day", "\u2581O", "'", "Con", "nor"]], "entropy": [[0.7440140247344971, 0.02058977261185646, 0.23544490337371826, 0.4008054733276367, 0.8019425868988037, 0.15704187750816345, 0.2831231355667114]], "avg_entropy": [0.377565967717341], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sandra Day O'Conner", "score_of_biggest_cluster": 0.9342764616012573, "score_of_first_answer": 0.9342764616012573, "max_score_of_first_answer": 0.9982670545578003, "min_score_of_first_answer": 0.8588997721672058, "avg_score": 0.9342764616012573, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.046769891085935746, "std_of_first_answer": 0.046769891085935746, "avg_entropy_of_first_answer": 0.377565967717341, "lex_sim": 1.0}
{"question": "The 3rd of his vice presidents (after John Nance Garner and Henry Wallace), who took over the reins of power when FDR kicked off?", "answer": ["Franklin D Roosevelt"], "scores": [[0.0008587330812588334, 0.3015390634536743, 0.24676580727100372, 0.985230028629303, 0.9981865286827087, 0.9970697164535522]], "normalized_score": [0.19929371774196625], "tokens": [["\u2581Franklin", "\u2581D", "\u2581Ro", "ose", "vel", "t"]], "entropy": [[0.4576610326766968, 1.2413349151611328, 0.9161514043807983, 0.08983170241117477, 0.019989483058452606, 0.028899095952510834]], "avg_entropy": [0.45897793894012767], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Presidency of Harry S. Truman", "score_of_biggest_cluster": 0.19929371774196625, "score_of_first_answer": 0.19929371774196625, "max_score_of_first_answer": 0.9981865286827087, "min_score_of_first_answer": 0.0008587330812588334, "avg_score": 0.19929371774196625, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4156556613688151, "std_of_first_answer": 0.4156556613688151, "avg_entropy_of_first_answer": 0.45897793894012767, "lex_sim": 1.0}
{"question": "Marat, Robespierre, Danton are names associated with what event at the end of the 18th century?", "answer": ["The French Revolution If your answers were consistent with those displayed then you know a lot about AI but nothing about science\u2026 Ask the Assistant This page was last updated: October 17 2016 9:"], "scores": [[0.39557749032974243, 0.919861912727356, 0.8924592137336731, 4.0489433672519226e-07, 0.03460652008652687, 0.07182110846042633, 0.2180212438106537, 0.0014424199471250176, 0.4234409034252167, 0.0726286992430687, 0.0044471570290625095, 0.017937107011675835, 0.4934520721435547, 0.018261687830090523, 0.12101183086633682, 0.4253123998641968, 0.6621521711349487, 0.15250369906425476, 0.9347608089447021, 0.01932540163397789, 0.03440636768937111, 0.8336507081985474, 0.005305565893650055, 0.0003441820736043155, 0.0004812162951566279, 0.11138036847114563, 0.03415027633309364, 0.9808146953582764, 5.86191781621892e-05, 0.020877670496702194, 0.29324308037757874, 0.523613691329956, 0.3936174511909485, 0.10955251753330231, 0.023000307381153107, 0.9571828842163086, 0.3295578956604004, 0.08970877528190613, 0.01975720189511776, 0.7419030666351318, 0.9969131946563721, 0.3203800618648529, 0.10193059593439102, 0.23548485338687897, 0.01697973906993866, 0.9498670101165771]], "normalized_score": [0.06463884562253952], "tokens": [["\u2581The", "\u2581French", "\u2581Revolution", "\u2581If", "\u2581your", "\u2581answers", "\u2581were", "\u2581consistent", "\u2581with", "\u2581those", "\u2581displayed", "\u2581then", "\u2581you", "\u2581know", "\u2581a", "\u2581lot", "\u2581about", "\u2581A", "I", "\u2581but", "\u2581nothing", "\u2581about", "\u2581science", "\u2026", "\u2581Ask", "\u2581the", "\u2581Ass", "istant", "\u2581This", "\u2581page", "\u2581was", "\u2581last", "\u2581updated", ":", "\u2581October", "\u2581", "1", "7", "\u2581", "2", "0", "1", "6", "\u2581", "9", ":"]], "entropy": [[1.3081027269363403, 0.5253013968467712, 0.4213373363018036, 0.3551706075668335, 2.32700252532959, 4.842501640319824, 2.2084310054779053, 3.0906596183776855, 2.1453804969787598, 3.2657973766326904, 3.1033787727355957, 2.5460333824157715, 2.60856032371521, 3.0432517528533936, 3.039372444152832, 2.444427967071533, 1.3754453659057617, 4.338540077209473, 0.4380381107330322, 3.108750343322754, 3.7990431785583496, 1.0170063972473145, 5.2360992431640625, 2.2036547660827637, 4.1798787117004395, 3.729907274246216, 3.3636937141418457, 0.13792650401592255, 3.7141568660736084, 4.757460594177246, 3.075291633605957, 2.556699514389038, 1.4887661933898926, 2.0145702362060547, 3.1075663566589355, 0.280007004737854, 1.643383502960205, 2.4543070793151855, 0.501557469367981, 0.7222369909286499, 0.030952943488955498, 0.6487147808074951, 2.3023457527160645, 3.190092086791992, 1.6568548679351807, 0.3914507031440735]], "avg_entropy": [2.276937122537714], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "French Revolution of 1789", "score_of_biggest_cluster": 0.06463884562253952, "score_of_first_answer": 0.06463884562253952, "max_score_of_first_answer": 0.9969131946563721, "min_score_of_first_answer": 4.0489433672519226e-07, "avg_score": 0.06463884562253952, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.34258199363242586, "std_of_first_answer": 0.34258199363242586, "avg_entropy_of_first_answer": 2.276937122537714, "lex_sim": 1.0}
{"question": "What are the French speaking citizens of Belgium called?", "answer": ["Belgian francophones"], "scores": [[0.04863257333636284, 0.5542050004005432, 0.006889781914651394, 0.8975900411605835, 0.6895490288734436]], "normalized_score": [0.16296309232711792], "tokens": [["\u2581Belg", "ian", "\u2581franc", "oph", "ones"]], "entropy": [[1.9518427848815918, 1.909726619720459, 1.2623549699783325, 0.4499875605106354, 0.6770366430282593]], "avg_entropy": [1.2501897156238555], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wallonian", "score_of_biggest_cluster": 0.16296309232711792, "score_of_first_answer": 0.16296309232711792, "max_score_of_first_answer": 0.8975900411605835, "min_score_of_first_answer": 0.006889781914651394, "avg_score": 0.16296309232711792, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35368265592284004, "std_of_first_answer": 0.35368265592284004, "avg_entropy_of_first_answer": 1.2501897156238555, "lex_sim": 1.0}
{"question": "In 1984, in Bophal, India, there was a leak of 30 tons of methyl isocyanate, which resulted in the deaths of 25,000 people. What American chemical company owned the plant where the leak occurred?", "answer": ["Union Carbide"], "scores": [[0.3903157114982605, 0.9757195115089417, 0.9916263818740845, 0.9976295232772827]], "normalized_score": [0.7834559679031372], "tokens": [["\u2581Union", "\u2581Car", "b", "ide"]], "entropy": [[3.292145013809204, 0.19791284203529358, 0.07409701496362686, 0.025887779891490936]], "avg_entropy": [0.8975106626749039], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Prest-O-Lite", "score_of_biggest_cluster": 0.7834559679031372, "score_of_first_answer": 0.7834559679031372, "max_score_of_first_answer": 0.9976295232772827, "min_score_of_first_answer": 0.3903157114982605, "avg_score": 0.7834559679031372, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2590694057824494, "std_of_first_answer": 0.2590694057824494, "avg_entropy_of_first_answer": 0.8975106626749039, "lex_sim": 1.0}
{"question": "Late British singer-songwriter John Simon Ritchie was better known by what name?", "answer": ["Sid Vicious (singer)"], "scores": [[0.31767359375953674, 0.9395312070846558, 0.9932570457458496, 0.004946277476847172, 0.0003887345956172794, 0.9906783103942871, 0.32744455337524414]], "normalized_score": [0.10917835682630539], "tokens": [["\u2581Sid", "\u2581V", "icious", "\u2581(", "sing", "er", ")"]], "entropy": [[2.3284096717834473, 0.3892785906791687, 0.059937525540590286, 0.20952990651130676, 4.597024440765381, 0.07135620713233948, 2.28399658203125]], "avg_entropy": [1.4199332749204976], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Simon Ritchie-Beverly", "score_of_biggest_cluster": 0.10917835682630539, "score_of_first_answer": 0.10917835682630539, "max_score_of_first_answer": 0.9932570457458496, "min_score_of_first_answer": 0.0003887345956172794, "avg_score": 0.10917835682630539, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4198950065038819, "std_of_first_answer": 0.4198950065038819, "avg_entropy_of_first_answer": 1.4199332749204976, "lex_sim": 1.0}
{"question": "What drink consists of vodka, Triple Sec, cranberry juice, and lime juice?", "answer": ["Vodca Cranberry"], "scores": [[0.17283333837985992, 0.9397164583206177, 0.008388807997107506, 0.10011813044548035, 0.9596387147903442, 0.7421483397483826]], "normalized_score": [0.21440714597702026], "tokens": [["\u2581V", "od", "ca", "\u2581C", "ran", "berry"]], "entropy": [[4.269399642944336, 0.5792485475540161, 1.4451197385787964, 3.927316665649414, 0.3572719097137451, 1.6545984745025635]], "avg_entropy": [2.038825829823812], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cosmopolitan (disambiguation)", "score_of_biggest_cluster": 0.21440714597702026, "score_of_first_answer": 0.21440714597702026, "max_score_of_first_answer": 0.9596387147903442, "min_score_of_first_answer": 0.008388807997107506, "avg_score": 0.21440714597702026, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40226174073188076, "std_of_first_answer": 0.40226174073188076, "avg_entropy_of_first_answer": 2.038825829823812, "lex_sim": 1.0}
{"question": "Which vegetable is added to Lasagne to make 'Lasagne Verdi'?", "answer": ["Sweet peppers"], "scores": [[0.015439542941749096, 0.37838903069496155, 0.24663962423801422, 0.2890078127384186]], "normalized_score": [0.14285200834274292], "tokens": [["\u2581S", "weet", "\u2581pe", "ppers"]], "entropy": [[4.282721996307373, 2.53727650642395, 3.141965389251709, 1.3347902297973633]], "avg_entropy": [2.824188530445099], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eulithis mellinata", "score_of_biggest_cluster": 0.14285200834274292, "score_of_first_answer": 0.14285200834274292, "max_score_of_first_answer": 0.37838903069496155, "min_score_of_first_answer": 0.015439542941749096, "avg_score": 0.14285200834274292, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1339700008661503, "std_of_first_answer": 0.1339700008661503, "avg_entropy_of_first_answer": 2.824188530445099, "lex_sim": 1.0}
{"question": "Kylie Minogue played Charlene Robinson in what soap opera?", "answer": ["Neighbours"], "scores": [[0.8552979230880737, 0.9882169365882874, 0.9857513308525085, 0.9692232608795166]], "normalized_score": [0.9479604959487915], "tokens": [["\u2581Ne", "igh", "b", "ours"]], "entropy": [[1.0223848819732666, 0.10496754199266434, 0.09793201833963394, 0.14328116178512573]], "avg_entropy": [0.34214140102267265], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "List of Neighbours characters (1986)", "score_of_biggest_cluster": 0.9479604959487915, "score_of_first_answer": 0.9479604959487915, "max_score_of_first_answer": 0.9882169365882874, "min_score_of_first_answer": 0.8552979230880737, "avg_score": 0.9479604959487915, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0549457427625907, "std_of_first_answer": 0.0549457427625907, "avg_entropy_of_first_answer": 0.34214140102267265, "lex_sim": 1.0}
{"question": "Which word begining with V describes the type of image formed when light is reflected from a plane mirror?", "answer": ["Virtual"], "scores": [[0.34144145250320435]], "normalized_score": [0.34144148230552673], "tokens": [["\u2581Virtual"]], "entropy": [[3.0888497829437256]], "avg_entropy": [3.0888497829437256], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Virtual (disambiguation)", "score_of_biggest_cluster": 0.34144148230552673, "score_of_first_answer": 0.34144148230552673, "max_score_of_first_answer": 0.34144145250320435, "min_score_of_first_answer": 0.34144145250320435, "avg_score": 0.34144148230552673, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.0888497829437256, "lex_sim": 1.0}
{"question": "According to the song, what killed the radio star?", "answer": ["The corruption of the morals of the band"], "scores": [[0.2842682898044586, 0.00841075275093317, 0.23154185712337494, 0.29404744505882263, 0.5135787725448608, 0.01422777958214283, 0.92397141456604, 0.5649732947349548, 0.6169705986976624, 0.0026939320378005505]], "normalized_score": [0.12629005312919617], "tokens": [["\u2581The", "\u2581cor", "ruption", "\u2581of", "\u2581the", "\u2581mor", "als", "\u2581of", "\u2581the", "\u2581band"]], "entropy": [[4.848844051361084, 6.17408561706543, 3.2233994007110596, 2.042414426803589, 3.3069398403167725, 6.181024551391602, 0.45378488302230835, 1.7464666366577148, 2.209442615509033, 4.943439960479736]], "avg_entropy": [3.512984198331833], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Video format (version 2)", "score_of_biggest_cluster": 0.12629005312919617, "score_of_first_answer": 0.12629005312919617, "max_score_of_first_answer": 0.92397141456604, "min_score_of_first_answer": 0.0026939320378005505, "avg_score": 0.12629005312919617, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2906912529242865, "std_of_first_answer": 0.2906912529242865, "avg_entropy_of_first_answer": 3.512984198331833, "lex_sim": 1.0}
{"question": "Davy Crocket was killed at the Alamo, which other famous American pioneer died in the same action?", "answer": ["William Travis"], "scores": [[0.19060266017913818, 0.48585179448127747, 0.9948503375053406]], "normalized_score": [0.451644629240036], "tokens": [["\u2581William", "\u2581Tra", "vis"]], "entropy": [[3.085930347442627, 1.4656040668487549, 0.045998286455869675]], "avg_entropy": [1.5325109002490838], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Colonel Bowie", "score_of_biggest_cluster": 0.451644629240036, "score_of_first_answer": 0.451644629240036, "max_score_of_first_answer": 0.9948503375053406, "min_score_of_first_answer": 0.19060266017913818, "avg_score": 0.451644629240036, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3321756387133123, "std_of_first_answer": 0.3321756387133123, "avg_entropy_of_first_answer": 1.5325109002490838, "lex_sim": 1.0}
{"question": "Considered unlucky in Italy, probably because its Roman numerals are an anagram of a Latin word which can mean loosely 'I have lived', implying 'I am dead' ?", "answer": ["Numeric Value: 196"], "scores": [[0.009984483942389488, 0.001918730209581554, 0.0010111770825460553, 0.08047337830066681, 0.5972229838371277, 0.27034860849380493, 0.08547934889793396, 0.10171614587306976]], "normalized_score": [0.03487517684698105], "tokens": [["\u2581N", "umeric", "\u2581Value", ":", "\u2581", "1", "9", "6"]], "entropy": [[5.311628341674805, 3.023552894592285, 3.2731189727783203, 2.550799608230591, 2.994201183319092, 2.0958070755004883, 2.5079140663146973, 2.617373466491699]], "avg_entropy": [3.046799451112747], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "seventeen", "score_of_biggest_cluster": 0.03487517684698105, "score_of_first_answer": 0.03487517684698105, "max_score_of_first_answer": 0.5972229838371277, "min_score_of_first_answer": 0.0010111770825460553, "avg_score": 0.03487517684698105, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19013628915995584, "std_of_first_answer": 0.19013628915995584, "avg_entropy_of_first_answer": 3.046799451112747, "lex_sim": 1.0}
{"question": "What did my true love bring to me on the Sixth Day of Christmas?", "answer": ["Three drummers, two pipers, one lute, and the partridge"], "scores": [[0.01971954107284546, 0.002918798476457596, 0.9675302505493164, 0.025006556883454323, 0.6714532375335693, 0.7694687843322754, 0.9951077699661255, 0.8228304982185364, 0.0859977975487709, 0.20021788775920868, 0.6497215032577515, 0.28135740756988525, 0.5470675230026245, 0.019754065200686455, 0.19080744683742523, 0.9898138642311096]], "normalized_score": [0.19325324892997742], "tokens": [["\u2581Three", "\u2581drum", "mers", ",", "\u2581two", "\u2581pip", "ers", ",", "\u2581one", "\u2581l", "ute", ",", "\u2581and", "\u2581the", "\u2581part", "ridge"]], "entropy": [[2.6636669635772705, 1.9235432147979736, 0.22480452060699463, 1.321619987487793, 1.90872061252594, 1.1838946342468262, 0.03323148563504219, 0.7011681199073792, 1.3921021223068237, 4.720338821411133, 1.7033579349517822, 1.36874520778656, 2.27805233001709, 1.8154661655426025, 5.0361151695251465, 0.07238510996103287]], "avg_entropy": [1.7717007750179619], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Six Geese-A-Laying", "score_of_biggest_cluster": 0.19325324892997742, "score_of_first_answer": 0.19325324892997742, "max_score_of_first_answer": 0.9951077699661255, "min_score_of_first_answer": 0.002918798476457596, "avg_score": 0.19325324892997742, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37391208609095133, "std_of_first_answer": 0.37391208609095133, "avg_entropy_of_first_answer": 1.7717007750179619, "lex_sim": 1.0}
{"question": "Lasting from 1299 to 1922, the Ottoman empire roughly corresponds to what modern day country?", "answer": ["Turkistan"], "scores": [[0.0016660112887620926, 0.05022081732749939]], "normalized_score": [0.009147044271230698], "tokens": [["\u2581Turk", "istan"]], "entropy": [[1.9091795682907104, 3.012918472290039]], "avg_entropy": [2.4610490202903748], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Turkey", "score_of_biggest_cluster": 0.009147044271230698, "score_of_first_answer": 0.009147044271230698, "max_score_of_first_answer": 0.05022081732749939, "min_score_of_first_answer": 0.0016660112887620926, "avg_score": 0.009147044271230698, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02427740301936865, "std_of_first_answer": 0.02427740301936865, "avg_entropy_of_first_answer": 2.4610490202903748, "lex_sim": 1.0}
{"question": "In a film script, what do the letters ECU mean?", "answer": ["English Civil War"], "scores": [[0.015270897187292576, 0.03090365044772625, 0.6069172024726868]], "normalized_score": [0.06591758131980896], "tokens": [["\u2581English", "\u2581Civil", "\u2581War"]], "entropy": [[3.8091492652893066, 5.299022674560547, 1.7414193153381348]], "avg_entropy": [3.616530418395996], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Extreme Close Up", "score_of_biggest_cluster": 0.06591758131980896, "score_of_first_answer": 0.06591758131980896, "max_score_of_first_answer": 0.6069172024726868, "min_score_of_first_answer": 0.015270897187292576, "avg_score": 0.06591758131980896, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2752940540280462, "std_of_first_answer": 0.2752940540280462, "avg_entropy_of_first_answer": 3.616530418395996, "lex_sim": 1.0}
{"question": "Which colour scores the most on an archery target", "answer": ["Red"], "scores": [[0.1629558950662613]], "normalized_score": [0.1629558950662613], "tokens": [["\u2581Red"]], "entropy": [[3.457439661026001]], "avg_entropy": [3.457439661026001], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gold compound", "score_of_biggest_cluster": 0.1629558950662613, "score_of_first_answer": 0.1629558950662613, "max_score_of_first_answer": 0.1629558950662613, "min_score_of_first_answer": 0.1629558950662613, "avg_score": 0.1629558950662613, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.457439661026001, "lex_sim": 1.0}
{"question": "To which family of trees do junipers belong?", "answer": ["Cupressaceae"], "scores": [[0.12343862652778625, 0.9656122922897339, 0.7954094409942627]], "normalized_score": [0.4559825360774994], "tokens": [["\u2581Cu", "press", "aceae"]], "entropy": [[3.5037598609924316, 0.26425647735595703, 0.9899137020111084]], "avg_entropy": [1.5859766801198323], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cyprus Tree", "score_of_biggest_cluster": 0.4559825360774994, "score_of_first_answer": 0.4559825360774994, "max_score_of_first_answer": 0.9656122922897339, "min_score_of_first_answer": 0.12343862652778625, "avg_score": 0.4559825360774994, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3635886365090766, "std_of_first_answer": 0.3635886365090766, "avg_entropy_of_first_answer": 1.5859766801198323, "lex_sim": 1.0}
{"question": "SS Ancon was the first ship to navigate what single-vowelled passage on 15 August 1914?", "answer": ["Northwest Passage"], "scores": [[0.04456373676657677, 0.3848711848258972, 0.09900211542844772, 0.9937446713447571]], "normalized_score": [0.20267680287361145], "tokens": [["\u2581North", "west", "\u2581Pass", "age"]], "entropy": [[2.96631121635437, 2.194159984588623, 1.3222298622131348, 0.054653409868478775]], "avg_entropy": [1.6343386182561517], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Panama Canal", "score_of_biggest_cluster": 0.20267680287361145, "score_of_first_answer": 0.20267680287361145, "max_score_of_first_answer": 0.9937446713447571, "min_score_of_first_answer": 0.04456373676657677, "avg_score": 0.20267680287361145, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37688935972926246, "std_of_first_answer": 0.37688935972926246, "avg_entropy_of_first_answer": 1.6343386182561517, "lex_sim": 1.0}
{"question": "Who famously had children with both Julius Caesar and Mark Antony?", "answer": ["Cleopatra VII"], "scores": [[0.9191349148750305, 0.9946157336235046, 0.9982909560203552, 0.30776548385620117]], "normalized_score": [0.7279942035675049], "tokens": [["\u2581Cle", "op", "atra", "\u2581VII"]], "entropy": [[0.6179497241973877, 0.04602036625146866, 0.0177457258105278, 1.1838548183441162]], "avg_entropy": [0.4663926586508751], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kleopatra", "score_of_biggest_cluster": 0.7279942035675049, "score_of_first_answer": 0.7279942035675049, "max_score_of_first_answer": 0.9982909560203552, "min_score_of_first_answer": 0.30776548385620117, "avg_score": 0.7279942035675049, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2887838524101578, "std_of_first_answer": 0.2887838524101578, "avg_entropy_of_first_answer": 0.4663926586508751, "lex_sim": 1.0}
{"question": "Dr Gunther Van Hagens caused controversy on TV by performing which medical procedure live?", "answer": ["Human autopsyi"], "scores": [[0.00553344190120697, 0.10435809940099716, 0.9667491316795349, 6.870486231491668e-06]], "normalized_score": [0.007869656197726727], "tokens": [["\u2581Human", "\u2581aut", "ops", "yi"]], "entropy": [[4.927006721496582, 2.0655672550201416, 0.22287726402282715, 0.2244715839624405]], "avg_entropy": [1.8599807061254978], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "AN AUTOPSY", "score_of_biggest_cluster": 0.007869656197726727, "score_of_first_answer": 0.007869656197726727, "max_score_of_first_answer": 0.9667491316795349, "min_score_of_first_answer": 6.870486231491668e-06, "avg_score": 0.007869656197726727, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4048865990392547, "std_of_first_answer": 0.4048865990392547, "avg_entropy_of_first_answer": 1.8599807061254978, "lex_sim": 1.0}
{"question": "In which Hampshire village is the National Motor Museum?", "answer": ["Beaulieu"], "scores": [[0.9419078826904297, 0.9926636219024658, 0.9923486709594727, 0.9968909621238708]], "normalized_score": [0.9806874394416809], "tokens": [["\u2581Be", "a", "ul", "ieu"]], "entropy": [[0.5364899039268494, 0.06328915804624557, 0.0582609586417675, 0.029838846996426582]], "avg_entropy": [0.17196971690282226], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Beaulieu", "score_of_biggest_cluster": 0.9806874394416809, "score_of_first_answer": 0.9806874394416809, "max_score_of_first_answer": 0.9968909621238708, "min_score_of_first_answer": 0.9419078826904297, "avg_score": 0.9806874394416809, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.022613822123610756, "std_of_first_answer": 0.022613822123610756, "avg_entropy_of_first_answer": 0.17196971690282226, "lex_sim": 1.0}
{"question": "In the 2015 MTV awards who was voted as the best male act", "answer": ["Justin Bieber"], "scores": [[0.38358864188194275, 0.8266050815582275, 0.9844911694526672, 0.9974820017814636]], "normalized_score": [0.7469992637634277], "tokens": [["\u2581Justin", "\u2581B", "ie", "ber"]], "entropy": [[3.035886764526367, 0.92805415391922, 0.15041586756706238, 0.02719929814338684]], "avg_entropy": [1.035389021039009], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bieber Fever", "score_of_biggest_cluster": 0.7469992637634277, "score_of_first_answer": 0.7469992637634277, "max_score_of_first_answer": 0.9974820017814636, "min_score_of_first_answer": 0.38358864188194275, "avg_score": 0.7469992637634277, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24855937056531924, "std_of_first_answer": 0.24855937056531924, "avg_entropy_of_first_answer": 1.035389021039009, "lex_sim": 1.0}
{"question": "Turin lies on which river?", "answer": ["Sangone River [Genoa, Italy]"], "scores": [[0.0007899945485405624, 0.04619902744889259, 0.03085712529718876, 0.00019681720004882663, 0.011944777332246304, 0.4492999017238617, 0.7161652445793152, 0.27201634645462036, 0.8025678992271423, 0.9399162530899048]], "normalized_score": [0.05299767851829529], "tokens": [["\u2581Sang", "one", "\u2581River", "\u2581[", "G", "eno", "a", ",", "\u2581Italy", "]"]], "entropy": [[2.140765428543091, 3.852642059326172, 0.5506744980812073, 0.3642672300338745, 5.473363399505615, 3.292710781097412, 0.8474997282028198, 2.1188528537750244, 1.4442529678344727, 0.4456586241722107]], "avg_entropy": [2.05306875705719], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Po (disambiguation)", "score_of_biggest_cluster": 0.05299767851829529, "score_of_first_answer": 0.05299767851829529, "max_score_of_first_answer": 0.9399162530899048, "min_score_of_first_answer": 0.00019681720004882663, "avg_score": 0.05299767851829529, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.353593564034343, "std_of_first_answer": 0.353593564034343, "avg_entropy_of_first_answer": 2.05306875705719, "lex_sim": 1.0}
{"question": "What slippery synthetic substance was Ronald Reagan compared to - because no scandals stuck to him?", "answer": ["Teflon"], "scores": [[0.2858438193798065, 0.9996181726455688, 0.9972038269042969]], "normalized_score": [0.6580349206924438], "tokens": [["\u2581T", "ef", "lon"]], "entropy": [[4.624458312988281, 0.004133269656449556, 0.02548259124159813]], "avg_entropy": [1.5513580579621096], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Teflon (trademark)", "score_of_biggest_cluster": 0.6580349206924438, "score_of_first_answer": 0.6580349206924438, "max_score_of_first_answer": 0.9996181726455688, "min_score_of_first_answer": 0.2858438193798065, "avg_score": 0.6580349206924438, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.33590883630178237, "std_of_first_answer": 0.33590883630178237, "avg_entropy_of_first_answer": 1.5513580579621096, "lex_sim": 1.0}
{"question": "Mark Bolland is the current Dutch born CEO of which retail chain?", "answer": ["Zalm's Tops"], "scores": [[0.006253221537917852, 0.00048146123299375176, 0.024524033069610596, 0.988544762134552, 0.001338059431873262, 0.15855439007282257]], "normalized_score": [0.015787659212946892], "tokens": [["\u2581Z", "alm", "'", "s", "\u2581T", "ops"]], "entropy": [[4.188833713531494, 1.586024284362793, 4.208844184875488, 0.11762160062789917, 1.8483049869537354, 5.021507263183594]], "avg_entropy": [2.828522672255834], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mands", "score_of_biggest_cluster": 0.015787659212946892, "score_of_first_answer": 0.015787659212946892, "max_score_of_first_answer": 0.988544762134552, "min_score_of_first_answer": 0.00048146123299375176, "avg_score": 0.015787659212946892, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35848191826131215, "std_of_first_answer": 0.35848191826131215, "avg_entropy_of_first_answer": 2.828522672255834, "lex_sim": 1.0}
{"question": "Who wrote the plays \u2018The Cocktail Party\u2019 and \u2018The Family Reunion\u2019?", "answer": ["T. S. Eliot"], "scores": [[0.6291255354881287, 0.6591885685920715, 0.5936394929885864, 0.9722371697425842, 0.93486487865448, 0.9983695149421692]], "normalized_score": [0.7789573669433594], "tokens": [["\u2581T", ".", "\u2581S", ".", "\u2581Eli", "ot"]], "entropy": [[2.2699265480041504, 1.1988084316253662, 0.8712714910507202, 0.16302162408828735, 0.2919342517852783, 0.014471744187176228]], "avg_entropy": [0.8015723484568298], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "TS Elliot", "score_of_biggest_cluster": 0.7789573669433594, "score_of_first_answer": 0.7789573669433594, "max_score_of_first_answer": 0.9983695149421692, "min_score_of_first_answer": 0.5936394929885864, "avg_score": 0.7789573669433594, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.17262140117098124, "std_of_first_answer": 0.17262140117098124, "avg_entropy_of_first_answer": 0.8015723484568298, "lex_sim": 1.0}
{"question": "Which word can mean a unit of measurement of a small island?", "answer": ["island, rock, or peak"], "scores": [[0.004128824919462204, 0.01320322509855032, 0.023793485015630722, 0.3023970127105713, 0.22757084667682648, 0.0028592075686901808]], "normalized_score": [0.025185512378811836], "tokens": [["\u2581island", ",", "\u2581rock", ",", "\u2581or", "\u2581peak"]], "entropy": [[5.023667812347412, 1.1690685749053955, 5.162821292877197, 1.1376758813858032, 4.1834516525268555, 4.676522254943848]], "avg_entropy": [3.5588679114977517], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "English inch", "score_of_biggest_cluster": 0.025185512378811836, "score_of_first_answer": 0.025185512378811836, "max_score_of_first_answer": 0.3023970127105713, "min_score_of_first_answer": 0.0028592075686901808, "avg_score": 0.025185512378811836, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1218568303905697, "std_of_first_answer": 0.1218568303905697, "avg_entropy_of_first_answer": 3.5588679114977517, "lex_sim": 1.0}
{"question": "What nickname has been given to the Velodrome in the Olympic Park because of the shape of its roof?", "answer": ["The VELOCE"], "scores": [[0.6296377778053284, 0.021448958665132523, 0.019156834110617638, 0.8657999038696289, 0.00416605407372117]], "normalized_score": [0.062228985130786896], "tokens": [["\u2581The", "\u2581V", "E", "LO", "CE"]], "entropy": [[2.3166182041168213, 3.8121135234832764, 3.6396985054016113, 0.826430082321167, 2.897819995880127]], "avg_entropy": [2.6985360622406005], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pringle", "score_of_biggest_cluster": 0.062228985130786896, "score_of_first_answer": 0.062228985130786896, "max_score_of_first_answer": 0.8657999038696289, "min_score_of_first_answer": 0.00416605407372117, "avg_score": 0.062228985130786896, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.366728380291547, "std_of_first_answer": 0.366728380291547, "avg_entropy_of_first_answer": 2.6985360622406005, "lex_sim": 1.0}
{"question": "Which optical toy was first patented in 1817 by David Brewster?", "answer": ["Kaleidoscope"], "scores": [[0.096468985080719, 0.9991126656532288, 0.9026983380317688, 0.9716481566429138]], "normalized_score": [0.5392168760299683], "tokens": [["\u2581K", "ale", "idos", "cope"]], "entropy": [[3.1770524978637695, 0.010937875136733055, 0.4039708077907562, 0.13740794360637665]], "avg_entropy": [0.9323422810994089], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kalidescope", "score_of_biggest_cluster": 0.5392168760299683, "score_of_first_answer": 0.5392168760299683, "max_score_of_first_answer": 0.9991126656532288, "min_score_of_first_answer": 0.096468985080719, "avg_score": 0.5392168760299683, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.37462598559146554, "std_of_first_answer": 0.37462598559146554, "avg_entropy_of_first_answer": 0.9323422810994089, "lex_sim": 1.0}
{"question": "\"In 2011 the Classic Brit Awards named who as \"\"Artist of the Decade\"\"?\"", "answer": ["Adele"], "scores": [[0.12512008845806122, 0.2226209044456482, 0.9959361553192139]], "normalized_score": [0.30272024869918823], "tokens": [["\u2581A", "de", "le"]], "entropy": [[3.597714900970459, 0.8422883152961731, 0.042633987963199615]], "avg_entropy": [1.4942124014099438], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Il DIvo", "score_of_biggest_cluster": 0.30272024869918823, "score_of_first_answer": 0.30272024869918823, "max_score_of_first_answer": 0.9959361553192139, "min_score_of_first_answer": 0.12512008845806122, "avg_score": 0.30272024869918823, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38956435906099274, "std_of_first_answer": 0.38956435906099274, "avg_entropy_of_first_answer": 1.4942124014099438, "lex_sim": 1.0}
{"question": "In which year was Sputnik, the first man-made satellite, launched?", "answer": ["1957"], "scores": [[0.6029232144355774, 0.9239597916603088, 0.9583678245544434, 0.9819372296333313, 0.9530296325683594]], "normalized_score": [0.8704172372817993], "tokens": [["\u2581", "1", "9", "5", "7"]], "entropy": [[2.205770969390869, 0.38047653436660767, 0.2884427309036255, 0.14514359831809998, 0.27461087703704834]], "avg_entropy": [0.6588889420032501], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and fifty-seven", "score_of_biggest_cluster": 0.8704172372817993, "score_of_first_answer": 0.8704172372817993, "max_score_of_first_answer": 0.9819372296333313, "min_score_of_first_answer": 0.6029232144355774, "avg_score": 0.8704172372817993, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.14176621551305035, "std_of_first_answer": 0.14176621551305035, "avg_entropy_of_first_answer": 0.6588889420032501, "lex_sim": 1.0}
{"question": "Botswana was known by what name until 1966?", "answer": ["Bechuanaland"], "scores": [[0.8969207406044006, 0.9859937429428101, 0.9955044388771057, 0.9934483170509338, 0.9953977465629578]], "normalized_score": [0.9726636409759521], "tokens": [["\u2581Be", "ch", "u", "anal", "and"]], "entropy": [[0.7019979953765869, 0.13418149948120117, 0.04294098913669586, 0.04581617936491966, 0.044578805565834045]], "avg_entropy": [0.19390309378504753], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bechuanaland protectorate", "score_of_biggest_cluster": 0.9726636409759521, "score_of_first_answer": 0.9726636409759521, "max_score_of_first_answer": 0.9955044388771057, "min_score_of_first_answer": 0.8969207406044006, "avg_score": 0.9726636409759521, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03842423069882714, "std_of_first_answer": 0.03842423069882714, "avg_entropy_of_first_answer": 0.19390309378504753, "lex_sim": 1.0}
{"question": "By the year 2010 there were roughly how many bicycles worldwide?", "answer": ["An estimate 550 million bicycles"], "scores": [[0.00047445279778912663, 0.060839854180812836, 0.09722167998552322, 0.17508016526699066, 0.08111028373241425, 0.7282019853591919, 0.7589713335037231, 0.023036912083625793, 0.9755390882492065, 0.9927396178245544]], "normalized_score": [0.11725837737321854], "tokens": [["\u2581An", "\u2581estimate", "\u2581", "5", "5", "0", "\u2581million", "\u2581b", "icy", "cles"]], "entropy": [[2.946683883666992, 1.7166272401809692, 1.3299607038497925, 1.9836534261703491, 1.4584521055221558, 1.3219430446624756, 1.1888507604599, 0.7266231775283813, 0.1242656335234642, 0.05059649795293808]], "avg_entropy": [1.2847656473517417], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1billion", "score_of_biggest_cluster": 0.11725837737321854, "score_of_first_answer": 0.11725837737321854, "max_score_of_first_answer": 0.9927396178245544, "min_score_of_first_answer": 0.00047445279778912663, "avg_score": 0.11725837737321854, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3973320895365847, "std_of_first_answer": 0.3973320895365847, "avg_entropy_of_first_answer": 1.2847656473517417, "lex_sim": 1.0}
{"question": "\"Which is the Muppet character who appeared in \"\"The Muppet Show\"\" from the third season and in a number of Muppet films, who throws fish which return to him and plays a fish organ (a line of fish that, when squeezed, each gargle a different note)?\"", "answer": ["Fozzie Bear"], "scores": [[0.08036332577466965, 0.7645029425621033, 0.9190938472747803, 0.8273747563362122]], "normalized_score": [0.4649163484573364], "tokens": [["\u2581F", "oz", "zie", "\u2581Bear"]], "entropy": [[4.492995738983154, 0.794451117515564, 0.4103415906429291, 0.7319700717926025]], "avg_entropy": [1.6074396297335625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Robin (Muppet)", "score_of_biggest_cluster": 0.4649163484573364, "score_of_first_answer": 0.4649163484573364, "max_score_of_first_answer": 0.9190938472747803, "min_score_of_first_answer": 0.08036332577466965, "avg_score": 0.4649163484573364, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3322090442000746, "std_of_first_answer": 0.3322090442000746, "avg_entropy_of_first_answer": 1.6074396297335625, "lex_sim": 1.0}
{"question": "Canuck is a nickname for someone from what country?", "answer": ["Canuk"], "scores": [[0.006511039100587368, 0.01407278049737215]], "normalized_score": [0.009572275914251804], "tokens": [["\u2581Can", "uk"]], "entropy": [[0.9842244386672974, 3.4049811363220215]], "avg_entropy": [2.1946027874946594], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Canada", "score_of_biggest_cluster": 0.009572275914251804, "score_of_first_answer": 0.009572275914251804, "max_score_of_first_answer": 0.01407278049737215, "min_score_of_first_answer": 0.006511039100587368, "avg_score": 0.009572275914251804, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.003780870698392391, "std_of_first_answer": 0.003780870698392391, "avg_entropy_of_first_answer": 2.1946027874946594, "lex_sim": 1.0}
{"question": "Which amendment to the U.S. Constitution says that no one may be forced to testify against himself", "answer": ["Constitutional 5th amendment"], "scores": [[0.0009893863461911678, 0.5355687141418457, 0.03612266853451729, 0.37427568435668945, 0.8279594779014587, 0.11155929416418076, 0.9758729934692383, 0.9959820508956909]], "normalized_score": [0.16828253865242004], "tokens": [["\u2581Constitution", "al", "\u2581", "5", "th", "\u2581am", "end", "ment"]], "entropy": [[2.426137924194336, 1.7318787574768066, 2.560908317565918, 2.7054717540740967, 0.6843203902244568, 1.171465277671814, 0.15981611609458923, 0.03851492702960968]], "avg_entropy": [1.4348141830414534], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Fifth (disambiguation)", "score_of_biggest_cluster": 0.16828253865242004, "score_of_first_answer": 0.16828253865242004, "max_score_of_first_answer": 0.9959820508956909, "min_score_of_first_answer": 0.0009893863461911678, "avg_score": 0.16828253865242004, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38885306044178625, "std_of_first_answer": 0.38885306044178625, "avg_entropy_of_first_answer": 1.4348141830414534, "lex_sim": 1.0}
{"question": "After the murder of his children's nanny, Sandra Rivett, Richard John Bingham disappeared on 8 November 1974, was presumed deceased on 11 December 1992 and declared legally dead in October 1999. By what name is he better known?", "answer": ["Lord Lucan"], "scores": [[0.7297289371490479, 0.9925417900085449, 0.9995962977409363]], "normalized_score": [0.8979352116584778], "tokens": [["\u2581Lord", "\u2581Luc", "an"]], "entropy": [[1.914005994796753, 0.07512468099594116, 0.005148166790604591]], "avg_entropy": [0.6647596141944329], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Richard Bingham, 7th Earl of Lucan", "score_of_biggest_cluster": 0.8979352116584778, "score_of_first_answer": 0.8979352116584778, "max_score_of_first_answer": 0.9995962977409363, "min_score_of_first_answer": 0.7297289371490479, "avg_score": 0.8979352116584778, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.12558695704585035, "std_of_first_answer": 0.12558695704585035, "avg_entropy_of_first_answer": 0.6647596141944329, "lex_sim": 1.0}
{"question": "The six series of which sitcom featuring Richard O'Sullivan, Paula Wilcox and Sally Thomsett were first broadcast in the 1970s?", "answer": ["Man About the House"], "scores": [[0.7326356172561646, 0.8199386596679688, 0.694547176361084, 0.9700913429260254]], "normalized_score": [0.7976198196411133], "tokens": [["\u2581Man", "\u2581About", "\u2581the", "\u2581House"]], "entropy": [[1.570405125617981, 0.5070646405220032, 0.649685263633728, 0.17777840793132782]], "avg_entropy": [0.72623335942626], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Man About the House (film)", "score_of_biggest_cluster": 0.7976198196411133, "score_of_first_answer": 0.7976198196411133, "max_score_of_first_answer": 0.9700913429260254, "min_score_of_first_answer": 0.694547176361084, "avg_score": 0.7976198196411133, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1059632047873927, "std_of_first_answer": 0.1059632047873927, "avg_entropy_of_first_answer": 0.72623335942626, "lex_sim": 1.0}
{"question": "Which comedy written by Johnny Speight which first aired in 1965, featured a fervent West Ham supporter?", "answer": ["Dad's Army"], "scores": [[0.10888854414224625, 0.8431215286254883, 0.9403988718986511, 0.9984188079833984, 0.9900463223457336]], "normalized_score": [0.6112664341926575], "tokens": [["\u2581D", "ad", "'", "s", "\u2581Army"]], "entropy": [[4.226792812347412, 0.8666243553161621, 0.39440542459487915, 0.016930455341935158, 0.06901109218597412]], "avg_entropy": [1.1147528279572725], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Else Garnett", "score_of_biggest_cluster": 0.6112664341926575, "score_of_first_answer": 0.6112664341926575, "max_score_of_first_answer": 0.9984188079833984, "min_score_of_first_answer": 0.10888854414224625, "avg_score": 0.6112664341926575, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3381880496635838, "std_of_first_answer": 0.3381880496635838, "avg_entropy_of_first_answer": 1.1147528279572725, "lex_sim": 1.0}
{"question": "Which English cricketer had figures for bowling 10 for 53 against Australia 1956?", "answer": ["Frank Tyson"], "scores": [[0.07238787412643433, 0.5665849447250366, 0.9947724938392639, 0.9979479908943176]], "normalized_score": [0.44920089840888977], "tokens": [["\u2581Frank", "\u2581T", "ys", "on"]], "entropy": [[3.7505245208740234, 2.945138454437256, 0.046664848923683167, 0.021539829671382904]], "avg_entropy": [1.6909669134765863], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jim Laker", "score_of_biggest_cluster": 0.44920089840888977, "score_of_first_answer": 0.44920089840888977, "max_score_of_first_answer": 0.9979479908943176, "min_score_of_first_answer": 0.07238787412643433, "avg_score": 0.44920089840888977, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3808800992127342, "std_of_first_answer": 0.3808800992127342, "avg_entropy_of_first_answer": 1.6909669134765863, "lex_sim": 1.0}
{"question": "What percentage of Mike Tyson's 1995 earnings came from endorsements?", "answer": ["22%"], "scores": [[0.27662503719329834, 0.13027843832969666, 0.07970689982175827, 0.27702459692955017]], "normalized_score": [0.1679556518793106], "tokens": [["\u2581", "2", "2", "%"]], "entropy": [[3.4554710388183594, 2.260281562805176, 2.5497796535491943, 2.312769889831543]], "avg_entropy": [2.644575536251068], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "0", "score_of_biggest_cluster": 0.1679556518793106, "score_of_first_answer": 0.1679556518793106, "max_score_of_first_answer": 0.27702459692955017, "min_score_of_first_answer": 0.07970689982175827, "avg_score": 0.1679556518793106, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08775691875701278, "std_of_first_answer": 0.08775691875701278, "avg_entropy_of_first_answer": 2.644575536251068, "lex_sim": 1.0}
{"question": "At the time of her construction in 2003, what was the longest, widest, tallest, largest passenger ship ever built?", "answer": ["SS Imperativ"], "scores": [[0.01011912152171135, 0.008666282519698143, 0.0008219198789447546]], "normalized_score": [0.004161678720265627], "tokens": [["\u2581SS", "\u2581Imper", "ativ"]], "entropy": [[3.719961166381836, 4.447828769683838, 0.6602505445480347]], "avg_entropy": [2.9426801602045694], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mary II", "score_of_biggest_cluster": 0.004161678720265627, "score_of_first_answer": 0.004161678720265627, "max_score_of_first_answer": 0.01011912152171135, "min_score_of_first_answer": 0.0008219198789447546, "avg_score": 0.004161678720265627, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.00408360849129623, "std_of_first_answer": 0.00408360849129623, "avg_entropy_of_first_answer": 2.9426801602045694, "lex_sim": 1.0}
{"question": "Which magazine is produced and sold for the benefit of homeless people?", "answer": ["Street Wise"], "scores": [[0.138543501496315, 0.14554360508918762, 0.951263427734375]], "normalized_score": [0.2676866352558136], "tokens": [["\u2581Street", "\u2581W", "ise"]], "entropy": [[4.311209678649902, 2.8787219524383545, 0.345327228307724]], "avg_entropy": [2.5117529531319938], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Big Issue in The North", "score_of_biggest_cluster": 0.2676866352558136, "score_of_first_answer": 0.2676866352558136, "max_score_of_first_answer": 0.951263427734375, "min_score_of_first_answer": 0.138543501496315, "avg_score": 0.2676866352558136, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38148061150063717, "std_of_first_answer": 0.38148061150063717, "avg_entropy_of_first_answer": 2.5117529531319938, "lex_sim": 1.0}
{"question": "Which is the largest city in Scandinavia by population ?", "answer": ["Osaka"], "scores": [[0.42331618070602417, 0.0012798610841855407]], "normalized_score": [0.023276297375559807], "tokens": [["\u2581Os", "aka"]], "entropy": [[1.8895654678344727, 0.04187038913369179]], "avg_entropy": [0.9657179284840822], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stockholm, Sweden", "score_of_biggest_cluster": 0.023276297375559807, "score_of_first_answer": 0.023276297375559807, "max_score_of_first_answer": 0.42331618070602417, "min_score_of_first_answer": 0.0012798610841855407, "avg_score": 0.023276297375559807, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21101815981091931, "std_of_first_answer": 0.21101815981091931, "avg_entropy_of_first_answer": 0.9657179284840822, "lex_sim": 1.0}
{"question": "Ari Folman's acclaimed 2008 film was 'Waltz with...' whom?", "answer": ["Death"], "scores": [[0.010843205265700817]], "normalized_score": [0.010843205265700817], "tokens": [["\u2581Death"]], "entropy": [[5.314130783081055]], "avg_entropy": [5.314130783081055], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bashir", "score_of_biggest_cluster": 0.010843205265700817, "score_of_first_answer": 0.010843205265700817, "max_score_of_first_answer": 0.010843205265700817, "min_score_of_first_answer": 0.010843205265700817, "avg_score": 0.010843205265700817, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.314130783081055, "lex_sim": 1.0}
{"question": "Cricketer W G Grace, who played first-class cricket from 1865 to 1908, also captained England from 1903 to 1908 in what other sport?", "answer": ["Association football"], "scores": [[0.10174345225095749, 0.6434004306793213]], "normalized_score": [0.255854994058609], "tokens": [["\u2581Association", "\u2581football"]], "entropy": [[3.7314469814300537, 1.2193317413330078]], "avg_entropy": [2.4753893613815308], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Crown-green bowls", "score_of_biggest_cluster": 0.255854994058609, "score_of_first_answer": 0.255854994058609, "max_score_of_first_answer": 0.6434004306793213, "min_score_of_first_answer": 0.10174345225095749, "avg_score": 0.255854994058609, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2708284892141819, "std_of_first_answer": 0.2708284892141819, "avg_entropy_of_first_answer": 2.4753893613815308, "lex_sim": 1.0}
{"question": "The Melrose Cup is awarded in which ball sport?", "answer": ["Polo"], "scores": [[0.020377472043037415, 0.9647232294082642]], "normalized_score": [0.14020921289920807], "tokens": [["\u2581Pol", "o"]], "entropy": [[3.059171676635742, 0.19635321199893951]], "avg_entropy": [1.6277624443173409], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Borders Game", "score_of_biggest_cluster": 0.14020921289920807, "score_of_first_answer": 0.14020921289920807, "max_score_of_first_answer": 0.9647232294082642, "min_score_of_first_answer": 0.020377472043037415, "avg_score": 0.14020921289920807, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4721728786826134, "std_of_first_answer": 0.4721728786826134, "avg_entropy_of_first_answer": 1.6277624443173409, "lex_sim": 1.0}
{"question": "\"Which European capital's English name is taken from \"\"black pool\"\" in its native language?\"", "answer": ["Bruxels"], "scores": [[0.009080545976758003, 0.056772153824567795, 0.24699504673480988]], "normalized_score": [0.050308939069509506], "tokens": [["\u2581Bru", "x", "els"]], "entropy": [[4.63701057434082, 1.8309297561645508, 3.654956340789795]], "avg_entropy": [3.374298890431722], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "UN/LOCODE:IEDUB", "score_of_biggest_cluster": 0.050308939069509506, "score_of_first_answer": 0.050308939069509506, "max_score_of_first_answer": 0.24699504673480988, "min_score_of_first_answer": 0.009080545976758003, "avg_score": 0.050308939069509506, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10277404996571836, "std_of_first_answer": 0.10277404996571836, "avg_entropy_of_first_answer": 3.374298890431722, "lex_sim": 1.0}
{"question": "In which country is the Moremi Game Reserve, a National Park, part of the Okavango Delta?", "answer": ["Botswana"], "scores": [[0.7289271950721741, 0.9800451397895813, 0.9912994503974915, 0.99598228931427]], "normalized_score": [0.9164244532585144], "tokens": [["\u2581B", "ots", "w", "ana"]], "entropy": [[1.6142020225524902, 0.15030282735824585, 0.07478169351816177, 0.03558363765478134]], "avg_entropy": [0.4687175452709198], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Country BWA", "score_of_biggest_cluster": 0.9164244532585144, "score_of_first_answer": 0.9164244532585144, "max_score_of_first_answer": 0.99598228931427, "min_score_of_first_answer": 0.7289271950721741, "avg_score": 0.9164244532585144, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.11281080043229326, "std_of_first_answer": 0.11281080043229326, "avg_entropy_of_first_answer": 0.4687175452709198, "lex_sim": 1.0}
{"question": "Suffrage is the right to do what?", "answer": ["Vote"], "scores": [[0.7463839054107666, 0.99736088514328]], "normalized_score": [0.8627943396568298], "tokens": [["\u2581V", "ote"]], "entropy": [[1.159658670425415, 0.02436807192862034]], "avg_entropy": [0.5920133711770177], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "I Voted", "score_of_biggest_cluster": 0.8627943396568298, "score_of_first_answer": 0.8627943396568298, "max_score_of_first_answer": 0.99736088514328, "min_score_of_first_answer": 0.7463839054107666, "avg_score": 0.8627943396568298, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1254884898662567, "std_of_first_answer": 0.1254884898662567, "avg_entropy_of_first_answer": 0.5920133711770177, "lex_sim": 1.0}
{"question": "In which country is the city of Kandahar?", "answer": ["Afghanistan"], "scores": [[0.8340866565704346, 0.9827169179916382]], "normalized_score": [0.9053568840026855], "tokens": [["\u2581Afghan", "istan"]], "entropy": [[1.1099357604980469, 0.13549964129924774]], "avg_entropy": [0.6227177008986473], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Afghanistan", "score_of_biggest_cluster": 0.9053568840026855, "score_of_first_answer": 0.9053568840026855, "max_score_of_first_answer": 0.9827169179916382, "min_score_of_first_answer": 0.8340866565704346, "avg_score": 0.9053568840026855, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0743151307106018, "std_of_first_answer": 0.0743151307106018, "avg_entropy_of_first_answer": 0.6227177008986473, "lex_sim": 1.0}
{"question": "The Kina is the monetary unit of which Oceania country?", "answer": ["Fijian dollar"], "scores": [[0.19940781593322754, 0.019815292209386826, 0.35146281123161316, 0.010046863928437233, 0.9958441853523254]], "normalized_score": [0.10679936408996582], "tokens": [["\u2581F", "ij", "ian", "\u2581doll", "ar"]], "entropy": [[3.307021141052246, 0.6368882060050964, 3.1017303466796875, 1.977470874786377, 0.04078587889671326]], "avg_entropy": [1.812779289484024], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ISO 3166-1:PG", "score_of_biggest_cluster": 0.10679936408996582, "score_of_first_answer": 0.10679936408996582, "max_score_of_first_answer": 0.9958441853523254, "min_score_of_first_answer": 0.010046863928437233, "avg_score": 0.10679936408996582, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3628690738808127, "std_of_first_answer": 0.3628690738808127, "avg_entropy_of_first_answer": 1.812779289484024, "lex_sim": 1.0}
{"question": "Who did Johnnie Cochran defend in a famous 1990s trial?", "answer": ["US basketball player Oj Simpson"], "scores": [[0.0018581970361992717, 0.07561872154474258, 0.7461735606193542, 0.6241467595100403, 0.006931311450898647, 0.3213689625263214, 0.9656022787094116]], "normalized_score": [0.10500486195087433], "tokens": [["\u2581US", "\u2581basketball", "\u2581player", "\u2581O", "j", "\u2581Sim", "pson"]], "entropy": [[2.484872579574585, 4.3750128746032715, 1.0676355361938477, 1.7740240097045898, 0.9477683305740356, 2.989025592803955, 0.23992443084716797]], "avg_entropy": [1.9826090506144933], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Orenthal James", "score_of_biggest_cluster": 0.10500486195087433, "score_of_first_answer": 0.10500486195087433, "max_score_of_first_answer": 0.9656022787094116, "min_score_of_first_answer": 0.0018581970361992717, "avg_score": 0.10500486195087433, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36133170858731944, "std_of_first_answer": 0.36133170858731944, "avg_entropy_of_first_answer": 1.9826090506144933, "lex_sim": 1.0}
{"question": "Which state on the Gulf of Mexico is nearest the end of the alphabet?", "answer": ["Alabama"], "scores": [[0.07023090124130249]], "normalized_score": [0.07023090869188309], "tokens": [["\u2581Alabama"]], "entropy": [[3.0407893657684326]], "avg_entropy": [3.0407893657684326], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Texas (U.S. State)", "score_of_biggest_cluster": 0.07023090869188309, "score_of_first_answer": 0.07023090869188309, "max_score_of_first_answer": 0.07023090124130249, "min_score_of_first_answer": 0.07023090124130249, "avg_score": 0.07023090869188309, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.0407893657684326, "lex_sim": 1.0}
{"question": "Filbert Fox is the mascot of which English football club?", "answer": ["Brentford"], "scores": [[0.011023937724530697, 0.42674052715301514, 0.9807974696159363]], "normalized_score": [0.1664792150259018], "tokens": [["\u2581Br", "ent", "ford"]], "entropy": [[3.7454922199249268, 1.0786197185516357, 0.1565587818622589]], "avg_entropy": [1.6602235734462738], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Leicester Fosse FC", "score_of_biggest_cluster": 0.1664792150259018, "score_of_first_answer": 0.1664792150259018, "max_score_of_first_answer": 0.9807974696159363, "min_score_of_first_answer": 0.011023937724530697, "avg_score": 0.1664792150259018, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39724888383516266, "std_of_first_answer": 0.39724888383516266, "avg_entropy_of_first_answer": 1.6602235734462738, "lex_sim": 1.0}
{"question": "\"What name is given to the maxim \"\"employees tend to rise to their level of incompetence.\"\"?\"", "answer": ["Peter principle"], "scores": [[0.8264501094818115, 0.17536374926567078]], "normalized_score": [0.3806959092617035], "tokens": [["\u2581Peter", "\u2581principle"]], "entropy": [[1.3833839893341064, 1.4368364810943604]], "avg_entropy": [1.4101102352142334], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hierarchiology", "score_of_biggest_cluster": 0.3806959092617035, "score_of_first_answer": 0.3806959092617035, "max_score_of_first_answer": 0.8264501094818115, "min_score_of_first_answer": 0.17536374926567078, "avg_score": 0.3806959092617035, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3255431801080704, "std_of_first_answer": 0.3255431801080704, "avg_entropy_of_first_answer": 1.4101102352142334, "lex_sim": 1.0}
{"question": "A Colles fracture is in which part of the body?", "answer": ["Ankle"], "scores": [[0.04909585788846016, 0.9214304685592651, 0.9929484128952026]], "normalized_score": [0.35547691583633423], "tokens": [["\u2581An", "k", "le"]], "entropy": [[3.761035680770874, 0.5308316946029663, 0.05096247419714928]], "avg_entropy": [1.4476099498569965], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wrist injuries", "score_of_biggest_cluster": 0.35547691583633423, "score_of_first_answer": 0.35547691583633423, "max_score_of_first_answer": 0.9929484128952026, "min_score_of_first_answer": 0.04909585788846016, "avg_score": 0.35547691583633423, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42907395567262435, "std_of_first_answer": 0.42907395567262435, "avg_entropy_of_first_answer": 1.4476099498569965, "lex_sim": 1.0}
{"question": "Kim Campbell was the first woman prime minister of which country", "answer": ["Canada"], "scores": [[0.8607105612754822]], "normalized_score": [0.8607105612754822], "tokens": [["\u2581Canada"]], "entropy": [[1.153719186782837]], "avg_entropy": [1.153719186782837], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Canada", "score_of_biggest_cluster": 0.8607105612754822, "score_of_first_answer": 0.8607105612754822, "max_score_of_first_answer": 0.8607105612754822, "min_score_of_first_answer": 0.8607105612754822, "avg_score": 0.8607105612754822, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.153719186782837, "lex_sim": 1.0}
{"question": "What does the now largely obsolete Bessemer Process produce?", "answer": ["H2S"], "scores": [[0.0013508428819477558, 0.014184530824422836, 0.13306330144405365]], "normalized_score": [0.013661322183907032], "tokens": [["\u2581H", "2", "S"]], "entropy": [[3.7123684883117676, 2.1877641677856445, 1.6616456508636475]], "avg_entropy": [2.52059276898702], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Steel", "score_of_biggest_cluster": 0.013661322183907032, "score_of_first_answer": 0.013661322183907032, "max_score_of_first_answer": 0.13306330144405365, "min_score_of_first_answer": 0.0013508428819477558, "avg_score": 0.013661322183907032, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05929684026687171, "std_of_first_answer": 0.05929684026687171, "avg_entropy_of_first_answer": 2.52059276898702, "lex_sim": 1.0}
{"question": "In 1976, which gymnast scored 7 maximum scores of 10 as she won three gold medals, one silver and one bronze?", "answer": ["Nadia Comaneci"], "scores": [[0.54008948802948, 0.9958117008209229, 0.6003076434135437, 0.68870609998703, 0.9817836880683899]], "normalized_score": [0.7375870943069458], "tokens": [["\u2581Nad", "ia", "\u2581Com", "ane", "ci"]], "entropy": [[1.2561933994293213, 0.041346170008182526, 2.3822617530822754, 1.89838707447052, 0.10976414382457733]], "avg_entropy": [1.1375905081629754], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nadia Elena Com\u0103neci", "score_of_biggest_cluster": 0.7375870943069458, "score_of_first_answer": 0.7375870943069458, "max_score_of_first_answer": 0.9958117008209229, "min_score_of_first_answer": 0.54008948802948, "avg_score": 0.7375870943069458, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1916931352139498, "std_of_first_answer": 0.1916931352139498, "avg_entropy_of_first_answer": 1.1375905081629754, "lex_sim": 1.0}
{"question": "Redfoo and Sky Blu were members of which electronic rock band?", "answer": ["LMFAO"], "scores": [[0.7660761475563049, 0.921229898929596, 0.9476616382598877, 0.9963809847831726]], "normalized_score": [0.9035031795501709], "tokens": [["\u2581L", "M", "FA", "O"]], "entropy": [[1.4314899444580078, 0.5547441244125366, 0.3492274880409241, 0.03796105831861496]], "avg_entropy": [0.5933556538075209], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "I AM NOT A WHORE", "score_of_biggest_cluster": 0.9035031795501709, "score_of_first_answer": 0.9035031795501709, "max_score_of_first_answer": 0.9963809847831726, "min_score_of_first_answer": 0.7660761475563049, "avg_score": 0.9035031795501709, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08617068538807586, "std_of_first_answer": 0.08617068538807586, "avg_entropy_of_first_answer": 0.5933556538075209, "lex_sim": 1.0}
{"question": "What six-letter word, taken from Yiddish, means \u2018to grumble, complain\u2019, especially when done to excess?", "answer": ["Grouching"], "scores": [[0.028564080595970154, 0.01920640468597412, 0.23029153048992157]], "normalized_score": [0.050178162753582], "tokens": [["\u2581Gr", "ouch", "ing"]], "entropy": [[4.632181644439697, 3.2308285236358643, 1.807020664215088]], "avg_entropy": [3.22334361076355], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kvetch (disambiguation)", "score_of_biggest_cluster": 0.050178162753582, "score_of_first_answer": 0.050178162753582, "max_score_of_first_answer": 0.23029153048992157, "min_score_of_first_answer": 0.01920640468597412, "avg_score": 0.050178162753582, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09737582432959968, "std_of_first_answer": 0.09737582432959968, "avg_entropy_of_first_answer": 3.22334361076355, "lex_sim": 1.0}
{"question": "Every major league baseball team has retired the number 42, in honor of what player who was born on January 31, 1919?", "answer": ["Jack Roosevelt Robinson"], "scores": [[0.8084269762039185, 0.21325871348381042, 0.9902076125144958, 0.9987215399742126, 0.9979008436203003, 0.8722705841064453]], "normalized_score": [0.7276281118392944], "tokens": [["\u2581Jack", "\u2581Ro", "ose", "vel", "t", "\u2581Robinson"]], "entropy": [[1.5684421062469482, 0.8827552795410156, 0.0990271046757698, 0.015547748655080795, 0.023251429200172424, 0.7611454725265503]], "avg_entropy": [0.5583615234742562], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jack Roosevelt %22Jackie%22 Robinson", "score_of_biggest_cluster": 0.7276281118392944, "score_of_first_answer": 0.7276281118392944, "max_score_of_first_answer": 0.9987215399742126, "min_score_of_first_answer": 0.21325871348381042, "avg_score": 0.7276281118392944, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2778805444403235, "std_of_first_answer": 0.2778805444403235, "avg_entropy_of_first_answer": 0.5583615234742562, "lex_sim": 1.0}
{"question": "\"What city is known to its inhabitants as \"\"auld reekie\"\"?\"", "answer": ["Athens"], "scores": [[0.0008350898860953748, 0.9453955292701721]], "normalized_score": [0.02809786982834339], "tokens": [["\u2581Ath", "ens"]], "entropy": [[1.1670787334442139, 0.4445522427558899]], "avg_entropy": [0.8058154881000519], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dun Eideann", "score_of_biggest_cluster": 0.02809786982834339, "score_of_first_answer": 0.02809786982834339, "max_score_of_first_answer": 0.9453955292701721, "min_score_of_first_answer": 0.0008350898860953748, "avg_score": 0.02809786982834339, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.47228021969203837, "std_of_first_answer": 0.47228021969203837, "avg_entropy_of_first_answer": 0.8058154881000519, "lex_sim": 1.0}
{"question": "Prince Rupert of the Rhine, who fought in the Thirty Years\u2019 War, was nephew of which English King, who made him Duke of Cumberland?", "answer": ["Charles I"], "scores": [[0.6775352954864502, 0.5668293833732605]], "normalized_score": [0.61971515417099], "tokens": [["\u2581Charles", "\u2581I"]], "entropy": [[1.4317376613616943, 1.5140039920806885]], "avg_entropy": [1.4728708267211914], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charles the First", "score_of_biggest_cluster": 0.61971515417099, "score_of_first_answer": 0.61971515417099, "max_score_of_first_answer": 0.6775352954864502, "min_score_of_first_answer": 0.5668293833732605, "avg_score": 0.61971515417099, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.05535295605659485, "std_of_first_answer": 0.05535295605659485, "avg_entropy_of_first_answer": 1.4728708267211914, "lex_sim": 1.0}
{"question": "In Greek mythology which of the Olympian gods was the goddess of fertility, agriculture, horticulture, grain and harvest?", "answer": ["Demeter"], "scores": [[0.8668624758720398, 0.9792152643203735]], "normalized_score": [0.9213278293609619], "tokens": [["\u2581Dem", "eter"]], "entropy": [[0.9211090803146362, 0.14527340233325958]], "avg_entropy": [0.5331912413239479], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "D\u00eam\u00eat\u00ear Iasi\u00f4n", "score_of_biggest_cluster": 0.9213278293609619, "score_of_first_answer": 0.9213278293609619, "max_score_of_first_answer": 0.9792152643203735, "min_score_of_first_answer": 0.8668624758720398, "avg_score": 0.9213278293609619, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05617639422416687, "std_of_first_answer": 0.05617639422416687, "avg_entropy_of_first_answer": 0.5331912413239479, "lex_sim": 1.0}
{"question": "Who was elected president of France follwing the death of President Pompidou?", "answer": ["Val\u00e9ry Giscard d'Estaing"], "scores": [[0.5206136703491211, 0.019933423027396202, 0.9926924109458923, 0.8995406627655029, 0.9911920428276062, 0.9957376718521118, 0.9034596085548401, 0.9530980587005615, 0.9448342323303223, 0.9994921684265137, 0.9958349466323853]], "normalized_score": [0.6402177214622498], "tokens": [["\u2581Val", "\u00e9", "ry", "\u2581G", "is", "card", "\u2581d", "'", "E", "sta", "ing"]], "entropy": [[2.021111488342285, 1.800647497177124, 0.07386800646781921, 0.7491010427474976, 0.09218152612447739, 0.04652665555477142, 0.4108594059944153, 0.3362407684326172, 0.3493024706840515, 0.005926355719566345, 0.035681210458278656]], "avg_entropy": [0.5383133116093549], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Giscard d\u2019Estaing", "score_of_biggest_cluster": 0.6402177214622498, "score_of_first_answer": 0.6402177214622498, "max_score_of_first_answer": 0.9994921684265137, "min_score_of_first_answer": 0.019933423027396202, "avg_score": 0.6402177214622498, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.29013639739154845, "std_of_first_answer": 0.29013639739154845, "avg_entropy_of_first_answer": 0.5383133116093549, "lex_sim": 1.0}
{"question": "What strife-torn country's Football Association was suspended by FIFA due to interference by its government?", "answer": ["F\u00e9d\u00e9ration Ivoirienne de Football"], "scores": [[0.003794420510530472, 0.0022704354487359524, 0.36139580607414246, 0.5663016438484192, 0.9403577446937561, 0.9918620586395264, 0.9657396674156189, 0.9676271080970764]], "normalized_score": [0.1876397281885147], "tokens": [["\u2581F", "\u00e9d\u00e9ration", "\u2581I", "vo", "iri", "enne", "\u2581de", "\u2581Football"]], "entropy": [[4.167986869812012, 3.432281017303467, 3.4578444957733154, 1.3033473491668701, 0.2794651985168457, 0.06311847269535065, 0.2282567024230957, 0.20621195435523987]], "avg_entropy": [1.6423140075057745], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eragh", "score_of_biggest_cluster": 0.1876397281885147, "score_of_first_answer": 0.1876397281885147, "max_score_of_first_answer": 0.9918620586395264, "min_score_of_first_answer": 0.0022704354487359524, "avg_score": 0.1876397281885147, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40452497995006326, "std_of_first_answer": 0.40452497995006326, "avg_entropy_of_first_answer": 1.6423140075057745, "lex_sim": 1.0}
{"question": "La Paz is the administrative capital of which South American country?", "answer": ["The Bolivian Republic"], "scores": [[0.0027985312044620514, 0.3855799734592438, 0.7379010319709778, 0.6478450298309326]], "normalized_score": [0.1507052630186081], "tokens": [["\u2581The", "\u2581Boliv", "ian", "\u2581Republic"]], "entropy": [[0.9455608129501343, 3.3893284797668457, 0.7887366414070129, 1.8556991815567017]], "avg_entropy": [1.7448312789201736], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bolivia, Plurinational State of", "score_of_biggest_cluster": 0.1507052630186081, "score_of_first_answer": 0.1507052630186081, "max_score_of_first_answer": 0.7379010319709778, "min_score_of_first_answer": 0.0027985312044620514, "avg_score": 0.1507052630186081, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.285482647912271, "std_of_first_answer": 0.285482647912271, "avg_entropy_of_first_answer": 1.7448312789201736, "lex_sim": 1.0}
{"question": "On T.V. who played Frank Cannon?", "answer": ["Robert Blake"], "scores": [[0.25585243105888367, 0.023117853328585625, 0.9966098666191101]], "normalized_score": [0.18064282834529877], "tokens": [["\u2581Robert", "\u2581Bla", "ke"]], "entropy": [[3.0142662525177, 3.1093130111694336, 0.03064170852303505]], "avg_entropy": [2.051406990736723], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bill Conrad", "score_of_biggest_cluster": 0.18064282834529877, "score_of_first_answer": 0.18064282834529877, "max_score_of_first_answer": 0.9966098666191101, "min_score_of_first_answer": 0.023117853328585625, "avg_score": 0.18064282834529877, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41507344222298004, "std_of_first_answer": 0.41507344222298004, "avg_entropy_of_first_answer": 2.051406990736723, "lex_sim": 1.0}
{"question": "What is Marge Simpson's maiden name ?", "answer": ["Applebottom"], "scores": [[0.00017789186676964164, 0.007154864724725485]], "normalized_score": [0.0011281812330707908], "tokens": [["\u2581Apple", "bottom"]], "entropy": [[3.481527090072632, 3.026820659637451]], "avg_entropy": [3.2541738748550415], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Augustus Jules Bouvier", "score_of_biggest_cluster": 0.0011281812330707908, "score_of_first_answer": 0.0011281812330707908, "max_score_of_first_answer": 0.007154864724725485, "min_score_of_first_answer": 0.00017789186676964164, "avg_score": 0.0011281812330707908, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0034884864289779216, "std_of_first_answer": 0.0034884864289779216, "avg_entropy_of_first_answer": 3.2541738748550415, "lex_sim": 1.0}
{"question": "Five-legged creatures have damaged which 1250 mile long wonder of the world?", "answer": ["The Great Wall of China"], "scores": [[0.42173832654953003, 0.5396082401275635, 0.9169890880584717, 0.8965944647789001, 0.9870327711105347]], "normalized_score": [0.7133172154426575], "tokens": [["\u2581The", "\u2581Great", "\u2581Wall", "\u2581of", "\u2581China"]], "entropy": [[3.4273624420166016, 2.830571174621582, 0.4295535385608673, 0.4468749463558197, 0.10853317379951477]], "avg_entropy": [1.448579055070877], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Great Barrier Reef", "score_of_biggest_cluster": 0.7133172154426575, "score_of_first_answer": 0.7133172154426575, "max_score_of_first_answer": 0.9870327711105347, "min_score_of_first_answer": 0.42173832654953003, "avg_score": 0.7133172154426575, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2269588614454409, "std_of_first_answer": 0.2269588614454409, "avg_entropy_of_first_answer": 1.448579055070877, "lex_sim": 1.0}
{"question": "Which sculptor's works include St Michael's Victory over the Devil at Coventry Cathedral and Jacob and the Angel?", "answer": ["George Malyon"], "scores": [[0.006659508217126131, 0.006192515138536692, 0.02559146098792553, 0.7827848792076111]], "normalized_score": [0.03014822117984295], "tokens": [["\u2581George", "\u2581Mal", "y", "on"]], "entropy": [[3.8123369216918945, 4.150121688842773, 2.4935710430145264, 1.2263485193252563]], "avg_entropy": [2.9205945432186127], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kathleen Epstein", "score_of_biggest_cluster": 0.03014822117984295, "score_of_first_answer": 0.03014822117984295, "max_score_of_first_answer": 0.7827848792076111, "min_score_of_first_answer": 0.006192515138536692, "avg_score": 0.03014822117984295, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33349879310492286, "std_of_first_answer": 0.33349879310492286, "avg_entropy_of_first_answer": 2.9205945432186127, "lex_sim": 1.0}
{"question": "Fireworks that burn red contain which element, the only one named after a place in UK?", "answer": ["Phosphorus"], "scores": [[0.18386134505271912, 0.984801709651947, 0.8008715510368347]], "normalized_score": [0.5253725647926331], "tokens": [["\u2581Ph", "osph", "orus"]], "entropy": [[4.406667232513428, 0.10912976413965225, 0.581934928894043]], "avg_entropy": [1.6992439751823742], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Strontium", "score_of_biggest_cluster": 0.5253725647926331, "score_of_first_answer": 0.5253725647926331, "max_score_of_first_answer": 0.984801709651947, "min_score_of_first_answer": 0.18386134505271912, "avg_score": 0.5253725647926331, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34254559553455155, "std_of_first_answer": 0.34254559553455155, "avg_entropy_of_first_answer": 1.6992439751823742, "lex_sim": 1.0}
{"question": "In what mountain range would you find the homeland of the Sherpa people?", "answer": ["Himalaya"], "scores": [[0.562259316444397, 0.9879768490791321, 0.28773778676986694]], "normalized_score": [0.5427003502845764], "tokens": [["\u2581H", "imal", "aya"]], "entropy": [[2.3149075508117676, 0.09770277887582779, 0.6955253481864929]], "avg_entropy": [1.0360452259580295], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Himilaya", "score_of_biggest_cluster": 0.5427003502845764, "score_of_first_answer": 0.5427003502845764, "max_score_of_first_answer": 0.9879768490791321, "min_score_of_first_answer": 0.28773778676986694, "avg_score": 0.5427003502845764, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2880841374458375, "std_of_first_answer": 0.2880841374458375, "avg_entropy_of_first_answer": 1.0360452259580295, "lex_sim": 1.0}
{"question": "Film musical interpretations of Romeo and Juliet and the memoir of Baroness von Trapp shared what lead character name?", "answer": ["Walt"], "scores": [[0.0017407980049028993, 0.26004794239997864]], "normalized_score": [0.021276531741023064], "tokens": [["\u2581W", "alt"]], "entropy": [[3.3672235012054443, 4.340591907501221]], "avg_entropy": [3.8539077043533325], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mar\u00eda", "score_of_biggest_cluster": 0.021276531741023064, "score_of_first_answer": 0.021276531741023064, "max_score_of_first_answer": 0.26004794239997864, "min_score_of_first_answer": 0.0017407980049028993, "avg_score": 0.021276531741023064, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12915357219753787, "std_of_first_answer": 0.12915357219753787, "avg_entropy_of_first_answer": 3.8539077043533325, "lex_sim": 1.0}
{"question": "Nouakchott is the capital of which Islamic African republic?", "answer": ["Mauritania"], "scores": [[0.8890945315361023, 0.9823493361473083, 0.983098566532135]], "normalized_score": [0.9504668712615967], "tokens": [["\u2581Maur", "it", "ania"]], "entropy": [[0.7051835656166077, 0.14328184723854065, 0.13021248579025269]], "avg_entropy": [0.32622596621513367], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Domain of Moor", "score_of_biggest_cluster": 0.9504668712615967, "score_of_first_answer": 0.9504668712615967, "max_score_of_first_answer": 0.983098566532135, "min_score_of_first_answer": 0.8890945315361023, "avg_score": 0.9504668712615967, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04413839160939539, "std_of_first_answer": 0.04413839160939539, "avg_entropy_of_first_answer": 0.32622596621513367, "lex_sim": 1.0}
{"question": "Which former leader of the Conservative Party was MP for Huntingdon?", "answer": ["Jonathan Aitken"], "scores": [[0.015767550095915794, 0.676656186580658, 0.9914446473121643, 0.9339134097099304]], "normalized_score": [0.3152657747268677], "tokens": [["\u2581Jonathan", "\u2581A", "it", "ken"]], "entropy": [[3.8130860328674316, 2.3116040229797363, 0.07876867800951004, 0.3009423613548279]], "avg_entropy": [1.6261002738028765], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John major", "score_of_biggest_cluster": 0.3152657747268677, "score_of_first_answer": 0.3152657747268677, "max_score_of_first_answer": 0.9914446473121643, "min_score_of_first_answer": 0.015767550095915794, "avg_score": 0.3152657747268677, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3873220804894878, "std_of_first_answer": 0.3873220804894878, "avg_entropy_of_first_answer": 1.6261002738028765, "lex_sim": 1.0}
{"question": "On which of the hills of ancient Rome were the main residences of the Caesars?", "answer": ["Palatine"], "scores": [[0.45918190479278564, 0.965013861656189, 0.9870926141738892]], "normalized_score": [0.759087860584259], "tokens": [["\u2581Pal", "at", "ine"]], "entropy": [[2.59378981590271, 0.21129152178764343, 0.1009766161441803]], "avg_entropy": [0.9686859846115112], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Palatine", "score_of_biggest_cluster": 0.759087860584259, "score_of_first_answer": 0.759087860584259, "max_score_of_first_answer": 0.9870926141738892, "min_score_of_first_answer": 0.45918190479278564, "avg_score": 0.759087860584259, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.24382214760769505, "std_of_first_answer": 0.24382214760769505, "avg_entropy_of_first_answer": 0.9686859846115112, "lex_sim": 1.0}
{"question": "\"Who won a gold medal at the 1976 Summer Olympics, married Kris Kardashian in 1991 and from 2007 starred in the E! reality series \"\"Keeping Up with the Kardashians\"\"?\"", "answer": ["Whoopi Goldberg"], "scores": [[0.0003171601565554738, 0.2804056704044342, 0.9588903784751892, 0.9634308815002441, 0.9954403638839722]], "normalized_score": [0.15224185585975647], "tokens": [["\u2581Who", "op", "i", "\u2581Gold", "berg"]], "entropy": [[1.4912036657333374, 2.5360710620880127, 0.2196016162633896, 0.3007778525352478, 0.04166753590106964]], "avg_entropy": [0.9178643465042114], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Caitlin Jenner", "score_of_biggest_cluster": 0.15224185585975647, "score_of_first_answer": 0.15224185585975647, "max_score_of_first_answer": 0.9954403638839722, "min_score_of_first_answer": 0.0003171601565554738, "avg_score": 0.15224185585975647, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4174058502133918, "std_of_first_answer": 0.4174058502133918, "avg_entropy_of_first_answer": 0.9178643465042114, "lex_sim": 1.0}
{"question": "\"In 2007, who became the first female to deliver football commentary on BBC's \"\"Match of the Day\"\"?\"", "answer": ["Suzanne Banier"], "scores": [[0.050607211887836456, 0.7166323661804199, 0.8821309804916382, 0.0063058845698833466, 0.0404159277677536]], "normalized_score": [0.09599930793046951], "tokens": [["\u2581Su", "z", "anne", "\u2581Ban", "ier"]], "entropy": [[3.2412986755371094, 0.9598333239555359, 0.48972028493881226, 3.130338668823242, 2.20039701461792]], "avg_entropy": [2.004317593574524], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "JACQUI OATLEY", "score_of_biggest_cluster": 0.09599930793046951, "score_of_first_answer": 0.09599930793046951, "max_score_of_first_answer": 0.8821309804916382, "min_score_of_first_answer": 0.0063058845698833466, "avg_score": 0.09599930793046951, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37963283582159274, "std_of_first_answer": 0.37963283582159274, "avg_entropy_of_first_answer": 2.004317593574524, "lex_sim": 1.0}
{"question": "Which US state was known as the 'State of Franklin' from 1784 until 1788?", "answer": ["Tennessee"], "scores": [[0.39064791798591614]], "normalized_score": [0.39064791798591614], "tokens": [["\u2581Tennessee"]], "entropy": [[2.4615979194641113]], "avg_entropy": [2.4615979194641113], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tennessee (river)", "score_of_biggest_cluster": 0.39064791798591614, "score_of_first_answer": 0.39064791798591614, "max_score_of_first_answer": 0.39064791798591614, "min_score_of_first_answer": 0.39064791798591614, "avg_score": 0.39064791798591614, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.4615979194641113, "lex_sim": 1.0}
{"question": "What was the Russian City of Nizhny Novgorod called between 1932 and 1990?", "answer": ["Gorky"], "scores": [[0.8365151286125183, 0.9962950348854065, 0.79470294713974]], "normalized_score": [0.8716768622398376], "tokens": [["\u2581G", "ork", "y"]], "entropy": [[1.0317375659942627, 0.03922336921095848, 0.7463037967681885]], "avg_entropy": [0.6057549106578032], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gorky", "score_of_biggest_cluster": 0.8716768622398376, "score_of_first_answer": 0.8716768622398376, "max_score_of_first_answer": 0.9962950348854065, "min_score_of_first_answer": 0.79470294713974, "avg_score": 0.8716768622398376, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08686979196842823, "std_of_first_answer": 0.08686979196842823, "avg_entropy_of_first_answer": 0.6057549106578032, "lex_sim": 1.0}
{"question": "\u2018The Darling Buds of May\u2019 is set in which English county?", "answer": ["Suffolk"], "scores": [[0.14827129244804382, 0.9993727803230286, 0.9931555390357971]], "normalized_score": [0.5279594659805298], "tokens": [["\u2581Su", "ff", "olk"]], "entropy": [[2.202376365661621, 0.008409883826971054, 0.05722038075327873]], "avg_entropy": [0.7560022100806236], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "K249DR", "score_of_biggest_cluster": 0.5279594659805298, "score_of_first_answer": 0.5279594659805298, "max_score_of_first_answer": 0.9993727803230286, "min_score_of_first_answer": 0.14827129244804382, "avg_score": 0.5279594659805298, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3997557291486323, "std_of_first_answer": 0.3997557291486323, "avg_entropy_of_first_answer": 0.7560022100806236, "lex_sim": 1.0}
{"question": "Produced until 2001, what was the name of the 128-bit game console produced by Sega that has developed quite a cult following?", "answer": ["Dreamcast"], "scores": [[0.200969398021698, 0.9653681516647339]], "normalized_score": [0.4404650628566742], "tokens": [["\u2581Dream", "cast"]], "entropy": [[2.095416784286499, 0.2628326416015625]], "avg_entropy": [1.1791247129440308], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "SEGA Dreamcast", "score_of_biggest_cluster": 0.4404650628566742, "score_of_first_answer": 0.4404650628566742, "max_score_of_first_answer": 0.9653681516647339, "min_score_of_first_answer": 0.200969398021698, "avg_score": 0.4404650628566742, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.38219937682151794, "std_of_first_answer": 0.38219937682151794, "avg_entropy_of_first_answer": 1.1791247129440308, "lex_sim": 1.0}
{"question": "In which British city is the Mitchell Library", "answer": ["The City of Glasgow"], "scores": [[0.00831291638314724, 0.08059504628181458, 0.8811655640602112, 0.8955264091491699]], "normalized_score": [0.15163499116897583], "tokens": [["\u2581The", "\u2581City", "\u2581of", "\u2581Glasgow"]], "entropy": [[0.9634823799133301, 4.1645426750183105, 0.6876189708709717, 0.739174485206604]], "avg_entropy": [1.638704627752304], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The West End of Glasgow, Glasgow, United Kingdom", "score_of_biggest_cluster": 0.15163499116897583, "score_of_first_answer": 0.15163499116897583, "max_score_of_first_answer": 0.8955264091491699, "min_score_of_first_answer": 0.00831291638314724, "avg_score": 0.15163499116897583, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4227496855851964, "std_of_first_answer": 0.4227496855851964, "avg_entropy_of_first_answer": 1.638704627752304, "lex_sim": 1.0}
{"question": "Which doctor - an eternal optimist \u2013 features in Voltaire\u2019s Candide?", "answer": ["Dr. Pangloss"], "scores": [[0.388215035200119, 0.5142783522605896, 0.9182100296020508, 0.9978688955307007, 0.9935831427574158]], "normalized_score": [0.711046576499939], "tokens": [["\u2581Dr", ".", "\u2581P", "ang", "loss"]], "entropy": [[2.4878599643707275, 1.0055668354034424, 0.6112599968910217, 0.025539085268974304, 0.0502982996404171]], "avg_entropy": [0.8361048363149166], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dr Pangloss", "score_of_biggest_cluster": 0.711046576499939, "score_of_first_answer": 0.711046576499939, "max_score_of_first_answer": 0.9978688955307007, "min_score_of_first_answer": 0.388215035200119, "avg_score": 0.711046576499939, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2587457233301338, "std_of_first_answer": 0.2587457233301338, "avg_entropy_of_first_answer": 0.8361048363149166, "lex_sim": 1.0}
{"question": "In 1919, which Dadaist created L.H.O.O.Q., a parody of the Mona Lisa, in which a moustache and goatee beard were added to a reproduction of the painting?", "answer": ["Ren\u00e9 Magritte"], "scores": [[0.007667146623134613, 0.8085058331489563, 0.9978334307670593, 0.9991723299026489]], "normalized_score": [0.2803843915462494], "tokens": [["\u2581Ren\u00e9", "\u2581Mag", "rit", "te"]], "entropy": [[1.8971871137619019, 1.3484017848968506, 0.020403120666742325, 0.01006557047367096]], "avg_entropy": [0.8190143974497914], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marcel du Champ", "score_of_biggest_cluster": 0.2803843915462494, "score_of_first_answer": 0.2803843915462494, "max_score_of_first_answer": 0.9991723299026489, "min_score_of_first_answer": 0.007667146623134613, "avg_score": 0.2803843915462494, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4090426968405161, "std_of_first_answer": 0.4090426968405161, "avg_entropy_of_first_answer": 0.8190143974497914, "lex_sim": 1.0}
{"question": "Luminol, a chemical that exhibits chemiluminescence, is often used by forensic examiners to try and detect trace amounts of what?", "answer": ["Genetically modified foods"], "scores": [[0.0002708030224312097, 0.13908612728118896, 0.5900483131408691, 0.5641289949417114, 0.08956528455018997, 0.40808480978012085]], "normalized_score": [0.08780425786972046], "tokens": [["\u2581Gen", "et", "ically", "\u2581modified", "\u2581food", "s"]], "entropy": [[1.58565354347229, 1.6733111143112183, 1.839428186416626, 2.1233177185058594, 3.3980612754821777, 1.3565634489059448]], "avg_entropy": [1.9960558811823528], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A blood", "score_of_biggest_cluster": 0.08780425786972046, "score_of_first_answer": 0.08780425786972046, "max_score_of_first_answer": 0.5900483131408691, "min_score_of_first_answer": 0.0002708030224312097, "avg_score": 0.08780425786972046, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23294318291007912, "std_of_first_answer": 0.23294318291007912, "avg_entropy_of_first_answer": 1.9960558811823528, "lex_sim": 1.0}
{"question": "The moons of which planet are named after Shakespearian characters?", "answer": ["Jupiter"], "scores": [[0.25868409872055054, 0.9781249761581421]], "normalized_score": [0.5030162930488586], "tokens": [["\u2581Jup", "iter"]], "entropy": [[4.140748023986816, 0.15561321377754211]], "avg_entropy": [2.1481806188821793], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Magnetosphere of Uranus", "score_of_biggest_cluster": 0.5030162930488586, "score_of_first_answer": 0.5030162930488586, "max_score_of_first_answer": 0.9781249761581421, "min_score_of_first_answer": 0.25868409872055054, "avg_score": 0.5030162930488586, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3597204387187958, "std_of_first_answer": 0.3597204387187958, "avg_entropy_of_first_answer": 2.1481806188821793, "lex_sim": 1.0}
{"question": "What is the opposite of a spring tide", "answer": ["Neap Tide"], "scores": [[0.19993774592876434, 0.9729324579238892, 0.08740009367465973, 0.9037028551101685]], "normalized_score": [0.35206982493400574], "tokens": [["\u2581Ne", "ap", "\u2581T", "ide"]], "entropy": [[3.1147806644439697, 0.15994784235954285, 0.9928675889968872, 0.37338030338287354]], "avg_entropy": [1.1602440997958183], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Compound Tides", "score_of_biggest_cluster": 0.35206982493400574, "score_of_first_answer": 0.35206982493400574, "max_score_of_first_answer": 0.9729324579238892, "min_score_of_first_answer": 0.08740009367465973, "avg_score": 0.35206982493400574, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40006104074989435, "std_of_first_answer": 0.40006104074989435, "avg_entropy_of_first_answer": 1.1602440997958183, "lex_sim": 1.0}
{"question": "What links do Bollywood, Hollywood and Lollywood have?", "answer": ["The first 3 letters"], "scores": [[0.038695815950632095, 0.02643892727792263, 0.022602573037147522, 0.5014038681983948, 0.20663239061832428]], "normalized_score": [0.07514331489801407], "tokens": [["\u2581The", "\u2581first", "\u2581", "3", "\u2581letters"]], "entropy": [[3.2236275672912598, 4.97080659866333, 2.275259256362915, 1.0547291040420532, 3.4486656188964844]], "avg_entropy": [2.9946176290512083], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Filmed", "score_of_biggest_cluster": 0.07514331489801407, "score_of_first_answer": 0.07514331489801407, "max_score_of_first_answer": 0.5014038681983948, "min_score_of_first_answer": 0.022602573037147522, "avg_score": 0.07514331489801407, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18447705911178083, "std_of_first_answer": 0.18447705911178083, "avg_entropy_of_first_answer": 2.9946176290512083, "lex_sim": 1.0}
{"question": "Which poet wrote \u2018The Lake Isle of Innisfree\u2019?", "answer": ["Thomas Parnell"], "scores": [[0.002994424430653453, 0.03736342117190361, 0.9036509394645691]], "normalized_score": [0.0465858057141304], "tokens": [["\u2581Thomas", "\u2581Par", "nell"]], "entropy": [[1.2279711961746216, 2.749490976333618, 0.5467936396598816]], "avg_entropy": [1.508085270722707], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "W. B. Yates", "score_of_biggest_cluster": 0.0465858057141304, "score_of_first_answer": 0.0465858057141304, "max_score_of_first_answer": 0.9036509394645691, "min_score_of_first_answer": 0.002994424430653453, "avg_score": 0.0465858057141304, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41670899123165117, "std_of_first_answer": 0.41670899123165117, "avg_entropy_of_first_answer": 1.508085270722707, "lex_sim": 1.0}
{"question": "In which European country is Lake Ree?", "answer": ["Sweden-n"], "scores": [[0.06788644194602966, 0.00020292759290896356, 0.0065534464083611965]], "normalized_score": [0.0044860546477139], "tokens": [["\u2581Sweden", "-", "n"]], "entropy": [[3.8016223907470703, 0.21585500240325928, 4.885869979858398]], "avg_entropy": [2.967782457669576], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Irlanda", "score_of_biggest_cluster": 0.0044860546477139, "score_of_first_answer": 0.0044860546477139, "max_score_of_first_answer": 0.06788644194602966, "min_score_of_first_answer": 0.00020292759290896356, "avg_score": 0.0044860546477139, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.030519799668117395, "std_of_first_answer": 0.030519799668117395, "avg_entropy_of_first_answer": 2.967782457669576, "lex_sim": 1.0}
{"question": "Which city was the subject of the 1949 song 'Dirty Old Town' by Ewan McColl?", "answer": ["Dublin"], "scores": [[0.5991674661636353]], "normalized_score": [0.5991674661636353], "tokens": [["\u2581Dublin"]], "entropy": [[2.4240524768829346]], "avg_entropy": [2.4240524768829346], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Salford", "score_of_biggest_cluster": 0.5991674661636353, "score_of_first_answer": 0.5991674661636353, "max_score_of_first_answer": 0.5991674661636353, "min_score_of_first_answer": 0.5991674661636353, "avg_score": 0.5991674661636353, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.4240524768829346, "lex_sim": 1.0}
{"question": "\"Which is the preposition in the sentence \"\"Mary put her dog in the basket\"\"?\"", "answer": ["Which"], "scores": [[0.0004824807692784816]], "normalized_score": [0.0004824807692784816], "tokens": [["\u2581Which"]], "entropy": [[3.4399337768554688]], "avg_entropy": [3.4399337768554688], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "In (disambiguation)", "score_of_biggest_cluster": 0.0004824807692784816, "score_of_first_answer": 0.0004824807692784816, "max_score_of_first_answer": 0.0004824807692784816, "min_score_of_first_answer": 0.0004824807692784816, "avg_score": 0.0004824807692784816, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.4399337768554688, "lex_sim": 1.0}
{"question": "What city has the busiest airport system?", "answer": ["City name"], "scores": [[0.0013597691431641579, 0.002709136577323079]], "normalized_score": [0.0019193224143236876], "tokens": [["\u2581City", "\u2581name"]], "entropy": [[3.534899950027466, 2.211024284362793]], "avg_entropy": [2.8729621171951294], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "London (UK)", "score_of_biggest_cluster": 0.0019193224143236876, "score_of_first_answer": 0.0019193224143236876, "max_score_of_first_answer": 0.002709136577323079, "min_score_of_first_answer": 0.0013597691431641579, "avg_score": 0.0019193224143236876, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0006746837170794606, "std_of_first_answer": 0.0006746837170794606, "avg_entropy_of_first_answer": 2.8729621171951294, "lex_sim": 1.0}
{"question": "In internet chat room language, what does 'BRB' mean?", "answer": ["Be Right Back"], "scores": [[0.6899267435073853, 0.29945021867752075, 0.9942384958267212]], "normalized_score": [0.5900281071662903], "tokens": [["\u2581Be", "\u2581Right", "\u2581Back"]], "entropy": [[1.6331822872161865, 0.7576655149459839, 0.048433199524879456]], "avg_entropy": [0.8130936672290167], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lmbo", "score_of_biggest_cluster": 0.5900281071662903, "score_of_first_answer": 0.5900281071662903, "max_score_of_first_answer": 0.9942384958267212, "min_score_of_first_answer": 0.29945021867752075, "avg_score": 0.5900281071662903, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28437227343881216, "std_of_first_answer": 0.28437227343881216, "avg_entropy_of_first_answer": 0.8130936672290167, "lex_sim": 1.0}
{"question": "Who did Joe Louis beat to take the world heavyweight boxing title in 1937?", "answer": ["James \"Buster\" Douglas Jnr"], "scores": [[0.3851023316383362, 0.02022491954267025, 0.18303170800209045, 0.8004213571548462, 0.9920618534088135, 0.3913549780845642, 9.253236930817366e-05, 0.8822789192199707]], "normalized_score": [0.1174328476190567], "tokens": [["\u2581James", "\u2581\"", "B", "uster", "\"", "\u2581Douglas", "\u2581J", "nr"]], "entropy": [[2.0978286266326904, 2.609382152557373, 1.450378179550171, 1.2092170715332031, 0.0748114287853241, 3.6438755989074707, 0.28314661979675293, 0.7361359596252441]], "avg_entropy": [1.5130969546735287], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "James Braddock", "score_of_biggest_cluster": 0.1174328476190567, "score_of_first_answer": 0.1174328476190567, "max_score_of_first_answer": 0.9920618534088135, "min_score_of_first_answer": 9.253236930817366e-05, "avg_score": 0.1174328476190567, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36557581157442187, "std_of_first_answer": 0.36557581157442187, "avg_entropy_of_first_answer": 1.5130969546735287, "lex_sim": 1.0}
{"question": "In which district of East London did Jack the Ripper operate?", "answer": ["Whitechapel"], "scores": [[0.6935015320777893, 0.9308292269706726, 0.9965971112251282, 0.9967086315155029]], "normalized_score": [0.8948522210121155], "tokens": [["\u2581White", "ch", "ap", "el"]], "entropy": [[1.9147651195526123, 0.3896467685699463, 0.028565630316734314, 0.027233125641942024]], "avg_entropy": [0.5900526610203087], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Whitechapel market", "score_of_biggest_cluster": 0.8948522210121155, "score_of_first_answer": 0.8948522210121155, "max_score_of_first_answer": 0.9967086315155029, "min_score_of_first_answer": 0.6935015320777893, "avg_score": 0.8948522210121155, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12469749220376433, "std_of_first_answer": 0.12469749220376433, "avg_entropy_of_first_answer": 0.5900526610203087, "lex_sim": 1.0}
{"question": "What Monopoly square is furthest from Go?", "answer": ["Oriental Avenue"], "scores": [[0.0023797941394150257, 0.4993462860584259]], "normalized_score": [0.03447232395410538], "tokens": [["\u2581Oriental", "\u2581Avenue"]], "entropy": [[4.37313175201416, 1.2864474058151245]], "avg_entropy": [2.8297895789146423], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Free Parking", "score_of_biggest_cluster": 0.03447232395410538, "score_of_first_answer": 0.03447232395410538, "max_score_of_first_answer": 0.4993462860584259, "min_score_of_first_answer": 0.0023797941394150257, "avg_score": 0.03447232395410538, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24848324595950544, "std_of_first_answer": 0.24848324595950544, "avg_entropy_of_first_answer": 2.8297895789146423, "lex_sim": 1.0}
{"question": "In which country will you find the 'Carisbrook Rugby Stadium'?", "answer": ["New Zealand"], "scores": [[0.8735663890838623, 0.8813566565513611]], "normalized_score": [0.8774528503417969], "tokens": [["\u2581New", "\u2581Zealand"]], "entropy": [[0.9361006617546082, 0.5818742513656616]], "avg_entropy": [0.7589874565601349], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "N Z", "score_of_biggest_cluster": 0.8774528503417969, "score_of_first_answer": 0.8774528503417969, "max_score_of_first_answer": 0.8813566565513611, "min_score_of_first_answer": 0.8735663890838623, "avg_score": 0.8774528503417969, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0038951337337493896, "std_of_first_answer": 0.0038951337337493896, "avg_entropy_of_first_answer": 0.7589874565601349, "lex_sim": 1.0}
{"question": "Which shipping forecast area is directly north of German Bight?", "answer": ["Dogger"], "scores": [[0.4926132261753082, 0.9849766492843628]], "normalized_score": [0.6965720057487488], "tokens": [["\u2581Dog", "ger"]], "entropy": [[3.0914018154144287, 0.10725505650043488]], "avg_entropy": [1.5993284359574318], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fisher", "score_of_biggest_cluster": 0.6965720057487488, "score_of_first_answer": 0.6965720057487488, "max_score_of_first_answer": 0.9849766492843628, "min_score_of_first_answer": 0.4926132261753082, "avg_score": 0.6965720057487488, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24618171155452728, "std_of_first_answer": 0.24618171155452728, "avg_entropy_of_first_answer": 1.5993284359574318, "lex_sim": 1.0}
{"question": "What is the symbol for the chemical element protactinium, atomic number 91?", "answer": ["Pa"], "scores": [[0.1411488652229309]], "normalized_score": [0.1411488801240921], "tokens": [["\u2581Pa"]], "entropy": [[4.127190589904785]], "avg_entropy": [4.127190589904785], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "(Pa.)", "score_of_biggest_cluster": 0.1411488801240921, "score_of_first_answer": 0.1411488801240921, "max_score_of_first_answer": 0.1411488652229309, "min_score_of_first_answer": 0.1411488652229309, "avg_score": 0.1411488801240921, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.127190589904785, "lex_sim": 1.0}
{"question": "Who was the first professional cricketer to captain England?", "answer": ["Walter Robins"], "scores": [[0.023801488801836967, 0.13357852399349213, 0.9509914517402649]], "normalized_score": [0.14460138976573944], "tokens": [["\u2581Walter", "\u2581Rob", "ins"]], "entropy": [[4.269294738769531, 4.28204345703125, 0.31035664677619934]], "avg_entropy": [2.9538982808589935], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sir leonard hutton", "score_of_biggest_cluster": 0.14460138976573944, "score_of_first_answer": 0.14460138976573944, "max_score_of_first_answer": 0.9509914517402649, "min_score_of_first_answer": 0.023801488801836967, "avg_score": 0.14460138976573944, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41364183638569685, "std_of_first_answer": 0.41364183638569685, "avg_entropy_of_first_answer": 2.9538982808589935, "lex_sim": 1.0}
{"question": "What Michelle Pfeiffer movie got a boost from the Coolio song Gangsta's Paradise?", "answer": ["Dangerous Minds"], "scores": [[0.4787830114364624, 0.9996015429496765, 0.994772732257843, 0.9331309199333191, 0.9985117316246033]], "normalized_score": [0.8499572277069092], "tokens": [["\u2581D", "anger", "ous", "\u2581M", "inds"]], "entropy": [[2.776095390319824, 0.005227903835475445, 0.050252582877874374, 0.5364412069320679, 0.015549658797681332]], "avg_entropy": [0.6767133485525847], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dangerous Minds (film)", "score_of_biggest_cluster": 0.8499572277069092, "score_of_first_answer": 0.8499572277069092, "max_score_of_first_answer": 0.9996015429496765, "min_score_of_first_answer": 0.4787830114364624, "avg_score": 0.8499572277069092, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.20264041964136476, "std_of_first_answer": 0.20264041964136476, "avg_entropy_of_first_answer": 0.6767133485525847, "lex_sim": 1.0}
{"question": "Romaine & Butterhead are types of what?", "answer": ["Vegetables"], "scores": [[0.02748892456293106, 0.9636596441268921, 0.5375338196754456]], "normalized_score": [0.24237941205501556], "tokens": [["\u2581Veg", "et", "ables"]], "entropy": [[3.738905668258667, 0.2746966481208801, 0.7202765941619873]], "avg_entropy": [1.5779596368471782], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "L. sativa", "score_of_biggest_cluster": 0.24237941205501556, "score_of_first_answer": 0.24237941205501556, "max_score_of_first_answer": 0.9636596441268921, "min_score_of_first_answer": 0.02748892456293106, "avg_score": 0.24237941205501556, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3827015996633039, "std_of_first_answer": 0.3827015996633039, "avg_entropy_of_first_answer": 1.5779596368471782, "lex_sim": 1.0}
{"question": "On July 21, 1969, who became the first man to set foot on the moon?", "answer": ["Neil Armstrong"], "scores": [[0.8440760374069214, 0.7152649164199829, 0.9926794767379761]], "normalized_score": [0.843113124370575], "tokens": [["\u2581Neil", "\u2581Arm", "strong"]], "entropy": [[0.9879925847053528, 1.4809095859527588, 0.060419224202632904]], "avg_entropy": [0.8431071316202482], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "That's one small step for a man one giant leap for mankind", "score_of_biggest_cluster": 0.843113124370575, "score_of_first_answer": 0.843113124370575, "max_score_of_first_answer": 0.9926794767379761, "min_score_of_first_answer": 0.7152649164199829, "avg_score": 0.843113124370575, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11335006021042152, "std_of_first_answer": 0.11335006021042152, "avg_entropy_of_first_answer": 0.8431071316202482, "lex_sim": 1.0}
{"question": "Who plays the bounty hunter Carson Wells in the 2007 film \u2018No Country For Old Men\u2019?", "answer": ["Javier Bardem"], "scores": [[0.7719663381576538, 0.9608151316642761, 0.9621853232383728, 0.9906460046768188]], "normalized_score": [0.9169673323631287], "tokens": [["\u2581J", "avier", "\u2581Bar", "dem"]], "entropy": [[1.4556704759597778, 0.3074067234992981, 0.3368494510650635, 0.0895976573228836]], "avg_entropy": [0.5473810769617558], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Woody Harelson", "score_of_biggest_cluster": 0.9169673323631287, "score_of_first_answer": 0.9169673323631287, "max_score_of_first_answer": 0.9906460046768188, "min_score_of_first_answer": 0.7719663381576538, "avg_score": 0.9169673323631287, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08709538393356439, "std_of_first_answer": 0.08709538393356439, "avg_entropy_of_first_answer": 0.5473810769617558, "lex_sim": 1.0}
{"question": "What is the name of the large sandbank, located 62 miles off the coast of north east England in the North Sea?", "answer": ["Dogger Bank (geology)"], "scores": [[0.4821562170982361, 0.9894104599952698, 0.5788630247116089, 0.0017887224676087499, 0.0010621864348649979, 0.12038760632276535, 0.8873287439346313]], "normalized_score": [0.0920616090297699], "tokens": [["\u2581Dog", "ger", "\u2581Bank", "\u2581(", "ge", "ology", ")"]], "entropy": [[2.9988350868225098, 0.09103240817785263, 1.0592701435089111, 0.21970349550247192, 5.599048614501953, 1.9981529712677002, 0.782952606678009]], "avg_entropy": [1.8212850466370583], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sailing Over the Dogger Bank", "score_of_biggest_cluster": 0.0920616090297699, "score_of_first_answer": 0.0920616090297699, "max_score_of_first_answer": 0.9894104599952698, "min_score_of_first_answer": 0.0010621864348649979, "avg_score": 0.0920616090297699, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3798140460753569, "std_of_first_answer": 0.3798140460753569, "avg_entropy_of_first_answer": 1.8212850466370583, "lex_sim": 1.0}
{"question": "Who provided the voice for Bob The Builder on his number one hit Can We Fix It?", "answer": ["Matthew Robertson"], "scores": [[0.007360360585153103, 0.08809476345777512, 0.5482826828956604]], "normalized_score": [0.0708409771323204], "tokens": [["\u2581Matthew", "\u2581Robert", "son"]], "entropy": [[4.356339931488037, 4.848407745361328, 2.5622096061706543]], "avg_entropy": [3.9223190943400064], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Neil morisey", "score_of_biggest_cluster": 0.0708409771323204, "score_of_first_answer": 0.0708409771323204, "max_score_of_first_answer": 0.5482826828956604, "min_score_of_first_answer": 0.007360360585153103, "avg_score": 0.0708409771323204, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23825474783133582, "std_of_first_answer": 0.23825474783133582, "avg_entropy_of_first_answer": 3.9223190943400064, "lex_sim": 1.0}
{"question": "\"In what city were travelers first asked to \"\"mind the gap\"\"?\"", "answer": ["\"On the station platform at the Kings Cross underground station"], "scores": [[0.0014814682072028518, 0.0159453172236681, 0.4769195318222046, 0.001042419346049428, 0.3436727225780487, 0.2762189507484436, 0.033311497420072556, 0.06834090501070023, 0.782932698726654, 0.12260579317808151, 0.9900250434875488, 0.80715411901474]], "normalized_score": [0.08725231140851974], "tokens": [["\u2581\"", "On", "\u2581the", "\u2581station", "\u2581platform", "\u2581at", "\u2581the", "\u2581Kings", "\u2581Cross", "\u2581under", "ground", "\u2581station"]], "entropy": [[1.0889606475830078, 3.4295291900634766, 2.649534225463867, 3.265603542327881, 2.955185890197754, 2.5064544677734375, 3.3577704429626465, 4.849393844604492, 0.8146155476570129, 2.7610604763031006, 0.08112867921590805, 1.1424570083618164]], "avg_entropy": [2.4084744968762], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "London (UK)", "score_of_biggest_cluster": 0.08725231140851974, "score_of_first_answer": 0.08725231140851974, "max_score_of_first_answer": 0.9900250434875488, "min_score_of_first_answer": 0.001042419346049428, "avg_score": 0.08725231140851974, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3424490140331318, "std_of_first_answer": 0.3424490140331318, "avg_entropy_of_first_answer": 2.4084744968762, "lex_sim": 1.0}
{"question": "Who wrote My Life and Loves in 1926, originally banned in the US?", "answer": ["Frank Harris"], "scores": [[0.19816847145557404, 0.9678978323936462]], "normalized_score": [0.4379575550556183], "tokens": [["\u2581Frank", "\u2581Harris"]], "entropy": [[4.571993827819824, 0.3096828758716583]], "avg_entropy": [2.4408383518457413], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Frank Harris", "score_of_biggest_cluster": 0.4379575550556183, "score_of_first_answer": 0.4379575550556183, "max_score_of_first_answer": 0.9678978323936462, "min_score_of_first_answer": 0.19816847145557404, "avg_score": 0.4379575550556183, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3848646804690361, "std_of_first_answer": 0.3848646804690361, "avg_entropy_of_first_answer": 2.4408383518457413, "lex_sim": 1.0}
{"question": "Whose portrait was on the back of the last English pound note issued?", "answer": ["Winston Churchill"], "scores": [[0.09911024570465088, 0.9854148626327515, 0.9990622401237488, 0.9083980321884155, 0.9919514656066895]], "normalized_score": [0.6149210333824158], "tokens": [["\u2581W", "inst", "on", "\u2581Church", "ill"]], "entropy": [[3.711292266845703, 0.12640057504177094, 0.010361751541495323, 0.5396113991737366, 0.06317031383514404]], "avg_entropy": [0.89016726128757], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Isaac Newton", "score_of_biggest_cluster": 0.6149210333824158, "score_of_first_answer": 0.6149210333824158, "max_score_of_first_answer": 0.9990622401237488, "min_score_of_first_answer": 0.09911024570465088, "avg_score": 0.6149210333824158, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35036974411431127, "std_of_first_answer": 0.35036974411431127, "avg_entropy_of_first_answer": 0.89016726128757, "lex_sim": 1.0}
{"question": "Which classical author wrote the Anabasis?", "answer": ["Herodotus of Halicarnassus The Anabasis"], "scores": [[0.005471289157867432, 0.8870154619216919, 0.9934535026550293, 0.0288524329662323, 0.933610737323761, 0.9592788815498352, 0.9856278896331787, 0.9814034700393677, 0.9568190574645996, 8.834178879624233e-05, 0.34801310300827026, 0.9738097786903381, 0.9961347579956055]], "normalized_score": [0.22333373129367828], "tokens": [["\u2581Her", "odot", "us", "\u2581of", "\u2581Hal", "ic", "arn", "ass", "us", "\u2581The", "\u2581An", "ab", "asis"]], "entropy": [[1.2821413278579712, 0.6708282232284546, 0.05943478271365166, 0.43198350071907043, 0.5428470373153687, 0.3234177231788635, 0.14206448197364807, 0.1180792897939682, 0.22157403826713562, 0.3322741389274597, 3.3598008155822754, 0.23685644567012787, 0.0407198965549469]], "avg_entropy": [0.5970785924448416], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Attic Muse", "score_of_biggest_cluster": 0.22333373129367828, "score_of_first_answer": 0.22333373129367828, "max_score_of_first_answer": 0.9961347579956055, "min_score_of_first_answer": 8.834178879624233e-05, "avg_score": 0.22333373129367828, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40937746437382866, "std_of_first_answer": 0.40937746437382866, "avg_entropy_of_first_answer": 0.5970785924448416, "lex_sim": 1.0}
{"question": "Who was British Prime Minister at the time of the first Aldermaston ban the bomb march?", "answer": ["Clement Attlee"], "scores": [[0.013530222699046135, 0.702590823173523, 0.9836257100105286]], "normalized_score": [0.2106747180223465], "tokens": [["\u2581Clement", "\u2581Att", "lee"]], "entropy": [[1.5885872840881348, 1.205020785331726, 0.12613505125045776]], "avg_entropy": [0.9732477068901062], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Harold Macmillan, Earl of Stockton", "score_of_biggest_cluster": 0.2106747180223465, "score_of_first_answer": 0.2106747180223465, "max_score_of_first_answer": 0.9836257100105286, "min_score_of_first_answer": 0.013530222699046135, "avg_score": 0.2106747180223465, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40754963886457884, "std_of_first_answer": 0.40754963886457884, "avg_entropy_of_first_answer": 0.9732477068901062, "lex_sim": 1.0}
{"question": "What is the nickname of the Australian national rugby union team?", "answer": ["Wallabies"], "scores": [[0.5994232892990112, 0.9218161702156067, 0.9901819825172424]], "normalized_score": [0.8178951740264893], "tokens": [["\u2581Wall", "ab", "ies"]], "entropy": [[1.3860689401626587, 0.32453957200050354, 0.06913928687572479]], "avg_entropy": [0.5932492663462957], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wallabies Rugby Team", "score_of_biggest_cluster": 0.8178951740264893, "score_of_first_answer": 0.8178951740264893, "max_score_of_first_answer": 0.9901819825172424, "min_score_of_first_answer": 0.5994232892990112, "avg_score": 0.8178951740264893, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17039281749435603, "std_of_first_answer": 0.17039281749435603, "avg_entropy_of_first_answer": 0.5932492663462957, "lex_sim": 1.0}
{"question": "The hall of fame for what sport is located in Saratoga Springs, NY, in which the announcement of new inductees is usually made on the first Saturday in May in Louisville, Ky?", "answer": ["Racing"], "scores": [[0.026677703484892845]], "normalized_score": [0.026677703484892845], "tokens": [["\u2581Racing"]], "entropy": [[2.7465944290161133]], "avg_entropy": [2.7465944290161133], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Scratch sheet", "score_of_biggest_cluster": 0.026677703484892845, "score_of_first_answer": 0.026677703484892845, "max_score_of_first_answer": 0.026677703484892845, "min_score_of_first_answer": 0.026677703484892845, "avg_score": 0.026677703484892845, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.7465944290161133, "lex_sim": 1.0}
{"question": "In yachting how many crew are there in the Flying Dutchman class", "answer": ["808"], "scores": [[0.17446178197860718, 0.08149594813585281, 0.009808256290853024, 0.002179434522986412]], "normalized_score": [0.023479720577597618], "tokens": [["\u2581", "8", "0", "8"]], "entropy": [[3.7476727962493896, 2.0674924850463867, 1.5842851400375366, 1.6911308765411377]], "avg_entropy": [2.2726453244686127], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2", "score_of_biggest_cluster": 0.023479720577597618, "score_of_first_answer": 0.023479720577597618, "max_score_of_first_answer": 0.17446178197860718, "min_score_of_first_answer": 0.002179434522986412, "avg_score": 0.023479720577597618, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06933752160790871, "std_of_first_answer": 0.06933752160790871, "avg_entropy_of_first_answer": 2.2726453244686127, "lex_sim": 1.0}
{"question": "Which English Rugby Union team play their home games at Adams Park?", "answer": ["Wycombe Wanderers"], "scores": [[0.2288023680448532, 0.9897617697715759, 0.9968283772468567, 0.9070003032684326, 0.9589148163795471, 0.9949231743812561]], "normalized_score": [0.7617247700691223], "tokens": [["\u2581Wy", "com", "be", "\u2581W", "ander", "ers"]], "entropy": [[2.979637622833252, 0.09308052808046341, 0.03176702558994293, 0.4672081470489502, 0.2098708152770996, 0.04187030717730522]], "avg_entropy": [0.6372390743345022], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wasps RFC", "score_of_biggest_cluster": 0.7617247700691223, "score_of_first_answer": 0.7617247700691223, "max_score_of_first_answer": 0.9968283772468567, "min_score_of_first_answer": 0.2288023680448532, "avg_score": 0.7617247700691223, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2777884262250883, "std_of_first_answer": 0.2777884262250883, "avg_entropy_of_first_answer": 0.6372390743345022, "lex_sim": 1.0}
{"question": "\"Which national anthem appears in The Beatles song \"\"All You Need Is Love\"\"?\"", "answer": ["All you need is love (song)"], "scores": [[0.00402174424380064, 0.185491144657135, 0.9070567488670349, 0.9478791356086731, 0.9007254242897034, 0.037263162434101105, 0.002764618955552578, 0.7951388359069824]], "normalized_score": [0.12144633382558823], "tokens": [["\u2581All", "\u2581you", "\u2581need", "\u2581is", "\u2581love", "\u2581(", "song", ")"]], "entropy": [[5.004544258117676, 2.56466007232666, 0.41340717673301697, 0.3478052020072937, 0.6476763486862183, 1.5785623788833618, 5.19820499420166, 1.145768404006958]], "avg_entropy": [2.1125786043703556], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Marseillese", "score_of_biggest_cluster": 0.12144633382558823, "score_of_first_answer": 0.12144633382558823, "max_score_of_first_answer": 0.9478791356086731, "min_score_of_first_answer": 0.002764618955552578, "avg_score": 0.12144633382558823, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4204500714447288, "std_of_first_answer": 0.4204500714447288, "avg_entropy_of_first_answer": 2.1125786043703556, "lex_sim": 1.0}
{"question": "Which woman was the singer with Big Brother and the Holding Company?", "answer": ["Janis Joplin"], "scores": [[0.8452503085136414, 0.9851587414741516, 0.8136053681373596, 0.998175859451294, 0.997562050819397]], "normalized_score": [0.9242947101593018], "tokens": [["\u2581Jan", "is", "\u2581J", "op", "lin"]], "entropy": [[1.0516445636749268, 0.10497243702411652, 1.1839146614074707, 0.021445687860250473, 0.024585401639342308]], "avg_entropy": [0.47731255032122133], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Janis Joplin discography", "score_of_biggest_cluster": 0.9242947101593018, "score_of_first_answer": 0.9242947101593018, "max_score_of_first_answer": 0.998175859451294, "min_score_of_first_answer": 0.8136053681373596, "avg_score": 0.9242947101593018, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08119640738686519, "std_of_first_answer": 0.08119640738686519, "avg_entropy_of_first_answer": 0.47731255032122133, "lex_sim": 1.0}
{"question": "The books 'The Edge of Reason' and 'Mad About The Boy' are sequels to which novel?", "answer": ["Difficulty"], "scores": [[0.001836568582803011, 0.019270872697234154, 0.8975346088409424, 0.4745582640171051]], "normalized_score": [0.062310654670000076], "tokens": [["\u2581D", "iff", "icult", "y"]], "entropy": [[2.8813908100128174, 4.061898231506348, 0.7296063899993896, 2.248105764389038]], "avg_entropy": [2.480250298976898], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Diary of Bridget Jones", "score_of_biggest_cluster": 0.062310654670000076, "score_of_first_answer": 0.062310654670000076, "max_score_of_first_answer": 0.8975346088409424, "min_score_of_first_answer": 0.001836568582803011, "avg_score": 0.062310654670000076, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36942417378409664, "std_of_first_answer": 0.36942417378409664, "avg_entropy_of_first_answer": 2.480250298976898, "lex_sim": 1.0}
{"question": "The usual graph shape for product failure rate, starting and ending high with a long flat bottom in between, is known as what sort of curve?", "answer": ["Weibull"], "scores": [[0.12339895963668823, 0.9671341180801392, 0.983080267906189]], "normalized_score": [0.4895484745502472], "tokens": [["\u2581We", "ib", "ull"]], "entropy": [[4.973292350769043, 0.1957615613937378, 0.1254253089427948]], "avg_entropy": [1.7648264070351918], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\ud83d\udec1", "score_of_biggest_cluster": 0.4895484745502472, "score_of_first_answer": 0.4895484745502472, "max_score_of_first_answer": 0.983080267906189, "min_score_of_first_answer": 0.12339895963668823, "avg_score": 0.4895484745502472, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40155188527011193, "std_of_first_answer": 0.40155188527011193, "avg_entropy_of_first_answer": 1.7648264070351918, "lex_sim": 1.0}
{"question": "What name is given to the study of insects ?", "answer": ["Insect taxonomy (scientific term)"], "scores": [[0.043073609471321106, 0.8433693647384644, 0.002047535963356495, 0.9510181546211243, 0.0019058650359511375, 0.0313582718372345, 0.3794771730899811, 0.9905158281326294, 0.017939817160367966, 0.720119059085846]], "normalized_score": [0.08535643666982651], "tokens": [["\u2581In", "sect", "\u2581tax", "onomy", "\u2581(", "s", "cient", "ific", "\u2581term", ")"]], "entropy": [[0.8602747321128845, 0.7940170764923096, 2.9609580039978027, 0.31025898456573486, 0.22163841128349304, 4.879986763000488, 1.9540650844573975, 0.07606663554906845, 2.2910079956054688, 1.5278265476226807]], "avg_entropy": [1.587610023468733], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ENTOMOLOGY", "score_of_biggest_cluster": 0.08535643666982651, "score_of_first_answer": 0.08535643666982651, "max_score_of_first_answer": 0.9905158281326294, "min_score_of_first_answer": 0.0019058650359511375, "avg_score": 0.08535643666982651, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40961875346996895, "std_of_first_answer": 0.40961875346996895, "avg_entropy_of_first_answer": 1.587610023468733, "lex_sim": 1.0}
{"question": "The administration of which American President was rocked by the 'Teapot Dome Scandal' of the 1920's?", "answer": ["Warren Harding"], "scores": [[0.34945744276046753, 0.175009623169899, 0.9964715242385864]], "normalized_score": [0.39352625608444214], "tokens": [["\u2581Warren", "\u2581Hard", "ing"]], "entropy": [[2.7291440963745117, 1.0576777458190918, 0.03594803065061569]], "avg_entropy": [1.2742566242814064], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "President Harding", "score_of_biggest_cluster": 0.39352625608444214, "score_of_first_answer": 0.39352625608444214, "max_score_of_first_answer": 0.9964715242385864, "min_score_of_first_answer": 0.175009623169899, "avg_score": 0.39352625608444214, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3533740414125676, "std_of_first_answer": 0.3533740414125676, "avg_entropy_of_first_answer": 1.2742566242814064, "lex_sim": 1.0}
{"question": "What is the meteorological name of the clouds commonly known as 'Mares Tails'?", "answer": ["Maracudas"], "scores": [[0.013632865622639656, 0.0019103583181276917, 0.05003514885902405, 0.8953981399536133]], "normalized_score": [0.032866109162569046], "tokens": [["\u2581Mar", "ac", "ud", "as"]], "entropy": [[3.1142539978027344, 1.6289341449737549, 3.8009588718414307, 0.7044215202331543]], "avg_entropy": [2.3121421337127686], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cirrus (building)", "score_of_biggest_cluster": 0.032866109162569046, "score_of_first_answer": 0.032866109162569046, "max_score_of_first_answer": 0.8953981399536133, "min_score_of_first_answer": 0.0019103583181276917, "avg_score": 0.032866109162569046, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3786693429404472, "std_of_first_answer": 0.3786693429404472, "avg_entropy_of_first_answer": 2.3121421337127686, "lex_sim": 1.0}
{"question": "My Colourful Life: From Red To Amber is the autobiography of which local sports personality who died in 2011?", "answer": ["Arthur Ward"], "scores": [[0.0066762566566467285, 0.0026607795152813196]], "normalized_score": [0.004214742686599493], "tokens": [["\u2581Arthur", "\u2581Ward"]], "entropy": [[5.2879638671875, 5.345100402832031]], "avg_entropy": [5.316532135009766], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ginger McCain", "score_of_biggest_cluster": 0.004214742686599493, "score_of_first_answer": 0.004214742686599493, "max_score_of_first_answer": 0.0066762566566467285, "min_score_of_first_answer": 0.0026607795152813196, "avg_score": 0.004214742686599493, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0020077385706827044, "std_of_first_answer": 0.0020077385706827044, "avg_entropy_of_first_answer": 5.316532135009766, "lex_sim": 1.0}
{"question": "For a point each, name the 9 countries surrounding Germany.", "answer": ["Austria, Switzerland, Liechenstein, France, Belgium, Netherlands, Luxembourg, Denmark, Norway"], "scores": [[0.2779284715652466, 0.9280277490615845, 0.03677159920334816, 0.9894964098930359, 0.1264767050743103, 0.008058473467826843, 0.9770503044128418, 0.9859657287597656, 0.4449654221534729, 0.984990656375885, 0.47935357689857483, 0.9971762895584106, 0.9884413480758667, 0.37514445185661316, 0.9900981783866882, 0.44724464416503906, 0.8740608096122742, 0.9817273616790771, 0.7198704481124878, 0.9937065243721008, 0.8194591403007507, 0.06591605395078659]], "normalized_score": [0.43385517597198486], "tokens": [["\u2581Austria", ",", "\u2581Switzerland", ",", "\u2581Lie", "chen", "stein", ",", "\u2581France", ",", "\u2581Belg", "ium", ",", "\u2581Netherlands", ",", "\u2581Luxemb", "ourg", ",", "\u2581Den", "mark", ",", "\u2581Norway"]], "entropy": [[3.566425323486328, 0.49118056893348694, 1.7783446311950684, 0.09305460751056671, 2.3356895446777344, 0.14685308933258057, 0.14092878997325897, 0.1167440339922905, 2.1338844299316406, 0.12405335903167725, 1.4658645391464233, 0.027488697320222855, 0.09741424769163132, 1.793030023574829, 0.08512997627258301, 1.426830530166626, 0.48800772428512573, 0.1346266269683838, 1.361358880996704, 0.05473064258694649, 0.6742287278175354, 2.3309619426727295]], "avg_entropy": [0.9484923153438352], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Denmark, Poland, Czech Republic, Austria, Switzerland, France, Luxembourg, Belgium, and the Netherlands", "score_of_biggest_cluster": 0.43385517597198486, "score_of_first_answer": 0.43385517597198486, "max_score_of_first_answer": 0.9971762895584106, "min_score_of_first_answer": 0.008058473467826843, "avg_score": 0.43385517597198486, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3640227559002825, "std_of_first_answer": 0.3640227559002825, "avg_entropy_of_first_answer": 0.9484923153438352, "lex_sim": 1.0}
{"question": "What swirly pattern is named after the administrative central town of Renfrewshire in Scotland?", "answer": ["The Wedgewood"], "scores": [[0.09516219049692154, 0.00879800133407116, 0.005274932365864515, 0.8589082956314087]], "normalized_score": [0.04413194581866264], "tokens": [["\u2581The", "\u2581W", "edge", "wood"]], "entropy": [[5.042163848876953, 4.919689178466797, 3.6085548400878906, 0.6388633847236633]], "avg_entropy": [3.552317813038826], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paisley, Renfrewshire", "score_of_biggest_cluster": 0.04413194581866264, "score_of_first_answer": 0.04413194581866264, "max_score_of_first_answer": 0.8589082956314087, "min_score_of_first_answer": 0.005274932365864515, "avg_score": 0.04413194581866264, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3579661688638921, "std_of_first_answer": 0.3579661688638921, "avg_entropy_of_first_answer": 3.552317813038826, "lex_sim": 1.0}
{"question": "A 'triskelion', a symbol consisting of three bent human legs is featured on the flag of which British crown dependency?", "answer": ["Isle of Man"], "scores": [[0.1813117414712906, 0.9664415121078491, 0.9455823302268982, 0.9402509927749634]], "normalized_score": [0.628255307674408], "tokens": [["\u2581Is", "le", "\u2581of", "\u2581Man"]], "entropy": [[2.636648178100586, 0.203210711479187, 0.26864299178123474, 0.3461894690990448]], "avg_entropy": [0.8636728376150131], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Man Island", "score_of_biggest_cluster": 0.628255307674408, "score_of_first_answer": 0.628255307674408, "max_score_of_first_answer": 0.9664415121078491, "min_score_of_first_answer": 0.1813117414712906, "avg_score": 0.628255307674408, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33332384297000983, "std_of_first_answer": 0.33332384297000983, "avg_entropy_of_first_answer": 0.8636728376150131, "lex_sim": 1.0}
{"question": "In Greek mythology what was the name of the goatlike man with pointed ears, horns and a tail?", "answer": ["Pan"], "scores": [[0.731139063835144]], "normalized_score": [0.731139063835144], "tokens": [["\u2581Pan"]], "entropy": [[1.546635389328003]], "avg_entropy": [1.546635389328003], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Saturos", "score_of_biggest_cluster": 0.731139063835144, "score_of_first_answer": 0.731139063835144, "max_score_of_first_answer": 0.731139063835144, "min_score_of_first_answer": 0.731139063835144, "avg_score": 0.731139063835144, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.546635389328003, "lex_sim": 1.0}
{"question": "What is the highest possible score in a game of ten pin bowling?", "answer": ["195 (140 in the US)"], "scores": [[0.29683130979537964, 0.03591899946331978, 0.02825138345360756, 0.09579232335090637, 0.01174870040267706, 0.12145780771970749, 0.02557893469929695, 0.10339418053627014, 0.07940082252025604, 0.2726709544658661, 0.2281237542629242, 0.8243703246116638]], "normalized_score": [0.09344646334648132], "tokens": [["\u2581", "1", "9", "5", "\u2581(", "1", "4", "0", "\u2581in", "\u2581the", "\u2581US", ")"]], "entropy": [[2.8986458778381348, 0.3808898329734802, 1.4883400201797485, 2.0159623622894287, 0.8952187299728394, 4.716235160827637, 2.565258741378784, 3.566896915435791, 3.8549208641052246, 3.912477731704712, 3.7122092247009277, 0.974216103553772]], "avg_entropy": [2.5817726304133735], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "300 points", "score_of_biggest_cluster": 0.09344646334648132, "score_of_first_answer": 0.09344646334648132, "max_score_of_first_answer": 0.8243703246116638, "min_score_of_first_answer": 0.01174870040267706, "avg_score": 0.09344646334648132, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2164300889331584, "std_of_first_answer": 0.2164300889331584, "avg_entropy_of_first_answer": 2.5817726304133735, "lex_sim": 1.0}
{"question": "What age came after the Bronze Age?", "answer": ["Iron Age"], "scores": [[0.6322181224822998, 0.533319890499115]], "normalized_score": [0.5806673169136047], "tokens": [["\u2581Iron", "\u2581Age"]], "entropy": [[1.4939346313476562, 1.1091583967208862]], "avg_entropy": [1.3015465140342712], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "First Iron Age", "score_of_biggest_cluster": 0.5806673169136047, "score_of_first_answer": 0.5806673169136047, "max_score_of_first_answer": 0.6322181224822998, "min_score_of_first_answer": 0.533319890499115, "avg_score": 0.5806673169136047, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04944911599159241, "std_of_first_answer": 0.04944911599159241, "avg_entropy_of_first_answer": 1.3015465140342712, "lex_sim": 1.0}
{"question": "In Ian Fleming\u2019s novel \u2018Moonraker\u2019, Sir Hugo Drax is suspected of cheating at which game in the Mayfair club \u2018Blades\u2019?", "answer": ["Croquet"], "scores": [[0.014874549582600594, 0.7550911903381348]], "normalized_score": [0.10597944259643555], "tokens": [["\u2581Cro", "quet"]], "entropy": [[3.7448651790618896, 1.2049446105957031]], "avg_entropy": [2.4749048948287964], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bridges", "score_of_biggest_cluster": 0.10597944259643555, "score_of_first_answer": 0.10597944259643555, "max_score_of_first_answer": 0.7550911903381348, "min_score_of_first_answer": 0.014874549582600594, "avg_score": 0.10597944259643555, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3701083203777671, "std_of_first_answer": 0.3701083203777671, "avg_entropy_of_first_answer": 2.4749048948287964, "lex_sim": 1.0}
{"question": "The 19th century invention Paige Compositor sought to replace the typewriter but ended up becoming a failure. Which American author lost nearly all his fortune for its development?", "answer": ["William Allen White"], "scores": [[0.017054954543709755, 0.03263344615697861, 0.7633506059646606]], "normalized_score": [0.07517600059509277], "tokens": [["\u2581William", "\u2581Allen", "\u2581White"]], "entropy": [[4.037659645080566, 4.395418643951416, 1.3517673015594482]], "avg_entropy": [3.26161519686381], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mark Twain", "score_of_biggest_cluster": 0.07517600059509277, "score_of_first_answer": 0.07517600059509277, "max_score_of_first_answer": 0.7633506059646606, "min_score_of_first_answer": 0.017054954543709755, "avg_score": 0.07517600059509277, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34819334609255553, "std_of_first_answer": 0.34819334609255553, "avg_entropy_of_first_answer": 3.26161519686381, "lex_sim": 1.0}
{"question": "Who is the current presenter of BBC Radio 4 programme Desert Island Discs?", "answer": ["Sue Lawley"], "scores": [[0.36645907163619995, 0.8625677227973938, 0.9919509887695312]], "normalized_score": [0.6793646812438965], "tokens": [["\u2581Sue", "\u2581Law", "ley"]], "entropy": [[1.8287770748138428, 0.6109937429428101, 0.048600468784570694]], "avg_entropy": [0.8294570955137411], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kirsty Young", "score_of_biggest_cluster": 0.6793646812438965, "score_of_first_answer": 0.6793646812438965, "max_score_of_first_answer": 0.9919509887695312, "min_score_of_first_answer": 0.36645907163619995, "avg_score": 0.6793646812438965, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2695889796770315, "std_of_first_answer": 0.2695889796770315, "avg_entropy_of_first_answer": 0.8294570955137411, "lex_sim": 1.0}
{"question": "In years and days, how old was Luigina Giavotti when she won a silver medal in gymnastics in 1928, and is still the youngest medal winner today?", "answer": ["1415-04-11"], "scores": [[0.37195008993148804, 0.7625139951705933, 0.15707607567310333, 0.010896475054323673, 0.0158404391258955, 0.012095144018530846, 0.022400222718715668, 0.05807196721434593, 0.471492201089859, 0.31825679540634155, 0.07452224940061569]], "normalized_score": [0.08337271958589554], "tokens": [["\u2581", "1", "4", "1", "5", "-", "0", "4", "-", "1", "1"]], "entropy": [[3.3421874046325684, 1.1036505699157715, 2.3021976947784424, 2.747109889984131, 2.6975810527801514, 2.535266637802124, 2.620114803314209, 2.7275495529174805, 2.1189956665039062, 1.6048222780227661, 2.311082601547241]], "avg_entropy": [2.3736871047453447], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "11 years and 302 days", "score_of_biggest_cluster": 0.08337271958589554, "score_of_first_answer": 0.08337271958589554, "max_score_of_first_answer": 0.7625139951705933, "min_score_of_first_answer": 0.010896475054323673, "avg_score": 0.08337271958589554, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23499014865324278, "std_of_first_answer": 0.23499014865324278, "avg_entropy_of_first_answer": 2.3736871047453447, "lex_sim": 1.0}
{"question": "Wanlockhead is the highest village in which European country?", "answer": ["Scotland"], "scores": [[0.847389280796051]], "normalized_score": [0.847389280796051], "tokens": [["\u2581Scotland"]], "entropy": [[1.0695221424102783]], "avg_entropy": [1.0695221424102783], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Scottish Nation", "score_of_biggest_cluster": 0.847389280796051, "score_of_first_answer": 0.847389280796051, "max_score_of_first_answer": 0.847389280796051, "min_score_of_first_answer": 0.847389280796051, "avg_score": 0.847389280796051, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.0695221424102783, "lex_sim": 1.0}
{"question": "Where did Jonathan Edwards set his 1995 triple jump world record?", "answer": ["Lille"], "scores": [[0.008203032426536083, 0.1882040798664093]], "normalized_score": [0.039291784167289734], "tokens": [["\u2581L", "ille"]], "entropy": [[5.149986743927002, 4.4336981773376465]], "avg_entropy": [4.791842460632324], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Swedish sin", "score_of_biggest_cluster": 0.039291784167289734, "score_of_first_answer": 0.039291784167289734, "max_score_of_first_answer": 0.1882040798664093, "min_score_of_first_answer": 0.008203032426536083, "avg_score": 0.039291784167289734, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09000052371993661, "std_of_first_answer": 0.09000052371993661, "avg_entropy_of_first_answer": 4.791842460632324, "lex_sim": 1.0}
{"question": "The eldest son of a French King was always given which title?", "answer": ["Duke"], "scores": [[0.10802547633647919]], "normalized_score": [0.10802546888589859], "tokens": [["\u2581Duke"]], "entropy": [[2.901834726333618]], "avg_entropy": [2.901834726333618], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dauphain", "score_of_biggest_cluster": 0.10802546888589859, "score_of_first_answer": 0.10802546888589859, "max_score_of_first_answer": 0.10802547633647919, "min_score_of_first_answer": 0.10802547633647919, "avg_score": 0.10802546888589859, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.901834726333618, "lex_sim": 1.0}
{"question": "Rugby League - which country are the current holders of the European Cup?", "answer": ["England"], "scores": [[0.21190673112869263]], "normalized_score": [0.21190673112869263], "tokens": [["\u2581England"]], "entropy": [[2.7814111709594727]], "avg_entropy": [2.7814111709594727], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Welsh nation", "score_of_biggest_cluster": 0.21190673112869263, "score_of_first_answer": 0.21190673112869263, "max_score_of_first_answer": 0.21190673112869263, "min_score_of_first_answer": 0.21190673112869263, "avg_score": 0.21190673112869263, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.7814111709594727, "lex_sim": 1.0}
{"question": "\"Mukhtar Al-Bakri, Sahim Alwan, Faysal Galab, Shafal Mosed, Yaseinn Taher and Yahya Goba were collectively known as the \"\"Lackawanna Six\"\" and by what other name?\"", "answer": ["Lackawanna Six"], "scores": [[0.13805940747261047, 0.9688416719436646, 0.9470351338386536, 0.9211713075637817, 0.286555677652359]], "normalized_score": [0.5068119764328003], "tokens": [["\u2581L", "ack", "aw", "anna", "\u2581Six"]], "entropy": [[3.6362204551696777, 0.3043222427368164, 0.4631603956222534, 0.4360165596008301, 3.256967067718506]], "avg_entropy": [1.6193373441696166], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lakawanna Six", "score_of_biggest_cluster": 0.5068119764328003, "score_of_first_answer": 0.5068119764328003, "max_score_of_first_answer": 0.9688416719436646, "min_score_of_first_answer": 0.13805940747261047, "avg_score": 0.5068119764328003, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3626490039232274, "std_of_first_answer": 0.3626490039232274, "avg_entropy_of_first_answer": 1.6193373441696166, "lex_sim": 1.0}
{"question": "Aboard which ship did Japan sign the surrender at the end of World War II?", "answer": ["U.S."], "scores": [[0.04205680638551712, 0.9074770212173462, 0.9806820154190063, 0.9946016073226929, 0.004413275048136711, 0.07215988636016846, 0.9886055588722229, 0.12500537931919098, 0.9947602152824402]], "normalized_score": [0.2246510535478592], "tokens": [["\u2581U", ".", "S", ".", "\u2581S", "urr", "ender", "\u2581Sh", "ip"]], "entropy": [[2.2350893020629883, 0.6676433086395264, 0.11274686455726624, 0.05702793225646019, 0.2994432747364044, 0.6160977482795715, 0.10019954293966293, 3.012892246246338, 0.04283925145864487]], "avg_entropy": [0.7937754967974292], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "U.S.S. Missouri", "score_of_biggest_cluster": 0.2246510535478592, "score_of_first_answer": 0.2246510535478592, "max_score_of_first_answer": 0.9947602152824402, "min_score_of_first_answer": 0.004413275048136711, "avg_score": 0.2246510535478592, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45496284069537263, "std_of_first_answer": 0.45496284069537263, "avg_entropy_of_first_answer": 0.7937754967974292, "lex_sim": 1.0}
{"question": "What is the only sequel to win an Academy Award for Best Picture?", "answer": ["Godfather II"], "scores": [[0.008571996353566647, 0.9664967656135559, 0.5358874797821045]], "normalized_score": [0.16435591876506805], "tokens": [["\u2581God", "father", "\u2581II"]], "entropy": [[3.3199260234832764, 0.2352447807788849, 1.6978570222854614]], "avg_entropy": [1.7510092755158742], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Godfather Part 2", "score_of_biggest_cluster": 0.16435591876506805, "score_of_first_answer": 0.16435591876506805, "max_score_of_first_answer": 0.9664967656135559, "min_score_of_first_answer": 0.008571996353566647, "avg_score": 0.16435591876506805, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3917348649376612, "std_of_first_answer": 0.3917348649376612, "avg_entropy_of_first_answer": 1.7510092755158742, "lex_sim": 1.0}
{"question": "Bjrn Ulvaeus and Benny Andersson wrote most of the songs for which pop group?", "answer": ["ABBA"], "scores": [[0.6182291507720947, 0.9976179003715515]], "normalized_score": [0.7853384613990784], "tokens": [["\u2581AB", "BA"]], "entropy": [[1.413395881652832, 0.028287604451179504]], "avg_entropy": [0.7208417430520058], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Infina AB", "score_of_biggest_cluster": 0.7853384613990784, "score_of_first_answer": 0.7853384613990784, "max_score_of_first_answer": 0.9976179003715515, "min_score_of_first_answer": 0.6182291507720947, "avg_score": 0.7853384613990784, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1896943747997284, "std_of_first_answer": 0.1896943747997284, "avg_entropy_of_first_answer": 0.7208417430520058, "lex_sim": 1.0}
{"question": "Which type of hat takes its name from an 1894 novel by George Du Maurier where the title character has the surname O'Ferrall ?", "answer": ["Trilby 13 Questions of the Week - 21st April... SciTeam have launched a new Quiz, and for the next few weeks you will be able to test how well you know science by answering"], "scores": [[0.435867041349411, 0.9959564805030823, 0.9932542443275452, 0.0002881806285586208, 0.02709881030023098, 0.012255470268428326, 0.00020047137513756752, 0.9338906407356262, 0.005780568812042475, 0.151020348072052, 0.09025558084249496, 0.03869885951280594, 0.21870511770248413, 0.29213419556617737, 0.05267910286784172, 0.17297551035881042, 0.06475962698459625, 0.0007777544087730348, 7.644220545444114e-07, 8.13236620160751e-05, 0.00021233434381429106, 0.0006144987419247627, 0.512557864189148, 0.6663894057273865, 0.0045425742864608765, 0.9638090133666992, 0.03866363316774368, 0.052250202745199203, 0.010946473106741905, 0.4495495855808258, 0.28582826256752014, 0.141596719622612, 0.7959491610527039, 0.043163035064935684, 0.24642512202262878, 0.5394765734672546, 0.5563154220581055, 0.9830379486083984, 0.050522252917289734, 0.00959194265305996, 0.24249662458896637, 0.9110093712806702, 0.8384467959403992, 0.013167059049010277, 0.06273455172777176, 0.3230842351913452]], "normalized_score": [0.047922320663928986], "tokens": [["\u2581Tr", "il", "by", "\u2581", "1", "3", "\u2581Question", "s", "\u2581of", "\u2581the", "\u2581Week", "\u2581-", "\u2581", "2", "1", "st", "\u2581April", "...", "\u2581Sci", "Team", "\u2581have", "\u2581launched", "\u2581a", "\u2581new", "\u2581Qu", "iz", ",", "\u2581and", "\u2581for", "\u2581the", "\u2581next", "\u2581few", "\u2581weeks", "\u2581you", "\u2581will", "\u2581be", "\u2581able", "\u2581to", "\u2581test", "\u2581how", "\u2581well", "\u2581you", "\u2581know", "\u2581science", "\u2581by", "\u2581answering"]], "entropy": [[3.1768085956573486, 0.0333513543009758, 0.04911557212471962, 0.7812294363975525, 3.31977915763855, 2.0427446365356445, 3.2892613410949707, 0.5856043100357056, 3.6120800971984863, 5.1834025382995605, 4.128927707672119, 3.3838067054748535, 5.216288089752197, 1.9134618043899536, 2.3772690296173096, 3.5997307300567627, 3.145756244659424, 1.1933722496032715, 2.3880250453948975, 3.09802508354187, 4.717624664306641, 5.131752014160156, 2.1338841915130615, 2.3341403007507324, 4.782904624938965, 0.2390134334564209, 5.17888069152832, 4.458348274230957, 3.8021128177642822, 2.6533501148223877, 1.93174409866333, 2.316649913787842, 0.9550839066505432, 2.631682872772217, 1.862669587135315, 1.8161754608154297, 2.4212446212768555, 0.1342751383781433, 3.75600004196167, 0.8591419458389282, 1.6850801706314087, 0.5423571467399597, 0.99521803855896, 2.6963257789611816, 2.8207900524139404, 2.5991463661193848]], "avg_entropy": [2.5646442608178956], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Trilby", "score_of_biggest_cluster": 0.047922320663928986, "score_of_first_answer": 0.047922320663928986, "max_score_of_first_answer": 0.9959564805030823, "min_score_of_first_answer": 7.644220545444114e-07, "avg_score": 0.047922320663928986, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34101298622345055, "std_of_first_answer": 0.34101298622345055, "avg_entropy_of_first_answer": 2.5646442608178956, "lex_sim": 1.0}
{"question": "What city had the largest library in the ancient world?", "answer": ["\u05d0\u05dc\u05e3 \u05e9\u05e0\u05d9\u05dd \u05d5\u05d0\u05e8\u05d1\u05e2\u05d4 \u05e9\u05e0\u05d9\u05dd"], "scores": [[0.0015968851512297988, 4.8162903112825006e-05, 0.07348740845918655, 0.0037778476253151894, 0.2460596114397049, 0.04804641753435135, 0.16439485549926758, 0.8105765581130981, 0.7394181489944458, 0.5568551421165466, 0.17746083438396454, 0.16343794763088226, 0.8144125938415527, 0.9936732053756714, 0.9960129261016846, 0.24731862545013428, 0.8873559832572937, 0.5378876328468323, 0.5421203970909119, 0.9827281832695007, 0.9850651621818542]], "normalized_score": [0.16027721762657166], "tokens": [["\u2581", "\u05d0", "\u05dc", "\u05e3", "\u2581", "\u05e9", "\u05e0", "\u05d9", "\u05dd", "\u2581", "\u05d5", "\u05d0", "\u05e8", "\u05d1", "\u05e2", "\u05d4", "\u2581", "\u05e9", "\u05e0", "\u05d9", "\u05dd"]], "entropy": [[1.6921027898788452, 3.4671542644500732, 4.171825408935547, 3.408581256866455, 4.112560749053955, 3.359984874725342, 3.00154447555542, 0.8360304832458496, 1.260662317276001, 2.1114115715026855, 2.719480514526367, 1.7595001459121704, 0.8213499784469604, 0.055423177778720856, 0.03756778687238693, 1.5162822008132935, 0.7052789330482483, 1.5789778232574463, 1.4701805114746094, 0.1108265370130539, 0.11150921881198883]], "avg_entropy": [1.824201667592639], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u1f08\u03bb\u03b5\u03be\u03ac\u03bd\u03b4\u03c1\u03b5\u03b9\u03b1", "score_of_biggest_cluster": 0.16027721762657166, "score_of_first_answer": 0.16027721762657166, "max_score_of_first_answer": 0.9960129261016846, "min_score_of_first_answer": 4.8162903112825006e-05, "avg_score": 0.16027721762657166, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3743519974989304, "std_of_first_answer": 0.3743519974989304, "avg_entropy_of_first_answer": 1.824201667592639, "lex_sim": 0.0}
{"question": "Which patriotic Irving Berlin song includes the line  From the mountains to the prairie to the oceans white with foam ?", "answer": ["Over There (song)"], "scores": [[0.012462934479117393, 0.868446946144104, 0.009866345673799515, 0.012752232141792774, 0.9687135815620422]], "normalized_score": [0.06668993830680847], "tokens": [["\u2581Over", "\u2581There", "\u2581(", "song", ")"]], "entropy": [[3.9357528686523438, 0.5556912422180176, 0.3758743405342102, 3.9447953701019287, 0.25466933846473694]], "avg_entropy": [1.8133566319942473], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "God Bless America (Celine Dion song)", "score_of_biggest_cluster": 0.06668993830680847, "score_of_first_answer": 0.06668993830680847, "max_score_of_first_answer": 0.9687135815620422, "min_score_of_first_answer": 0.009866345673799515, "avg_score": 0.06668993830680847, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44541291832138197, "std_of_first_answer": 0.44541291832138197, "avg_entropy_of_first_answer": 1.8133566319942473, "lex_sim": 1.0}
{"question": "Psychologists says men who wear what during sex are insecure?", "answer": ["Briefs"], "scores": [[0.00363399856723845, 0.9877910614013672, 0.8819847106933594]], "normalized_score": [0.14683747291564941], "tokens": [["\u2581Br", "ief", "s"]], "entropy": [[5.526797294616699, 0.10798928141593933, 0.596961259841919]], "avg_entropy": [2.0772492786248526], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tube sock", "score_of_biggest_cluster": 0.14683747291564941, "score_of_first_answer": 0.14683747291564941, "max_score_of_first_answer": 0.9877910614013672, "min_score_of_first_answer": 0.00363399856723845, "avg_score": 0.14683747291564941, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4411172786445433, "std_of_first_answer": 0.4411172786445433, "avg_entropy_of_first_answer": 2.0772492786248526, "lex_sim": 1.0}
{"question": "In which month of the year does pheasant shooting legally begin?", "answer": ["November"], "scores": [[0.21261194348335266]], "normalized_score": [0.21261192858219147], "tokens": [["\u2581November"]], "entropy": [[2.5276923179626465]], "avg_entropy": [2.5276923179626465], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ocotber", "score_of_biggest_cluster": 0.21261192858219147, "score_of_first_answer": 0.21261192858219147, "max_score_of_first_answer": 0.21261194348335266, "min_score_of_first_answer": 0.21261194348335266, "avg_score": 0.21261192858219147, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.5276923179626465, "lex_sim": 1.0}
{"question": "Who is the author of the 'Harry Potter' books?", "answer": ["J. K. Rowling"], "scores": [[0.8374680280685425, 0.5870405435562134, 0.4724058508872986, 0.9639498591423035, 0.9687883853912354, 0.9864603281021118]], "normalized_score": [0.7733672261238098], "tokens": [["\u2581J", ".", "\u2581K", ".", "\u2581Row", "ling"]], "entropy": [[1.1007461547851562, 1.1572589874267578, 0.933697521686554, 0.20323771238327026, 0.26198065280914307, 0.09676767885684967]], "avg_entropy": [0.6256147846579552], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "J.K.Rowlings", "score_of_biggest_cluster": 0.7733672261238098, "score_of_first_answer": 0.7733672261238098, "max_score_of_first_answer": 0.9864603281021118, "min_score_of_first_answer": 0.4724058508872986, "avg_score": 0.7733672261238098, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20172868232080235, "std_of_first_answer": 0.20172868232080235, "avg_entropy_of_first_answer": 0.6256147846579552, "lex_sim": 1.0}
{"question": "River Phoenix died during the making of which movie?", "answer": ["The Rainbow"], "scores": [[0.13504935801029205, 0.05886157229542732, 0.21786169707775116]], "normalized_score": [0.1200886070728302], "tokens": [["\u2581The", "\u2581Rain", "bow"]], "entropy": [[4.695188522338867, 3.0965147018432617, 1.8301892280578613]], "avg_entropy": [3.2072974840799966], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dark Blood (film)", "score_of_biggest_cluster": 0.1200886070728302, "score_of_first_answer": 0.1200886070728302, "max_score_of_first_answer": 0.21786169707775116, "min_score_of_first_answer": 0.05886157229542732, "avg_score": 0.1200886070728302, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0649303061193329, "std_of_first_answer": 0.0649303061193329, "avg_entropy_of_first_answer": 3.2072974840799966, "lex_sim": 1.0}
{"question": "What Family Guy character moved from Quahog, Rhode Island to Stoolbend, Virginia to spin off his own show?", "answer": ["Stewie Griffin"], "scores": [[0.03441520407795906, 0.9787559509277344, 0.6737145185470581, 0.987169623374939]], "normalized_score": [0.3868772089481354], "tokens": [["\u2581Ste", "wie", "\u2581Griff", "in"]], "entropy": [[2.839104175567627, 0.15282855927944183, 0.966059684753418, 0.10097866505384445]], "avg_entropy": [1.0147427711635828], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cleveland Orenthal Brown", "score_of_biggest_cluster": 0.3868772089481354, "score_of_first_answer": 0.3868772089481354, "max_score_of_first_answer": 0.987169623374939, "min_score_of_first_answer": 0.03441520407795906, "avg_score": 0.3868772089481354, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38726599476546303, "std_of_first_answer": 0.38726599476546303, "avg_entropy_of_first_answer": 1.0147427711635828, "lex_sim": 1.0}
{"question": "What is the surname of singer Florence of 'Florence and the Machine' ?", "answer": ["Machin"], "scores": [[0.016217172145843506, 0.869625449180603]], "normalized_score": [0.11875548213720322], "tokens": [["\u2581Mach", "in"]], "entropy": [[0.9643963575363159, 0.7077574133872986]], "avg_entropy": [0.8360768854618073], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Welsh Regiment", "score_of_biggest_cluster": 0.11875548213720322, "score_of_first_answer": 0.11875548213720322, "max_score_of_first_answer": 0.869625449180603, "min_score_of_first_answer": 0.016217172145843506, "avg_score": 0.11875548213720322, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42670413851737976, "std_of_first_answer": 0.42670413851737976, "avg_entropy_of_first_answer": 0.8360768854618073, "lex_sim": 1.0}
{"question": "What the note is at the 12th fret of the 1st and 6th strings of a standard-tuned 6-string guitar?", "answer": ["A"], "scores": [[0.21437934041023254]], "normalized_score": [0.21437934041023254], "tokens": [["\u2581A"]], "entropy": [[3.2629387378692627]], "avg_entropy": [3.2629387378692627], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u24a0", "score_of_biggest_cluster": 0.21437934041023254, "score_of_first_answer": 0.21437934041023254, "max_score_of_first_answer": 0.21437934041023254, "min_score_of_first_answer": 0.21437934041023254, "avg_score": 0.21437934041023254, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.2629387378692627, "lex_sim": 1.0}
{"question": "What is, or was, a barouche?", "answer": ["A type of carriage for four passengers: A four-wheeled, horse-drawn, passenger vehicle, used largely in the 18th & early 19th centuries."], "scores": [[0.5089640021324158, 0.10484874993562698, 0.9958011507987976, 0.4035423994064331, 0.01477111503481865, 0.2622407078742981, 0.16748924553394318, 0.0009739777306094766, 0.012845311313867569, 0.07618457078933716, 0.8882885575294495, 0.9409381747245789, 0.9171982407569885, 0.978681743144989, 0.9976779818534851, 0.13321107625961304, 0.1990581452846527, 0.9825647473335266, 0.9843806028366089, 0.9998449087142944, 0.053898170590400696, 0.04007614776492119, 0.47092586755752563, 0.1242743656039238, 0.028339030221104622, 0.0015918059507384896, 0.5295453667640686, 0.6510648131370544, 0.6652041077613831, 0.997557520866394, 0.5129009485244751, 0.9411557912826538, 0.008416066877543926, 0.07086554169654846, 0.9798516631126404, 0.996487021446228, 0.998941957950592, 0.9968761205673218, 0.7791643738746643, 0.4291786849498749, 0.018289724364876747, 0.08422138541936874, 0.11282306909561157, 0.9954288005828857, 0.9778629541397095, 0.5801058411598206, 0.6517063975334167, 0.007855406031012535, 0.7696194052696228]], "normalized_score": [0.22313573956489563], "tokens": [["\u2581A", "\u2581type", "\u2581of", "\u2581carriage", "\u2581for", "\u2581four", "\u2581passengers", ":", "\u2581A", "\u2581four", "-", "w", "he", "e", "led", ",", "\u2581horse", "-", "dra", "wn", ",", "\u2581passenger", "\u2581vehicle", ",", "\u2581used", "\u2581largely", "\u2581in", "\u2581the", "\u2581", "1", "8", "th", "\u2581&", "\u2581early", "\u2581", "1", "9", "th", "\u2581centuries", ".", "\u2581The", "\u2581name", "\u2581der", "ives", "\u2581from", "\u2581the", "\u2581French", "\u2581car", "ros"]], "entropy": [[2.9364449977874756, 3.856034994125366, 0.04279157146811485, 2.627915859222412, 1.5824387073516846, 2.489351272583008, 2.112717390060425, 1.7964234352111816, 3.599870204925537, 4.407761096954346, 0.5979064702987671, 0.3334515392780304, 0.2903255224227905, 0.11188512295484543, 0.022616777569055557, 3.079192876815796, 3.2433040142059326, 0.1210898831486702, 0.10736443102359772, 0.0020250079687684774, 1.681204080581665, 3.62579345703125, 1.7011457681655884, 2.7787303924560547, 4.121309280395508, 2.9558863639831543, 1.6624290943145752, 2.012420177459717, 1.8403512239456177, 0.019650261849164963, 1.0360474586486816, 0.3069949746131897, 1.0378687381744385, 0.4752005934715271, 0.14757892489433289, 0.024393487721681595, 0.0103535708039999, 0.03210526332259178, 0.8998871445655823, 1.8392938375473022, 1.2498753070831299, 4.591409206390381, 2.8760976791381836, 0.03924139216542244, 0.1905134618282318, 1.993844985961914, 2.228287935256958, 2.9977712631225586, 1.0229167938232422]], "avg_entropy": [1.6073370060018664], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A (FOUR WHEELED) CARRIAGE", "score_of_biggest_cluster": 0.22313573956489563, "score_of_first_answer": 0.22313573956489563, "max_score_of_first_answer": 0.9998449087142944, "min_score_of_first_answer": 0.0009739777306094766, "avg_score": 0.22313573956489563, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39855328814734753, "std_of_first_answer": 0.39855328814734753, "avg_entropy_of_first_answer": 1.6073370060018664, "lex_sim": 1.0}
{"question": "What is the largest natural lake in Wales", "answer": ["Llanishen \u3142"], "scores": [[0.4901866912841797, 0.05533455312252045, 0.03742746636271477, 0.9980973601341248, 0.0009389280457980931, 4.3592144720605575e-06, 0.07452168315649033, 0.046743396669626236]], "normalized_score": [0.018619611859321594], "tokens": [["\u2581L", "lan", "ish", "en", "\u2581", "<0xE3>", "<0x85>", "<0x82>"]], "entropy": [[1.853266954421997, 1.822094202041626, 3.5624163150787354, 0.020555056631565094, 2.0051686763763428, 2.2960264682769775, 2.676712989807129, 3.002561330795288]], "avg_entropy": [2.1548502491787076], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lake Bala", "score_of_biggest_cluster": 0.018619611859321594, "score_of_first_answer": 0.018619611859321594, "max_score_of_first_answer": 0.9980973601341248, "min_score_of_first_answer": 4.3592144720605575e-06, "avg_score": 0.018619611859321594, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33280041101188984, "std_of_first_answer": 0.33280041101188984, "avg_entropy_of_first_answer": 2.1548502491787076, "lex_sim": 1.0}
{"question": "\"Which game has been played in England since Tudor Times, with the earliest reference being in 1744 in \"\"A Little Pretty Pocketbook\"\" where it is called Baseball?\"", "answer": ["Rounders"], "scores": [[0.13593930006027222, 0.9835249185562134]], "normalized_score": [0.36564967036247253], "tokens": [["\u2581Round", "ers"]], "entropy": [[3.237640380859375, 0.1078481376171112]], "avg_entropy": [1.672744259238243], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Irish rounders", "score_of_biggest_cluster": 0.36564967036247253, "score_of_first_answer": 0.36564967036247253, "max_score_of_first_answer": 0.9835249185562134, "min_score_of_first_answer": 0.13593930006027222, "avg_score": 0.36564967036247253, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4237928092479706, "std_of_first_answer": 0.4237928092479706, "avg_entropy_of_first_answer": 1.672744259238243, "lex_sim": 1.0}
{"question": "What colour are the berries of the mistletoe plant?", "answer": ["White"], "scores": [[0.19553744792938232]], "normalized_score": [0.19553746283054352], "tokens": [["\u2581White"]], "entropy": [[3.004168748855591]], "avg_entropy": [3.004168748855591], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "White (Colour)", "score_of_biggest_cluster": 0.19553746283054352, "score_of_first_answer": 0.19553746283054352, "max_score_of_first_answer": 0.19553744792938232, "min_score_of_first_answer": 0.19553744792938232, "avg_score": 0.19553746283054352, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.004168748855591, "lex_sim": 1.0}
{"question": "What name is given to belief in more than one god?", "answer": ["Polytheism"], "scores": [[0.5121709704399109, 0.9837312698364258, 0.9259701371192932]], "normalized_score": [0.7755851149559021], "tokens": [["\u2581Poly", "the", "ism"]], "entropy": [[2.144587516784668, 0.14069810509681702, 0.34622108936309814]], "avg_entropy": [0.8771689037481943], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Polytheists", "score_of_biggest_cluster": 0.7755851149559021, "score_of_first_answer": 0.7755851149559021, "max_score_of_first_answer": 0.9837312698364258, "min_score_of_first_answer": 0.5121709704399109, "avg_score": 0.7755851149559021, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.210009315868566, "std_of_first_answer": 0.210009315868566, "avg_entropy_of_first_answer": 0.8771689037481943, "lex_sim": 1.0}
{"question": "What was the name of the lion in C S Lewis's book 'The Lion, The Witch and The Wardrobe'?", "answer": ["Aslan"], "scores": [[0.8769280910491943, 0.9673399925231934]], "normalized_score": [0.9210253357887268], "tokens": [["\u2581As", "lan"]], "entropy": [[0.78236323595047, 0.3073517680168152]], "avg_entropy": [0.5448575019836426], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ASLAN", "score_of_biggest_cluster": 0.9210253357887268, "score_of_first_answer": 0.9210253357887268, "max_score_of_first_answer": 0.9673399925231934, "min_score_of_first_answer": 0.8769280910491943, "avg_score": 0.9210253357887268, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.04520595073699951, "std_of_first_answer": 0.04520595073699951, "avg_entropy_of_first_answer": 0.5448575019836426, "lex_sim": 1.0}
{"question": "Who directed the 2014 film 'Jersey Boys'?", "answer": ["Clint Eastwood"], "scores": [[0.9757770299911499, 0.9989149570465088, 0.9781904220581055, 0.9932676553726196]], "normalized_score": [0.9864888787269592], "tokens": [["\u2581Cl", "int", "\u2581East", "wood"]], "entropy": [[0.23541203141212463, 0.010236422531306744, 0.21937429904937744, 0.06267007440328598]], "avg_entropy": [0.1319232068490237], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Crint Eastwood", "score_of_biggest_cluster": 0.9864888787269592, "score_of_first_answer": 0.9864888787269592, "max_score_of_first_answer": 0.9989149570465088, "min_score_of_first_answer": 0.9757770299911499, "avg_score": 0.9864888787269592, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.009797421334924774, "std_of_first_answer": 0.009797421334924774, "avg_entropy_of_first_answer": 0.1319232068490237, "lex_sim": 1.0}
{"question": "What breed of dog is Marmaduke in the newspaper cartoon strip of that name?", "answer": ["Cockapoo"], "scores": [[0.00929794181138277, 0.10285031795501709, 0.640367329120636, 0.9805213212966919]], "normalized_score": [0.15653793513774872], "tokens": [["\u2581C", "ock", "ap", "oo"]], "entropy": [[3.8261778354644775, 0.9682863354682922, 1.711440086364746, 0.12351584434509277]], "avg_entropy": [1.6573550254106522], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Great daine", "score_of_biggest_cluster": 0.15653793513774872, "score_of_first_answer": 0.15653793513774872, "max_score_of_first_answer": 0.9805213212966919, "min_score_of_first_answer": 0.00929794181138277, "avg_score": 0.15653793513774872, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39727282323772567, "std_of_first_answer": 0.39727282323772567, "avg_entropy_of_first_answer": 1.6573550254106522, "lex_sim": 1.0}
{"question": "As at March 2001, who holds the record for scoring the most league goals for the soccer team Arsenal?", "answer": ["Thierry Henry"], "scores": [[0.28567031025886536, 0.987200140953064, 0.9827160835266113, 0.885144829750061]], "normalized_score": [0.7037658095359802], "tokens": [["\u2581Th", "ier", "ry", "\u2581Henry"]], "entropy": [[2.768280029296875, 0.12665554881095886, 0.10968474298715591, 0.7466049194335938]], "avg_entropy": [0.9378063101321459], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mr Wright", "score_of_biggest_cluster": 0.7037658095359802, "score_of_first_answer": 0.7037658095359802, "max_score_of_first_answer": 0.987200140953064, "min_score_of_first_answer": 0.28567031025886536, "avg_score": 0.7037658095359802, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2912625700916973, "std_of_first_answer": 0.2912625700916973, "avg_entropy_of_first_answer": 0.9378063101321459, "lex_sim": 1.0}
{"question": "Mount Godwin-Austen is better known by which name?", "answer": ["Nanga Parbat"], "scores": [[0.01853729411959648, 0.8873670101165771, 0.993690013885498, 0.9909781217575073]], "normalized_score": [0.35675179958343506], "tokens": [["\u2581N", "anga", "\u2581Par", "bat"]], "entropy": [[2.158979892730713, 0.6528075933456421, 0.05372566357254982, 0.07139216363430023]], "avg_entropy": [0.7342263283208013], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mount K-2", "score_of_biggest_cluster": 0.35675179958343506, "score_of_first_answer": 0.35675179958343506, "max_score_of_first_answer": 0.993690013885498, "min_score_of_first_answer": 0.01853729411959648, "avg_score": 0.35675179958343506, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40876920929390004, "std_of_first_answer": 0.40876920929390004, "avg_entropy_of_first_answer": 0.7342263283208013, "lex_sim": 1.0}
{"question": "\"Which European city in the \"\"low countries\"\" has been dubbed \"\"The Venice Of The North\"\"?\"", "answer": ["Amsterdam"], "scores": [[0.6816445589065552]], "normalized_score": [0.6816445589065552], "tokens": [["\u2581Amsterdam"]], "entropy": [[1.700655460357666]], "avg_entropy": [1.700655460357666], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brugges", "score_of_biggest_cluster": 0.6816445589065552, "score_of_first_answer": 0.6816445589065552, "max_score_of_first_answer": 0.6816445589065552, "min_score_of_first_answer": 0.6816445589065552, "avg_score": 0.6816445589065552, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.700655460357666, "lex_sim": 1.0}
{"question": "What international organisation was created in 1958 to bring about economic integration between European nations?", "answer": ["Economic Community of West African States"], "scores": [[0.011179430410265923, 0.9896532893180847, 0.320207417011261, 0.6669662594795227, 0.1490418016910553, 0.5818622708320618, 0.9358241558074951]], "normalized_score": [0.29441896080970764], "tokens": [["\u2581Econom", "ic", "\u2581Community", "\u2581of", "\u2581West", "\u2581African", "\u2581States"]], "entropy": [[2.3336472511291504, 0.09096917510032654, 2.8643813133239746, 1.1281940937042236, 1.5459165573120117, 1.404740810394287, 0.3607768416404724]], "avg_entropy": [1.3898037203720637], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The European Community within the Union", "score_of_biggest_cluster": 0.29441896080970764, "score_of_first_answer": 0.29441896080970764, "max_score_of_first_answer": 0.9896532893180847, "min_score_of_first_answer": 0.011179430410265923, "avg_score": 0.29441896080970764, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34955162713955873, "std_of_first_answer": 0.34955162713955873, "avg_entropy_of_first_answer": 1.3898037203720637, "lex_sim": 1.0}
{"question": "What is nicknamed 'The Cathedral of the Railways'?", "answer": ["Waterloo Station"], "scores": [[0.0038227569311857224, 0.9440416693687439, 0.9970447421073914, 0.6927645802497864]], "normalized_score": [0.2234431654214859], "tokens": [["\u2581Water", "lo", "o", "\u2581Station"]], "entropy": [[2.477548122406006, 0.39695310592651367, 0.028088940307497978, 1.156815528869629]], "avg_entropy": [1.0148514243774116], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "St. Pancras", "score_of_biggest_cluster": 0.2234431654214859, "score_of_first_answer": 0.2234431654214859, "max_score_of_first_answer": 0.9970447421073914, "min_score_of_first_answer": 0.0038227569311857224, "avg_score": 0.2234431654214859, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3955753933509476, "std_of_first_answer": 0.3955753933509476, "avg_entropy_of_first_answer": 1.0148514243774116, "lex_sim": 1.0}
{"question": "Which is the style of music, characterised by staccato offbeat rhythms, and owing much to the influence of mento, jazz, ska and rhythm and blues?", "answer": ["Ska"], "scores": [[0.3700852394104004, 0.9748875498771667]], "normalized_score": [0.600659191608429], "tokens": [["\u2581S", "ka"]], "entropy": [[2.4627084732055664, 0.17518578469753265]], "avg_entropy": [1.3189471289515495], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Reggae", "score_of_biggest_cluster": 0.600659191608429, "score_of_first_answer": 0.600659191608429, "max_score_of_first_answer": 0.9748875498771667, "min_score_of_first_answer": 0.3700852394104004, "avg_score": 0.600659191608429, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3024011552333832, "std_of_first_answer": 0.3024011552333832, "avg_entropy_of_first_answer": 1.3189471289515495, "lex_sim": 1.0}
{"question": "What country became democratic after the death of General\u00edsimo Franco?", "answer": ["Spain"], "scores": [[0.8630333542823792]], "normalized_score": [0.8630333542823792], "tokens": [["\u2581Spain"]], "entropy": [[1.1412945985794067]], "avg_entropy": [1.1412945985794067], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Islands of Spain", "score_of_biggest_cluster": 0.8630333542823792, "score_of_first_answer": 0.8630333542823792, "max_score_of_first_answer": 0.8630333542823792, "min_score_of_first_answer": 0.8630333542823792, "avg_score": 0.8630333542823792, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.1412945985794067, "lex_sim": 1.0}
{"question": "Sixty year old Cristina Fernandez de Kirchner is the current president of which South American country ?", "answer": ["Argentina, President Cristina Fernandez de Kirchner"], "scores": [[0.8272725343704224, 0.005742780864238739, 0.023796625435352325, 0.17514389753341675, 0.9932858347892761, 0.8969433307647705, 0.9585685729980469, 0.998565137386322, 0.869182288646698, 0.979073166847229, 0.9989664554595947]], "normalized_score": [0.3628498613834381], "tokens": [["\u2581Argentina", ",", "\u2581President", "\u2581Crist", "ina", "\u2581Fern", "ande", "z", "\u2581de", "\u2581Kirch", "ner"]], "entropy": [[1.1841329336166382, 0.22158896923065186, 4.543673515319824, 2.006545066833496, 0.05985172837972641, 0.6557080149650574, 0.2534518837928772, 0.015196168795228004, 0.582155704498291, 0.15396054089069366, 0.010329951532185078]], "avg_entropy": [0.880599497986788], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Arxintina", "score_of_biggest_cluster": 0.3628498613834381, "score_of_first_answer": 0.3628498613834381, "max_score_of_first_answer": 0.9989664554595947, "min_score_of_first_answer": 0.005742780864238739, "avg_score": 0.3628498613834381, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39399351690091544, "std_of_first_answer": 0.39399351690091544, "avg_entropy_of_first_answer": 0.880599497986788, "lex_sim": 1.0}
{"question": "Which car manufacturer produces models called Concerto, Inspire, Legend, Torneo, etc?", "answer": ["Honda"], "scores": [[0.06410034000873566, 0.9889430999755859]], "normalized_score": [0.25177687406539917], "tokens": [["\u2581H", "onda"]], "entropy": [[3.6092450618743896, 0.089872345328331]], "avg_entropy": [1.8495587036013603], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Honda Technologies", "score_of_biggest_cluster": 0.25177687406539917, "score_of_first_answer": 0.25177687406539917, "max_score_of_first_answer": 0.9889430999755859, "min_score_of_first_answer": 0.06410034000873566, "avg_score": 0.25177687406539917, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46242137998342514, "std_of_first_answer": 0.46242137998342514, "avg_entropy_of_first_answer": 1.8495587036013603, "lex_sim": 1.0}
{"question": "What colour is the star on the national flag of Liberia?", "answer": ["Red Yellow Blue"], "scores": [[0.2718748152256012, 0.00011626197374425828, 0.9493967890739441, 0.07471174746751785]], "normalized_score": [0.038695525377988815], "tokens": [["\u2581Red", "\u2581Y", "ellow", "\u2581Blue"]], "entropy": [[2.81070613861084, 0.7700875401496887, 0.36248087882995605, 2.373138427734375]], "avg_entropy": [1.579103246331215], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A. White (Hampshire cricketer)", "score_of_biggest_cluster": 0.038695525377988815, "score_of_first_answer": 0.038695525377988815, "max_score_of_first_answer": 0.9493967890739441, "min_score_of_first_answer": 0.00011626197374425828, "avg_score": 0.038695525377988815, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3744607739135032, "std_of_first_answer": 0.3744607739135032, "avg_entropy_of_first_answer": 1.579103246331215, "lex_sim": 1.0}
{"question": "Who voices Donkey in the Shrek series of films?", "answer": ["Eddie Kaye Thomas"], "scores": [[0.8253390789031982, 0.9696404933929443, 0.0008733727154321969, 0.6954411864280701, 0.9702203869819641]], "normalized_score": [0.2161298543214798], "tokens": [["\u2581Edd", "ie", "\u2581K", "aye", "\u2581Thomas"]], "entropy": [[1.2472879886627197, 0.15307527780532837, 0.16777655482292175, 2.0815482139587402, 0.20422601699829102]], "avg_entropy": [0.7707828104496002], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eddy Murphy", "score_of_biggest_cluster": 0.2161298543214798, "score_of_first_answer": 0.2161298543214798, "max_score_of_first_answer": 0.9702203869819641, "min_score_of_first_answer": 0.0008733727154321969, "avg_score": 0.2161298543214798, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3605378025170702, "std_of_first_answer": 0.3605378025170702, "avg_entropy_of_first_answer": 0.7707828104496002, "lex_sim": 1.0}
{"question": "\"What film, released in 2000, is the story of Gilbert and Sullivan's first production of \"\"The Mikado\"\"?\"", "answer": ["The Mikado (film)"], "scores": [[0.19951993227005005, 0.24965068697929382, 0.9965483546257019, 0.015287459827959538, 0.30244752764701843, 0.969541609287262]], "normalized_score": [0.24616612493991852], "tokens": [["\u2581The", "\u2581Mik", "ado", "\u2581(", "film", ")"]], "entropy": [[3.243455410003662, 3.4345295429229736, 0.0386606827378273, 0.5936682224273682, 2.738523483276367, 0.21796870231628418]], "avg_entropy": [1.7111343406140804], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Topsy-Turvy", "score_of_biggest_cluster": 0.24616612493991852, "score_of_first_answer": 0.24616612493991852, "max_score_of_first_answer": 0.9965483546257019, "min_score_of_first_answer": 0.015287459827959538, "avg_score": 0.24616612493991852, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3834240413003933, "std_of_first_answer": 0.3834240413003933, "avg_entropy_of_first_answer": 1.7111343406140804, "lex_sim": 1.0}
{"question": "Who won the best actor award at the 2008 Oscar awards ceremony for his role in the film 'There Will Be Blood'?", "answer": ["Daniel Day Lewis"], "scores": [[0.9872898459434509, 0.9395829439163208, 0.3007240891456604]], "normalized_score": [0.6534053087234497], "tokens": [["\u2581Daniel", "\u2581Day", "\u2581Lewis"]], "entropy": [[0.12039323151111603, 0.45100265741348267, 0.7894436120986938]], "avg_entropy": [0.45361316700776416], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Daniel Day Lewis", "score_of_biggest_cluster": 0.6534053087234497, "score_of_first_answer": 0.6534053087234497, "max_score_of_first_answer": 0.9872898459434509, "min_score_of_first_answer": 0.3007240891456604, "avg_score": 0.6534053087234497, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.31301209121609, "std_of_first_answer": 0.31301209121609, "avg_entropy_of_first_answer": 0.45361316700776416, "lex_sim": 1.0}
{"question": "Mrs Lovett made meat pies from material supplied by whom?", "answer": ["Mr Sweeney Todd"], "scores": [[0.12264235317707062, 0.767128586769104, 0.9609620571136475, 0.9319059252738953, 0.4224080443382263, 0.9963400363922119]], "normalized_score": [0.5731757283210754], "tokens": [["\u2581Mr", "\u2581S", "ween", "ey", "\u2581Tod", "d"]], "entropy": [[3.779808521270752, 1.6063928604125977, 0.24353283643722534, 0.26549196243286133, 0.8944464921951294, 0.0327853262424469]], "avg_entropy": [1.1370763331651688], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sweeny tod", "score_of_biggest_cluster": 0.5731757283210754, "score_of_first_answer": 0.5731757283210754, "max_score_of_first_answer": 0.9963400363922119, "min_score_of_first_answer": 0.12264235317707062, "avg_score": 0.5731757283210754, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3226463604123814, "std_of_first_answer": 0.3226463604123814, "avg_entropy_of_first_answer": 1.1370763331651688, "lex_sim": 1.0}
{"question": "Which European country has the world\u2019s oldest national flag?", "answer": ["Sweden (National and Civil)"], "scores": [[0.05587088689208031, 0.007549669593572617, 0.0008991580689325929, 0.0011067141313105822, 0.23933537304401398, 0.27723428606987]], "normalized_score": [0.01741030439734459], "tokens": [["\u2581Sweden", "\u2581(", "National", "\u2581and", "\u2581Civil", ")"]], "entropy": [[3.995723009109497, 0.3291590213775635, 4.4215545654296875, 2.6870462894439697, 3.8640167713165283, 2.803067207336426]], "avg_entropy": [3.016761144002279], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eastern Denmark", "score_of_biggest_cluster": 0.01741030439734459, "score_of_first_answer": 0.01741030439734459, "max_score_of_first_answer": 0.27723428606987, "min_score_of_first_answer": 0.0008991580689325929, "avg_score": 0.01741030439734459, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11609451317896176, "std_of_first_answer": 0.11609451317896176, "avg_entropy_of_first_answer": 3.016761144002279, "lex_sim": 1.0}
{"question": "In professional darts, which player is nicknamed 'The Artist'?", "answer": ["Andy 'The Artist' Jenkins"], "scores": [[0.015357189811766148, 0.05061362683773041, 0.6550971865653992, 0.35023653507232666, 0.9920766353607178, 0.9790697693824768, 0.08888624608516693]], "normalized_score": [0.20534846186637878], "tokens": [["\u2581Andy", "\u2581'", "The", "\u2581Art", "ist", "'", "\u2581Jenkins"]], "entropy": [[4.0596795082092285, 2.5964269638061523, 1.730778694152832, 2.903226852416992, 0.06843744963407516, 0.16330048441886902, 2.80108642578125]], "avg_entropy": [2.0461337683456287], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kevin Painter", "score_of_biggest_cluster": 0.20534846186637878, "score_of_first_answer": 0.20534846186637878, "max_score_of_first_answer": 0.9920766353607178, "min_score_of_first_answer": 0.015357189811766148, "avg_score": 0.20534846186637878, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39723270220661966, "std_of_first_answer": 0.39723270220661966, "avg_entropy_of_first_answer": 2.0461337683456287, "lex_sim": 1.0}
{"question": "What would you find on Carmen Miranda's hat?", "answer": ["A fruit or vegetable"], "scores": [[0.14708733558654785, 0.08002937585115433, 0.00427518505603075, 0.34542375802993774, 0.9761987924575806]], "normalized_score": [0.11115628480911255], "tokens": [["\u2581A", "\u2581fruit", "\u2581or", "\u2581veget", "able"]], "entropy": [[3.8750391006469727, 3.205653667449951, 2.033566951751709, 3.225541591644287, 0.12025906145572662]], "avg_entropy": [2.4920120745897294], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fruiting", "score_of_biggest_cluster": 0.11115628480911255, "score_of_first_answer": 0.11115628480911255, "max_score_of_first_answer": 0.9761987924575806, "min_score_of_first_answer": 0.00427518505603075, "avg_score": 0.11115628480911255, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3515589688791926, "std_of_first_answer": 0.3515589688791926, "avg_entropy_of_first_answer": 2.4920120745897294, "lex_sim": 1.0}
{"question": "September 23, 1939 saw the death of what famed neurologist, best known for founding the field of psychoanalysis?", "answer": ["Sigmund Freud (neurologist)"], "scores": [[0.9079119563102722, 0.9301103949546814, 0.9277138113975525, 0.9970736503601074, 0.002723982557654381, 0.033346161246299744, 0.6495195627212524, 0.9837239980697632, 0.9711606502532959, 0.8345543146133423]], "normalized_score": [0.36017873883247375], "tokens": [["\u2581Sig", "mund", "\u2581Fre", "ud", "\u2581(", "ne", "uro", "log", "ist", ")"]], "entropy": [[0.7157196998596191, 0.42182666063308716, 0.585494875907898, 0.029555724933743477, 0.17376266419887543, 4.793896675109863, 1.1890101432800293, 0.12916156649589539, 0.17538148164749146, 0.865280032157898]], "avg_entropy": [0.9079089524224401], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Zigi Frojd", "score_of_biggest_cluster": 0.36017873883247375, "score_of_first_answer": 0.36017873883247375, "max_score_of_first_answer": 0.9970736503601074, "min_score_of_first_answer": 0.002723982557654381, "avg_score": 0.36017873883247375, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36550369675122407, "std_of_first_answer": 0.36550369675122407, "avg_entropy_of_first_answer": 0.9079089524224401, "lex_sim": 1.0}
{"question": "By what name was the shipping area Fitzroy known until 2002?", "answer": ["South Atlantic"], "scores": [[0.005169787909835577, 0.052248213440179825]], "normalized_score": [0.01643509231507778], "tokens": [["\u2581South", "\u2581Atlantic"]], "entropy": [[5.071878910064697, 3.816519260406494]], "avg_entropy": [4.444199085235596], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Finisterre", "score_of_biggest_cluster": 0.01643509231507778, "score_of_first_answer": 0.01643509231507778, "max_score_of_first_answer": 0.052248213440179825, "min_score_of_first_answer": 0.005169787909835577, "avg_score": 0.01643509231507778, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.023539212765172124, "std_of_first_answer": 0.023539212765172124, "avg_entropy_of_first_answer": 4.444199085235596, "lex_sim": 1.0}
{"question": "In the television series \u2018Thunderbirds\u2019, what is Lady Penelope\u2019s surname?", "answer": ["Pitstop"], "scores": [[0.0014523145509883761, 0.488384485244751]], "normalized_score": [0.026632456108927727], "tokens": [["\u2581Pit", "stop"]], "entropy": [[2.6903109550476074, 2.7410342693328857]], "avg_entropy": [2.7156726121902466], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Creighton-Ward", "score_of_biggest_cluster": 0.026632456108927727, "score_of_first_answer": 0.026632456108927727, "max_score_of_first_answer": 0.488384485244751, "min_score_of_first_answer": 0.0014523145509883761, "avg_score": 0.026632456108927727, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2434660853468813, "std_of_first_answer": 0.2434660853468813, "avg_entropy_of_first_answer": 2.7156726121902466, "lex_sim": 1.0}
{"question": "Which Indian born author who lived from 1903 to 1950 had the real name Eric Arthur Blair ?", "answer": ["George Orwell"], "scores": [[0.8664802312850952, 0.9776221513748169, 0.9701635241508484]], "normalized_score": [0.9366806149482727], "tokens": [["\u2581George", "\u2581Or", "well"]], "entropy": [[1.1205703020095825, 0.21830958127975464, 0.2175517976284027]], "avg_entropy": [0.5188105603059133], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Geoge orwell", "score_of_biggest_cluster": 0.9366806149482727, "score_of_first_answer": 0.9366806149482727, "max_score_of_first_answer": 0.9776221513748169, "min_score_of_first_answer": 0.8664802312850952, "avg_score": 0.9366806149482727, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.05072626181638697, "std_of_first_answer": 0.05072626181638697, "avg_entropy_of_first_answer": 0.5188105603059133, "lex_sim": 1.0}
{"question": "Which Radio 2 DJ christened \u2018Dallas\u2019 character Lucy Ewing \u2018The Poison Dwarf\u2019?", "answer": ["Kenny Everett"], "scores": [[0.10033681988716125, 0.8216140270233154, 0.976035475730896, 0.9931421279907227]], "normalized_score": [0.53168123960495], "tokens": [["\u2581Ken", "ny", "\u2581Ever", "ett"]], "entropy": [[4.33338737487793, 0.8322878479957581, 0.22813686728477478, 0.05895550176501274]], "avg_entropy": [1.3631918979808688], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tezza Wogan", "score_of_biggest_cluster": 0.53168123960495, "score_of_first_answer": 0.53168123960495, "max_score_of_first_answer": 0.9931421279907227, "min_score_of_first_answer": 0.10033681988716125, "avg_score": 0.53168123960495, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36552622805625395, "std_of_first_answer": 0.36552622805625395, "avg_entropy_of_first_answer": 1.3631918979808688, "lex_sim": 1.0}
{"question": "Where were the FIBA (Basketball) World Championships held in 2002?", "answer": ["Indianapolis, Indiana"], "scores": [[0.5757988095283508, 0.989840567111969, 0.5111925601959229, 0.35841840505599976]], "normalized_score": [0.5684636235237122], "tokens": [["\u2581Indian", "apolis", ",", "\u2581Indiana"]], "entropy": [[2.5239362716674805, 0.10354754328727722, 1.096339464187622, 1.6496586799621582]], "avg_entropy": [1.3433704897761345], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "INpolis (city), Marion County, IN", "score_of_biggest_cluster": 0.5684636235237122, "score_of_first_answer": 0.5684636235237122, "max_score_of_first_answer": 0.989840567111969, "min_score_of_first_answer": 0.35841840505599976, "avg_score": 0.5684636235237122, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2337194617034429, "std_of_first_answer": 0.2337194617034429, "avg_entropy_of_first_answer": 1.3433704897761345, "lex_sim": 1.0}
{"question": "\"In economics, whose law states that, \"\"Bad Money Drives Out Good\"\"?\"", "answer": ["Ludwig von Mises"], "scores": [[0.007530677132308483, 0.4992605149745941, 0.9730019569396973, 0.9814642071723938]], "normalized_score": [0.24478647112846375], "tokens": [["\u2581Ludwig", "\u2581von", "\u2581M", "ises"]], "entropy": [[5.647834777832031, 1.4841856956481934, 0.26155662536621094, 0.1605626940727234]], "avg_entropy": [1.8885349482297897], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Thomas Gresham", "score_of_biggest_cluster": 0.24478647112846375, "score_of_first_answer": 0.24478647112846375, "max_score_of_first_answer": 0.9814642071723938, "min_score_of_first_answer": 0.007530677132308483, "avg_score": 0.24478647112846375, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4015207428310076, "std_of_first_answer": 0.4015207428310076, "avg_entropy_of_first_answer": 1.8885349482297897, "lex_sim": 1.0}
{"question": "What is the title of the most famous painting by Franz Hals?", "answer": ["A Laughing Cavalier"], "scores": [[0.08638052642345428, 0.06634225696325302, 0.900910496711731, 0.9913257956504822, 0.520427942276001, 0.9899388551712036]], "normalized_score": [0.37168827652931213], "tokens": [["\u2581A", "\u2581L", "aug", "hing", "\u2581Caval", "ier"]], "entropy": [[2.7157769203186035, 4.817267894744873, 0.6297136545181274, 0.06193730607628822, 1.9281442165374756, 0.07526172697544098]], "avg_entropy": [1.7046836198618014], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Laughing Cavalier", "score_of_biggest_cluster": 0.37168827652931213, "score_of_first_answer": 0.37168827652931213, "max_score_of_first_answer": 0.9913257956504822, "min_score_of_first_answer": 0.06634225696325302, "avg_score": 0.37168827652931213, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.39798030714567995, "std_of_first_answer": 0.39798030714567995, "avg_entropy_of_first_answer": 1.7046836198618014, "lex_sim": 1.0}
{"question": "The Appian Way ran from Rome to which Italian port?", "answer": ["Bai"], "scores": [[0.10359690338373184, 0.03287319093942642]], "normalized_score": [0.05835719034075737], "tokens": [["\u2581B", "ai"]], "entropy": [[3.4134116172790527, 2.2979021072387695]], "avg_entropy": [2.855656862258911], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brindisi", "score_of_biggest_cluster": 0.05835719034075737, "score_of_first_answer": 0.05835719034075737, "max_score_of_first_answer": 0.10359690338373184, "min_score_of_first_answer": 0.03287319093942642, "avg_score": 0.05835719034075737, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.03536185622215271, "std_of_first_answer": 0.03536185622215271, "avg_entropy_of_first_answer": 2.855656862258911, "lex_sim": 1.0}
{"question": "Which novelist, born in Bombay in 1865, was awarded the Nobel prize for Literature in 1907?", "answer": ["Rudyard Kipling"], "scores": [[0.7164853811264038, 0.9582270383834839, 0.9702889919281006, 0.9935779571533203]], "normalized_score": [0.9019753932952881], "tokens": [["\u2581Rud", "yard", "\u2581Ki", "pling"]], "entropy": [[1.455486536026001, 0.3199642300605774, 0.2622619867324829, 0.045759666711091995]], "avg_entropy": [0.5208681048825383], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joseph Rudyard Kipling", "score_of_biggest_cluster": 0.9019753932952881, "score_of_first_answer": 0.9019753932952881, "max_score_of_first_answer": 0.9935779571533203, "min_score_of_first_answer": 0.7164853811264038, "avg_score": 0.9019753932952881, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.11224224680380401, "std_of_first_answer": 0.11224224680380401, "avg_entropy_of_first_answer": 0.5208681048825383, "lex_sim": 1.0}
{"question": "Which token in a game of Monopoly was once known as Rich Uncle Pennybags?", "answer": ["Old Kent Road"], "scores": [[0.01386994682252407, 0.22462882101535797, 0.5965688228607178]], "normalized_score": [0.1229514628648758], "tokens": [["\u2581Old", "\u2581Kent", "\u2581Road"]], "entropy": [[4.632382392883301, 4.030318260192871, 1.7849702835083008]], "avg_entropy": [3.4825569788614907], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stovepipe hat", "score_of_biggest_cluster": 0.1229514628648758, "score_of_first_answer": 0.1229514628648758, "max_score_of_first_answer": 0.5965688228607178, "min_score_of_first_answer": 0.01386994682252407, "avg_score": 0.1229514628648758, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24090031321411565, "std_of_first_answer": 0.24090031321411565, "avg_entropy_of_first_answer": 3.4825569788614907, "lex_sim": 1.0}
{"question": "Who would take silk as part of their job?", "answer": ["a silk weaver"], "scores": [[0.04591837525367737, 0.03242221474647522, 0.7533155679702759, 0.03823264688253403, 0.9706237316131592]], "normalized_score": [0.13300205767154694], "tokens": [["\u2581a", "\u2581sil", "k", "\u2581we", "aver"]], "entropy": [[4.803013801574707, 4.356095314025879, 0.9110674262046814, 3.30975079536438, 0.1721399426460266]], "avg_entropy": [2.7104134559631348], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Split legal profession", "score_of_biggest_cluster": 0.13300205767154694, "score_of_first_answer": 0.13300205767154694, "max_score_of_first_answer": 0.9706237316131592, "min_score_of_first_answer": 0.03242221474647522, "avg_score": 0.13300205767154694, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40907674742568556, "std_of_first_answer": 0.40907674742568556, "avg_entropy_of_first_answer": 2.7104134559631348, "lex_sim": 1.0}
{"question": "Which group had UK top ten hits in 1987 with 'Sweet Little Mystery' and 'Angel Eyes'?", "answer": ["Woolf-Woolf"], "scores": [[0.020162388682365417, 0.00038581155240535736, 0.029988782480359077, 0.004544989671558142, 0.024950170889496803, 0.44365736842155457, 0.9797975420951843]], "normalized_score": [0.027367694303393364], "tokens": [["\u2581W", "ool", "f", "-", "W", "ool", "f"]], "entropy": [[5.0710296630859375, 0.625304102897644, 2.694662570953369, 3.2660515308380127, 5.464102268218994, 3.185948371887207, 0.15260697901248932]], "avg_entropy": [2.9228150695562363], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wet wet wet", "score_of_biggest_cluster": 0.027367694303393364, "score_of_first_answer": 0.027367694303393364, "max_score_of_first_answer": 0.9797975420951843, "min_score_of_first_answer": 0.00038581155240535736, "avg_score": 0.027367694303393364, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.345555347999993, "std_of_first_answer": 0.345555347999993, "avg_entropy_of_first_answer": 2.9228150695562363, "lex_sim": 1.0}
{"question": "Which river originates in the Taurus Mountains, and flows through Syria and Iraq?", "answer": ["Tigris"], "scores": [[0.565712571144104, 0.9834681749343872]], "normalized_score": [0.7458956837654114], "tokens": [["\u2581Tig", "ris"]], "entropy": [[1.5196387767791748, 0.12797510623931885]], "avg_entropy": [0.8238069415092468], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Euphr\u00e1t\u0113s", "score_of_biggest_cluster": 0.7458956837654114, "score_of_first_answer": 0.7458956837654114, "max_score_of_first_answer": 0.9834681749343872, "min_score_of_first_answer": 0.565712571144104, "avg_score": 0.7458956837654114, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2088778018951416, "std_of_first_answer": 0.2088778018951416, "avg_entropy_of_first_answer": 0.8238069415092468, "lex_sim": 1.0}
{"question": "Which TV series, filmed in and around Aylesbury in Buckinghamshire, is adapted from the novels of Caroline Graham?", "answer": ["Midsomer Murders"], "scores": [[0.6850610971450806, 0.991521954536438, 0.928119421005249, 0.9182168841362, 0.9930300116539001]], "normalized_score": [0.8951764106750488], "tokens": [["\u2581M", "ids", "omer", "\u2581Mur", "ders"]], "entropy": [[2.057512044906616, 0.06866440922021866, 0.28592604398727417, 0.4197729229927063, 0.046629611402750015]], "avg_entropy": [0.575701006501913], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Midsummer Murders", "score_of_biggest_cluster": 0.8951764106750488, "score_of_first_answer": 0.8951764106750488, "max_score_of_first_answer": 0.9930300116539001, "min_score_of_first_answer": 0.6850610971450806, "avg_score": 0.8951764106750488, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11340305412581365, "std_of_first_answer": 0.11340305412581365, "avg_entropy_of_first_answer": 0.575701006501913, "lex_sim": 1.0}
{"question": "What was the parliamentary constituency of Tony Blair when he was leader of the Labour Party?", "answer": ["Edinburgh South"], "scores": [[0.009651711210608482, 0.2611238360404968]], "normalized_score": [0.05020250380039215], "tokens": [["\u2581Edinburgh", "\u2581South"]], "entropy": [[2.098862648010254, 2.0927834510803223]], "avg_entropy": [2.095823049545288], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Segfield", "score_of_biggest_cluster": 0.05020250380039215, "score_of_first_answer": 0.05020250380039215, "max_score_of_first_answer": 0.2611238360404968, "min_score_of_first_answer": 0.009651711210608482, "avg_score": 0.05020250380039215, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12573606241494417, "std_of_first_answer": 0.12573606241494417, "avg_entropy_of_first_answer": 2.095823049545288, "lex_sim": 1.0}
{"question": "How is seasonal allergic rhinitis more commonly known ?", "answer": ["Hay fever"], "scores": [[0.7163330912590027, 0.6727814674377441, 0.99628746509552]], "normalized_score": [0.7830531597137451], "tokens": [["\u2581Hay", "\u2581fe", "ver"]], "entropy": [[1.3534858226776123, 1.0395550727844238, 0.03686089813709259]], "avg_entropy": [0.8099672645330429], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hayfever", "score_of_biggest_cluster": 0.7830531597137451, "score_of_first_answer": 0.7830531597137451, "max_score_of_first_answer": 0.99628746509552, "min_score_of_first_answer": 0.6727814674377441, "avg_score": 0.7830531597137451, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1433439243715841, "std_of_first_answer": 0.1433439243715841, "avg_entropy_of_first_answer": 0.8099672645330429, "lex_sim": 1.0}
{"question": "Which method of painting consists of applying pigment to wet plaster?", "answer": ["Fresco"], "scores": [[0.3212144076824188, 0.9940539002418518, 0.9830840229988098]], "normalized_score": [0.6796185374259949], "tokens": [["\u2581F", "res", "co"]], "entropy": [[2.7082364559173584, 0.05453544482588768, 0.12251666188240051]], "avg_entropy": [0.9617628542085489], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fresco painting", "score_of_biggest_cluster": 0.6796185374259949, "score_of_first_answer": 0.6796185374259949, "max_score_of_first_answer": 0.9940539002418518, "min_score_of_first_answer": 0.3212144076824188, "avg_score": 0.6796185374259949, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3146258286765108, "std_of_first_answer": 0.3146258286765108, "avg_entropy_of_first_answer": 0.9617628542085489, "lex_sim": 1.0}
{"question": "In 2001, Dennis Tito paid a reported $20 million to be the first tourist to go where?", "answer": ["Space"], "scores": [[0.3885989189147949]], "normalized_score": [0.3885989189147949], "tokens": [["\u2581Space"]], "entropy": [[2.9208974838256836]], "avg_entropy": [2.9208974838256836], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Space", "score_of_biggest_cluster": 0.3885989189147949, "score_of_first_answer": 0.3885989189147949, "max_score_of_first_answer": 0.3885989189147949, "min_score_of_first_answer": 0.3885989189147949, "avg_score": 0.3885989189147949, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.9208974838256836, "lex_sim": 1.0}
{"question": "Nicknamed 'Robo Cop' when a policeman, Ray Mallon is the directly elected Mayor of which town?", "answer": ["Walker"], "scores": [[0.00035267745261080563]], "normalized_score": [0.000352677539922297], "tokens": [["\u2581Walker"]], "entropy": [[1.4068070650100708]], "avg_entropy": [1.4068070650100708], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Port Darlington", "score_of_biggest_cluster": 0.000352677539922297, "score_of_first_answer": 0.000352677539922297, "max_score_of_first_answer": 0.00035267745261080563, "min_score_of_first_answer": 0.00035267745261080563, "avg_score": 0.000352677539922297, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.4068070650100708, "lex_sim": 1.0}
{"question": "In the Harry Potter books, what type of creature is Aragog?", "answer": ["Acromantula \u32c1"], "scores": [[0.1900232583284378, 0.9660327434539795, 0.9477092027664185, 0.8913393020629883, 0.0004414726863615215, 0.00012272775347810239, 0.005526062101125717, 0.02270541340112686]], "normalized_score": [0.03183196857571602], "tokens": [["\u2581Ac", "rom", "ant", "ula", "\u2581", "<0xE3>", "<0x8B>", "<0x81>"]], "entropy": [[3.7834463119506836, 0.2779948115348816, 0.33082932233810425, 0.5403657555580139, 0.4744020402431488, 4.188806056976318, 2.1070618629455566, 3.187854766845703]], "avg_entropy": [1.8613451160490513], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Giant spider", "score_of_biggest_cluster": 0.03183196857571602, "score_of_first_answer": 0.03183196857571602, "max_score_of_first_answer": 0.9660327434539795, "min_score_of_first_answer": 0.00012272775347810239, "avg_score": 0.03183196857571602, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.43582053766507045, "std_of_first_answer": 0.43582053766507045, "avg_entropy_of_first_answer": 1.8613451160490513, "lex_sim": 1.0}
{"question": "Canberra is the capital of which country?", "answer": ["Australia"], "scores": [[0.9161931872367859]], "normalized_score": [0.9161931872367859], "tokens": [["\u2581Australia"]], "entropy": [[0.7025680541992188]], "avg_entropy": [0.7025680541992188], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Australia (Commonwealth realm)", "score_of_biggest_cluster": 0.9161931872367859, "score_of_first_answer": 0.9161931872367859, "max_score_of_first_answer": 0.9161931872367859, "min_score_of_first_answer": 0.9161931872367859, "avg_score": 0.9161931872367859, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.7025680541992188, "lex_sim": 1.0}
{"question": "The majority of the land area of Africa lies in which hemisphere?", "answer": ["Southern"], "scores": [[0.16342727839946747]], "normalized_score": [0.16342727839946747], "tokens": [["\u2581Southern"]], "entropy": [[2.353285312652588]], "avg_entropy": [2.353285312652588], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Northern (disambiguation)", "score_of_biggest_cluster": 0.16342727839946747, "score_of_first_answer": 0.16342727839946747, "max_score_of_first_answer": 0.16342727839946747, "min_score_of_first_answer": 0.16342727839946747, "avg_score": 0.16342727839946747, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.353285312652588, "lex_sim": 1.0}
{"question": "The Tumut Hydroelectric Power Station is in which Australian state?", "answer": ["New South Wales"], "scores": [[0.8103517889976501, 0.9228489995002747, 0.978200376033783]], "normalized_score": [0.9010398983955383], "tokens": [["\u2581New", "\u2581South", "\u2581Wales"]], "entropy": [[1.1707665920257568, 0.4137513041496277, 0.16259081661701202]], "avg_entropy": [0.5823695709307989], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "AU-NSW", "score_of_biggest_cluster": 0.9010398983955383, "score_of_first_answer": 0.9010398983955383, "max_score_of_first_answer": 0.978200376033783, "min_score_of_first_answer": 0.8103517889976501, "avg_score": 0.9010398983955383, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0698351594232624, "std_of_first_answer": 0.0698351594232624, "avg_entropy_of_first_answer": 0.5823695709307989, "lex_sim": 1.0}
{"question": "Jimi Hendrix presented the broken parts of the guitar that he destroyed on-stage at the 1968 Miami Pop Festival to whom? Not sure if the recipient named it Moon Unit.", "answer": ["John Phillips"], "scores": [[0.02763804793357849, 0.05103733018040657, 0.9177455306053162]], "normalized_score": [0.10898647457361221], "tokens": [["\u2581John", "\u2581Phill", "ips"]], "entropy": [[5.266226768493652, 3.3989267349243164, 0.29858624935150146]], "avg_entropy": [2.9879132509231567], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Frank zappa", "score_of_biggest_cluster": 0.10898647457361221, "score_of_first_answer": 0.10898647457361221, "max_score_of_first_answer": 0.9177455306053162, "min_score_of_first_answer": 0.02763804793357849, "avg_score": 0.10898647457361221, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4141956008406428, "std_of_first_answer": 0.4141956008406428, "avg_entropy_of_first_answer": 2.9879132509231567, "lex_sim": 1.0}
{"question": "In which Cornish town is there a branch of the Tate Gallery?", "answer": ["St Ives, Cornwall"], "scores": [[0.7006065845489502, 0.6602635383605957, 0.9827691912651062, 0.10100500285625458, 0.7772756814956665, 0.9841243624687195]], "normalized_score": [0.5722711682319641], "tokens": [["\u2581St", "\u2581I", "ves", ",", "\u2581Corn", "wall"]], "entropy": [[1.63649582862854, 0.8664625883102417, 0.10629002004861832, 0.5214247703552246, 1.055173635482788, 0.1386781930923462]], "avg_entropy": [0.7207541726529598], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "St.Ives", "score_of_biggest_cluster": 0.5722711682319641, "score_of_first_answer": 0.5722711682319641, "max_score_of_first_answer": 0.9841243624687195, "min_score_of_first_answer": 0.10100500285625458, "avg_score": 0.5722711682319641, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.296373883158108, "std_of_first_answer": 0.296373883158108, "avg_entropy_of_first_answer": 0.7207541726529598, "lex_sim": 1.0}
{"question": "Who was the Republican Presidential Nominee in the 2008 American Election?", "answer": ["US Senator John McCain"], "scores": [[0.0004586217110045254, 0.4840563237667084, 0.8910660147666931, 0.7667444348335266, 0.9922771453857422]], "normalized_score": [0.171992227435112], "tokens": [["\u2581US", "\u2581Senator", "\u2581John", "\u2581McC", "ain"]], "entropy": [[1.1125181913375854, 2.0261754989624023, 0.6148646473884583, 0.9405694007873535, 0.07225535809993744]], "avg_entropy": [0.9532766193151474], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jon McCain", "score_of_biggest_cluster": 0.171992227435112, "score_of_first_answer": 0.171992227435112, "max_score_of_first_answer": 0.9922771453857422, "min_score_of_first_answer": 0.0004586217110045254, "avg_score": 0.171992227435112, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35655959441625107, "std_of_first_answer": 0.35655959441625107, "avg_entropy_of_first_answer": 0.9532766193151474, "lex_sim": 1.0}
{"question": "What is the most common bird in the world?", "answer": ["White Domestic Turkey"], "scores": [[0.005646783392876387, 0.008948901668190956, 0.9500502347946167, 0.04395710304379463]], "normalized_score": [0.03811418637633324], "tokens": [["\u2581White", "\u2581Dom", "estic", "\u2581Turkey"]], "entropy": [[3.752279281616211, 4.244294166564941, 0.45092523097991943, 3.536360740661621]], "avg_entropy": [2.995964854955673], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Domestic cock", "score_of_biggest_cluster": 0.03811418637633324, "score_of_first_answer": 0.03811418637633324, "max_score_of_first_answer": 0.9500502347946167, "min_score_of_first_answer": 0.005646783392876387, "avg_score": 0.03811418637633324, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40321198831835675, "std_of_first_answer": 0.40321198831835675, "avg_entropy_of_first_answer": 2.995964854955673, "lex_sim": 1.0}
{"question": "Tomorrow marks the anniversary of the debut of what long running children's television series, originally produced by the Children's Television Workshop?", "answer": ["Sesame Street"], "scores": [[0.6246192455291748, 0.9830975532531738, 0.9684532284736633]], "normalized_score": [0.8409371376037598], "tokens": [["\u2581Ses", "ame", "\u2581Street"]], "entropy": [[2.131779193878174, 0.1353740096092224, 0.20365527272224426]], "avg_entropy": [0.8236028254032135], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Furry Arms", "score_of_biggest_cluster": 0.8409371376037598, "score_of_first_answer": 0.8409371376037598, "max_score_of_first_answer": 0.9830975532531738, "min_score_of_first_answer": 0.6246192455291748, "avg_score": 0.8409371376037598, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.16564451937285618, "std_of_first_answer": 0.16564451937285618, "avg_entropy_of_first_answer": 0.8236028254032135, "lex_sim": 1.0}
{"question": "What is the name of the line separating the numerator and denominator of a fraction if it is written as a diagonal line?", "answer": ["Fraction bar"], "scores": [[0.02782369591295719, 0.9531412720680237, 0.25827184319496155]], "normalized_score": [0.18991082906723022], "tokens": [["\u2581Fra", "ction", "\u2581bar"]], "entropy": [[4.812193393707275, 0.23680740594863892, 1.9980781078338623]], "avg_entropy": [2.349026302496592], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Solidus Line", "score_of_biggest_cluster": 0.18991082906723022, "score_of_first_answer": 0.18991082906723022, "max_score_of_first_answer": 0.9531412720680237, "min_score_of_first_answer": 0.02782369591295719, "avg_score": 0.18991082906723022, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.39329978508194985, "std_of_first_answer": 0.39329978508194985, "avg_entropy_of_first_answer": 2.349026302496592, "lex_sim": 1.0}
{"question": "Apart from an Atlantic coastline, what country is surrounded by Senegal?", "answer": ["The Gambia is surrounded by Senegal."], "scores": [[0.1313367486000061, 0.8827985525131226, 0.997488260269165, 0.99603670835495, 0.0002741685602813959, 0.3248319923877716, 0.9534702301025391, 0.9294862747192383, 0.9949718713760376, 0.9987151622772217, 0.19763725996017456]], "normalized_score": [0.300216406583786], "tokens": [["\u2581The", "\u2581G", "amb", "ia", "\u2581is", "\u2581surrounded", "\u2581by", "\u2581Sen", "eg", "al", "."]], "entropy": [[2.676342010498047, 0.8109906911849976, 0.0268554650247097, 0.03958171606063843, 0.23220032453536987, 2.811086416244507, 0.3000710606575012, 0.5329437255859375, 0.0424598753452301, 0.013985191471874714, 1.1543769836425781]], "avg_entropy": [0.7855357691137628], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Gambia", "score_of_biggest_cluster": 0.300216406583786, "score_of_first_answer": 0.300216406583786, "max_score_of_first_answer": 0.9987151622772217, "min_score_of_first_answer": 0.0002741685602813959, "avg_score": 0.300216406583786, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.39323993599459056, "std_of_first_answer": 0.39323993599459056, "avg_entropy_of_first_answer": 0.7855357691137628, "lex_sim": 1.0}
{"question": "Which war veterna, who died in 2009 had the nickname 'The Last Fighting Tommy'?", "answer": ["John Edward Ethelbert Travers"], "scores": [[0.015250302851200104, 0.005788874812424183, 0.0036666085943579674, 0.9595873355865479, 0.9880822896957397, 0.17872671782970428, 0.9488071203231812]], "normalized_score": [0.09109261631965637], "tokens": [["\u2581John", "\u2581Edward", "\u2581Eth", "el", "bert", "\u2581Tra", "vers"]], "entropy": [[2.632849931716919, 5.308286666870117, 5.165905952453613, 0.2720409631729126, 0.09106741100549698, 4.666986465454102, 0.2861465513706207]], "avg_entropy": [2.6318977060062543], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Harry Patch", "score_of_biggest_cluster": 0.09109261631965637, "score_of_first_answer": 0.09109261631965637, "max_score_of_first_answer": 0.9880822896957397, "min_score_of_first_answer": 0.0036666085943579674, "avg_score": 0.09109261631965637, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4561947295222871, "std_of_first_answer": 0.4561947295222871, "avg_entropy_of_first_answer": 2.6318977060062543, "lex_sim": 1.0}
{"question": "In the Bible, where was Jesus betrayed by Judas, and subsequently arrested?", "answer": ["Jerusalem"], "scores": [[0.07507333904504776]], "normalized_score": [0.07507334649562836], "tokens": [["\u2581Jerusalem"]], "entropy": [[3.1430087089538574]], "avg_entropy": [3.1430087089538574], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gethsemane", "score_of_biggest_cluster": 0.07507334649562836, "score_of_first_answer": 0.07507334649562836, "max_score_of_first_answer": 0.07507333904504776, "min_score_of_first_answer": 0.07507333904504776, "avg_score": 0.07507334649562836, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.1430087089538574, "lex_sim": 1.0}
{"question": "In April 1814 which King gained the French throne to begin the Bourbon Restoration?", "answer": ["Louis XVIII of France"], "scores": [[0.4336388409137726, 0.8761101365089417, 0.052021630108356476, 0.9839748740196228]], "normalized_score": [0.3734338581562042], "tokens": [["\u2581Louis", "\u2581XVIII", "\u2581of", "\u2581France"]], "entropy": [[1.2369837760925293, 0.7674132585525513, 0.5401554107666016, 0.14157763123512268]], "avg_entropy": [0.6715325191617012], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Louis XVIII of France", "score_of_biggest_cluster": 0.3734338581562042, "score_of_first_answer": 0.3734338581562042, "max_score_of_first_answer": 0.9839748740196228, "min_score_of_first_answer": 0.052021630108356476, "avg_score": 0.3734338581562042, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.37111115325800503, "std_of_first_answer": 0.37111115325800503, "avg_entropy_of_first_answer": 0.6715325191617012, "lex_sim": 1.0}
{"question": "What arrow-toting Roman god is now most commonly seen around Valentine's Day?", "answer": ["Eros"], "scores": [[0.19467990100383759, 0.9839059710502625]], "normalized_score": [0.4376605153083801], "tokens": [["\u2581E", "ros"]], "entropy": [[1.1467533111572266, 0.1366051435470581]], "avg_entropy": [0.6416792273521423], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cupid (holiday character)", "score_of_biggest_cluster": 0.4376605153083801, "score_of_first_answer": 0.4376605153083801, "max_score_of_first_answer": 0.9839059710502625, "min_score_of_first_answer": 0.19467990100383759, "avg_score": 0.4376605153083801, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39461303502321243, "std_of_first_answer": 0.39461303502321243, "avg_entropy_of_first_answer": 0.6416792273521423, "lex_sim": 1.0}
{"question": "How many months is the gestation period for an elephant?", "answer": ["23"], "scores": [[0.16871996223926544, 0.7064158320426941, 0.028646910563111305]], "normalized_score": [0.15058033168315887], "tokens": [["\u2581", "2", "3"]], "entropy": [[3.135714054107666, 1.012024164199829, 0.7701150178909302]], "avg_entropy": [1.6392844120661418], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "twenty-two", "score_of_biggest_cluster": 0.15058033168315887, "score_of_first_answer": 0.15058033168315887, "max_score_of_first_answer": 0.7064158320426941, "min_score_of_first_answer": 0.028646910563111305, "avg_score": 0.15058033168315887, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29213923984846163, "std_of_first_answer": 0.29213923984846163, "avg_entropy_of_first_answer": 1.6392844120661418, "lex_sim": 1.0}
{"question": "Which famous conductor conducted the very first performance of Hoist's 'The Planets' in 1918?", "answer": ["Pierre Monteux HALO 2 - How to Start (by The Infinite Quest)"], "scores": [[0.01428139116615057, 0.7921973466873169, 0.9801326990127563, 4.464850007934729e-06, 0.004577380605041981, 0.04319312423467636, 0.04191579297184944, 0.19338296353816986, 0.014288711361587048, 0.010737471282482147, 0.1467793732881546, 0.0012076511047780514, 0.006503008306026459, 0.009463835507631302, 0.006833911407738924, 0.005087294150143862, 0.09350024163722992, 0.022881299257278442, 0.8983431458473206, 0.7254493236541748]], "normalized_score": [0.024571707472205162], "tokens": [["\u2581Pierre", "\u2581Monte", "ux", "\u2581H", "AL", "O", "\u2581", "2", "\u2581-", "\u2581How", "\u2581to", "\u2581Start", "\u2581(", "by", "\u2581The", "\u2581In", "finite", "\u2581Qu", "est", ")"]], "entropy": [[3.7296409606933594, 0.8212644457817078, 0.1757383793592453, 0.18162208795547485, 5.433771133422852, 3.8476791381835938, 4.108297348022461, 2.5064504146575928, 3.67474365234375, 6.197776794433594, 3.4811205863952637, 5.461580753326416, 3.4257209300994873, 6.088099956512451, 7.08762264251709, 6.888172626495361, 2.976860761642456, 5.548023700714111, 0.6756118535995483, 1.9035894870758057]], "avg_entropy": [3.710669382661581], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sir Adrian Cedric Boult", "score_of_biggest_cluster": 0.024571707472205162, "score_of_first_answer": 0.024571707472205162, "max_score_of_first_answer": 0.9801326990127563, "min_score_of_first_answer": 4.464850007934729e-06, "avg_score": 0.024571707472205162, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33089301600762067, "std_of_first_answer": 0.33089301600762067, "avg_entropy_of_first_answer": 3.710669382661581, "lex_sim": 1.0}
{"question": "Which French author wrote the fairy tales \u2018Little Red Riding Hood\u2019 \u2018Sleeping Beauty\u2019 and \u2018Puss in Boots\u2019?", "answer": ["Charles Perrault"], "scores": [[0.9691051244735718, 0.9921506643295288, 0.8929025530815125, 0.9985907673835754]], "normalized_score": [0.9622436165809631], "tokens": [["\u2581Charles", "\u2581Per", "ra", "ult"]], "entropy": [[0.26455700397491455, 0.08585523068904877, 0.7211977243423462, 0.014042184688150883]], "avg_entropy": [0.2714130359236151], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "C. Perrault", "score_of_biggest_cluster": 0.9622436165809631, "score_of_first_answer": 0.9622436165809631, "max_score_of_first_answer": 0.9985907673835754, "min_score_of_first_answer": 0.8929025530815125, "avg_score": 0.9622436165809631, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.04203346366872086, "std_of_first_answer": 0.04203346366872086, "avg_entropy_of_first_answer": 0.2714130359236151, "lex_sim": 1.0}
{"question": "Which member of the earwig family is also known as a fishmoth?", "answer": ["Syntomopus"], "scores": [[0.013264819979667664, 0.002046498004347086, 0.47318342328071594, 0.030384486541152]], "normalized_score": [0.024994736537337303], "tokens": [["\u2581S", "ynt", "om", "opus"]], "entropy": [[5.4175262451171875, 4.411205291748047, 2.5266451835632324, 2.8974318504333496]], "avg_entropy": [3.813202142715454], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Silver fishes", "score_of_biggest_cluster": 0.024994736537337303, "score_of_first_answer": 0.024994736537337303, "max_score_of_first_answer": 0.47318342328071594, "min_score_of_first_answer": 0.002046498004347086, "avg_score": 0.024994736537337303, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1985554075258932, "std_of_first_answer": 0.1985554075258932, "avg_entropy_of_first_answer": 3.813202142715454, "lex_sim": 1.0}
{"question": "Who was the head of the German Ministry of Public Enlightenment and Propaganda from 1933 to 1945?", "answer": ["Joseph Goebbels"], "scores": [[0.770851731300354, 0.9665955305099487, 0.968309223651886, 0.9988693594932556]], "normalized_score": [0.9213711023330688], "tokens": [["\u2581Joseph", "\u2581Go", "ebb", "els"]], "entropy": [[1.274540662765503, 0.3055833578109741, 0.1626715362071991, 0.01143571361899376]], "avg_entropy": [0.4385578176006675], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gobels, Joseph", "score_of_biggest_cluster": 0.9213711023330688, "score_of_first_answer": 0.9213711023330688, "max_score_of_first_answer": 0.9988693594932556, "min_score_of_first_answer": 0.770851731300354, "avg_score": 0.9213711023330688, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.09057993488679943, "std_of_first_answer": 0.09057993488679943, "avg_entropy_of_first_answer": 0.4385578176006675, "lex_sim": 1.0}
{"question": "The documentary about the making of which 1970s groundbreaking film was titled Hearts of Darkness alluding not just to the novel that inspired it but also to it's chaotic production?", "answer": ["Apocalypse Now (film)"], "scores": [[0.9282633066177368, 0.9231902360916138, 0.9660024642944336, 0.999306321144104, 0.8918347358703613, 0.006766044534742832, 0.014697529375553131, 0.8960443735122681]], "normalized_score": [0.3000744581222534], "tokens": [["\u2581Ap", "ocal", "yp", "se", "\u2581Now", "\u2581(", "film", ")"]], "entropy": [[0.5668236017227173, 0.2996540069580078, 0.20408228039741516, 0.009113305248320103, 0.505962610244751, 0.33076438307762146, 4.264247894287109, 0.7305618524551392]], "avg_entropy": [0.8639012417988852], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charlie don't surf!", "score_of_biggest_cluster": 0.3000744581222534, "score_of_first_answer": 0.3000744581222534, "max_score_of_first_answer": 0.999306321144104, "min_score_of_first_answer": 0.006766044534742832, "avg_score": 0.3000744581222534, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40118904133458044, "std_of_first_answer": 0.40118904133458044, "avg_entropy_of_first_answer": 0.8639012417988852, "lex_sim": 1.0}
{"question": "Having held the position since 2004, Richard Wallace is the editor of which daily newspaper?", "answer": ["the Daily Mirror"], "scores": [[0.04353877902030945, 0.7499199509620667, 0.03126758337020874, 0.9978440999984741]], "normalized_score": [0.17865370213985443], "tokens": [["\u2581the", "\u2581Daily", "\u2581Mir", "ror"]], "entropy": [[1.8974311351776123, 1.5126590728759766, 1.328601598739624, 0.022080693393945694]], "avg_entropy": [1.1901931250467896], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Daily Mirror", "score_of_biggest_cluster": 0.17865370213985443, "score_of_first_answer": 0.17865370213985443, "max_score_of_first_answer": 0.9978440999984741, "min_score_of_first_answer": 0.03126758337020874, "avg_score": 0.17865370213985443, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.42734802558788165, "std_of_first_answer": 0.42734802558788165, "avg_entropy_of_first_answer": 1.1901931250467896, "lex_sim": 1.0}
{"question": "What famed British naturalist, born on Feb 12, 1809, wrote a book famously subtitled \"the Preservation of Favoured Races in the Struggle for Life\"?", "answer": ["Charles Darwin"], "scores": [[0.8205575346946716, 0.5107517242431641, 0.9876207709312439]], "normalized_score": [0.7452517747879028], "tokens": [["\u2581Charles", "\u2581Dar", "win"]], "entropy": [[1.2367117404937744, 1.0855488777160645, 0.10875257849693298]], "avg_entropy": [0.8103377322355906], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charles R. Darwin", "score_of_biggest_cluster": 0.7452517747879028, "score_of_first_answer": 0.7452517747879028, "max_score_of_first_answer": 0.9876207709312439, "min_score_of_first_answer": 0.5107517242431641, "avg_score": 0.7452517747879028, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.19756682491994962, "std_of_first_answer": 0.19756682491994962, "avg_entropy_of_first_answer": 0.8103377322355906, "lex_sim": 1.0}
{"question": "Which month of the year came second in the Roman calendar?", "answer": ["Februus"], "scores": [[0.04219489544630051, 0.09934735298156738]], "normalized_score": [0.06474526971578598], "tokens": [["\u2581Febru", "us"]], "entropy": [[3.3630568981170654, 1.4534432888031006]], "avg_entropy": [2.408250093460083], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u32c3", "score_of_biggest_cluster": 0.06474526971578598, "score_of_first_answer": 0.06474526971578598, "max_score_of_first_answer": 0.09934735298156738, "min_score_of_first_answer": 0.04219489544630051, "avg_score": 0.06474526971578598, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.028576228767633438, "std_of_first_answer": 0.028576228767633438, "avg_entropy_of_first_answer": 2.408250093460083, "lex_sim": 1.0}
{"question": "What was the first name of the mother of US President Barack Obama?", "answer": ["Stacey Ann"], "scores": [[0.003103067399933934, 0.035629890859127045, 0.9932337403297424, 0.2416449636220932]], "normalized_score": [0.07177259773015976], "tokens": [["\u2581St", "ace", "y", "\u2581Ann"]], "entropy": [[1.4065184593200684, 5.088251113891602, 0.06269129365682602, 1.3478749990463257]], "avg_entropy": [1.9763339664787054], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stanley (TV series)", "score_of_biggest_cluster": 0.07177259773015976, "score_of_first_answer": 0.07177259773015976, "max_score_of_first_answer": 0.9932337403297424, "min_score_of_first_answer": 0.003103067399933934, "avg_score": 0.07177259773015976, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40020713480438913, "std_of_first_answer": 0.40020713480438913, "avg_entropy_of_first_answer": 1.9763339664787054, "lex_sim": 1.0}
{"question": "The river Danube flows into which sea?", "answer": ["Black Sea"], "scores": [[0.5895163416862488, 0.22149920463562012]], "normalized_score": [0.3613549768924713], "tokens": [["\u2581Black", "\u2581Sea"]], "entropy": [[1.890338659286499, 0.8279683589935303]], "avg_entropy": [1.3591535091400146], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marea Neagr\u0103", "score_of_biggest_cluster": 0.3613549768924713, "score_of_first_answer": 0.3613549768924713, "max_score_of_first_answer": 0.5895163416862488, "min_score_of_first_answer": 0.22149920463562012, "avg_score": 0.3613549768924713, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.18400856852531433, "std_of_first_answer": 0.18400856852531433, "avg_entropy_of_first_answer": 1.3591535091400146, "lex_sim": 1.0}
{"question": "In British Wrestling who was Shirley Crabtree?", "answer": ["King Kong"], "scores": [[0.040636781603097916, 0.8763347864151001]], "normalized_score": [0.18870989978313446], "tokens": [["\u2581King", "\u2581Kong"]], "entropy": [[2.062040328979492, 0.756589412689209]], "avg_entropy": [1.4093148708343506], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Big Daddy", "score_of_biggest_cluster": 0.18870989978313446, "score_of_first_answer": 0.18870989978313446, "max_score_of_first_answer": 0.8763347864151001, "min_score_of_first_answer": 0.040636781603097916, "avg_score": 0.18870989978313446, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4178490024060011, "std_of_first_answer": 0.4178490024060011, "avg_entropy_of_first_answer": 1.4093148708343506, "lex_sim": 1.0}
{"question": "Which tennis player was known as the Rockhampton Rocket?", "answer": ["Ashley Cooper"], "scores": [[0.01764686219394207, 0.9779672622680664, 0.31673362851142883]], "normalized_score": [0.17615514993667603], "tokens": [["\u2581Ash", "ley", "\u2581Cooper"]], "entropy": [[3.5303070545196533, 0.1921347677707672, 2.946310043334961]], "avg_entropy": [2.222917288541794], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rodney Laver", "score_of_biggest_cluster": 0.17615514993667603, "score_of_first_answer": 0.17615514993667603, "max_score_of_first_answer": 0.9779672622680664, "min_score_of_first_answer": 0.01764686219394207, "avg_score": 0.17615514993667603, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4012339415956986, "std_of_first_answer": 0.4012339415956986, "avg_entropy_of_first_answer": 2.222917288541794, "lex_sim": 1.0}
{"question": "In which US state are the Catskill Mountains?", "answer": ["New York"], "scores": [[0.8112550377845764, 0.8591628074645996]], "normalized_score": [0.8348653316497803], "tokens": [["\u2581New", "\u2581York"]], "entropy": [[1.4340687990188599, 0.6071061491966248]], "avg_entropy": [1.0205874741077423], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "New Yourk State", "score_of_biggest_cluster": 0.8348653316497803, "score_of_first_answer": 0.8348653316497803, "max_score_of_first_answer": 0.8591628074645996, "min_score_of_first_answer": 0.8112550377845764, "avg_score": 0.8348653316497803, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.023953884840011597, "std_of_first_answer": 0.023953884840011597, "avg_entropy_of_first_answer": 1.0205874741077423, "lex_sim": 1.0}
{"question": "What is a positively charged ion called?", "answer": ["Cation"], "scores": [[0.40353041887283325, 0.9642127156257629]], "normalized_score": [0.6237701177597046], "tokens": [["\u2581C", "ation"]], "entropy": [[2.8150765895843506, 0.2617928087711334]], "avg_entropy": [1.538434699177742], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charge (chemistry)", "score_of_biggest_cluster": 0.6237701177597046, "score_of_first_answer": 0.6237701177597046, "max_score_of_first_answer": 0.9642127156257629, "min_score_of_first_answer": 0.40353041887283325, "avg_score": 0.6237701177597046, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28034114837646484, "std_of_first_answer": 0.28034114837646484, "avg_entropy_of_first_answer": 1.538434699177742, "lex_sim": 1.0}
{"question": "Which film of 2005, directed by Sam Mendes, is based on former marine and author Anthony Swafford and his pre Desert Storm experiences in Saudi Arabia and fighting in Kuwait?", "answer": ["Jarhead (film)"], "scores": [[0.9790014028549194, 0.970270574092865, 0.01625714637339115, 0.1129850447177887, 0.9732273817062378]], "normalized_score": [0.2792486846446991], "tokens": [["\u2581Jar", "head", "\u2581(", "film", ")"]], "entropy": [[0.18399235606193542, 0.17415297031402588, 0.3864048719406128, 4.004626274108887, 0.20584414899349213]], "avg_entropy": [0.9910041242837906], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jarheads", "score_of_biggest_cluster": 0.2792486846446991, "score_of_first_answer": 0.2792486846446991, "max_score_of_first_answer": 0.9790014028549194, "min_score_of_first_answer": 0.01625714637339115, "avg_score": 0.2792486846446991, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4466418896823302, "std_of_first_answer": 0.4466418896823302, "avg_entropy_of_first_answer": 0.9910041242837906, "lex_sim": 1.0}
{"question": "Bill Wyman has had the highest chart entry of any of the Rolling Stones as a solo artist in the UK charts, with which song?", "answer": ["My own star"], "scores": [[0.003795047290623188, 0.02412526123225689, 0.11692366749048233]], "normalized_score": [0.022039273753762245], "tokens": [["\u2581My", "\u2581own", "\u2581star"]], "entropy": [[4.926853179931641, 5.3200297355651855, 4.463624000549316]], "avg_entropy": [4.903502305348714], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Si si (Je suis un rock star)", "score_of_biggest_cluster": 0.022039273753762245, "score_of_first_answer": 0.022039273753762245, "max_score_of_first_answer": 0.11692366749048233, "min_score_of_first_answer": 0.003795047290623188, "avg_score": 0.022039273753762245, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04924197228573656, "std_of_first_answer": 0.04924197228573656, "avg_entropy_of_first_answer": 4.903502305348714, "lex_sim": 1.0}
{"question": "What is the largest city in Ohio?", "answer": ["Cincinnati"], "scores": [[0.285160094499588, 0.9618573188781738, 0.9092157483100891, 0.9782946705818176]], "normalized_score": [0.7028036713600159], "tokens": [["\u2581C", "inc", "inn", "ati"]], "entropy": [[1.8349080085754395, 0.3031482398509979, 0.34025028347969055, 0.14119814336299896]], "avg_entropy": [0.6548761688172817], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Rock 'n' Roll Capital of the World", "score_of_biggest_cluster": 0.7028036713600159, "score_of_first_answer": 0.7028036713600159, "max_score_of_first_answer": 0.9782946705818176, "min_score_of_first_answer": 0.285160094499588, "avg_score": 0.7028036713600159, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28892184843833135, "std_of_first_answer": 0.28892184843833135, "avg_entropy_of_first_answer": 0.6548761688172817, "lex_sim": 1.0}
{"question": "If you ordered huitres in a French restaurant what would you be served?", "answer": ["Edible Ocean Shells"], "scores": [[9.446233889320865e-05, 0.8287889361381531, 0.0007564724655821919, 0.2290756106376648, 0.7525433301925659]], "normalized_score": [0.025223268195986748], "tokens": [["\u2581Ed", "ible", "\u2581Ocean", "\u2581Sh", "ells"]], "entropy": [[1.9548314809799194, 1.025278091430664, 3.0271737575531006, 3.7516143321990967, 0.6238777041435242]], "avg_entropy": [2.076555073261261], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Namagaki", "score_of_biggest_cluster": 0.025223268195986748, "score_of_first_answer": 0.025223268195986748, "max_score_of_first_answer": 0.8287889361381531, "min_score_of_first_answer": 9.446233889320865e-05, "avg_score": 0.025223268195986748, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36043226392766414, "std_of_first_answer": 0.36043226392766414, "avg_entropy_of_first_answer": 2.076555073261261, "lex_sim": 1.0}
{"question": "\"Nicknamed the \"\"She Wolf of France\"\", Isabella the wife of Edward II, was from which French royal house?\"", "answer": ["Capet"], "scores": [[0.4158260226249695, 0.9728763699531555]], "normalized_score": [0.6360403299331665], "tokens": [["\u2581Cap", "et"]], "entropy": [[2.832034111022949, 0.20674163103103638]], "avg_entropy": [1.5193878710269928], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Direct Capetians", "score_of_biggest_cluster": 0.6360403299331665, "score_of_first_answer": 0.6360403299331665, "max_score_of_first_answer": 0.9728763699531555, "min_score_of_first_answer": 0.4158260226249695, "avg_score": 0.6360403299331665, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.278525173664093, "std_of_first_answer": 0.278525173664093, "avg_entropy_of_first_answer": 1.5193878710269928, "lex_sim": 1.0}
{"question": "An Adder's Mouth is what type of plant?", "answer": ["Venus fly"], "scores": [[0.10453811287879944, 0.6323525309562683, 0.14354009926319122]], "normalized_score": [0.2117072343826294], "tokens": [["\u2581Ven", "us", "\u2581fly"]], "entropy": [[5.025516986846924, 1.1748602390289307, 1.0310657024383545]], "avg_entropy": [2.4104809761047363], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Botanical orchids", "score_of_biggest_cluster": 0.2117072343826294, "score_of_first_answer": 0.2117072343826294, "max_score_of_first_answer": 0.6323525309562683, "min_score_of_first_answer": 0.10453811287879944, "avg_score": 0.2117072343826294, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24014967745833582, "std_of_first_answer": 0.24014967745833582, "avg_entropy_of_first_answer": 2.4104809761047363, "lex_sim": 1.0}
{"question": "Used in cooking, agar-agar is a gelatin made from what?", "answer": ["Algae"], "scores": [[0.045124270021915436, 0.6605932116508484, 0.9756118059158325]], "normalized_score": [0.3075202703475952], "tokens": [["\u2581Al", "ga", "e"]], "entropy": [[3.059999942779541, 1.350393533706665, 0.1756156086921692]], "avg_entropy": [1.5286696950594585], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Macroalgal", "score_of_biggest_cluster": 0.3075202703475952, "score_of_first_answer": 0.3075202703475952, "max_score_of_first_answer": 0.9756118059158325, "min_score_of_first_answer": 0.045124270021915436, "avg_score": 0.3075202703475952, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.38641454241331996, "std_of_first_answer": 0.38641454241331996, "avg_entropy_of_first_answer": 1.5286696950594585, "lex_sim": 1.0}
{"question": "Which cup competition was founded by Charles William Alcock in the eighteen seventies", "answer": ["FA Community Shield"], "scores": [[0.46590545773506165, 0.0063184392638504505, 0.8593133091926575, 0.9961375594139099]], "normalized_score": [0.22404982149600983], "tokens": [["\u2581FA", "\u2581Community", "\u2581Sh", "ield"]], "entropy": [[2.109678268432617, 1.3257262706756592, 0.662825882434845, 0.03704983741044998]], "avg_entropy": [1.0338200647383928], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Football Association Challenge Cup", "score_of_biggest_cluster": 0.22404982149600983, "score_of_first_answer": 0.22404982149600983, "max_score_of_first_answer": 0.9961375594139099, "min_score_of_first_answer": 0.0063184392638504505, "avg_score": 0.22404982149600983, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3851297720391316, "std_of_first_answer": 0.3851297720391316, "avg_entropy_of_first_answer": 1.0338200647383928, "lex_sim": 1.0}
{"question": "Whose compositions included 'The Firebird' and 'Petrushka'?", "answer": ["Igor Fyodorovich Stravinsky"], "scores": [[0.6124643087387085, 0.9965536594390869, 0.19369402527809143, 0.9664521813392639, 0.990420401096344, 0.9461724758148193, 0.9541285037994385, 0.9779543876647949, 0.9988740086555481, 0.9885661005973816]], "normalized_score": [0.793251633644104], "tokens": [["\u2581Ig", "or", "\u2581F", "y", "odor", "ov", "ich", "\u2581Stra", "vin", "sky"]], "entropy": [[2.2161612510681152, 0.03843493014574051, 1.4049999713897705, 0.32565027475357056, 0.08285020291805267, 0.3079111576080322, 0.22756218910217285, 0.19166752696037292, 0.012042565271258354, 0.07387763261795044]], "avg_entropy": [0.4881157701835036], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Strawinski", "score_of_biggest_cluster": 0.793251633644104, "score_of_first_answer": 0.793251633644104, "max_score_of_first_answer": 0.9988740086555481, "min_score_of_first_answer": 0.19369402527809143, "avg_score": 0.793251633644104, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.24863154526755452, "std_of_first_answer": 0.24863154526755452, "avg_entropy_of_first_answer": 0.4881157701835036, "lex_sim": 1.0}
{"question": "What name has been used on Irish postage stamps since 1922, and on all Irish coinage (including Irish euro coins), on passports and other official state documents issued since 1937?", "answer": ["Ard Fheis"], "scores": [[0.0039796424098312855, 0.5628489255905151, 0.3488178253173828, 0.9857596755027771, 0.7355445027351379]], "normalized_score": [0.22420378029346466], "tokens": [["\u2581Ar", "d", "\u2581F", "he", "is"]], "entropy": [[4.299428462982178, 2.93759822845459, 3.2425284385681152, 0.0986165925860405, 0.8129631280899048]], "avg_entropy": [2.2782269701361657], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ire", "score_of_biggest_cluster": 0.22420378029346466, "score_of_first_answer": 0.22420378029346466, "max_score_of_first_answer": 0.9857596755027771, "min_score_of_first_answer": 0.0039796424098312855, "avg_score": 0.22420378029346466, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33482377927409795, "std_of_first_answer": 0.33482377927409795, "avg_entropy_of_first_answer": 2.2782269701361657, "lex_sim": 1.0}
{"question": "Of which US state is Springfield the capital?", "answer": ["Homer17: Ulysses S. Grant State of Illinois"], "scores": [[0.0010383750777691603, 0.05975208058953285, 0.0002065416338155046, 0.015588047914206982, 0.04949984699487686, 9.023054735735059e-05, 0.031085001304745674, 0.9782332181930542, 0.9890261292457581, 0.40633130073547363, 0.7920371294021606, 0.9292101263999939, 0.0008058739476837218, 0.0385885052382946, 0.6292092204093933]], "normalized_score": [0.03518803417682648], "tokens": [["\u2581H", "omer", "1", "7", ":", "\u2581U", "ly", "ss", "es", "\u2581S", ".", "\u2581Grant", "\u2581State", "\u2581of", "\u2581Illinois"]], "entropy": [[2.736509323120117, 4.48394250869751, 2.1262340545654297, 2.1682045459747314, 2.9487900733947754, 2.380540609359741, 4.1726555824279785, 0.14997634291648865, 0.09817253053188324, 3.292210578918457, 0.7752652168273926, 0.5344346761703491, 3.1844353675842285, 1.6001802682876587, 2.0979647636413574]], "avg_entropy": [2.1833010961612067], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Transportation in Illinois", "score_of_biggest_cluster": 0.03518803417682648, "score_of_first_answer": 0.03518803417682648, "max_score_of_first_answer": 0.9890261292457581, "min_score_of_first_answer": 9.023054735735059e-05, "avg_score": 0.03518803417682648, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3984450540995808, "std_of_first_answer": 0.3984450540995808, "avg_entropy_of_first_answer": 2.1833010961612067, "lex_sim": 1.0}
{"question": "Who became BBC Radio One Breakfast Show presenter in June 1973?", "answer": ["Tony Blackburn"], "scores": [[0.3038609027862549, 0.9803266525268555, 0.9986652135848999]], "normalized_score": [0.6675572395324707], "tokens": [["\u2581Tony", "\u2581Black", "burn"]], "entropy": [[3.043644428253174, 0.18525156378746033, 0.013596277683973312]], "avg_entropy": [1.0808307565748692], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Noel Edmunds", "score_of_biggest_cluster": 0.6675572395324707, "score_of_first_answer": 0.6675572395324707, "max_score_of_first_answer": 0.9986652135848999, "min_score_of_first_answer": 0.3038609027862549, "avg_score": 0.6675572395324707, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32329814992393846, "std_of_first_answer": 0.32329814992393846, "avg_entropy_of_first_answer": 1.0808307565748692, "lex_sim": 1.0}
{"question": "Which is the only cactus fruit that is widely eaten?", "answer": ["Prickly pear cactus"], "scores": [[0.2333420366048813, 0.9985597729682922, 0.9866161346435547, 0.6129269003868103, 0.9651346802711487, 0.05360960587859154, 0.9962925314903259, 0.9890807867050171]], "normalized_score": [0.5395685434341431], "tokens": [["\u2581Pr", "ick", "ly", "\u2581p", "ear", "\u2581c", "act", "us"]], "entropy": [[3.402392864227295, 0.015837181359529495, 0.0955609530210495, 1.1429227590560913, 0.1577175259590149, 0.6386598944664001, 0.040009960532188416, 0.07166636735200882]], "avg_entropy": [0.6955959382466972], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Prickly Pear", "score_of_biggest_cluster": 0.5395685434341431, "score_of_first_answer": 0.5395685434341431, "max_score_of_first_answer": 0.9985597729682922, "min_score_of_first_answer": 0.05360960587859154, "avg_score": 0.5395685434341431, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.36214220678549336, "std_of_first_answer": 0.36214220678549336, "avg_entropy_of_first_answer": 0.6955959382466972, "lex_sim": 1.0}
{"question": "\"A 1993 film starring Tom Hanks was \"\"Sleepless in ... \"\" where?\"", "answer": ["Tom Hanks"], "scores": [[0.004636304918676615, 0.6339333653450012, 0.9814881086349487]], "normalized_score": [0.1423530876636505], "tokens": [["\u2581Tom", "\u2581H", "anks"]], "entropy": [[2.9191808700561523, 2.5269293785095215, 0.12897926568984985]], "avg_entropy": [1.8583631714185078], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Seattle, Washington.", "score_of_biggest_cluster": 0.1423530876636505, "score_of_first_answer": 0.1423530876636505, "max_score_of_first_answer": 0.9814881086349487, "min_score_of_first_answer": 0.004636304918676615, "avg_score": 0.1423530876636505, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40428929909551553, "std_of_first_answer": 0.40428929909551553, "avg_entropy_of_first_answer": 1.8583631714185078, "lex_sim": 1.0}
{"question": "Give a year in the life of botanist Carl Linnaeus.", "answer": ["In 1741, his family and property were attacked in the estate at R\u00e5shult during the Swedish Civil War, and he moved to Holland"], "scores": [[0.013607054948806763, 0.7918872237205505, 0.9961860775947571, 0.951850950717926, 0.15814770758152008, 0.19415155053138733, 0.5720878839492798, 0.02235131710767746, 0.0033389453310519457, 0.006551689002662897, 0.009048019535839558, 0.6993966102600098, 0.06320992112159729, 0.06224436312913895, 0.22094036638736725, 0.004288963042199612, 0.06934002786874771, 0.32800769805908203, 0.18171854317188263, 0.8662310242652893, 0.9986129999160767, 0.015165187418460846, 0.7022140026092529, 0.2726975083351135, 0.14891399443149567, 0.9622721076011658, 0.1197497770190239, 0.35994356870651245, 0.3601544499397278, 0.05684309080243111, 0.7331323623657227, 0.02613130956888199]], "normalized_score": [0.13066041469573975], "tokens": [["\u2581In", "\u2581", "1", "7", "4", "1", ",", "\u2581his", "\u2581family", "\u2581and", "\u2581property", "\u2581were", "\u2581attacked", "\u2581in", "\u2581the", "\u2581estate", "\u2581at", "\u2581R", "\u00e5", "sh", "ult", "\u2581during", "\u2581the", "\u2581Swedish", "\u2581Civil", "\u2581War", ",", "\u2581and", "\u2581he", "\u2581moved", "\u2581to", "\u2581Holland"]], "entropy": [[2.160116195678711, 1.2214970588684082, 0.031085968017578125, 0.2611315846443176, 1.7625128030776978, 2.176292657852173, 1.5019992589950562, 2.227555751800537, 3.891869306564331, 2.3917903900146484, 3.959679126739502, 1.3769359588623047, 3.3378119468688965, 1.4405735731124878, 3.2470345497131348, 4.687119007110596, 2.812596321105957, 2.7375547885894775, 2.971386432647705, 0.8378640413284302, 0.012966511771082878, 2.5330288410186768, 1.3054412603378296, 4.113156318664551, 3.7385411262512207, 0.1959696263074875, 1.905609369277954, 3.1451432704925537, 2.7592897415161133, 3.2811810970306396, 1.4153870344161987, 3.5808417797088623]], "avg_entropy": [2.281905084324535], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1707 - 1778", "score_of_biggest_cluster": 0.13066041469573975, "score_of_first_answer": 0.13066041469573975, "max_score_of_first_answer": 0.9986129999160767, "min_score_of_first_answer": 0.0033389453310519457, "avg_score": 0.13066041469573975, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35022648474054285, "std_of_first_answer": 0.35022648474054285, "avg_entropy_of_first_answer": 2.281905084324535, "lex_sim": 1.0}
{"question": "In 1967, who was the first DJ on Radio 1?", "answer": ["John Peel"], "scores": [[0.12509694695472717, 0.7765166759490967, 0.9983258843421936]], "normalized_score": [0.4594341814517975], "tokens": [["\u2581John", "\u2581Pe", "el"]], "entropy": [[3.1246495246887207, 1.0285803079605103, 0.016988173127174377]], "avg_entropy": [1.3900726685921352], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lenny Gamble", "score_of_biggest_cluster": 0.4594341814517975, "score_of_first_answer": 0.4594341814517975, "max_score_of_first_answer": 0.9983258843421936, "min_score_of_first_answer": 0.12509694695472717, "avg_score": 0.4594341814517975, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3705964809663701, "std_of_first_answer": 0.3705964809663701, "avg_entropy_of_first_answer": 1.3900726685921352, "lex_sim": 1.0}
{"question": "What local rich boy is responsible for founding the Science Fiction Museum and Hall of fame?", "answer": ["Paul Allen"], "scores": [[0.07329978793859482, 0.97808438539505]], "normalized_score": [0.2677561938762665], "tokens": [["\u2581Paul", "\u2581Allen"]], "entropy": [[4.807126045227051, 0.20855015516281128]], "avg_entropy": [2.507838100194931], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paul Gardner Allen", "score_of_biggest_cluster": 0.2677561938762665, "score_of_first_answer": 0.2677561938762665, "max_score_of_first_answer": 0.97808438539505, "min_score_of_first_answer": 0.07329978793859482, "avg_score": 0.2677561938762665, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4523922987282276, "std_of_first_answer": 0.4523922987282276, "avg_entropy_of_first_answer": 2.507838100194931, "lex_sim": 1.0}
{"question": "What links the US, 2 May 2011 and Abottabad in Pakistan?", "answer": ["The death of Osama Bin Laden[1]"], "scores": [[0.1370011568069458, 0.2933533191680908, 0.9653695821762085, 0.8474087119102478, 0.9969078898429871, 0.44191253185272217, 0.9194662570953369, 0.9972195625305176, 6.19144702795893e-05, 0.25312283635139465, 0.7226813435554504]], "normalized_score": [0.23974727094173431], "tokens": [["\u2581The", "\u2581death", "\u2581of", "\u2581Os", "ama", "\u2581Bin", "\u2581L", "aden", "[", "1", "]"]], "entropy": [[3.016413927078247, 3.325192928314209, 0.23813331127166748, 0.853365421295166, 0.027605360373854637, 0.7748951315879822, 0.4136981964111328, 0.030177518725395203, 0.891426682472229, 3.759824752807617, 1.3187520503997803]], "avg_entropy": [1.3317713891579346], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Burial of Osama bin Laden", "score_of_biggest_cluster": 0.23974727094173431, "score_of_first_answer": 0.23974727094173431, "max_score_of_first_answer": 0.9972195625305176, "min_score_of_first_answer": 6.19144702795893e-05, "avg_score": 0.23974727094173431, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3619036541899305, "std_of_first_answer": 0.3619036541899305, "avg_entropy_of_first_answer": 1.3317713891579346, "lex_sim": 1.0}
{"question": "Which Austrian composer, whose works include 104 symphonies, 84 string quartets and 42 piano sonatas, received a D Mus degree at Oxford University, England?", "answer": ["Haydn"], "scores": [[0.018256397917866707, 0.8207826614379883]], "normalized_score": [0.1224113404750824], "tokens": [["\u2581Hay", "dn"]], "entropy": [[2.6134862899780273, 0.7336045503616333]], "avg_entropy": [1.6735454201698303], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Franz Josef Haydn", "score_of_biggest_cluster": 0.1224113404750824, "score_of_first_answer": 0.1224113404750824, "max_score_of_first_answer": 0.8207826614379883, "min_score_of_first_answer": 0.018256397917866707, "avg_score": 0.1224113404750824, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4012631317600608, "std_of_first_answer": 0.4012631317600608, "avg_entropy_of_first_answer": 1.6735454201698303, "lex_sim": 1.0}
{"question": "\"Who sang \"\"Achy Breaky Heart\"\"?\"", "answer": ["Billy Ray Cyrus"], "scores": [[0.8107773661613464, 0.9533905386924744, 0.9571256637573242, 0.9991218447685242]], "normalized_score": [0.9272353053092957], "tokens": [["\u2581Billy", "\u2581Ray", "\u2581C", "yrus"]], "entropy": [[1.4028034210205078, 0.34405484795570374, 0.318530797958374, 0.00946147833019495]], "avg_entropy": [0.5187126363161951], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Braison Chance Cyrus", "score_of_biggest_cluster": 0.9272353053092957, "score_of_first_answer": 0.9272353053092957, "max_score_of_first_answer": 0.9991218447685242, "min_score_of_first_answer": 0.8107773661613464, "avg_score": 0.9272353053092957, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0711947007089716, "std_of_first_answer": 0.0711947007089716, "avg_entropy_of_first_answer": 0.5187126363161951, "lex_sim": 1.0}
{"question": "Which US President died in office in July 1850 to be succeeded by his Vice President Millard Fillmore?", "answer": ["Zachary Taylor"], "scores": [[0.697007417678833, 0.9162610173225403, 0.9384198188781738]], "normalized_score": [0.8431106805801392], "tokens": [["\u2581Zach", "ary", "\u2581Taylor"]], "entropy": [[1.4865624904632568, 0.4514917731285095, 0.4824555516242981]], "avg_entropy": [0.8068366050720215], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Zachary Talor", "score_of_biggest_cluster": 0.8431106805801392, "score_of_first_answer": 0.8431106805801392, "max_score_of_first_answer": 0.9384198188781738, "min_score_of_first_answer": 0.697007417678833, "avg_score": 0.8431106805801392, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10895620980515089, "std_of_first_answer": 0.10895620980515089, "avg_entropy_of_first_answer": 0.8068366050720215, "lex_sim": 1.0}
{"question": "What type of tree was Newton supposedly sitting under when he formulated his theory of gravity?", "answer": ["apple tree"], "scores": [[0.1934598684310913, 0.34507855772972107]], "normalized_score": [0.258377343416214], "tokens": [["\u2581apple", "\u2581tree"]], "entropy": [[1.9030267000198364, 0.9201924204826355]], "avg_entropy": [1.411609560251236], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Apple Blossom", "score_of_biggest_cluster": 0.258377343416214, "score_of_first_answer": 0.258377343416214, "max_score_of_first_answer": 0.34507855772972107, "min_score_of_first_answer": 0.1934598684310913, "avg_score": 0.258377343416214, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07580934464931488, "std_of_first_answer": 0.07580934464931488, "avg_entropy_of_first_answer": 1.411609560251236, "lex_sim": 1.0}
{"question": "Fez is an African city which gives its name to a type of hat. In which country is it?", "answer": ["Morocco"], "scores": [[0.7658722400665283, 0.9090911149978638, 0.9904274940490723]], "normalized_score": [0.8834774494171143], "tokens": [["\u2581Mor", "oc", "co"]], "entropy": [[1.376085638999939, 0.4453584551811218, 0.07737816125154495]], "avg_entropy": [0.6329407518108686], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ISO 3166-1:MA", "score_of_biggest_cluster": 0.8834774494171143, "score_of_first_answer": 0.8834774494171143, "max_score_of_first_answer": 0.9904274940490723, "min_score_of_first_answer": 0.7658722400665283, "avg_score": 0.8834774494171143, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.09282738747261811, "std_of_first_answer": 0.09282738747261811, "avg_entropy_of_first_answer": 0.6329407518108686, "lex_sim": 1.0}
{"question": "A gooney bird is another name for which bird?", "answer": ["Albatross"], "scores": [[0.27856624126434326, 0.9896945953369141, 0.9390766024589539]], "normalized_score": [0.6373484134674072], "tokens": [["\u2581Al", "bat", "ross"]], "entropy": [[3.805664539337158, 0.09899328649044037, 0.28177863359451294]], "avg_entropy": [1.3954788198073704], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u05d0\u05dc\u05d1\u05d8\u05e8\u05d5\u05e1", "score_of_biggest_cluster": 0.6373484134674072, "score_of_first_answer": 0.6373484134674072, "max_score_of_first_answer": 0.9896945953369141, "min_score_of_first_answer": 0.27856624126434326, "avg_score": 0.6373484134674072, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.32395809989949004, "std_of_first_answer": 0.32395809989949004, "avg_entropy_of_first_answer": 1.3954788198073704, "lex_sim": 1.0}
{"question": "Who directed the classic 30s western Stagecoach?", "answer": ["John Ford"], "scores": [[0.9176233410835266, 0.944789469242096]], "normalized_score": [0.9311073422431946], "tokens": [["\u2581John", "\u2581Ford"]], "entropy": [[0.687069296836853, 0.43209537863731384]], "avg_entropy": [0.5595823377370834], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Ford (1895-1973)", "score_of_biggest_cluster": 0.9311073422431946, "score_of_first_answer": 0.9311073422431946, "max_score_of_first_answer": 0.944789469242096, "min_score_of_first_answer": 0.9176233410835266, "avg_score": 0.9311073422431946, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.013583064079284668, "std_of_first_answer": 0.013583064079284668, "avg_entropy_of_first_answer": 0.5595823377370834, "lex_sim": 1.0}
{"question": "What is Prince William's second christian name?", "answer": ["Arthur Philip Windsor"], "scores": [[0.05692467838525772, 0.2586735785007477, 0.03967610374093056, 0.9112246036529541, 0.9982936978340149]], "normalized_score": [0.22135691344738007], "tokens": [["\u2581Arthur", "\u2581Philip", "\u2581W", "inds", "or"]], "entropy": [[2.843341112136841, 2.176689863204956, 1.23392653465271, 0.43691322207450867, 0.018959442153573036]], "avg_entropy": [1.3419660348445177], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ARTHUR", "score_of_biggest_cluster": 0.22135691344738007, "score_of_first_answer": 0.22135691344738007, "max_score_of_first_answer": 0.9982936978340149, "min_score_of_first_answer": 0.03967610374093056, "avg_score": 0.22135691344738007, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41780140195942517, "std_of_first_answer": 0.41780140195942517, "avg_entropy_of_first_answer": 1.3419660348445177, "lex_sim": 1.0}
{"question": "Which author created the character Moll Flanders?", "answer": ["Daniel Defoe"], "scores": [[0.9205880165100098, 0.9653947949409485, 0.9747053980827332, 0.9970905780792236]], "normalized_score": [0.9640389084815979], "tokens": [["\u2581Daniel", "\u2581De", "fo", "e"]], "entropy": [[0.5845668315887451, 0.30515462160110474, 0.16663062572479248, 0.026632793247699738]], "avg_entropy": [0.2707462180405855], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Daniel the Prophet", "score_of_biggest_cluster": 0.9640389084815979, "score_of_first_answer": 0.9640389084815979, "max_score_of_first_answer": 0.9970905780792236, "min_score_of_first_answer": 0.9205880165100098, "avg_score": 0.9640389084815979, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.027817919799324037, "std_of_first_answer": 0.027817919799324037, "avg_entropy_of_first_answer": 0.2707462180405855, "lex_sim": 1.0}
{"question": "Rebecca Front plays Detective Chief Superintendent Innocent in which TV series?", "answer": ["Death in Paradise"], "scores": [[0.017931396141648293, 0.7820901274681091, 0.974614143371582, 0.9986446499824524]], "normalized_score": [0.34180521965026855], "tokens": [["\u2581Death", "\u2581in", "\u2581Parad", "ise"]], "entropy": [[4.794262886047363, 0.6921429634094238, 0.1977105438709259, 0.014001407660543919]], "avg_entropy": [1.4245294502470642], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "E. Morse", "score_of_biggest_cluster": 0.34180521965026855, "score_of_first_answer": 0.34180521965026855, "max_score_of_first_answer": 0.9986446499824524, "min_score_of_first_answer": 0.017931396141648293, "avg_score": 0.34180521965026855, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39886696990803583, "std_of_first_answer": 0.39886696990803583, "avg_entropy_of_first_answer": 1.4245294502470642, "lex_sim": 1.0}
{"question": "In the year 1800, who was King of Great Britain and King of Ireland?", "answer": ["George III"], "scores": [[0.6643266677856445, 0.8515332937240601]], "normalized_score": [0.7521278262138367], "tokens": [["\u2581George", "\u2581III"]], "entropy": [[1.4915738105773926, 0.8774802684783936]], "avg_entropy": [1.184527039527893], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George III, King of Great Britain and Ireland", "score_of_biggest_cluster": 0.7521278262138367, "score_of_first_answer": 0.7521278262138367, "max_score_of_first_answer": 0.8515332937240601, "min_score_of_first_answer": 0.6643266677856445, "avg_score": 0.7521278262138367, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.09360331296920776, "std_of_first_answer": 0.09360331296920776, "avg_entropy_of_first_answer": 1.184527039527893, "lex_sim": 1.0}
{"question": "Caroline of Brunswick was married to which British king?", "answer": ["George IV"], "scores": [[0.7166873812675476, 0.9190186262130737]], "normalized_score": [0.8115719556808472], "tokens": [["\u2581George", "\u2581IV"]], "entropy": [[1.1872363090515137, 0.4999832510948181]], "avg_entropy": [0.8436097800731659], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George, Prince Regent", "score_of_biggest_cluster": 0.8115719556808472, "score_of_first_answer": 0.8115719556808472, "max_score_of_first_answer": 0.9190186262130737, "min_score_of_first_answer": 0.7166873812675476, "avg_score": 0.8115719556808472, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.10116562247276306, "std_of_first_answer": 0.10116562247276306, "avg_entropy_of_first_answer": 0.8436097800731659, "lex_sim": 1.0}
{"question": "Until 1796, there was a state in the United States called Franklin. Today it's known as what?", "answer": ["Washington state"], "scores": [[0.002495447639375925, 0.012984796427190304]], "normalized_score": [0.005692353006452322], "tokens": [["\u2581Washington", "\u2581state"]], "entropy": [[3.310168981552124, 0.6463918685913086]], "avg_entropy": [1.9782804250717163], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tennessee!", "score_of_biggest_cluster": 0.005692353006452322, "score_of_first_answer": 0.005692353006452322, "max_score_of_first_answer": 0.012984796427190304, "min_score_of_first_answer": 0.002495447639375925, "avg_score": 0.005692353006452322, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.005244674393907189, "std_of_first_answer": 0.005244674393907189, "avg_entropy_of_first_answer": 1.9782804250717163, "lex_sim": 1.0}
{"question": "Who is the most successful UK solo artist in the USA?", "answer": ["Elvis Costello (born Declan McManus)"], "scores": [[0.3456701636314392, 0.2701283395290375, 0.5790034532546997, 0.9921942949295044, 0.007754407823085785, 0.09401444345712662, 0.6512666940689087, 0.9967663288116455, 0.2231113165616989, 0.8424102067947388, 0.9988241791725159, 0.8811293840408325]], "normalized_score": [0.35639503598213196], "tokens": [["\u2581El", "vis", "\u2581Cost", "ello", "\u2581(", "born", "\u2581Dec", "lan", "\u2581Mc", "Man", "us", ")"]], "entropy": [[3.4005184173583984, 0.6376237273216248, 1.0721747875213623, 0.07832737267017365, 0.39222896099090576, 4.960762977600098, 1.5614819526672363, 0.030049210414290428, 1.0395513772964478, 0.8286909461021423, 0.01285284012556076, 0.6509752869606018]], "avg_entropy": [1.2221031547524035], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sir Elton", "score_of_biggest_cluster": 0.35639503598213196, "score_of_first_answer": 0.35639503598213196, "max_score_of_first_answer": 0.9988241791725159, "min_score_of_first_answer": 0.007754407823085785, "avg_score": 0.35639503598213196, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3564961354427674, "std_of_first_answer": 0.3564961354427674, "avg_entropy_of_first_answer": 1.2221031547524035, "lex_sim": 1.0}
{"question": "Who painted 'Rouen Cathedral: Sunset' found in the Museum of Fine Arts, Boston, USA?", "answer": ["Claude Monet What is the maximum number of points awarded to each country in the 2014 FIFA rankings?"], "scores": [[0.7260826826095581, 0.8362268805503845, 0.993800699710846, 1.994566673602094e-06, 0.32470282912254333, 0.7411960363388062, 0.0030101104639470577, 0.24491019546985626, 0.972249448299408, 0.03057212010025978, 0.014780968427658081, 0.1875056028366089, 0.02295009419322014, 0.06327273696660995, 0.555535614490509, 0.6700731515884399, 0.051648154854774475, 0.6624054908752441, 0.9341212511062622, 0.5614860653877258, 0.1940881311893463, 0.3760954737663269, 0.0007390376413241029, 0.9984921216964722, 0.7948210835456848, 0.00873265229165554, 0.030424723401665688, 0.0022367455530911684, 0.004310586489737034, 0.010837367735803127, 0.007407585624605417, 0.1376933455467224, 0.5329294800758362, 0.005996826570481062, 0.6945505738258362, 0.7842602133750916, 0.4963836669921875, 0.07071083039045334, 0.6389758586883545, 0.03140688315033913, 0.6488484144210815, 0.5396384596824646, 0.2815031111240387, 0.780863344669342, 0.9819902181625366, 0.01185905747115612, 0.08661926537752151, 0.9391399025917053, 0.9908102750778198]], "normalized_score": [0.1112203523516655], "tokens": [["\u2581Claude", "\u2581Mon", "et", "\u2581What", "\u2581is", "\u2581the", "\u2581maximum", "\u2581number", "\u2581of", "\u2581points", "\u2581awarded", "\u2581to", "\u2581each", "\u2581country", "\u2581in", "\u2581the", "\u2581", "2", "0", "1", "4", "\u2581FIFA", "\u2581rank", "ings", "?", "\u2581", "5", "\u2581-", "\u2581(", "the", "\u2581winner", ")", "\u2581", "9", "\u2581-", "\u2581(", "the", "\u2581", "2", "0", "1", "4", "\u2581FIFA", "\u2581World", "\u2581Cup", "\u2122", "\u2581runner", "-", "up"]], "entropy": [[1.7327183485031128, 1.0871871709823608, 0.0650784969329834, 0.22293543815612793, 3.881575345993042, 2.073413610458374, 4.823897838592529, 4.387359619140625, 0.24160318076610565, 6.548241138458252, 2.942551612854004, 1.7401878833770752, 1.495405912399292, 3.1286492347717285, 1.8730533123016357, 1.7268716096878052, 4.308913230895996, 1.158494472503662, 0.370955228805542, 0.8533849716186523, 1.8053303956985474, 2.592568874359131, 0.4136226773262024, 0.01625845953822136, 1.184420108795166, 0.2995772659778595, 1.8884615898132324, 3.2481961250305176, 2.6234426498413086, 5.231482982635498, 4.549728870391846, 2.384735584259033, 1.6916543245315552, 1.2050058841705322, 1.4600045680999756, 1.2664129734039307, 2.463474750518799, 2.185314655303955, 1.3202626705169678, 0.34631773829460144, 1.1476064920425415, 1.1809418201446533, 2.365412712097168, 1.1749684810638428, 0.14700952172279358, 3.269122838973999, 3.102414131164551, 0.3215884864330292, 0.0649370551109314]], "avg_entropy": [1.951280660091006], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "MONET", "score_of_biggest_cluster": 0.1112203523516655, "score_of_first_answer": 0.1112203523516655, "max_score_of_first_answer": 0.9984921216964722, "min_score_of_first_answer": 1.994566673602094e-06, "avg_score": 0.1112203523516655, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3643758777103976, "std_of_first_answer": 0.3643758777103976, "avg_entropy_of_first_answer": 1.951280660091006, "lex_sim": 1.0}
{"question": "On a computer keyboard you need to press 'shift' and which number to produce the \u00a3 sign?", "answer": ["Keyboard"], "scores": [[0.002455533482134342, 0.3394755423069]], "normalized_score": [0.028872020542621613], "tokens": [["\u2581Key", "board"]], "entropy": [[2.3624563217163086, 2.53206205368042]], "avg_entropy": [2.4472591876983643], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "3", "score_of_biggest_cluster": 0.028872020542621613, "score_of_first_answer": 0.028872020542621613, "max_score_of_first_answer": 0.3394755423069, "min_score_of_first_answer": 0.002455533482134342, "avg_score": 0.028872020542621613, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.16851000441238284, "std_of_first_answer": 0.16851000441238284, "avg_entropy_of_first_answer": 2.4472591876983643, "lex_sim": 1.0}
{"question": "What is worn around the neck in a \u2018gurning\u2019 contest?", "answer": ["A carrot"], "scores": [[0.49300551414489746, 0.035040661692619324, 0.9448682069778442]], "normalized_score": [0.2536676526069641], "tokens": [["\u2581A", "\u2581car", "rot"]], "entropy": [[3.2944884300231934, 5.795460224151611, 0.3473987579345703]], "avg_entropy": [3.145782470703125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Horsecollar", "score_of_biggest_cluster": 0.2536676526069641, "score_of_first_answer": 0.2536676526069641, "max_score_of_first_answer": 0.9448682069778442, "min_score_of_first_answer": 0.035040661692619324, "avg_score": 0.2536676526069641, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3714383246868579, "std_of_first_answer": 0.3714383246868579, "avg_entropy_of_first_answer": 3.145782470703125, "lex_sim": 1.0}
{"question": "Why was the 1967 R.A.C. Rally not staged", "answer": ["Insurance not covering"], "scores": [[0.018663756549358368, 0.05521674081683159, 0.851982593536377, 0.0016378110740333796, 0.02289552614092827]], "normalized_score": [0.03187887370586395], "tokens": [["\u2581In", "sur", "ance", "\u2581not", "\u2581covering"]], "entropy": [[4.682146072387695, 4.490880012512207, 0.6623482704162598, 3.913792371749878, 3.2407190799713135]], "avg_entropy": [3.3979771614074705], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Outbreak of foot and mouth disease", "score_of_biggest_cluster": 0.03187887370586395, "score_of_first_answer": 0.03187887370586395, "max_score_of_first_answer": 0.851982593536377, "min_score_of_first_answer": 0.0016378110740333796, "avg_score": 0.03187887370586395, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3314054246359317, "std_of_first_answer": 0.3314054246359317, "avg_entropy_of_first_answer": 3.3979771614074705, "lex_sim": 1.0}
{"question": "Which of Puccini's operatic characters commits suicide with her father's sword?", "answer": ["Liu"], "scores": [[0.04213733971118927, 0.9185129404067993]], "normalized_score": [0.1967325359582901], "tokens": [["\u2581Li", "u"]], "entropy": [[3.6142406463623047, 0.5689460039138794]], "avg_entropy": [2.091593325138092], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Madam Butterfly", "score_of_biggest_cluster": 0.1967325359582901, "score_of_first_answer": 0.1967325359582901, "max_score_of_first_answer": 0.9185129404067993, "min_score_of_first_answer": 0.04213733971118927, "avg_score": 0.1967325359582901, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.438187800347805, "std_of_first_answer": 0.438187800347805, "avg_entropy_of_first_answer": 2.091593325138092, "lex_sim": 1.0}
{"question": "Which Italian liqueur is a key ingredient of the well known cocktail, Harvey Wallbanger?", "answer": ["Carpano"], "scores": [[0.003018841613084078, 0.328818142414093, 0.8084287643432617]], "normalized_score": [0.09292786568403244], "tokens": [["\u2581Car", "p", "ano"]], "entropy": [[3.5436906814575195, 3.6430115699768066, 1.2916146516799927]], "avg_entropy": [2.8261056343714395], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Galliano", "score_of_biggest_cluster": 0.09292786568403244, "score_of_first_answer": 0.09292786568403244, "max_score_of_first_answer": 0.8084287643432617, "min_score_of_first_answer": 0.003018841613084078, "avg_score": 0.09292786568403244, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3307998180675875, "std_of_first_answer": 0.3307998180675875, "avg_entropy_of_first_answer": 2.8261056343714395, "lex_sim": 1.0}
{"question": "The Han ethnic people constitute c.90% of mainland?", "answer": ["China"], "scores": [[0.7084049582481384]], "normalized_score": [0.7084049582481384], "tokens": [["\u2581China"]], "entropy": [[1.7352144718170166]], "avg_entropy": [1.7352144718170166], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chinese People's Republic", "score_of_biggest_cluster": 0.7084049582481384, "score_of_first_answer": 0.7084049582481384, "max_score_of_first_answer": 0.7084049582481384, "min_score_of_first_answer": 0.7084049582481384, "avg_score": 0.7084049582481384, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.7352144718170166, "lex_sim": 1.0}
{"question": "In the London 2012 Olympics where did the Taekwondo take place", "answer": ["ExCeL"], "scores": [[0.14922013878822327, 0.8978858590126038, 0.3691839873790741, 0.9867401719093323]], "normalized_score": [0.4700278043746948], "tokens": [["\u2581Ex", "C", "e", "L"]], "entropy": [[3.130023241043091, 0.7296543121337891, 0.7261655330657959, 0.09867383539676666]], "avg_entropy": [1.1711292304098606], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Excell", "score_of_biggest_cluster": 0.4700278043746948, "score_of_first_answer": 0.4700278043746948, "max_score_of_first_answer": 0.9867401719093323, "min_score_of_first_answer": 0.14922013878822327, "avg_score": 0.4700278043746948, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35170305836794785, "std_of_first_answer": 0.35170305836794785, "avg_entropy_of_first_answer": 1.1711292304098606, "lex_sim": 1.0}
{"question": "Which British footballer has the most number of International caps?", "answer": ["David Beckham"], "scores": [[0.2970452606678009, 0.8174037337303162, 0.988093912601471]], "normalized_score": [0.621373176574707], "tokens": [["\u2581David", "\u2581Beck", "ham"]], "entropy": [[3.1385996341705322, 1.0819146633148193, 0.10080938041210175]], "avg_entropy": [1.4404412259658177], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Peter Leslie Shilton OBE", "score_of_biggest_cluster": 0.621373176574707, "score_of_first_answer": 0.621373176574707, "max_score_of_first_answer": 0.988093912601471, "min_score_of_first_answer": 0.2970452606678009, "avg_score": 0.621373176574707, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29391161123907866, "std_of_first_answer": 0.29391161123907866, "avg_entropy_of_first_answer": 1.4404412259658177, "lex_sim": 1.0}
{"question": "\"Which country did Tsar Nicholas I of Russia call \"\"the sick man of Europe\"\"?\"", "answer": ["The Ottoman Empire"], "scores": [[0.04964073747396469, 0.8477914333343506, 0.9516942501068115, 0.9033281803131104]], "normalized_score": [0.43613162636756897], "tokens": [["\u2581The", "\u2581Ott", "oman", "\u2581Empire"]], "entropy": [[3.3044164180755615, 1.061567783355713, 0.2203638106584549, 0.5129196643829346]], "avg_entropy": [1.274816919118166], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Turkey", "score_of_biggest_cluster": 0.43613162636756897, "score_of_first_answer": 0.43613162636756897, "max_score_of_first_answer": 0.9516942501068115, "min_score_of_first_answer": 0.04964073747396469, "avg_score": 0.43613162636756897, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3704513062909246, "std_of_first_answer": 0.3704513062909246, "avg_entropy_of_first_answer": 1.274816919118166, "lex_sim": 1.0}
{"question": "What is the lowest level of the atmosphere, which lies below the stratosphere, called?", "answer": ["Troposphere"], "scores": [[0.6467595100402832, 0.989636242389679, 0.9663144946098328]], "normalized_score": [0.8520118594169617], "tokens": [["\u2581Tro", "pos", "phere"]], "entropy": [[1.7179811000823975, 0.06438566744327545, 0.21204984188079834]], "avg_entropy": [0.6648055364688238], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tropospherics", "score_of_biggest_cluster": 0.8520118594169617, "score_of_first_answer": 0.8520118594169617, "max_score_of_first_answer": 0.989636242389679, "min_score_of_first_answer": 0.6467595100402832, "avg_score": 0.8520118594169617, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1564266763514853, "std_of_first_answer": 0.1564266763514853, "avg_entropy_of_first_answer": 0.6648055364688238, "lex_sim": 1.0}
{"question": "Primula veris is the Latin name for which common meadow flower?", "answer": ["Primrose"], "scores": [[0.2592293322086334, 0.849684476852417]], "normalized_score": [0.4693220257759094], "tokens": [["\u2581Prim", "rose"]], "entropy": [[2.7644450664520264, 0.56011962890625]], "avg_entropy": [1.6622823476791382], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cowslips", "score_of_biggest_cluster": 0.4693220257759094, "score_of_first_answer": 0.4693220257759094, "max_score_of_first_answer": 0.849684476852417, "min_score_of_first_answer": 0.2592293322086334, "avg_score": 0.4693220257759094, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2952275723218918, "std_of_first_answer": 0.2952275723218918, "avg_entropy_of_first_answer": 1.6622823476791382, "lex_sim": 1.0}
{"question": "A group of which fish is called a Lap?", "answer": ["Lapwings"], "scores": [[0.026137813925743103, 0.0635065957903862, 0.9845682382583618]], "normalized_score": [0.11779078841209412], "tokens": [["\u2581Lap", "w", "ings"]], "entropy": [[5.8224992752075195, 5.247260093688965, 0.13705530762672424]], "avg_entropy": [3.7356048921744027], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Codfish", "score_of_biggest_cluster": 0.11779078841209412, "score_of_first_answer": 0.11779078841209412, "max_score_of_first_answer": 0.9845682382583618, "min_score_of_first_answer": 0.026137813925743103, "avg_score": 0.11779078841209412, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4432631339622419, "std_of_first_answer": 0.4432631339622419, "avg_entropy_of_first_answer": 3.7356048921744027, "lex_sim": 1.0}
{"question": "Which saint established a monastery and Christian community on the island of Iona in the year 563?", "answer": ["Saint Columba"], "scores": [[0.14729607105255127, 0.8889187574386597, 0.9704415202140808, 0.9644797444343567]], "normalized_score": [0.5916691422462463], "tokens": [["\u2581Saint", "\u2581Col", "um", "ba"]], "entropy": [[1.8479037284851074, 0.7831823825836182, 0.19356146454811096, 0.20464907586574554]], "avg_entropy": [0.7573241628706455], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "St Colm", "score_of_biggest_cluster": 0.5916691422462463, "score_of_first_answer": 0.5916691422462463, "max_score_of_first_answer": 0.9704415202140808, "min_score_of_first_answer": 0.14729607105255127, "avg_score": 0.5916691422462463, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34530355864952494, "std_of_first_answer": 0.34530355864952494, "avg_entropy_of_first_answer": 0.7573241628706455, "lex_sim": 1.0}
{"question": "What name was given to the day of serious conflict in Londonderry on January 30th 1972?", "answer": ["Bloody Sunday"], "scores": [[0.7461605668067932, 0.9999282360076904, 0.8574408292770386]], "normalized_score": [0.8616583347320557], "tokens": [["\u2581Blo", "ody", "\u2581Sunday"]], "entropy": [[1.481220006942749, 0.0008771073771640658, 0.6406939625740051]], "avg_entropy": [0.7075970256313061], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Blut Sonntag", "score_of_biggest_cluster": 0.8616583347320557, "score_of_first_answer": 0.8616583347320557, "max_score_of_first_answer": 0.9999282360076904, "min_score_of_first_answer": 0.7461605668067932, "avg_score": 0.8616583347320557, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10386101173305758, "std_of_first_answer": 0.10386101173305758, "avg_entropy_of_first_answer": 0.7075970256313061, "lex_sim": 1.0}
{"question": "Which continent has the world's highest waterfall, largest river by volume, longest mountain range, driest place, largest rainforest, highest capital city, and highest commercially navigable lake?", "answer": ["Asia"], "scores": [[0.0479133166372776]], "normalized_score": [0.0479133166372776], "tokens": [["\u2581Asia"]], "entropy": [[1.581736445426941]], "avg_entropy": [1.581736445426941], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "South amerika", "score_of_biggest_cluster": 0.0479133166372776, "score_of_first_answer": 0.0479133166372776, "max_score_of_first_answer": 0.0479133166372776, "min_score_of_first_answer": 0.0479133166372776, "avg_score": 0.0479133166372776, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.581736445426941, "lex_sim": 1.0}
{"question": "Which city in Egypt has been characterised as the 'world's greatest open air museum' and contains the ruins of the temple complexes at Karnak as well as monuments that include the Valley of the Kings and the Valley of the Queens?", "answer": ["Thebes"], "scores": [[0.14669278264045715, 0.9210891723632812]], "normalized_score": [0.3675828278064728], "tokens": [["\u2581The", "bes"]], "entropy": [[2.5945286750793457, 0.6749981641769409]], "avg_entropy": [1.6347634196281433], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Al-Uqsur", "score_of_biggest_cluster": 0.3675828278064728, "score_of_first_answer": 0.3675828278064728, "max_score_of_first_answer": 0.9210891723632812, "min_score_of_first_answer": 0.14669278264045715, "avg_score": 0.3675828278064728, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.38719819486141205, "std_of_first_answer": 0.38719819486141205, "avg_entropy_of_first_answer": 1.6347634196281433, "lex_sim": 1.0}
{"question": "Who allegedly solved the riddle of the Gordian knot simply by slicing it with his sword?", "answer": ["Alexander the Great"], "scores": [[0.5197417736053467, 0.8360695242881775, 0.9567957520484924]], "normalized_score": [0.7463624477386475], "tokens": [["\u2581Alexander", "\u2581the", "\u2581Great"]], "entropy": [[2.654226064682007, 0.9099454283714294, 0.28274163603782654]], "avg_entropy": [1.2823043763637543], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u039c\u03ad\u03b3\u03b1\u03c2 \u1f08\u03bb\u03ad\u03be\u03b1\u03bd\u03b4\u03c1\u03bf\u03c2", "score_of_biggest_cluster": 0.7463624477386475, "score_of_first_answer": 0.7463624477386475, "max_score_of_first_answer": 0.9567957520484924, "min_score_of_first_answer": 0.5197417736053467, "avg_score": 0.7463624477386475, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1842866872849885, "std_of_first_answer": 0.1842866872849885, "avg_entropy_of_first_answer": 1.2823043763637543, "lex_sim": 1.0}
{"question": "\"'The Doors' generated a controversy with the single \"\"Hello, I Love You\"\" in 1968 when the press pointed out it's resemblance to the 1965 hit \"\"All Day and All of the Night\"\" of which band?\"", "answer": ["The Kinks 250 points are given for winning the top division in a league, 200 points for winning a cup competition and 100 points for earning a place in a European competition, with fewer points for placing"], "scores": [[0.7010159492492676, 0.8482577800750732, 0.9782788157463074, 0.0009346804581582546, 0.03618768975138664, 0.019486920908093452, 0.038692228496074677, 0.07255516946315765, 0.0015018529957160354, 0.02475053071975708, 0.5528640747070312, 0.00020003838289994746, 0.2431817501783371, 0.004410192836076021, 0.004950437694787979, 0.1340019702911377, 0.0899934247136116, 0.0920567736029625, 0.16817903518676758, 0.45636463165283203, 0.5299140810966492, 0.8617739677429199, 0.9870878458023071, 0.47133222222328186, 0.8176402449607849, 0.45317915081977844, 0.046703580766916275, 0.11672937870025635, 0.2834872603416443, 0.23683956265449524, 0.9537872076034546, 0.8808233141899109, 0.5847413539886475, 0.9493072032928467, 0.8748019337654114, 0.9541547298431396, 0.0010334206745028496, 0.9963570237159729, 0.49286606907844543, 0.34534016251564026, 0.9502729773521423, 0.18179214000701904, 0.5869790315628052, 0.7123295068740845, 0.02296900749206543, 0.14378096163272858, 0.0020226724445819855, 0.9588562846183777, 0.25771892070770264, 0.009792852215468884]], "normalized_score": [0.12979115545749664], "tokens": [["\u2581The", "\u2581K", "inks", "\u2581", "2", "5", "0", "\u2581points", "\u2581are", "\u2581given", "\u2581for", "\u2581winning", "\u2581the", "\u2581top", "\u2581division", "\u2581in", "\u2581a", "\u2581league", ",", "\u2581", "2", "0", "0", "\u2581points", "\u2581for", "\u2581winning", "\u2581a", "\u2581cup", "\u2581competition", "\u2581and", "\u2581", "1", "0", "0", "\u2581points", "\u2581for", "\u2581ear", "ning", "\u2581a", "\u2581place", "\u2581in", "\u2581a", "\u2581European", "\u2581competition", ",", "\u2581with", "\u2581fewer", "\u2581points", "\u2581for", "\u2581placing"]], "entropy": [[1.4555037021636963, 0.9016603231430054, 0.1929222196340561, 0.22924433648586273, 3.866129159927368, 2.2756195068359375, 3.5523695945739746, 3.7283804416656494, 1.4259740114212036, 4.198147773742676, 1.7227727174758911, 3.144516944885254, 3.190199375152588, 4.869655132293701, 3.605236530303955, 2.3082363605499268, 2.9946658611297607, 5.043356418609619, 3.2590770721435547, 2.8119583129882812, 1.0583865642547607, 0.631557822227478, 0.09573401510715485, 0.8955944776535034, 0.6854398250579834, 2.0808300971984863, 0.7818318009376526, 3.5118649005889893, 1.9974329471588135, 1.4210690259933472, 0.40337425470352173, 0.5520739555358887, 1.0912871360778809, 0.24777305126190186, 0.49299001693725586, 0.3006444573402405, 2.9627742767333984, 0.030958309769630432, 1.907694935798645, 3.1535558700561523, 0.3516606092453003, 1.2503383159637451, 2.1052422523498535, 1.517463207244873, 1.4578166007995605, 4.4267072677612305, 3.2636075019836426, 0.3370143175125122, 2.2849836349487305, 3.989781379699707]], "avg_entropy": [2.001262172460556], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Kinks", "score_of_biggest_cluster": 0.12979115545749664, "score_of_first_answer": 0.12979115545749664, "max_score_of_first_answer": 0.9963570237159729, "min_score_of_first_answer": 0.00020003838289994746, "avg_score": 0.12979115545749664, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3662609292286022, "std_of_first_answer": 0.3662609292286022, "avg_entropy_of_first_answer": 2.001262172460556, "lex_sim": 1.0}
{"question": "Which German philosopher wrote \u2018Das Kapital\u2019?", "answer": ["Karl Marx"], "scores": [[0.8699028491973877, 0.8002159595489502]], "normalized_score": [0.8343321681022644], "tokens": [["\u2581Karl", "\u2581Marx"]], "entropy": [[0.9437966346740723, 1.1535308361053467]], "avg_entropy": [1.0486637353897095], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Karl Marx", "score_of_biggest_cluster": 0.8343321681022644, "score_of_first_answer": 0.8343321681022644, "max_score_of_first_answer": 0.8699028491973877, "min_score_of_first_answer": 0.8002159595489502, "avg_score": 0.8343321681022644, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03484344482421875, "std_of_first_answer": 0.03484344482421875, "avg_entropy_of_first_answer": 1.0486637353897095, "lex_sim": 1.0}
{"question": "The site of Carthage is now in a suburb of which modem capital city?", "answer": ["Tunis"], "scores": [[0.8445977568626404, 0.9889677166938782]], "normalized_score": [0.9139364957809448], "tokens": [["\u2581Tun", "is"]], "entropy": [[1.2148765325546265, 0.09731119871139526]], "avg_entropy": [0.6560938656330109], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Emirate of Tunis", "score_of_biggest_cluster": 0.9139364957809448, "score_of_first_answer": 0.9139364957809448, "max_score_of_first_answer": 0.9889677166938782, "min_score_of_first_answer": 0.8445977568626404, "avg_score": 0.9139364957809448, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0721849799156189, "std_of_first_answer": 0.0721849799156189, "avg_entropy_of_first_answer": 0.6560938656330109, "lex_sim": 1.0}
{"question": "In medicine an emetic is administered to induce what?", "answer": ["Vomiting"], "scores": [[0.7607037425041199, 0.9934648871421814, 0.9100736975669861]], "normalized_score": [0.8827035427093506], "tokens": [["\u2581V", "om", "iting"]], "entropy": [[1.3762941360473633, 0.0632261410355568, 0.47045859694480896]], "avg_entropy": [0.6366596246759096], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Vomitting", "score_of_biggest_cluster": 0.8827035427093506, "score_of_first_answer": 0.8827035427093506, "max_score_of_first_answer": 0.9934648871421814, "min_score_of_first_answer": 0.7607037425041199, "avg_score": 0.8827035427093506, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0962884696151618, "std_of_first_answer": 0.0962884696151618, "avg_entropy_of_first_answer": 0.6366596246759096, "lex_sim": 1.0}
{"question": "Ameliorate means to make something that was bad, what?", "answer": ["Worse"], "scores": [[0.12707746028900146, 0.9948989748954773]], "normalized_score": [0.35556888580322266], "tokens": [["\u2581Wor", "se"]], "entropy": [[2.9830894470214844, 0.03909935802221298]], "avg_entropy": [1.5110944025218487], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Better", "score_of_biggest_cluster": 0.35556888580322266, "score_of_first_answer": 0.35556888580322266, "max_score_of_first_answer": 0.9948989748954773, "min_score_of_first_answer": 0.12707746028900146, "avg_score": 0.35556888580322266, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4339107573032379, "std_of_first_answer": 0.4339107573032379, "avg_entropy_of_first_answer": 1.5110944025218487, "lex_sim": 1.0}
{"question": "In what field was the 15th century Flemish Jan van Eyck famous?", "answer": ["Van Eyck"], "scores": [[0.0005509982584044337, 0.37649428844451904, 0.902495801448822, 0.9700810313224792]], "normalized_score": [0.11608884483575821], "tokens": [["\u2581Van", "\u2581E", "y", "ck"]], "entropy": [[2.1559548377990723, 2.730822801589966, 0.6945916414260864, 0.19765296578407288]], "avg_entropy": [1.4447555616497993], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Oil paintings", "score_of_biggest_cluster": 0.11608884483575821, "score_of_first_answer": 0.11608884483575821, "max_score_of_first_answer": 0.9700810313224792, "min_score_of_first_answer": 0.0005509982584044337, "avg_score": 0.11608884483575821, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39752490855540623, "std_of_first_answer": 0.39752490855540623, "avg_entropy_of_first_answer": 1.4447555616497993, "lex_sim": 1.0}
{"question": "The bumblebee or hog-nosed is the smallest mammal in the world - what is it?", "answer": ["The humane society"], "scores": [[0.05817107483744621, 0.02780987322330475, 0.0029322707559913397, 0.10260272771120071]], "normalized_score": [0.026412975043058395], "tokens": [["\u2581The", "\u2581hum", "ane", "\u2581society"]], "entropy": [[4.989260673522949, 4.271639347076416, 1.743431806564331, 4.1127519607543945]], "avg_entropy": [3.7792709469795227], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bat Conservation", "score_of_biggest_cluster": 0.026412975043058395, "score_of_first_answer": 0.026412975043058395, "max_score_of_first_answer": 0.10260272771120071, "min_score_of_first_answer": 0.0029322707559913397, "avg_score": 0.026412975043058395, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.03716043426274041, "std_of_first_answer": 0.03716043426274041, "avg_entropy_of_first_answer": 3.7792709469795227, "lex_sim": 1.0}
{"question": "What is 'The Old Lady of Threadneedle Street'?", "answer": ["The Bank of England"], "scores": [[0.6092018485069275, 0.8621824979782104, 0.9806154370307922, 0.9941555261611938]], "normalized_score": [0.8459181785583496], "tokens": [["\u2581The", "\u2581Bank", "\u2581of", "\u2581England"]], "entropy": [[1.6732888221740723, 0.9008557796478271, 0.12122233211994171, 0.057493433356285095]], "avg_entropy": [0.6882150918245316], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "UK Central Bank", "score_of_biggest_cluster": 0.8459181785583496, "score_of_first_answer": 0.8459181785583496, "max_score_of_first_answer": 0.9941555261611938, "min_score_of_first_answer": 0.6092018485069275, "avg_score": 0.8459181785583496, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.15446746662337793, "std_of_first_answer": 0.15446746662337793, "avg_entropy_of_first_answer": 0.6882150918245316, "lex_sim": 1.0}
{"question": "What are the last six words of Rudyard Kipling\u2019s \u2018If\u2019?", "answer": ["If you can take it with a smile..."], "scores": [[0.28734081983566284, 0.944831371307373, 0.9888257384300232, 0.0018633499275892973, 0.4611969590187073, 0.7273330688476562, 0.9340050220489502, 0.47909533977508545, 0.036769766360521317]], "normalized_score": [0.241178497672081], "tokens": [["\u2581If", "\u2581you", "\u2581can", "\u2581take", "\u2581it", "\u2581with", "\u2581a", "\u2581smile", "..."]], "entropy": [[3.7546520233154297, 0.41694819927215576, 0.11410913616418839, 1.259958028793335, 1.6416378021240234, 1.2338531017303467, 0.37564945220947266, 1.504467248916626, 2.052116870880127]], "avg_entropy": [1.3725990959339671], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "YOU\u2019LL BE A MAN, MY SON", "score_of_biggest_cluster": 0.241178497672081, "score_of_first_answer": 0.241178497672081, "max_score_of_first_answer": 0.9888257384300232, "min_score_of_first_answer": 0.0018633499275892973, "avg_score": 0.241178497672081, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3609022832321609, "std_of_first_answer": 0.3609022832321609, "avg_entropy_of_first_answer": 1.3725990959339671, "lex_sim": 1.0}
{"question": "Who was the first man sent into space, in 1961?", "answer": ["Astronaut Yuri Gagarin"], "scores": [[0.0033442990388721228, 0.43498069047927856, 0.9630990624427795, 0.8195037841796875, 0.981823742389679, 0.916074812412262, 0.9764179587364197, 0.99391770362854]], "normalized_score": [0.4218115210533142], "tokens": [["\u2581A", "str", "onaut", "\u2581Y", "uri", "\u2581G", "ag", "arin"]], "entropy": [[0.8886443376541138, 3.351789951324463, 0.20552369952201843, 1.0110384225845337, 0.12377851456403732, 0.5292668342590332, 0.15005949139595032, 0.05970863252878189]], "avg_entropy": [0.7899762354791164], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u042e\u0301\u0440\u0438\u0439 \u0410\u043b\u0435\u043a\u0441\u0435\u0301\u0435\u0432\u0438\u0447 \u0413\u0430\u0433\u0430\u0301\u0440\u0438\u043d", "score_of_biggest_cluster": 0.4218115210533142, "score_of_first_answer": 0.4218115210533142, "max_score_of_first_answer": 0.99391770362854, "min_score_of_first_answer": 0.0033442990388721228, "avg_score": 0.4218115210533142, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3350492841815055, "std_of_first_answer": 0.3350492841815055, "avg_entropy_of_first_answer": 0.7899762354791164, "lex_sim": 1.0}
{"question": "Kagoshima international airport is in which country?", "answer": ["Japan"], "scores": [[0.8753520250320435]], "normalized_score": [0.8753520250320435], "tokens": [["\u2581Japan"]], "entropy": [[0.9876078963279724]], "avg_entropy": [0.9876078963279724], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u65e5\u672c\u570b", "score_of_biggest_cluster": 0.8753520250320435, "score_of_first_answer": 0.8753520250320435, "max_score_of_first_answer": 0.8753520250320435, "min_score_of_first_answer": 0.8753520250320435, "avg_score": 0.8753520250320435, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.9876078963279724, "lex_sim": 1.0}
{"question": "The name of which Russian spacecraft means 'travelling companion' or 'satellite'?", "answer": ["MIR"], "scores": [[0.2603573203086853, 0.8691197037696838]], "normalized_score": [0.4756907522678375], "tokens": [["\u2581M", "IR"]], "entropy": [[3.4071075916290283, 1.0130538940429688]], "avg_entropy": [2.2100807428359985], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "First artificial satellite", "score_of_biggest_cluster": 0.4756907522678375, "score_of_first_answer": 0.4756907522678375, "max_score_of_first_answer": 0.8691197037696838, "min_score_of_first_answer": 0.2603573203086853, "avg_score": 0.4756907522678375, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30438119173049927, "std_of_first_answer": 0.30438119173049927, "avg_entropy_of_first_answer": 2.2100807428359985, "lex_sim": 1.0}
{"question": "The ports of Puck and Jastarnia are in which European country?", "answer": ["Poland"], "scores": [[0.8686867952346802]], "normalized_score": [0.8686867952346802], "tokens": [["\u2581Poland"]], "entropy": [[1.0222687721252441]], "avg_entropy": [1.0222687721252441], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Etymology of Poland", "score_of_biggest_cluster": 0.8686867952346802, "score_of_first_answer": 0.8686867952346802, "max_score_of_first_answer": 0.8686867952346802, "min_score_of_first_answer": 0.8686867952346802, "avg_score": 0.8686867952346802, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.0222687721252441, "lex_sim": 1.0}
{"question": "Which Oscar-nominated film had You Sexy Thing as its theme song?", "answer": ["American Pie 2"], "scores": [[0.008435489609837532, 0.06122155860066414, 0.04910086467862129, 0.7993430495262146]], "normalized_score": [0.06709795445203781], "tokens": [["\u2581American", "\u2581Pie", "\u2581", "2"]], "entropy": [[4.477280616760254, 2.9854111671447754, 0.6039924621582031, 0.7229437828063965]], "avg_entropy": [2.1974070072174072], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Full Monty", "score_of_biggest_cluster": 0.06709795445203781, "score_of_first_answer": 0.06709795445203781, "max_score_of_first_answer": 0.7993430495262146, "min_score_of_first_answer": 0.008435489609837532, "avg_score": 0.06709795445203781, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3295649018643811, "std_of_first_answer": 0.3295649018643811, "avg_entropy_of_first_answer": 2.1974070072174072, "lex_sim": 1.0}
{"question": "What is the name of Beethoven's only opera?", "answer": ["Fidelio"], "scores": [[0.8481477499008179, 0.9554570317268372, 0.9983065128326416]], "normalized_score": [0.9317845702171326], "tokens": [["\u2581F", "idel", "io"]], "entropy": [[0.9472306966781616, 0.36260515451431274, 0.019536249339580536]], "avg_entropy": [0.44312403351068497], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Leonore", "score_of_biggest_cluster": 0.9317845702171326, "score_of_first_answer": 0.9317845702171326, "max_score_of_first_answer": 0.9983065128326416, "min_score_of_first_answer": 0.8481477499008179, "avg_score": 0.9317845702171326, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06315678381990539, "std_of_first_answer": 0.06315678381990539, "avg_entropy_of_first_answer": 0.44312403351068497, "lex_sim": 1.0}
{"question": "Which 2013 film is a development of the 1964 film \u2018Mary Poppins\u2019?", "answer": ["Saving Mr Banks (film)"], "scores": [[0.7275726199150085, 0.9922498464584351, 0.9355238080024719, 0.5834625959396362, 0.9998098015785217, 0.006155447103083134, 0.009531360119581223, 0.9232040047645569]], "normalized_score": [0.26070499420166016], "tokens": [["\u2581S", "aving", "\u2581Mr", "\u2581B", "anks", "\u2581(", "film", ")"]], "entropy": [[1.601137638092041, 0.06548968702554703, 0.48557302355766296, 1.0330547094345093, 0.002803062554448843, 0.24745270609855652, 4.226407051086426, 0.5619897842407227]], "avg_entropy": [1.0279884577612393], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Saving Mr Banks", "score_of_biggest_cluster": 0.26070499420166016, "score_of_first_answer": 0.26070499420166016, "max_score_of_first_answer": 0.9998098015785217, "min_score_of_first_answer": 0.006155447103083134, "avg_score": 0.26070499420166016, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.39223454787717826, "std_of_first_answer": 0.39223454787717826, "avg_entropy_of_first_answer": 1.0279884577612393, "lex_sim": 1.0}
{"question": "What does \u201cSPF\u201d mean on sunscreen containers?", "answer": ["sun protection factor"], "scores": [[0.022476255893707275, 0.8465837836265564, 0.9782344698905945]], "normalized_score": [0.2650202214717865], "tokens": [["\u2581sun", "\u2581protection", "\u2581factor"]], "entropy": [[1.4472458362579346, 0.9566540718078613, 0.17827408015727997]], "avg_entropy": [0.8607246627410253], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sunblock", "score_of_biggest_cluster": 0.2650202214717865, "score_of_first_answer": 0.2650202214717865, "max_score_of_first_answer": 0.9782344698905945, "min_score_of_first_answer": 0.022476255893707275, "avg_score": 0.2650202214717865, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4229471840000646, "std_of_first_answer": 0.4229471840000646, "avg_entropy_of_first_answer": 0.8607246627410253, "lex_sim": 1.0}
{"question": "What is the capital of Taiwan?", "answer": ["Tainan"], "scores": [[0.023884374648332596, 0.2955529987812042, 0.9481074213981628]], "normalized_score": [0.18845266103744507], "tokens": [["\u2581T", "ain", "an"]], "entropy": [[0.947076141834259, 3.4278972148895264, 0.45262986421585083]], "avg_entropy": [1.6092010736465454], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "T\u00e2i-pak", "score_of_biggest_cluster": 0.18845266103744507, "score_of_first_answer": 0.18845266103744507, "max_score_of_first_answer": 0.9481074213981628, "min_score_of_first_answer": 0.023884374648332596, "avg_score": 0.18845266103744507, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38784581070872004, "std_of_first_answer": 0.38784581070872004, "avg_entropy_of_first_answer": 1.6092010736465454, "lex_sim": 1.0}
{"question": "What German battleship was sunk in November 1944 by Barnes-Wallis' Tallboy bombs, dropped by Lancaster bombers?", "answer": ["Scharnhorst"], "scores": [[0.35513415932655334, 0.9641283750534058, 0.9755775332450867, 0.9968478083610535]], "normalized_score": [0.7596341371536255], "tokens": [["\u2581Sch", "arn", "hor", "st"]], "entropy": [[2.4921088218688965, 0.347627729177475, 0.22461429238319397, 0.03576279059052467]], "avg_entropy": [0.7750284085050225], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "TIRPITZ", "score_of_biggest_cluster": 0.7596341371536255, "score_of_first_answer": 0.7596341371536255, "max_score_of_first_answer": 0.9968478083610535, "min_score_of_first_answer": 0.35513415932655334, "avg_score": 0.7596341371536255, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27033248180806574, "std_of_first_answer": 0.27033248180806574, "avg_entropy_of_first_answer": 0.7750284085050225, "lex_sim": 1.0}
{"question": "Which town in East Sussex now stands on the site of the battle of Hastings?", "answer": ["Battle Abbey"], "scores": [[0.5111351609230042, 0.05073932558298111, 0.9805512428283691]], "normalized_score": [0.29406967759132385], "tokens": [["\u2581Battle", "\u2581Ab", "bey"]], "entropy": [[2.0726964473724365, 1.9656471014022827, 0.17067508399486542]], "avg_entropy": [1.4030062109231949], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Battle", "score_of_biggest_cluster": 0.29406967759132385, "score_of_first_answer": 0.29406967759132385, "max_score_of_first_answer": 0.9805512428283691, "min_score_of_first_answer": 0.05073932558298111, "avg_score": 0.29406967759132385, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37960007970397336, "std_of_first_answer": 0.37960007970397336, "avg_entropy_of_first_answer": 1.4030062109231949, "lex_sim": 1.0}
{"question": "Which organisation awards the Dickin Medal?", "answer": ["PDSA"], "scores": [[0.21487155556678772, 0.8697317838668823, 0.9986268281936646]], "normalized_score": [0.5714644193649292], "tokens": [["\u2581P", "D", "SA"]], "entropy": [[2.603883743286133, 0.7944948077201843, 0.0148536441847682]], "avg_entropy": [1.1377440650636952], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "PDSA", "score_of_biggest_cluster": 0.5714644193649292, "score_of_first_answer": 0.5714644193649292, "max_score_of_first_answer": 0.9986268281936646, "min_score_of_first_answer": 0.21487155556678772, "avg_score": 0.5714644193649292, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.34314366580384137, "std_of_first_answer": 0.34314366580384137, "avg_entropy_of_first_answer": 1.1377440650636952, "lex_sim": 1.0}
{"question": "According to legend who killed the lion represented by the constellation Leo?", "answer": ["Nemean Lion"], "scores": [[0.05248057469725609, 0.28165203332901, 0.6119595766067505, 0.8852887749671936]], "normalized_score": [0.2991436719894409], "tokens": [["\u2581N", "eme", "an", "\u2581Lion"]], "entropy": [[2.5999207496643066, 2.3406896591186523, 2.049201011657715, 0.6155351996421814]], "avg_entropy": [1.9013366550207138], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hurcules", "score_of_biggest_cluster": 0.2991436719894409, "score_of_first_answer": 0.2991436719894409, "max_score_of_first_answer": 0.8852887749671936, "min_score_of_first_answer": 0.05248057469725609, "avg_score": 0.2991436719894409, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3169478515016315, "std_of_first_answer": 0.3169478515016315, "avg_entropy_of_first_answer": 1.9013366550207138, "lex_sim": 1.0}
{"question": "\"Published in 1900, who was the author of the children's book \"\"The Wonderful Wizard of Oz\"\"?\"", "answer": ["L Frank Baum"], "scores": [[0.6833961606025696, 0.2738109827041626, 0.9889758825302124, 0.9965986609458923]], "normalized_score": [0.6553258895874023], "tokens": [["\u2581L", "\u2581Frank", "\u2581Ba", "um"]], "entropy": [[1.757014274597168, 1.2879180908203125, 0.10817880928516388, 0.03304309397935867]], "avg_entropy": [0.7965385671705008], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Floyd Akens", "score_of_biggest_cluster": 0.6553258895874023, "score_of_first_answer": 0.6553258895874023, "max_score_of_first_answer": 0.9965986609458923, "min_score_of_first_answer": 0.2738109827041626, "avg_score": 0.6553258895874023, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29508216643216545, "std_of_first_answer": 0.29508216643216545, "avg_entropy_of_first_answer": 0.7965385671705008, "lex_sim": 1.0}
{"question": "In the fairy tale Thumbelina, what creature wants the miniature girl to marry her son ?", "answer": ["Frog"], "scores": [[0.049152299761772156, 0.9651434421539307]], "normalized_score": [0.21780499815940857], "tokens": [["\u2581F", "rog"]], "entropy": [[3.514893054962158, 0.2561087906360626]], "avg_entropy": [1.8855009227991104], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "TOAD", "score_of_biggest_cluster": 0.21780499815940857, "score_of_first_answer": 0.21780499815940857, "max_score_of_first_answer": 0.9651434421539307, "min_score_of_first_answer": 0.049152299761772156, "avg_score": 0.21780499815940857, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45799557119607925, "std_of_first_answer": 0.45799557119607925, "avg_entropy_of_first_answer": 1.8855009227991104, "lex_sim": 1.0}
{"question": "What, loosely translating as 'self-boiler', is a traditional metal Russian tea urn called?", "answer": ["Samovar"], "scores": [[0.641315758228302, 0.9983615279197693, 0.9900155663490295]], "normalized_score": [0.8590146899223328], "tokens": [["\u2581Sam", "ov", "ar"]], "entropy": [[1.4709311723709106, 0.01837972365319729, 0.0829853042960167]], "avg_entropy": [0.5240987334400415], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Samowar", "score_of_biggest_cluster": 0.8590146899223328, "score_of_first_answer": 0.8590146899223328, "max_score_of_first_answer": 0.9983615279197693, "min_score_of_first_answer": 0.641315758228302, "avg_score": 0.8590146899223328, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.16638071900598764, "std_of_first_answer": 0.16638071900598764, "avg_entropy_of_first_answer": 0.5240987334400415, "lex_sim": 1.0}
{"question": "A mill in Woodbridge, Suffolk, England, built in the 12th century, reconstructed in 1792, further restored in 2010 and currently in full working order is a early English example of a mill powered by what?", "answer": ["The tide"], "scores": [[0.011866840533912182, 0.1284375786781311, 0.8991705775260925]], "normalized_score": [0.1110767275094986], "tokens": [["\u2581The", "\u2581t", "ide"]], "entropy": [[3.3246114253997803, 3.92545747756958, 0.4170117974281311]], "avg_entropy": [2.555693566799164], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Compound Tides", "score_of_biggest_cluster": 0.1110767275094986, "score_of_first_answer": 0.1110767275094986, "max_score_of_first_answer": 0.8991705775260925, "min_score_of_first_answer": 0.011866840533912182, "avg_score": 0.1110767275094986, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3936899533311501, "std_of_first_answer": 0.3936899533311501, "avg_entropy_of_first_answer": 2.555693566799164, "lex_sim": 1.0}
{"question": "As at 2010 who is the MP for Belfast West?", "answer": ["Naomi Long"], "scores": [[0.8901971578598022, 0.9443984627723694, 0.9856443405151367]], "normalized_score": [0.9392630457878113], "tokens": [["\u2581Na", "omi", "\u2581Long"]], "entropy": [[0.783670961856842, 0.3955831527709961, 0.1430586874485016]], "avg_entropy": [0.4407709340254466], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Adams, Gerard", "score_of_biggest_cluster": 0.9392630457878113, "score_of_first_answer": 0.9392630457878113, "max_score_of_first_answer": 0.9856443405151367, "min_score_of_first_answer": 0.8901971578598022, "avg_score": 0.9392630457878113, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.03908561622591724, "std_of_first_answer": 0.03908561622591724, "avg_entropy_of_first_answer": 0.4407709340254466, "lex_sim": 1.0}
{"question": "What is the star sign of the date of the Battle of Waterloo?", "answer": ["Cancer"], "scores": [[0.22295843064785004, 0.9879013299942017]], "normalized_score": [0.4693196415901184], "tokens": [["\u2581Can", "cer"]], "entropy": [[2.7696237564086914, 0.10664224624633789]], "avg_entropy": [1.4381330013275146], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gemini (single)", "score_of_biggest_cluster": 0.4693196415901184, "score_of_first_answer": 0.4693196415901184, "max_score_of_first_answer": 0.9879013299942017, "min_score_of_first_answer": 0.22295843064785004, "avg_score": 0.4693196415901184, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3824714496731758, "std_of_first_answer": 0.3824714496731758, "avg_entropy_of_first_answer": 1.4381330013275146, "lex_sim": 1.0}
{"question": "Which infamous character became Pope Alexander VI?", "answer": ["Rodrigo Borgia"], "scores": [[0.8333213925361633, 0.9922274351119995, 0.9890068769454956, 0.7736547589302063, 0.9926256537437439]], "normalized_score": [0.9111526608467102], "tokens": [["\u2581Rod", "r", "igo", "\u2581Borg", "ia"]], "entropy": [[1.2747297286987305, 0.0780850425362587, 0.09187670052051544, 1.2995285987854004, 0.06230427324771881]], "avg_entropy": [0.5613048687577248], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rodrigo de Borja y Dams", "score_of_biggest_cluster": 0.9111526608467102, "score_of_first_answer": 0.9111526608467102, "max_score_of_first_answer": 0.9926256537437439, "min_score_of_first_answer": 0.7736547589302063, "avg_score": 0.9111526608467102, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.09392539244070774, "std_of_first_answer": 0.09392539244070774, "avg_entropy_of_first_answer": 0.5613048687577248, "lex_sim": 1.0}
{"question": "What sort of game would you be playing if you played 'Spoil five'?", "answer": ["Darts This is an AI system."], "scores": [[0.017427749931812286, 0.5411615967750549, 1.6428946310043102e-06, 0.3601329028606415, 0.06471986323595047, 0.01796596869826317, 0.8465225696563721, 0.026186097413301468, 0.17880433797836304, 0.022999346256256104, 0.04777989909052849, 0.8136651515960693, 0.03935632482171059, 0.3701445758342743, 0.4092593193054199, 0.014446329325437546, 0.9515875577926636, 0.062419403344392776, 0.7007524371147156, 0.27695271372795105, 0.004510755185037851, 0.8389182090759277, 0.05865646153688431, 0.985903263092041, 0.022944077849388123, 0.5338915586471558, 0.13405841588974, 0.8824692368507385, 0.0003116136067546904, 0.08273908495903015, 0.1410382091999054, 0.04800213873386383, 0.24251294136047363, 0.3674008250236511, 0.04750407859683037, 0.9624300599098206, 0.8542118668556213, 0.588431715965271, 0.0295742005109787, 0.26598867774009705, 0.004590201657265425, 0.9656867384910583, 0.15294356644153595, 0.028441721573472023, 0.0011792666045948863, 0.9935048818588257, 0.7662801146507263]], "normalized_score": [0.09312903136014938], "tokens": [["\u2581D", "arts", "\u2581This", "\u2581is", "\u2581an", "\u2581A", "I", "\u2581system", ".", "\u2581I", "'", "m", "\u2581sorry", ",", "\u2581I", "\u2581did", "\u2581not", "\u2581quite", "\u2581understand", "\u2581the", "\u2581request", ".", "\u2581Could", "\u2581you", "\u2581try", "\u2581again", "\u2581please", "?", "\u2581We", "'", "ve", "\u2581never", "\u2581heard", "\u2581of", "\u2581such", "\u2581a", "\u2581game", ".", "\u2581Please", "\u2581try", "\u2581not", "\u2581to", "\u2581use", "\u2581any", "\u2581ins", "ult", "ing"]], "entropy": [[5.554866790771484, 1.8587840795516968, 0.3000180125236511, 3.6387834548950195, 3.1187357902526855, 4.974582672119141, 1.1436243057250977, 4.600029945373535, 3.485870599746704, 3.7085537910461426, 3.212066650390625, 0.7101401090621948, 3.529214859008789, 2.230985641479492, 1.7800135612487793, 2.2810158729553223, 0.35249143838882446, 1.8723533153533936, 1.1859838962554932, 1.6348226070404053, 1.3033480644226074, 0.9470480680465698, 2.121243953704834, 0.10768609493970871, 1.3511265516281128, 2.0575523376464844, 1.0717922449111938, 0.5605027079582214, 0.6326663494110107, 3.401818037033081, 1.15346360206604, 4.0648417472839355, 2.5747687816619873, 1.7672837972640991, 2.43339204788208, 0.26241084933280945, 0.7373279333114624, 1.531453251838684, 1.7305982112884521, 3.8437533378601074, 2.424736261367798, 0.28875935077667236, 3.2255594730377197, 4.040960311889648, 3.8766279220581055, 0.05542758107185364, 0.5950769186019897]], "avg_entropy": [2.1134928336485905], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "CARDS", "score_of_biggest_cluster": 0.09312903136014938, "score_of_first_answer": 0.09312903136014938, "max_score_of_first_answer": 0.9935048818588257, "min_score_of_first_answer": 1.6428946310043102e-06, "avg_score": 0.09312903136014938, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3551283537705334, "std_of_first_answer": 0.3551283537705334, "avg_entropy_of_first_answer": 2.1134928336485905, "lex_sim": 1.0}
{"question": "What is the physics term for mass times velocity?", "answer": ["Momentum"], "scores": [[0.4307471215724945, 0.936021625995636, 0.9914616346359253]], "normalized_score": [0.7366503477096558], "tokens": [["\u2581M", "oment", "um"]], "entropy": [[3.0357680320739746, 0.544830322265625, 0.08017598092556]], "avg_entropy": [1.2202581117550533], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Momentum (physics)", "score_of_biggest_cluster": 0.7366503477096558, "score_of_first_answer": 0.7366503477096558, "max_score_of_first_answer": 0.9914616346359253, "min_score_of_first_answer": 0.4307471215724945, "avg_score": 0.7366503477096558, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.252273371241518, "std_of_first_answer": 0.252273371241518, "avg_entropy_of_first_answer": 1.2202581117550533, "lex_sim": 1.0}
{"question": "What instrument was played by jazz musician Bix Beiderbeck?", "answer": ["Cornet"], "scores": [[0.09774088859558105, 0.9867140650749207]], "normalized_score": [0.31055158376693726], "tokens": [["\u2581Cor", "net"]], "entropy": [[3.021209716796875, 0.09685850143432617]], "avg_entropy": [1.5590341091156006], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Trumpet.", "score_of_biggest_cluster": 0.31055158376693726, "score_of_first_answer": 0.31055158376693726, "max_score_of_first_answer": 0.9867140650749207, "min_score_of_first_answer": 0.09774088859558105, "avg_score": 0.31055158376693726, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4444865882396698, "std_of_first_answer": 0.4444865882396698, "avg_entropy_of_first_answer": 1.5590341091156006, "lex_sim": 1.0}
{"question": "What was the most common type of gun used in the Revolutionary War?", "answer": ["Rifle"], "scores": [[0.08293996006250381, 0.9960333704948425, 0.7438742518424988]], "normalized_score": [0.39462000131607056], "tokens": [["\u2581R", "if", "le"]], "entropy": [[3.4821853637695312, 0.03792719542980194, 0.6439265012741089]], "avg_entropy": [1.388013020157814], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Smoothbore musket", "score_of_biggest_cluster": 0.39462000131607056, "score_of_first_answer": 0.39462000131607056, "max_score_of_first_answer": 0.9960333704948425, "min_score_of_first_answer": 0.08293996006250381, "avg_score": 0.39462000131607056, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3850191821116348, "std_of_first_answer": 0.3850191821116348, "avg_entropy_of_first_answer": 1.388013020157814, "lex_sim": 1.0}
{"question": "What is the pleasant activity of osculation?", "answer": ["Osculation is an activity, often pleasant, of making contact with another individual by kissing, rubbing the face in proximity, and various other acts which involve the transfer of saliva, sweat, oil, or other bodily flu"], "scores": [[0.04276074096560478, 0.8183562159538269, 0.7575549483299255, 0.11852069199085236, 0.02539685368537903, 0.27705681324005127, 0.014363912865519524, 0.020646505057811737, 0.4549166262149811, 0.7423372268676758, 0.4651830494403839, 0.009821354411542416, 0.4839611053466797, 0.7221143245697021, 0.33358705043792725, 0.017231201753020287, 0.17040929198265076, 0.2534790635108948, 0.9952040910720825, 0.21494652330875397, 0.05030103772878647, 0.9997097849845886, 0.058988284319639206, 0.15560781955718994, 0.006439144257456064, 0.0009716518106870353, 0.9964050054550171, 0.12715816497802734, 0.060723066329956055, 0.005741378292441368, 0.5931410789489746, 0.02232709899544716, 0.020854245871305466, 0.10125577449798584, 0.26145702600479126, 0.033294904977083206, 0.9339883327484131, 0.29038000106811523, 0.993212103843689, 0.1148095428943634, 0.033502377569675446, 0.9981780052185059, 0.8158632516860962, 0.005762932356446981, 0.8099246025085449, 0.4908013641834259, 0.5811097025871277, 0.43442302942276, 0.9997368454933167, 0.6955723166465759]], "normalized_score": [0.1465301364660263], "tokens": [["\u2581O", "sc", "ulation", "\u2581is", "\u2581an", "\u2581activity", ",", "\u2581often", "\u2581pleasant", ",", "\u2581of", "\u2581making", "\u2581contact", "\u2581with", "\u2581another", "\u2581individual", "\u2581by", "\u2581kiss", "ing", ",", "\u2581rub", "bing", "\u2581the", "\u2581face", "\u2581in", "\u2581proxim", "ity", ",", "\u2581and", "\u2581various", "\u2581other", "\u2581acts", "\u2581which", "\u2581involve", "\u2581the", "\u2581transfer", "\u2581of", "\u2581sal", "iva", ",", "\u2581swe", "at", ",", "\u2581oil", ",", "\u2581or", "\u2581other", "\u2581bod", "ily", "\u2581flu"]], "entropy": [[3.2158560752868652, 1.2314453125, 0.9884615540504456, 1.2927263975143433, 2.7704837322235107, 3.7613985538482666, 3.0118885040283203, 4.706630229949951, 3.149339199066162, 1.2179354429244995, 2.4064340591430664, 2.325592279434204, 2.872239112854004, 1.418529987335205, 2.445932149887085, 2.1604702472686768, 2.8147900104522705, 3.0127429962158203, 0.041382867842912674, 2.036917209625244, 3.7768630981445312, 0.003503810614347458, 1.7657285928726196, 2.3188817501068115, 1.8341643810272217, 3.040827989578247, 0.032886307686567307, 1.1248222589492798, 2.1285133361816406, 3.907777786254883, 2.2880215644836426, 3.677051067352295, 2.309037208557129, 4.015450477600098, 3.6071252822875977, 3.528414249420166, 0.3906090259552002, 3.508481025695801, 0.05282080918550491, 2.2758398056030273, 4.592354774475098, 0.017055287957191467, 0.674791693687439, 2.205889940261841, 0.8305227756500244, 2.2460451126098633, 2.4530935287475586, 2.2061989307403564, 0.0032339240424335003, 1.266312837600708]], "avg_entropy": [2.1792702910955994], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kissings", "score_of_biggest_cluster": 0.1465301364660263, "score_of_first_answer": 0.1465301364660263, "max_score_of_first_answer": 0.9997368454933167, "min_score_of_first_answer": 0.0009716518106870353, "avg_score": 0.1465301364660263, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.35690114939545314, "std_of_first_answer": 0.35690114939545314, "avg_entropy_of_first_answer": 2.1792702910955994, "lex_sim": 1.0}
{"question": "In which year was the battle of Rorke's Drift?", "answer": ["January 1879"], "scores": [[0.02276252582669258, 0.752641499042511, 0.34590792655944824, 0.8600954413414001, 0.9184074401855469, 0.9857447743415833]], "normalized_score": [0.40802422165870667], "tokens": [["\u2581January", "\u2581", "1", "8", "7", "9"]], "entropy": [[2.1416990756988525, 1.0553739070892334, 0.7286509275436401, 0.659016489982605, 0.47151440382003784, 0.10594183951616287]], "avg_entropy": [0.8603661072750887], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, eight hundred and seventy-nine", "score_of_biggest_cluster": 0.40802422165870667, "score_of_first_answer": 0.40802422165870667, "max_score_of_first_answer": 0.9857447743415833, "min_score_of_first_answer": 0.02276252582669258, "avg_score": 0.40802422165870667, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3476843787169029, "std_of_first_answer": 0.3476843787169029, "avg_entropy_of_first_answer": 0.8603661072750887, "lex_sim": 1.0}
{"question": "What is the generic word for plants that grow in water or water-logged conditions?", "answer": ["Hydrophyte"], "scores": [[0.6541305184364319, 0.9586629867553711, 0.6465498208999634, 0.15898644924163818]], "normalized_score": [0.5038753747940063], "tokens": [["\u2581H", "ydro", "phy", "te"]], "entropy": [[1.7042523622512817, 0.2063835710287094, 1.3732287883758545, 0.5661476850509644]], "avg_entropy": [0.9625031016767025], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hydrophyte", "score_of_biggest_cluster": 0.5038753747940063, "score_of_first_answer": 0.5038753747940063, "max_score_of_first_answer": 0.9586629867553711, "min_score_of_first_answer": 0.15898644924163818, "avg_score": 0.5038753747940063, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2864197504131192, "std_of_first_answer": 0.2864197504131192, "avg_entropy_of_first_answer": 0.9625031016767025, "lex_sim": 1.0}
{"question": "What name besides glove is given commonly to a baseball player's handwear?", "answer": ["Baseball glove"], "scores": [[0.0227064099162817, 0.30401700735092163, 0.9271150231361389]], "normalized_score": [0.18566353619098663], "tokens": [["\u2581Baseball", "\u2581glo", "ve"]], "entropy": [[3.091647148132324, 2.151693344116211, 0.2672005295753479]], "avg_entropy": [1.8368470072746277], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mitt", "score_of_biggest_cluster": 0.18566353619098663, "score_of_first_answer": 0.18566353619098663, "max_score_of_first_answer": 0.9271150231361389, "min_score_of_first_answer": 0.0227064099162817, "avg_score": 0.18566353619098663, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3779097083770693, "std_of_first_answer": 0.3779097083770693, "avg_entropy_of_first_answer": 1.8368470072746277, "lex_sim": 1.0}
{"question": "What is the main attraction at Taronga Park, Sydney?", "answer": ["Western Plains Zoo"], "scores": [[0.007238691672682762, 0.37436792254447937, 0.9442085027694702, 0.8729891777038574, 0.9989789724349976]], "normalized_score": [0.2949296236038208], "tokens": [["\u2581Western", "\u2581Pla", "ins", "\u2581Z", "oo"]], "entropy": [[3.659930944442749, 3.547919750213623, 0.23196803033351898, 0.8565357327461243, 0.011208273470401764]], "avg_entropy": [1.6615125462412834], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Zoo museum", "score_of_biggest_cluster": 0.2949296236038208, "score_of_first_answer": 0.2949296236038208, "max_score_of_first_answer": 0.9989789724349976, "min_score_of_first_answer": 0.007238691672682762, "avg_score": 0.2949296236038208, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3864295443786243, "std_of_first_answer": 0.3864295443786243, "avg_entropy_of_first_answer": 1.6615125462412834, "lex_sim": 1.0}
{"question": "Which English clergyman/chemist discovered oxygen?", "answer": ["Joseph Priestley"], "scores": [[0.543014407157898, 0.9647328853607178, 0.999267041683197, 0.7525848746299744]], "normalized_score": [0.7922529578208923], "tokens": [["\u2581Joseph", "\u2581Pri", "est", "ley"]], "entropy": [[2.092149019241333, 0.25062641501426697, 0.008782348595559597, 0.6044466495513916]], "avg_entropy": [0.7390011081006378], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Priestley, Joseph", "score_of_biggest_cluster": 0.7922529578208923, "score_of_first_answer": 0.7922529578208923, "max_score_of_first_answer": 0.999267041683197, "min_score_of_first_answer": 0.543014407157898, "avg_score": 0.7922529578208923, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.18319801448116088, "std_of_first_answer": 0.18319801448116088, "avg_entropy_of_first_answer": 0.7390011081006378, "lex_sim": 1.0}
{"question": "How many dot positions are usually used in each letter of the Braille system?", "answer": ["6"], "scores": [[0.11354167014360428, 0.6924362778663635]], "normalized_score": [0.28039324283599854], "tokens": [["\u2581", "6"]], "entropy": [[2.454406261444092, 1.191374659538269]], "avg_entropy": [1.8228904604911804], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Six", "score_of_biggest_cluster": 0.28039324283599854, "score_of_first_answer": 0.28039324283599854, "max_score_of_first_answer": 0.6924362778663635, "min_score_of_first_answer": 0.11354167014360428, "avg_score": 0.28039324283599854, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2894473038613796, "std_of_first_answer": 0.2894473038613796, "avg_entropy_of_first_answer": 1.8228904604911804, "lex_sim": 1.0}
{"question": "Which is the film, about canine reincarnation and the relationships between father and son and master and dog, based on an Alan Sharp adaptation of a short novel by Irish author Lord Dunsany?", "answer": ["Beethoven"], "scores": [[0.003461571643128991, 0.646102786064148, 0.9852843880653381]], "normalized_score": [0.13013041019439697], "tokens": [["\u2581Be", "eth", "oven"]], "entropy": [[4.424378395080566, 1.9133694171905518, 0.1087217852473259]], "avg_entropy": [2.148823199172815], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dean Spanley", "score_of_biggest_cluster": 0.13013041019439697, "score_of_first_answer": 0.13013041019439697, "max_score_of_first_answer": 0.9852843880653381, "min_score_of_first_answer": 0.003461571643128991, "avg_score": 0.13013041019439697, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40715925528196173, "std_of_first_answer": 0.40715925528196173, "avg_entropy_of_first_answer": 2.148823199172815, "lex_sim": 1.0}
{"question": "Which early Bob Dylan album and its title song reference a Texan gunfighter?", "answer": ["New Morning"], "scores": [[0.002554144710302353, 0.5546213984489441, 0.9970280528068542]], "normalized_score": [0.11219750344753265], "tokens": [["\u2581New", "\u2581Mor", "ning"]], "entropy": [[4.169918060302734, 2.7556185722351074, 0.032143522053956985]], "avg_entropy": [2.319226718197266], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Wesley Harding (disambiguation)", "score_of_biggest_cluster": 0.11219750344753265, "score_of_first_answer": 0.11219750344753265, "max_score_of_first_answer": 0.9970280528068542, "min_score_of_first_answer": 0.002554144710302353, "avg_score": 0.11219750344753265, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40681421563104403, "std_of_first_answer": 0.40681421563104403, "avg_entropy_of_first_answer": 2.319226718197266, "lex_sim": 1.0}
{"question": "Which car manufacturer produced models called \u2018The Imp\u2019 and \u2018The Husky\u2019?", "answer": ["Ariel"], "scores": [[0.0071914768777787685, 0.1340334266424179]], "normalized_score": [0.031046710908412933], "tokens": [["\u2581A", "riel"]], "entropy": [[3.7909998893737793, 2.8835771083831787]], "avg_entropy": [3.337288498878479], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hillman", "score_of_biggest_cluster": 0.031046710908412933, "score_of_first_answer": 0.031046710908412933, "max_score_of_first_answer": 0.1340334266424179, "min_score_of_first_answer": 0.0071914768777787685, "avg_score": 0.031046710908412933, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06342097488231957, "std_of_first_answer": 0.06342097488231957, "avg_entropy_of_first_answer": 3.337288498878479, "lex_sim": 1.0}
{"question": "In which country were the 1948 Winter Olympics held?", "answer": ["Yugoslavia"], "scores": [[0.00618387944996357, 0.9722952842712402, 0.9893291592597961, 0.994057297706604]], "normalized_score": [0.2773019075393677], "tokens": [["\u2581Y", "ug", "oslav", "ia"]], "entropy": [[3.0174002647399902, 0.2772620916366577, 0.08394916355609894, 0.05695527791976929]], "avg_entropy": [0.858891699463129], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ISO 3166-1:CH", "score_of_biggest_cluster": 0.2773019075393677, "score_of_first_answer": 0.2773019075393677, "max_score_of_first_answer": 0.994057297706604, "min_score_of_first_answer": 0.00618387944996357, "avg_score": 0.2773019075393677, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42401546785282584, "std_of_first_answer": 0.42401546785282584, "avg_entropy_of_first_answer": 0.858891699463129, "lex_sim": 1.0}
{"question": "If you are standing at Tanjung Piai, the southernmost point of mainland Asia, you can see the skyline of what country across a strait?", "answer": ["Malaysia"], "scores": [[0.5148593783378601, 0.9904152154922485]], "normalized_score": [0.7140900492668152], "tokens": [["\u2581Malays", "ia"]], "entropy": [[2.297651767730713, 0.07193037867546082]], "avg_entropy": [1.1847910732030869], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Singapore City (disambiguation)", "score_of_biggest_cluster": 0.7140900492668152, "score_of_first_answer": 0.7140900492668152, "max_score_of_first_answer": 0.9904152154922485, "min_score_of_first_answer": 0.5148593783378601, "avg_score": 0.7140900492668152, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2377779185771942, "std_of_first_answer": 0.2377779185771942, "avg_entropy_of_first_answer": 1.1847910732030869, "lex_sim": 1.0}
{"question": "What is the US state capital of Louisiana?", "answer": ["Baton Rouge"], "scores": [[0.8899315595626831, 0.9955234527587891, 0.7559211850166321, 0.9990866184234619]], "normalized_score": [0.904423713684082], "tokens": [["\u2581Bat", "on", "\u2581Rou", "ge"]], "entropy": [[0.7478775978088379, 0.052258219569921494, 1.1397302150726318, 0.011676216498017311]], "avg_entropy": [0.48788556223735213], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Baton Rogue, West Florida", "score_of_biggest_cluster": 0.904423713684082, "score_of_first_answer": 0.904423713684082, "max_score_of_first_answer": 0.9990866184234619, "min_score_of_first_answer": 0.7559211850166321, "avg_score": 0.904423713684082, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09923917640298374, "std_of_first_answer": 0.09923917640298374, "avg_entropy_of_first_answer": 0.48788556223735213, "lex_sim": 1.0}
{"question": "An oropendola is what type of creature?", "answer": ["Puzzle bird"], "scores": [[0.0038120802491903305, 0.005202760919928551, 0.7021525502204895, 0.18699078261852264]], "normalized_score": [0.04017093777656555], "tokens": [["\u2581P", "uzz", "le", "\u2581bird"]], "entropy": [[2.901385545730591, 4.03148078918457, 1.2270612716674805, 2.5162410736083984]], "avg_entropy": [2.66904217004776], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Avians", "score_of_biggest_cluster": 0.04017093777656555, "score_of_first_answer": 0.04017093777656555, "max_score_of_first_answer": 0.7021525502204895, "min_score_of_first_answer": 0.0038120802491903305, "avg_score": 0.04017093777656555, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2856367145772913, "std_of_first_answer": 0.2856367145772913, "avg_entropy_of_first_answer": 2.66904217004776, "lex_sim": 1.0}
{"question": "In the Crimean War, what did Florence Nightingale begin to be called?", "answer": ["The Lady with the Lamp"], "scores": [[0.5731562972068787, 0.8777390122413635, 0.724138081073761, 0.9783041477203369, 0.9410059452056885, 0.9950931072235107]], "normalized_score": [0.8328465223312378], "tokens": [["\u2581The", "\u2581Lady", "\u2581with", "\u2581the", "\u2581L", "amp"]], "entropy": [[1.7991535663604736, 0.7222355604171753, 0.8932963609695435, 0.144717276096344, 0.335541307926178, 0.04060579091310501]], "avg_entropy": [0.6559249771138033], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Lady with the Lamp", "score_of_biggest_cluster": 0.8328465223312378, "score_of_first_answer": 0.8328465223312378, "max_score_of_first_answer": 0.9950931072235107, "min_score_of_first_answer": 0.5731562972068787, "avg_score": 0.8328465223312378, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.15222273141581624, "std_of_first_answer": 0.15222273141581624, "avg_entropy_of_first_answer": 0.6559249771138033, "lex_sim": 1.0}
{"question": "Which Roman Emperor was allegedy killed by his wife, Aggripina, in AD54?", "answer": ["Claudius"], "scores": [[0.47798454761505127, 0.9917090535163879]], "normalized_score": [0.6884922385215759], "tokens": [["\u2581Claud", "ius"]], "entropy": [[1.7805736064910889, 0.07228652387857437]], "avg_entropy": [0.9264300651848316], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Claudius", "score_of_biggest_cluster": 0.6884922385215759, "score_of_first_answer": 0.6884922385215759, "max_score_of_first_answer": 0.9917090535163879, "min_score_of_first_answer": 0.47798454761505127, "avg_score": 0.6884922385215759, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.25686225295066833, "std_of_first_answer": 0.25686225295066833, "avg_entropy_of_first_answer": 0.9264300651848316, "lex_sim": 1.0}
{"question": "Which group did Steve Marriot go on to form when he left the 'Small Faces'?", "answer": ["Humble Pie"], "scores": [[0.8792932629585266, 0.9873409867286682, 0.9875766038894653]], "normalized_score": [0.9500006437301636], "tokens": [["\u2581Hum", "ble", "\u2581Pie"]], "entropy": [[0.622044563293457, 0.11736199259757996, 0.11021945625543594]], "avg_entropy": [0.283208670715491], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Umbles", "score_of_biggest_cluster": 0.9500006437301636, "score_of_first_answer": 0.9500006437301636, "max_score_of_first_answer": 0.9875766038894653, "min_score_of_first_answer": 0.8792932629585266, "avg_score": 0.9500006437301636, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05098981167353293, "std_of_first_answer": 0.05098981167353293, "avg_entropy_of_first_answer": 0.283208670715491, "lex_sim": 1.0}
{"question": "Irish writer and poet Oscar Wilde, who died in 1900, is buried in which European country?", "answer": ["P\u00e8re Lachaise Cemetery, Paris, France"], "scores": [[0.017434127628803253, 0.09312963485717773, 0.6623589396476746, 0.9981924891471863, 0.9914297461509705, 0.9996538162231445, 0.5510990619659424, 0.9545630812644958, 0.6443665027618408, 0.7901856899261475, 0.49439606070518494, 0.9793376922607422]], "normalized_score": [0.476779580116272], "tokens": [["\u2581P", "\u00e8re", "\u2581L", "ach", "a", "ise", "\u2581C", "emetery", ",", "\u2581Paris", ",", "\u2581France"]], "entropy": [[3.0955734252929688, 4.81947135925293, 0.9511554837226868, 0.02281716652214527, 0.07857926189899445, 0.0044481889344751835, 1.4685084819793701, 0.2517929673194885, 1.1806963682174683, 0.6793792843818665, 0.8550267219543457, 0.1866992861032486]], "avg_entropy": [1.1328456662983324], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "La Republique francaise", "score_of_biggest_cluster": 0.476779580116272, "score_of_first_answer": 0.476779580116272, "max_score_of_first_answer": 0.9996538162231445, "min_score_of_first_answer": 0.017434127628803253, "avg_score": 0.476779580116272, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3308544740864723, "std_of_first_answer": 0.3308544740864723, "avg_entropy_of_first_answer": 1.1328456662983324, "lex_sim": 1.0}
{"question": "Which actor played Michael Corleone in the Godfather series of films", "answer": ["Al Pacino"], "scores": [[0.8425668478012085, 0.9325908422470093, 0.9957889914512634]], "normalized_score": [0.92148357629776], "tokens": [["\u2581Al", "\u2581Pac", "ino"]], "entropy": [[1.0112090110778809, 0.47360479831695557, 0.040611930191516876]], "avg_entropy": [0.5084752465287844], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Al pacino", "score_of_biggest_cluster": 0.92148357629776, "score_of_first_answer": 0.92148357629776, "max_score_of_first_answer": 0.9957889914512634, "min_score_of_first_answer": 0.8425668478012085, "avg_score": 0.92148357629776, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06287143048718004, "std_of_first_answer": 0.06287143048718004, "avg_entropy_of_first_answer": 0.5084752465287844, "lex_sim": 1.0}
{"question": "Which geological perioed falls between the Triassic and the Cretaceous periods?", "answer": ["Jurassic"], "scores": [[0.8689997792243958, 0.9900640845298767, 0.9922927618026733]], "normalized_score": [0.9486534595489502], "tokens": [["\u2581Jur", "ass", "ic"]], "entropy": [[0.917790412902832, 0.10026435554027557, 0.07594634592533112]], "avg_entropy": [0.3646670381228129], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dinosaur types of the jurassic", "score_of_biggest_cluster": 0.9486534595489502, "score_of_first_answer": 0.9486534595489502, "max_score_of_first_answer": 0.9922927618026733, "min_score_of_first_answer": 0.8689997792243958, "avg_score": 0.9486534595489502, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05760275126003815, "std_of_first_answer": 0.05760275126003815, "avg_entropy_of_first_answer": 0.3646670381228129, "lex_sim": 1.0}
{"question": "\"\"\"Wuthering Heights\"\" by Emily Bront tells the story of which two lovers?\"", "answer": ["??????"], "scores": [[0.0008666110807098448, 0.32361316680908203, 0.23130513727664948, 0.6046477556228638]], "normalized_score": [0.0791378989815712], "tokens": [["\u2581?", "??", "??", "??"]], "entropy": [[1.8930754661560059, 2.703214168548584, 2.234872579574585, 1.5804039239883423]], "avg_entropy": [2.1028915345668793], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Catherine and Heathcliff", "score_of_biggest_cluster": 0.0791378989815712, "score_of_first_answer": 0.0791378989815712, "max_score_of_first_answer": 0.6046477556228638, "min_score_of_first_answer": 0.0008666110807098448, "avg_score": 0.0791378989815712, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21631933096611133, "std_of_first_answer": 0.21631933096611133, "avg_entropy_of_first_answer": 2.1028915345668793, "lex_sim": 0.0}
{"question": "Who retired from the \u2018Eggheads\u2019 team in May 2014?", "answer": ["CJ de Mooi"], "scores": [[0.06261364370584488, 0.7714483141899109, 0.8567585945129395, 0.9833833575248718, 0.9997192025184631, 0.9711769819259644]], "normalized_score": [0.5836093425750732], "tokens": [["\u2581C", "J", "\u2581de", "\u2581M", "oo", "i"]], "entropy": [[5.521087646484375, 1.1423759460449219, 0.659894585609436, 0.1481672078371048, 0.003829473629593849, 0.18926402926445007]], "avg_entropy": [1.2774364814783137], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "DAPHNE Fowler", "score_of_biggest_cluster": 0.5836093425750732, "score_of_first_answer": 0.5836093425750732, "max_score_of_first_answer": 0.9997192025184631, "min_score_of_first_answer": 0.06261364370584488, "avg_score": 0.5836093425750732, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3282771068477116, "std_of_first_answer": 0.3282771068477116, "avg_entropy_of_first_answer": 1.2774364814783137, "lex_sim": 1.0}
{"question": "Which are the only two colours to appear on the flags of Ukraine and Palau?", "answer": ["PURPLE AND WHITE"], "scores": [[0.0025032805278897285, 0.005376026034355164, 0.9564635157585144, 0.9941269159317017, 0.23811869323253632, 0.3021315932273865, 0.9961428046226501]], "normalized_score": [0.1372412145137787], "tokens": [["\u2581P", "UR", "P", "LE", "\u2581AND", "\u2581WH", "ITE"]], "entropy": [[3.3434674739837646, 1.5303603410720825, 0.3562721312046051, 0.04788247495889664, 1.924403429031372, 2.4395530223846436, 0.0299935694783926]], "avg_entropy": [1.3817046345876796], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Yellow & Blue", "score_of_biggest_cluster": 0.1372412145137787, "score_of_first_answer": 0.1372412145137787, "max_score_of_first_answer": 0.9961428046226501, "min_score_of_first_answer": 0.0025032805278897285, "avg_score": 0.1372412145137787, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43070769828123556, "std_of_first_answer": 0.43070769828123556, "avg_entropy_of_first_answer": 1.3817046345876796, "lex_sim": 1.0}
{"question": "\"Sportscaster Jack Brickhouse who once said \"\"Any team can have a bad century\"\" provided play-by-play coverage of what American major league team's games from 1948 to 1981?\"", "answer": ["The Chicago Cubs"], "scores": [[0.20381489396095276, 0.742861270904541, 0.7302806973457336, 0.9973646998405457]], "normalized_score": [0.576264500617981], "tokens": [["\u2581The", "\u2581Chicago", "\u2581C", "ubs"]], "entropy": [[2.110539436340332, 1.2864513397216797, 0.7731958627700806, 0.03239951282739639]], "avg_entropy": [1.0506465379148722], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chicaco Cubs", "score_of_biggest_cluster": 0.576264500617981, "score_of_first_answer": 0.576264500617981, "max_score_of_first_answer": 0.9973646998405457, "min_score_of_first_answer": 0.20381489396095276, "avg_score": 0.576264500617981, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2887172777563971, "std_of_first_answer": 0.2887172777563971, "avg_entropy_of_first_answer": 1.0506465379148722, "lex_sim": 1.0}
{"question": "American businessman and philanthropist Solomon Guggenheim (1861-1949) established a famous eponymous international network of?", "answer": ["Museums"], "scores": [[0.36383524537086487, 0.955245316028595]], "normalized_score": [0.5895353555679321], "tokens": [["\u2581Museum", "s"]], "entropy": [[2.1247987747192383, 0.23270165920257568]], "avg_entropy": [1.178750216960907], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "History of museums", "score_of_biggest_cluster": 0.5895353555679321, "score_of_first_answer": 0.5895353555679321, "max_score_of_first_answer": 0.955245316028595, "min_score_of_first_answer": 0.36383524537086487, "avg_score": 0.5895353555679321, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29570503532886505, "std_of_first_answer": 0.29570503532886505, "avg_entropy_of_first_answer": 1.178750216960907, "lex_sim": 1.0}
{"question": "What is vellum parchment made from", "answer": ["Pig skin that appears velvety"], "scores": [[0.020414212718605995, 0.40478867292404175, 0.22742725908756256, 0.0007530945003964007, 0.00015923933824524283, 0.0018489818321540952, 0.8517364859580994, 0.9882408380508423]], "normalized_score": [0.027741210535168648], "tokens": [["\u2581P", "ig", "\u2581skin", "\u2581that", "\u2581appears", "\u2581vel", "v", "ety"]], "entropy": [[4.107259750366211, 2.757767677307129, 1.6607537269592285, 0.576529860496521, 1.323242425918579, 3.2693395614624023, 0.49766606092453003, 0.08963809162378311]], "avg_entropy": [1.785274644382298], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Calfskin", "score_of_biggest_cluster": 0.027741210535168648, "score_of_first_answer": 0.027741210535168648, "max_score_of_first_answer": 0.9882408380508423, "min_score_of_first_answer": 0.00015923933824524283, "avg_score": 0.027741210535168648, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3774444666859373, "std_of_first_answer": 0.3774444666859373, "avg_entropy_of_first_answer": 1.785274644382298, "lex_sim": 1.0}
{"question": "Alan Coren, who died in 2007, and Sandi Tostvig were rival captains on which TV game show?", "answer": ["It's A Knockout"], "scores": [[0.014720109291374683, 0.8312656879425049, 0.9382750988006592, 0.20390556752681732, 0.9080455899238586, 0.9996358156204224, 0.9382868409156799]], "normalized_score": [0.41137903928756714], "tokens": [["\u2581It", "'", "s", "\u2581A", "\u2581Kno", "ck", "out"]], "entropy": [[4.543185234069824, 1.1897852420806885, 0.2680339515209198, 1.5409479141235352, 0.7022767066955566, 0.0038300154265016317, 0.2940961718559265]], "avg_entropy": [1.2203078908247076], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Call My Bluff", "score_of_biggest_cluster": 0.41137903928756714, "score_of_first_answer": 0.41137903928756714, "max_score_of_first_answer": 0.9996358156204224, "min_score_of_first_answer": 0.014720109291374683, "avg_score": 0.41137903928756714, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3739651202341673, "std_of_first_answer": 0.3739651202341673, "avg_entropy_of_first_answer": 1.2203078908247076, "lex_sim": 1.0}
{"question": "Where was the first permanent European settlement established in North America?", "answer": ["St. Augustine"], "scores": [[0.24033115804195404, 0.39408114552497864, 0.33160078525543213, 0.9937765598297119]], "normalized_score": [0.42031511664390564], "tokens": [["\u2581St", ".", "\u2581August", "ine"]], "entropy": [[3.7588634490966797, 1.4075350761413574, 1.4107308387756348, 0.0525372140109539]], "avg_entropy": [1.6574166445061564], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Floreda", "score_of_biggest_cluster": 0.42031511664390564, "score_of_first_answer": 0.42031511664390564, "max_score_of_first_answer": 0.9937765598297119, "min_score_of_first_answer": 0.24033115804195404, "avg_score": 0.42031511664390564, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.295979762691685, "std_of_first_answer": 0.295979762691685, "avg_entropy_of_first_answer": 1.6574166445061564, "lex_sim": 1.0}
{"question": "The Yalu river forms a sort of natural border between China and which of its neighbours?", "answer": ["North Korea"], "scores": [[0.8023711442947388, 0.9281983971595764]], "normalized_score": [0.8629945516586304], "tokens": [["\u2581North", "\u2581Korea"]], "entropy": [[1.092917561531067, 0.4590728282928467]], "avg_entropy": [0.7759951949119568], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Korea north", "score_of_biggest_cluster": 0.8629945516586304, "score_of_first_answer": 0.8629945516586304, "max_score_of_first_answer": 0.9281983971595764, "min_score_of_first_answer": 0.8023711442947388, "avg_score": 0.8629945516586304, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06291362643241882, "std_of_first_answer": 0.06291362643241882, "avg_entropy_of_first_answer": 0.7759951949119568, "lex_sim": 1.0}
{"question": "What physics term refers to the largely prohibited selling method by which customers are said to have agreed to buy something unless they refuse unsolicited goods or services?", "answer": ["Deceptive Advertising Techniques"], "scores": [[0.003623054828494787, 0.7632378935813904, 0.8471693396568298, 0.011445355601608753, 0.9881020784378052, 0.9168574213981628, 0.026703478768467903, 0.9963200092315674, 0.5677767395973206]], "normalized_score": [0.19273187220096588], "tokens": [["\u2581De", "cept", "ive", "\u2581Ad", "vert", "ising", "\u2581Te", "chni", "ques"]], "entropy": [[5.569693565368652, 1.0889544486999512, 0.5135162472724915, 3.6238245964050293, 0.10432595014572144, 0.3700351417064667, 1.1985485553741455, 0.03162456303834915, 0.7114040851593018]], "avg_entropy": [1.46799190590779], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Principle of inertia", "score_of_biggest_cluster": 0.19273187220096588, "score_of_first_answer": 0.19273187220096588, "max_score_of_first_answer": 0.9963200092315674, "min_score_of_first_answer": 0.003623054828494787, "avg_score": 0.19273187220096588, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41082106779018285, "std_of_first_answer": 0.41082106779018285, "avg_entropy_of_first_answer": 1.46799190590779, "lex_sim": 1.0}
{"question": "\"In the TV version of \"\"The Odd Couple\"\", the part of Felix Ungar was played by Tony Randall. Which actor played the part in the original 1968 film?\"", "answer": ["Walter Matthau"], "scores": [[0.5873280167579651, 0.9671693444252014, 0.9950898885726929]], "normalized_score": [0.8268280029296875], "tokens": [["\u2581Walter", "\u2581Matth", "au"]], "entropy": [[1.2194316387176514, 0.22167006134986877, 0.04947013407945633]], "avg_entropy": [0.49685727804899216], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "JACK LEMMON", "score_of_biggest_cluster": 0.8268280029296875, "score_of_first_answer": 0.8268280029296875, "max_score_of_first_answer": 0.9950898885726929, "min_score_of_first_answer": 0.5873280167579651, "avg_score": 0.8268280029296875, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1859894665760974, "std_of_first_answer": 0.1859894665760974, "avg_entropy_of_first_answer": 0.49685727804899216, "lex_sim": 1.0}
{"question": "Who was the last man to wim the Men's Singles Championship at Wimbledon prior to Roger Federer?", "answer": ["Pete Sampras"], "scores": [[0.22156693041324615, 0.9981722831726074, 0.988403856754303, 0.9877732396125793, 0.9954209923744202]], "normalized_score": [0.735295295715332], "tokens": [["\u2581P", "ete", "\u2581Sam", "pr", "as"]], "entropy": [[3.520413398742676, 0.020413078367710114, 0.12487614154815674, 0.12208966165781021, 0.036628417670726776]], "avg_entropy": [0.764884139597416], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Leyton Hewitt", "score_of_biggest_cluster": 0.735295295715332, "score_of_first_answer": 0.735295295715332, "max_score_of_first_answer": 0.9981722831726074, "min_score_of_first_answer": 0.22156693041324615, "avg_score": 0.735295295715332, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30837614829040655, "std_of_first_answer": 0.30837614829040655, "avg_entropy_of_first_answer": 0.764884139597416, "lex_sim": 1.0}
{"question": "To which order of monks does or did Pope Francis belong?", "answer": ["Franciscan"], "scores": [[0.4267347455024719, 0.6496993899345398]], "normalized_score": [0.5265446901321411], "tokens": [["\u2581Francis", "can"]], "entropy": [[2.6037776470184326, 0.8842073678970337]], "avg_entropy": [1.7439925074577332], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Society of Jesuits", "score_of_biggest_cluster": 0.5265446901321411, "score_of_first_answer": 0.5265446901321411, "max_score_of_first_answer": 0.6496993899345398, "min_score_of_first_answer": 0.4267347455024719, "avg_score": 0.5265446901321411, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11148232221603394, "std_of_first_answer": 0.11148232221603394, "avg_entropy_of_first_answer": 1.7439925074577332, "lex_sim": 1.0}
{"question": "Which 2004 \"puppet\" movie, made by the makers of South Park, satirises Americas politics and jingoism?", "answer": ["Team America: World Police"], "scores": [[0.9787017107009888, 0.9857860803604126, 0.4123627543449402, 0.951831579208374, 0.9665298461914062]], "normalized_score": [0.8178949952125549], "tokens": [["\u2581Team", "\u2581America", ":", "\u2581World", "\u2581Police"]], "entropy": [[0.1883641630411148, 0.11614899337291718, 1.5407800674438477, 0.32068097591400146, 0.19567501544952393]], "avg_entropy": [0.472329843044281], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "America, fuck yeah", "score_of_biggest_cluster": 0.8178949952125549, "score_of_first_answer": 0.8178949952125549, "max_score_of_first_answer": 0.9857860803604126, "min_score_of_first_answer": 0.4123627543449402, "avg_score": 0.8178949952125549, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22363738500446745, "std_of_first_answer": 0.22363738500446745, "avg_entropy_of_first_answer": 0.472329843044281, "lex_sim": 1.0}
{"question": "King Robert Baratheon and Queen Cersei Lannister are characters in which US television fantasy drama series?", "answer": ["A Game of Thrones (TV series)"], "scores": [[0.06033303216099739, 0.5114374160766602, 0.9121456146240234, 0.9737304449081421, 0.986687183380127, 0.9986801743507385, 0.024943748489022255, 0.03998382389545441, 0.5775280594825745, 0.9367726445198059]], "normalized_score": [0.32836058735847473], "tokens": [["\u2581A", "\u2581Game", "\u2581of", "\u2581Th", "ron", "es", "\u2581(", "TV", "\u2581series", ")"]], "entropy": [[1.5912302732467651, 1.7478127479553223, 0.3252919912338257, 0.20500612258911133, 0.09160186350345612, 0.013938390649855137, 0.5655015707015991, 3.8813419342041016, 1.4841995239257812, 0.43234315514564514]], "avg_entropy": [1.0338267573155462], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A Game of Thrones (TV series)", "score_of_biggest_cluster": 0.32836058735847473, "score_of_first_answer": 0.32836058735847473, "max_score_of_first_answer": 0.9986801743507385, "min_score_of_first_answer": 0.024943748489022255, "avg_score": 0.32836058735847473, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.40035547925053827, "std_of_first_answer": 0.40035547925053827, "avg_entropy_of_first_answer": 1.0338267573155462, "lex_sim": 1.0}
{"question": "In which New York Borough is Central Park situated?", "answer": ["FRIGS"], "scores": [[0.00026983811403624713, 0.000654661504086107, 0.0023801440838724375]], "normalized_score": [0.0007491596043109894], "tokens": [["\u2581F", "RI", "GS"]], "entropy": [[1.6679298877716064, 5.63416051864624, 4.029654502868652]], "avg_entropy": [3.7772483030954995], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Manhattan, New York (state)", "score_of_biggest_cluster": 0.0007491596043109894, "score_of_first_answer": 0.0007491596043109894, "max_score_of_first_answer": 0.0023801440838724375, "min_score_of_first_answer": 0.00026983811403624713, "avg_score": 0.0007491596043109894, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0009176522253980427, "std_of_first_answer": 0.0009176522253980427, "avg_entropy_of_first_answer": 3.7772483030954995, "lex_sim": 1.0}
{"question": "By what name was the artist Jacopo Comin (or Jacopo Robusti) better known?", "answer": ["Tintoretto"], "scores": [[0.7459591627120972, 0.983741044998169, 0.9877923130989075, 0.9573462009429932]], "normalized_score": [0.9127095937728882], "tokens": [["\u2581T", "int", "or", "etto"]], "entropy": [[1.5676281452178955, 0.12476173043251038, 0.0894899070262909, 0.24445749819278717]], "avg_entropy": [0.506584320217371], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tintoretto, Il", "score_of_biggest_cluster": 0.9127095937728882, "score_of_first_answer": 0.9127095937728882, "max_score_of_first_answer": 0.9877923130989075, "min_score_of_first_answer": 0.7459591627120972, "avg_score": 0.9127095937728882, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.10042037965873722, "std_of_first_answer": 0.10042037965873722, "avg_entropy_of_first_answer": 0.506584320217371, "lex_sim": 1.0}
{"question": "What city will host the 2016 Olympics?", "answer": ["Rio de Janeiro"], "scores": [[0.88933265209198, 0.6557914018630981, 0.9237699508666992]], "normalized_score": [0.8137004971504211], "tokens": [["\u2581Rio", "\u2581de", "\u2581Janeiro"]], "entropy": [[0.9304873943328857, 1.1621131896972656, 0.3853514790534973]], "avg_entropy": [0.8259840210278829], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "S\u00e3o Sebasti\u00e3o do Rio de Janeiro", "score_of_biggest_cluster": 0.8137004971504211, "score_of_first_answer": 0.8137004971504211, "max_score_of_first_answer": 0.9237699508666992, "min_score_of_first_answer": 0.6557914018630981, "avg_score": 0.8137004971504211, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.11904245079963137, "std_of_first_answer": 0.11904245079963137, "avg_entropy_of_first_answer": 0.8259840210278829, "lex_sim": 1.0}
{"question": "Who co-starred with George Clooney in the 2013 movie Gravity?", "answer": ["Alfonso Cuar\u00f3n"], "scores": [[0.009834316559135914, 0.8644266724586487, 0.445692241191864, 0.24308380484580994]], "normalized_score": [0.17420710623264313], "tokens": [["\u2581Alfonso", "\u2581Cu", "ar", "\u00f3n"]], "entropy": [[0.5614895820617676, 0.9985324144363403, 1.1459369659423828, 1.3807992935180664]], "avg_entropy": [1.0216895639896393], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sandra Annette Bullock", "score_of_biggest_cluster": 0.17420710623264313, "score_of_first_answer": 0.17420710623264313, "max_score_of_first_answer": 0.8644266724586487, "min_score_of_first_answer": 0.009834316559135914, "avg_score": 0.17420710623264313, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31396269787919073, "std_of_first_answer": 0.31396269787919073, "avg_entropy_of_first_answer": 1.0216895639896393, "lex_sim": 1.0}
{"question": "Eric Arthur Blair, born 25 June, 1903, was the birth name of what famed English author, who penned such works as The Road To Wiggan Pier and Down and Out in Paris and London?", "answer": ["George Orwell"], "scores": [[0.902153730392456, 0.9801098108291626, 0.9579197764396667]], "normalized_score": [0.9461532235145569], "tokens": [["\u2581George", "\u2581Or", "well"]], "entropy": [[0.8256175518035889, 0.20014363527297974, 0.2793426513671875]], "avg_entropy": [0.4350346128145854], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Geoge orwell", "score_of_biggest_cluster": 0.9461532235145569, "score_of_first_answer": 0.9461532235145569, "max_score_of_first_answer": 0.9801098108291626, "min_score_of_first_answer": 0.902153730392456, "avg_score": 0.9461532235145569, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03279464725124258, "std_of_first_answer": 0.03279464725124258, "avg_entropy_of_first_answer": 0.4350346128145854, "lex_sim": 1.0}
{"question": "The equatorial Republic of Kiribati's 'Kiritimati' (Christmas) Island, among the first land to see each new day/year, is in which ocean?", "answer": ["Pacific Ocean"], "scores": [[0.6808406114578247, 0.059648189693689346]], "normalized_score": [0.20152147114276886], "tokens": [["\u2581Pacific", "\u2581Ocean"]], "entropy": [[1.7489416599273682, 0.5098714828491211]], "avg_entropy": [1.1294065713882446], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pacific Basin", "score_of_biggest_cluster": 0.20152147114276886, "score_of_first_answer": 0.20152147114276886, "max_score_of_first_answer": 0.6808406114578247, "min_score_of_first_answer": 0.059648189693689346, "avg_score": 0.20152147114276886, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3105962108820677, "std_of_first_answer": 0.3105962108820677, "avg_entropy_of_first_answer": 1.1294065713882446, "lex_sim": 1.0}
{"question": "Which city, since the 14th century BC, has been controlled by Israelites, Judaeans, Babylonians, Persians, Greeks, Hasmoneans, Romans, Byzantines, Arabs, Mamluks, Turks, and the British and for a short time after World War II was occupied by Israel and Jordan?", "answer": ["Hebron or Heba or Beth-El or Al-Khalil"], "scores": [[0.003468582173809409, 0.9811999201774597, 0.002461776603013277, 0.06832180917263031, 0.001331132953055203, 0.02253740094602108, 0.006142314523458481, 0.42918702960014343, 0.21203751862049103, 0.22348089516162872, 0.052166204899549484, 0.9018754959106445, 0.9230672121047974, 0.9689666628837585, 0.9806114435195923]], "normalized_score": [0.0850752517580986], "tokens": [["\u2581Heb", "ron", "\u2581or", "\u2581Heb", "a", "\u2581or", "\u2581Beth", "-", "El", "\u2581or", "\u2581Al", "-", "K", "hal", "il"]], "entropy": [[1.8288614749908447, 0.12475428730249405, 0.7124857306480408, 3.369029998779297, 1.3669859170913696, 1.7378406524658203, 4.5919952392578125, 2.3501458168029785, 2.8391945362091064, 1.597320556640625, 4.033081531524658, 0.7323493361473083, 0.5854425430297852, 0.18009260296821594, 0.17360670864582062]], "avg_entropy": [1.748212462166945], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Neighbourhoods of Jerusalem", "score_of_biggest_cluster": 0.0850752517580986, "score_of_first_answer": 0.0850752517580986, "max_score_of_first_answer": 0.9811999201774597, "min_score_of_first_answer": 0.001331132953055203, "avg_score": 0.0850752517580986, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4156069389921643, "std_of_first_answer": 0.4156069389921643, "avg_entropy_of_first_answer": 1.748212462166945, "lex_sim": 1.0}
{"question": "What type of beverage does a barista prepare and serve ?", "answer": ["Coffee"], "scores": [[0.4141148626804352, 0.8768178820610046, 0.9746266007423401]], "normalized_score": [0.7073312401771545], "tokens": [["\u2581C", "off", "ee"]], "entropy": [[2.8693482875823975, 0.6010537147521973, 0.17238560318946838]], "avg_entropy": [1.2142625351746876], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "COFFEE", "score_of_biggest_cluster": 0.7073312401771545, "score_of_first_answer": 0.7073312401771545, "max_score_of_first_answer": 0.9746266007423401, "min_score_of_first_answer": 0.4141148626804352, "avg_score": 0.7073312401771545, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2444572305467218, "std_of_first_answer": 0.2444572305467218, "avg_entropy_of_first_answer": 1.2142625351746876, "lex_sim": 1.0}
{"question": "Which 1989 music video of Madonna attracted criticism for showing images like her making love to Saint Martin de Porres, use of Catholic iconography including a scene where she develops stigmata as well as cross burning?", "answer": ["Like a Prayer"], "scores": [[0.44435012340545654, 0.8240354657173157, 0.6677417159080505, 0.9989657402038574, 0.9988743662834167]], "normalized_score": [0.7541682124137878], "tokens": [["\u2581Like", "\u2581a", "\u2581P", "ray", "er"]], "entropy": [[2.8743441104888916, 0.5473516583442688, 1.01993989944458, 0.008685242384672165, 0.013228476978838444]], "avg_entropy": [0.8927098775282503], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Like a Prayer", "score_of_biggest_cluster": 0.7541682124137878, "score_of_first_answer": 0.7541682124137878, "max_score_of_first_answer": 0.9989657402038574, "min_score_of_first_answer": 0.44435012340545654, "avg_score": 0.7541682124137878, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.21110334475610965, "std_of_first_answer": 0.21110334475610965, "avg_entropy_of_first_answer": 0.8927098775282503, "lex_sim": 1.0}
{"question": "What's missing: Amphibians, Fish, Mammals, Reptiles, Invertebrates?", "answer": ["Vertebrates"], "scores": [[0.011260585859417915, 0.9806563258171082, 0.985954225063324, 0.9582468867301941]], "normalized_score": [0.3195972144603729], "tokens": [["\u2581Ver", "te", "br", "ates"]], "entropy": [[3.189051628112793, 0.17219719290733337, 0.09874489158391953, 0.2039985954761505]], "avg_entropy": [0.9159980770200491], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Avians", "score_of_biggest_cluster": 0.3195972144603729, "score_of_first_answer": 0.3195972144603729, "max_score_of_first_answer": 0.985954225063324, "min_score_of_first_answer": 0.011260585859417915, "avg_score": 0.3195972144603729, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.417420410997991, "std_of_first_answer": 0.417420410997991, "avg_entropy_of_first_answer": 0.9159980770200491, "lex_sim": 1.0}
{"question": "The Buddh International Formula One circuit is in which country?", "answer": ["India"], "scores": [[0.7964908480644226]], "normalized_score": [0.7964908480644226], "tokens": [["\u2581India"]], "entropy": [[1.4678058624267578]], "avg_entropy": [1.4678058624267578], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u0d2d\u0d3e\u0d30\u0d24 \u0d2e\u0d39\u0d3e\u0d30\u0d3e\u0d1c\u0d4d\u0d2f\u0d02", "score_of_biggest_cluster": 0.7964908480644226, "score_of_first_answer": 0.7964908480644226, "max_score_of_first_answer": 0.7964908480644226, "min_score_of_first_answer": 0.7964908480644226, "avg_score": 0.7964908480644226, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.4678058624267578, "lex_sim": 1.0}
{"question": "'Baby Come Back' was a number one hit in 1968 for which group, founded by Eddy Grant?", "answer": ["The Equals"], "scores": [[0.4798874855041504, 0.9709725975990295, 0.9710261821746826]], "normalized_score": [0.7677016258239746], "tokens": [["\u2581The", "\u2581Equ", "als"]], "entropy": [[2.9875259399414062, 0.25894567370414734, 0.19905303418636322]], "avg_entropy": [1.1485082159439723], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Equally", "score_of_biggest_cluster": 0.7677016258239746, "score_of_first_answer": 0.7677016258239746, "max_score_of_first_answer": 0.9710261821746826, "min_score_of_first_answer": 0.4798874855041504, "avg_score": 0.7677016258239746, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23151237297383284, "std_of_first_answer": 0.23151237297383284, "avg_entropy_of_first_answer": 1.1485082159439723, "lex_sim": 1.0}
{"question": "Monterrey, Puebla and Toluca are cities in which country?", "answer": ["Mexico"], "scores": [[0.8883300423622131]], "normalized_score": [0.8883300423622131], "tokens": [["\u2581Mexico"]], "entropy": [[0.9628759026527405]], "avg_entropy": [0.9628759026527405], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mexican Republic", "score_of_biggest_cluster": 0.8883300423622131, "score_of_first_answer": 0.8883300423622131, "max_score_of_first_answer": 0.8883300423622131, "min_score_of_first_answer": 0.8883300423622131, "avg_score": 0.8883300423622131, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.9628759026527405, "lex_sim": 1.0}
{"question": "Which group of around 200 Greek islands in the Aegean Sea includes Andros, Naxos, Paros and Milos?", "answer": ["Cyclades"], "scores": [[0.8116365671157837, 0.9746026992797852, 0.9221863150596619]], "normalized_score": [0.9001936912536621], "tokens": [["\u2581Cy", "cl", "ades"]], "entropy": [[1.0119060277938843, 0.1805022656917572, 0.3136517405509949]], "avg_entropy": [0.5020200113455454], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cyclades Prefecture", "score_of_biggest_cluster": 0.9001936912536621, "score_of_first_answer": 0.9001936912536621, "max_score_of_first_answer": 0.9746026992797852, "min_score_of_first_answer": 0.8116365671157837, "avg_score": 0.9001936912536621, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06792699049360226, "std_of_first_answer": 0.06792699049360226, "avg_entropy_of_first_answer": 0.5020200113455454, "lex_sim": 1.0}
{"question": "What is the title of the last Harry Potter novel, published in 2007?", "answer": ["The Deathly Hallows (2007 novel)"], "scores": [[0.13828806579113007, 0.8199042677879333, 0.9951573014259338, 0.9212024807929993, 0.9964683055877686, 0.0033122373279184103, 0.035799939185380936, 0.9012622833251953, 0.9737528562545776, 0.9428060054779053, 0.01448849681764841, 0.8985678553581238]], "normalized_score": [0.2671754062175751], "tokens": [["\u2581The", "\u2581Death", "ly", "\u2581Hall", "ows", "\u2581(", "2", "0", "0", "7", "\u2581novel", ")"]], "entropy": [[1.8443862199783325, 1.364331841468811, 0.04707957059144974, 0.39309367537498474, 0.02454424649477005, 0.2511122226715088, 4.816734790802002, 0.6101033091545105, 0.15648986399173737, 0.3284131586551666, 0.4398835301399231, 0.5517131686210632]], "avg_entropy": [0.9023237998286883], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "HarryPotterandtheDeathlyHallows", "score_of_biggest_cluster": 0.2671754062175751, "score_of_first_answer": 0.2671754062175751, "max_score_of_first_answer": 0.9964683055877686, "min_score_of_first_answer": 0.0033122373279184103, "avg_score": 0.2671754062175751, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.41993316399927205, "std_of_first_answer": 0.41993316399927205, "avg_entropy_of_first_answer": 0.9023237998286883, "lex_sim": 1.0}
{"question": "John Lloyd Wright, son of famed architect Frank, invented what children's toy?", "answer": ["The kaleidoscope"], "scores": [[0.23117975890636444, 0.006969032343477011, 0.31331774592399597, 0.9399259686470032, 0.9963520765304565]], "normalized_score": [0.21623338758945465], "tokens": [["\u2581The", "\u2581k", "ale", "idos", "cope"]], "entropy": [[4.147884368896484, 4.777681350708008, 2.0172276496887207, 0.34107697010040283, 0.026492031291127205]], "avg_entropy": [2.2620724741369487], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lincoln log", "score_of_biggest_cluster": 0.21623338758945465, "score_of_first_answer": 0.21623338758945465, "max_score_of_first_answer": 0.9963520765304565, "min_score_of_first_answer": 0.006969032343477011, "avg_score": 0.21623338758945465, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39750832036338984, "std_of_first_answer": 0.39750832036338984, "avg_entropy_of_first_answer": 2.2620724741369487, "lex_sim": 1.0}
{"question": "Scone Castle shares its grounds with which racecourse?", "answer": ["Epsom"], "scores": [[0.021662071347236633, 0.8700375556945801, 0.9964814186096191]], "normalized_score": [0.2658086121082306], "tokens": [["\u2581E", "ps", "om"]], "entropy": [[4.242610931396484, 0.828188419342041, 0.04026079177856445]], "avg_entropy": [1.7036867141723633], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Perth, AU-WA", "score_of_biggest_cluster": 0.2658086121082306, "score_of_first_answer": 0.2658086121082306, "max_score_of_first_answer": 0.9964814186096191, "min_score_of_first_answer": 0.021662071347236633, "avg_score": 0.2658086121082306, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43282043655635766, "std_of_first_answer": 0.43282043655635766, "avg_entropy_of_first_answer": 1.7036867141723633, "lex_sim": 1.0}
{"question": "What was the actor Richard Burton's real surname?", "answer": ["Jenkins"], "scores": [[0.3869549334049225]], "normalized_score": [0.3869549334049225], "tokens": [["\u2581Jenkins"]], "entropy": [[3.0033721923828125]], "avg_entropy": [3.0033721923828125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jenkins", "score_of_biggest_cluster": 0.3869549334049225, "score_of_first_answer": 0.3869549334049225, "max_score_of_first_answer": 0.3869549334049225, "min_score_of_first_answer": 0.3869549334049225, "avg_score": 0.3869549334049225, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.0033721923828125, "lex_sim": 1.0}
{"question": "Played on TV by Gregor Fisher, whose boat was called The Vital Spark?", "answer": ["\u32e1?"], "scores": [[0.004045956768095493, 4.3504685891093686e-05, 0.014776878990232944, 0.1438564658164978, 0.00769163528457284]], "normalized_score": [0.004918328952044249], "tokens": [["\u2581", "<0xE3>", "<0x8B>", "<0xA1>", "?"]], "entropy": [[5.211797714233398, 3.8667349815368652, 2.700854539871216, 3.6612749099731445, 5.565099716186523]], "avg_entropy": [4.201152372360229], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Para Handy", "score_of_biggest_cluster": 0.004918328952044249, "score_of_first_answer": 0.004918328952044249, "max_score_of_first_answer": 0.1438564658164978, "min_score_of_first_answer": 4.3504685891093686e-05, "avg_score": 0.004918328952044249, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05510055434320864, "std_of_first_answer": 0.05510055434320864, "avg_entropy_of_first_answer": 4.201152372360229, "lex_sim": 0.0}
{"question": "\"What is the more common name of the \"\"simple plurality\"\" system of voting?\"", "answer": ["Alternate voting"], "scores": [[0.04768846929073334, 0.20008517801761627, 0.06526502966880798]], "normalized_score": [0.08539576828479767], "tokens": [["\u2581Altern", "ate", "\u2581voting"]], "entropy": [[3.9477500915527344, 0.8589820265769958, 2.6003847122192383]], "avg_entropy": [2.469038943449656], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "First-past-the-post electoral system", "score_of_biggest_cluster": 0.08539576828479767, "score_of_first_answer": 0.08539576828479767, "max_score_of_first_answer": 0.20008517801761627, "min_score_of_first_answer": 0.04768846929073334, "avg_score": 0.08539576828479767, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06807688848019583, "std_of_first_answer": 0.06807688848019583, "avg_entropy_of_first_answer": 2.469038943449656, "lex_sim": 1.0}
{"question": "Las Vegas is in which US State?", "answer": ["Nevada"], "scores": [[0.9199721813201904, 0.9956156015396118]], "normalized_score": [0.9570468664169312], "tokens": [["\u2581Nev", "ada"]], "entropy": [[0.6220961809158325, 0.0399211049079895]], "avg_entropy": [0.331008642911911], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Silver State", "score_of_biggest_cluster": 0.9570468664169312, "score_of_first_answer": 0.9570468664169312, "max_score_of_first_answer": 0.9956156015396118, "min_score_of_first_answer": 0.9199721813201904, "avg_score": 0.9570468664169312, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03782171010971069, "std_of_first_answer": 0.03782171010971069, "avg_entropy_of_first_answer": 0.331008642911911, "lex_sim": 1.0}
{"question": "Lee Kuan Yew and Goh Chok Tong were the first two Prime Ministers of which country?", "answer": ["Republic of Singapore"], "scores": [[0.0071364822797477245, 0.9550419449806213, 0.9813182950019836]], "normalized_score": [0.1884106695652008], "tokens": [["\u2581Republic", "\u2581of", "\u2581Singapore"]], "entropy": [[0.9747220873832703, 0.2574922442436218, 0.1573154628276825]], "avg_entropy": [0.46317659815152484], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Singapore City (disambiguation)", "score_of_biggest_cluster": 0.1884106695652008, "score_of_first_answer": 0.1884106695652008, "max_score_of_first_answer": 0.9813182950019836, "min_score_of_first_answer": 0.0071364822797477245, "avg_score": 0.1884106695652008, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45316730011324907, "std_of_first_answer": 0.45316730011324907, "avg_entropy_of_first_answer": 0.46317659815152484, "lex_sim": 1.0}
{"question": "Who wrote the 1930\u2019s novel \u2018Tender is the Night\u2019?", "answer": ["F Scott Fitzgerald"], "scores": [[0.7853811383247375, 0.2731863558292389, 0.9591161012649536, 0.9928746223449707, 0.9938527345657349]], "normalized_score": [0.726984977722168], "tokens": [["\u2581F", "\u2581Scott", "\u2581Fitz", "ger", "ald"]], "entropy": [[1.4326648712158203, 0.787488579750061, 0.25351619720458984, 0.06387703120708466, 0.05238399654626846]], "avg_entropy": [0.5179861351847649], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Francis Scott Key Fitzgerald", "score_of_biggest_cluster": 0.726984977722168, "score_of_first_answer": 0.726984977722168, "max_score_of_first_answer": 0.9938527345657349, "min_score_of_first_answer": 0.2731863558292389, "avg_score": 0.726984977722168, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2748962974745765, "std_of_first_answer": 0.2748962974745765, "avg_entropy_of_first_answer": 0.5179861351847649, "lex_sim": 1.0}
{"question": "What is the name of a bishop's staff that is shaped like a shepherd's crook?", "answer": ["Papal crook In the US, the average age of people at death is 77 years and 4 months."], "scores": [[0.004966001026332378, 0.866836667060852, 0.04484275355935097, 0.4669104516506195, 4.158573119639186e-06, 0.14752641320228577, 0.014116980135440826, 0.5751113295555115, 0.14397893846035004, 0.0019923667423427105, 0.05464223027229309, 0.608103334903717, 0.026595087721943855, 0.04734095185995102, 0.06687324494123459, 0.5113489031791687, 0.3068479597568512, 0.8628478050231934, 0.20339985191822052, 0.2392052412033081, 0.15678247809410095, 0.7474151253700256, 0.07485434412956238, 0.9171401858329773, 0.6660967469215393, 0.21598689258098602, 0.35758113861083984, 0.7183182835578918, 0.748173177242279, 0.5794183015823364, 0.9307186603546143, 0.8935680985450745, 0.22726956009864807, 0.9852442741394043, 0.9719179272651672, 0.7821851968765259, 0.27654916048049927, 0.3178364932537079, 0.8801673054695129, 0.9351123571395874, 0.9330217242240906, 0.0916341170668602, 0.9706690311431885, 0.9546190500259399, 0.003171030431985855, 0.045648396015167236, 0.45378953218460083, 0.018257521092891693, 0.25864842534065247]], "normalized_score": [0.17481499910354614], "tokens": [["\u2581Pap", "al", "\u2581cro", "ok", "\u2581In", "\u2581the", "\u2581US", ",", "\u2581the", "\u2581average", "\u2581age", "\u2581of", "\u2581people", "\u2581at", "\u2581death", "\u2581is", "\u2581", "7", "7", "\u2581years", "\u2581and", "\u2581", "4", "\u2581months", ".", "\u2581In", "\u2581the", "\u2581UK", ",", "\u2581the", "\u2581average", "\u2581age", "\u2581at", "\u2581death", "\u2581is", "\u2581", "8", "1", "\u2581years", "\u2581and", "\u2581", "5", "\u2581months", ".", "\u2581To", "\u2581find", "\u2581the", "\u2581age", "\u2581difference"]], "entropy": [[3.295787811279297, 1.0188920497894287, 3.428804874420166, 0.8384298086166382, 0.32174819707870483, 4.860202789306641, 5.573010444641113, 2.970075845718384, 3.960554838180542, 6.33894157409668, 5.0115814208984375, 1.4604170322418213, 3.4168925285339355, 3.9822826385498047, 2.647942543029785, 1.9733715057373047, 3.2873306274414062, 0.7046003937721252, 1.9446873664855957, 1.4140125513076782, 1.996900200843811, 1.3623156547546387, 2.082736015319824, 0.5389835834503174, 1.4220426082611084, 2.925096035003662, 3.0289804935455322, 1.4334938526153564, 0.9892100691795349, 1.4731674194335938, 0.4897479712963104, 0.5067098736763, 1.1414501667022705, 0.12517061829566956, 0.2194807231426239, 1.2278262376785278, 0.6782310009002686, 1.2286831140518188, 0.6058342456817627, 0.355114221572876, 0.4771738350391388, 2.171769618988037, 0.23261401057243347, 0.32151782512664795, 3.049656867980957, 3.7879812717437744, 1.8448317050933838, 3.5054688453674316, 2.523890495300293]], "avg_entropy": [2.044809090239661], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pateritsa", "score_of_biggest_cluster": 0.17481499910354614, "score_of_first_answer": 0.17481499910354614, "max_score_of_first_answer": 0.9852442741394043, "min_score_of_first_answer": 4.158573119639186e-06, "avg_score": 0.17481499910354614, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35564475597170486, "std_of_first_answer": 0.35564475597170486, "avg_entropy_of_first_answer": 2.044809090239661, "lex_sim": 1.0}
{"question": "The term \u2018Heavy metal\u2019 is said to have first appeared in which 1968 song?", "answer": ["Paranoid"], "scores": [[0.08932365477085114, 0.9755688309669495, 0.9916620254516602]], "normalized_score": [0.44210898876190186], "tokens": [["\u2581Par", "ano", "id"]], "entropy": [[5.339620113372803, 0.1867092251777649, 0.0643547847867012]], "avg_entropy": [1.8635613744457562], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Born to Be Wild", "score_of_biggest_cluster": 0.44210898876190186, "score_of_first_answer": 0.44210898876190186, "max_score_of_first_answer": 0.9916620254516602, "min_score_of_first_answer": 0.08932365477085114, "avg_score": 0.44210898876190186, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42162437712079337, "std_of_first_answer": 0.42162437712079337, "avg_entropy_of_first_answer": 1.8635613744457562, "lex_sim": 1.0}
{"question": "What kind of an organism is a truffle?", "answer": ["Earthworm"], "scores": [[0.0031680536922067404, 0.6544697284698486, 0.9986181259155273]], "normalized_score": [0.12745606899261475], "tokens": [["\u2581Earth", "w", "orm"]], "entropy": [[3.2285361289978027, 1.8145750761032104, 0.014391398057341576]], "avg_entropy": [1.685834201052785], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fungus kingdom", "score_of_biggest_cluster": 0.12745606899261475, "score_of_first_answer": 0.12745606899261475, "max_score_of_first_answer": 0.9986181259155273, "min_score_of_first_answer": 0.0031680536922067404, "avg_score": 0.12745606899261475, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41278900143105185, "std_of_first_answer": 0.41278900143105185, "avg_entropy_of_first_answer": 1.685834201052785, "lex_sim": 1.0}
{"question": "Where is the only place in the United States that has a royal palace?", "answer": ["Houston, Texas"], "scores": [[0.02343275211751461, 0.5888602137565613, 0.835614800453186]], "normalized_score": [0.22591614723205566], "tokens": [["\u2581Houston", ",", "\u2581Texas"]], "entropy": [[4.886777877807617, 1.0279960632324219, 0.8597676157951355]], "avg_entropy": [2.2581805189450583], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Honolulu, Hawaii, USA", "score_of_biggest_cluster": 0.22591614723205566, "score_of_first_answer": 0.22591614723205566, "max_score_of_first_answer": 0.835614800453186, "min_score_of_first_answer": 0.02343275211751461, "avg_score": 0.22591614723205566, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33997315581448234, "std_of_first_answer": 0.33997315581448234, "avg_entropy_of_first_answer": 2.2581805189450583, "lex_sim": 1.0}
{"question": "\"In which North American city would you find a baseball team called the\"\"Blue Jays\"\"?\"", "answer": ["Which North American City would you find a baseball team called the Blue Jays?"], "scores": [[0.0004028103721793741, 0.23177313804626465, 0.9702441692352295, 0.12263385951519012, 0.40620940923690796, 0.9512069225311279, 0.9909473061561584, 0.9642778635025024, 0.9175670146942139, 0.9953104853630066, 0.9850607514381409, 0.9586231112480164, 0.1570904552936554, 0.9711188077926636, 0.9996275901794434, 0.6959803700447083]], "normalized_score": [0.39664873480796814], "tokens": [["\u2581Which", "\u2581North", "\u2581American", "\u2581City", "\u2581would", "\u2581you", "\u2581find", "\u2581a", "\u2581baseball", "\u2581team", "\u2581called", "\u2581the", "\u2581Blue", "\u2581J", "ays", "?"]], "entropy": [[1.148359775543213, 4.050224304199219, 0.22731822729110718, 0.6678468585014343, 1.7069759368896484, 0.3376010060310364, 0.08754684776067734, 0.25946441292762756, 0.419929176568985, 0.04789178445935249, 0.1298484206199646, 0.28788769245147705, 2.2178938388824463, 0.2034335732460022, 0.004797365050762892, 1.1713285446166992]], "avg_entropy": [0.8105217353149783], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "New VR", "score_of_biggest_cluster": 0.39664873480796814, "score_of_first_answer": 0.39664873480796814, "max_score_of_first_answer": 0.9996275901794434, "min_score_of_first_answer": 0.0004028103721793741, "avg_score": 0.39664873480796814, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3673242836588193, "std_of_first_answer": 0.3673242836588193, "avg_entropy_of_first_answer": 0.8105217353149783, "lex_sim": 1.0}
{"question": "If Monday's child is fair of face what is Friday's child?", "answer": ["Togetherness"], "scores": [[0.006762327626347542, 0.0042572892270982265, 0.48183026909828186, 0.4599161148071289, 0.9956966638565063]], "normalized_score": [0.09132418781518936], "tokens": [["\u2581T", "og", "eth", "ern", "ess"]], "entropy": [[5.286775588989258, 4.09214973449707, 2.662320852279663, 2.475524663925171, 0.04040591046214104]], "avg_entropy": [2.9114353500306605], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "LOVING AND GIVING", "score_of_biggest_cluster": 0.09132418781518936, "score_of_first_answer": 0.09132418781518936, "max_score_of_first_answer": 0.9956966638565063, "min_score_of_first_answer": 0.0042572892270982265, "avg_score": 0.09132418781518936, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3676567937085, "std_of_first_answer": 0.3676567937085, "avg_entropy_of_first_answer": 2.9114353500306605, "lex_sim": 1.0}
{"question": "What is the common name for the perennial-flowering houseplant, 'Saintpaulia'?", "answer": ["African violet"], "scores": [[0.5936002135276794, 0.6603789329528809, 0.9475404620170593]], "normalized_score": [0.7188335657119751], "tokens": [["\u2581African", "\u2581vio", "let"]], "entropy": [[2.706371307373047, 0.9738696813583374, 0.23046670854091644]], "avg_entropy": [1.3035692324241002], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Saintpaulia ionantha", "score_of_biggest_cluster": 0.7188335657119751, "score_of_first_answer": 0.7188335657119751, "max_score_of_first_answer": 0.9475404620170593, "min_score_of_first_answer": 0.5936002135276794, "avg_score": 0.7188335657119751, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.15354870405157764, "std_of_first_answer": 0.15354870405157764, "avg_entropy_of_first_answer": 1.3035692324241002, "lex_sim": 1.0}
{"question": "In 1934, local miners in Ashington, Northumberland, formed a group to study what?", "answer": ["Nine Inch Nails"], "scores": [[0.0011473445920273662, 0.10749263316392899, 0.0014092253986746073, 0.8807302117347717, 0.9594738483428955, 0.9851226210594177]], "normalized_score": [0.07245524972677231], "tokens": [["\u2581N", "ine", "\u2581In", "ch", "\u2581N", "ails"]], "entropy": [[4.946739196777344, 4.322537899017334, 4.143568515777588, 0.7657510042190552, 0.37831008434295654, 0.08227633684873581]], "avg_entropy": [2.439863839497169], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ART (disambiguation)", "score_of_biggest_cluster": 0.07245524972677231, "score_of_first_answer": 0.07245524972677231, "max_score_of_first_answer": 0.9851226210594177, "min_score_of_first_answer": 0.0011473445920273662, "avg_score": 0.07245524972677231, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45501435658536005, "std_of_first_answer": 0.45501435658536005, "avg_entropy_of_first_answer": 2.439863839497169, "lex_sim": 1.0}
{"question": "The lead singer of which band is known as Suggs?", "answer": ["Madness"], "scores": [[0.09428137540817261, 0.9982781410217285]], "normalized_score": [0.30678826570510864], "tokens": [["\u2581Mad", "ness"]], "entropy": [[3.9039063453674316, 0.02146313339471817]], "avg_entropy": [1.962684739381075], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Madness", "score_of_biggest_cluster": 0.30678826570510864, "score_of_first_answer": 0.30678826570510864, "max_score_of_first_answer": 0.9982781410217285, "min_score_of_first_answer": 0.09428137540817261, "avg_score": 0.30678826570510864, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.45199838280677795, "std_of_first_answer": 0.45199838280677795, "avg_entropy_of_first_answer": 1.962684739381075, "lex_sim": 1.0}
{"question": "As at 2012 Dame Marjorie Scardino is/was head of what high-performing global media/learning corporation?", "answer": ["Pearson"], "scores": [[0.512166440486908, 0.998256504535675]], "normalized_score": [0.7150338888168335], "tokens": [["\u2581Pear", "son"]], "entropy": [[2.9597604274749756, 0.01876441016793251]], "avg_entropy": [1.489262418821454], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Parker Publishing Company, Inc.", "score_of_biggest_cluster": 0.7150338888168335, "score_of_first_answer": 0.7150338888168335, "max_score_of_first_answer": 0.998256504535675, "min_score_of_first_answer": 0.512166440486908, "avg_score": 0.7150338888168335, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24304503202438354, "std_of_first_answer": 0.24304503202438354, "avg_entropy_of_first_answer": 1.489262418821454, "lex_sim": 1.0}
{"question": "What modern popular pet population originated entirely from single female found in Syria in 1930?", "answer": ["A cat breed originating from one female"], "scores": [[0.03893763944506645, 0.05693632364273071, 0.012853671796619892, 0.9404336810112, 0.009181870147585869, 0.6735451221466064, 0.4624132215976715, 0.03519861400127411, 0.6906726360321045]], "normalized_score": [0.10715853422880173], "tokens": [["\u2581A", "\u2581cat", "\u2581bre", "ed", "\u2581origin", "ating", "\u2581from", "\u2581one", "\u2581female"]], "entropy": [[4.505370140075684, 5.412456512451172, 1.7786288261413574, 0.2886844277381897, 2.0553345680236816, 0.7508246898651123, 1.3779047727584839, 3.102771759033203, 1.6950527429580688]], "avg_entropy": [2.32966982656055], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "H\u00f6rcs\u00f6g", "score_of_biggest_cluster": 0.10715853422880173, "score_of_first_answer": 0.10715853422880173, "max_score_of_first_answer": 0.9404336810112, "min_score_of_first_answer": 0.009181870147585869, "avg_score": 0.10715853422880173, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3476412059037737, "std_of_first_answer": 0.3476412059037737, "avg_entropy_of_first_answer": 2.32966982656055, "lex_sim": 1.0}
{"question": "In January 2013 Novak Djokovi and Victoria Azarenka won the respective men's and women's titles in which Championship?", "answer": ["The Australian Open"], "scores": [[0.0535719208419323, 0.931509256362915, 0.8313700556755066]], "normalized_score": [0.3461834490299225], "tokens": [["\u2581The", "\u2581Australian", "\u2581Open"]], "entropy": [[1.1904969215393066, 0.5696753859519958, 0.6737445592880249]], "avg_entropy": [0.8113056222597758], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Australian Open - Main Draw Wildcard Entries", "score_of_biggest_cluster": 0.3461834490299225, "score_of_first_answer": 0.3461834490299225, "max_score_of_first_answer": 0.931509256362915, "min_score_of_first_answer": 0.0535719208419323, "avg_score": 0.3461834490299225, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39239602489301867, "std_of_first_answer": 0.39239602489301867, "avg_entropy_of_first_answer": 0.8113056222597758, "lex_sim": 1.0}
{"question": "Which Rugby League team plays home games at Derwent Park?", "answer": ["Newtown Jets"], "scores": [[0.2640020549297333, 0.07318073511123657, 0.666788637638092, 0.9995424747467041]], "normalized_score": [0.33685916662216187], "tokens": [["\u2581New", "town", "\u2581J", "ets"]], "entropy": [[3.5422277450561523, 1.1511764526367188, 1.3963831663131714, 0.005538867320865393]], "avg_entropy": [1.523831557831727], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Workington Town", "score_of_biggest_cluster": 0.33685916662216187, "score_of_first_answer": 0.33685916662216187, "max_score_of_first_answer": 0.9995424747467041, "min_score_of_first_answer": 0.07318073511123657, "avg_score": 0.33685916662216187, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35889683817262896, "std_of_first_answer": 0.35889683817262896, "avg_entropy_of_first_answer": 1.523831557831727, "lex_sim": 1.0}
{"question": "Which US state is nicknamed both 'the Colourful State' and 'the Land of Enchantment'?", "answer": ["NM"], "scores": [[0.001614362932741642, 0.31693923473358154]], "normalized_score": [0.022619793191552162], "tokens": [["\u2581N", "M"]], "entropy": [[0.8487127423286438, 3.8521275520324707]], "avg_entropy": [2.3504201471805573], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "State of New Mexico", "score_of_biggest_cluster": 0.022619793191552162, "score_of_first_answer": 0.022619793191552162, "max_score_of_first_answer": 0.31693923473358154, "min_score_of_first_answer": 0.001614362932741642, "avg_score": 0.022619793191552162, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.15766243590041995, "std_of_first_answer": 0.15766243590041995, "avg_entropy_of_first_answer": 2.3504201471805573, "lex_sim": 1.0}
{"question": "What is the forename of Lewis Hamilton\u2019s father, a prominent member of his entourage?", "answer": ["Anthony"], "scores": [[0.8633734583854675]], "normalized_score": [0.8633734583854675], "tokens": [["\u2581Anthony"]], "entropy": [[0.9759658575057983]], "avg_entropy": [0.9759658575057983], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Anthony", "score_of_biggest_cluster": 0.8633734583854675, "score_of_first_answer": 0.8633734583854675, "max_score_of_first_answer": 0.8633734583854675, "min_score_of_first_answer": 0.8633734583854675, "avg_score": 0.8633734583854675, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.9759658575057983, "lex_sim": 1.0}
{"question": "What is the last name name of the doctor who first described a certain disease in An Essay on the Shaking Palsy written in 1817?", "answer": ["Charcot"], "scores": [[0.3485701382160187, 0.8570745587348938]], "normalized_score": [0.5465807914733887], "tokens": [["\u2581Char", "cot"]], "entropy": [[4.078243255615234, 0.871894359588623]], "avg_entropy": [2.4750688076019287], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dr. (James) Parkinson", "score_of_biggest_cluster": 0.5465807914733887, "score_of_first_answer": 0.5465807914733887, "max_score_of_first_answer": 0.8570745587348938, "min_score_of_first_answer": 0.3485701382160187, "avg_score": 0.5465807914733887, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25425221025943756, "std_of_first_answer": 0.25425221025943756, "avg_entropy_of_first_answer": 2.4750688076019287, "lex_sim": 1.0}
{"question": "In which city can you visit the Luigi Ferraris Stadium, the gardens of the Villa Durazzo-Pallavicini, the Piazza De Ferrari, the cemetery of Staglieno, the Edoardo Chiossone Museum of Oriental Art and the Torre della Lanterna?", "answer": ["Genoa"], "scores": [[0.8525280952453613, 0.9048815965652466]], "normalized_score": [0.8783148527145386], "tokens": [["\u2581Gen", "oa"]], "entropy": [[1.0144366025924683, 0.40015900135040283]], "avg_entropy": [0.7072978019714355], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Quarto di Genova", "score_of_biggest_cluster": 0.8783148527145386, "score_of_first_answer": 0.8783148527145386, "max_score_of_first_answer": 0.9048815965652466, "min_score_of_first_answer": 0.8525280952453613, "avg_score": 0.8783148527145386, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.026176750659942627, "std_of_first_answer": 0.026176750659942627, "avg_entropy_of_first_answer": 0.7072978019714355, "lex_sim": 1.0}
{"question": "\"In the film of the same name, what was the name of \"\"The Hustler\"\"?\"", "answer": ["Fast Eddie Felson"], "scores": [[0.14202840626239777, 0.4695096015930176, 0.9619856476783752, 0.25949737429618835, 0.9979925155639648]], "normalized_score": [0.44064590334892273], "tokens": [["\u2581Fast", "\u2581Edd", "ie", "\u2581Fel", "son"]], "entropy": [[3.900968551635742, 2.4868927001953125, 0.17478503286838531, 1.3384016752243042, 0.01910613477230072]], "avg_entropy": [1.584030818939209], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\"\"\"Fast Eddie\"\" Felson\"", "score_of_biggest_cluster": 0.44064590334892273, "score_of_first_answer": 0.44064590334892273, "max_score_of_first_answer": 0.9979925155639648, "min_score_of_first_answer": 0.14202840626239777, "avg_score": 0.44064590334892273, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3539570962871526, "std_of_first_answer": 0.3539570962871526, "avg_entropy_of_first_answer": 1.584030818939209, "lex_sim": 1.0}
{"question": "In January 1971, Idi Amin deposed Milton Obote in a coup to become President of which country?", "answer": ["Uganda"], "scores": [[0.9270374774932861, 0.9893118739128113, 0.9902822971343994]], "normalized_score": [0.9684187173843384], "tokens": [["\u2581U", "g", "anda"]], "entropy": [[0.5741394758224487, 0.10137976706027985, 0.07513589411973953]], "avg_entropy": [0.2502183790008227], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ugandese", "score_of_biggest_cluster": 0.9684187173843384, "score_of_first_answer": 0.9684187173843384, "max_score_of_first_answer": 0.9902822971343994, "min_score_of_first_answer": 0.9270374774932861, "avg_score": 0.9684187173843384, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.029587815401467984, "std_of_first_answer": 0.029587815401467984, "avg_entropy_of_first_answer": 0.2502183790008227, "lex_sim": 1.0}
{"question": "Who played the title role in the 2008 film 'The Curious Case Of Benjamin Button'?", "answer": ["Brad Pitt"], "scores": [[0.9177109599113464, 0.944763720035553]], "normalized_score": [0.9311391115188599], "tokens": [["\u2581Brad", "\u2581Pitt"]], "entropy": [[0.6146485805511475, 0.3590646982192993]], "avg_entropy": [0.4868566393852234], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bradley Pitt", "score_of_biggest_cluster": 0.9311391115188599, "score_of_first_answer": 0.9311391115188599, "max_score_of_first_answer": 0.944763720035553, "min_score_of_first_answer": 0.9177109599113464, "avg_score": 0.9311391115188599, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.013526380062103271, "std_of_first_answer": 0.013526380062103271, "avg_entropy_of_first_answer": 0.4868566393852234, "lex_sim": 1.0}
{"question": "Which Christmas condiment is made from fruit sometimes referred to as marshworts?", "answer": ["Marmalade"], "scores": [[0.2844272553920746, 0.14649787545204163, 0.9869938492774963]], "normalized_score": [0.3451747000217438], "tokens": [["\u2581Mar", "mal", "ade"]], "entropy": [[3.2422537803649902, 2.3290634155273438, 0.10061901062726974]], "avg_entropy": [1.8906454021732013], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cranberry sauce", "score_of_biggest_cluster": 0.3451747000217438, "score_of_first_answer": 0.3451747000217438, "max_score_of_first_answer": 0.9869938492774963, "min_score_of_first_answer": 0.14649787545204163, "avg_score": 0.3451747000217438, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3680365040335604, "std_of_first_answer": 0.3680365040335604, "avg_entropy_of_first_answer": 1.8906454021732013, "lex_sim": 1.0}
{"question": "The Rio Grande forms the border of Texas with what other area?", "answer": ["Ciudad Ju\u00e1rez"], "scores": [[0.0019358081044629216, 0.8343286514282227, 0.041950006037950516, 0.9983972907066345]], "normalized_score": [0.09068985283374786], "tokens": [["\u2581Ciudad", "\u2581Ju", "\u00e1", "rez"]], "entropy": [[2.9725122451782227, 1.0066949129104614, 0.8104826211929321, 0.0178985632956028]], "avg_entropy": [1.2018970856443048], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mexican Republic", "score_of_biggest_cluster": 0.09068985283374786, "score_of_first_answer": 0.09068985283374786, "max_score_of_first_answer": 0.9983972907066345, "min_score_of_first_answer": 0.0019358081044629216, "avg_score": 0.09068985283374786, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4511782015164492, "std_of_first_answer": 0.4511782015164492, "avg_entropy_of_first_answer": 1.2018970856443048, "lex_sim": 1.0}
{"question": "What magazine was started by Hugh Hefner?", "answer": ["Playboy"], "scores": [[0.8977948427200317, 0.9848400950431824]], "normalized_score": [0.9403107762336731], "tokens": [["\u2581Play", "boy"]], "entropy": [[0.8264791965484619, 0.12206102907657623]], "avg_entropy": [0.4742701128125191], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Playboy Cybergirl", "score_of_biggest_cluster": 0.9403107762336731, "score_of_first_answer": 0.9403107762336731, "max_score_of_first_answer": 0.9848400950431824, "min_score_of_first_answer": 0.8977948427200317, "avg_score": 0.9403107762336731, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04352262616157532, "std_of_first_answer": 0.04352262616157532, "avg_entropy_of_first_answer": 0.4742701128125191, "lex_sim": 1.0}
{"question": "In the Bible, name the son of Saul who was a friend of David?", "answer": ["Jonathan"], "scores": [[0.8084973096847534]], "normalized_score": [0.8084973096847534], "tokens": [["\u2581Jonathan"]], "entropy": [[0.9058188199996948]], "avg_entropy": [0.9058188199996948], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Y'hochanan", "score_of_biggest_cluster": 0.8084973096847534, "score_of_first_answer": 0.8084973096847534, "max_score_of_first_answer": 0.8084973096847534, "min_score_of_first_answer": 0.8084973096847534, "avg_score": 0.8084973096847534, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.9058188199996948, "lex_sim": 1.0}
{"question": "\"In what sport are the \"\"stones\"\" aimed at the \"\"house\"\"?\"", "answer": ["Scottish curling"], "scores": [[0.0012297198409214616, 0.12399458140134811, 0.9938549399375916]], "normalized_score": [0.05331433564424515], "tokens": [["\u2581Scottish", "\u2581cur", "ling"]], "entropy": [[3.644382953643799, 4.222585678100586, 0.05053386464715004]], "avg_entropy": [2.6391674987971783], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Curling sheet", "score_of_biggest_cluster": 0.05331433564424515, "score_of_first_answer": 0.05331433564424515, "max_score_of_first_answer": 0.9938549399375916, "min_score_of_first_answer": 0.0012297198409214616, "avg_score": 0.05331433564424515, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4418437484197723, "std_of_first_answer": 0.4418437484197723, "avg_entropy_of_first_answer": 2.6391674987971783, "lex_sim": 1.0}
{"question": "When were the first Winter Paralympics held?", "answer": ["1976 Innsbruck"], "scores": [[0.46908479928970337, 0.9204500317573547, 0.9731566905975342, 0.8859744668006897, 0.9811317920684814, 0.0019275599624961615, 0.3434165120124817, 0.9715427160263062, 0.9995785355567932]], "normalized_score": [0.3951331377029419], "tokens": [["\u2581", "1", "9", "7", "6", "\u2581In", "ns", "b", "ruck"]], "entropy": [[2.649209499359131, 0.38126999139785767, 0.1915055811405182, 0.5099584460258484, 0.1283140331506729, 1.1119544506072998, 3.407296657562256, 0.1974620223045349, 0.0056864675134420395]], "avg_entropy": [0.9536285721179512], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1976", "score_of_biggest_cluster": 0.3951331377029419, "score_of_first_answer": 0.3951331377029419, "max_score_of_first_answer": 0.9995785355567932, "min_score_of_first_answer": 0.0019275599624961615, "avg_score": 0.3951331377029419, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3434073514350886, "std_of_first_answer": 0.3434073514350886, "avg_entropy_of_first_answer": 0.9536285721179512, "lex_sim": 1.0}
{"question": "Which figure from Scottish history was played by Mel Gibson in a 1995 film?", "answer": ["William Wallace"], "scores": [[0.626930296421051, 0.9702432751655579, 0.9997802376747131]], "normalized_score": [0.8472303152084351], "tokens": [["\u2581William", "\u2581Wal", "lace"]], "entropy": [[1.2489484548568726, 0.28377416729927063, 0.0027919060084968805]], "avg_entropy": [0.51183817605488], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wallace's Larder", "score_of_biggest_cluster": 0.8472303152084351, "score_of_first_answer": 0.8472303152084351, "max_score_of_first_answer": 0.9997802376747131, "min_score_of_first_answer": 0.626930296421051, "avg_score": 0.8472303152084351, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.16923137094967308, "std_of_first_answer": 0.16923137094967308, "avg_entropy_of_first_answer": 0.51183817605488, "lex_sim": 1.0}
{"question": "Which country, formerly known as Pleasant Island, is the smallest island country in the world?", "answer": ["Tuvalu"], "scores": [[0.11993106454610825, 0.9825931787490845]], "normalized_score": [0.34328335523605347], "tokens": [["\u2581Tu", "valu"]], "entropy": [[3.3780312538146973, 0.11750724911689758]], "avg_entropy": [1.7477692514657974], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "List of islands of Nauru", "score_of_biggest_cluster": 0.34328335523605347, "score_of_first_answer": 0.34328335523605347, "max_score_of_first_answer": 0.9825931787490845, "min_score_of_first_answer": 0.11993106454610825, "avg_score": 0.34328335523605347, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4313310571014881, "std_of_first_answer": 0.4313310571014881, "avg_entropy_of_first_answer": 1.7477692514657974, "lex_sim": 1.0}
{"question": "The Koppen System is a classification system for what?", "answer": ["Climates"], "scores": [[0.46639975905418396, 0.4392695426940918, 0.8139986991882324]], "normalized_score": [0.5504328608512878], "tokens": [["\u2581Cl", "im", "ates"]], "entropy": [[2.629117488861084, 0.8352670073509216, 0.5782973766326904]], "avg_entropy": [1.3475606242815654], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Climate", "score_of_biggest_cluster": 0.5504328608512878, "score_of_first_answer": 0.5504328608512878, "max_score_of_first_answer": 0.8139986991882324, "min_score_of_first_answer": 0.4392695426940918, "avg_score": 0.5504328608512878, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.17061425384926385, "std_of_first_answer": 0.17061425384926385, "avg_entropy_of_first_answer": 1.3475606242815654, "lex_sim": 1.0}
{"question": "8\u00c2\u00a0 Fort Ville-Marie was the original name of which Canadian city?", "answer": ["Quebec"], "scores": [[0.009319581091403961]], "normalized_score": [0.009319580160081387], "tokens": [["\u2581Quebec"]], "entropy": [[1.414900541305542]], "avg_entropy": [1.414900541305542], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Motnreal", "score_of_biggest_cluster": 0.009319580160081387, "score_of_first_answer": 0.009319580160081387, "max_score_of_first_answer": 0.009319581091403961, "min_score_of_first_answer": 0.009319581091403961, "avg_score": 0.009319580160081387, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.414900541305542, "lex_sim": 1.0}
{"question": "What is the name of the tool used to sharpen a knife?", "answer": ["Carbolace"], "scores": [[0.0010560934897512197, 0.005318623036146164, 0.0029731125105172396]], "normalized_score": [0.002556059742346406], "tokens": [["\u2581Car", "bol", "ace"]], "entropy": [[3.4812066555023193, 2.8982439041137695, 2.2021195888519287]], "avg_entropy": [2.8605233828226724], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Steel", "score_of_biggest_cluster": 0.002556059742346406, "score_of_first_answer": 0.002556059742346406, "max_score_of_first_answer": 0.005318623036146164, "min_score_of_first_answer": 0.0010560934897512197, "avg_score": 0.002556059742346406, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.001743098763240807, "std_of_first_answer": 0.001743098763240807, "avg_entropy_of_first_answer": 2.8605233828226724, "lex_sim": 1.0}
{"question": "What is the currency of Pakistan?", "answer": ["Pakistani Rupees"], "scores": [[0.46010228991508484, 0.9559212923049927, 0.9978145360946655, 0.34441426396369934, 0.9880774021148682, 0.10055392235517502]], "normalized_score": [0.49670618772506714], "tokens": [["\u2581Pak", "ist", "ani", "\u2581Ru", "pe", "es"]], "entropy": [[1.781356692314148, 0.37286925315856934, 0.02428988367319107, 0.9844595193862915, 0.09017486870288849, 0.3804977834224701]], "avg_entropy": [0.605608000109593], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u0930\u0942", "score_of_biggest_cluster": 0.49670618772506714, "score_of_first_answer": 0.49670618772506714, "max_score_of_first_answer": 0.9978145360946655, "min_score_of_first_answer": 0.10055392235517502, "avg_score": 0.49670618772506714, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3558379007342788, "std_of_first_answer": 0.3558379007342788, "avg_entropy_of_first_answer": 0.605608000109593, "lex_sim": 1.0}
{"question": "What would you expect if you asked for Focaccia in an Italian restaurant?", "answer": ["Bread"], "scores": [[0.07616913318634033, 0.9285591244697571]], "normalized_score": [0.26594647765159607], "tokens": [["\u2581B", "read"]], "entropy": [[4.397542476654053, 0.47746437788009644]], "avg_entropy": [2.4375034272670746], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Somun", "score_of_biggest_cluster": 0.26594647765159607, "score_of_first_answer": 0.26594647765159607, "max_score_of_first_answer": 0.9285591244697571, "min_score_of_first_answer": 0.07616913318634033, "avg_score": 0.26594647765159607, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4261949956417084, "std_of_first_answer": 0.4261949956417084, "avg_entropy_of_first_answer": 2.4375034272670746, "lex_sim": 1.0}
{"question": "Who was the American historian who devised the Decimal System for classifying books, in 1876?", "answer": ["Melvil Dewey"], "scores": [[0.4011901319026947, 0.9136133790016174, 0.8610909581184387, 0.992365300655365, 0.9979416728019714]], "normalized_score": [0.7924789190292358], "tokens": [["\u2581Mel", "vil", "\u2581De", "we", "y"]], "entropy": [[3.1266517639160156, 0.39676448702812195, 0.9300389289855957, 0.08151324838399887, 0.02099781483411789]], "avg_entropy": [0.91119324862957], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Melvil Dewey", "score_of_biggest_cluster": 0.7924789190292358, "score_of_first_answer": 0.7924789190292358, "max_score_of_first_answer": 0.9979416728019714, "min_score_of_first_answer": 0.4011901319026947, "avg_score": 0.7924789190292358, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.22196853235319794, "std_of_first_answer": 0.22196853235319794, "avg_entropy_of_first_answer": 0.91119324862957, "lex_sim": 1.0}
{"question": "The 1932 film \u2018The Mask of Fu Manchu\u2019 revolves around Fu Manchu\u2019s quest for the sword and mask of which historical figure?", "answer": ["Christopher Columbus"], "scores": [[0.0037942847702652216, 0.8349739909172058, 0.9985640645027161]], "normalized_score": [0.14680007100105286], "tokens": [["\u2581Christopher", "\u2581Columb", "us"]], "entropy": [[3.873161792755127, 1.045611023902893, 0.015218988060951233]], "avg_entropy": [1.6446639349063237], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tem\u00fcjin", "score_of_biggest_cluster": 0.14680007100105286, "score_of_first_answer": 0.14680007100105286, "max_score_of_first_answer": 0.9985640645027161, "min_score_of_first_answer": 0.0037942847702652216, "avg_score": 0.14680007100105286, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4355313907241712, "std_of_first_answer": 0.4355313907241712, "avg_entropy_of_first_answer": 1.6446639349063237, "lex_sim": 1.0}
{"question": "Who wrote The female of the species more deadly than the male?", "answer": ["Matilda Joslyn Gage"], "scores": [[0.003778260899707675, 0.4810946583747864, 0.14565826952457428, 0.9131203293800354, 0.9769099950790405, 0.9997492432594299]], "normalized_score": [0.24861183762550354], "tokens": [["\u2581Mat", "ilda", "\u2581Jos", "lyn", "\u2581G", "age"]], "entropy": [[5.4894819259643555, 1.3233342170715332, 4.709131717681885, 0.6691458225250244, 0.227774977684021, 0.0031142113730311394]], "avg_entropy": [2.0703304787166417], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joseph Rudyard Kipling", "score_of_biggest_cluster": 0.24861183762550354, "score_of_first_answer": 0.24861183762550354, "max_score_of_first_answer": 0.9997492432594299, "min_score_of_first_answer": 0.003778260899707675, "avg_score": 0.24861183762550354, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40309051860202316, "std_of_first_answer": 0.40309051860202316, "avg_entropy_of_first_answer": 2.0703304787166417, "lex_sim": 1.0}
{"question": "Who had the most expensive presidential inauguration ceremony of the 20th century?", "answer": ["President Theodore Roosevelt"], "scores": [[0.017377464100718498, 0.03734349459409714, 0.9952793121337891, 0.9674549102783203, 0.9930237531661987, 0.999277651309967, 0.998239278793335]], "normalized_score": [0.3480690121650696], "tokens": [["\u2581President", "\u2581The", "odore", "\u2581Ro", "ose", "vel", "t"]], "entropy": [[3.3433451652526855, 3.3818960189819336, 0.04551133140921593, 0.29737094044685364, 0.05024842172861099, 0.007557754870504141, 0.019319450482726097]], "avg_entropy": [1.020749869024647], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George Walker Bush", "score_of_biggest_cluster": 0.3480690121650696, "score_of_first_answer": 0.3480690121650696, "max_score_of_first_answer": 0.999277651309967, "min_score_of_first_answer": 0.017377464100718498, "avg_score": 0.3480690121650696, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4353192118903257, "std_of_first_answer": 0.4353192118903257, "avg_entropy_of_first_answer": 1.020749869024647, "lex_sim": 1.0}
{"question": "In which country were motorized ambulances first used?", "answer": ["USA"], "scores": [[0.055599313229322433]], "normalized_score": [0.05559930577874184], "tokens": [["\u2581USA"]], "entropy": [[3.9701945781707764]], "avg_entropy": [3.9701945781707764], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "La Republique francaise", "score_of_biggest_cluster": 0.05559930577874184, "score_of_first_answer": 0.05559930577874184, "max_score_of_first_answer": 0.055599313229322433, "min_score_of_first_answer": 0.055599313229322433, "avg_score": 0.05559930577874184, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.9701945781707764, "lex_sim": 1.0}
{"question": "Which village in Sweden has four chemical elements named after it?", "answer": ["Avangard"], "scores": [[0.00803335104137659, 0.10118146985769272, 0.7870289087295532]], "normalized_score": [0.08616472035646439], "tokens": [["\u2581Av", "ang", "ard"]], "entropy": [[3.5441808700561523, 2.580825090408325, 1.3519850969314575]], "avg_entropy": [2.4923303524653115], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ytterby", "score_of_biggest_cluster": 0.08616472035646439, "score_of_first_answer": 0.08616472035646439, "max_score_of_first_answer": 0.7870289087295532, "min_score_of_first_answer": 0.00803335104137659, "avg_score": 0.08616472035646439, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34735466350755523, "std_of_first_answer": 0.34735466350755523, "avg_entropy_of_first_answer": 2.4923303524653115, "lex_sim": 1.0}
{"question": "Which French racehorse, which won the Derby and Prix de I\u2019Arc de Triomphe in 1965, was the highest rated horse by Timeform until Frankel\u2019s performances in 2012?", "answer": ["Nuveen"], "scores": [[0.003057446563616395, 0.33271270990371704, 0.2869211733341217]], "normalized_score": [0.06633310765028], "tokens": [["\u2581Nu", "ve", "en"]], "entropy": [[4.556612968444824, 2.6229028701782227, 3.5231692790985107]], "avg_entropy": [3.567561705907186], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sea-Bird", "score_of_biggest_cluster": 0.06633310765028, "score_of_first_answer": 0.06633310765028, "max_score_of_first_answer": 0.33271270990371704, "min_score_of_first_answer": 0.003057446563616395, "avg_score": 0.06633310765028, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14581116892340043, "std_of_first_answer": 0.14581116892340043, "avg_entropy_of_first_answer": 3.567561705907186, "lex_sim": 1.0}
{"question": "Whose character ran a bookshop in the film 'Notting Hill'?", "answer": ["William Thacker"], "scores": [[0.2444961816072464, 0.8698181509971619, 0.953549325466156]], "normalized_score": [0.587509036064148], "tokens": [["\u2581William", "\u2581Th", "acker"]], "entropy": [[3.8135669231414795, 1.037438154220581, 0.26300305128097534]], "avg_entropy": [1.7046693762143452], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hugh Grant", "score_of_biggest_cluster": 0.587509036064148, "score_of_first_answer": 0.587509036064148, "max_score_of_first_answer": 0.953549325466156, "min_score_of_first_answer": 0.2444961816072464, "avg_score": 0.587509036064148, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3163673736155911, "std_of_first_answer": 0.3163673736155911, "avg_entropy_of_first_answer": 1.7046693762143452, "lex_sim": 1.0}
{"question": "Which river forms the eastern section of the border between England and Scotland?", "answer": ["Tweed"], "scores": [[0.4092460572719574, 0.9883880019187927]], "normalized_score": [0.6359983682632446], "tokens": [["\u2581Twe", "ed"]], "entropy": [[2.4503443241119385, 0.10515624284744263]], "avg_entropy": [1.2777502834796906], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "TWEED", "score_of_biggest_cluster": 0.6359983682632446, "score_of_first_answer": 0.6359983682632446, "max_score_of_first_answer": 0.9883880019187927, "min_score_of_first_answer": 0.4092460572719574, "avg_score": 0.6359983682632446, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.28957097232341766, "std_of_first_answer": 0.28957097232341766, "avg_entropy_of_first_answer": 1.2777502834796906, "lex_sim": 1.0}
{"question": "In Greek mythology, who was the first woman on Earth", "answer": ["Pandora"], "scores": [[0.6493358016014099, 0.9483673572540283, 0.9919805526733398]], "normalized_score": [0.8484957814216614], "tokens": [["\u2581P", "and", "ora"]], "entropy": [[2.3735527992248535, 0.3157263398170471, 0.07371944934129715]], "avg_entropy": [0.9209995294610659], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pandora (myth)", "score_of_biggest_cluster": 0.8484957814216614, "score_of_first_answer": 0.8484957814216614, "max_score_of_first_answer": 0.9919805526733398, "min_score_of_first_answer": 0.6493358016014099, "avg_score": 0.8484957814216614, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.15228898243064543, "std_of_first_answer": 0.15228898243064543, "avg_entropy_of_first_answer": 0.9209995294610659, "lex_sim": 1.0}
{"question": "Gluteus medius, Gluteus maximus and Piriformis are muscles in human?", "answer": ["Mm-hm..."], "scores": [[0.0023550563491880894, 0.2992793023586273, 0.22015070915222168, 0.4826405942440033, 0.003180509665980935]], "normalized_score": [0.04735695198178291], "tokens": [["\u2581M", "m", "-", "hm", "..."]], "entropy": [[4.703658103942871, 4.22206449508667, 2.840569496154785, 1.2495923042297363, 0.9478186964988708]], "avg_entropy": [2.7927406191825868], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Asscheeks", "score_of_biggest_cluster": 0.04735695198178291, "score_of_first_answer": 0.04735695198178291, "max_score_of_first_answer": 0.4826405942440033, "min_score_of_first_answer": 0.0023550563491880894, "avg_score": 0.04735695198178291, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1832692015735503, "std_of_first_answer": 0.1832692015735503, "avg_entropy_of_first_answer": 2.7927406191825868, "lex_sim": 1.0}
{"question": "Which major city is situated at the mouth of the Yangtze River in China?", "answer": ["Nanyang"], "scores": [[0.021633874624967575, 0.017031613737344742, 0.9587998390197754]], "normalized_score": [0.07069240510463715], "tokens": [["\u2581N", "any", "ang"]], "entropy": [[1.4033246040344238, 1.684436321258545, 0.3305223882198334]], "avg_entropy": [1.139427771170934], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Economy of Shanghai", "score_of_biggest_cluster": 0.07069240510463715, "score_of_first_answer": 0.07069240510463715, "max_score_of_first_answer": 0.9587998390197754, "min_score_of_first_answer": 0.017031613737344742, "avg_score": 0.07069240510463715, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44287302115178023, "std_of_first_answer": 0.44287302115178023, "avg_entropy_of_first_answer": 1.139427771170934, "lex_sim": 1.0}
{"question": "Who played 'Callan' in the TV series of the same name?", "answer": ["Edward Woodward"], "scores": [[0.8196700811386108, 0.9987390637397766, 0.9901070594787598]], "normalized_score": [0.9323760271072388], "tokens": [["\u2581Edward", "\u2581Wood", "ward"]], "entropy": [[0.8960239887237549, 0.014726463705301285, 0.08468061685562134]], "avg_entropy": [0.3318103564282258], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Edward Woodward", "score_of_biggest_cluster": 0.9323760271072388, "score_of_first_answer": 0.9323760271072388, "max_score_of_first_answer": 0.9987390637397766, "min_score_of_first_answer": 0.8196700811386108, "avg_score": 0.9323760271072388, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08245468493737718, "std_of_first_answer": 0.08245468493737718, "avg_entropy_of_first_answer": 0.3318103564282258, "lex_sim": 1.0}
{"question": "FIDE (Federation Internationale Des Echecs) is the world federation of which activity?", "answer": ["Chess"], "scores": [[0.7758138179779053, 0.9970006346702576]], "normalized_score": [0.8794810175895691], "tokens": [["\u2581Ch", "ess"]], "entropy": [[1.2407379150390625, 0.03216196596622467]], "avg_entropy": [0.6364499405026436], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chess match", "score_of_biggest_cluster": 0.8794810175895691, "score_of_first_answer": 0.8794810175895691, "max_score_of_first_answer": 0.9970006346702576, "min_score_of_first_answer": 0.7758138179779053, "avg_score": 0.8794810175895691, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11059340834617615, "std_of_first_answer": 0.11059340834617615, "avg_entropy_of_first_answer": 0.6364499405026436, "lex_sim": 1.0}
{"question": "Which Dutch admiral boasted in 1667 that he had swept the British from the sea?", "answer": ["Van Tromp"], "scores": [[0.03857358545064926, 0.7157275676727295, 0.9966834187507629, 0.9481318593025208]], "normalized_score": [0.401898056268692], "tokens": [["\u2581Van", "\u2581T", "rom", "p"]], "entropy": [[3.131575584411621, 1.8738914728164673, 0.03445581719279289, 0.3707337975502014]], "avg_entropy": [1.3526641679927707], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Maarten Harpertsz. Tromp", "score_of_biggest_cluster": 0.401898056268692, "score_of_first_answer": 0.401898056268692, "max_score_of_first_answer": 0.9966834187507629, "min_score_of_first_answer": 0.03857358545064926, "avg_score": 0.401898056268692, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3823540097855792, "std_of_first_answer": 0.3823540097855792, "avg_entropy_of_first_answer": 1.3526641679927707, "lex_sim": 1.0}
{"question": "What name is given to the dark gap between the rings of the planet Saturn, discovered in 1675?", "answer": ["Cassini Division"], "scores": [[0.746812105178833, 0.8770478963851929, 0.1458643525838852]], "normalized_score": [0.45715269446372986], "tokens": [["\u2581Cass", "ini", "\u2581Division"]], "entropy": [[1.3922593593597412, 0.8774096965789795, 1.9375932216644287]], "avg_entropy": [1.4024207592010498], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cassini's Division", "score_of_biggest_cluster": 0.45715269446372986, "score_of_first_answer": 0.45715269446372986, "max_score_of_first_answer": 0.8770478963851929, "min_score_of_first_answer": 0.1458643525838852, "avg_score": 0.45715269446372986, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.31845616083883754, "std_of_first_answer": 0.31845616083883754, "avg_entropy_of_first_answer": 1.4024207592010498, "lex_sim": 1.0}
{"question": "In what month is the Munich Oktoberfest beer festival held?", "answer": ["M\u00e4rz"], "scores": [[2.4208509785239585e-05]], "normalized_score": [2.4208515242207795e-05], "tokens": [["\u2581M\u00e4rz"]], "entropy": [[2.1419577598571777]], "avg_entropy": [2.1419577598571777], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Setember", "score_of_biggest_cluster": 2.4208515242207795e-05, "score_of_first_answer": 2.4208515242207795e-05, "max_score_of_first_answer": 2.4208509785239585e-05, "min_score_of_first_answer": 2.4208509785239585e-05, "avg_score": 2.4208515242207795e-05, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.1419577598571777, "lex_sim": 1.0}
{"question": "Which famous cowboy was created by George W. Trendle?", "answer": ["Roy Rogers"], "scores": [[0.24454888701438904, 0.7371185421943665, 0.9979925155639648]], "normalized_score": [0.5645166635513306], "tokens": [["\u2581Roy", "\u2581Rog", "ers"]], "entropy": [[3.223076343536377, 1.572696328163147, 0.021833445876836777]], "avg_entropy": [1.6058687058587868], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lone Ranger (comics)", "score_of_biggest_cluster": 0.5645166635513306, "score_of_first_answer": 0.5645166635513306, "max_score_of_first_answer": 0.9979925155639648, "min_score_of_first_answer": 0.24454888701438904, "avg_score": 0.5645166635513306, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.312402410822202, "std_of_first_answer": 0.312402410822202, "avg_entropy_of_first_answer": 1.6058687058587868, "lex_sim": 1.0}
{"question": "Astrakhan is the curly hair of which young animal?", "answer": ["Lamb"], "scores": [[0.1942354142665863]], "normalized_score": [0.1942354142665863], "tokens": [["\u2581Lamb"]], "entropy": [[3.7753167152404785]], "avg_entropy": [3.7753167152404785], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lamb", "score_of_biggest_cluster": 0.1942354142665863, "score_of_first_answer": 0.1942354142665863, "max_score_of_first_answer": 0.1942354142665863, "min_score_of_first_answer": 0.1942354142665863, "avg_score": 0.1942354142665863, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.7753167152404785, "lex_sim": 1.0}
{"question": "What is the name of the commercial airport which opened in 2005 on the site of the former RAF Finningley?", "answer": ["Doncaster\u2013Sheffield Airport"], "scores": [[0.5576620697975159, 0.9651163816452026, 0.9947789907455444, 0.0004325076297391206, 0.9719725251197815, 0.998927891254425, 0.9959457516670227, 0.13182874023914337]], "normalized_score": [0.2714957892894745], "tokens": [["\u2581Don", "c", "aster", "\u2013", "She", "ff", "ield", "\u2581Airport"]], "entropy": [[1.4608409404754639, 0.32500818371772766, 0.05040379986166954, 1.955100655555725, 0.19590085744857788, 0.011976083740592003, 0.04025863856077194, 1.7026678323745728]], "avg_entropy": [0.7177696239668876], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Robin Hood Doncaster Sheffield Airport", "score_of_biggest_cluster": 0.2714957892894745, "score_of_first_answer": 0.2714957892894745, "max_score_of_first_answer": 0.998927891254425, "min_score_of_first_answer": 0.0004325076297391206, "avg_score": 0.2714957892894745, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3937846499370031, "std_of_first_answer": 0.3937846499370031, "avg_entropy_of_first_answer": 0.7177696239668876, "lex_sim": 1.0}
{"question": "Set up in 1954 as a merger of smaller groups, the Front de Libration Nationale fought a war for independence from France until 1962, when the French government signed a cease-fire agreement. The FLN became the only legal party in which country?", "answer": ["Algieria"], "scores": [[0.005199718754738569, 0.4486604630947113, 0.9250710606575012]], "normalized_score": [0.12922829389572144], "tokens": [["\u2581Alg", "ier", "ia"]], "entropy": [[1.0431764125823975, 3.0234932899475098, 0.41163086891174316]], "avg_entropy": [1.4927668571472168], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Popular Democratic Republic of Algeria", "score_of_biggest_cluster": 0.12922829389572144, "score_of_first_answer": 0.12922829389572144, "max_score_of_first_answer": 0.9250710606575012, "min_score_of_first_answer": 0.005199718754738569, "avg_score": 0.12922829389572144, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.37561620112004496, "std_of_first_answer": 0.37561620112004496, "avg_entropy_of_first_answer": 1.4927668571472168, "lex_sim": 1.0}
{"question": "Amal Clooney is an internationally renowned?", "answer": ["Barrister and human rights lawyer"], "scores": [[0.08629002422094345, 0.98148512840271, 0.9995529055595398, 0.06343428790569305, 0.5211796760559082, 0.9361132383346558, 0.728411078453064]], "normalized_score": [0.40881210565567017], "tokens": [["\u2581Bar", "r", "ister", "\u2581and", "\u2581human", "\u2581rights", "\u2581lawyer"]], "entropy": [[3.1353325843811035, 0.1796959787607193, 0.005335143301635981, 0.6995807886123657, 2.5266051292419434, 0.3214096426963806, 1.03857421875]], "avg_entropy": [1.1295047836777354], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Human rights law", "score_of_biggest_cluster": 0.40881210565567017, "score_of_first_answer": 0.40881210565567017, "max_score_of_first_answer": 0.9995529055595398, "min_score_of_first_answer": 0.06343428790569305, "avg_score": 0.40881210565567017, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37620482929179816, "std_of_first_answer": 0.37620482929179816, "avg_entropy_of_first_answer": 1.1295047836777354, "lex_sim": 1.0}
{"question": "By what name is comic book villain Oswald Chesterfield Cobblepot better known?", "answer": ["Batman's nemesis 'The Penguin'"], "scores": [[0.0007246198365464807, 0.8625924587249756, 0.7627154588699341, 0.9984777569770813, 0.41668829321861267, 0.9801371693611145, 0.0021567882504314184, 0.8152134418487549, 0.9881761074066162, 0.993710994720459, 0.9992654919624329, 0.9305212497711182]], "normalized_score": [0.2869842052459717], "tokens": [["\u2581Bat", "man", "'", "s", "\u2581nem", "esis", "\u2581'", "The", "\u2581P", "engu", "in", "'"]], "entropy": [[1.0887279510498047, 0.8829948902130127, 0.9563707709312439, 0.017975445836782455, 2.641942024230957, 0.119975745677948, 2.085015296936035, 0.7387415170669556, 0.11283841729164124, 0.060391563922166824, 0.008601190522313118, 0.38935619592666626]], "avg_entropy": [0.758577584133794], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Penguin (comics)", "score_of_biggest_cluster": 0.2869842052459717, "score_of_first_answer": 0.2869842052459717, "max_score_of_first_answer": 0.9992654919624329, "min_score_of_first_answer": 0.0007246198365464807, "avg_score": 0.2869842052459717, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.361619874519149, "std_of_first_answer": 0.361619874519149, "avg_entropy_of_first_answer": 0.758577584133794, "lex_sim": 1.0}
{"question": "Which Gilbert and Sullivan opera has the alternate title 'Bunthome's Bride'?", "answer": ["The Pirates of Penzance"], "scores": [[0.4953747093677521, 0.15620385110378265, 0.9751003384590149, 0.9432063102722168, 0.9852411150932312, 0.9915645122528076, 0.9945630431175232]], "normalized_score": [0.6827377676963806], "tokens": [["\u2581The", "\u2581Pir", "ates", "\u2581of", "\u2581P", "enz", "ance"]], "entropy": [[2.4257867336273193, 2.7940564155578613, 0.11923366039991379, 0.2712939381599426, 0.13774192333221436, 0.0832454115152359, 0.04866255447268486]], "avg_entropy": [0.8400029481521675], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Patience", "score_of_biggest_cluster": 0.6827377676963806, "score_of_first_answer": 0.6827377676963806, "max_score_of_first_answer": 0.9945630431175232, "min_score_of_first_answer": 0.15620385110378265, "avg_score": 0.6827377676963806, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30864015580864534, "std_of_first_answer": 0.30864015580864534, "avg_entropy_of_first_answer": 0.8400029481521675, "lex_sim": 1.0}
{"question": "What movie had a sexy, supernatural pottery scene?", "answer": ["Ghost"], "scores": [[0.6081946492195129]], "normalized_score": [0.6081946492195129], "tokens": [["\u2581Ghost"]], "entropy": [[2.4881811141967773]], "avg_entropy": [2.4881811141967773], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ghost", "score_of_biggest_cluster": 0.6081946492195129, "score_of_first_answer": 0.6081946492195129, "max_score_of_first_answer": 0.6081946492195129, "min_score_of_first_answer": 0.6081946492195129, "avg_score": 0.6081946492195129, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.4881811141967773, "lex_sim": 1.0}
{"question": "June 2, 2004 saw the beginning of the streak in which what Edmonds, WA native won 74 consecutive games on Jeopardy?", "answer": ["Ken Jennings"], "scores": [[0.5174508690834045, 0.9677135944366455, 0.9921506643295288]], "normalized_score": [0.7920109629631042], "tokens": [["\u2581Ken", "\u2581Jenn", "ings"]], "entropy": [[3.2613866329193115, 0.3067675232887268, 0.07937884330749512]], "avg_entropy": [1.2158443331718445], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ken William Jennings III", "score_of_biggest_cluster": 0.7920109629631042, "score_of_first_answer": 0.7920109629631042, "max_score_of_first_answer": 0.9921506643295288, "min_score_of_first_answer": 0.5174508690834045, "avg_score": 0.7920109629631042, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.21824389716150905, "std_of_first_answer": 0.21824389716150905, "avg_entropy_of_first_answer": 1.2158443331718445, "lex_sim": 1.0}
{"question": "Which character did Felicity Kendall play in 'The Good Life'? (Christian name and surname required)", "answer": ["Margo Leadbetter"], "scores": [[0.6248974204063416, 0.9658678770065308, 0.678407609462738, 0.9936782717704773, 0.9000110626220703, 0.9988952279090881]], "normalized_score": [0.8456781506538391], "tokens": [["\u2581Mar", "go", "\u2581Le", "ad", "bet", "ter"]], "entropy": [[1.9629912376403809, 0.2012348175048828, 1.9832398891448975, 0.0624757781624794, 0.5119413137435913, 0.0100063756108284]], "avg_entropy": [0.7886482353011767], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Barbara Good", "score_of_biggest_cluster": 0.8456781506538391, "score_of_first_answer": 0.8456781506538391, "max_score_of_first_answer": 0.9988952279090881, "min_score_of_first_answer": 0.6248974204063416, "avg_score": 0.8456781506538391, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.15177788686274504, "std_of_first_answer": 0.15177788686274504, "avg_entropy_of_first_answer": 0.7886482353011767, "lex_sim": 1.0}
{"question": "Sandra Bullock won this year's 'Oscar' for Best Actress for her role as 'Leigh Anne Tuohy', in which film?", "answer": ["'The Blind Side'"], "scores": [[0.03979744389653206, 0.9250814318656921, 0.988182008266449, 0.9978834986686707, 0.9528568983078003, 0.9478504061698914]], "normalized_score": [0.5657439231872559], "tokens": [["\u2581'", "The", "\u2581Bl", "ind", "\u2581Side", "'"]], "entropy": [[1.137401819229126, 0.49546289443969727, 0.11339214444160461, 0.01963832601904869, 0.28442931175231934, 0.33722710609436035]], "avg_entropy": [0.39792526699602604], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Blind Side (film)", "score_of_biggest_cluster": 0.5657439231872559, "score_of_first_answer": 0.5657439231872559, "max_score_of_first_answer": 0.9978834986686707, "min_score_of_first_answer": 0.03979744389653206, "avg_score": 0.5657439231872559, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3446988934042235, "std_of_first_answer": 0.3446988934042235, "avg_entropy_of_first_answer": 0.39792526699602604, "lex_sim": 1.0}
{"question": "Who did Jack Ruby shoot in November 1963?", "answer": ["John F. Kennedy"], "scores": [[0.23773562908172607, 0.8211736083030701, 0.8239513039588928, 0.9671531915664673]], "normalized_score": [0.6280316114425659], "tokens": [["\u2581John", "\u2581F", ".", "\u2581Kennedy"]], "entropy": [[1.93743097782135, 0.9907979965209961, 0.6113969683647156, 0.2592625617980957]], "avg_entropy": [0.9497221261262894], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Oswald the Lone Assassin", "score_of_biggest_cluster": 0.6280316114425659, "score_of_first_answer": 0.6280316114425659, "max_score_of_first_answer": 0.9671531915664673, "min_score_of_first_answer": 0.23773562908172607, "avg_score": 0.6280316114425659, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28039293951391264, "std_of_first_answer": 0.28039293951391264, "avg_entropy_of_first_answer": 0.9497221261262894, "lex_sim": 1.0}
{"question": "On what day of the month did the Roman \u2018Ides\u2019 of March fall?", "answer": ["The fifth"], "scores": [[0.17425435781478882, 0.025625057518482208]], "normalized_score": [0.06682273745536804], "tokens": [["\u2581The", "\u2581fifth"]], "entropy": [[2.386707067489624, 1.8280737400054932]], "avg_entropy": [2.1073904037475586], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "fifteen", "score_of_biggest_cluster": 0.06682273745536804, "score_of_first_answer": 0.06682273745536804, "max_score_of_first_answer": 0.17425435781478882, "min_score_of_first_answer": 0.025625057518482208, "avg_score": 0.06682273745536804, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0743146501481533, "std_of_first_answer": 0.0743146501481533, "avg_entropy_of_first_answer": 2.1073904037475586, "lex_sim": 1.0}
{"question": "In which events did Gert Fredriksson win six Olympic gold medals?", "answer": ["Gym, Sailing (one design dinghy), Canoe, Boxing, Cycling & Athletics"], "scores": [[0.031204327940940857, 0.04513068124651909, 0.0759635716676712, 0.005246303975582123, 0.4204021692276001, 0.9636704921722412, 0.012695683166384697, 0.03263520821928978, 0.019010119140148163, 0.10359299927949905, 0.9997133612632751, 0.7893742918968201, 0.6328709125518799, 0.0453973188996315, 0.9813140630722046, 0.4368753731250763, 0.3371538519859314, 0.024955585598945618, 0.9876640439033508, 0.7607254981994629, 0.04525202140212059, 0.9933831095695496, 0.002894067205488682, 0.061552319675683975]], "normalized_score": [0.12069341540336609], "tokens": [["\u2581G", "ym", ",", "\u2581S", "ail", "ing", "\u2581(", "one", "\u2581design", "\u2581d", "ing", "hy", "),", "\u2581C", "ano", "e", ",", "\u2581Box", "ing", ",", "\u2581Cy", "cling", "\u2581&", "\u2581Athletics"]], "entropy": [[4.602319717407227, 1.543631911277771, 3.1721456050872803, 4.901705741882324, 1.911194920539856, 0.2390364408493042, 0.9867324829101562, 4.710712432861328, 2.9371533393859863, 2.115084648132324, 0.0035676085390150547, 0.6945725679397583, 1.6625735759735107, 3.9874563217163086, 0.11036381125450134, 0.7374505400657654, 2.2590317726135254, 3.8493146896362305, 0.08525937795639038, 0.8854053020477295, 3.8495326042175293, 0.05108408257365227, 1.4282584190368652, 3.9635415077209473]], "avg_entropy": [2.1119637259010537], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Flatwater", "score_of_biggest_cluster": 0.12069341540336609, "score_of_first_answer": 0.12069341540336609, "max_score_of_first_answer": 0.9997133612632751, "min_score_of_first_answer": 0.002894067205488682, "avg_score": 0.12069341540336609, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3944188512337388, "std_of_first_answer": 0.3944188512337388, "avg_entropy_of_first_answer": 2.1119637259010537, "lex_sim": 1.0}
{"question": "Which Olympic games saw the introduction of the Fosbury Flop?", "answer": ["1968 Mexico City Olympic Games"], "scores": [[0.311910480260849, 0.9728760123252869, 0.9883037805557251, 0.9903156161308289, 0.9441343545913696, 0.23576492071151733, 0.48571112751960754, 0.04124629870057106, 0.791255533695221]], "normalized_score": [0.4665811359882355], "tokens": [["\u2581", "1", "9", "6", "8", "\u2581Mexico", "\u2581City", "\u2581Olympic", "\u2581Games"]], "entropy": [[2.398664712905884, 0.18992212414741516, 0.09014879167079926, 0.07324358820915222, 0.28124603629112244, 2.4679155349731445, 1.6761939525604248, 1.840362787246704, 0.6683550477027893]], "avg_entropy": [1.0762280639674928], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "MEXICO 1968", "score_of_biggest_cluster": 0.4665811359882355, "score_of_first_answer": 0.4665811359882355, "max_score_of_first_answer": 0.9903156161308289, "min_score_of_first_answer": 0.04124629870057106, "avg_score": 0.4665811359882355, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3533299470522206, "std_of_first_answer": 0.3533299470522206, "avg_entropy_of_first_answer": 1.0762280639674928, "lex_sim": 1.0}
{"question": "What sporting term, meaning a second chance, comes from the French word for fishing again?", "answer": ["Double"], "scores": [[0.007781257387250662]], "normalized_score": [0.007781258784234524], "tokens": [["\u2581Double"]], "entropy": [[4.583446502685547]], "avg_entropy": [4.583446502685547], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rep\u00eachage", "score_of_biggest_cluster": 0.007781258784234524, "score_of_first_answer": 0.007781258784234524, "max_score_of_first_answer": 0.007781257387250662, "min_score_of_first_answer": 0.007781257387250662, "avg_score": 0.007781258784234524, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.583446502685547, "lex_sim": 1.0}
{"question": "Planets in our solar system?", "answer": ["2"], "scores": [[0.017739932984113693, 0.01134189311414957]], "normalized_score": [0.014184659346938133], "tokens": [["\u2581", "2"]], "entropy": [[3.2550153732299805, 2.4378745555877686]], "avg_entropy": [2.8464449644088745], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "8", "score_of_biggest_cluster": 0.014184659346938133, "score_of_first_answer": 0.014184659346938133, "max_score_of_first_answer": 0.017739932984113693, "min_score_of_first_answer": 0.01134189311414957, "avg_score": 0.014184659346938133, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0031990199349820614, "std_of_first_answer": 0.0031990199349820614, "avg_entropy_of_first_answer": 2.8464449644088745, "lex_sim": 1.0}
{"question": "In June 2009 American neuroscientist Dr Gary Arendash claimed to have evidence that drinking a few cups of coffee a day could prevent or reverse the effects of what?", "answer": ["Alzheimer's"], "scores": [[0.7420191168785095, 0.9190335869789124, 0.941267192363739, 0.9235062599182129, 0.9980629086494446]], "normalized_score": [0.9003501534461975], "tokens": [["\u2581Al", "z", "heimer", "'", "s"]], "entropy": [[1.503783941268921, 0.4263421297073364, 0.26100045442581177, 0.47118985652923584, 0.021563049405813217]], "avg_entropy": [0.5367758862674237], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alzheimer's diseases", "score_of_biggest_cluster": 0.9003501534461975, "score_of_first_answer": 0.9003501534461975, "max_score_of_first_answer": 0.9980629086494446, "min_score_of_first_answer": 0.7420191168785095, "avg_score": 0.9003501534461975, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08611383031261671, "std_of_first_answer": 0.08611383031261671, "avg_entropy_of_first_answer": 0.5367758862674237, "lex_sim": 1.0}
{"question": "In which country will the 2019 Rugby Union World Cup be played?", "answer": ["Japan"], "scores": [[0.5806915760040283]], "normalized_score": [0.5806915760040283], "tokens": [["\u2581Japan"]], "entropy": [[1.7975881099700928]], "avg_entropy": [1.7975881099700928], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u65e5\u672c\u570b", "score_of_biggest_cluster": 0.5806915760040283, "score_of_first_answer": 0.5806915760040283, "max_score_of_first_answer": 0.5806915760040283, "min_score_of_first_answer": 0.5806915760040283, "avg_score": 0.5806915760040283, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.7975881099700928, "lex_sim": 1.0}
{"question": "Who of these musicians was inducted into the Rock and Roll Hall of Fame in 2014?", "answer": ["The Beach Boys"], "scores": [[0.035921934992074966, 0.06779386103153229, 0.9833441376686096]], "normalized_score": [0.13378843665122986], "tokens": [["\u2581The", "\u2581Beach", "\u2581Boys"]], "entropy": [[5.319146633148193, 4.494443893432617, 0.12034553289413452]], "avg_entropy": [3.3113120198249817], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cat stevens", "score_of_biggest_cluster": 0.13378843665122986, "score_of_first_answer": 0.13378843665122986, "max_score_of_first_answer": 0.9833441376686096, "min_score_of_first_answer": 0.035921934992074966, "avg_score": 0.13378843665122986, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4392995635534355, "std_of_first_answer": 0.4392995635534355, "avg_entropy_of_first_answer": 3.3113120198249817, "lex_sim": 1.0}
{"question": "What island is noted for its biodiversity and 80% unique wildlife due to more than 80 million years' isolation from continental land mass?", "answer": ["Easter Island"], "scores": [[0.03492595627903938, 0.907717227935791, 0.4964926540851593]], "normalized_score": [0.25061315298080444], "tokens": [["\u2581E", "aster", "\u2581Island"]], "entropy": [[4.404894828796387, 0.6748241782188416, 1.356520414352417]], "avg_entropy": [2.1454131404558816], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Island of the moon", "score_of_biggest_cluster": 0.25061315298080444, "score_of_first_answer": 0.25061315298080444, "max_score_of_first_answer": 0.907717227935791, "min_score_of_first_answer": 0.03492595627903938, "avg_score": 0.25061315298080444, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35651306202383376, "std_of_first_answer": 0.35651306202383376, "avg_entropy_of_first_answer": 2.1454131404558816, "lex_sim": 1.0}
{"question": "In the English translations of the 'Asterix' comics, what is the name of the village leader, a middle-aged, obese man with red hair, pigtails and a huge moustache?", "answer": ["Chief O'Granite"], "scores": [[0.08322244882583618, 0.012652559205889702, 0.24841058254241943, 0.0571487657725811, 0.15201659500598907, 0.2717988193035126]], "normalized_score": [0.09228314459323883], "tokens": [["\u2581Chief", "\u2581O", "'", "G", "ran", "ite"]], "entropy": [[3.8155157566070557, 2.9332222938537598, 4.048697471618652, 3.76840877532959, 4.495580673217773, 3.921267509460449]], "avg_entropy": [3.8304487466812134], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Unhygienix", "score_of_biggest_cluster": 0.09228314459323883, "score_of_first_answer": 0.09228314459323883, "max_score_of_first_answer": 0.2717988193035126, "min_score_of_first_answer": 0.012652559205889702, "avg_score": 0.09228314459323883, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0962092408896647, "std_of_first_answer": 0.0962092408896647, "avg_entropy_of_first_answer": 3.8304487466812134, "lex_sim": 1.0}
{"question": "Christian is the lead character in the film 2015 adaptation of what extraordinarily successful book?", "answer": ["1 QUESTION PER PERSON"], "scores": [[0.11925172060728073, 0.07563608884811401, 0.0006678624195046723, 0.6646474003791809, 0.8849737048149109, 0.9510496854782104, 0.03917456418275833, 0.7173207402229309, 0.29974809288978577, 0.40556401014328003, 0.9925230145454407]], "normalized_score": [0.1896658092737198], "tokens": [["\u2581", "1", "\u2581Q", "UE", "ST", "ION", "\u2581P", "ER", "\u2581P", "ER", "SON"]], "entropy": [[3.6511125564575195, 1.4360582828521729, 0.7770020365715027, 2.033578872680664, 0.4037797451019287, 0.31080344319343567, 3.4055981636047363, 1.3092626333236694, 3.4141628742218018, 1.5505077838897705, 0.057995542883872986]], "avg_entropy": [1.6681692667982795], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fifty shades of Grey", "score_of_biggest_cluster": 0.1896658092737198, "score_of_first_answer": 0.1896658092737198, "max_score_of_first_answer": 0.9925230145454407, "min_score_of_first_answer": 0.0006678624195046723, "avg_score": 0.1896658092737198, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36844035395322045, "std_of_first_answer": 0.36844035395322045, "avg_entropy_of_first_answer": 1.6681692667982795, "lex_sim": 1.0}
{"question": "In Greek mythology who gave King Midas the power to turn everything he touched into gold?", "answer": ["Dionysos"], "scores": [[0.47725173830986023, 0.9009641408920288, 0.9565479755401611, 0.13909389078617096]], "normalized_score": [0.4890659749507904], "tokens": [["\u2581D", "ion", "ys", "os"]], "entropy": [[2.571279764175415, 0.7899340987205505, 0.2269231081008911, 0.8394843339920044]], "avg_entropy": [1.1069053262472153], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "God of wine", "score_of_biggest_cluster": 0.4890659749507904, "score_of_first_answer": 0.4890659749507904, "max_score_of_first_answer": 0.9565479755401611, "min_score_of_first_answer": 0.13909389078617096, "avg_score": 0.4890659749507904, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3331079812086531, "std_of_first_answer": 0.3331079812086531, "avg_entropy_of_first_answer": 1.1069053262472153, "lex_sim": 1.0}
{"question": "Which Gilbert and Sullivan operetta is sub titled 'Bunthornes's Bride'?", "answer": ["Patience"], "scores": [[0.2031274437904358, 0.7527862191200256]], "normalized_score": [0.39103907346725464], "tokens": [["\u2581P", "atience"]], "entropy": [[2.4632530212402344, 0.9849555492401123]], "avg_entropy": [1.7241042852401733], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Impatience", "score_of_biggest_cluster": 0.39103907346725464, "score_of_first_answer": 0.39103907346725464, "max_score_of_first_answer": 0.7527862191200256, "min_score_of_first_answer": 0.2031274437904358, "avg_score": 0.39103907346725464, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2748293876647949, "std_of_first_answer": 0.2748293876647949, "avg_entropy_of_first_answer": 1.7241042852401733, "lex_sim": 1.0}
{"question": "How long does it take, approximately, for light to travel from the Sun to the Earth?", "answer": ["8 minutes"], "scores": [[0.2692110538482666, 0.7838696241378784, 0.6036638617515564]], "normalized_score": [0.5031651854515076], "tokens": [["\u2581", "8", "\u2581minutes"]], "entropy": [[2.6211702823638916, 0.9414270520210266, 1.7242834568023682]], "avg_entropy": [1.7622935970624287], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "About 8 minutes", "score_of_biggest_cluster": 0.5031651854515076, "score_of_first_answer": 0.5031651854515076, "max_score_of_first_answer": 0.7838696241378784, "min_score_of_first_answer": 0.2692110538482666, "avg_score": 0.5031651854515076, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.21323076739745753, "std_of_first_answer": 0.21323076739745753, "avg_entropy_of_first_answer": 1.7622935970624287, "lex_sim": 1.0}
{"question": "What compete at the Burghley Trials, Badminton, the Rolex Kentucky Three Day, the Adelaide Trials, the Luhmhlen Trials and the toiles de Pau?", "answer": ["A horse"], "scores": [[0.016655873507261276, 0.4246900975704193]], "normalized_score": [0.08410460501909256], "tokens": [["\u2581A", "\u2581horse"]], "entropy": [[3.712137222290039, 4.169867515563965]], "avg_entropy": [3.941002368927002], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Horseback-riding", "score_of_biggest_cluster": 0.08410460501909256, "score_of_first_answer": 0.08410460501909256, "max_score_of_first_answer": 0.4246900975704193, "min_score_of_first_answer": 0.016655873507261276, "avg_score": 0.08410460501909256, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20401711203157902, "std_of_first_answer": 0.20401711203157902, "avg_entropy_of_first_answer": 3.941002368927002, "lex_sim": 1.0}
{"question": "What European country has 227 inhabited islands?", "answer": ["Malta"], "scores": [[0.10771480202674866, 0.9971191883087158]], "normalized_score": [0.327726274728775], "tokens": [["\u2581Mal", "ta"]], "entropy": [[2.778346061706543, 0.033472150564193726]], "avg_entropy": [1.4059091061353683], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yunanistan", "score_of_biggest_cluster": 0.327726274728775, "score_of_first_answer": 0.327726274728775, "max_score_of_first_answer": 0.9971191883087158, "min_score_of_first_answer": 0.10771480202674866, "avg_score": 0.327726274728775, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4447021931409836, "std_of_first_answer": 0.4447021931409836, "avg_entropy_of_first_answer": 1.4059091061353683, "lex_sim": 1.0}
{"question": "'Paralympics' is a combination of Olympics and what other word?", "answer": ["Para"], "scores": [[0.039367955178022385]], "normalized_score": [0.039367951452732086], "tokens": [["\u2581Para"]], "entropy": [[5.4045257568359375]], "avg_entropy": [5.4045257568359375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paralel", "score_of_biggest_cluster": 0.039367951452732086, "score_of_first_answer": 0.039367951452732086, "max_score_of_first_answer": 0.039367955178022385, "min_score_of_first_answer": 0.039367955178022385, "avg_score": 0.039367951452732086, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.4045257568359375, "lex_sim": 1.0}
{"question": "Which rugby union team position wears shirt number 15?", "answer": ["Scrum half"], "scores": [[0.09641841053962708, 0.9937113523483276, 0.3268541693687439]], "normalized_score": [0.31520378589630127], "tokens": [["\u2581Sc", "rum", "\u2581half"]], "entropy": [[3.3266048431396484, 0.057966530323028564, 1.3215183019638062]], "avg_entropy": [1.5686965584754944], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Full Back", "score_of_biggest_cluster": 0.31520378589630127, "score_of_first_answer": 0.31520378589630127, "max_score_of_first_answer": 0.9937113523483276, "min_score_of_first_answer": 0.09641841053962708, "avg_score": 0.31520378589630127, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3804870803556832, "std_of_first_answer": 0.3804870803556832, "avg_entropy_of_first_answer": 1.5686965584754944, "lex_sim": 1.0}
{"question": "What is Mexico's most commonly used language?", "answer": ["Spanish"], "scores": [[0.8670642971992493]], "normalized_score": [0.8670642971992493], "tokens": [["\u2581Spanish"]], "entropy": [[0.9768871068954468]], "avg_entropy": [0.9768871068954468], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Spanish", "score_of_biggest_cluster": 0.8670642971992493, "score_of_first_answer": 0.8670642971992493, "max_score_of_first_answer": 0.8670642971992493, "min_score_of_first_answer": 0.8670642971992493, "avg_score": 0.8670642971992493, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.9768871068954468, "lex_sim": 1.0}
{"question": "Opened in 1637, in which city was the first public opera house?", "answer": ["Venice"], "scores": [[0.4828929603099823, 0.9910436868667603]], "normalized_score": [0.6917861104011536], "tokens": [["\u2581Ven", "ice"]], "entropy": [[2.8594818115234375, 0.08470159769058228]], "avg_entropy": [1.4720917046070099], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sestiere (Venice)", "score_of_biggest_cluster": 0.6917861104011536, "score_of_first_answer": 0.6917861104011536, "max_score_of_first_answer": 0.9910436868667603, "min_score_of_first_answer": 0.4828929603099823, "avg_score": 0.6917861104011536, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.254075363278389, "std_of_first_answer": 0.254075363278389, "avg_entropy_of_first_answer": 1.4720917046070099, "lex_sim": 1.0}
{"question": "The War of the Triple Alliance (1864 - 1870) was fought between the allied countries of Argentina, Brazil, and Uruguay and which other country?", "answer": ["Paraguay"], "scores": [[0.847631573677063, 0.9971311092376709, 0.998239278793335]], "normalized_score": [0.9449265003204346], "tokens": [["\u2581Par", "agu", "ay"]], "entropy": [[0.8126270174980164, 0.02797088772058487, 0.01872161775827408]], "avg_entropy": [0.28643984099229175], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Administrative divisions of Paraguay", "score_of_biggest_cluster": 0.9449265003204346, "score_of_first_answer": 0.9449265003204346, "max_score_of_first_answer": 0.998239278793335, "min_score_of_first_answer": 0.847631573677063, "avg_score": 0.9449265003204346, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07073740171538322, "std_of_first_answer": 0.07073740171538322, "avg_entropy_of_first_answer": 0.28643984099229175, "lex_sim": 1.0}
{"question": "What is a vent in the Earth's crust that spouts a fountain of boiling water called?", "answer": ["Fumarole"], "scores": [[0.04277622327208519, 0.8413704633712769, 0.958909809589386]], "normalized_score": [0.32557857036590576], "tokens": [["\u2581F", "umar", "ole"]], "entropy": [[3.295125961303711, 0.930599570274353, 0.22419193387031555]], "avg_entropy": [1.4833058218161266], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Geyser", "score_of_biggest_cluster": 0.32557857036590576, "score_of_first_answer": 0.32557857036590576, "max_score_of_first_answer": 0.958909809589386, "min_score_of_first_answer": 0.04277622327208519, "avg_score": 0.32557857036590576, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40700382283320485, "std_of_first_answer": 0.40700382283320485, "avg_entropy_of_first_answer": 1.4833058218161266, "lex_sim": 1.0}
{"question": "Who married Sarah Hugill in 1972, had two children, divorced in 1983, married Sarah Brightman in 1984, divorced in 1990, married Madeleine Gurdon in 1991, and had three children?", "answer": ["Rod Stewart"], "scores": [[0.010418706573545933, 0.9012628793716431]], "normalized_score": [0.09690199047327042], "tokens": [["\u2581Rod", "\u2581Stewart"]], "entropy": [[2.2660655975341797, 0.6914671659469604]], "avg_entropy": [1.47876638174057], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Andrew Lloyd Weber", "score_of_biggest_cluster": 0.09690199047327042, "score_of_first_answer": 0.09690199047327042, "max_score_of_first_answer": 0.9012628793716431, "min_score_of_first_answer": 0.010418706573545933, "avg_score": 0.09690199047327042, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44542208639904857, "std_of_first_answer": 0.44542208639904857, "avg_entropy_of_first_answer": 1.47876638174057, "lex_sim": 1.0}
{"question": "Who, in the years prior to World War II, conceived and oversaw the development of an integrated air defence system for England which included radar, human observers, raid plotting and radio control of aircraft?", "answer": ["Henry Tizard"], "scores": [[0.004268054384738207, 0.6526063084602356, 0.9974699020385742]], "normalized_score": [0.14058110117912292], "tokens": [["\u2581Henry", "\u2581T", "izard"]], "entropy": [[3.3770461082458496, 2.0753068923950195, 0.027592159807682037]], "avg_entropy": [1.8266483868161838], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hugh Caswell Tremenheere Dowding", "score_of_biggest_cluster": 0.14058110117912292, "score_of_first_answer": 0.14058110117912292, "max_score_of_first_answer": 0.9974699020385742, "min_score_of_first_answer": 0.004268054384738207, "avg_score": 0.14058110117912292, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41173390814624483, "std_of_first_answer": 0.41173390814624483, "avg_entropy_of_first_answer": 1.8266483868161838, "lex_sim": 1.0}
{"question": "Who composed the 1965 choral work Chichester Psalms?", "answer": ["Leonard Bernstein"], "scores": [[0.6833730340003967, 0.9845262765884399, 0.9950302839279175]], "normalized_score": [0.8747966885566711], "tokens": [["\u2581Leonard", "\u2581Bern", "stein"]], "entropy": [[2.01955509185791, 0.15063005685806274, 0.04875366389751434]], "avg_entropy": [0.7396462708711624], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ballets by Leonard Bernstein", "score_of_biggest_cluster": 0.8747966885566711, "score_of_first_answer": 0.8747966885566711, "max_score_of_first_answer": 0.9950302839279175, "min_score_of_first_answer": 0.6833730340003967, "avg_score": 0.8747966885566711, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14450446018639998, "std_of_first_answer": 0.14450446018639998, "avg_entropy_of_first_answer": 0.7396462708711624, "lex_sim": 1.0}
{"question": "What is Koo Stark's real first name ?", "answer": ["Kathleen Ann"], "scores": [[0.009493178687989712, 0.6000133752822876, 0.9982049465179443, 0.020508354529738426]], "normalized_score": [0.10391560941934586], "tokens": [["\u2581Kath", "le", "en", "\u2581Ann"]], "entropy": [[5.432586669921875, 0.949258029460907, 0.01866382732987404, 1.6330618858337402]], "avg_entropy": [2.008392603136599], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kathleen", "score_of_biggest_cluster": 0.10391560941934586, "score_of_first_answer": 0.10391560941934586, "max_score_of_first_answer": 0.9982049465179443, "min_score_of_first_answer": 0.009493178687989712, "avg_score": 0.10391560941934586, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4165827963169441, "std_of_first_answer": 0.4165827963169441, "avg_entropy_of_first_answer": 2.008392603136599, "lex_sim": 1.0}
{"question": "A Cuba Libre is based on which spirit", "answer": ["Rum"], "scores": [[0.3952190577983856]], "normalized_score": [0.3952190577983856], "tokens": [["\u2581Rum"]], "entropy": [[3.140056610107422]], "avg_entropy": [3.140056610107422], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Spiced Rum", "score_of_biggest_cluster": 0.3952190577983856, "score_of_first_answer": 0.3952190577983856, "max_score_of_first_answer": 0.3952190577983856, "min_score_of_first_answer": 0.3952190577983856, "avg_score": 0.3952190577983856, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.140056610107422, "lex_sim": 1.0}
{"question": "Which German-born chemist split the uranium atom in 1939 and won the Nobel prize in 1944?", "answer": ["Otto Hahn"], "scores": [[0.6771093010902405, 0.7858805060386658, 0.9991629123687744]], "normalized_score": [0.8101221919059753], "tokens": [["\u2581Otto", "\u2581H", "ahn"]], "entropy": [[1.7558815479278564, 1.3427995443344116, 0.009369123727083206]], "avg_entropy": [1.0360167386631172], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Otto Hahn", "score_of_biggest_cluster": 0.8101221919059753, "score_of_first_answer": 0.8101221919059753, "max_score_of_first_answer": 0.9991629123687744, "min_score_of_first_answer": 0.6771093010902405, "avg_score": 0.8101221919059753, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.13376558617947168, "std_of_first_answer": 0.13376558617947168, "avg_entropy_of_first_answer": 1.0360167386631172, "lex_sim": 1.0}
{"question": "What is the name of the tallest skyscraper in Saudi Arabia, one of the world's 50th tallest buildings, that is in Riyadh?", "answer": ["Kingdom Tower"], "scores": [[0.6576096415519714, 0.33587566018104553]], "normalized_score": [0.46997347474098206], "tokens": [["\u2581Kingdom", "\u2581Tower"]], "entropy": [[1.799243688583374, 1.4469194412231445]], "avg_entropy": [1.6230815649032593], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kingdom Center", "score_of_biggest_cluster": 0.46997347474098206, "score_of_first_answer": 0.46997347474098206, "max_score_of_first_answer": 0.6576096415519714, "min_score_of_first_answer": 0.33587566018104553, "avg_score": 0.46997347474098206, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.16086699068546295, "std_of_first_answer": 0.16086699068546295, "avg_entropy_of_first_answer": 1.6230815649032593, "lex_sim": 1.0}
{"question": "The Florentine Girdle was a type of what?", "answer": ["Girdle"], "scores": [[0.03467525541782379, 0.8777849674224854, 0.9866781234741211]], "normalized_score": [0.3108334541320801], "tokens": [["\u2581G", "ird", "le"]], "entropy": [[5.685505390167236, 0.6828999519348145, 0.10657680034637451]], "avg_entropy": [2.1583273808161416], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Girdle of chastity", "score_of_biggest_cluster": 0.3108334541320801, "score_of_first_answer": 0.3108334541320801, "max_score_of_first_answer": 0.9866781234741211, "min_score_of_first_answer": 0.03467525541782379, "avg_score": 0.3108334541320801, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4254411000752154, "std_of_first_answer": 0.4254411000752154, "avg_entropy_of_first_answer": 2.1583273808161416, "lex_sim": 1.0}
{"question": "In April 1963, Lester Bowles Pearson became Prime Minister of which country?", "answer": ["CANADA"], "scores": [[0.003920820076018572, 0.16473110020160675, 0.9738196134567261, 0.978792130947113]], "normalized_score": [0.15751804411411285], "tokens": [["\u2581C", "AN", "AD", "A"]], "entropy": [[2.2473578453063965, 2.511901617050171, 0.21426144242286682, 0.15359780192375183]], "avg_entropy": [1.2817796766757965], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Canada", "score_of_biggest_cluster": 0.15751804411411285, "score_of_first_answer": 0.15751804411411285, "max_score_of_first_answer": 0.978792130947113, "min_score_of_first_answer": 0.003920820076018572, "avg_score": 0.15751804411411285, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.44960274120223037, "std_of_first_answer": 0.44960274120223037, "avg_entropy_of_first_answer": 1.2817796766757965, "lex_sim": 1.0}
{"question": "Which double glazing firm used a feather in their advertising commercials", "answer": ["The Warmest Place in the Universe"], "scores": [[0.017018796876072884, 0.012898040935397148, 0.14249210059642792, 0.058256082236766815, 0.14808762073516846, 0.17597942054271698, 0.6646320819854736, 0.1876206398010254, 0.9979584217071533]], "normalized_score": [0.12182226032018661], "tokens": [["\u2581The", "\u2581W", "arm", "est", "\u2581Place", "\u2581in", "\u2581the", "\u2581Un", "iverse"]], "entropy": [[5.6783952713012695, 5.803127288818359, 3.659073829650879, 4.4687604904174805, 3.4783878326416016, 2.3456737995147705, 1.7169650793075562, 2.093791961669922, 0.022519536316394806]], "avg_entropy": [3.2518550099598036], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mt. Everest expedition", "score_of_biggest_cluster": 0.12182226032018661, "score_of_first_answer": 0.12182226032018661, "max_score_of_first_answer": 0.9979584217071533, "min_score_of_first_answer": 0.012898040935397148, "avg_score": 0.12182226032018661, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3175027030004723, "std_of_first_answer": 0.3175027030004723, "avg_entropy_of_first_answer": 3.2518550099598036, "lex_sim": 1.0}
{"question": "What is the appropriate name of the largest type of 'frog'?", "answer": ["Toad"], "scores": [[0.6145620346069336, 0.9562062621116638]], "normalized_score": [0.7665820717811584], "tokens": [["\u2581To", "ad"]], "entropy": [[2.6957199573516846, 0.20310375094413757]], "avg_entropy": [1.449411854147911], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Goliath (La Ronde)", "score_of_biggest_cluster": 0.7665820717811584, "score_of_first_answer": 0.7665820717811584, "max_score_of_first_answer": 0.9562062621116638, "min_score_of_first_answer": 0.6145620346069336, "avg_score": 0.7665820717811584, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1708221137523651, "std_of_first_answer": 0.1708221137523651, "avg_entropy_of_first_answer": 1.449411854147911, "lex_sim": 1.0}
{"question": "In November 2010, Genoveva Aonma and sisters Bilguissa Simpor and Salimata Simpor of which national women's soccer team were accused by newspapers in Nigeria and Cameroon of being men, not women?", "answer": ["Moroccan women's national soccer team"], "scores": [[0.004347450099885464, 0.9208229780197144, 0.34084680676460266, 0.06185102462768555, 0.7929296493530273, 0.9978844523429871, 0.2713162302970886, 0.49222373962402344, 0.9992738366127014, 0.9724423885345459]], "normalized_score": [0.31173232197761536], "tokens": [["\u2581Mor", "oc", "can", "\u2581women", "'", "s", "\u2581national", "\u2581s", "occer", "\u2581team"]], "entropy": [[3.2553963661193848, 0.6482481956481934, 0.7032551765441895, 0.980344831943512, 0.8204748630523682, 0.022635994479060173, 1.478224754333496, 1.2278270721435547, 0.006894631311297417, 0.18536421656608582]], "avg_entropy": [0.9328666102141142], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "GNQ", "score_of_biggest_cluster": 0.31173232197761536, "score_of_first_answer": 0.31173232197761536, "max_score_of_first_answer": 0.9992738366127014, "min_score_of_first_answer": 0.004347450099885464, "avg_score": 0.31173232197761536, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37758127672268216, "std_of_first_answer": 0.37758127672268216, "avg_entropy_of_first_answer": 0.9328666102141142, "lex_sim": 1.0}
{"question": "Who was married to Guinevere?", "answer": ["King Arthur"], "scores": [[0.4150870144367218, 0.9514572024345398]], "normalized_score": [0.6284405589103699], "tokens": [["\u2581King", "\u2581Arthur"]], "entropy": [[2.1778299808502197, 0.3573068380355835]], "avg_entropy": [1.2675684094429016], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Arthurian legend", "score_of_biggest_cluster": 0.6284405589103699, "score_of_first_answer": 0.6284405589103699, "max_score_of_first_answer": 0.9514572024345398, "min_score_of_first_answer": 0.4150870144367218, "avg_score": 0.6284405589103699, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.268185093998909, "std_of_first_answer": 0.268185093998909, "avg_entropy_of_first_answer": 1.2675684094429016, "lex_sim": 1.0}
{"question": "Which mythical hero wielded a magical sword called 'Gram'?", "answer": ["Sigurd"], "scores": [[0.10791651904582977, 0.9274699687957764]], "normalized_score": [0.316368967294693], "tokens": [["\u2581Sig", "urd"]], "entropy": [[3.428398609161377, 0.4843921661376953]], "avg_entropy": [1.9563953876495361], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Siegfried (name)", "score_of_biggest_cluster": 0.316368967294693, "score_of_first_answer": 0.316368967294693, "max_score_of_first_answer": 0.9274699687957764, "min_score_of_first_answer": 0.10791651904582977, "avg_score": 0.316368967294693, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4097767248749733, "std_of_first_answer": 0.4097767248749733, "avg_entropy_of_first_answer": 1.9563953876495361, "lex_sim": 1.0}
{"question": "In which year did South Korea hold its first 'Grand Prix' motor race?", "answer": ["1988"], "scores": [[0.5390945076942444, 0.8666692972183228, 0.981006383895874, 0.6218041181564331, 0.22465817630290985]], "normalized_score": [0.5771294236183167], "tokens": [["\u2581", "1", "9", "8", "8"]], "entropy": [[2.518117904663086, 0.5125049352645874, 0.14888368546962738, 1.2440035343170166, 2.1941800117492676]], "avg_entropy": [1.323538014292717], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "two thousand and ten", "score_of_biggest_cluster": 0.5771294236183167, "score_of_first_answer": 0.5771294236183167, "max_score_of_first_answer": 0.981006383895874, "min_score_of_first_answer": 0.22465817630290985, "avg_score": 0.5771294236183167, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2647509333603629, "std_of_first_answer": 0.2647509333603629, "avg_entropy_of_first_answer": 1.323538014292717, "lex_sim": 1.0}
{"question": "With which famous male criminal would you associate the address 25 Cromwell Street, Gloucester?", "answer": ["John Pewtis"], "scores": [[0.09038050472736359, 0.0014618918066844344, 0.01894809678196907, 0.0011356496252119541]], "normalized_score": [0.007302138023078442], "tokens": [["\u2581John", "\u2581P", "ew", "tis"]], "entropy": [[3.6074161529541016, 1.3619685173034668, 4.279538631439209, 2.8083910942077637]], "avg_entropy": [3.0143285989761353], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "FRED WEST", "score_of_biggest_cluster": 0.007302138023078442, "score_of_first_answer": 0.007302138023078442, "max_score_of_first_answer": 0.09038050472736359, "min_score_of_first_answer": 0.0011356496252119541, "avg_score": 0.007302138023078442, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.03673971783279114, "std_of_first_answer": 0.03673971783279114, "avg_entropy_of_first_answer": 3.0143285989761353, "lex_sim": 1.0}
{"question": "\"What country's film industry is called \"\"Nollywood\"\"?\"", "answer": ["Nigeria"], "scores": [[0.9054802656173706, 0.9750863313674927]], "normalized_score": [0.9396389722824097], "tokens": [["\u2581Niger", "ia"]], "entropy": [[0.7384082078933716, 0.15338820219039917]], "avg_entropy": [0.4458982050418854], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nigerian Independence", "score_of_biggest_cluster": 0.9396389722824097, "score_of_first_answer": 0.9396389722824097, "max_score_of_first_answer": 0.9750863313674927, "min_score_of_first_answer": 0.9054802656173706, "avg_score": 0.9396389722824097, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.034803032875061035, "std_of_first_answer": 0.034803032875061035, "avg_entropy_of_first_answer": 0.4458982050418854, "lex_sim": 1.0}
{"question": "What is the minimum number of darts a person needs to throw to complete a leg from 501?", "answer": ["Three darts"], "scores": [[0.09781154245138168, 0.02744736149907112, 0.9870328903198242]], "normalized_score": [0.1383802443742752], "tokens": [["\u2581Three", "\u2581d", "arts"]], "entropy": [[3.0127711296081543, 0.5077043771743774, 0.07529568672180176]], "avg_entropy": [1.1985903978347778], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "9", "score_of_biggest_cluster": 0.1383802443742752, "score_of_first_answer": 0.1383802443742752, "max_score_of_first_answer": 0.9870328903198242, "min_score_of_first_answer": 0.02744736149907112, "avg_score": 0.1383802443742752, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43671375198918766, "std_of_first_answer": 0.43671375198918766, "avg_entropy_of_first_answer": 1.1985903978347778, "lex_sim": 1.0}
{"question": "In 1941, Iran was invaded by Britain and which other country?", "answer": ["the Soviet Union"], "scores": [[0.07736463844776154, 0.5591345429420471, 0.9817850589752197]], "normalized_score": [0.34889259934425354], "tokens": [["\u2581the", "\u2581Soviet", "\u2581Union"]], "entropy": [[2.7334160804748535, 1.9207401275634766, 0.13990119099617004]], "avg_entropy": [1.5980191330115001], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Russia (1991-1993)", "score_of_biggest_cluster": 0.34889259934425354, "score_of_first_answer": 0.34889259934425354, "max_score_of_first_answer": 0.9817850589752197, "min_score_of_first_answer": 0.07736463844776154, "avg_score": 0.34889259934425354, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3694909405965944, "std_of_first_answer": 0.3694909405965944, "avg_entropy_of_first_answer": 1.5980191330115001, "lex_sim": 1.0}
{"question": "A popular 1960s TV cowboy series, what is a cutaneous covering that has not been tanned?", "answer": ["Buckaroo"], "scores": [[0.005372572690248489, 0.10593099147081375, 0.9926077127456665]], "normalized_score": [0.08266613632440567], "tokens": [["\u2581Buck", "ar", "oo"]], "entropy": [[5.290445327758789, 1.9194550514221191, 0.07353345304727554]], "avg_entropy": [2.4278112774093947], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Raw-hide", "score_of_biggest_cluster": 0.08266613632440567, "score_of_first_answer": 0.08266613632440567, "max_score_of_first_answer": 0.9926077127456665, "min_score_of_first_answer": 0.005372572690248489, "avg_score": 0.08266613632440567, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4435890021428258, "std_of_first_answer": 0.4435890021428258, "avg_entropy_of_first_answer": 2.4278112774093947, "lex_sim": 1.0}
{"question": "Which coral atoll, the largest island in the Chagos Archipelago, was forcibly depopulated in 1971 to make way for a joint UK/USA military base?", "answer": ["Diego Garcia"], "scores": [[0.9267643094062805, 0.9818618893623352, 0.9921588897705078]], "normalized_score": [0.9664965271949768], "tokens": [["\u2581Diego", "\u2581Gar", "cia"]], "entropy": [[0.6431344747543335, 0.14963801205158234, 0.06762807071208954]], "avg_entropy": [0.28680018583933514], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Deigo garcia", "score_of_biggest_cluster": 0.9664965271949768, "score_of_first_answer": 0.9664965271949768, "max_score_of_first_answer": 0.9921588897705078, "min_score_of_first_answer": 0.9267643094062805, "avg_score": 0.9664965271949768, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.028709701564750098, "std_of_first_answer": 0.028709701564750098, "avg_entropy_of_first_answer": 0.28680018583933514, "lex_sim": 1.0}
{"question": "Taka is the basic monetary unit of which country?", "answer": ["Tonga"], "scores": [[0.031124550849199295, 0.33701398968696594, 0.9892536997795105]], "normalized_score": [0.21811532974243164], "tokens": [["\u2581T", "ong", "a"]], "entropy": [[3.97263240814209, 2.1099071502685547, 0.0831456407904625]], "avg_entropy": [2.055228399733702], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bangladeish", "score_of_biggest_cluster": 0.21811532974243164, "score_of_first_answer": 0.21811532974243164, "max_score_of_first_answer": 0.9892536997795105, "min_score_of_first_answer": 0.031124550849199295, "avg_score": 0.21811532974243164, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3995826231128916, "std_of_first_answer": 0.3995826231128916, "avg_entropy_of_first_answer": 2.055228399733702, "lex_sim": 1.0}
{"question": "Which official residence is No.12 Downing Street?", "answer": ["UK HM Government 12 DOWNING STREET"], "scores": [[0.014298256486654282, 0.002217605710029602, 0.4807928502559662, 0.2267201989889145, 0.002631840296089649, 0.7692223191261292, 0.1778983473777771, 0.009008672088384628, 0.9175431728363037, 0.9648865461349487, 0.9852272272109985, 0.9199883937835693, 0.9349142909049988, 0.9991806149482727]], "normalized_score": [0.16177064180374146], "tokens": [["\u2581UK", "\u2581H", "M", "\u2581Government", "\u2581", "1", "2", "\u2581D", "OW", "N", "ING", "\u2581ST", "RE", "ET"]], "entropy": [[3.556398630142212, 1.649958848953247, 2.0804574489593506, 2.623248815536499, 1.989335060119629, 1.4479255676269531, 0.7922148108482361, 1.6474876403808594, 0.6707371473312378, 0.266056627035141, 0.12163451313972473, 0.4986380338668823, 0.35066407918930054, 0.009259190410375595]], "avg_entropy": [1.2645726009671177], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chief whip in the House of Commons", "score_of_biggest_cluster": 0.16177064180374146, "score_of_first_answer": 0.16177064180374146, "max_score_of_first_answer": 0.9991806149482727, "min_score_of_first_answer": 0.002217605710029602, "avg_score": 0.16177064180374146, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41853164157264744, "std_of_first_answer": 0.41853164157264744, "avg_entropy_of_first_answer": 1.2645726009671177, "lex_sim": 1.0}
{"question": "What is the name of me political party, led by Alexis Tsipras,which won the general election in Greece in January 2015?", "answer": ["Coalition of the Radical Left"], "scores": [[0.053971294313669205, 0.985529899597168, 0.983475923538208, 0.92266446352005, 0.9641920924186707, 0.947636604309082, 0.9813017249107361, 0.9557821154594421]], "normalized_score": [0.6715461611747742], "tokens": [["\u2581Co", "al", "ition", "\u2581of", "\u2581the", "\u2581Rad", "ical", "\u2581Left"]], "entropy": [[1.9330530166625977, 0.11685244739055634, 0.15842583775520325, 0.5036047697067261, 0.27368780970573425, 0.31903278827667236, 0.13693222403526306, 0.28420203924179077]], "avg_entropy": [0.46572386659681797], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Coalition of the radical left", "score_of_biggest_cluster": 0.6715461611747742, "score_of_first_answer": 0.6715461611747742, "max_score_of_first_answer": 0.985529899597168, "min_score_of_first_answer": 0.053971294313669205, "avg_score": 0.6715461611747742, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3012733461012453, "std_of_first_answer": 0.3012733461012453, "avg_entropy_of_first_answer": 0.46572386659681797, "lex_sim": 1.0}
{"question": "'Sanstorm', 'Walkaway' and 'Flying' were top ten hits for which group in the mid '90's?", "answer": ["ABC"], "scores": [[0.0049524023197591305]], "normalized_score": [0.004952403251081705], "tokens": [["\u2581ABC"]], "entropy": [[4.814220428466797]], "avg_entropy": [4.814220428466797], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "CAST", "score_of_biggest_cluster": 0.004952403251081705, "score_of_first_answer": 0.004952403251081705, "max_score_of_first_answer": 0.0049524023197591305, "min_score_of_first_answer": 0.0049524023197591305, "avg_score": 0.004952403251081705, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.814220428466797, "lex_sim": 1.0}
{"question": "What is the name for the process by which heat is transferred by the motion of a fluid?", "answer": ["Fourier's law"], "scores": [[0.00016285930178128183, 0.48887738585472107, 0.998348593711853, 0.39499664306640625]], "normalized_score": [0.07485520839691162], "tokens": [["\u2581Fourier", "'", "s", "\u2581law"]], "entropy": [[1.1377487182617188, 2.372922420501709, 0.01911243051290512, 1.7067886590957642]], "avg_entropy": [1.3091430570930243], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Heat convection", "score_of_biggest_cluster": 0.07485520839691162, "score_of_first_answer": 0.07485520839691162, "max_score_of_first_answer": 0.998348593711853, "min_score_of_first_answer": 0.00016285930178128183, "avg_score": 0.07485520839691162, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35562607183450823, "std_of_first_answer": 0.35562607183450823, "avg_entropy_of_first_answer": 1.3091430570930243, "lex_sim": 1.0}
{"question": "The Royal Navy introduced limes to the standard diet for crewmen to prevent what?", "answer": ["Scurvy"], "scores": [[0.6728841066360474, 0.9984793066978455, 0.9947138428688049]], "normalized_score": [0.8742973804473877], "tokens": [["\u2581Sc", "ur", "vy"]], "entropy": [[1.1083110570907593, 0.013259283266961575, 0.03866750746965408]], "avg_entropy": [0.38674594927579165], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Scruvy", "score_of_biggest_cluster": 0.8742973804473877, "score_of_first_answer": 0.8742973804473877, "max_score_of_first_answer": 0.9984793066978455, "min_score_of_first_answer": 0.6728841066360474, "avg_score": 0.8742973804473877, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1526072636048991, "std_of_first_answer": 0.1526072636048991, "avg_entropy_of_first_answer": 0.38674594927579165, "lex_sim": 1.0}
{"question": "Who was the friend of Little Jackie Paper?", "answer": ["Jack Frost"], "scores": [[0.02761242352426052, 0.6398957967758179, 0.9992766976356506]], "normalized_score": [0.260395348072052], "tokens": [["\u2581Jack", "\u2581F", "rost"]], "entropy": [[4.869972229003906, 2.007211208343506, 0.0092382300645113]], "avg_entropy": [2.2954738891373077], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Honalee", "score_of_biggest_cluster": 0.260395348072052, "score_of_first_answer": 0.260395348072052, "max_score_of_first_answer": 0.9992766976356506, "min_score_of_first_answer": 0.02761242352426052, "avg_score": 0.260395348072052, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40113408989008836, "std_of_first_answer": 0.40113408989008836, "avg_entropy_of_first_answer": 2.2954738891373077, "lex_sim": 1.0}
{"question": "In golf, the holing of a ball directly from a bunker is known as a \u2018Golden \u2018what\u2019?", "answer": ["Birdie"], "scores": [[0.3461904525756836, 0.9318495988845825]], "normalized_score": [0.5679765939712524], "tokens": [["\u2581Bird", "ie"]], "entropy": [[3.55019474029541, 0.31779158115386963]], "avg_entropy": [1.93399316072464], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ferret food", "score_of_biggest_cluster": 0.5679765939712524, "score_of_first_answer": 0.5679765939712524, "max_score_of_first_answer": 0.9318495988845825, "min_score_of_first_answer": 0.3461904525756836, "avg_score": 0.5679765939712524, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29282957315444946, "std_of_first_answer": 0.29282957315444946, "avg_entropy_of_first_answer": 1.93399316072464, "lex_sim": 1.0}
{"question": "On what date does official grouse shooting start", "answer": ["12th August"], "scores": [[0.4416174292564392, 0.9734634160995483, 0.5574131608009338, 0.4227868318557739, 0.4471980035305023]], "normalized_score": [0.5385584831237793], "tokens": [["\u2581", "1", "2", "th", "\u2581August"]], "entropy": [[2.206070899963379, 0.19890549778938293, 1.572957158088684, 1.8767673969268799, 1.7215030193328857]], "avg_entropy": [1.5152407944202424], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Aug 12", "score_of_biggest_cluster": 0.5385584831237793, "score_of_first_answer": 0.5385584831237793, "max_score_of_first_answer": 0.9734634160995483, "min_score_of_first_answer": 0.4227868318557739, "avg_score": 0.5385584831237793, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.20792497433906976, "std_of_first_answer": 0.20792497433906976, "avg_entropy_of_first_answer": 1.5152407944202424, "lex_sim": 1.0}
{"question": "Which composer did pianist Clara Wieck marry in 1840?", "answer": ["Fr\u00e9d\u00e9ric Chopin"], "scores": [[0.0012105269124731421, 0.31859010457992554, 0.9812154173851013, 0.8451383113861084, 0.9989331364631653, 0.9989598989486694]], "normalized_score": [0.2614150643348694], "tokens": [["\u2581Fr", "\u00e9d", "\u00e9ric", "\u2581Ch", "op", "in"]], "entropy": [[1.0568612813949585, 2.8378987312316895, 0.15465322136878967, 0.6931295394897461, 0.01231759786605835, 0.012279939837753773]], "avg_entropy": [0.794523385198166], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Schumann, Robert Alexander", "score_of_biggest_cluster": 0.2614150643348694, "score_of_first_answer": 0.2614150643348694, "max_score_of_first_answer": 0.9989598989486694, "min_score_of_first_answer": 0.0012105269124731421, "avg_score": 0.2614150643348694, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3899024953674752, "std_of_first_answer": 0.3899024953674752, "avg_entropy_of_first_answer": 0.794523385198166, "lex_sim": 1.0}
{"question": "Which long-running British TV crime series was created by writer Glen Chandler?", "answer": ["The Bill"], "scores": [[0.3239808678627014, 0.6464855074882507]], "normalized_score": [0.4576559066772461], "tokens": [["\u2581The", "\u2581Bill"]], "entropy": [[3.7332255840301514, 1.8314368724822998]], "avg_entropy": [2.7823312282562256], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Killer (Taggart)", "score_of_biggest_cluster": 0.4576559066772461, "score_of_first_answer": 0.4576559066772461, "max_score_of_first_answer": 0.6464855074882507, "min_score_of_first_answer": 0.3239808678627014, "avg_score": 0.4576559066772461, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.16125231981277466, "std_of_first_answer": 0.16125231981277466, "avg_entropy_of_first_answer": 2.7823312282562256, "lex_sim": 1.0}
{"question": "\"What purple berry from South America has been promoted as a \"\"superfood\"\"?\"", "answer": ["Acai berry"], "scores": [[0.39318785071372986, 0.922809898853302, 0.9666568040847778, 0.271079957485199, 0.8169381022453308]], "normalized_score": [0.5998660326004028], "tokens": [["\u2581A", "ca", "i", "\u2581ber", "ry"]], "entropy": [[3.128108024597168, 0.4734639823436737, 0.2498764842748642, 1.335541009902954, 0.5004119277000427]], "avg_entropy": [1.1374802857637405], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Assai palm", "score_of_biggest_cluster": 0.5998660326004028, "score_of_first_answer": 0.5998660326004028, "max_score_of_first_answer": 0.9666568040847778, "min_score_of_first_answer": 0.271079957485199, "avg_score": 0.5998660326004028, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28607188089120333, "std_of_first_answer": 0.28607188089120333, "avg_entropy_of_first_answer": 1.1374802857637405, "lex_sim": 1.0}
{"question": "From which country was Fulgencio Batista ousted in 1959?", "answer": ["Fulgens Batista"], "scores": [[0.002615888137370348, 0.007872351445257664, 0.014855250716209412, 0.7879839539527893]], "normalized_score": [0.022157983854413033], "tokens": [["\u2581Ful", "gens", "\u2581Bat", "ista"]], "entropy": [[1.2440065145492554, 0.7333431243896484, 3.366178035736084, 1.378833532333374]], "avg_entropy": [1.6805903017520905], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Republic of Cuba", "score_of_biggest_cluster": 0.022157983854413033, "score_of_first_answer": 0.022157983854413033, "max_score_of_first_answer": 0.7879839539527893, "min_score_of_first_answer": 0.002615888137370348, "avg_score": 0.022157983854413033, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3375769630972014, "std_of_first_answer": 0.3375769630972014, "avg_entropy_of_first_answer": 1.6805903017520905, "lex_sim": 1.0}
{"question": "The John Brown Shipyard is/was on which river?", "answer": ["River Avon"], "scores": [[0.042625389993190765, 0.021640334278345108, 0.9962713718414307]], "normalized_score": [0.09722322225570679], "tokens": [["\u2581River", "\u2581Av", "on"]], "entropy": [[2.5108020305633545, 0.8871323466300964, 0.0414714589715004]], "avg_entropy": [1.1464686120549838], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Clyde", "score_of_biggest_cluster": 0.09722322225570679, "score_of_first_answer": 0.09722322225570679, "max_score_of_first_answer": 0.9962713718414307, "min_score_of_first_answer": 0.021640334278345108, "avg_score": 0.09722322225570679, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4545799881502271, "std_of_first_answer": 0.4545799881502271, "avg_entropy_of_first_answer": 1.1464686120549838, "lex_sim": 1.0}
{"question": "\"From which musical does the song \"\"Till There Was You\"\" come from?\"", "answer": ["The Music Man"], "scores": [[0.7278413772583008, 0.8933672904968262, 0.9810255169868469]], "normalized_score": [0.8608266115188599], "tokens": [["\u2581The", "\u2581Music", "\u2581Man"]], "entropy": [[1.5519707202911377, 0.710594654083252, 0.1665755957365036]], "avg_entropy": [0.809713656703631], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Music Man (disambiguation)", "score_of_biggest_cluster": 0.8608266115188599, "score_of_first_answer": 0.8608266115188599, "max_score_of_first_answer": 0.9810255169868469, "min_score_of_first_answer": 0.7278413772583008, "avg_score": 0.8608266115188599, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1049788344989956, "std_of_first_answer": 0.1049788344989956, "avg_entropy_of_first_answer": 0.809713656703631, "lex_sim": 1.0}
{"question": "What nickname was given to the Morris Cowley due to its appearance?", "answer": ["The Cowley"], "scores": [[0.17967325448989868, 0.21120531857013702, 0.7686271667480469]], "normalized_score": [0.30782321095466614], "tokens": [["\u2581The", "\u2581Cow", "ley"]], "entropy": [[4.517144203186035, 4.538658142089844, 1.2229009866714478]], "avg_entropy": [3.4262344439824424], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bullnose", "score_of_biggest_cluster": 0.30782321095466614, "score_of_first_answer": 0.30782321095466614, "max_score_of_first_answer": 0.7686271667480469, "min_score_of_first_answer": 0.17967325448989868, "avg_score": 0.30782321095466614, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2705098270253197, "std_of_first_answer": 0.2705098270253197, "avg_entropy_of_first_answer": 3.4262344439824424, "lex_sim": 1.0}
{"question": "Who was the longest reigning British king?", "answer": ["Queen Victoria of The United Kingdom of Great Britain and Ireland"], "scores": [[0.04264737293124199, 0.7703725099563599, 0.00388269848190248, 0.006405512802302837, 0.9283561110496521, 0.9902032613754272, 0.17790542542934418, 0.9933308959007263, 0.9834669232368469, 0.9256011843681335, 0.9637238383293152]], "normalized_score": [0.23423351347446442], "tokens": [["\u2581Queen", "\u2581Victoria", "\u2581of", "\u2581The", "\u2581United", "\u2581Kingdom", "\u2581of", "\u2581Great", "\u2581Britain", "\u2581and", "\u2581Ireland"]], "entropy": [[2.7894468307495117, 0.9014643430709839, 0.5910466909408569, 1.688334345817566, 0.4459397792816162, 0.07932785153388977, 1.3419179916381836, 0.06508368998765945, 0.12110085785388947, 0.39613205194473267, 0.21485067903995514]], "avg_entropy": [0.7849677374417131], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George III, King of Great Britain and Ireland", "score_of_biggest_cluster": 0.23423351347446442, "score_of_first_answer": 0.23423351347446442, "max_score_of_first_answer": 0.9933308959007263, "min_score_of_first_answer": 0.00388269848190248, "avg_score": 0.23423351347446442, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42879798348991843, "std_of_first_answer": 0.42879798348991843, "avg_entropy_of_first_answer": 0.7849677374417131, "lex_sim": 1.0}
{"question": "What is secreted by the pancreas to regulate blood sugar levels?", "answer": ["Pancreatic juice"], "scores": [[0.09741951525211334, 0.9928620457649231, 0.8327934145927429, 0.510722279548645, 0.8233007192611694]], "normalized_score": [0.5081162452697754], "tokens": [["\u2581Pan", "cre", "atic", "\u2581ju", "ice"]], "entropy": [[2.0074241161346436, 0.06950940936803818, 0.7879534959793091, 2.5002641677856445, 0.47961175441741943]], "avg_entropy": [1.168952588737011], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Insulin antagonists", "score_of_biggest_cluster": 0.5081162452697754, "score_of_first_answer": 0.5081162452697754, "max_score_of_first_answer": 0.9928620457649231, "min_score_of_first_answer": 0.09741951525211334, "avg_score": 0.5081162452697754, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31803595816548297, "std_of_first_answer": 0.31803595816548297, "avg_entropy_of_first_answer": 1.168952588737011, "lex_sim": 1.0}
{"question": "How many edges are there on a tetrahedron?", "answer": ["Six"], "scores": [[0.058929894119501114]], "normalized_score": [0.058929890394210815], "tokens": [["\u2581Six"]], "entropy": [[2.3801825046539307]], "avg_entropy": [2.3801825046539307], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "six", "score_of_biggest_cluster": 0.058929890394210815, "score_of_first_answer": 0.058929890394210815, "max_score_of_first_answer": 0.058929894119501114, "min_score_of_first_answer": 0.058929894119501114, "avg_score": 0.058929890394210815, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.3801825046539307, "lex_sim": 1.0}
{"question": "In which city did the gangland St Valentine\u2019s Day Massacre take place?", "answer": ["Chicago"], "scores": [[0.8797907829284668]], "normalized_score": [0.8797907829284668], "tokens": [["\u2581Chicago"]], "entropy": [[0.9357470870018005]], "avg_entropy": [0.9357470870018005], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chi-Beria", "score_of_biggest_cluster": 0.8797907829284668, "score_of_first_answer": 0.8797907829284668, "max_score_of_first_answer": 0.8797907829284668, "min_score_of_first_answer": 0.8797907829284668, "avg_score": 0.8797907829284668, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.9357470870018005, "lex_sim": 1.0}
{"question": "What year was the final Harry Potter book, Harry Potter and the Deathly Hallows, released?", "answer": ["2007"], "scores": [[0.4960562586784363, 0.9162107706069946, 0.9712018370628357, 0.906921923160553, 0.9609125256538391]], "normalized_score": [0.8260720372200012], "tokens": [["\u2581", "2", "0", "0", "7"]], "entropy": [[2.519944906234741, 0.4439701735973358, 0.18214620649814606, 0.329926460981369, 0.241481751203537]], "avg_entropy": [0.7434938997030258], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "two thousand and seven", "score_of_biggest_cluster": 0.8260720372200012, "score_of_first_answer": 0.8260720372200012, "max_score_of_first_answer": 0.9712018370628357, "min_score_of_first_answer": 0.4960562586784363, "avg_score": 0.8260720372200012, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.17882465010708734, "std_of_first_answer": 0.17882465010708734, "avg_entropy_of_first_answer": 0.7434938997030258, "lex_sim": 1.0}
{"question": "What was Groucho Marx's real first name?", "answer": ["Julius"], "scores": [[0.9036163687705994]], "normalized_score": [0.9036163687705994], "tokens": [["\u2581Julius"]], "entropy": [[0.7322675585746765]], "avg_entropy": [0.7322675585746765], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gens Iulia", "score_of_biggest_cluster": 0.9036163687705994, "score_of_first_answer": 0.9036163687705994, "max_score_of_first_answer": 0.9036163687705994, "min_score_of_first_answer": 0.9036163687705994, "avg_score": 0.9036163687705994, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.7322675585746765, "lex_sim": 1.0}
{"question": "The Victorian producer Richard d'Oyly Carte is most associated with the staging of whose works?", "answer": ["Gilbert and Sullivan"], "scores": [[0.923105001449585, 0.7365153431892395, 0.9743144512176514, 0.9996803998947144, 0.9952006340026855]], "normalized_score": [0.9199850559234619], "tokens": [["\u2581Gilbert", "\u2581and", "\u2581S", "ull", "ivan"]], "entropy": [[0.5668578743934631, 0.8686572313308716, 0.24746857583522797, 0.004570102319121361, 0.03436794877052307]], "avg_entropy": [0.3443843465298414], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gilbert & Sullivan", "score_of_biggest_cluster": 0.9199850559234619, "score_of_first_answer": 0.9199850559234619, "max_score_of_first_answer": 0.9996803998947144, "min_score_of_first_answer": 0.7365153431892395, "avg_score": 0.9199850559234619, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0984523752917579, "std_of_first_answer": 0.0984523752917579, "avg_entropy_of_first_answer": 0.3443843465298414, "lex_sim": 1.0}
{"question": "Which US state is called 'Old Dominion'?", "answer": ["Virginia"], "scores": [[0.8616822957992554]], "normalized_score": [0.8616822957992554], "tokens": [["\u2581Virginia"]], "entropy": [[0.9468011856079102]], "avg_entropy": [0.9468011856079102], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Virginia", "score_of_biggest_cluster": 0.8616822957992554, "score_of_first_answer": 0.8616822957992554, "max_score_of_first_answer": 0.8616822957992554, "min_score_of_first_answer": 0.8616822957992554, "avg_score": 0.8616822957992554, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.9468011856079102, "lex_sim": 1.0}
{"question": "Nomophobia is the fear of being without or losing what?", "answer": ["Mobile phone"], "scores": [[0.2582356333732605, 0.6614574790000916]], "normalized_score": [0.4132939577102661], "tokens": [["\u2581Mobile", "\u2581phone"]], "entropy": [[2.727139472961426, 1.421691656112671]], "avg_entropy": [2.0744155645370483], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cellular telephone", "score_of_biggest_cluster": 0.4132939577102661, "score_of_first_answer": 0.4132939577102661, "max_score_of_first_answer": 0.6614574790000916, "min_score_of_first_answer": 0.2582356333732605, "avg_score": 0.4132939577102661, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.20161092281341553, "std_of_first_answer": 0.20161092281341553, "avg_entropy_of_first_answer": 2.0744155645370483, "lex_sim": 1.0}
{"question": "What is the medical description of the loss of hair from the head or body, sometimes to the extent of baldness?", "answer": ["Aplasia"], "scores": [[0.025455305352807045, 0.12794479727745056, 0.9203396439552307]], "normalized_score": [0.14418374001979828], "tokens": [["\u2581A", "pl", "asia"]], "entropy": [[1.0513192415237427, 3.897555351257324, 0.5757607817649841]], "avg_entropy": [1.8415451248486836], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "NEC cicatricial alopecia", "score_of_biggest_cluster": 0.14418374001979828, "score_of_first_answer": 0.14418374001979828, "max_score_of_first_answer": 0.9203396439552307, "min_score_of_first_answer": 0.025455305352807045, "avg_score": 0.14418374001979828, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3998904945207391, "std_of_first_answer": 0.3998904945207391, "avg_entropy_of_first_answer": 1.8415451248486836, "lex_sim": 1.0}
{"question": "How many Oscars did the 1994 film 'Forrest Gump' win?", "answer": ["6 Oscars"], "scores": [[0.16597196459770203, 0.8259785175323486, 0.03818140923976898, 0.998485267162323, 0.9987497329711914]], "normalized_score": [0.3495672345161438], "tokens": [["\u2581", "6", "\u2581O", "sc", "ars"]], "entropy": [[3.33967924118042, 0.7805821895599365, 1.0334452390670776, 0.016496721655130386, 0.013693633489310741]], "avg_entropy": [1.0367794049903751], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Six", "score_of_biggest_cluster": 0.3495672345161438, "score_of_first_answer": 0.3495672345161438, "max_score_of_first_answer": 0.9987497329711914, "min_score_of_first_answer": 0.03818140923976898, "avg_score": 0.3495672345161438, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4177867454012924, "std_of_first_answer": 0.4177867454012924, "avg_entropy_of_first_answer": 1.0367794049903751, "lex_sim": 1.0}
{"question": "Which planet in our solar system has the shortest year?", "answer": ["Venus"], "scores": [[0.36131420731544495, 0.9920378923416138]], "normalized_score": [0.5986964106559753], "tokens": [["\u2581Ven", "us"]], "entropy": [[1.990647315979004, 0.07530330121517181]], "avg_entropy": [1.0329753085970879], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mercury", "score_of_biggest_cluster": 0.5986964106559753, "score_of_first_answer": 0.5986964106559753, "max_score_of_first_answer": 0.9920378923416138, "min_score_of_first_answer": 0.36131420731544495, "avg_score": 0.5986964106559753, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3153618425130844, "std_of_first_answer": 0.3153618425130844, "avg_entropy_of_first_answer": 1.0329753085970879, "lex_sim": 1.0}
{"question": "What colour is the flower of the Celandine?", "answer": ["Yellow"], "scores": [[0.7280193567276001, 0.9933951497077942]], "normalized_score": [0.8504180908203125], "tokens": [["\u2581Y", "ellow"]], "entropy": [[1.6797168254852295, 0.05514942854642868]], "avg_entropy": [0.8674331270158291], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "YELLOW", "score_of_biggest_cluster": 0.8504180908203125, "score_of_first_answer": 0.8504180908203125, "max_score_of_first_answer": 0.9933951497077942, "min_score_of_first_answer": 0.7280193567276001, "avg_score": 0.8504180908203125, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.13268789649009705, "std_of_first_answer": 0.13268789649009705, "avg_entropy_of_first_answer": 0.8674331270158291, "lex_sim": 1.0}
{"question": "In Chigley on Children's TV who owned the steam railway and drove the steam engine 'Bessie'?", "answer": ["Mr Edward Chiggy Buggins"], "scores": [[0.2239065319299698, 0.006133011542260647, 0.011818382889032364, 0.809941828250885, 0.0058400495909154415, 0.01322553027421236, 0.03980547562241554, 0.6995587944984436]], "normalized_score": [0.04801960289478302], "tokens": [["\u2581Mr", "\u2581Edward", "\u2581Ch", "ig", "gy", "\u2581B", "ugg", "ins"]], "entropy": [[5.053617477416992, 3.757143020629883, 5.345512866973877, 1.0754737854003906, 0.4567842483520508, 3.6131844520568848, 4.886363983154297, 1.1408014297485352]], "avg_entropy": [3.1661101579666138], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "LORD BELBOROUGH", "score_of_biggest_cluster": 0.04801960289478302, "score_of_first_answer": 0.04801960289478302, "max_score_of_first_answer": 0.809941828250885, "min_score_of_first_answer": 0.0058400495909154415, "avg_score": 0.04801960289478302, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3138225591806305, "std_of_first_answer": 0.3138225591806305, "avg_entropy_of_first_answer": 3.1661101579666138, "lex_sim": 1.0}
{"question": "In what year did Edward Vlll abdicate?", "answer": ["Edward VIII"], "scores": [[0.0036308327689766884, 0.10602008551359177]], "normalized_score": [0.01961991749703884], "tokens": [["\u2581Edward", "\u2581VIII"]], "entropy": [[2.300497055053711, 2.37967586517334]], "avg_entropy": [2.3400864601135254], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and thirty-six", "score_of_biggest_cluster": 0.01961991749703884, "score_of_first_answer": 0.01961991749703884, "max_score_of_first_answer": 0.10602008551359177, "min_score_of_first_answer": 0.0036308327689766884, "avg_score": 0.01961991749703884, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05119462637230754, "std_of_first_answer": 0.05119462637230754, "avg_entropy_of_first_answer": 2.3400864601135254, "lex_sim": 1.0}
{"question": "Coxa is the medical name for which joint of the human body?", "answer": ["Knee"], "scores": [[0.3425055742263794, 0.9989486336708069, 0.960734486579895]], "normalized_score": [0.6901413798332214], "tokens": [["\u2581K", "ne", "e"]], "entropy": [[2.9033117294311523, 0.010761890560388565, 0.21481448411941528]], "avg_entropy": [1.0429627013703187], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "HIP", "score_of_biggest_cluster": 0.6901413798332214, "score_of_first_answer": 0.6901413798332214, "max_score_of_first_answer": 0.9989486336708069, "min_score_of_first_answer": 0.3425055742263794, "avg_score": 0.6901413798332214, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.300847838874748, "std_of_first_answer": 0.300847838874748, "avg_entropy_of_first_answer": 1.0429627013703187, "lex_sim": 1.0}
{"question": "\"What movie's sequel is subtitled \"\"Havana Nights\"\"?\"", "answer": ["Romeo Must Die"], "scores": [[0.025367556139826775, 0.9848992824554443, 0.26074451208114624, 0.9506732821464539]], "normalized_score": [0.2805299460887909], "tokens": [["\u2581Rome", "o", "\u2581Must", "\u2581Die"]], "entropy": [[4.731579780578613, 0.15016572177410126, 1.8453885316848755, 0.409420371055603]], "avg_entropy": [1.7841386012732983], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jimmy Castle", "score_of_biggest_cluster": 0.2805299460887909, "score_of_first_answer": 0.2805299460887909, "max_score_of_first_answer": 0.9848992824554443, "min_score_of_first_answer": 0.025367556139826775, "avg_score": 0.2805299460887909, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4208523635204448, "std_of_first_answer": 0.4208523635204448, "avg_entropy_of_first_answer": 1.7841386012732983, "lex_sim": 1.0}
{"question": "Which Scottish University was established as the world's first Mechanics Institute in 1821 ?", "answer": ["Glasgow University"], "scores": [[0.5365118384361267, 0.10639476776123047]], "normalized_score": [0.2389185130596161], "tokens": [["\u2581Glasgow", "\u2581University"]], "entropy": [[1.9145740270614624, 1.0317471027374268]], "avg_entropy": [1.4731605648994446], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Heriot\u2013Watt University", "score_of_biggest_cluster": 0.2389185130596161, "score_of_first_answer": 0.2389185130596161, "max_score_of_first_answer": 0.5365118384361267, "min_score_of_first_answer": 0.10639476776123047, "avg_score": 0.2389185130596161, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21505853533744812, "std_of_first_answer": 0.21505853533744812, "avg_entropy_of_first_answer": 1.4731605648994446, "lex_sim": 1.0}
{"question": "What is the name given to the official residence provided for the Olympic athletes?", "answer": ["Olympic village"], "scores": [[0.47072306275367737, 0.2933656573295593]], "normalized_score": [0.37160998582839966], "tokens": [["\u2581Olympic", "\u2581village"]], "entropy": [[2.6876776218414307, 1.0439330339431763]], "avg_entropy": [1.8658053278923035], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paralympic Village", "score_of_biggest_cluster": 0.37160998582839966, "score_of_first_answer": 0.37160998582839966, "max_score_of_first_answer": 0.47072306275367737, "min_score_of_first_answer": 0.2933656573295593, "avg_score": 0.37160998582839966, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08867870271205902, "std_of_first_answer": 0.08867870271205902, "avg_entropy_of_first_answer": 1.8658053278923035, "lex_sim": 1.0}
{"question": "\"What is \"\"Too much money chasing too few goods\"\"?\"", "answer": ["Inflation"], "scores": [[0.39873987436294556, 0.9855353832244873, 0.9940392971038818]], "normalized_score": [0.7310075163841248], "tokens": [["\u2581In", "fl", "ation"]], "entropy": [[3.7088639736175537, 0.12595021724700928, 0.05105452612042427]], "avg_entropy": [1.295289572328329], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Inflation rate", "score_of_biggest_cluster": 0.7310075163841248, "score_of_first_answer": 0.7310075163841248, "max_score_of_first_answer": 0.9940392971038818, "min_score_of_first_answer": 0.39873987436294556, "avg_score": 0.7310075163841248, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2786440757364204, "std_of_first_answer": 0.2786440757364204, "avg_entropy_of_first_answer": 1.295289572328329, "lex_sim": 1.0}
{"question": "Labyrinth, Broad-crested, Sharp-crested, Piano keys, and Compound are types of?", "answer": ["Acoustic guitar top"], "scores": [[0.005081207025796175, 0.7117540240287781, 0.8361074328422546, 0.37607139348983765, 0.0024385754950344563]], "normalized_score": [0.07737363129854202], "tokens": [["\u2581Ac", "oust", "ic", "\u2581guitar", "\u2581top"]], "entropy": [[5.599323749542236, 1.3566477298736572, 0.5758097767829895, 2.7409448623657227, 1.2479093074798584]], "avg_entropy": [2.3041270852088926], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Drowning machine", "score_of_biggest_cluster": 0.07737363129854202, "score_of_first_answer": 0.07737363129854202, "max_score_of_first_answer": 0.8361074328422546, "min_score_of_first_answer": 0.0024385754950344563, "avg_score": 0.07737363129854202, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34670709069148414, "std_of_first_answer": 0.34670709069148414, "avg_entropy_of_first_answer": 2.3041270852088926, "lex_sim": 1.0}
{"question": "The borders of which European country, that were set under the reign of Afonso III (1248 to 1279), are the same today?", "answer": ["Portugal"], "scores": [[0.8160660862922668]], "normalized_score": [0.8160660862922668], "tokens": [["\u2581Portugal"]], "entropy": [[1.1584458351135254]], "avg_entropy": [1.1584458351135254], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Portoga\u0142o", "score_of_biggest_cluster": 0.8160660862922668, "score_of_first_answer": 0.8160660862922668, "max_score_of_first_answer": 0.8160660862922668, "min_score_of_first_answer": 0.8160660862922668, "avg_score": 0.8160660862922668, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.1584458351135254, "lex_sim": 1.0}
{"question": "Eventually a newspaper photographer for the Daily Bugle, what is the alter ego of Spiderman?", "answer": ["Peter Parker"], "scores": [[0.9371639490127563, 0.9169949889183044]], "normalized_score": [0.9270246028900146], "tokens": [["\u2581Peter", "\u2581Parker"]], "entropy": [[0.5674518346786499, 0.6320420503616333]], "avg_entropy": [0.5997469425201416], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wall Crawler", "score_of_biggest_cluster": 0.9270246028900146, "score_of_first_answer": 0.9270246028900146, "max_score_of_first_answer": 0.9371639490127563, "min_score_of_first_answer": 0.9169949889183044, "avg_score": 0.9270246028900146, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.010084480047225952, "std_of_first_answer": 0.010084480047225952, "avg_entropy_of_first_answer": 0.5997469425201416, "lex_sim": 1.0}
{"question": "By what name did we know soap star and sometime singer William Piddington", "answer": ["Vip Prakash"], "scores": [[0.002921744715422392, 0.005764835048466921, 0.02111048437654972, 0.10557039082050323, 0.7371839284896851]], "normalized_score": [0.03078995645046234], "tokens": [["\u2581V", "ip", "\u2581P", "rak", "ash"]], "entropy": [[5.302955627441406, 3.3343381881713867, 4.792508125305176, 3.9960527420043945, 1.4195492267608643]], "avg_entropy": [3.7690807819366454], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "William Tarmey", "score_of_biggest_cluster": 0.03078995645046234, "score_of_first_answer": 0.03078995645046234, "max_score_of_first_answer": 0.7371839284896851, "min_score_of_first_answer": 0.002921744715422392, "avg_score": 0.03078995645046234, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.283832165532763, "std_of_first_answer": 0.283832165532763, "avg_entropy_of_first_answer": 3.7690807819366454, "lex_sim": 1.0}
{"question": "Nephritis is a condition, in humans, where which part of the body becomes inflamed?", "answer": ["Kidney"], "scores": [[0.489838182926178, 0.3759477436542511]], "normalized_score": [0.4291311502456665], "tokens": [["\u2581Kid", "ney"]], "entropy": [[2.3665552139282227, 0.6834313869476318]], "avg_entropy": [1.5249933004379272], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Human kidney", "score_of_biggest_cluster": 0.4291311502456665, "score_of_first_answer": 0.4291311502456665, "max_score_of_first_answer": 0.489838182926178, "min_score_of_first_answer": 0.3759477436542511, "avg_score": 0.4291311502456665, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.05694521963596344, "std_of_first_answer": 0.05694521963596344, "avg_entropy_of_first_answer": 1.5249933004379272, "lex_sim": 1.0}
{"question": "Who painted Girl with a Pearl Earring?", "answer": ["Johannes Vermeer"], "scores": [[0.751498818397522, 0.9295397996902466, 0.9850366711616516, 0.9991784691810608]], "normalized_score": [0.9105901122093201], "tokens": [["\u2581Johannes", "\u2581V", "erme", "er"]], "entropy": [[1.507732629776001, 0.5976414084434509, 0.1575997918844223, 0.009110680781304836]], "avg_entropy": [0.5680211277212948], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Johannes Vermeer van Delft", "score_of_biggest_cluster": 0.9105901122093201, "score_of_first_answer": 0.9105901122093201, "max_score_of_first_answer": 0.9991784691810608, "min_score_of_first_answer": 0.751498818397522, "avg_score": 0.9105901122093201, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.09865126918481543, "std_of_first_answer": 0.09865126918481543, "avg_entropy_of_first_answer": 0.5680211277212948, "lex_sim": 1.0}
{"question": "Which Flemish Baroque painter\u2019s Antwerp home is now a museum?", "answer": ["Anthony Van Dyck"], "scores": [[0.10600638389587402, 0.3841000199317932, 0.9407596588134766, 0.8649249076843262]], "normalized_score": [0.42663684487342834], "tokens": [["\u2581Anthony", "\u2581Van", "\u2581Dy", "ck"]], "entropy": [[1.907050371170044, 1.1616847515106201, 0.44202327728271484, 0.4278656244277954]], "avg_entropy": [0.9846560060977936], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pieter Pauwel Rubens", "score_of_biggest_cluster": 0.42663684487342834, "score_of_first_answer": 0.42663684487342834, "max_score_of_first_answer": 0.9407596588134766, "min_score_of_first_answer": 0.10600638389587402, "avg_score": 0.42663684487342834, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3443217846233671, "std_of_first_answer": 0.3443217846233671, "avg_entropy_of_first_answer": 0.9846560060977936, "lex_sim": 1.0}
{"question": "The SI unit the ohm is often represented by the upper-case form of which Greek letter?", "answer": ["\u03a9"], "scores": [[0.05703301727771759, 0.1828814446926117]], "normalized_score": [0.10212874412536621], "tokens": [["\u2581", "\u03a9"]], "entropy": [[2.9088070392608643, 2.335103988647461]], "avg_entropy": [2.6219555139541626], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "OMEGA (disambiguation)", "score_of_biggest_cluster": 0.10212874412536621, "score_of_first_answer": 0.10212874412536621, "max_score_of_first_answer": 0.1828814446926117, "min_score_of_first_answer": 0.05703301727771759, "avg_score": 0.10212874412536621, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06292421370744705, "std_of_first_answer": 0.06292421370744705, "avg_entropy_of_first_answer": 2.6219555139541626, "lex_sim": 0.0}
{"question": "What is the name of pigment that gives plants and leaves their colors and also absorbs light in the blue and red portions of the spectrum?", "answer": ["Anthocyanin"], "scores": [[0.16962945461273193, 0.9542260766029358, 0.9925031661987305, 0.8988366723060608]], "normalized_score": [0.6164407134056091], "tokens": [["\u2581Anth", "oc", "yan", "in"]], "entropy": [[1.7059566974639893, 0.3453613519668579, 0.06281466037034988, 0.5098128318786621]], "avg_entropy": [0.6559863854199648], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "C55H70O6N4Mg", "score_of_biggest_cluster": 0.6164407134056091, "score_of_first_answer": 0.6164407134056091, "max_score_of_first_answer": 0.9925031661987305, "min_score_of_first_answer": 0.16962945461273193, "avg_score": 0.6164407134056091, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.338910266937304, "std_of_first_answer": 0.338910266937304, "avg_entropy_of_first_answer": 0.6559863854199648, "lex_sim": 1.0}
{"question": "Which 2004 film was released with the advertising tagline \u201cA romantic comedy \u2013 with zombies!\u201d?", "answer": ["Shaun of the Dead"], "scores": [[0.7828232645988464, 0.9768632650375366, 0.9979494214057922, 0.8522151112556458, 0.9921683669090271, 0.9058555960655212]], "normalized_score": [0.9143937826156616], "tokens": [["\u2581Sh", "a", "un", "\u2581of", "\u2581the", "\u2581Dead"]], "entropy": [[1.528141975402832, 0.14391742646694183, 0.022194158285856247, 0.48279473185539246, 0.05738364905118942, 0.3519657254219055]], "avg_entropy": [0.4310662777473529], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rom-zom-com", "score_of_biggest_cluster": 0.9143937826156616, "score_of_first_answer": 0.9143937826156616, "max_score_of_first_answer": 0.9979494214057922, "min_score_of_first_answer": 0.7828232645988464, "avg_score": 0.9143937826156616, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0796927704046221, "std_of_first_answer": 0.0796927704046221, "avg_entropy_of_first_answer": 0.4310662777473529, "lex_sim": 1.0}
{"question": "Which handheld electronic toy had to be treated as a pet?", "answer": ["Tamagotchi"], "scores": [[0.448794960975647, 0.9645847678184509, 0.91400545835495, 0.9740930795669556]], "normalized_score": [0.7879241108894348], "tokens": [["\u2581Tam", "ag", "ot", "chi"]], "entropy": [[3.17415452003479, 0.21170586347579956, 0.5047919154167175, 0.12794649600982666]], "avg_entropy": [1.0046496987342834], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Saaiko! Tamagotchi", "score_of_biggest_cluster": 0.7879241108894348, "score_of_first_answer": 0.7879241108894348, "max_score_of_first_answer": 0.9740930795669556, "min_score_of_first_answer": 0.448794960975647, "avg_score": 0.7879241108894348, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2186117051234095, "std_of_first_answer": 0.2186117051234095, "avg_entropy_of_first_answer": 1.0046496987342834, "lex_sim": 1.0}
{"question": "Which trade is historically associated with Flamenco music and dance?", "answer": ["Bull fighting"], "scores": [[0.09134438633918762, 0.05195888504385948]], "normalized_score": [0.06889232993125916], "tokens": [["\u2581Bull", "\u2581fighting"]], "entropy": [[4.196066856384277, 1.4366443157196045]], "avg_entropy": [2.816355586051941], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Blacksmith shop", "score_of_biggest_cluster": 0.06889232993125916, "score_of_first_answer": 0.06889232993125916, "max_score_of_first_answer": 0.09134438633918762, "min_score_of_first_answer": 0.05195888504385948, "avg_score": 0.06889232993125916, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01969275064766407, "std_of_first_answer": 0.01969275064766407, "avg_entropy_of_first_answer": 2.816355586051941, "lex_sim": 1.0}
{"question": "Who, c1842, painted the picture Snowstorm - Steamboat off a Harbour's Mouth, now in the Tate?", "answer": ["J. M. W. Turner"], "scores": [[0.6090710163116455, 0.4575287401676178, 0.47364217042922974, 0.9936701059341431, 0.9708777666091919, 0.9686614274978638, 0.9908298254013062]], "normalized_score": [0.740606963634491], "tokens": [["\u2581J", ".", "\u2581M", ".", "\u2581W", ".", "\u2581Turner"]], "entropy": [[2.323847770690918, 1.3630335330963135, 1.0393790006637573, 0.05933790281414986, 0.22143903374671936, 0.18500955402851105, 0.07382132858037949]], "avg_entropy": [0.7522668748029641], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Turner collection", "score_of_biggest_cluster": 0.740606963634491, "score_of_first_answer": 0.740606963634491, "max_score_of_first_answer": 0.9936701059341431, "min_score_of_first_answer": 0.4575287401676178, "avg_score": 0.740606963634491, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23579304420872063, "std_of_first_answer": 0.23579304420872063, "avg_entropy_of_first_answer": 0.7522668748029641, "lex_sim": 1.0}
{"question": "In Egyptian mythology, of what is Nut the goddess?", "answer": ["Sky goddess"], "scores": [[0.4180332124233246, 0.018870558589696884, 0.9602938890457153]], "normalized_score": [0.19639627635478973], "tokens": [["\u2581Sky", "\u2581god", "dess"]], "entropy": [[2.50215744972229, 0.47372984886169434, 0.20593039691448212]], "avg_entropy": [1.0606058984994888], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Sky", "score_of_biggest_cluster": 0.19639627635478973, "score_of_first_answer": 0.19639627635478973, "max_score_of_first_answer": 0.9602938890457153, "min_score_of_first_answer": 0.018870558589696884, "avg_score": 0.19639627635478973, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3858116052562124, "std_of_first_answer": 0.3858116052562124, "avg_entropy_of_first_answer": 1.0606058984994888, "lex_sim": 1.0}
{"question": "The third most translated author, which Frenchman, whose birthday is celebrated yesterday, is considered the father of Science Fiction (along with Hugo Gemsback and H.G. Wells) and wrote works such as Around the World in 80 Days and A Journey to the Center of the Earth?", "answer": ["Jules Verne"], "scores": [[0.963527262210846, 0.9115826487541199, 0.9979767203330994]], "normalized_score": [0.9570327997207642], "tokens": [["\u2581Jules", "\u2581Ver", "ne"]], "entropy": [[0.3332468867301941, 0.5650336742401123, 0.022813305258750916]], "avg_entropy": [0.3070312887430191], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Verne, Jules", "score_of_biggest_cluster": 0.9570327997207642, "score_of_first_answer": 0.9570327997207642, "max_score_of_first_answer": 0.9979767203330994, "min_score_of_first_answer": 0.9115826487541199, "avg_score": 0.9570327997207642, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03551047362880643, "std_of_first_answer": 0.03551047362880643, "avg_entropy_of_first_answer": 0.3070312887430191, "lex_sim": 1.0}
{"question": "What is the young of a hawk called?", "answer": ["Eyass"], "scores": [[0.4136662781238556, 0.5159898400306702, 0.39308100938796997]], "normalized_score": [0.43778184056282043], "tokens": [["\u2581E", "y", "ass"]], "entropy": [[2.8470988273620605, 1.6004139184951782, 1.8000481128692627]], "avg_entropy": [2.082520286242167], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Falco (bird)", "score_of_biggest_cluster": 0.43778184056282043, "score_of_first_answer": 0.43778184056282043, "max_score_of_first_answer": 0.5159898400306702, "min_score_of_first_answer": 0.39308100938796997, "avg_score": 0.43778184056282043, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05374884521775773, "std_of_first_answer": 0.05374884521775773, "avg_entropy_of_first_answer": 2.082520286242167, "lex_sim": 1.0}
{"question": "What dark brown food paste is the Australian version of Marmite in this country?", "answer": ["Vegemite"], "scores": [[0.8929616808891296, 0.9840829968452454, 0.9993808269500732]], "normalized_score": [0.9576317071914673], "tokens": [["\u2581Veg", "em", "ite"]], "entropy": [[0.8619099259376526, 0.12196367979049683, 0.007871067151427269]], "avg_entropy": [0.3305815576265256], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "New Vegemite", "score_of_biggest_cluster": 0.9576317071914673, "score_of_first_answer": 0.9576317071914673, "max_score_of_first_answer": 0.9993808269500732, "min_score_of_first_answer": 0.8929616808891296, "avg_score": 0.9576317071914673, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04697771632005836, "std_of_first_answer": 0.04697771632005836, "avg_entropy_of_first_answer": 0.3305815576265256, "lex_sim": 1.0}
{"question": "In which North African country was Mohammed Al-Fayed, owner of Harrods department store in London, born?", "answer": ["Egypt"], "scores": [[0.6867827773094177]], "normalized_score": [0.6867827773094177], "tokens": [["\u2581Egypt"]], "entropy": [[1.4725630283355713]], "avg_entropy": [1.4725630283355713], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "EGY", "score_of_biggest_cluster": 0.6867827773094177, "score_of_first_answer": 0.6867827773094177, "max_score_of_first_answer": 0.6867827773094177, "min_score_of_first_answer": 0.6867827773094177, "avg_score": 0.6867827773094177, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.4725630283355713, "lex_sim": 1.0}
{"question": "A \u2018Beaver Moon\u2019 is the first full moon which occurs during which month of the year?", "answer": ["November"], "scores": [[0.6663185358047485]], "normalized_score": [0.6663185358047485], "tokens": [["\u2581November"]], "entropy": [[1.6455247402191162]], "avg_entropy": [1.6455247402191162], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The month of November", "score_of_biggest_cluster": 0.6663185358047485, "score_of_first_answer": 0.6663185358047485, "max_score_of_first_answer": 0.6663185358047485, "min_score_of_first_answer": 0.6663185358047485, "avg_score": 0.6663185358047485, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.6455247402191162, "lex_sim": 1.0}
{"question": "Whom did Andy Murray defeat in the Wimbledon men's singles final in 2013 ?", "answer": ["Novak Djokovic TROUBLESHOOTING: The user is struggling with the response system because the dialog system is giving the response in a different language."], "scores": [[0.2558148503303528, 0.9718703031539917, 0.8807525634765625, 0.9868766069412231, 0.9767122864723206, 0.9864456057548523, 5.297993993735872e-06, 0.002525616902858019, 0.22648900747299194, 0.8875041007995605, 0.43806105852127075, 0.9857380390167236, 0.999651312828064, 0.5767569541931152, 0.5364585518836975, 0.05186248570680618, 0.04426703229546547, 0.12233329564332962, 0.007244282402098179, 0.23583847284317017, 0.5808992385864258, 0.007804500870406628, 0.032714106142520905, 0.0052860695868730545, 0.15805597603321075, 0.003939588088542223, 0.003032958833500743, 0.40989425778388977, 0.0110484529286623, 0.15790584683418274, 0.026134997606277466, 0.07776055485010147, 0.34581467509269714, 0.1400836557149887, 0.05080096051096916, 0.3641083836555481, 0.008646106347441673, 0.3507639765739441, 0.6495735049247742, 0.004591853357851505, 0.017957014963030815, 0.03717699646949768, 0.7602353692054749]], "normalized_score": [0.08747958391904831], "tokens": [["\u2581Nov", "ak", "\u2581Dj", "ok", "ov", "ic", "\u2581T", "RO", "UB", "LE", "SH", "O", "OT", "ING", ":", "\u2581The", "\u2581user", "\u2581is", "\u2581struggling", "\u2581with", "\u2581the", "\u2581response", "\u2581system", "\u2581because", "\u2581the", "\u2581dialog", "\u2581system", "\u2581is", "\u2581giving", "\u2581the", "\u2581response", "\u2581in", "\u2581a", "\u2581different", "\u2581language", ".", "\u2581How", "\u2581do", "\u2581you", "\u2581go", "\u2581around", "\u2581that", "?"]], "entropy": [[1.4939112663269043, 0.18349410593509674, 0.8295981884002686, 0.10762496292591095, 0.16672642529010773, 0.10042016953229904, 0.3179822862148285, 4.996092796325684, 3.0266165733337402, 0.5505704879760742, 2.6493070125579834, 0.10931131988763809, 0.004147280938923359, 1.7239092588424683, 2.178739309310913, 4.579749584197998, 4.5724945068359375, 4.0610575675964355, 4.931656837463379, 1.5347825288772583, 2.7306389808654785, 5.120152473449707, 3.6459455490112305, 2.0717642307281494, 2.5663552284240723, 4.786188125610352, 0.614313006401062, 3.2267494201660156, 4.549093246459961, 3.918853998184204, 2.8060672283172607, 4.160907745361328, 3.2227487564086914, 4.996408462524414, 2.7774739265441895, 2.267272710800171, 3.915238857269287, 1.650235891342163, 1.0392673015594482, 4.0235490798950195, 0.91655433177948, 1.564001441001892, 1.2942107915878296]], "avg_entropy": [2.4647019361037503], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Novak Djokovi\u0107", "score_of_biggest_cluster": 0.08747958391904831, "score_of_first_answer": 0.08747958391904831, "max_score_of_first_answer": 0.999651312828064, "min_score_of_first_answer": 5.297993993735872e-06, "avg_score": 0.08747958391904831, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3589236297747723, "std_of_first_answer": 0.3589236297747723, "avg_entropy_of_first_answer": 2.4647019361037503, "lex_sim": 1.0}
{"question": "What is the colour of the bull of an archery target?", "answer": ["Yellow"], "scores": [[0.17960911989212036, 0.9930264949798584]], "normalized_score": [0.4223228693008423], "tokens": [["\u2581Y", "ellow"]], "entropy": [[3.125673294067383, 0.05830331891775131]], "avg_entropy": [1.591988306492567], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "GOLD (disambiguation)", "score_of_biggest_cluster": 0.4223228693008423, "score_of_first_answer": 0.4223228693008423, "max_score_of_first_answer": 0.9930264949798584, "min_score_of_first_answer": 0.17960911989212036, "avg_score": 0.4223228693008423, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.406708687543869, "std_of_first_answer": 0.406708687543869, "avg_entropy_of_first_answer": 1.591988306492567, "lex_sim": 1.0}
{"question": "Which 18th Century composer wrote \u2018The Four Seasons\u2019?", "answer": ["Antonio Vivaldi"], "scores": [[0.5953284502029419, 0.797227144241333, 0.990196704864502, 0.9901496171951294]], "normalized_score": [0.8259243369102478], "tokens": [["\u2581Antonio", "\u2581V", "ival", "di"]], "entropy": [[1.9377769231796265, 1.2425308227539062, 0.09456229954957962, 0.08282559365034103]], "avg_entropy": [0.8394239097833633], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Vivaldi", "score_of_biggest_cluster": 0.8259243369102478, "score_of_first_answer": 0.8259243369102478, "max_score_of_first_answer": 0.990196704864502, "min_score_of_first_answer": 0.5953284502029419, "avg_score": 0.8259243369102478, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.16336770435391734, "std_of_first_answer": 0.16336770435391734, "avg_entropy_of_first_answer": 0.8394239097833633, "lex_sim": 1.0}
{"question": "Willow, Reed, Grasshopper, and Dartford are species of which bird?", "answer": ["Sparrowhawk"], "scores": [[0.026588287204504013, 0.5935196280479431, 0.9118422865867615, 0.21459762752056122, 0.914415180683136]], "normalized_score": [0.30914533138275146], "tokens": [["\u2581S", "par", "row", "h", "awk"]], "entropy": [[4.375443458557129, 0.8504668474197388, 0.402454137802124, 1.140653371810913, 0.3093530833721161]], "avg_entropy": [1.4156741797924042], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Warblers", "score_of_biggest_cluster": 0.30914533138275146, "score_of_first_answer": 0.30914533138275146, "max_score_of_first_answer": 0.914415180683136, "min_score_of_first_answer": 0.026588287204504013, "avg_score": 0.30914533138275146, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36069135773963784, "std_of_first_answer": 0.36069135773963784, "avg_entropy_of_first_answer": 1.4156741797924042, "lex_sim": 1.0}
{"question": "The Angostura Bridge crosses the Orinoco River in which country?", "answer": ["Venezuela"], "scores": [[0.9018262028694153]], "normalized_score": [0.9018262028694153], "tokens": [["\u2581Venezuela"]], "entropy": [[0.6868956089019775]], "avg_entropy": [0.6868956089019775], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ISO 3166-1:VE", "score_of_biggest_cluster": 0.9018262028694153, "score_of_first_answer": 0.9018262028694153, "max_score_of_first_answer": 0.9018262028694153, "min_score_of_first_answer": 0.9018262028694153, "avg_score": 0.9018262028694153, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.6868956089019775, "lex_sim": 1.0}
{"question": "The River Yarra flows through which city?", "answer": ["Melbourne"], "scores": [[0.8355304002761841]], "normalized_score": [0.8355304002761841], "tokens": [["\u2581Melbourne"]], "entropy": [[1.3271523714065552]], "avg_entropy": [1.3271523714065552], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "MELBOURNE", "score_of_biggest_cluster": 0.8355304002761841, "score_of_first_answer": 0.8355304002761841, "max_score_of_first_answer": 0.8355304002761841, "min_score_of_first_answer": 0.8355304002761841, "avg_score": 0.8355304002761841, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.3271523714065552, "lex_sim": 1.0}
{"question": "Swede Carl Linnaeus, 1707-78, considered the father of taxonomy of living things, is the lectotype (example specimen) for which species?", "answer": ["Canis lupus familiaris"], "scores": [[0.01278027892112732, 0.9359358549118042, 0.6477671265602112, 0.9832180738449097, 0.9627863168716431, 0.18741606175899506, 0.987709641456604]], "normalized_score": [0.38940635323524475], "tokens": [["\u2581Can", "is", "\u2581l", "up", "us", "\u2581familiar", "is"]], "entropy": [[5.3189005851745605, 0.4604451656341553, 1.8350489139556885, 0.16894052922725677, 0.20970535278320312, 2.6083314418792725, 0.09391355514526367]], "avg_entropy": [1.5278979348284858], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Modern humanity's origins", "score_of_biggest_cluster": 0.38940635323524475, "score_of_first_answer": 0.38940635323524475, "max_score_of_first_answer": 0.987709641456604, "min_score_of_first_answer": 0.01278027892112732, "avg_score": 0.38940635323524475, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3818565691883536, "std_of_first_answer": 0.3818565691883536, "avg_entropy_of_first_answer": 1.5278979348284858, "lex_sim": 1.0}
{"question": "What is the Christian service or ceremony, named from Greek meaning grateful, commemorating the Last Supper?", "answer": ["The Eucharist"], "scores": [[0.12141372263431549, 0.7399835586547852, 0.949429988861084, 0.9845359921455383]], "normalized_score": [0.5383269786834717], "tokens": [["\u2581The", "\u2581E", "uchar", "ist"]], "entropy": [[2.2662081718444824, 1.5550215244293213, 0.3875477910041809, 0.10924239456653595]], "avg_entropy": [1.0795049704611301], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Communion (church)", "score_of_biggest_cluster": 0.5383269786834717, "score_of_first_answer": 0.5383269786834717, "max_score_of_first_answer": 0.9845359921455383, "min_score_of_first_answer": 0.12141372263431549, "avg_score": 0.5383269786834717, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3462410434415355, "std_of_first_answer": 0.3462410434415355, "avg_entropy_of_first_answer": 1.0795049704611301, "lex_sim": 1.0}
{"question": "\"During an infamous court case who said of Lord Astor's denial \"\"Well he would, wouldn't he\"\"?\"", "answer": ["John Mortimer"], "scores": [[0.10572598874568939, 0.23691679537296295, 0.9732545018196106]], "normalized_score": [0.2899577021598816], "tokens": [["\u2581John", "\u2581Mort", "imer"]], "entropy": [[5.0525054931640625, 4.1349945068359375, 0.1416175365447998]], "avg_entropy": [3.109705845514933], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mandy-Rice Davies", "score_of_biggest_cluster": 0.2899577021598816, "score_of_first_answer": 0.2899577021598816, "max_score_of_first_answer": 0.9732545018196106, "min_score_of_first_answer": 0.10572598874568939, "avg_score": 0.2899577021598816, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3818100118987151, "std_of_first_answer": 0.3818100118987151, "avg_entropy_of_first_answer": 3.109705845514933, "lex_sim": 1.0}
{"question": "Which line on the London Undergound rail system is the only line which interconnects with every other line?", "answer": ["Circle line"], "scores": [[0.24088768661022186, 0.15770575404167175]], "normalized_score": [0.19490863382816315], "tokens": [["\u2581Circle", "\u2581line"]], "entropy": [[2.545295238494873, 1.167419195175171]], "avg_entropy": [1.856357216835022], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jubilee line", "score_of_biggest_cluster": 0.19490863382816315, "score_of_first_answer": 0.19490863382816315, "max_score_of_first_answer": 0.24088768661022186, "min_score_of_first_answer": 0.15770575404167175, "avg_score": 0.19490863382816315, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.041590966284275055, "std_of_first_answer": 0.041590966284275055, "avg_entropy_of_first_answer": 1.856357216835022, "lex_sim": 1.0}
{"question": "Which British TV series starring John Simm as DCI Sam Tyler and Philip Glenister as DCI Gene Hunt was remade in the USA starring Jason O'Mara and Harvey Keitel?", "answer": ["Life on Mars (UK TV series)"], "scores": [[0.9515770077705383, 0.856158971786499, 0.9864186644554138, 0.024150047451257706, 0.2935580015182495, 0.012500707060098648, 0.6964722871780396, 0.8867782950401306]], "normalized_score": [0.28537479043006897], "tokens": [["\u2581Life", "\u2581on", "\u2581Mars", "\u2581(", "UK", "\u2581TV", "\u2581series", ")"]], "entropy": [[0.3165521025657654, 0.6345994472503662, 0.08376245200634003, 0.49628889560699463, 3.0519468784332275, 1.7962441444396973, 1.2472319602966309, 0.6880522966384888]], "avg_entropy": [1.0393347721546888], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Life on Mars (planet)", "score_of_biggest_cluster": 0.28537479043006897, "score_of_first_answer": 0.28537479043006897, "max_score_of_first_answer": 0.9864186644554138, "min_score_of_first_answer": 0.012500707060098648, "avg_score": 0.28537479043006897, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38724834999860375, "std_of_first_answer": 0.38724834999860375, "avg_entropy_of_first_answer": 1.0393347721546888, "lex_sim": 1.0}
{"question": "In what country is Romansch an official language?", "answer": ["Svizzerland"], "scores": [[0.0008384673856198788, 0.29744309186935425, 0.6787679195404053, 0.5385186076164246]], "normalized_score": [0.0977131724357605], "tokens": [["\u2581Sv", "iz", "zer", "land"]], "entropy": [[1.0483055114746094, 1.588611125946045, 1.734994649887085, 1.6882727146148682]], "avg_entropy": [1.5150460004806519], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ISO 3166-1:CH", "score_of_biggest_cluster": 0.0977131724357605, "score_of_first_answer": 0.0977131724357605, "max_score_of_first_answer": 0.6787679195404053, "min_score_of_first_answer": 0.0008384673856198788, "avg_score": 0.0977131724357605, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2573735704852075, "std_of_first_answer": 0.2573735704852075, "avg_entropy_of_first_answer": 1.5150460004806519, "lex_sim": 1.0}
{"question": "\"\"\"Fawlty Towers\"\" was written by John Cleese and Connie Booth, who played main characters. Which character did Connie Booth play?\"", "answer": ["Sybil Fawlty"], "scores": [[0.24644890427589417, 0.9853664636611938, 0.567662239074707, 0.9929475784301758, 0.9986006617546082, 0.9970393180847168]], "normalized_score": [0.717366099357605], "tokens": [["\u2581Sy", "bil", "\u2581F", "aw", "l", "ty"]], "entropy": [[2.397538185119629, 0.10388152301311493, 0.9859042167663574, 0.06508943438529968, 0.013830039650201797, 0.02575495094060898]], "avg_entropy": [0.5986663916458687], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Polly", "score_of_biggest_cluster": 0.717366099357605, "score_of_first_answer": 0.717366099357605, "max_score_of_first_answer": 0.9986006617546082, "min_score_of_first_answer": 0.24644890427589417, "avg_score": 0.717366099357605, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2916139343594237, "std_of_first_answer": 0.2916139343594237, "avg_entropy_of_first_answer": 0.5986663916458687, "lex_sim": 1.0}
{"question": "The technical term abecedarian refers to what form of organization?", "answer": ["An alphabetical list"], "scores": [[0.028787309303879738, 0.45337769389152527, 0.7217684388160706, 0.3076895475387573]], "normalized_score": [0.23202939331531525], "tokens": [["\u2581An", "\u2581alphabet", "ical", "\u2581list"]], "entropy": [[5.373171806335449, 3.322122097015381, 1.1470520496368408, 2.81575870513916]], "avg_entropy": [3.1645261645317078], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "List of letters in the english language", "score_of_biggest_cluster": 0.23202939331531525, "score_of_first_answer": 0.23202939331531525, "max_score_of_first_answer": 0.7217684388160706, "min_score_of_first_answer": 0.028787309303879738, "avg_score": 0.23202939331531525, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25037549989802915, "std_of_first_answer": 0.25037549989802915, "avg_entropy_of_first_answer": 3.1645261645317078, "lex_sim": 1.0}
{"question": "What highly durable footwear is used to name a species of bat and crab, alluding respectively to nose and body shapes?", "answer": ["Chopines"], "scores": [[0.023830043151974678, 0.01815398782491684, 0.3783664405345917]], "normalized_score": [0.05470199137926102], "tokens": [["\u2581Ch", "op", "ines"]], "entropy": [[4.652805328369141, 4.354127883911133, 1.6677634716033936]], "avg_entropy": [3.558232227961222], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Horse-Shoes", "score_of_biggest_cluster": 0.05470199137926102, "score_of_first_answer": 0.05470199137926102, "max_score_of_first_answer": 0.3783664405345917, "min_score_of_first_answer": 0.01815398782491684, "avg_score": 0.05470199137926102, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.16848385539545208, "std_of_first_answer": 0.16848385539545208, "avg_entropy_of_first_answer": 3.558232227961222, "lex_sim": 1.0}
{"question": "Precisely where were the Winter Olympics of 1932 and 1980 both held?", "answer": ["Lake Placid, NY; Lake Placid, NY"], "scores": [[0.18198862671852112, 0.9792658686637878, 0.998943030834198, 0.9977508187294006, 0.6342915892601013, 0.0992080494761467, 0.00878867320716381, 0.48166316747665405, 0.9758357405662537, 0.9993399977684021, 0.9988222718238831, 0.9761896133422852, 0.9288911819458008]], "normalized_score": [0.46042680740356445], "tokens": [["\u2581Lake", "\u2581P", "lac", "id", ",", "\u2581NY", ";", "\u2581Lake", "\u2581P", "lac", "id", ",", "\u2581NY"]], "entropy": [[4.084965705871582, 0.2096766233444214, 0.013569319620728493, 0.023926183581352234, 1.2648472785949707, 1.3822216987609863, 1.6086968183517456, 2.694545269012451, 0.22853948175907135, 0.008702087216079235, 0.013476694002747536, 0.1882149577140808, 0.5179946422576904]], "avg_entropy": [0.9414905200067621], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lake Placid (disambiguation)", "score_of_biggest_cluster": 0.46042680740356445, "score_of_first_answer": 0.46042680740356445, "max_score_of_first_answer": 0.9993399977684021, "min_score_of_first_answer": 0.00878867320716381, "avg_score": 0.46042680740356445, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37176536746930056, "std_of_first_answer": 0.37176536746930056, "avg_entropy_of_first_answer": 0.9414905200067621, "lex_sim": 1.0}
{"question": "The authorised version of the Holy Bible was made at the order of which King?", "answer": ["King James"], "scores": [[0.523944079875946, 0.8978690505027771]], "normalized_score": [0.6858813166618347], "tokens": [["\u2581King", "\u2581James"]], "entropy": [[1.6281683444976807, 0.6397138237953186]], "avg_entropy": [1.1339410841464996], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "James VI of Scotland, James I of England", "score_of_biggest_cluster": 0.6858813166618347, "score_of_first_answer": 0.6858813166618347, "max_score_of_first_answer": 0.8978690505027771, "min_score_of_first_answer": 0.523944079875946, "avg_score": 0.6858813166618347, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.18696248531341553, "std_of_first_answer": 0.18696248531341553, "avg_entropy_of_first_answer": 1.1339410841464996, "lex_sim": 1.0}
{"question": "\"What type of sentence is \"\"pack my red box with five dozen quality jugs\"\"?\"", "answer": ["Wh Question sentence"], "scores": [[0.004715265706181526, 0.011894715018570423, 0.004611498676240444]], "normalized_score": [0.00637139193713665], "tokens": [["\u2581Wh", "\u2581Question", "\u2581sentence"]], "entropy": [[4.919879913330078, 2.224025011062622, 1.0969438552856445]], "avg_entropy": [2.7469495932261148], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Holo-alphabetic sentence", "score_of_biggest_cluster": 0.00637139193713665, "score_of_first_answer": 0.00637139193713665, "max_score_of_first_answer": 0.011894715018570423, "min_score_of_first_answer": 0.004611498676240444, "avg_score": 0.00637139193713665, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.003409146199968658, "std_of_first_answer": 0.003409146199968658, "avg_entropy_of_first_answer": 2.7469495932261148, "lex_sim": 1.0}
{"question": "The 1947 novel \"I, the Jury\", by New York author Mickey Spillane, was the first to feature which famous detective?", "answer": ["Mike Hammer"], "scores": [[0.49302735924720764, 0.9909447431564331, 0.9735838770866394]], "normalized_score": [0.780604898929596], "tokens": [["\u2581Mike", "\u2581Ham", "mer"]], "entropy": [[3.4149532318115234, 0.09708771109580994, 0.24441185593605042]], "avg_entropy": [1.2521509329477947], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mike Hammer", "score_of_biggest_cluster": 0.780604898929596, "score_of_first_answer": 0.780604898929596, "max_score_of_first_answer": 0.9909447431564331, "min_score_of_first_answer": 0.49302735924720764, "avg_score": 0.780604898929596, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23073738985831693, "std_of_first_answer": 0.23073738985831693, "avg_entropy_of_first_answer": 1.2521509329477947, "lex_sim": 1.0}
{"question": "Which local radio station broadcasts to Wigan and St. Helens on 102.4 FM?", "answer": ["Smooth Radio"], "scores": [[0.0067759123630821705, 0.931439220905304, 0.4007621109485626]], "normalized_score": [0.13624992966651917], "tokens": [["\u2581Sm", "ooth", "\u2581Radio"]], "entropy": [[4.361713409423828, 0.41775697469711304, 1.9801411628723145]], "avg_entropy": [2.2532038489977517], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "102.4 Wish FM", "score_of_biggest_cluster": 0.13624992966651917, "score_of_first_answer": 0.13624992966651917, "max_score_of_first_answer": 0.931439220905304, "min_score_of_first_answer": 0.0067759123630821705, "avg_score": 0.13624992966651917, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.378864612856203, "std_of_first_answer": 0.378864612856203, "avg_entropy_of_first_answer": 2.2532038489977517, "lex_sim": 1.0}
{"question": "In which country could you drive along the 1700 kilometre Bruce Highway?", "answer": ["Oceania"], "scores": [[0.0018396376399323344, 0.2885648012161255, 0.9606128334999084]], "normalized_score": [0.0798928514122963], "tokens": [["\u2581O", "ce", "ania"]], "entropy": [[1.269993543624878, 2.1879570484161377, 0.2607511281967163]], "avg_entropy": [1.239567240079244], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Australia (Commonwealth realm)", "score_of_biggest_cluster": 0.0798928514122963, "score_of_first_answer": 0.0798928514122963, "max_score_of_first_answer": 0.9606128334999084, "min_score_of_first_answer": 0.0018396376399323344, "avg_score": 0.0798928514122963, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.40181614344869654, "std_of_first_answer": 0.40181614344869654, "avg_entropy_of_first_answer": 1.239567240079244, "lex_sim": 1.0}
{"question": "Which US group of the 1970s consisted of Dewy Bunnell, Dan Peek and Gerry Beckley and had hits including Tin Man and Sister Golden Hair?", "answer": ["America"], "scores": [[0.9149102568626404]], "normalized_score": [0.9149102568626404], "tokens": [["\u2581America"]], "entropy": [[0.6412992477416992]], "avg_entropy": [0.6412992477416992], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The United States of America", "score_of_biggest_cluster": 0.9149102568626404, "score_of_first_answer": 0.9149102568626404, "max_score_of_first_answer": 0.9149102568626404, "min_score_of_first_answer": 0.9149102568626404, "avg_score": 0.9149102568626404, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.6412992477416992, "lex_sim": 1.0}
{"question": "What was the name of the British nurse who was shot by a German firing squad during the First World War after being court-martialled for helping Allied troops escape from a military hospital in Belgium?", "answer": ["Edith Cavell"], "scores": [[0.9625037312507629, 0.9975038170814514, 0.9511780738830566, 0.9824779629707336]], "normalized_score": [0.9732522368431091], "tokens": [["\u2581Ed", "ith", "\u2581Cav", "ell"]], "entropy": [[0.30325081944465637, 0.023011375218629837, 0.4260237514972687, 0.13535176217556]], "avg_entropy": [0.22190942708402872], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cavell, Edith Louisa", "score_of_biggest_cluster": 0.9732522368431091, "score_of_first_answer": 0.9732522368431091, "max_score_of_first_answer": 0.9975038170814514, "min_score_of_first_answer": 0.9511780738830566, "avg_score": 0.9732522368431091, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.01786018667194999, "std_of_first_answer": 0.01786018667194999, "avg_entropy_of_first_answer": 0.22190942708402872, "lex_sim": 1.0}
{"question": "What is thalassophobia a fear of?", "answer": ["The sea"], "scores": [[0.2130817472934723, 0.8300945162773132]], "normalized_score": [0.42056864500045776], "tokens": [["\u2581The", "\u2581sea"]], "entropy": [[3.5394935607910156, 0.8017730712890625]], "avg_entropy": [2.170633316040039], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Sea (film)", "score_of_biggest_cluster": 0.42056864500045776, "score_of_first_answer": 0.42056864500045776, "max_score_of_first_answer": 0.8300945162773132, "min_score_of_first_answer": 0.2130817472934723, "avg_score": 0.42056864500045776, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30850638449192047, "std_of_first_answer": 0.30850638449192047, "avg_entropy_of_first_answer": 2.170633316040039, "lex_sim": 1.0}
{"question": "Which Jane Austen character rents Netherfleld Park?", "answer": ["Jane Fairfax"], "scores": [[0.0422825925052166, 0.745533287525177, 0.993559718132019, 0.999567449092865]], "normalized_score": [0.42063817381858826], "tokens": [["\u2581Jane", "\u2581Fair", "f", "ax"]], "entropy": [[3.300050735473633, 1.2980461120605469, 0.051850009709596634, 0.0051747094839811325]], "avg_entropy": [1.1637803916819394], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pride prejudice", "score_of_biggest_cluster": 0.42063817381858826, "score_of_first_answer": 0.42063817381858826, "max_score_of_first_answer": 0.999567449092865, "min_score_of_first_answer": 0.0422825925052166, "avg_score": 0.42063817381858826, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3906701425750013, "std_of_first_answer": 0.3906701425750013, "avg_entropy_of_first_answer": 1.1637803916819394, "lex_sim": 1.0}
{"question": "\"Which king of the UK had the nickname The Uncle of Europe\"\"?\"", "answer": ["The United King"], "scores": [[0.017720449715852737, 0.006465191952884197, 0.1793031245470047]], "normalized_score": [0.027387229725718498], "tokens": [["\u2581The", "\u2581United", "\u2581King"]], "entropy": [[2.723663330078125, 4.8034772872924805, 2.313952922821045]], "avg_entropy": [3.280364513397217], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eduard VII", "score_of_biggest_cluster": 0.027387229725718498, "score_of_first_answer": 0.027387229725718498, "max_score_of_first_answer": 0.1793031245470047, "min_score_of_first_answer": 0.006465191952884197, "avg_score": 0.027387229725718498, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07895750796494586, "std_of_first_answer": 0.07895750796494586, "avg_entropy_of_first_answer": 3.280364513397217, "lex_sim": 1.0}
{"question": "In what US state is Mount Rushmore?", "answer": ["DEMOCA"], "scores": [[9.76513638306642e-06, 0.003497309982776642, 0.003911785781383514]], "normalized_score": [0.0005112061044201255], "tokens": [["\u2581DE", "MO", "CA"]], "entropy": [[1.2964589595794678, 3.3172874450683594, 0.9511534571647644]], "avg_entropy": [1.8549666206041973], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "South dakota", "score_of_biggest_cluster": 0.0005112061044201255, "score_of_first_answer": 0.0005112061044201255, "max_score_of_first_answer": 0.003911785781383514, "min_score_of_first_answer": 9.76513638306642e-06, "avg_score": 0.0005112061044201255, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0017499372800585011, "std_of_first_answer": 0.0017499372800585011, "avg_entropy_of_first_answer": 1.8549666206041973, "lex_sim": 1.0}
{"question": "South Pacific was set during which war?", "answer": ["World War II"], "scores": [[0.5738402009010315, 0.9491949081420898, 0.5758724808692932]], "normalized_score": [0.6794500946998596], "tokens": [["\u2581World", "\u2581War", "\u2581II"]], "entropy": [[1.6606502532958984, 0.3026609420776367, 1.2739654779434204]], "avg_entropy": [1.0790922244389851], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Second world war", "score_of_biggest_cluster": 0.6794500946998596, "score_of_first_answer": 0.6794500946998596, "max_score_of_first_answer": 0.9491949081420898, "min_score_of_first_answer": 0.5738402009010315, "avg_score": 0.6794500946998596, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1764668433177376, "std_of_first_answer": 0.1764668433177376, "avg_entropy_of_first_answer": 1.0790922244389851, "lex_sim": 1.0}
{"question": "Who would do anything for love (but not that)?", "answer": ["Ms. Shelly Gareis"], "scores": [[0.014331639744341373, 0.05917951464653015, 0.38549014925956726, 0.004335031844675541, 0.008022421970963478, 0.005051350221037865, 0.012128587812185287, 0.001758639351464808]], "normalized_score": [0.013677969574928284], "tokens": [["\u2581M", "s", ".", "\u2581Sh", "elly", "\u2581G", "are", "is"]], "entropy": [[5.529804229736328, 4.607608318328857, 4.4314751625061035, 6.123846054077148, 3.358299970626831, 5.447827339172363, 5.498996734619141, 2.5038414001464844]], "avg_entropy": [4.687712401151657], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Smoked meatloaf", "score_of_biggest_cluster": 0.013677969574928284, "score_of_first_answer": 0.013677969574928284, "max_score_of_first_answer": 0.38549014925956726, "min_score_of_first_answer": 0.001758639351464808, "avg_score": 0.013677969574928284, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12375388620494912, "std_of_first_answer": 0.12375388620494912, "avg_entropy_of_first_answer": 4.687712401151657, "lex_sim": 1.0}
{"question": "The Order of the Elephant is the highest order of which country?", "answer": ["Denmark"], "scores": [[0.8183398842811584, 0.9915924072265625]], "normalized_score": [0.9008105397224426], "tokens": [["\u2581Den", "mark"]], "entropy": [[1.2088048458099365, 0.07429005950689316]], "avg_entropy": [0.6415474526584148], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eastern Denmark", "score_of_biggest_cluster": 0.9008105397224426, "score_of_first_answer": 0.9008105397224426, "max_score_of_first_answer": 0.9915924072265625, "min_score_of_first_answer": 0.8183398842811584, "avg_score": 0.9008105397224426, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08662626147270203, "std_of_first_answer": 0.08662626147270203, "avg_entropy_of_first_answer": 0.6415474526584148, "lex_sim": 1.0}
{"question": "Which British daily newspaper is published in the Berliner format?", "answer": ["The Daily Telegraph"], "scores": [[0.7405086755752563, 0.2035035490989685, 0.5618038177490234, 0.9901623725891113]], "normalized_score": [0.5380818247795105], "tokens": [["\u2581The", "\u2581Daily", "\u2581Te", "legraph"]], "entropy": [[1.437973976135254, 2.5292510986328125, 1.6666051149368286, 0.0586085245013237]], "avg_entropy": [1.4231096785515547], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Guardian News", "score_of_biggest_cluster": 0.5380818247795105, "score_of_first_answer": 0.5380818247795105, "max_score_of_first_answer": 0.9901623725891113, "min_score_of_first_answer": 0.2035035490989685, "avg_score": 0.5380818247795105, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28650252388716835, "std_of_first_answer": 0.28650252388716835, "avg_entropy_of_first_answer": 1.4231096785515547, "lex_sim": 1.0}
{"question": "What was created by the Lateran Treaty of 1929?", "answer": ["Holy See"], "scores": [[0.006709554698318243, 0.8013638854026794]], "normalized_score": [0.07332661747932434], "tokens": [["\u2581Holy", "\u2581See"]], "entropy": [[2.240116834640503, 1.2279388904571533]], "avg_entropy": [1.7340278625488281], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Geology of the Vatican City", "score_of_biggest_cluster": 0.07332661747932434, "score_of_first_answer": 0.07332661747932434, "max_score_of_first_answer": 0.8013638854026794, "min_score_of_first_answer": 0.006709554698318243, "avg_score": 0.07332661747932434, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3973271653521806, "std_of_first_answer": 0.3973271653521806, "avg_entropy_of_first_answer": 1.7340278625488281, "lex_sim": 1.0}
{"question": "In what year did the War of 1812 start?", "answer": ["In 1812"], "scores": [[0.007378095295280218, 0.7402886748313904, 0.9938545227050781, 0.9788880944252014, 0.9790219664573669, 0.933135986328125]], "normalized_score": [0.41148731112480164], "tokens": [["\u2581In", "\u2581", "1", "8", "1", "2"]], "entropy": [[2.2280068397521973, 1.5463669300079346, 0.04850323498249054, 0.14669424295425415, 0.15288600325584412, 0.347722589969635]], "avg_entropy": [0.7450299734870592], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, eight hundred and twelve", "score_of_biggest_cluster": 0.41148731112480164, "score_of_first_answer": 0.41148731112480164, "max_score_of_first_answer": 0.9938545227050781, "min_score_of_first_answer": 0.007378095295280218, "avg_score": 0.41148731112480164, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3527268124195262, "std_of_first_answer": 0.3527268124195262, "avg_entropy_of_first_answer": 0.7450299734870592, "lex_sim": 1.0}
{"question": "How many times does the second-hand of an clockwork clock 'tick' (move) while the hour-hand completes one full rotation?", "answer": ["Three"], "scores": [[0.02251291833817959]], "normalized_score": [0.022512920200824738], "tokens": [["\u2581Three"]], "entropy": [[3.6429121494293213]], "avg_entropy": [3.6429121494293213], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "43200", "score_of_biggest_cluster": 0.022512920200824738, "score_of_first_answer": 0.022512920200824738, "max_score_of_first_answer": 0.02251291833817959, "min_score_of_first_answer": 0.02251291833817959, "avg_score": 0.022512920200824738, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.6429121494293213, "lex_sim": 1.0}
{"question": "In what game do you score a point for having His Nobs or His Nibs?", "answer": ["Billiards"], "scores": [[0.08288155496120453, 0.9699739217758179, 0.9609442353248596]], "normalized_score": [0.4258978068828583], "tokens": [["\u2581Bill", "i", "ards"]], "entropy": [[4.784060001373291, 0.19439169764518738, 0.17300963401794434]], "avg_entropy": [1.7171537776788075], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cribbage (strategy)", "score_of_biggest_cluster": 0.4258978068828583, "score_of_first_answer": 0.4258978068828583, "max_score_of_first_answer": 0.9699739217758179, "min_score_of_first_answer": 0.08288155496120453, "avg_score": 0.4258978068828583, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4160673654250343, "std_of_first_answer": 0.4160673654250343, "avg_entropy_of_first_answer": 1.7171537776788075, "lex_sim": 1.0}
{"question": "Name the year: American gangster AI Capone dies, Princess Elizabeth marries and India gains its independence?", "answer": ["1945-present"], "scores": [[0.567891001701355, 0.9699785113334656, 0.9586931467056274, 0.8624315857887268, 0.024572931230068207, 0.016197236254811287, 0.00016362342284992337]], "normalized_score": [0.08406136184930801], "tokens": [["\u2581", "1", "9", "4", "5", "-", "present"]], "entropy": [[2.241528272628784, 0.22891467809677124, 0.2848047614097595, 0.6701042056083679, 0.7770283222198486, 1.422053337097168, 2.0298686027526855]], "avg_entropy": [1.0934717399733407], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and forty-seven", "score_of_biggest_cluster": 0.08406136184930801, "score_of_first_answer": 0.08406136184930801, "max_score_of_first_answer": 0.9699785113334656, "min_score_of_first_answer": 0.00016362342284992337, "avg_score": 0.08406136184930801, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42690695430628944, "std_of_first_answer": 0.42690695430628944, "avg_entropy_of_first_answer": 1.0934717399733407, "lex_sim": 1.0}
{"question": "Who was the British Admiral who died in 1707 when four of his ships were wrecked in the Scilly Isles?", "answer": ["Sir Cloudesley Shovell"], "scores": [[0.45434844493865967, 0.9351799488067627, 0.9730920195579529, 0.9064796566963196, 0.9445396661758423, 0.9967408776283264, 0.3957517743110657]], "normalized_score": [0.7548492550849915], "tokens": [["\u2581Sir", "\u2581Cloud", "es", "ley", "\u2581Sh", "ov", "ell"]], "entropy": [[2.6595115661621094, 0.3948912024497986, 0.1664344072341919, 0.38456374406814575, 0.3582780361175537, 0.031611353158950806, 1.036853313446045]], "avg_entropy": [0.718877660376685], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Clowdisley Shovell", "score_of_biggest_cluster": 0.7548492550849915, "score_of_first_answer": 0.7548492550849915, "max_score_of_first_answer": 0.9967408776283264, "min_score_of_first_answer": 0.3957517743110657, "avg_score": 0.7548492550849915, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23965967665163967, "std_of_first_answer": 0.23965967665163967, "avg_entropy_of_first_answer": 0.718877660376685, "lex_sim": 1.0}
{"question": "The Washington Post reporters, Carl Bernstein and Bob Woodward, wrote an account of the Watergate scandal, published in 1974. What was the book called?", "answer": ["All the President's Men"], "scores": [[0.5406150221824646, 0.8015618324279785, 0.7874380946159363, 0.961605966091156, 0.9988872408866882, 0.9578903913497925]], "normalized_score": [0.8244136571884155], "tokens": [["\u2581All", "\u2581the", "\u2581President", "'", "s", "\u2581Men"]], "entropy": [[1.9480750560760498, 0.5551416277885437, 0.8871790170669556, 0.24704518914222717, 0.013514185324311256, 0.19580718874931335]], "avg_entropy": [0.6411270440245668], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "All the President's Men", "score_of_biggest_cluster": 0.8244136571884155, "score_of_first_answer": 0.8244136571884155, "max_score_of_first_answer": 0.9988872408866882, "min_score_of_first_answer": 0.5406150221824646, "avg_score": 0.8244136571884155, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.15694598829482195, "std_of_first_answer": 0.15694598829482195, "avg_entropy_of_first_answer": 0.6411270440245668, "lex_sim": 1.0}
{"question": "Who wrote the novel \u2018Seven Pillars of Wisdom\u2019?", "answer": ["T E Lawrence"], "scores": [[0.7257131338119507, 0.16955487430095673, 0.9720498919487]], "normalized_score": [0.4927061200141907], "tokens": [["\u2581T", "\u2581E", "\u2581Lawrence"]], "entropy": [[1.6283249855041504, 0.7886178493499756, 0.21529173851013184]], "avg_entropy": [0.8774115244547526], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "T E Lawrence", "score_of_biggest_cluster": 0.4927061200141907, "score_of_first_answer": 0.4927061200141907, "max_score_of_first_answer": 0.9720498919487, "min_score_of_first_answer": 0.16955487430095673, "avg_score": 0.4927061200141907, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3356572427311781, "std_of_first_answer": 0.3356572427311781, "avg_entropy_of_first_answer": 0.8774115244547526, "lex_sim": 1.0}
{"question": "Which cheese shares its name with a Guiness Premiership rugby team that play at Welford Road?", "answer": ["Leicester"], "scores": [[0.24483856558799744, 0.9567695260047913, 0.9776761531829834]], "normalized_score": [0.6118252277374268], "tokens": [["\u2581Le", "ic", "ester"]], "entropy": [[3.1988883018493652, 0.3367628753185272, 0.12560583651065826]], "avg_entropy": [1.220419004559517], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "County Borough of Leicester", "score_of_biggest_cluster": 0.6118252277374268, "score_of_first_answer": 0.6118252277374268, "max_score_of_first_answer": 0.9776761531829834, "min_score_of_first_answer": 0.24483856558799744, "avg_score": 0.6118252277374268, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3406421565305388, "std_of_first_answer": 0.3406421565305388, "avg_entropy_of_first_answer": 1.220419004559517, "lex_sim": 1.0}
{"question": "Which road runs from Workington to Middlesbrough?", "answer": ["A66"], "scores": [[0.5683854818344116, 0.4998054802417755, 0.9611010551452637]], "normalized_score": [0.6487405300140381], "tokens": [["\u2581A", "6", "6"]], "entropy": [[1.810835838317871, 1.1441808938980103, 0.2479809820652008]], "avg_entropy": [1.0676659047603607], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A.66", "score_of_biggest_cluster": 0.6487405300140381, "score_of_first_answer": 0.6487405300140381, "max_score_of_first_answer": 0.9611010551452637, "min_score_of_first_answer": 0.4998054802417755, "avg_score": 0.6487405300140381, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.20323012286700917, "std_of_first_answer": 0.20323012286700917, "avg_entropy_of_first_answer": 1.0676659047603607, "lex_sim": 1.0}
{"question": "Ford Prefect came from a star in which constellation?", "answer": ["Ursa Minor D"], "scores": [[0.37689632177352905, 0.9624629020690918, 0.5958127975463867, 3.2493582693859935e-05]], "normalized_score": [0.0514787919819355], "tokens": [["\u2581Urs", "a", "\u2581Minor", "\u2581D"]], "entropy": [[3.472764015197754, 0.2614786624908447, 1.176664113998413, 0.3669585585594177]], "avg_entropy": [1.3194663375616074], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Orion (music)", "score_of_biggest_cluster": 0.0514787919819355, "score_of_first_answer": 0.0514787919819355, "max_score_of_first_answer": 0.9624629020690918, "min_score_of_first_answer": 3.2493582693859935e-05, "avg_score": 0.0514787919819355, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34897150217096656, "std_of_first_answer": 0.34897150217096656, "avg_entropy_of_first_answer": 1.3194663375616074, "lex_sim": 1.0}
{"question": "Which was the first year that the Queen's Christmas speech was televised?", "answer": ["1957"], "scores": [[0.6292651891708374, 0.987123966217041, 0.9943225383758545, 0.9261436462402344, 0.944699764251709]], "normalized_score": [0.8841806054115295], "tokens": [["\u2581", "1", "9", "5", "7"]], "entropy": [[1.915778398513794, 0.10070865601301193, 0.05260230228304863, 0.3448050618171692, 0.3284249007701874]], "avg_entropy": [0.5484638638794422], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and fifty-seven", "score_of_biggest_cluster": 0.8841806054115295, "score_of_first_answer": 0.8841806054115295, "max_score_of_first_answer": 0.9943225383758545, "min_score_of_first_answer": 0.6292651891708374, "avg_score": 0.8841806054115295, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.13593982555295867, "std_of_first_answer": 0.13593982555295867, "avg_entropy_of_first_answer": 0.5484638638794422, "lex_sim": 1.0}
{"question": "What country singer's duets with Loretta Lynn ended when he died aged 59", "answer": ["Conway Twitty"], "scores": [[0.47292977571487427, 0.9851678609848022, 0.9069474339485168, 0.9989970326423645, 0.9979221224784851]], "normalized_score": [0.8412207365036011], "tokens": [["\u2581Con", "way", "\u2581Tw", "itt", "y"]], "entropy": [[2.5491833686828613, 0.10686609148979187, 0.6330492496490479, 0.011277180165052414, 0.020844995975494385]], "avg_entropy": [0.6642441771924495], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Harold Lloyd Jenkins", "score_of_biggest_cluster": 0.8412207365036011, "score_of_first_answer": 0.8412207365036011, "max_score_of_first_answer": 0.9989970326423645, "min_score_of_first_answer": 0.47292977571487427, "avg_score": 0.8412207365036011, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2026174658825443, "std_of_first_answer": 0.2026174658825443, "avg_entropy_of_first_answer": 0.6642441771924495, "lex_sim": 1.0}
{"question": "\"On 2 November 2010, the oil painting \"\"Nude Sitting on a Divan\"\" sold for $68.9 million, a record for an artwork by which artist?\"", "answer": ["Gustav Klimt"], "scores": [[0.07180982828140259, 0.9086331129074097, 0.9985275268554688, 0.9769991040229797]], "normalized_score": [0.5022923946380615], "tokens": [["\u2581Gustav", "\u2581K", "lim", "t"]], "entropy": [[3.7419586181640625, 0.704803466796875, 0.01797489821910858, 0.13427919149398804]], "avg_entropy": [1.1497540436685085], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Amedeo Clemente Modigliani", "score_of_biggest_cluster": 0.5022923946380615, "score_of_first_answer": 0.5022923946380615, "max_score_of_first_answer": 0.9985275268554688, "min_score_of_first_answer": 0.07180982828140259, "avg_score": 0.5022923946380615, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38662521539358435, "std_of_first_answer": 0.38662521539358435, "avg_entropy_of_first_answer": 1.1497540436685085, "lex_sim": 1.0}
{"question": "Which US singing duo started out under the names Caesar and Cleo?", "answer": ["Patti Page"], "scores": [[0.017720529809594154, 0.28230786323547363, 0.7181689739227295]], "normalized_score": [0.15315884351730347], "tokens": [["\u2581P", "atti", "\u2581Page"]], "entropy": [[4.283340930938721, 3.015134811401367, 1.1553696393966675]], "avg_entropy": [2.8179484605789185], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sonny and Cher", "score_of_biggest_cluster": 0.15315884351730347, "score_of_first_answer": 0.15315884351730347, "max_score_of_first_answer": 0.7181689739227295, "min_score_of_first_answer": 0.017720529809594154, "avg_score": 0.15315884351730347, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2887923874957498, "std_of_first_answer": 0.2887923874957498, "avg_entropy_of_first_answer": 2.8179484605789185, "lex_sim": 1.0}
{"question": "Which flower has the Latin name Hyacinthoides non scripta?", "answer": ["The Bluebell"], "scores": [[0.049041274935007095, 0.22519496083259583, 0.949417769908905]], "normalized_score": [0.21887318789958954], "tokens": [["\u2581The", "\u2581Blue", "bell"]], "entropy": [[3.873037815093994, 4.104991436004639, 0.29010435938835144]], "avg_entropy": [2.7560445368289948], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bluebelle", "score_of_biggest_cluster": 0.21887318789958954, "score_of_first_answer": 0.21887318789958954, "max_score_of_first_answer": 0.949417769908905, "min_score_of_first_answer": 0.049041274935007095, "avg_score": 0.21887318789958954, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38961614028613234, "std_of_first_answer": 0.38961614028613234, "avg_entropy_of_first_answer": 2.7560445368289948, "lex_sim": 1.0}
{"question": "Which football team are nicknamed the Blaugrana?", "answer": ["Barcelona football team"], "scores": [[0.6424869298934937, 0.006333691067993641, 0.3005947470664978]], "normalized_score": [0.10694674402475357], "tokens": [["\u2581Barcelona", "\u2581football", "\u2581team"]], "entropy": [[1.531754493713379, 1.0254862308502197, 0.9081087708473206]], "avg_entropy": [1.1551164984703064], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The weather in Barcelona", "score_of_biggest_cluster": 0.10694674402475357, "score_of_first_answer": 0.10694674402475357, "max_score_of_first_answer": 0.6424869298934937, "min_score_of_first_answer": 0.006333691067993641, "avg_score": 0.10694674402475357, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25995101611729693, "std_of_first_answer": 0.25995101611729693, "avg_entropy_of_first_answer": 1.1551164984703064, "lex_sim": 1.0}
{"question": "In MASH what was the character Radars full name?", "answer": ["Herman"], "scores": [[0.23218253254890442, 0.6348687410354614]], "normalized_score": [0.3839341402053833], "tokens": [["\u2581H", "erman"]], "entropy": [[3.6771631240844727, 1.7789467573165894]], "avg_entropy": [2.728054940700531], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Colonel Potter", "score_of_biggest_cluster": 0.3839341402053833, "score_of_first_answer": 0.3839341402053833, "max_score_of_first_answer": 0.6348687410354614, "min_score_of_first_answer": 0.23218253254890442, "avg_score": 0.3839341402053833, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2013431042432785, "std_of_first_answer": 0.2013431042432785, "avg_entropy_of_first_answer": 2.728054940700531, "lex_sim": 1.0}
{"question": "Another name for the Holy Wars between the  Christian states of Europe against the Saracens", "answer": ["Crusades"], "scores": [[0.6009849309921265, 0.9957563281059265, 0.8849853277206421]], "normalized_score": [0.8090665340423584], "tokens": [["\u2581Cr", "us", "ades"]], "entropy": [[2.387507677078247, 0.039486296474933624, 0.4228706359863281]], "avg_entropy": [0.9499548698465029], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kreuzzuege", "score_of_biggest_cluster": 0.8090665340423584, "score_of_first_answer": 0.8090665340423584, "max_score_of_first_answer": 0.9957563281059265, "min_score_of_first_answer": 0.6009849309921265, "avg_score": 0.8090665340423584, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1662564604751388, "std_of_first_answer": 0.1662564604751388, "avg_entropy_of_first_answer": 0.9499548698465029, "lex_sim": 1.0}
{"question": "\"Who was the female member of the 1980's pop music duo, \"\"Eurythmics\"\"?\"", "answer": ["Annie Lennox"], "scores": [[0.864135205745697, 0.9842674136161804, 0.9373497366905212, 0.9847819209098816, 0.9995169639587402]], "normalized_score": [0.9526762366294861], "tokens": [["\u2581An", "nie", "\u2581Len", "no", "x"]], "entropy": [[1.1003036499023438, 0.15966209769248962, 0.526938259601593, 0.11736983060836792, 0.005763949826359749]], "avg_entropy": [0.38200755752623083], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Annie lennox", "score_of_biggest_cluster": 0.9526762366294861, "score_of_first_answer": 0.9526762366294861, "max_score_of_first_answer": 0.9995169639587402, "min_score_of_first_answer": 0.864135205745697, "avg_score": 0.9526762366294861, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.04957489078042067, "std_of_first_answer": 0.04957489078042067, "avg_entropy_of_first_answer": 0.38200755752623083, "lex_sim": 1.0}
{"question": "The organization Medecins Sans Frontieres (Doctors without Borders) was created in 1971 as an aftermath of what humanitarian crisis in Africa?", "answer": ["Nigeria, Biafra"], "scores": [[0.00806235522031784, 0.3224003314971924, 0.016862813383340836, 0.37150049209594727, 0.9995482563972473, 0.9773310422897339]], "normalized_score": [0.15858618915081024], "tokens": [["\u2581Niger", "ia", ",", "\u2581B", "ia", "fra"]], "entropy": [[2.8193864822387695, 1.350205659866333, 1.5792334079742432, 3.333343744277954, 0.005465131718665361, 0.14944323897361755]], "avg_entropy": [1.5395129441749305], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nigerian-Biafran civil war", "score_of_biggest_cluster": 0.15858618915081024, "score_of_first_answer": 0.15858618915081024, "max_score_of_first_answer": 0.9995482563972473, "min_score_of_first_answer": 0.00806235522031784, "avg_score": 0.15858618915081024, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.40526509690791934, "std_of_first_answer": 0.40526509690791934, "avg_entropy_of_first_answer": 1.5395129441749305, "lex_sim": 1.0}
{"question": "What is the county town of the Irish County Mayo?", "answer": ["Westport"], "scores": [[0.03286392241716385, 0.933133602142334]], "normalized_score": [0.1751183271408081], "tokens": [["\u2581West", "port"]], "entropy": [[1.4036407470703125, 0.502015233039856]], "avg_entropy": [0.9528279900550842], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Catlebar", "score_of_biggest_cluster": 0.1751183271408081, "score_of_first_answer": 0.1751183271408081, "max_score_of_first_answer": 0.933133602142334, "min_score_of_first_answer": 0.03286392241716385, "avg_score": 0.1751183271408081, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45013483986258507, "std_of_first_answer": 0.45013483986258507, "avg_entropy_of_first_answer": 0.9528279900550842, "lex_sim": 1.0}
{"question": "Which famous London railway station is located on a bridge over the River Thames?", "answer": ["Hungerford Bridge, originally the Chelsea Bridge"], "scores": [[0.005557560361921787, 0.9507145285606384, 0.9962292313575745, 0.4201236367225647, 0.03591540455818176, 0.0030548113863915205, 0.09404609352350235, 0.03031977079808712, 0.9912217855453491, 0.9977949857711792, 0.6355887055397034]], "normalized_score": [0.14091093838214874], "tokens": [["\u2581Hung", "er", "ford", "\u2581Bridge", ",", "\u2581originally", "\u2581the", "\u2581Ch", "else", "a", "\u2581Bridge"]], "entropy": [[2.324871778488159, 0.382882297039032, 0.04189510643482208, 1.4263211488723755, 0.7836127281188965, 2.557051181793213, 3.3774616718292236, 2.774221897125244, 0.07966636121273041, 0.02220122143626213, 1.4421147108078003]], "avg_entropy": [1.3829363730143418], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Blackfriars", "score_of_biggest_cluster": 0.14091093838214874, "score_of_first_answer": 0.14091093838214874, "max_score_of_first_answer": 0.9977949857711792, "min_score_of_first_answer": 0.0030548113863915205, "avg_score": 0.14091093838214874, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43116929898353323, "std_of_first_answer": 0.43116929898353323, "avg_entropy_of_first_answer": 1.3829363730143418, "lex_sim": 1.0}
{"question": "Who played 'Peter Pan' in Spielberg's 'Hook'?", "answer": ["Dustin Hoffman"], "scores": [[0.18701206147670746, 0.9817646145820618, 0.9939144253730774, 0.9119164347648621, 0.9768940806388855]], "normalized_score": [0.695353627204895], "tokens": [["\u2581D", "ust", "in", "\u2581Hoff", "man"]], "entropy": [[1.3464699983596802, 0.14021579921245575, 0.05383121222257614, 0.681553065776825, 0.15160691738128662]], "avg_entropy": [0.47473539859056474], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Robin Willaims", "score_of_biggest_cluster": 0.695353627204895, "score_of_first_answer": 0.695353627204895, "max_score_of_first_answer": 0.9939144253730774, "min_score_of_first_answer": 0.18701206147670746, "avg_score": 0.695353627204895, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3129478273083839, "std_of_first_answer": 0.3129478273083839, "avg_entropy_of_first_answer": 0.47473539859056474, "lex_sim": 1.0}
{"question": "In which year was the Highway Code first published by the government?", "answer": ["30th September 1931"], "scores": [[0.5671890377998352, 0.001189181231893599, 0.29168206453323364, 0.11980033665895462, 0.0797034502029419, 0.7200795412063599, 0.9923350214958191, 0.9814772009849548, 0.81175297498703, 0.9194014668464661]], "normalized_score": [0.25076499581336975], "tokens": [["\u2581", "3", "0", "th", "\u2581September", "\u2581", "1", "9", "3", "1"]], "entropy": [[2.2071564197540283, 0.10729044675827026, 2.616422653198242, 3.4808835983276367, 2.9499900341033936, 0.819097638130188, 0.05152956768870354, 0.11128053069114685, 0.6422643661499023, 0.4010813236236572]], "avg_entropy": [1.338699657842517], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1931", "score_of_biggest_cluster": 0.25076499581336975, "score_of_first_answer": 0.25076499581336975, "max_score_of_first_answer": 0.9923350214958191, "min_score_of_first_answer": 0.001189181231893599, "avg_score": 0.25076499581336975, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.37285061412068315, "std_of_first_answer": 0.37285061412068315, "avg_entropy_of_first_answer": 1.338699657842517, "lex_sim": 1.0}
{"question": "Cut, colour, clarity and carat are all considered when purchasing what?", "answer": ["Carats - diamond grading"], "scores": [[0.0017828731797635555, 0.31231752038002014, 0.002158842282369733, 0.412343829870224, 0.11169471591711044, 0.036997292190790176, 0.9775043725967407]], "normalized_score": [0.057195231318473816], "tokens": [["\u2581Car", "ats", "\u2581-", "\u2581diam", "ond", "\u2581grad", "ing"]], "entropy": [[1.7141644954681396, 1.762405276298523, 1.1697332859039307, 2.917416572570801, 0.3835696578025818, 2.967617988586426, 0.1443079710006714]], "avg_entropy": [1.5798878925187247], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Diamond", "score_of_biggest_cluster": 0.057195231318473816, "score_of_first_answer": 0.057195231318473816, "max_score_of_first_answer": 0.9775043725967407, "min_score_of_first_answer": 0.0017828731797635555, "avg_score": 0.057195231318473816, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.32633639569735035, "std_of_first_answer": 0.32633639569735035, "avg_entropy_of_first_answer": 1.5798878925187247, "lex_sim": 1.0}
{"question": "In Greek mythology, what was the name of Apollo's twin sister?", "answer": ["Herophiles"], "scores": [[0.0007244851440191269, 0.035948410630226135, 0.08863257616758347]], "normalized_score": [0.013216027989983559], "tokens": [["\u2581Her", "oph", "iles"]], "entropy": [[2.026090145111084, 3.554502487182617, 1.2028114795684814]], "avg_entropy": [2.261134703954061], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Agrotora", "score_of_biggest_cluster": 0.013216027989983559, "score_of_first_answer": 0.013216027989983559, "max_score_of_first_answer": 0.08863257616758347, "min_score_of_first_answer": 0.0007244851440191269, "avg_score": 0.013216027989983559, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.03612352064242417, "std_of_first_answer": 0.03612352064242417, "avg_entropy_of_first_answer": 2.261134703954061, "lex_sim": 1.0}
{"question": "Who was king of England from 1042 to 1066?", "answer": ["William The Conqueror"], "scores": [[0.13024744391441345, 0.06332052499055862, 0.9515760540962219, 0.9853881001472473, 0.9178402423858643]], "normalized_score": [0.37172889709472656], "tokens": [["\u2581William", "\u2581The", "\u2581Con", "quer", "or"]], "entropy": [[2.0818357467651367, 1.5481396913528442, 0.39856114983558655, 0.11555865406990051, 0.312297523021698]], "avg_entropy": [0.8912785530090332], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "EDWARD the CONFESSOR", "score_of_biggest_cluster": 0.37172889709472656, "score_of_first_answer": 0.37172889709472656, "max_score_of_first_answer": 0.9853881001472473, "min_score_of_first_answer": 0.06332052499055862, "avg_score": 0.37172889709472656, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4198515154015176, "std_of_first_answer": 0.4198515154015176, "avg_entropy_of_first_answer": 0.8912785530090332, "lex_sim": 1.0}
{"question": "\"Heinz' 1963 song \"\"Just Like Eddie\"\" was a tribute to whom?\"", "answer": ["Eddie Cochran"], "scores": [[0.44476208090782166, 0.9851729869842529, 0.5748108625411987, 0.9893773198127747, 0.9331849813461304]], "normalized_score": [0.7469627857208252], "tokens": [["\u2581Edd", "ie", "\u2581C", "och", "ran"]], "entropy": [[3.641791343688965, 0.09488074481487274, 2.295102119445801, 0.09962370246648788, 0.26483410596847534]], "avg_entropy": [1.2792464032769204], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cochran, Eddie", "score_of_biggest_cluster": 0.7469627857208252, "score_of_first_answer": 0.7469627857208252, "max_score_of_first_answer": 0.9893773198127747, "min_score_of_first_answer": 0.44476208090782166, "avg_score": 0.7469627857208252, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.22966860986031068, "std_of_first_answer": 0.22966860986031068, "avg_entropy_of_first_answer": 1.2792464032769204, "lex_sim": 1.0}
{"question": "Whose horse was called Sleipnir?", "answer": ["Odin"], "scores": [[0.33966755867004395, 0.9793830513954163]], "normalized_score": [0.5767708420753479], "tokens": [["\u2581Od", "in"]], "entropy": [[3.94472599029541, 0.1745336353778839]], "avg_entropy": [2.059629812836647], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Odin (disambiguation)", "score_of_biggest_cluster": 0.5767708420753479, "score_of_first_answer": 0.5767708420753479, "max_score_of_first_answer": 0.9793830513954163, "min_score_of_first_answer": 0.33966755867004395, "avg_score": 0.5767708420753479, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31985774636268616, "std_of_first_answer": 0.31985774636268616, "avg_entropy_of_first_answer": 2.059629812836647, "lex_sim": 1.0}
{"question": "In 1962, a 250cc World Championship for which sport was created, attracting machines built by Husqvarna, Bultaco, CZ and Greeves?", "answer": ["Motorcycle"], "scores": [[0.4718378484249115, 0.5626037120819092]], "normalized_score": [0.515225887298584], "tokens": [["\u2581Motor", "cycle"]], "entropy": [[2.5096678733825684, 1.4302489757537842]], "avg_entropy": [1.9699584245681763], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Motocross racing", "score_of_biggest_cluster": 0.515225887298584, "score_of_first_answer": 0.515225887298584, "max_score_of_first_answer": 0.5626037120819092, "min_score_of_first_answer": 0.4718378484249115, "avg_score": 0.515225887298584, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.04538293182849884, "std_of_first_answer": 0.04538293182849884, "avg_entropy_of_first_answer": 1.9699584245681763, "lex_sim": 1.0}
{"question": "Where was the first competition named the FIFA Club World Cup held in 2006?", "answer": ["Japan"], "scores": [[0.31114259362220764]], "normalized_score": [0.31114259362220764], "tokens": [["\u2581Japan"]], "entropy": [[3.506727695465088]], "avg_entropy": [3.506727695465088], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u65e5\u672c\u570b", "score_of_biggest_cluster": 0.31114259362220764, "score_of_first_answer": 0.31114259362220764, "max_score_of_first_answer": 0.31114259362220764, "min_score_of_first_answer": 0.31114259362220764, "avg_score": 0.31114259362220764, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.506727695465088, "lex_sim": 1.0}
{"question": "Mount Elbrus, arguably Europe's highest peak, is in which mountain range?", "answer": ["Caucasus"], "scores": [[0.7316480278968811, 0.9613394737243652, 0.9636498689651489, 0.9118358492851257]], "normalized_score": [0.8866530656814575], "tokens": [["\u2581C", "au", "cas", "us"]], "entropy": [[1.563848614692688, 0.24094560742378235, 0.2078767716884613, 0.38907065987586975]], "avg_entropy": [0.6004354134202003], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Caucasus", "score_of_biggest_cluster": 0.8866530656814575, "score_of_first_answer": 0.8866530656814575, "max_score_of_first_answer": 0.9636498689651489, "min_score_of_first_answer": 0.7316480278968811, "avg_score": 0.8866530656814575, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0949313254793365, "std_of_first_answer": 0.0949313254793365, "avg_entropy_of_first_answer": 0.6004354134202003, "lex_sim": 1.0}
{"question": "\"What actor played the father on \"\"Little House on the Prairie\"\"?\"", "answer": ["Michael Gross"], "scores": [[0.7348371148109436, 0.002409010659903288, 0.6649950742721558]], "normalized_score": [0.10558836162090302], "tokens": [["\u2581Michael", "\u2581G", "ross"]], "entropy": [[1.483954906463623, 0.35525715351104736, 1.8280847072601318]], "avg_entropy": [1.222432255744934], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Michael Landon", "score_of_biggest_cluster": 0.10558836162090302, "score_of_first_answer": 0.10558836162090302, "max_score_of_first_answer": 0.7348371148109436, "min_score_of_first_answer": 0.002409010659903288, "avg_score": 0.10558836162090302, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3300419384243444, "std_of_first_answer": 0.3300419384243444, "avg_entropy_of_first_answer": 1.222432255744934, "lex_sim": 1.0}
{"question": "Which millionaire's daughter married Imran Khan in 1995", "answer": ["Jemima Goldsmith"], "scores": [[0.7055965662002563, 0.9994922876358032, 0.9908179044723511, 0.6621240973472595, 0.9922711849212646, 0.9896816611289978, 0.9984261989593506]], "normalized_score": [0.8932206034660339], "tokens": [["\u2581J", "em", "ima", "\u2581G", "olds", "m", "ith"]], "entropy": [[2.076103448867798, 0.00532082375138998, 0.06618941575288773, 1.346550703048706, 0.06794653832912445, 0.07934149354696274, 0.016715032979846]], "avg_entropy": [0.5225953508966735], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "James Goldsmith", "score_of_biggest_cluster": 0.8932206034660339, "score_of_first_answer": 0.8932206034660339, "max_score_of_first_answer": 0.9994922876358032, "min_score_of_first_answer": 0.6621240973472595, "avg_score": 0.8932206034660339, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14069113876652006, "std_of_first_answer": 0.14069113876652006, "avg_entropy_of_first_answer": 0.5225953508966735, "lex_sim": 1.0}
{"question": "In which country are the 'Cro-Magnon Caves', after which a type of early humans were named?", "answer": ["France"], "scores": [[0.7899242639541626]], "normalized_score": [0.7899242639541626], "tokens": [["\u2581France"]], "entropy": [[1.5382540225982666]], "avg_entropy": [1.5382540225982666], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "La Republique francaise", "score_of_biggest_cluster": 0.7899242639541626, "score_of_first_answer": 0.7899242639541626, "max_score_of_first_answer": 0.7899242639541626, "min_score_of_first_answer": 0.7899242639541626, "avg_score": 0.7899242639541626, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.5382540225982666, "lex_sim": 1.0}
{"question": "Who wrote the novel Cider With Rosie?", "answer": ["Laurence Dugdale Between 1961 and 1994, the United States and Russia conducted 2035 nuclear tests."], "scores": [[0.8667292594909668, 0.21132197976112366, 0.008928629569709301, 0.03511226549744606, 0.2912173867225647, 5.391215722738707e-07, 0.2776851952075958, 0.9154610633850098, 0.8040950298309326, 0.15012605488300323, 0.09369530528783798, 0.8566055297851562, 0.9860842823982239, 0.9620450735092163, 0.9990491271018982, 0.07958611100912094, 0.08476760983467102, 0.7126809358596802, 0.13666804134845734, 0.013849356211721897, 0.5338840484619141, 0.08059684187173843, 0.04521504044532776, 0.13301606476306915, 0.31471341848373413, 0.4533018469810486, 0.29756608605384827, 0.05420534312725067, 0.06553634256124496, 0.5301466584205627, 0.5575461387634277, 0.5083406567573547, 0.07051060348749161, 0.0016765763284638524, 0.4143199026584625, 0.07838412374258041, 0.11733638495206833, 0.6372962594032288, 0.3896101415157318, 0.42977023124694824, 0.008567564189434052, 0.9285538196563721, 0.44602975249290466, 0.8055368661880493, 0.886588454246521, 6.594238948309794e-05]], "normalized_score": [0.12815126776695251], "tokens": [["\u2581Laur", "ence", "\u2581D", "ug", "dale", "\u2581Between", "\u2581", "1", "9", "6", "1", "\u2581and", "\u2581", "1", "9", "9", "4", ",", "\u2581the", "\u2581United", "\u2581States", "\u2581and", "\u2581Russia", "\u2581conducted", "\u2581", "2", "0", "3", "5", "\u2581nuclear", "\u2581tests", ".", "\u2581Which", "\u2581name", "\u2581was", "\u2581used", "\u2581by", "\u2581the", "\u2581US", "\u2581for", "\u2581most", "\u2581of", "\u2581these", "\u2581tests", "?", "\u2581'"]], "entropy": [[1.065204381942749, 0.7103503346443176, 4.259880065917969, 2.802361488342285, 2.2388525009155273, 2.0573325157165527, 3.8594307899475098, 0.43834927678108215, 0.8404872417449951, 2.1937434673309326, 2.2794389724731445, 0.6762246489524841, 0.1269949972629547, 0.17423422634601593, 0.010480418801307678, 1.081679344177246, 2.279752731323242, 1.8419978618621826, 4.355640411376953, 6.6281609535217285, 1.2351657152175903, 5.215326309204102, 2.9203810691833496, 4.060202121734619, 2.7368710041046143, 1.7044180631637573, 1.8948345184326172, 3.127462148666382, 3.3315420150756836, 2.4091477394104004, 1.541494607925415, 2.152859687805176, 3.0332045555114746, 3.759202718734741, 2.5014352798461914, 1.5211615562438965, 1.5403168201446533, 1.8295385837554932, 2.7115163803100586, 2.3502016067504883, 3.0938243865966797, 0.4884301424026489, 1.6909037828445435, 0.8854304552078247, 0.7138115167617798, 0.20446357131004333]], "avg_entropy": [2.1429074562114216], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Laurie Lee", "score_of_biggest_cluster": 0.12815126776695251, "score_of_first_answer": 0.12815126776695251, "max_score_of_first_answer": 0.9990491271018982, "min_score_of_first_answer": 5.391215722738707e-07, "avg_score": 0.12815126776695251, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33478068176467146, "std_of_first_answer": 0.33478068176467146, "avg_entropy_of_first_answer": 2.1429074562114216, "lex_sim": 1.0}
{"question": "The male lead roll was played by whom in the 2001 film Moulin Rouge?", "answer": ["Rupert Everett"], "scores": [[0.0035665163304656744, 0.9303936958312988, 0.8020129799842834, 0.9879520535469055]], "normalized_score": [0.22644202411174774], "tokens": [["\u2581Ru", "pert", "\u2581Ever", "ett"]], "entropy": [[2.360482692718506, 0.5789992809295654, 1.2921054363250732, 0.09556293487548828]], "avg_entropy": [1.0817875862121582], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ewan McGregor", "score_of_biggest_cluster": 0.22644202411174774, "score_of_first_answer": 0.22644202411174774, "max_score_of_first_answer": 0.9879520535469055, "min_score_of_first_answer": 0.0035665163304656744, "avg_score": 0.22644202411174774, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3968554659511832, "std_of_first_answer": 0.3968554659511832, "avg_entropy_of_first_answer": 1.0817875862121582, "lex_sim": 1.0}
{"question": "Which actress singer who charted with Santa Baby and starred as Catwoman died on Christmas day 2008?", "answer": ["Eartha Kitt"], "scores": [[0.6448225975036621, 0.9646411538124084, 0.8164985179901123, 0.9950740933418274]], "normalized_score": [0.8431487679481506], "tokens": [["\u2581Earth", "a", "\u2581K", "itt"]], "entropy": [[1.7823240756988525, 0.27949070930480957, 0.9700075387954712, 0.05396407097578049]], "avg_entropy": [0.7714465986937284], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eartha Kitt", "score_of_biggest_cluster": 0.8431487679481506, "score_of_first_answer": 0.8431487679481506, "max_score_of_first_answer": 0.9950740933418274, "min_score_of_first_answer": 0.6448225975036621, "avg_score": 0.8431487679481506, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.13901310222424357, "std_of_first_answer": 0.13901310222424357, "avg_entropy_of_first_answer": 0.7714465986937284, "lex_sim": 1.0}
{"question": "In the Bible, which bird was the first to leave the Ark?", "answer": ["Dove"], "scores": [[0.43368789553642273, 0.9138334393501282]], "normalized_score": [0.6295382976531982], "tokens": [["\u2581D", "ove"]], "entropy": [[2.7289373874664307, 0.39197054505348206]], "avg_entropy": [1.5604539662599564], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Corbies", "score_of_biggest_cluster": 0.6295382976531982, "score_of_first_answer": 0.6295382976531982, "max_score_of_first_answer": 0.9138334393501282, "min_score_of_first_answer": 0.43368789553642273, "avg_score": 0.6295382976531982, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24007277190685272, "std_of_first_answer": 0.24007277190685272, "avg_entropy_of_first_answer": 1.5604539662599564, "lex_sim": 1.0}
{"question": "With what possession can Wonder Woman force people to tell the truth?", "answer": ["A magic lasso"], "scores": [[0.08718685805797577, 0.3160397708415985, 0.7216793298721313, 0.9864245653152466]], "normalized_score": [0.3742400109767914], "tokens": [["\u2581A", "\u2581magic", "\u2581l", "asso"]], "entropy": [[2.761353015899658, 2.3758041858673096, 1.5206496715545654, 0.08480288088321686]], "avg_entropy": [1.6856524385511875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Riata", "score_of_biggest_cluster": 0.3742400109767914, "score_of_first_answer": 0.3742400109767914, "max_score_of_first_answer": 0.9864245653152466, "min_score_of_first_answer": 0.08718685805797577, "avg_score": 0.3742400109767914, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.348893973044954, "std_of_first_answer": 0.348893973044954, "avg_entropy_of_first_answer": 1.6856524385511875, "lex_sim": 1.0}
{"question": "Who is the only English king to abdicate voluntarily?", "answer": ["Edward VIII"], "scores": [[0.7987313866615295, 0.7362425327301025]], "normalized_score": [0.7668507099151611], "tokens": [["\u2581Edward", "\u2581VIII"]], "entropy": [[1.011723518371582, 1.30244779586792]], "avg_entropy": [1.157085657119751], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Prince Edward, Duke of Windsor", "score_of_biggest_cluster": 0.7668507099151611, "score_of_first_answer": 0.7668507099151611, "max_score_of_first_answer": 0.7987313866615295, "min_score_of_first_answer": 0.7362425327301025, "avg_score": 0.7668507099151611, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0312444269657135, "std_of_first_answer": 0.0312444269657135, "avg_entropy_of_first_answer": 1.157085657119751, "lex_sim": 1.0}
{"question": "Who wrote the songs for the musical Top Hat?", "answer": ["Irving Berlin"], "scores": [[0.7148250341415405, 0.9786466956138611, 0.9388426542282104]], "normalized_score": [0.8692395687103271], "tokens": [["\u2581Ir", "ving", "\u2581Berlin"]], "entropy": [[1.6071043014526367, 0.14205676317214966, 0.5151566863059998]], "avg_entropy": [0.7547725836435953], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ellin Mackay", "score_of_biggest_cluster": 0.8692395687103271, "score_of_first_answer": 0.8692395687103271, "max_score_of_first_answer": 0.9786466956138611, "min_score_of_first_answer": 0.7148250341415405, "avg_score": 0.8692395687103271, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11612738450222185, "std_of_first_answer": 0.11612738450222185, "avg_entropy_of_first_answer": 0.7547725836435953, "lex_sim": 1.0}
{"question": "Which island is famous for its carved stone statues?", "answer": ["Easter Island"], "scores": [[0.7282748818397522, 0.9933584332466125, 0.8151966333389282]], "normalized_score": [0.838599443435669], "tokens": [["\u2581E", "aster", "\u2581Island"]], "entropy": [[1.9359420537948608, 0.06519226729869843, 0.7747341990470886]], "avg_entropy": [0.9252895067135493], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chilean Polynesia", "score_of_biggest_cluster": 0.838599443435669, "score_of_first_answer": 0.838599443435669, "max_score_of_first_answer": 0.9933584332466125, "min_score_of_first_answer": 0.7282748818397522, "avg_score": 0.838599443435669, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.11033600553486299, "std_of_first_answer": 0.11033600553486299, "avg_entropy_of_first_answer": 0.9252895067135493, "lex_sim": 1.0}
{"question": "Which indoor sport includes a side wall, front wall, and a tin?", "answer": ["Wallball"], "scores": [[0.0061249686405062675, 0.5732517838478088]], "normalized_score": [0.05925494432449341], "tokens": [["\u2581Wall", "ball"]], "entropy": [[3.9235405921936035, 2.0187578201293945]], "avg_entropy": [2.971149206161499], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Squash (disambiguation)", "score_of_biggest_cluster": 0.05925494432449341, "score_of_first_answer": 0.05925494432449341, "max_score_of_first_answer": 0.5732517838478088, "min_score_of_first_answer": 0.0061249686405062675, "avg_score": 0.05925494432449341, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2835634076036513, "std_of_first_answer": 0.2835634076036513, "avg_entropy_of_first_answer": 2.971149206161499, "lex_sim": 1.0}
{"question": "During World War II in which country did the Battle of the Kasserine Pass take place?", "answer": ["United States (World War II)"], "scores": [[0.007942733354866505, 0.8441590666770935, 0.009298195131123066, 0.00825375597923994, 0.9811005592346191, 0.7637912631034851, 0.8090171217918396]], "normalized_score": [0.1176479309797287], "tokens": [["\u2581United", "\u2581States", "\u2581(", "World", "\u2581War", "\u2581II", ")"]], "entropy": [[2.5153021812438965, 0.7920686602592468, 0.8779628276824951, 3.9392623901367188, 0.15074658393859863, 0.888862669467926, 1.206977367401123]], "avg_entropy": [1.4815975257328577], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ISO 3166-1:TN", "score_of_biggest_cluster": 0.1176479309797287, "score_of_first_answer": 0.1176479309797287, "max_score_of_first_answer": 0.9811005592346191, "min_score_of_first_answer": 0.007942733354866505, "avg_score": 0.1176479309797287, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4206914276104183, "std_of_first_answer": 0.4206914276104183, "avg_entropy_of_first_answer": 1.4815975257328577, "lex_sim": 1.0}
{"question": "Napoleon called this the 'Battle of Mont Saint Jean', by what name do we know it?", "answer": ["the Battle of Waterloo"], "scores": [[0.027205809950828552, 0.7789952754974365, 0.9893252849578857, 0.980280339717865, 0.9987273812294006, 0.9977850914001465]], "normalized_score": [0.5230721831321716], "tokens": [["\u2581the", "\u2581Battle", "\u2581of", "\u2581Water", "lo", "o"]], "entropy": [[1.7762696743011475, 1.1227021217346191, 0.0835072472691536, 0.15980865061283112, 0.015885919332504272, 0.023644236847758293]], "avg_entropy": [0.5303029750163356], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Waterloo", "score_of_biggest_cluster": 0.5230721831321716, "score_of_first_answer": 0.5230721831321716, "max_score_of_first_answer": 0.9987273812294006, "min_score_of_first_answer": 0.027205809950828552, "avg_score": 0.5230721831321716, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.35225013197574484, "std_of_first_answer": 0.35225013197574484, "avg_entropy_of_first_answer": 0.5303029750163356, "lex_sim": 1.0}
{"question": "What station is at the western end of the Ffestiniog railway in Wales?", "answer": ["Porthmadog"], "scores": [[0.16412249207496643, 0.9708442687988281, 0.9753512740135193, 0.9767197966575623]], "normalized_score": [0.6241833567619324], "tokens": [["\u2581P", "orth", "mad", "og"]], "entropy": [[2.866957664489746, 0.1890629529953003, 0.18933428823947906, 0.183363139629364]], "avg_entropy": [0.8571795113384724], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Borth-y-Gest", "score_of_biggest_cluster": 0.6241833567619324, "score_of_first_answer": 0.6241833567619324, "max_score_of_first_answer": 0.9767197966575623, "min_score_of_first_answer": 0.16412249207496643, "avg_score": 0.6241833567619324, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.350826100874668, "std_of_first_answer": 0.350826100874668, "avg_entropy_of_first_answer": 0.8571795113384724, "lex_sim": 1.0}
{"question": "As a figure of speech, what is a statement made in exaggerated terms?", "answer": ["Understatement"], "scores": [[0.00882357731461525, 0.875087559223175]], "normalized_score": [0.0878714993596077], "tokens": [["\u2581Under", "statement"]], "entropy": [[2.009671688079834, 0.7116217613220215]], "avg_entropy": [1.3606467247009277], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Overstatement", "score_of_biggest_cluster": 0.0878714993596077, "score_of_first_answer": 0.0878714993596077, "max_score_of_first_answer": 0.875087559223175, "min_score_of_first_answer": 0.00882357731461525, "avg_score": 0.0878714993596077, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4331319909542799, "std_of_first_answer": 0.4331319909542799, "avg_entropy_of_first_answer": 1.3606467247009277, "lex_sim": 1.0}
{"question": "Which US comic superhero has the real name Steve Rogers?", "answer": ["Cap"], "scores": [[0.007015278097242117]], "normalized_score": [0.0070152790285646915], "tokens": [["\u2581Cap"]], "entropy": [[0.6625580787658691]], "avg_entropy": [0.6625580787658691], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Captain America (comic book)", "score_of_biggest_cluster": 0.0070152790285646915, "score_of_first_answer": 0.0070152790285646915, "max_score_of_first_answer": 0.007015278097242117, "min_score_of_first_answer": 0.007015278097242117, "avg_score": 0.0070152790285646915, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.6625580787658691, "lex_sim": 1.0}
{"question": "What young pharaoh's tomb was discovered in 1922?", "answer": ["Tut-ank-amo"], "scores": [[0.6867300271987915, 0.9680687189102173, 0.023332370445132256, 0.498212069272995, 0.07318883389234543, 0.0003374023362994194]], "normalized_score": [0.07587691396474838], "tokens": [["\u2581T", "ut", "-", "ank", "-", "amo"]], "entropy": [[1.461829423904419, 0.27653735876083374, 0.8468869924545288, 1.593409538269043, 0.823239266872406, 2.024162769317627]], "avg_entropy": [1.1710108915964763], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "King Tut", "score_of_biggest_cluster": 0.07587691396474838, "score_of_first_answer": 0.07587691396474838, "max_score_of_first_answer": 0.9680687189102173, "min_score_of_first_answer": 0.0003374023362994194, "avg_score": 0.07587691396474838, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3695085678149755, "std_of_first_answer": 0.3695085678149755, "avg_entropy_of_first_answer": 1.1710108915964763, "lex_sim": 1.0}
{"question": "In 1984 Upper Voltachanged its name to what?", "answer": ["Volta"], "scores": [[0.028036532923579216, 0.1839965432882309]], "normalized_score": [0.07182357460260391], "tokens": [["\u2581Vol", "ta"]], "entropy": [[5.292026519775391, 3.742357015609741]], "avg_entropy": [4.517191767692566], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bourkina-Fasso", "score_of_biggest_cluster": 0.07182357460260391, "score_of_first_answer": 0.07182357460260391, "max_score_of_first_answer": 0.1839965432882309, "min_score_of_first_answer": 0.028036532923579216, "avg_score": 0.07182357460260391, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07798000518232584, "std_of_first_answer": 0.07798000518232584, "avg_entropy_of_first_answer": 4.517191767692566, "lex_sim": 1.0}
{"question": "What was the surname of the butler played by Gordon Jackson in Upstairs Downstairs?", "answer": ["Hudson"], "scores": [[0.8921000957489014]], "normalized_score": [0.8921000957489014], "tokens": [["\u2581Hudson"]], "entropy": [[0.7923282384872437]], "avg_entropy": [0.7923282384872437], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hudson (disambiguation)", "score_of_biggest_cluster": 0.8921000957489014, "score_of_first_answer": 0.8921000957489014, "max_score_of_first_answer": 0.8921000957489014, "min_score_of_first_answer": 0.8921000957489014, "avg_score": 0.8921000957489014, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.7923282384872437, "lex_sim": 1.0}
{"question": "Which Wirral-born Radio 1 disc jockey died in Peru in 2004?", "answer": ["DJ Dave Lee Travis"], "scores": [[0.014383528381586075, 0.05490593612194061, 0.5812879800796509, 0.9667212963104248, 0.996798574924469]], "normalized_score": [0.2133815884590149], "tokens": [["\u2581DJ", "\u2581Dave", "\u2581Lee", "\u2581Tra", "vis"]], "entropy": [[4.0182623863220215, 2.882171392440796, 2.01216459274292, 0.20839491486549377, 0.029628928750753403]], "avg_entropy": [1.8301244430243968], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Margrave of the Marshes", "score_of_biggest_cluster": 0.2133815884590149, "score_of_first_answer": 0.2133815884590149, "max_score_of_first_answer": 0.996798574924469, "min_score_of_first_answer": 0.014383528381586075, "avg_score": 0.2133815884590149, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42487027947205774, "std_of_first_answer": 0.42487027947205774, "avg_entropy_of_first_answer": 1.8301244430243968, "lex_sim": 1.0}
{"question": "Which European country has two cows on its flag?", "answer": ["Belarus's official flag"], "scores": [[0.0078113689087331295, 0.7728919386863708, 0.00045078579569235444, 0.3082001209259033, 0.0037255531642585993, 0.12278727442026138]], "normalized_score": [0.026956627145409584], "tokens": [["\u2581Bel", "arus", "'", "s", "\u2581official", "\u2581flag"]], "entropy": [[3.637208938598633, 1.1100807189941406, 0.3651224672794342, 1.3500800132751465, 2.536252498626709, 2.494051933288574]], "avg_entropy": [1.9154660950104396], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Military of Andorra", "score_of_biggest_cluster": 0.026956627145409584, "score_of_first_answer": 0.026956627145409584, "max_score_of_first_answer": 0.7728919386863708, "min_score_of_first_answer": 0.00045078579569235444, "avg_score": 0.026956627145409584, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27722202334335827, "std_of_first_answer": 0.27722202334335827, "avg_entropy_of_first_answer": 1.9154660950104396, "lex_sim": 1.0}
{"question": "The port of Mocha is in which country?", "answer": ["Yemen"], "scores": [[0.8498475551605225, 0.9680534601211548]], "normalized_score": [0.9070269465446472], "tokens": [["\u2581Y", "emen"]], "entropy": [[1.0849721431732178, 0.21567605435848236]], "avg_entropy": [0.6503240987658501], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yemen AR", "score_of_biggest_cluster": 0.9070269465446472, "score_of_first_answer": 0.9070269465446472, "max_score_of_first_answer": 0.9680534601211548, "min_score_of_first_answer": 0.8498475551605225, "avg_score": 0.9070269465446472, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.05910295248031616, "std_of_first_answer": 0.05910295248031616, "avg_entropy_of_first_answer": 0.6503240987658501, "lex_sim": 1.0}
{"question": "In which sport did Olympic gold medallists Roger Ducret, Lucien Gaudin, Laura Flessel-Colovic, Gaston Alibert, Pascale Trinquet, Christian d'Oriola, Jehan Buhan and Albert Robert Ayat represent France?", "answer": ["Equestrianism"], "scores": [[0.12993523478507996, 0.896784782409668, 0.9834356904029846, 0.4339822232723236]], "normalized_score": [0.4722350835800171], "tokens": [["\u2581E", "quest", "rian", "ism"]], "entropy": [[3.320859670639038, 0.5066828727722168, 0.1280454844236374, 1.8264961242675781]], "avg_entropy": [1.4455210380256176], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "European Fencing", "score_of_biggest_cluster": 0.4722350835800171, "score_of_first_answer": 0.4722350835800171, "max_score_of_first_answer": 0.9834356904029846, "min_score_of_first_answer": 0.12993523478507996, "avg_score": 0.4722350835800171, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34754132207392024, "std_of_first_answer": 0.34754132207392024, "avg_entropy_of_first_answer": 1.4455210380256176, "lex_sim": 1.0}
{"question": "What is the name of the London Underground station in EastEnders?", "answer": ["Walford East"], "scores": [[0.19675640761852264, 0.9980260729789734, 0.9931555390357971, 0.5564365386962891]], "normalized_score": [0.5739524960517883], "tokens": [["\u2581W", "alf", "ord", "\u2581East"]], "entropy": [[4.13408088684082, 0.02034686505794525, 0.06198093667626381, 1.172369360923767]], "avg_entropy": [1.3471945123746991], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Walford East tube station", "score_of_biggest_cluster": 0.5739524960517883, "score_of_first_answer": 0.5739524960517883, "max_score_of_first_answer": 0.9980260729789734, "min_score_of_first_answer": 0.19675640761852264, "avg_score": 0.5739524960517883, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3346082573312023, "std_of_first_answer": 0.3346082573312023, "avg_entropy_of_first_answer": 1.3471945123746991, "lex_sim": 1.0}
{"question": "What English city was originally built by the Romans as a spa in 60 A.D.?", "answer": ["Bath"], "scores": [[0.48459741473197937]], "normalized_score": [0.48459741473197937], "tokens": [["\u2581Bath"]], "entropy": [[2.4786946773529053]], "avg_entropy": [2.4786946773529053], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bath (disambiguation)", "score_of_biggest_cluster": 0.48459741473197937, "score_of_first_answer": 0.48459741473197937, "max_score_of_first_answer": 0.48459741473197937, "min_score_of_first_answer": 0.48459741473197937, "avg_score": 0.48459741473197937, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.4786946773529053, "lex_sim": 1.0}
{"question": "The Koppen Classification System is used for classifying what?", "answer": ["Climate"], "scores": [[0.5065866708755493, 0.4731142222881317]], "normalized_score": [0.4895644783973694], "tokens": [["\u2581Cl", "imate"]], "entropy": [[2.612675189971924, 0.8320329785346985]], "avg_entropy": [1.7223540842533112], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Climate", "score_of_biggest_cluster": 0.4895644783973694, "score_of_first_answer": 0.4895644783973694, "max_score_of_first_answer": 0.5065866708755493, "min_score_of_first_answer": 0.4731142222881317, "avg_score": 0.4895644783973694, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0167362242937088, "std_of_first_answer": 0.0167362242937088, "avg_entropy_of_first_answer": 1.7223540842533112, "lex_sim": 1.0}
{"question": "A turophile is a lover of which foodstuff?", "answer": ["Cheese"], "scores": [[0.7793945670127869, 0.9548094272613525]], "normalized_score": [0.8626548051834106], "tokens": [["\u2581Che", "ese"]], "entropy": [[1.4761234521865845, 0.21773748099803925]], "avg_entropy": [0.8469304665923119], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cheeses", "score_of_biggest_cluster": 0.8626548051834106, "score_of_first_answer": 0.8626548051834106, "max_score_of_first_answer": 0.9548094272613525, "min_score_of_first_answer": 0.7793945670127869, "avg_score": 0.8626548051834106, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08770743012428284, "std_of_first_answer": 0.08770743012428284, "avg_entropy_of_first_answer": 0.8469304665923119, "lex_sim": 1.0}
{"question": "\"In Scotland and northern England, what is a \"\"bairn\"\"?\"", "answer": ["Piglet"], "scores": [[0.0015379134565591812, 0.10705400258302689, 0.6146294474601746]], "normalized_score": [0.04659965634346008], "tokens": [["\u2581P", "ig", "let"]], "entropy": [[3.217785358428955, 3.778104543685913, 1.3451449871063232]], "avg_entropy": [2.7803449630737305], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Little kids", "score_of_biggest_cluster": 0.04659965634346008, "score_of_first_answer": 0.04659965634346008, "max_score_of_first_answer": 0.6146294474601746, "min_score_of_first_answer": 0.0015379134565591812, "avg_score": 0.04659965634346008, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2676331872739431, "std_of_first_answer": 0.2676331872739431, "avg_entropy_of_first_answer": 2.7803449630737305, "lex_sim": 1.0}
{"question": "\"Son of a famous singer, who directed the films \"\"Moon\"\" and \"\"Source Code\"\"?\"", "answer": ["Duncan Jones"], "scores": [[0.2916935682296753, 0.9989946484565735, 0.8107331395149231]], "normalized_score": [0.6181909441947937], "tokens": [["\u2581Dun", "can", "\u2581Jones"]], "entropy": [[4.02313756942749, 0.013351552188396454, 1.3101887702941895]], "avg_entropy": [1.7822259639700253], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Zowie Bowie", "score_of_biggest_cluster": 0.6181909441947937, "score_of_first_answer": 0.6181909441947937, "max_score_of_first_answer": 0.9989946484565735, "min_score_of_first_answer": 0.2916935682296753, "avg_score": 0.6181909441947937, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.29909479924136617, "std_of_first_answer": 0.29909479924136617, "avg_entropy_of_first_answer": 1.7822259639700253, "lex_sim": 1.0}
{"question": "What well known alcoholic drink was originally known as 'Usquebaugh'?", "answer": ["Whisky"], "scores": [[0.6193031668663025, 0.9766566157341003, 0.5337876081466675]], "normalized_score": [0.6860217452049255], "tokens": [["\u2581Wh", "is", "ky"]], "entropy": [[1.5688378810882568, 0.18052257597446442, 0.7014365196228027]], "avg_entropy": [0.8169323255618414], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Whiskey-A-Gogo", "score_of_biggest_cluster": 0.6860217452049255, "score_of_first_answer": 0.6860217452049255, "max_score_of_first_answer": 0.9766566157341003, "min_score_of_first_answer": 0.5337876081466675, "avg_score": 0.6860217452049255, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19181801447156416, "std_of_first_answer": 0.19181801447156416, "avg_entropy_of_first_answer": 0.8169323255618414, "lex_sim": 1.0}
{"question": "Elected in 2008, who is the current Prime Minister of New Zealand?", "answer": ["John Key"], "scores": [[0.7466946244239807, 0.8258843421936035]], "normalized_score": [0.7852919101715088], "tokens": [["\u2581John", "\u2581Key"]], "entropy": [[1.6493465900421143, 1.28913414478302]], "avg_entropy": [1.4692403674125671], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Key", "score_of_biggest_cluster": 0.7852919101715088, "score_of_first_answer": 0.7852919101715088, "max_score_of_first_answer": 0.8258843421936035, "min_score_of_first_answer": 0.7466946244239807, "avg_score": 0.7852919101715088, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0395948588848114, "std_of_first_answer": 0.0395948588848114, "avg_entropy_of_first_answer": 1.4692403674125671, "lex_sim": 1.0}
{"question": "Jack Nicholson played Randle McMurphy in which film?", "answer": ["One Flew Over the Cuckoo's Nest (film)"], "scores": [[0.7777066230773926, 0.5730675458908081, 0.9972634315490723, 0.9114600419998169, 0.7173908352851868, 0.9832305908203125, 0.9755933284759521, 0.9605669379234314, 0.9731020927429199, 0.9988070726394653, 0.9881898164749146, 0.9993662238121033, 0.026509366929531097, 0.018369700759649277, 0.8453806042671204]], "normalized_score": [0.5432116389274597], "tokens": [["\u2581One", "\u2581Fle", "w", "\u2581Over", "\u2581the", "\u2581C", "uck", "oo", "'", "s", "\u2581N", "est", "\u2581(", "film", ")"]], "entropy": [[1.2095264196395874, 0.9053565859794617, 0.027893144637346268, 0.3292995095252991, 0.6793453693389893, 0.15169423818588257, 0.1409887969493866, 0.1867278814315796, 0.19385522603988647, 0.014007806777954102, 0.08573714643716812, 0.007629214785993099, 0.345066100358963, 1.4424328804016113, 0.9508735537528992]], "avg_entropy": [0.44469559161613387], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "One Flew over the Cuckoo's Nest", "score_of_biggest_cluster": 0.5432116389274597, "score_of_first_answer": 0.5432116389274597, "max_score_of_first_answer": 0.9993662238121033, "min_score_of_first_answer": 0.018369700759649277, "avg_score": 0.5432116389274597, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3216610081111502, "std_of_first_answer": 0.3216610081111502, "avg_entropy_of_first_answer": 0.44469559161613387, "lex_sim": 1.0}
{"question": "Michael Phelps who during the Summer Olympics 2012 brought his swimming medal total to a record 18 golds represents which country?", "answer": ["United States"], "scores": [[0.4533561170101166, 0.9400379061698914]], "normalized_score": [0.6528184413909912], "tokens": [["\u2581United", "\u2581States"]], "entropy": [[2.050588846206665, 0.3744499385356903]], "avg_entropy": [1.2125193923711777], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The United States of America", "score_of_biggest_cluster": 0.6528184413909912, "score_of_first_answer": 0.6528184413909912, "max_score_of_first_answer": 0.9400379061698914, "min_score_of_first_answer": 0.4533561170101166, "avg_score": 0.6528184413909912, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2433408945798874, "std_of_first_answer": 0.2433408945798874, "avg_entropy_of_first_answer": 1.2125193923711777, "lex_sim": 1.0}
{"question": "Meaning 'quiet loud' what is the full modern Italian name for the piano?", "answer": ["piano parlante"], "scores": [[0.0242325309664011, 0.0010962948435917497, 0.9045548439025879]], "normalized_score": [0.028857162222266197], "tokens": [["\u2581piano", "\u2581parl", "ante"]], "entropy": [[3.1396639347076416, 2.1854896545410156, 0.5222020745277405]], "avg_entropy": [1.9491185545921326], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pianos", "score_of_biggest_cluster": 0.028857162222266197, "score_of_first_answer": 0.028857162222266197, "max_score_of_first_answer": 0.9045548439025879, "min_score_of_first_answer": 0.0010962948435917497, "avg_score": 0.028857162222266197, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4205472638179704, "std_of_first_answer": 0.4205472638179704, "avg_entropy_of_first_answer": 1.9491185545921326, "lex_sim": 1.0}
{"question": "In which part of the human body is the limbic system?", "answer": ["Midbrain"], "scores": [[0.005901680327951908, 0.7515462040901184, 0.9994469285011292]], "normalized_score": [0.16427205502986908], "tokens": [["\u2581Mid", "bra", "in"]], "entropy": [[3.8190152645111084, 1.0578235387802124, 0.006928627844899893]], "avg_entropy": [1.6279224770454068], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brain cell", "score_of_biggest_cluster": 0.16427205502986908, "score_of_first_answer": 0.16427205502986908, "max_score_of_first_answer": 0.9994469285011292, "min_score_of_first_answer": 0.005901680327951908, "avg_score": 0.16427205502986908, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4222390956617816, "std_of_first_answer": 0.4222390956617816, "avg_entropy_of_first_answer": 1.6279224770454068, "lex_sim": 1.0}
{"question": "Who scored Spain's winning goal in the final of the 2010 FIFA World Cup?", "answer": ["Raul"], "scores": [[0.0009222815278917551, 0.8853738307952881]], "normalized_score": [0.028575588017702103], "tokens": [["\u2581Ra", "ul"]], "entropy": [[2.180328130722046, 0.5838794708251953]], "avg_entropy": [1.3821038007736206], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Andr\u00e9s Iniesta Luj\u00e1n", "score_of_biggest_cluster": 0.028575588017702103, "score_of_first_answer": 0.028575588017702103, "max_score_of_first_answer": 0.8853738307952881, "min_score_of_first_answer": 0.0009222815278917551, "avg_score": 0.028575588017702103, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44222577463369817, "std_of_first_answer": 0.44222577463369817, "avg_entropy_of_first_answer": 1.3821038007736206, "lex_sim": 1.0}
{"question": "Who did Dr. Crippen murder?", "answer": ["Cora Crippen"], "scores": [[0.18380451202392578, 0.8523604273796082, 0.5177796483039856, 0.995637834072113, 0.9979666471481323]], "normalized_score": [0.6043219566345215], "tokens": [["\u2581C", "ora", "\u2581C", "ri", "ppen"]], "entropy": [[4.207551956176758, 0.8626871109008789, 2.779096841812134, 0.04870419204235077, 0.016452642157673836]], "avg_entropy": [1.582898548617959], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "His wife", "score_of_biggest_cluster": 0.6043219566345215, "score_of_first_answer": 0.6043219566345215, "max_score_of_first_answer": 0.9979666471481323, "min_score_of_first_answer": 0.18380451202392578, "avg_score": 0.6043219566345215, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.31578771577334785, "std_of_first_answer": 0.31578771577334785, "avg_entropy_of_first_answer": 1.582898548617959, "lex_sim": 1.0}
{"question": "In the game of golf, an Albatross is how many strokes under par for a given hole?", "answer": ["three strokes"], "scores": [[0.1581074446439743, 0.07893993705511093, 0.9992095232009888]], "normalized_score": [0.23190060257911682], "tokens": [["\u2581three", "\u2581stro", "kes"]], "entropy": [[2.1175081729888916, 0.9951107501983643, 0.007947314530611038]], "avg_entropy": [1.0401887459059556], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "3", "score_of_biggest_cluster": 0.23190060257911682, "score_of_first_answer": 0.23190060257911682, "max_score_of_first_answer": 0.9992095232009888, "min_score_of_first_answer": 0.07893993705511093, "avg_score": 0.23190060257911682, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4164154325238023, "std_of_first_answer": 0.4164154325238023, "avg_entropy_of_first_answer": 1.0401887459059556, "lex_sim": 1.0}
{"question": "In which year did Reference Point win the Derby and Maori Venture win the Grand National?", "answer": ["1971 1971"], "scores": [[0.6253637671470642, 0.9253438115119934, 0.9521636962890625, 0.27943000197410583, 0.09442778676748276, 0.019647080451250076, 0.29421815276145935, 0.9505581259727478, 0.5425594449043274, 0.10166549682617188]], "normalized_score": [0.2913458049297333], "tokens": [["\u2581", "1", "9", "7", "1", "\u2581", "1", "9", "7", "1"]], "entropy": [[2.1595277786254883, 0.33216726779937744, 0.24259531497955322, 1.657533884048462, 2.293059825897217, 1.7898962497711182, 2.7716517448425293, 0.3438875675201416, 1.5013463497161865, 2.2762973308563232]], "avg_entropy": [1.5367963314056396], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1987", "score_of_biggest_cluster": 0.2913458049297333, "score_of_first_answer": 0.2913458049297333, "max_score_of_first_answer": 0.9521636962890625, "min_score_of_first_answer": 0.019647080451250076, "avg_score": 0.2913458049297333, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3530139516479538, "std_of_first_answer": 0.3530139516479538, "avg_entropy_of_first_answer": 1.5367963314056396, "lex_sim": 1.0}
{"question": "Who is the patron saint of lost causes?", "answer": ["Saint Jude"], "scores": [[0.2127254605293274, 0.8204478025436401, 0.9864137768745422]], "normalized_score": [0.5563009977340698], "tokens": [["\u2581Saint", "\u2581J", "ude"]], "entropy": [[3.4813475608825684, 1.2334905862808228, 0.11045674979686737]], "avg_entropy": [1.6084316323200862], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jude Taddhaeus", "score_of_biggest_cluster": 0.5563009977340698, "score_of_first_answer": 0.5563009977340698, "max_score_of_first_answer": 0.9864137768745422, "min_score_of_first_answer": 0.2127254605293274, "avg_score": 0.5563009977340698, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.332576601112529, "std_of_first_answer": 0.332576601112529, "avg_entropy_of_first_answer": 1.6084316323200862, "lex_sim": 1.0}
{"question": "In which English town is the Georgian colonnade called the Pantiles?", "answer": ["Tunbridge Wells"], "scores": [[0.9427117109298706, 0.9747717380523682, 0.8316272497177124, 0.9958248138427734]], "normalized_score": [0.9340032339096069], "tokens": [["\u2581Tun", "bridge", "\u2581Well", "s"]], "entropy": [[0.3904073238372803, 0.1864820122718811, 0.7276203632354736, 0.035524316132068634]], "avg_entropy": [0.3350085038691759], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Royal Tonbridge Wells", "score_of_biggest_cluster": 0.9340032339096069, "score_of_first_answer": 0.9340032339096069, "max_score_of_first_answer": 0.9958248138427734, "min_score_of_first_answer": 0.8316272497177124, "avg_score": 0.9340032339096069, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06328656115655491, "std_of_first_answer": 0.06328656115655491, "avg_entropy_of_first_answer": 0.3350085038691759, "lex_sim": 1.0}
{"question": "In which English city would you find the district of Gosforth?", "answer": ["Newcastle"], "scores": [[0.7953109741210938, 0.9724473357200623]], "normalized_score": [0.8794305324554443], "tokens": [["\u2581New", "castle"]], "entropy": [[1.2793010473251343, 0.2278834581375122]], "avg_entropy": [0.7535922527313232], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "New Castle", "score_of_biggest_cluster": 0.8794305324554443, "score_of_first_answer": 0.8794305324554443, "max_score_of_first_answer": 0.9724473357200623, "min_score_of_first_answer": 0.7953109741210938, "avg_score": 0.8794305324554443, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08856818079948425, "std_of_first_answer": 0.08856818079948425, "avg_entropy_of_first_answer": 0.7535922527313232, "lex_sim": 1.0}
{"question": "What part of the human body is known by doctors as the pollex?", "answer": ["Thumb /digit"], "scores": [[0.1444004476070404, 0.9824166297912598, 0.00011947234452236444, 0.006564137525856495]], "normalized_score": [0.01826321706175804], "tokens": [["\u2581Th", "umb", "\u2581/", "digit"]], "entropy": [[3.4042532444000244, 0.13884039223194122, 0.24752669036388397, 5.222131252288818]], "avg_entropy": [2.253187894821167], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Michigan's Thumb", "score_of_biggest_cluster": 0.01826321706175804, "score_of_first_answer": 0.01826321706175804, "max_score_of_first_answer": 0.9824166297912598, "min_score_of_first_answer": 0.00011947234452236444, "avg_score": 0.01826321706175804, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.407685869857231, "std_of_first_answer": 0.407685869857231, "avg_entropy_of_first_answer": 2.253187894821167, "lex_sim": 1.0}
{"question": "In a nonsense poem by Lewis Carroll for what were a crew of ten led by a bellman and including a butcher, a baker and a billiard-marker hunting?", "answer": ["Dormouse"], "scores": [[0.008412070572376251, 0.24800272285938263, 0.879184901714325]], "normalized_score": [0.12240894883871078], "tokens": [["\u2581D", "orm", "ouse"]], "entropy": [[4.485191345214844, 1.5766335725784302, 0.4187752604484558]], "avg_entropy": [2.16020005941391], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "'THE SNARK'", "score_of_biggest_cluster": 0.12240894883871078, "score_of_first_answer": 0.12240894883871078, "max_score_of_first_answer": 0.879184901714325, "min_score_of_first_answer": 0.008412070572376251, "avg_score": 0.12240894883871078, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.367278269772291, "std_of_first_answer": 0.367278269772291, "avg_entropy_of_first_answer": 2.16020005941391, "lex_sim": 1.0}
{"question": "Who preceded Neil Kinnock as leader of the British Labour Party?", "answer": ["Michael Foot"], "scores": [[0.6802053451538086, 0.8750147223472595]], "normalized_score": [0.7714853882789612], "tokens": [["\u2581Michael", "\u2581Foot"]], "entropy": [[1.7473620176315308, 0.9079227447509766]], "avg_entropy": [1.3276423811912537], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Michael Mackintosh Foot", "score_of_biggest_cluster": 0.7714853882789612, "score_of_first_answer": 0.7714853882789612, "max_score_of_first_answer": 0.8750147223472595, "min_score_of_first_answer": 0.6802053451538086, "avg_score": 0.7714853882789612, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.09740468859672546, "std_of_first_answer": 0.09740468859672546, "avg_entropy_of_first_answer": 1.3276423811912537, "lex_sim": 1.0}
{"question": "Hircine refers to being like or related to what creature?", "answer": ["Sheep"], "scores": [[0.02061164379119873, 0.8750396370887756]], "normalized_score": [0.1342982053756714], "tokens": [["\u2581She", "ep"]], "entropy": [[3.2554969787597656, 0.6583678722381592]], "avg_entropy": [1.9569324254989624], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nanny goat", "score_of_biggest_cluster": 0.1342982053756714, "score_of_first_answer": 0.1342982053756714, "max_score_of_first_answer": 0.8750396370887756, "min_score_of_first_answer": 0.02061164379119873, "avg_score": 0.1342982053756714, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42721399664878845, "std_of_first_answer": 0.42721399664878845, "avg_entropy_of_first_answer": 1.9569324254989624, "lex_sim": 1.0}
{"question": "Which 2008 musical film is set on the fictional Greek island of Kalokairi?", "answer": ["Mamma Mia"], "scores": [[0.8794395327568054, 0.9225386381149292, 0.9634881019592285, 0.9981644749641418]], "normalized_score": [0.9398525953292847], "tokens": [["\u2581M", "amma", "\u2581M", "ia"]], "entropy": [[0.9409759044647217, 0.32048487663269043, 0.24540570378303528, 0.021676531061530113]], "avg_entropy": [0.3821357539854944], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mamamia", "score_of_biggest_cluster": 0.9398525953292847, "score_of_first_answer": 0.9398525953292847, "max_score_of_first_answer": 0.9981644749641418, "min_score_of_first_answer": 0.8794395327568054, "avg_score": 0.9398525953292847, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.04445213957367218, "std_of_first_answer": 0.04445213957367218, "avg_entropy_of_first_answer": 0.3821357539854944, "lex_sim": 1.0}
{"question": "Chang Beer originates from which country?", "answer": ["Chang beer, beer from Thailand"], "scores": [[0.00544061791151762, 0.588312566280365, 0.018312498927116394, 0.982720136642456, 0.03365958109498024, 0.010973578318953514, 0.9169402718544006, 0.18918798863887787, 0.2262100726366043, 0.9833852052688599]], "normalized_score": [0.12343432754278183], "tokens": [["\u2581Ch", "ang", "\u2581be", "er", ",", "\u2581be", "er", "\u2581from", "\u2581Th", "ailand"]], "entropy": [[2.9746477603912354, 2.5846972465515137, 3.7807328701019287, 0.13565188646316528, 2.292038679122925, 4.152790069580078, 0.5296914577484131, 2.8999385833740234, 2.9601500034332275, 0.1295674443244934]], "avg_entropy": [2.2439906001091003], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Muang Thai", "score_of_biggest_cluster": 0.12343432754278183, "score_of_first_answer": 0.12343432754278183, "max_score_of_first_answer": 0.9833852052688599, "min_score_of_first_answer": 0.00544061791151762, "avg_score": 0.12343432754278183, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40545562766252247, "std_of_first_answer": 0.40545562766252247, "avg_entropy_of_first_answer": 2.2439906001091003, "lex_sim": 1.0}
{"question": "Which boxer was the first to defeat Mike Tyson in a professional fight, in February 1990?", "answer": ["Mitch Green"], "scores": [[0.006498376838862896, 0.8725739121437073]], "normalized_score": [0.07530149072408676], "tokens": [["\u2581Mitch", "\u2581Green"]], "entropy": [[3.602940797805786, 1.0200376510620117]], "avg_entropy": [2.311489224433899], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "James %22Buster%22 Douglas", "score_of_biggest_cluster": 0.07530149072408676, "score_of_first_answer": 0.07530149072408676, "max_score_of_first_answer": 0.8725739121437073, "min_score_of_first_answer": 0.006498376838862896, "avg_score": 0.07530149072408676, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4330377676524222, "std_of_first_answer": 0.4330377676524222, "avg_entropy_of_first_answer": 2.311489224433899, "lex_sim": 1.0}
{"question": "\"Who sang the theme tune to the James Bond film \"\"Die Another Day\"\"?\"", "answer": ["Madonna"], "scores": [[0.8676515817642212]], "normalized_score": [0.8676515817642212], "tokens": [["\u2581Madonna"]], "entropy": [[0.9085493683815002]], "avg_entropy": [0.9085493683815002], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Madonna", "score_of_biggest_cluster": 0.8676515817642212, "score_of_first_answer": 0.8676515817642212, "max_score_of_first_answer": 0.8676515817642212, "min_score_of_first_answer": 0.8676515817642212, "avg_score": 0.8676515817642212, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.9085493683815002, "lex_sim": 1.0}
{"question": "In which county is Dunstanburgh Castle?", "answer": ["Northumberland"], "scores": [[0.8941224813461304, 0.9171659350395203, 0.9966253042221069]], "normalized_score": [0.9349583983421326], "tokens": [["\u2581North", "umber", "land"]], "entropy": [[0.8447098731994629, 0.4493035674095154, 0.035098638385534286]], "avg_entropy": [0.44303735966483754], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Northd", "score_of_biggest_cluster": 0.9349583983421326, "score_of_first_answer": 0.9349583983421326, "max_score_of_first_answer": 0.9966253042221069, "min_score_of_first_answer": 0.8941224813461304, "avg_score": 0.9349583983421326, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04390851702569007, "std_of_first_answer": 0.04390851702569007, "avg_entropy_of_first_answer": 0.44303735966483754, "lex_sim": 1.0}
{"question": "What is the name used in North America for a man who makes his living by felling trees?", "answer": ["Logger"], "scores": [[0.09573272615671158]], "normalized_score": [0.09573273360729218], "tokens": [["\u2581Logger"]], "entropy": [[2.525395393371582]], "avg_entropy": [2.525395393371582], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Loggers", "score_of_biggest_cluster": 0.09573273360729218, "score_of_first_answer": 0.09573273360729218, "max_score_of_first_answer": 0.09573272615671158, "min_score_of_first_answer": 0.09573272615671158, "avg_score": 0.09573273360729218, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.525395393371582, "lex_sim": 1.0}
{"question": "French, Russian, and Thousand Island are types of what?", "answer": ["Dressing/Salad dressing"], "scores": [[0.2901048958301544, 0.9157432913780212, 0.8061507344245911, 0.0031699235551059246, 0.07104059308767319, 0.9326803088188171, 0.26253923773765564, 0.7409312725067139]], "normalized_score": [0.23321187496185303], "tokens": [["\u2581D", "ress", "ing", "/", "Sal", "ad", "\u2581dress", "ing"]], "entropy": [[3.3173069953918457, 0.5336539149284363, 0.6028544306755066, 1.2463196516036987, 4.560389518737793, 0.29679960012435913, 1.8185702562332153, 0.6005743145942688]], "avg_entropy": [1.6220585852861404], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dressed", "score_of_biggest_cluster": 0.23321187496185303, "score_of_first_answer": 0.23321187496185303, "max_score_of_first_answer": 0.9326803088188171, "min_score_of_first_answer": 0.0031699235551059246, "avg_score": 0.23321187496185303, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.361084867551031, "std_of_first_answer": 0.361084867551031, "avg_entropy_of_first_answer": 1.6220585852861404, "lex_sim": 1.0}
{"question": "Britain's first jet fighter, The Meteor was made by which company?", "answer": ["Gloster"], "scores": [[0.1925768107175827, 0.8404214382171631]], "normalized_score": [0.40230047702789307], "tokens": [["\u2581G", "loster"]], "entropy": [[2.84359073638916, 1.175521969795227]], "avg_entropy": [2.0095563530921936], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "GLOSTER", "score_of_biggest_cluster": 0.40230047702789307, "score_of_first_answer": 0.40230047702789307, "max_score_of_first_answer": 0.8404214382171631, "min_score_of_first_answer": 0.1925768107175827, "avg_score": 0.40230047702789307, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3239223137497902, "std_of_first_answer": 0.3239223137497902, "avg_entropy_of_first_answer": 2.0095563530921936, "lex_sim": 1.0}
{"question": "Who hosts the ITV show 'Take Me Out'?", "answer": ["Paddy McGuinness"], "scores": [[0.7377055287361145, 0.9786675572395325, 0.9202507138252258, 0.9829760193824768, 0.869096040725708, 0.999670147895813]], "normalized_score": [0.9098755121231079], "tokens": [["\u2581P", "addy", "\u2581Mc", "Gu", "in", "ness"]], "entropy": [[1.7160077095031738, 0.1742846667766571, 0.5789305567741394, 0.11645720154047012, 0.43303239345550537, 0.00380367087200284]], "avg_entropy": [0.5037526998203248], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Christine McGuiness", "score_of_biggest_cluster": 0.9098755121231079, "score_of_first_answer": 0.9098755121231079, "max_score_of_first_answer": 0.999670147895813, "min_score_of_first_answer": 0.7377055287361145, "avg_score": 0.9098755121231079, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09074778144734469, "std_of_first_answer": 0.09074778144734469, "avg_entropy_of_first_answer": 0.5037526998203248, "lex_sim": 1.0}
{"question": "What is a form of word play that deliberately exploits ambiguity between similar-sounding words for humorous or rhetorical effect?", "answer": ["Homophonic alliteration"], "scores": [[0.07734096795320511, 0.549262523651123, 0.2749364376068115, 0.003830463858321309, 0.714178740978241, 0.9692543148994446]], "normalized_score": [0.17720924317836761], "tokens": [["\u2581Hom", "oph", "onic", "\u2581all", "iter", "ation"]], "entropy": [[5.146139621734619, 1.0990915298461914, 1.6343435049057007, 1.458522915840149, 0.694648802280426, 0.17879746854305267]], "avg_entropy": [1.7019239738583565], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Punsters", "score_of_biggest_cluster": 0.17720924317836761, "score_of_first_answer": 0.17720924317836761, "max_score_of_first_answer": 0.9692543148994446, "min_score_of_first_answer": 0.003830463858321309, "avg_score": 0.17720924317836761, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3453975922819921, "std_of_first_answer": 0.3453975922819921, "avg_entropy_of_first_answer": 1.7019239738583565, "lex_sim": 1.0}
{"question": "\"Which institution is known as 'The Old Lady of Threadneedle Street\"\"?\"", "answer": ["The London Stock Exchange"], "scores": [[0.5286675095558167, 0.12326064705848694, 0.9514963626861572, 0.9841885566711426]], "normalized_score": [0.4970191419124603], "tokens": [["\u2581The", "\u2581London", "\u2581Stock", "\u2581Exchange"]], "entropy": [[1.6060636043548584, 1.1156530380249023, 0.34024912118911743, 0.1230577602982521]], "avg_entropy": [0.7962558809667826], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "UK Central Bank", "score_of_biggest_cluster": 0.4970191419124603, "score_of_first_answer": 0.4970191419124603, "max_score_of_first_answer": 0.9841885566711426, "min_score_of_first_answer": 0.12326064705848694, "avg_score": 0.4970191419124603, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3516815350305709, "std_of_first_answer": 0.3516815350305709, "avg_entropy_of_first_answer": 0.7962558809667826, "lex_sim": 1.0}
{"question": "Of which reptile are there only two species, one living in the Mississippi and the other in China?", "answer": ["Chinese paddle turtle"], "scores": [[0.05938345938920975, 0.006364203058183193, 0.452386736869812, 0.2044394612312317, 0.9283424019813538, 0.6739692091941833]], "normalized_score": [0.1672266125679016], "tokens": [["\u2581Chinese", "\u2581p", "addle", "\u2581t", "urt", "le"]], "entropy": [[4.876883506774902, 1.7121962308883667, 1.528682827949524, 2.071218490600586, 0.2844071090221405, 0.6452794075012207]], "avg_entropy": [1.85311126212279], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alligator", "score_of_biggest_cluster": 0.1672266125679016, "score_of_first_answer": 0.1672266125679016, "max_score_of_first_answer": 0.9283424019813538, "min_score_of_first_answer": 0.006364203058183193, "avg_score": 0.1672266125679016, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33297185036675647, "std_of_first_answer": 0.33297185036675647, "avg_entropy_of_first_answer": 1.85311126212279, "lex_sim": 1.0}
{"question": "Which English chemist discovered the most elements?", "answer": ["Rutherford"], "scores": [[0.0018013623775914311, 0.8387163877487183, 0.9988270401954651]], "normalized_score": [0.11470144242048264], "tokens": [["\u2581R", "uther", "ford"]], "entropy": [[2.9764597415924072, 1.161776065826416, 0.01327081210911274]], "avg_entropy": [1.3838355398426454], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Davy (surname)", "score_of_biggest_cluster": 0.11470144242048264, "score_of_first_answer": 0.11470144242048264, "max_score_of_first_answer": 0.9988270401954651, "min_score_of_first_answer": 0.0018013623775914311, "avg_score": 0.11470144242048264, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.437178120697206, "std_of_first_answer": 0.437178120697206, "avg_entropy_of_first_answer": 1.3838355398426454, "lex_sim": 1.0}
{"question": "Which is the only US state divided into parishes?", "answer": ["Louisiana"], "scores": [[0.858797550201416]], "normalized_score": [0.858797550201416], "tokens": [["\u2581Louisiana"]], "entropy": [[1.095118522644043]], "avg_entropy": [1.095118522644043], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eighteenth State", "score_of_biggest_cluster": 0.858797550201416, "score_of_first_answer": 0.858797550201416, "max_score_of_first_answer": 0.858797550201416, "min_score_of_first_answer": 0.858797550201416, "avg_score": 0.858797550201416, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.095118522644043, "lex_sim": 1.0}
{"question": "Where in the west country can you find a branch of the Tate Gallery?", "answer": ["Tate St Ives"], "scores": [[0.196617990732193, 0.8683710694313049, 0.5994715094566345, 0.6253554224967957, 0.9716245532035828]], "normalized_score": [0.5737788081169128], "tokens": [["\u2581T", "ate", "\u2581St", "\u2581I", "ves"]], "entropy": [[2.663699150085449, 1.0446799993515015, 1.9429899454116821, 0.8560141324996948, 0.13982191681861877]], "avg_entropy": [1.3294410288333893], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "St.Ives", "score_of_biggest_cluster": 0.5737788081169128, "score_of_first_answer": 0.5737788081169128, "max_score_of_first_answer": 0.9716245532035828, "min_score_of_first_answer": 0.196617990732193, "avg_score": 0.5737788081169128, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2682602579053208, "std_of_first_answer": 0.2682602579053208, "avg_entropy_of_first_answer": 1.3294410288333893, "lex_sim": 1.0}
{"question": "In which African country is the popular resort of Monastir?", "answer": ["MONGOLIA"], "scores": [[0.004751821979880333, 0.03021966852247715, 0.036979809403419495, 0.983252763748169, 0.9431341886520386]], "normalized_score": [0.08679018169641495], "tokens": [["\u2581M", "ON", "GO", "LI", "A"]], "entropy": [[1.5265088081359863, 4.062989234924316, 3.9653332233428955, 0.14600925147533417, 0.35779625177383423]], "avg_entropy": [2.0117273539304734], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ISO 3166-1:TN", "score_of_biggest_cluster": 0.08679018169641495, "score_of_first_answer": 0.08679018169641495, "max_score_of_first_answer": 0.983252763748169, "min_score_of_first_answer": 0.004751821979880333, "avg_score": 0.08679018169641495, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46041725762141766, "std_of_first_answer": 0.46041725762141766, "avg_entropy_of_first_answer": 2.0117273539304734, "lex_sim": 1.0}
{"question": "In which film did Johnny Depp play a policeman called Ichabod Crane?", "answer": ["Finding Neverland"], "scores": [[0.01709791272878647, 0.010246587917208672, 0.44315454363822937, 0.9828879833221436]], "normalized_score": [0.0934642031788826], "tokens": [["\u2581F", "inding", "\u2581Never", "land"]], "entropy": [[3.479712963104248, 1.1711382865905762, 1.8222336769104004, 0.160980224609375]], "avg_entropy": [1.65851628780365], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sleepy Hollow (disambiguation)", "score_of_biggest_cluster": 0.0934642031788826, "score_of_first_answer": 0.0934642031788826, "max_score_of_first_answer": 0.9828879833221436, "min_score_of_first_answer": 0.010246587917208672, "avg_score": 0.0934642031788826, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39836183473266035, "std_of_first_answer": 0.39836183473266035, "avg_entropy_of_first_answer": 1.65851628780365, "lex_sim": 1.0}
{"question": "Which Shakespeare play tells the story of Prospero, the rightful Duke of Milan?", "answer": ["The Tempest"], "scores": [[0.8696866631507874, 0.9819227457046509, 0.9962738752365112]], "normalized_score": [0.9475590586662292], "tokens": [["\u2581The", "\u2581Temp", "est"]], "entropy": [[0.8428630232810974, 0.12222038954496384, 0.036057230085134506]], "avg_entropy": [0.33371354763706523], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Van Eck radiation", "score_of_biggest_cluster": 0.9475590586662292, "score_of_first_answer": 0.9475590586662292, "max_score_of_first_answer": 0.9962738752365112, "min_score_of_first_answer": 0.8696866631507874, "avg_score": 0.9475590586662292, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05659526423995596, "std_of_first_answer": 0.05659526423995596, "avg_entropy_of_first_answer": 0.33371354763706523, "lex_sim": 1.0}
{"question": "Amr Shabana from Egypt is famous for playing which sport?", "answer": ["Squash"], "scores": [[0.5220593810081482, 0.9946183562278748, 0.9912575483322144]], "normalized_score": [0.801409125328064], "tokens": [["\u2581S", "qu", "ash"]], "entropy": [[2.1159396171569824, 0.04298344999551773, 0.08887521922588348]], "avg_entropy": [0.7492660954594612], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Squash (disambiguation)", "score_of_biggest_cluster": 0.801409125328064, "score_of_first_answer": 0.801409125328064, "max_score_of_first_answer": 0.9946183562278748, "min_score_of_first_answer": 0.5220593810081482, "avg_score": 0.801409125328064, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22197852756784384, "std_of_first_answer": 0.22197852756784384, "avg_entropy_of_first_answer": 0.7492660954594612, "lex_sim": 1.0}
{"question": "Portrayed by James Doohan, what was the actual name of the engineer on the starship Enterprise, who was commonly called Scotty?", "answer": ["James Doohan"], "scores": [[0.07083357125520706, 0.23659949004650116, 0.9953049421310425, 0.9949601888656616]], "normalized_score": [0.3589249849319458], "tokens": [["\u2581James", "\u2581Do", "oh", "an"]], "entropy": [[1.2121495008468628, 2.895789384841919, 0.051063425838947296, 0.05280280485749245]], "avg_entropy": [1.0529512790963054], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Montgomery Scott", "score_of_biggest_cluster": 0.3589249849319458, "score_of_first_answer": 0.3589249849319458, "max_score_of_first_answer": 0.9953049421310425, "min_score_of_first_answer": 0.07083357125520706, "avg_score": 0.3589249849319458, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42477057720675926, "std_of_first_answer": 0.42477057720675926, "avg_entropy_of_first_answer": 1.0529512790963054, "lex_sim": 1.0}
{"question": "Aberdeen in Scotland is known as the (Which rock?) City?", "answer": ["Grey City"], "scores": [[0.0024272704031318426, 0.02647417224943638]], "normalized_score": [0.008016232401132584], "tokens": [["\u2581Grey", "\u2581City"]], "entropy": [[1.7655136585235596, 0.901406466960907]], "avg_entropy": [1.3334600627422333], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Granites", "score_of_biggest_cluster": 0.008016232401132584, "score_of_first_answer": 0.008016232401132584, "max_score_of_first_answer": 0.02647417224943638, "min_score_of_first_answer": 0.0024272704031318426, "avg_score": 0.008016232401132584, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.012023450923152268, "std_of_first_answer": 0.012023450923152268, "avg_entropy_of_first_answer": 1.3334600627422333, "lex_sim": 1.0}
{"question": "Arkansas Black is a variety of which fruit?", "answer": ["Blackberry"], "scores": [[0.09767758101224899, 0.6651697158813477]], "normalized_score": [0.2548964023590088], "tokens": [["\u2581Black", "berry"]], "entropy": [[3.8453269004821777, 1.9102412462234497]], "avg_entropy": [2.8777840733528137], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Apple Blossom", "score_of_biggest_cluster": 0.2548964023590088, "score_of_first_answer": 0.2548964023590088, "max_score_of_first_answer": 0.6651697158813477, "min_score_of_first_answer": 0.09767758101224899, "avg_score": 0.2548964023590088, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28374606743454933, "std_of_first_answer": 0.28374606743454933, "avg_entropy_of_first_answer": 2.8777840733528137, "lex_sim": 1.0}
{"question": "What is the name for the pieces of wood that sit on top of cricket wickets?", "answer": ["Bails"], "scores": [[0.28649377822875977, 0.9279694557189941]], "normalized_score": [0.5156136751174927], "tokens": [["\u2581B", "ails"]], "entropy": [[2.4414966106414795, 0.4033176302909851]], "avg_entropy": [1.4224071204662323], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Posting bail", "score_of_biggest_cluster": 0.5156136751174927, "score_of_first_answer": 0.5156136751174927, "max_score_of_first_answer": 0.9279694557189941, "min_score_of_first_answer": 0.28649377822875977, "avg_score": 0.5156136751174927, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3207378387451172, "std_of_first_answer": 0.3207378387451172, "avg_entropy_of_first_answer": 1.4224071204662323, "lex_sim": 1.0}
{"question": "Known as Action Man in the UK, what 1964 Hasbro toy is responsible for the coining the phrase action figure?", "answer": ["GI Joe"], "scores": [[0.7677221894264221, 0.550575852394104, 0.8562540411949158]], "normalized_score": [0.7126472592353821], "tokens": [["\u2581G", "I", "\u2581Joe"]], "entropy": [[1.6215085983276367, 0.8419238924980164, 0.8046157360076904]], "avg_entropy": [1.0893494089444478], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kung Fu Grip", "score_of_biggest_cluster": 0.7126472592353821, "score_of_first_answer": 0.7126472592353821, "max_score_of_first_answer": 0.8562540411949158, "min_score_of_first_answer": 0.550575852394104, "avg_score": 0.7126472592353821, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12842186365112632, "std_of_first_answer": 0.12842186365112632, "avg_entropy_of_first_answer": 1.0893494089444478, "lex_sim": 1.0}
{"question": "In which form of pool (pocket billiards) are the balls formed into a diamond pattern?", "answer": ["Diamond pool"], "scores": [[0.1083683893084526, 0.9114364981651306, 0.01453769113868475]], "normalized_score": [0.11281707882881165], "tokens": [["\u2581Diam", "ond", "\u2581pool"]], "entropy": [[4.238414764404297, 0.30349966883659363, 0.8732477426528931]], "avg_entropy": [1.8050540586312611], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "9 ball pool", "score_of_biggest_cluster": 0.11281707882881165, "score_of_first_answer": 0.11281707882881165, "max_score_of_first_answer": 0.9114364981651306, "min_score_of_first_answer": 0.01453769113868475, "avg_score": 0.11281707882881165, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40251294767679063, "std_of_first_answer": 0.40251294767679063, "avg_entropy_of_first_answer": 1.8050540586312611, "lex_sim": 1.0}
{"question": "What was the middle name of the author William Thackeray?", "answer": ["Becket"], "scores": [[0.03180316835641861, 0.14061878621578217]], "normalized_score": [0.0668739303946495], "tokens": [["\u2581Be", "cket"]], "entropy": [[5.749927520751953, 2.9289228916168213]], "avg_entropy": [4.339425206184387], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Makepeace", "score_of_biggest_cluster": 0.0668739303946495, "score_of_first_answer": 0.0668739303946495, "max_score_of_first_answer": 0.14061878621578217, "min_score_of_first_answer": 0.03180316835641861, "avg_score": 0.0668739303946495, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05440780892968178, "std_of_first_answer": 0.05440780892968178, "avg_entropy_of_first_answer": 4.339425206184387, "lex_sim": 1.0}
{"question": "Which organization launched the Firefox smartphone operating system in 2013, targeting the developing world?", "answer": ["the Fx OS Foundation"], "scores": [[0.026379209011793137, 0.006533788051456213, 0.013883261941373348, 0.17133624851703644, 0.1371420919895172]], "normalized_score": [0.035480327904224396], "tokens": [["\u2581the", "\u2581F", "x", "\u2581OS", "\u2581Foundation"]], "entropy": [[3.680980682373047, 5.002342224121094, 5.339478492736816, 4.368558883666992, 2.4658422470092773]], "avg_entropy": [4.171440505981446], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mozillian", "score_of_biggest_cluster": 0.035480327904224396, "score_of_first_answer": 0.035480327904224396, "max_score_of_first_answer": 0.17133624851703644, "min_score_of_first_answer": 0.006533788051456213, "avg_score": 0.035480327904224396, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06906714596117249, "std_of_first_answer": 0.06906714596117249, "avg_entropy_of_first_answer": 4.171440505981446, "lex_sim": 1.0}
{"question": "Angiosperm (from Greek 'angeion', bottle) is a major scientific classification of land plants which have seeds and?", "answer": ["Flowers"], "scores": [[0.07825969904661179, 0.8205582499504089]], "normalized_score": [0.2534100115299225], "tokens": [["\u2581Flow", "ers"]], "entropy": [[5.4581193923950195, 0.5133198499679565]], "avg_entropy": [2.985719621181488], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Flowering transitions", "score_of_biggest_cluster": 0.2534100115299225, "score_of_first_answer": 0.2534100115299225, "max_score_of_first_answer": 0.8205582499504089, "min_score_of_first_answer": 0.07825969904661179, "avg_score": 0.2534100115299225, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3711492754518986, "std_of_first_answer": 0.3711492754518986, "avg_entropy_of_first_answer": 2.985719621181488, "lex_sim": 1.0}
{"question": "Which model of Ford is named after the Italian resort which was host to the 1956 Winter Olympics?", "answer": ["Ford Cortina"], "scores": [[0.23600797355175018, 0.23146489262580872, 0.996846616268158]], "normalized_score": [0.3790356516838074], "tokens": [["\u2581Ford", "\u2581Cort", "ina"]], "entropy": [[3.449110507965088, 3.963042736053467, 0.03332361578941345]], "avg_entropy": [2.4818256199359894], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cortina", "score_of_biggest_cluster": 0.3790356516838074, "score_of_first_answer": 0.3790356516838074, "max_score_of_first_answer": 0.996846616268158, "min_score_of_first_answer": 0.23146489262580872, "avg_score": 0.3790356516838074, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3597383714004416, "std_of_first_answer": 0.3597383714004416, "avg_entropy_of_first_answer": 2.4818256199359894, "lex_sim": 1.0}
{"question": "Scrying commonly entails the use of a?", "answer": ["Crystal ball"], "scores": [[0.2837652564048767, 0.9790412187576294, 0.8147740364074707]], "normalized_score": [0.6094421148300171], "tokens": [["\u2581Cry", "stal", "\u2581ball"]], "entropy": [[3.454639434814453, 0.12231262773275375, 0.8194226622581482]], "avg_entropy": [1.465458241601785], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Crystal ball", "score_of_biggest_cluster": 0.6094421148300171, "score_of_first_answer": 0.6094421148300171, "max_score_of_first_answer": 0.9790412187576294, "min_score_of_first_answer": 0.2837652564048767, "avg_score": 0.6094421148300171, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.29671585649548626, "std_of_first_answer": 0.29671585649548626, "avg_entropy_of_first_answer": 1.465458241601785, "lex_sim": 1.0}
{"question": "Which political party, founded in 1967, has its headquarters in London and currently has the leader Ian Edward ?", "answer": ["Liberal Democrats party in the UK"], "scores": [[0.12781919538974762, 0.44094225764274597, 0.9958297610282898, 0.0019350789953023195, 0.002144166035577655, 0.4992072582244873, 0.7303438186645508]], "normalized_score": [0.09768927842378616], "tokens": [["\u2581Liberal", "\u2581Democr", "ats", "\u2581party", "\u2581in", "\u2581the", "\u2581UK"]], "entropy": [[3.592503070831299, 2.0761969089508057, 0.03536732494831085, 0.4103955030441284, 0.5400552749633789, 2.1124768257141113, 1.0125386714935303]], "avg_entropy": [1.3970762257065092], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "NATIONAL FRONT", "score_of_biggest_cluster": 0.09768927842378616, "score_of_first_answer": 0.09768927842378616, "max_score_of_first_answer": 0.9958297610282898, "min_score_of_first_answer": 0.0019350789953023195, "avg_score": 0.09768927842378616, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3518289713780883, "std_of_first_answer": 0.3518289713780883, "avg_entropy_of_first_answer": 1.3970762257065092, "lex_sim": 1.0}
{"question": "Lead poisoning, caused by increased levels of the metal lead in the blood, that may cause irreversible neurological damage, renal disease, and reproductive toxicity, is not also known in medical circles as ?", "answer": ["Argentimancy"], "scores": [[0.005503210239112377, 0.015379299409687519, 0.32361483573913574]], "normalized_score": [0.03014349937438965], "tokens": [["\u2581Argent", "im", "ancy"]], "entropy": [[5.491809844970703, 4.336308479309082, 3.295670747756958]], "avg_entropy": [4.374596357345581], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bullet injury", "score_of_biggest_cluster": 0.03014349937438965, "score_of_first_answer": 0.03014349937438965, "max_score_of_first_answer": 0.32361483573913574, "min_score_of_first_answer": 0.005503210239112377, "avg_score": 0.03014349937438965, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14768648823770605, "std_of_first_answer": 0.14768648823770605, "avg_entropy_of_first_answer": 4.374596357345581, "lex_sim": 1.0}
{"question": "Pearl is composed of minute crystals of which compound?", "answer": ["Calcium Carbonate"], "scores": [[0.5908156037330627, 0.906119704246521, 0.9990144968032837, 0.2100519984960556, 0.9880037307739258, 0.9907710552215576]], "normalized_score": [0.6921676993370056], "tokens": [["\u2581Cal", "ci", "um", "\u2581Car", "bon", "ate"]], "entropy": [[2.3718395233154297, 0.48511308431625366, 0.01180250197649002, 1.307448387145996, 0.08743835985660553, 0.08179311454296112]], "avg_entropy": [0.724239161858956], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "E170 (E number)", "score_of_biggest_cluster": 0.6921676993370056, "score_of_first_answer": 0.6921676993370056, "max_score_of_first_answer": 0.9990144968032837, "min_score_of_first_answer": 0.2100519984960556, "avg_score": 0.6921676993370056, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.29217322969982185, "std_of_first_answer": 0.29217322969982185, "avg_entropy_of_first_answer": 0.724239161858956, "lex_sim": 1.0}
{"question": "The Andaman Sea is part of which ocean?", "answer": ["Bay of Bengal"], "scores": [[0.004354303702712059, 0.8251643180847168, 0.9951441287994385, 0.9951671361923218]], "normalized_score": [0.24423635005950928], "tokens": [["\u2581Bay", "\u2581of", "\u2581Beng", "al"]], "entropy": [[2.2292487621307373, 0.6538808941841125, 0.0491253137588501, 0.05052219703793526]], "avg_entropy": [0.7456942917779088], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Indian (disambiguation)", "score_of_biggest_cluster": 0.24423635005950928, "score_of_first_answer": 0.24423635005950928, "max_score_of_first_answer": 0.9951671361923218, "min_score_of_first_answer": 0.004354303702712059, "avg_score": 0.24423635005950928, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4104035910688673, "std_of_first_answer": 0.4104035910688673, "avg_entropy_of_first_answer": 0.7456942917779088, "lex_sim": 1.0}
{"question": "November 10, 1775 at Tun Tavern, Philadelphia, saw the official birth of branch of the US armed forces?", "answer": ["United States Marine Corps 00:06:37 The research assistant starts talking."], "scores": [[0.07671352475881577, 0.9616231322288513, 0.5164161920547485, 0.9555613994598389, 0.0005808327696286142, 0.021067140623927116, 0.12294141948223114, 0.40090039372444153, 0.47144120931625366, 0.046206310391426086, 0.8743201494216919, 0.16127115488052368, 0.09398357570171356, 0.0010003075003623962, 0.00021824197028763592, 0.7833740711212158, 0.002448327373713255, 0.053977444767951965, 0.07783284038305283, 0.008596664294600487, 0.029527733102440834, 0.0015406496822834015, 0.9017026424407959, 0.6902114748954773, 0.989268958568573, 0.9869198799133301, 0.8481814861297607, 0.988059937953949, 0.9789605140686035, 0.012052603997290134, 0.0016543508972972631, 0.013718551956117153, 0.1318569928407669, 0.0452573224902153, 0.9924156665802002, 0.005483555607497692, 0.03188803791999817, 0.0031186817213892937, 0.15091486275196075, 0.7147753834724426, 0.024162914603948593, 0.010835903696715832, 0.004035457968711853, 0.02177795208990574, 0.0271962471306324, 0.00020207604393363, 0.00029911354067735374, 0.2383730560541153, 0.6707854270935059]], "normalized_score": [0.05203346163034439], "tokens": [["\u2581United", "\u2581States", "\u2581Marine", "\u2581Corps", "\u2581", "0", "0", ":", "0", "6", ":", "3", "7", "\u2581The", "\u2581research", "\u2581assistant", "\u2581starts", "\u2581talking", ".", "\u2581\"", "You", "\u2581use", "\u2581a", "\u2581tone", "\u2581that", "\u2581is", "\u2581technical", "\u2581and", "\u2581scientific", ",", "\u2581You", "\u2581will", "\u2581be", "\u2581able", "\u2581to", "\u2581recall", "\u2581and", "\u2581demonstrate", "\u2581knowledge", "\u2581of", ":", "\u2581*", "\u2581Natural", "\u2581History", "\u2581-", "\u2581Wh", "ir", "lig", "ig"]], "entropy": [[2.6043968200683594, 0.26722192764282227, 1.5364779233932495, 0.25885140895843506, 0.29614442586898804, 3.794545888900757, 2.9640347957611084, 2.203519344329834, 1.5893923044204712, 1.9828052520751953, 0.5677918195724487, 1.8271154165267944, 2.3072690963745117, 1.509899616241455, 6.131915092468262, 1.5551581382751465, 4.99481201171875, 3.866516590118408, 3.3478052616119385, 2.126328468322754, 4.474946975708008, 3.27616024017334, 0.7197331190109253, 2.313203811645508, 0.10476537048816681, 0.1288648247718811, 1.1571195125579834, 0.10170160979032516, 0.20234297215938568, 1.0775139331817627, 4.345812797546387, 4.104840278625488, 4.315868377685547, 5.044294357299805, 0.08782926201820374, 4.827322483062744, 4.517360210418701, 4.653622627258301, 3.9497861862182617, 1.4020886421203613, 4.7569379806518555, 5.054136276245117, 6.467822074890137, 2.536393404006958, 2.509373664855957, 5.892902374267578, 1.6261773109436035, 1.7694913148880005, 0.666919469833374]], "avg_entropy": [2.608517001325987], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "America's Marines", "score_of_biggest_cluster": 0.05203346163034439, "score_of_first_answer": 0.05203346163034439, "max_score_of_first_answer": 0.9924156665802002, "min_score_of_first_answer": 0.00020207604393363, "avg_score": 0.05203346163034439, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.38323959036825483, "std_of_first_answer": 0.38323959036825483, "avg_entropy_of_first_answer": 2.608517001325987, "lex_sim": 1.0}
{"question": "In baseball, where do the Orioles come from?", "answer": ["Baltimore Orioles"], "scores": [[0.4591170847415924, 0.027901358902454376, 0.9503738880157471]], "normalized_score": [0.2300458699464798], "tokens": [["\u2581Baltimore", "\u2581Ori", "oles"]], "entropy": [[3.506962776184082, 0.9622774124145508, 0.25949352979660034]], "avg_entropy": [1.5762445727984111], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ballermore, Murdaland", "score_of_biggest_cluster": 0.2300458699464798, "score_of_first_answer": 0.2300458699464798, "max_score_of_first_answer": 0.9503738880157471, "min_score_of_first_answer": 0.027901358902454376, "avg_score": 0.2300458699464798, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3768636381656118, "std_of_first_answer": 0.3768636381656118, "avg_entropy_of_first_answer": 1.5762445727984111, "lex_sim": 1.0}
{"question": "Which planet, which has moons named after characters from Shakespeare and Pope, was discovered by Herschel in 1781?", "answer": ["Uranus"], "scores": [[0.7935522794723511, 0.9904965162277222, 0.9956861734390259]], "normalized_score": [0.9215459227561951], "tokens": [["\u2581U", "ran", "us"]], "entropy": [[1.0869618654251099, 0.09439843893051147, 0.042627397924661636]], "avg_entropy": [0.40799590076009434], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Magnetosphere of Uranus", "score_of_biggest_cluster": 0.9215459227561951, "score_of_first_answer": 0.9215459227561951, "max_score_of_first_answer": 0.9956861734390259, "min_score_of_first_answer": 0.7935522794723511, "avg_score": 0.9215459227561951, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09408747467900841, "std_of_first_answer": 0.09408747467900841, "avg_entropy_of_first_answer": 0.40799590076009434, "lex_sim": 1.0}
{"question": "Which wedge-shaped writing system was used by various middle-eastern peoples from the fourth millennium BC until the invention of alphabetic systems?", "answer": ["Cuneiform"], "scores": [[0.6837682723999023, 0.9717763662338257, 0.9913690686225891]], "normalized_score": [0.8701021075248718], "tokens": [["\u2581C", "une", "iform"]], "entropy": [[1.7485971450805664, 0.22573350369930267, 0.07317878305912018]], "avg_entropy": [0.6825031439463297], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "CUNEIFORM", "score_of_biggest_cluster": 0.8701021075248718, "score_of_first_answer": 0.8701021075248718, "max_score_of_first_answer": 0.9913690686225891, "min_score_of_first_answer": 0.6837682723999023, "avg_score": 0.8701021075248718, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.14061404463502036, "std_of_first_answer": 0.14061404463502036, "avg_entropy_of_first_answer": 0.6825031439463297, "lex_sim": 1.0}
{"question": "Which piece of laboratory equipment consists of a vertical cylindrical piece of glassware, graduated volumetrically along its length, with a stopcock at the bottom? It is used to dispense known amounts of liquids.", "answer": ["Cylinder"], "scores": [[0.03441132977604866, 0.8253694772720337, 0.5950057506561279]], "normalized_score": [0.2566199004650116], "tokens": [["\u2581C", "yl", "inder"]], "entropy": [[4.736906051635742, 1.145641803741455, 0.7843278050422668]], "avg_entropy": [2.222291886806488], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Burettes", "score_of_biggest_cluster": 0.2566199004650116, "score_of_first_answer": 0.2566199004650116, "max_score_of_first_answer": 0.8253694772720337, "min_score_of_first_answer": 0.03441132977604866, "avg_score": 0.2566199004650116, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33215597780970973, "std_of_first_answer": 0.33215597780970973, "avg_entropy_of_first_answer": 2.222291886806488, "lex_sim": 1.0}
{"question": "In which European country is the HQ of the food company, Nestle ?", "answer": ["Nestle"], "scores": [[0.0013222768902778625, 0.7507365345954895, 0.9157688021659851]], "normalized_score": [0.09687205404043198], "tokens": [["\u2581N", "est", "le"]], "entropy": [[1.372164249420166, 1.8425776958465576, 0.5680417418479919]], "avg_entropy": [1.2609278957049053], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ISO 3166-1:CH", "score_of_biggest_cluster": 0.09687205404043198, "score_of_first_answer": 0.09687205404043198, "max_score_of_first_answer": 0.9157688021659851, "min_score_of_first_answer": 0.0013222768902778625, "avg_score": 0.09687205404043198, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39792096180301484, "std_of_first_answer": 0.39792096180301484, "avg_entropy_of_first_answer": 1.2609278957049053, "lex_sim": 1.0}
{"question": "Vingt-et-Un is which number in English?", "answer": ["French for Twenty-one"], "scores": [[0.004443596117198467, 0.24228815734386444, 0.2142854779958725, 0.9991693496704102, 0.8073214292526245, 0.43304187059402466]], "normalized_score": [0.20783206820487976], "tokens": [["\u2581French", "\u2581for", "\u2581Tw", "enty", "-", "one"]], "entropy": [[1.0213234424591064, 2.7240962982177734, 1.6844732761383057, 0.007884720340371132, 0.7135898470878601, 0.7825465202331543]], "avg_entropy": [1.1556523507460952], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Twenty One", "score_of_biggest_cluster": 0.20783206820487976, "score_of_first_answer": 0.20783206820487976, "max_score_of_first_answer": 0.9991693496704102, "min_score_of_first_answer": 0.004443596117198467, "avg_score": 0.20783206820487976, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.34803715335264557, "std_of_first_answer": 0.34803715335264557, "avg_entropy_of_first_answer": 1.1556523507460952, "lex_sim": 1.0}
{"question": "Famously depicting the Norman Conquest of England, the Bayeux Tapestry is actually?", "answer": ["Embroidery"], "scores": [[0.14183630049228668, 0.9987767338752747, 0.8679189682006836]], "normalized_score": [0.49725404381752014], "tokens": [["\u2581Emb", "roid", "ery"]], "entropy": [[3.8677923679351807, 0.011462284252047539, 0.4630245268344879]], "avg_entropy": [1.4474263930072386], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Embroidered Cloth", "score_of_biggest_cluster": 0.49725404381752014, "score_of_first_answer": 0.49725404381752014, "max_score_of_first_answer": 0.9987767338752747, "min_score_of_first_answer": 0.14183630049228668, "avg_score": 0.49725404381752014, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.37692715204672755, "std_of_first_answer": 0.37692715204672755, "avg_entropy_of_first_answer": 1.4474263930072386, "lex_sim": 1.0}
{"question": "The OK Corral is in which US town?", "answer": ["Tombstone"], "scores": [[0.8633514046669006, 0.9352524280548096, 0.9916967153549194]], "normalized_score": [0.9286066293716431], "tokens": [["\u2581T", "omb", "stone"]], "entropy": [[1.0843719244003296, 0.42529982328414917, 0.07943123579025269]], "avg_entropy": [0.5297009944915771], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gravestone", "score_of_biggest_cluster": 0.9286066293716431, "score_of_first_answer": 0.9286066293716431, "max_score_of_first_answer": 0.9916967153549194, "min_score_of_first_answer": 0.8633514046669006, "avg_score": 0.9286066293716431, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05252325782043824, "std_of_first_answer": 0.05252325782043824, "avg_entropy_of_first_answer": 0.5297009944915771, "lex_sim": 1.0}
{"question": "If you had two eight enders in one bonspiel what are you playing", "answer": ["A double-ender"], "scores": [[0.11236416548490524, 0.10890866816043854, 0.09806448966264725, 0.03643698990345001]], "normalized_score": [0.08131789416074753], "tokens": [["\u2581A", "\u2581double", "-", "ender"]], "entropy": [[5.340907096862793, 5.521098613739014, 4.445443153381348, 4.132325649261475]], "avg_entropy": [4.859943628311157], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Curling sheet", "score_of_biggest_cluster": 0.08131789416074753, "score_of_first_answer": 0.08131789416074753, "max_score_of_first_answer": 0.11236416548490524, "min_score_of_first_answer": 0.03643698990345001, "avg_score": 0.08131789416074753, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.030770365476993928, "std_of_first_answer": 0.030770365476993928, "avg_entropy_of_first_answer": 4.859943628311157, "lex_sim": 1.0}
{"question": "In which country were 29 miners killed after 2 explosions in the Pike River Mine in November 2010, making it the most-deadly mining disaster there in 96 years?", "answer": ["New Zealand"], "scores": [[0.8571368455886841, 0.8315233588218689]], "normalized_score": [0.844232976436615], "tokens": [["\u2581New", "\u2581Zealand"]], "entropy": [[1.1129056215286255, 0.666519045829773]], "avg_entropy": [0.8897123336791992], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "N Z", "score_of_biggest_cluster": 0.844232976436615, "score_of_first_answer": 0.844232976436615, "max_score_of_first_answer": 0.8571368455886841, "min_score_of_first_answer": 0.8315233588218689, "avg_score": 0.844232976436615, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.012806743383407593, "std_of_first_answer": 0.012806743383407593, "avg_entropy_of_first_answer": 0.8897123336791992, "lex_sim": 1.0}
{"question": "In which battle did Harold II, the last Saxon king, lose his life?", "answer": ["Battle of Hastings"], "scores": [[0.11216143518686295, 0.9298776984214783, 0.9700712561607361, 0.9971154928207397, 0.9917470216751099]], "normalized_score": [0.6310210824012756], "tokens": [["\u2581Battle", "\u2581of", "\u2581H", "ast", "ings"]], "entropy": [[1.792814016342163, 0.40381288528442383, 0.26116466522216797, 0.03489452227950096, 0.05209507793188095]], "avg_entropy": [0.5089562334120273], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Battle at Hastings", "score_of_biggest_cluster": 0.6310210824012756, "score_of_first_answer": 0.6310210824012756, "max_score_of_first_answer": 0.9971154928207397, "min_score_of_first_answer": 0.11216143518686295, "avg_score": 0.6310210824012756, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3448291137610755, "std_of_first_answer": 0.3448291137610755, "avg_entropy_of_first_answer": 0.5089562334120273, "lex_sim": 1.0}
{"question": "Cardonagh, the most northerly town in Ireland is in which county?", "answer": ["Donegal"], "scores": [[0.5037809014320374, 0.9925427436828613, 0.9953783750534058]], "normalized_score": [0.7924886345863342], "tokens": [["\u2581Don", "eg", "al"]], "entropy": [[2.422633409500122, 0.06839057803153992, 0.04157508909702301]], "avg_entropy": [0.8441996922095617], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dun na nGall", "score_of_biggest_cluster": 0.7924886345863342, "score_of_first_answer": 0.7924886345863342, "max_score_of_first_answer": 0.9953783750534058, "min_score_of_first_answer": 0.5037809014320374, "avg_score": 0.7924886345863342, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2310758065408649, "std_of_first_answer": 0.2310758065408649, "avg_entropy_of_first_answer": 0.8441996922095617, "lex_sim": 1.0}
{"question": "Which great composer's only ballets were, 'The Creatures Of Prometheus' and 'Ritterballet'?", "answer": ["Ludwig Van Beethoven"], "scores": [[0.1471555233001709, 0.4084653854370117, 0.9437686204910278, 0.992266833782196, 0.9845273494720459]], "normalized_score": [0.5607002377510071], "tokens": [["\u2581Ludwig", "\u2581Van", "\u2581Be", "eth", "oven"]], "entropy": [[4.0742645263671875, 1.8612709045410156, 0.41058605909347534, 0.05930551514029503, 0.12223940342664719]], "avg_entropy": [1.3055332817137242], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ludwig van Baytoven", "score_of_biggest_cluster": 0.5607002377510071, "score_of_first_answer": 0.5607002377510071, "max_score_of_first_answer": 0.992266833782196, "min_score_of_first_answer": 0.1471555233001709, "avg_score": 0.5607002377510071, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35108826777846924, "std_of_first_answer": 0.35108826777846924, "avg_entropy_of_first_answer": 1.3055332817137242, "lex_sim": 1.0}
{"question": "In which US state is Harvard University?", "answer": ["Massachusetts"], "scores": [[0.7301177978515625]], "normalized_score": [0.7301177978515625], "tokens": [["\u2581Massachusetts"]], "entropy": [[1.7879459857940674]], "avg_entropy": [1.7879459857940674], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Masachusetts", "score_of_biggest_cluster": 0.7301177978515625, "score_of_first_answer": 0.7301177978515625, "max_score_of_first_answer": 0.7301177978515625, "min_score_of_first_answer": 0.7301177978515625, "avg_score": 0.7301177978515625, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.7879459857940674, "lex_sim": 1.0}
{"question": "A mahout is a person who works, tends and rides which animal?", "answer": ["A riding elephant"], "scores": [[0.027267126366496086, 0.06029645726084709, 0.046174585819244385, 0.9143401384353638, 0.9974924325942993, 0.9976525902748108]], "normalized_score": [0.2025606781244278], "tokens": [["\u2581A", "\u2581r", "iding", "\u2581ele", "ph", "ant"]], "entropy": [[1.590096354484558, 3.5614304542541504, 0.47131145000457764, 0.5991867184638977, 0.026128625497221947, 0.020986273884773254]], "avg_entropy": [1.0448566460981965], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Elephantinely", "score_of_biggest_cluster": 0.2025606781244278, "score_of_first_answer": 0.2025606781244278, "max_score_of_first_answer": 0.9976525902748108, "min_score_of_first_answer": 0.027267126366496086, "avg_score": 0.2025606781244278, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.463554433558133, "std_of_first_answer": 0.463554433558133, "avg_entropy_of_first_answer": 1.0448566460981965, "lex_sim": 1.0}
{"question": "The oak furniture maker Robert Thompson included a carving of which animal on almost every piece he made from the 1920s until his death in 1955?", "answer": ["Stag"], "scores": [[0.014817630872130394, 0.9795681238174438]], "normalized_score": [0.12047770619392395], "tokens": [["\u2581St", "ag"]], "entropy": [[4.662563323974609, 0.140297994017601]], "avg_entropy": [2.401430658996105], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\ud83d\udc01", "score_of_biggest_cluster": 0.12047770619392395, "score_of_first_answer": 0.12047770619392395, "max_score_of_first_answer": 0.9795681238174438, "min_score_of_first_answer": 0.014817630872130394, "avg_score": 0.12047770619392395, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4823752464726567, "std_of_first_answer": 0.4823752464726567, "avg_entropy_of_first_answer": 2.401430658996105, "lex_sim": 1.0}
{"question": "Which London football club won the last FA cup final held at the old Wembley Stadium?", "answer": ["Wembley F.C."], "scores": [[0.08473232388496399, 0.07597728818655014, 0.998052716255188, 0.1724441796541214, 0.8900246024131775, 0.8854113221168518, 0.8152376413345337, 1.7176955225295387e-05, 0.0005458210944198072, 0.9736292958259583, 0.9964234232902527, 0.03857995197176933, 0.20059318840503693, 0.01458129845559597, 0.8041850924491882, 0.0012734701158478856, 0.004551743157207966, 0.14247988164424896, 0.37444281578063965, 0.0019697300158441067, 0.3532178997993469, 0.06410637497901917, 0.8529659509658813, 0.017698371782898903, 0.00045022345148026943, 0.004822523333132267, 0.5795548558235168, 0.14989349246025085, 0.9091790914535522, 0.035284366458654404, 0.6875185966491699, 0.4336507320404053, 0.011000481434166431, 0.9822956919670105, 0.06796489655971527, 0.026755552738904953, 0.03132323548197746, 0.1226702407002449, 0.008903665468096733, 0.014138118363916874, 0.787223219871521, 0.6182753443717957, 0.04250108450651169, 0.19766327738761902, 0.9661664962768555, 0.05221349373459816, 0.019176136702299118]], "normalized_score": [0.07423610985279083], "tokens": [["\u2581W", "emble", "y", "\u2581F", ".", "C", ".", "\u2581I", "\u2581haven", "'", "t", "\u2581found", "\u2581a", "\u2581way", "\u2581to", "\u2581save", "\u2581answers", "\u2581yet", ".", "\u2581In", "\u2581the", "\u2581mean", "\u2581time", "\u2581the", "\u2581interface", "\u2581of", "\u2581the", "\u2581A", "I", "\u2581seems", "\u2581to", "\u2581be", "\u2581able", "\u2581to", "\u2581answer", "\u2581anything", "\u2581I", "\u2581throw", "\u2581in", "\u2581to", "\u2581it", ".", "\u2581It", "'", "s", "\u2581very", "\u2581weird"]], "entropy": [[2.914421319961548, 1.295188546180725, 0.021421480923891068, 3.0065150260925293, 0.6900234818458557, 0.4927622079849243, 0.6519513130187988, 0.5737072825431824, 2.783067226409912, 0.2007690817117691, 0.03576125204563141, 2.645869731903076, 2.781735897064209, 4.459634780883789, 0.8718684911727905, 4.651691913604736, 3.4522504806518555, 3.4475111961364746, 2.2530083656311035, 3.1963531970977783, 3.7842938899993896, 3.409675121307373, 0.5740116834640503, 1.9871412515640259, 5.127232551574707, 3.5467586517333984, 2.577731132507324, 4.4988603591918945, 0.5786690711975098, 4.320561408996582, 2.012294292449951, 3.172797203063965, 5.178979873657227, 0.17373886704444885, 4.620306968688965, 4.229123115539551, 3.4833993911743164, 2.3192973136901855, 0.30104851722717285, 3.02685546875, 1.1694862842559814, 1.9405540227890015, 3.737063407897949, 3.822958469390869, 0.18007750809192657, 4.596056938171387, 4.459936618804932]], "avg_entropy": [2.5373281203210354], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Pensioners", "score_of_biggest_cluster": 0.07423610985279083, "score_of_first_answer": 0.07423610985279083, "max_score_of_first_answer": 0.998052716255188, "min_score_of_first_answer": 1.7176955225295387e-05, "avg_score": 0.07423610985279083, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37529686084135006, "std_of_first_answer": 0.37529686084135006, "avg_entropy_of_first_answer": 2.5373281203210354, "lex_sim": 1.0}
{"question": "Which Disney character is brought to life by the Blue Fairy ?", "answer": ["Peter Pan"], "scores": [[0.3804614543914795, 0.9645306468009949]], "normalized_score": [0.6057778000831604], "tokens": [["\u2581Peter", "\u2581Pan"]], "entropy": [[2.0308659076690674, 0.24616369605064392]], "avg_entropy": [1.1385148018598557], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pin-Occhio", "score_of_biggest_cluster": 0.6057778000831604, "score_of_first_answer": 0.6057778000831604, "max_score_of_first_answer": 0.9645306468009949, "min_score_of_first_answer": 0.3804614543914795, "avg_score": 0.6057778000831604, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2920345962047577, "std_of_first_answer": 0.2920345962047577, "avg_entropy_of_first_answer": 1.1385148018598557, "lex_sim": 1.0}
{"question": "In which year did the Battle of Gettysburg take place?", "answer": ["July 1, 1863"], "scores": [[0.07095732539892197, 0.7452966570854187, 0.8865549564361572, 0.06648104637861252, 0.9236721992492676, 0.8065866827964783, 0.9629961252212524, 0.99335116147995, 0.9894710779190063]], "normalized_score": [0.5066055655479431], "tokens": [["\u2581July", "\u2581", "1", ",", "\u2581", "1", "8", "6", "3"]], "entropy": [[2.4642391204833984, 1.061434030532837, 0.5070756673812866, 1.7752196788787842, 0.42148157954216003, 0.5506134629249573, 0.21727663278579712, 0.056714676320552826, 0.08082631230354309]], "avg_entropy": [0.7927645734614797], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, eight hundred and sixty-three", "score_of_biggest_cluster": 0.5066055655479431, "score_of_first_answer": 0.5066055655479431, "max_score_of_first_answer": 0.99335116147995, "min_score_of_first_answer": 0.06648104637861252, "avg_score": 0.5066055655479431, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3546442466749119, "std_of_first_answer": 0.3546442466749119, "avg_entropy_of_first_answer": 0.7927645734614797, "lex_sim": 1.0}
{"question": "Maria Fitzherbert was \u2018married\u2019 to which British monarch?", "answer": ["George IV"], "scores": [[0.7192106246948242, 0.7608116269111633]], "normalized_score": [0.7397187352180481], "tokens": [["\u2581George", "\u2581IV"]], "entropy": [[1.0805668830871582, 1.1246342658996582]], "avg_entropy": [1.1026005744934082], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George, Prince Regent", "score_of_biggest_cluster": 0.7397187352180481, "score_of_first_answer": 0.7397187352180481, "max_score_of_first_answer": 0.7608116269111633, "min_score_of_first_answer": 0.7192106246948242, "avg_score": 0.7397187352180481, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.020800501108169556, "std_of_first_answer": 0.020800501108169556, "avg_entropy_of_first_answer": 1.1026005744934082, "lex_sim": 1.0}
{"question": "In the book David Copperfield what was Mr Micawber's first name?", "answer": ["Wilkins"], "scores": [[0.15948227047920227, 0.8529757857322693]], "normalized_score": [0.36882856488227844], "tokens": [["\u2581Wil", "kins"]], "entropy": [[3.615894317626953, 0.9372162222862244]], "avg_entropy": [2.2765552699565887], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wilkin", "score_of_biggest_cluster": 0.36882856488227844, "score_of_first_answer": 0.36882856488227844, "max_score_of_first_answer": 0.8529757857322693, "min_score_of_first_answer": 0.15948227047920227, "avg_score": 0.36882856488227844, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3467467576265335, "std_of_first_answer": 0.3467467576265335, "avg_entropy_of_first_answer": 2.2765552699565887, "lex_sim": 1.0}
{"question": "What was the theme of the designs that appeared on the reverse of \u00a31 coins from 2004 - 2007?", "answer": ["Famous People and Events"], "scores": [[0.003941680770367384, 0.9961851239204407, 0.03548489138484001, 0.0342208668589592, 0.28689807653427124]], "normalized_score": [0.06717640161514282], "tokens": [["\u2581Fam", "ous", "\u2581People", "\u2581and", "\u2581Events"]], "entropy": [[5.640388488769531, 0.03612365946173668, 4.870395183563232, 1.4239075183868408, 3.815983772277832]], "avg_entropy": [3.1573597244918346], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bridges", "score_of_biggest_cluster": 0.06717640161514282, "score_of_first_answer": 0.06717640161514282, "max_score_of_first_answer": 0.9961851239204407, "min_score_of_first_answer": 0.003941680770367384, "avg_score": 0.06717640161514282, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37656273950490377, "std_of_first_answer": 0.37656273950490377, "avg_entropy_of_first_answer": 3.1573597244918346, "lex_sim": 1.0}
{"question": "Who was the first US-born winner of golf's British Open?", "answer": ["Bobby Jones"], "scores": [[0.10785307735204697, 0.6806187033653259]], "normalized_score": [0.2709369361400604], "tokens": [["\u2581Bobby", "\u2581Jones"]], "entropy": [[3.9411745071411133, 1.4274107217788696]], "avg_entropy": [2.6842926144599915], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Walter Hagen", "score_of_biggest_cluster": 0.2709369361400604, "score_of_first_answer": 0.2709369361400604, "max_score_of_first_answer": 0.6806187033653259, "min_score_of_first_answer": 0.10785307735204697, "avg_score": 0.2709369361400604, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2863828130066395, "std_of_first_answer": 0.2863828130066395, "avg_entropy_of_first_answer": 2.6842926144599915, "lex_sim": 1.0}
{"question": "\"Which American TV character said \"\"I am not capable of that emotion''?\"", "answer": ["Tony Newman, The Greatest American Hero"], "scores": [[0.012219464406371117, 0.0012782432604581118, 0.8718865513801575, 0.09864826500415802, 0.04944361373782158, 0.01441929116845131, 0.7871765494346619, 0.9946343898773193, 0.9624431133270264]], "normalized_score": [0.09644158184528351], "tokens": [["\u2581Tony", "\u2581New", "man", ",", "\u2581The", "\u2581Great", "est", "\u2581American", "\u2581Hero"]], "entropy": [[5.772933006286621, 1.3837541341781616, 0.6940618753433228, 1.4122350215911865, 5.375934600830078, 4.538880825042725, 1.353886365890503, 0.054394710808992386, 0.26346898078918457]], "avg_entropy": [2.316616613417864], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mr Spock", "score_of_biggest_cluster": 0.09644158184528351, "score_of_first_answer": 0.09644158184528351, "max_score_of_first_answer": 0.9946343898773193, "min_score_of_first_answer": 0.0012782432604581118, "avg_score": 0.09644158184528351, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4359077941536555, "std_of_first_answer": 0.4359077941536555, "avg_entropy_of_first_answer": 2.316616613417864, "lex_sim": 1.0}
{"question": "A hafiz is a person who has memorized what book?", "answer": ["Qur\u02be\u0101n"], "scores": [[0.15774884819984436, 0.97554612159729, 0.00036427745362743735, 0.769020676612854, 0.9953266382217407]], "normalized_score": [0.13381659984588623], "tokens": [["\u2581Q", "ur", "\u02be", "\u0101", "n"]], "entropy": [[2.0734310150146484, 0.18749813735485077, 1.0123813152313232, 0.7113025188446045, 0.03892882913351059]], "avg_entropy": [0.8047083631157875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Curaan", "score_of_biggest_cluster": 0.13381659984588623, "score_of_first_answer": 0.13381659984588623, "max_score_of_first_answer": 0.9953266382217407, "min_score_of_first_answer": 0.00036427745362743735, "avg_score": 0.13381659984588623, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41927427257199584, "std_of_first_answer": 0.41927427257199584, "avg_entropy_of_first_answer": 0.8047083631157875, "lex_sim": 1.0}
{"question": "Who helped bring photography to the masses with his Kodak roll-film technology?", "answer": ["George Eastman"], "scores": [[0.9221884608268738, 0.987027108669281, 0.9960250854492188]], "normalized_score": [0.9678462147712708], "tokens": [["\u2581George", "\u2581East", "man"]], "entropy": [[0.5970157384872437, 0.1326247602701187, 0.03581622615456581]], "avg_entropy": [0.2551522416373094], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George Eastman", "score_of_biggest_cluster": 0.9678462147712708, "score_of_first_answer": 0.9678462147712708, "max_score_of_first_answer": 0.9960250854492188, "min_score_of_first_answer": 0.9221884608268738, "avg_score": 0.9678462147712708, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03289184460856365, "std_of_first_answer": 0.03289184460856365, "avg_entropy_of_first_answer": 0.2551522416373094, "lex_sim": 1.0}
{"question": "Who became the acting leader of the Labour Party following the sudden death of John Smith in 1994?", "answer": ["Tony Blair"], "scores": [[0.4397318959236145, 0.9728116989135742, 0.993911623954773]], "normalized_score": [0.7519486546516418], "tokens": [["\u2581Tony", "\u2581Bla", "ir"]], "entropy": [[2.231743097305298, 0.22343209385871887, 0.04833534359931946]], "avg_entropy": [0.8345035115877787], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Margaret Becket", "score_of_biggest_cluster": 0.7519486546516418, "score_of_first_answer": 0.7519486546516418, "max_score_of_first_answer": 0.993911623954773, "min_score_of_first_answer": 0.4397318959236145, "avg_score": 0.7519486546516418, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2564142599070772, "std_of_first_answer": 0.2564142599070772, "avg_entropy_of_first_answer": 0.8345035115877787, "lex_sim": 1.0}
{"question": "Who was Pope during most of the 1914-18 Great War, succeeding Pius X in September 1914?", "answer": ["Benedict XV"], "scores": [[0.8892801403999329, 0.983343780040741, 0.9320887923240662]], "normalized_score": [0.9341151714324951], "tokens": [["\u2581Bened", "ict", "\u2581XV"]], "entropy": [[0.6213542222976685, 0.11466802656650543, 0.46474409103393555]], "avg_entropy": [0.4002554466327031], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Giacomo Della Chiesa", "score_of_biggest_cluster": 0.9341151714324951, "score_of_first_answer": 0.9341151714324951, "max_score_of_first_answer": 0.983343780040741, "min_score_of_first_answer": 0.8892801403999329, "avg_score": 0.9341151714324951, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.038452890005352854, "std_of_first_answer": 0.038452890005352854, "avg_entropy_of_first_answer": 0.4002554466327031, "lex_sim": 1.0}
{"question": "Who wrote 'The Legend Of Sleepy Hollow'?", "answer": ["Washington Irving"], "scores": [[0.8880673050880432, 0.9919800758361816, 0.9620306491851807]], "normalized_score": [0.9463372230529785], "tokens": [["\u2581Washington", "\u2581Ir", "ving"]], "entropy": [[0.8724122047424316, 0.07682410627603531, 0.2441665530204773]], "avg_entropy": [0.3978009546796481], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kickerbocker's History", "score_of_biggest_cluster": 0.9463372230529785, "score_of_first_answer": 0.9463372230529785, "max_score_of_first_answer": 0.9919800758361816, "min_score_of_first_answer": 0.8880673050880432, "avg_score": 0.9463372230529785, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.043672274895980016, "std_of_first_answer": 0.043672274895980016, "avg_entropy_of_first_answer": 0.3978009546796481, "lex_sim": 1.0}
{"question": "What is the name of the giraffe that Toys-r-us uses as its' mascot?", "answer": ["Geoffrey"], "scores": [[0.5144854187965393, 0.9889312982559204]], "normalized_score": [0.7132956981658936], "tokens": [["\u2581Geoff", "rey"]], "entropy": [[3.255176544189453, 0.0863255113363266]], "avg_entropy": [1.6707510277628899], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Geoffrey", "score_of_biggest_cluster": 0.7132956981658936, "score_of_first_answer": 0.7132956981658936, "max_score_of_first_answer": 0.9889312982559204, "min_score_of_first_answer": 0.5144854187965393, "avg_score": 0.7132956981658936, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23722293972969055, "std_of_first_answer": 0.23722293972969055, "avg_entropy_of_first_answer": 1.6707510277628899, "lex_sim": 1.0}
{"question": "What company's mascots include a gecko, a caveman, and a talking pile of money?", "answer": ["AMEX"], "scores": [[0.05663502588868141, 0.004561092704534531, 0.9963351488113403]], "normalized_score": [0.06360919773578644], "tokens": [["\u2581A", "ME", "X"]], "entropy": [[5.432936668395996, 2.5045342445373535, 0.03918483480811119]], "avg_entropy": [2.6588852492471537], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Even a caveman can do it", "score_of_biggest_cluster": 0.06360919773578644, "score_of_first_answer": 0.06360919773578644, "max_score_of_first_answer": 0.9963351488113403, "min_score_of_first_answer": 0.004561092704534531, "avg_score": 0.06360919773578644, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45574893111408427, "std_of_first_answer": 0.45574893111408427, "avg_entropy_of_first_answer": 2.6588852492471537, "lex_sim": 1.0}
{"question": "The Suez Canal joins the Red Sea and which other body of water?", "answer": ["Mediterranean Sea, Alexandria"], "scores": [[0.5765236616134644, 0.9596308469772339, 0.995701014995575, 0.4646829664707184, 0.005419742316007614, 0.0021005745511502028, 0.969373881816864]], "normalized_score": [0.16117136180400848], "tokens": [["\u2581Mediter", "rane", "an", "\u2581Sea", ",", "\u2581Alexand", "ria"]], "entropy": [[1.5437006950378418, 0.25905561447143555, 0.042747680097818375, 1.0241265296936035, 0.19395501911640167, 4.779509544372559, 0.20410636067390442]], "avg_entropy": [1.1496002062090807], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mediterranian", "score_of_biggest_cluster": 0.16117136180400848, "score_of_first_answer": 0.16117136180400848, "max_score_of_first_answer": 0.995701014995575, "min_score_of_first_answer": 0.0021005745511502028, "avg_score": 0.16117136180400848, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.40441999000984363, "std_of_first_answer": 0.40441999000984363, "avg_entropy_of_first_answer": 1.1496002062090807, "lex_sim": 1.0}
{"question": "In the Anna Sewell novel, what is Black Beauty?", "answer": ["a stallion"], "scores": [[0.08531133085489273, 0.02740984782576561, 0.9246319532394409, 0.9968840479850769]], "normalized_score": [0.21546746790409088], "tokens": [["\u2581a", "\u2581st", "all", "ion"]], "entropy": [[2.4637038707733154, 1.4059216976165771, 0.49245136976242065, 0.03290202096104622]], "avg_entropy": [1.0987447397783399], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lightning (game)", "score_of_biggest_cluster": 0.21546746790409088, "score_of_first_answer": 0.21546746790409088, "max_score_of_first_answer": 0.9968840479850769, "min_score_of_first_answer": 0.02740984782576561, "avg_score": 0.21546746790409088, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45338205494412226, "std_of_first_answer": 0.45338205494412226, "avg_entropy_of_first_answer": 1.0987447397783399, "lex_sim": 1.0}
{"question": "Quantrill's Raiders, a loosely organised force of Partisan rangers under the leadership of William Clarke Quantril, fought in which war?", "answer": ["American Civil War"], "scores": [[0.5240626931190491, 0.7569420337677002, 0.5104371309280396]], "normalized_score": [0.5872135162353516], "tokens": [["\u2581American", "\u2581Civil", "\u2581War"]], "entropy": [[2.11220645904541, 0.824374794960022, 0.8854866623878479]], "avg_entropy": [1.27402263879776], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Civil War in the United States", "score_of_biggest_cluster": 0.5872135162353516, "score_of_first_answer": 0.5872135162353516, "max_score_of_first_answer": 0.7569420337677002, "min_score_of_first_answer": 0.5104371309280396, "avg_score": 0.5872135162353516, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.11312879109422248, "std_of_first_answer": 0.11312879109422248, "avg_entropy_of_first_answer": 1.27402263879776, "lex_sim": 1.0}
{"question": "Argentina invaded UK's Falkland Islands, Israel invaded Southern Lebanon, Canada became officially independent of the UK, Leonid Brezhnev, leader of the USSR, died, all in what year?", "answer": ["1982"], "scores": [[0.6795888543128967, 0.9876471757888794, 0.9911752939224243, 0.610099196434021, 0.9515035152435303]], "normalized_score": [0.8267265558242798], "tokens": [["\u2581", "1", "9", "8", "2"]], "entropy": [[1.777144432067871, 0.10005352646112442, 0.07600744813680649, 1.1015797853469849, 0.29448312520980835]], "avg_entropy": [0.669853663444519], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and eighty-two", "score_of_biggest_cluster": 0.8267265558242798, "score_of_first_answer": 0.8267265558242798, "max_score_of_first_answer": 0.9911752939224243, "min_score_of_first_answer": 0.610099196434021, "avg_score": 0.8267265558242798, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.16467707474845766, "std_of_first_answer": 0.16467707474845766, "avg_entropy_of_first_answer": 0.669853663444519, "lex_sim": 1.0}
{"question": "What is the nickname of Thierry Guetta, the French-born street-artist?", "answer": ["Banksy"], "scores": [[0.3542528748512268, 0.7355401515960693, 0.9947243332862854]], "normalized_score": [0.6375890374183655], "tokens": [["\u2581B", "anks", "y"]], "entropy": [[2.7763190269470215, 1.8665993213653564, 0.05230104550719261]], "avg_entropy": [1.5650731312731903], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mr Brainwash", "score_of_biggest_cluster": 0.6375890374183655, "score_of_first_answer": 0.6375890374183655, "max_score_of_first_answer": 0.9947243332862854, "min_score_of_first_answer": 0.3542528748512268, "avg_score": 0.6375890374183655, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26305050561016186, "std_of_first_answer": 0.26305050561016186, "avg_entropy_of_first_answer": 1.5650731312731903, "lex_sim": 1.0}
{"question": "What sea lies directly south of the Irish Sea, bounded by England, Wales, Ireland and Brittany?", "answer": ["Atlantic Ocean"], "scores": [[0.11266956478357315, 0.3218236565589905]], "normalized_score": [0.19041986763477325], "tokens": [["\u2581Atlantic", "\u2581Ocean"]], "entropy": [[2.5367043018341064, 1.1053130626678467]], "avg_entropy": [1.8210086822509766], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mer celtique", "score_of_biggest_cluster": 0.19041986763477325, "score_of_first_answer": 0.19041986763477325, "max_score_of_first_answer": 0.3218236565589905, "min_score_of_first_answer": 0.11266956478357315, "avg_score": 0.19041986763477325, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.10457704588770866, "std_of_first_answer": 0.10457704588770866, "avg_entropy_of_first_answer": 1.8210086822509766, "lex_sim": 1.0}
{"question": "What term is used when a clot forms in a blood vessel?", "answer": ["Thrombus"], "scores": [[0.3169669508934021, 0.9966210126876831, 0.3610233664512634]], "normalized_score": [0.48494571447372437], "tokens": [["\u2581Th", "rom", "bus"]], "entropy": [[2.6637868881225586, 0.036738552153110504, 1.2332594394683838]], "avg_entropy": [1.311261626581351], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Thrombo-embolic disease", "score_of_biggest_cluster": 0.48494571447372437, "score_of_first_answer": 0.48494571447372437, "max_score_of_first_answer": 0.9966210126876831, "min_score_of_first_answer": 0.3169669508934021, "avg_score": 0.48494571447372437, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.310529114642617, "std_of_first_answer": 0.310529114642617, "avg_entropy_of_first_answer": 1.311261626581351, "lex_sim": 1.0}
{"question": "What name was given to the floating harbours used on D-Day?", "answer": ["Floating roadways"], "scores": [[0.0014628487406298518, 0.992085874080658, 0.005027794279158115, 0.7646292448043823]], "normalized_score": [0.048600900918245316], "tokens": [["\u2581Flo", "ating", "\u2581road", "ways"]], "entropy": [[1.649984359741211, 0.06812818348407745, 3.9594566822052, 1.0855107307434082]], "avg_entropy": [1.6907699890434742], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mulberry tea", "score_of_biggest_cluster": 0.048600900918245316, "score_of_first_answer": 0.048600900918245316, "max_score_of_first_answer": 0.992085874080658, "min_score_of_first_answer": 0.0014628487406298518, "avg_score": 0.048600900918245316, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4448865143209721, "std_of_first_answer": 0.4448865143209721, "avg_entropy_of_first_answer": 1.6907699890434742, "lex_sim": 1.0}
{"question": "Bonham\u2019s, Christies, Forbes and Fellows are are what type of business?", "answer": ["Antiquarioy"], "scores": [[0.008824306540191174, 0.22267989814281464, 0.00016142123786266893, 0.005960874259471893]], "normalized_score": [0.006594139151275158], "tokens": [["\u2581Anti", "qu", "ario", "y"]], "entropy": [[1.8298559188842773, 0.6107236742973328, 1.4659557342529297, 3.0906219482421875]], "avg_entropy": [1.7492893189191818], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Prominent auction", "score_of_biggest_cluster": 0.006594139151275158, "score_of_first_answer": 0.006594139151275158, "max_score_of_first_answer": 0.22267989814281464, "min_score_of_first_answer": 0.00016142123786266893, "avg_score": 0.006594139151275158, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09431751574412557, "std_of_first_answer": 0.09431751574412557, "avg_entropy_of_first_answer": 1.7492893189191818, "lex_sim": 1.0}
{"question": "The Old Faithful geyser is in which American national park?", "answer": ["Yellowstone National Park"], "scores": [[0.9023544192314148, 0.9603185057640076, 0.9591928720474243, 0.3692873418331146, 0.9757015705108643]], "normalized_score": [0.7857347726821899], "tokens": [["\u2581Y", "ellow", "stone", "\u2581National", "\u2581Park"]], "entropy": [[0.7676215171813965, 0.33021557331085205, 0.27240878343582153, 1.2682151794433594, 0.1547490358352661]], "avg_entropy": [0.5586420178413392], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yellowstone National Park Archives", "score_of_biggest_cluster": 0.7857347726821899, "score_of_first_answer": 0.7857347726821899, "max_score_of_first_answer": 0.9757015705108643, "min_score_of_first_answer": 0.3692873418331146, "avg_score": 0.7857347726821899, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23338258628850952, "std_of_first_answer": 0.23338258628850952, "avg_entropy_of_first_answer": 0.5586420178413392, "lex_sim": 1.0}
{"question": "Who was Britain's last reigning Tudor Monarch?", "answer": ["Queen Elizabeth I"], "scores": [[0.29566967487335205, 0.6472553610801697, 0.16379937529563904]], "normalized_score": [0.3153055012226105], "tokens": [["\u2581Queen", "\u2581Elizabeth", "\u2581I"]], "entropy": [[2.12504243850708, 1.3593392372131348, 1.509992241859436]], "avg_entropy": [1.6647913058598836], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Gloriana", "score_of_biggest_cluster": 0.3153055012226105, "score_of_first_answer": 0.3153055012226105, "max_score_of_first_answer": 0.6472553610801697, "min_score_of_first_answer": 0.16379937529563904, "avg_score": 0.3153055012226105, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.20405118117415946, "std_of_first_answer": 0.20405118117415946, "avg_entropy_of_first_answer": 1.6647913058598836, "lex_sim": 1.0}
{"question": "The music of which band finally became available for download from iTunes in November 2010?", "answer": ["The Rolling Stones"], "scores": [[0.41711947321891785, 0.15224245190620422, 0.9943774342536926, 0.9663809537887573, 0.9992607235908508]], "normalized_score": [0.5715246200561523], "tokens": [["\u2581The", "\u2581Rol", "ling", "\u2581St", "ones"]], "entropy": [[3.504471778869629, 2.311769962310791, 0.04803549498319626, 0.21855111420154572, 0.008487073704600334]], "avg_entropy": [1.2182630848139524], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fab 4", "score_of_biggest_cluster": 0.5715246200561523, "score_of_first_answer": 0.5715246200561523, "max_score_of_first_answer": 0.9992607235908508, "min_score_of_first_answer": 0.15224245190620422, "avg_score": 0.5715246200561523, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35413583322922415, "std_of_first_answer": 0.35413583322922415, "avg_entropy_of_first_answer": 1.2182630848139524, "lex_sim": 1.0}
{"question": "After the United States and the Soviet Union, what country became the third in the world to test an atom bomb (in 1952)?", "answer": ["Great Britain"], "scores": [[0.09591562300920486, 0.8660427927970886]], "normalized_score": [0.2882135212421417], "tokens": [["\u2581Great", "\u2581Britain"]], "entropy": [[3.7035937309265137, 0.5800814032554626]], "avg_entropy": [2.141837567090988], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Great\u2013Britain", "score_of_biggest_cluster": 0.2882135212421417, "score_of_first_answer": 0.2882135212421417, "max_score_of_first_answer": 0.8660427927970886, "min_score_of_first_answer": 0.09591562300920486, "avg_score": 0.2882135212421417, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3850635848939419, "std_of_first_answer": 0.3850635848939419, "avg_entropy_of_first_answer": 2.141837567090988, "lex_sim": 1.0}
{"question": "In the year 2000 Charity Shield match which player was sent off", "answer": ["Rio Ferdinand"], "scores": [[0.008358925580978394, 0.9351003170013428]], "normalized_score": [0.08841060101985931], "tokens": [["\u2581Rio", "\u2581Ferdinand"]], "entropy": [[5.489805698394775, 0.5493162870407104]], "avg_entropy": [3.019560992717743], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Roy Keane", "score_of_biggest_cluster": 0.08841060101985931, "score_of_first_answer": 0.08841060101985931, "max_score_of_first_answer": 0.9351003170013428, "min_score_of_first_answer": 0.008358925580978394, "avg_score": 0.08841060101985931, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4633706957101822, "std_of_first_answer": 0.4633706957101822, "avg_entropy_of_first_answer": 3.019560992717743, "lex_sim": 1.0}
{"question": "Which comedian was named outstanding newcomer at the 2009 London Evening Standard theatre awards?", "answer": ["Chris Addison"], "scores": [[0.006721036974340677, 0.7221483588218689, 0.9951196908950806]], "normalized_score": [0.16903604567050934], "tokens": [["\u2581Chris", "\u2581Add", "ison"]], "entropy": [[4.790643692016602, 1.7573323249816895, 0.04100324586033821]], "avg_entropy": [2.1963264209528766], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lenworth Henry", "score_of_biggest_cluster": 0.16903604567050934, "score_of_first_answer": 0.16903604567050934, "max_score_of_first_answer": 0.9951196908950806, "min_score_of_first_answer": 0.006721036974340677, "avg_score": 0.16903604567050934, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41677085364769834, "std_of_first_answer": 0.41677085364769834, "avg_entropy_of_first_answer": 2.1963264209528766, "lex_sim": 1.0}
{"question": "Ray Harryhausen is famous in the world of cinema for what?", "answer": ["Special effects, stop-motion animation"], "scores": [[0.17821472883224487, 0.8410617113113403, 0.011514763347804546, 0.11692456156015396, 0.507813036441803, 0.9251633882522583, 0.9995403289794922, 0.689004123210907]], "normalized_score": [0.29981935024261475], "tokens": [["\u2581Special", "\u2581effects", ",", "\u2581stop", "-", "m", "otion", "\u2581animation"]], "entropy": [[3.6931848526000977, 0.7936493158340454, 1.2123008966445923, 4.203641414642334, 0.9327445030212402, 0.42211705446243286, 0.004756022244691849, 1.51910400390625]], "avg_entropy": [1.5976872579194605], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brief history of special effects", "score_of_biggest_cluster": 0.29981935024261475, "score_of_first_answer": 0.29981935024261475, "max_score_of_first_answer": 0.9995403289794922, "min_score_of_first_answer": 0.011514763347804546, "avg_score": 0.29981935024261475, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36439341249744756, "std_of_first_answer": 0.36439341249744756, "avg_entropy_of_first_answer": 1.5976872579194605, "lex_sim": 1.0}
{"question": "In what outdoor sport, sanctioned by the NHPA, do you score 3 points for a ringer, 2 for a leaner, and the closet scores a point?", "answer": ["Corn hole"], "scores": [[0.012207671999931335, 0.04722714051604271]], "normalized_score": [0.02401111274957657], "tokens": [["\u2581Corn", "\u2581hole"]], "entropy": [[4.511635780334473, 0.9702186584472656]], "avg_entropy": [2.740927219390869], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Horseshoe pit", "score_of_biggest_cluster": 0.02401111274957657, "score_of_first_answer": 0.02401111274957657, "max_score_of_first_answer": 0.04722714051604271, "min_score_of_first_answer": 0.012207671999931335, "avg_score": 0.02401111274957657, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.017509734258055687, "std_of_first_answer": 0.017509734258055687, "avg_entropy_of_first_answer": 2.740927219390869, "lex_sim": 1.0}
{"question": "Who was Margaret Thatcher's first Chancellor of the Exchequer?", "answer": ["Denis The Bear Thatcher"], "scores": [[0.2597888708114624, 0.9684146046638489, 0.0021770999301224947, 0.0007048422121442854, 0.009936962276697159, 0.9756036400794983, 0.9976792931556702]], "normalized_score": [0.06252077221870422], "tokens": [["\u2581Den", "is", "\u2581The", "\u2581Bear", "\u2581Th", "atch", "er"]], "entropy": [[2.44439435005188, 0.24835556745529175, 4.603542804718018, 3.2672154903411865, 4.279779434204102, 0.19004708528518677, 0.025149472057819366]], "avg_entropy": [2.151212029159069], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lord Howe of Aberavon", "score_of_biggest_cluster": 0.06252077221870422, "score_of_first_answer": 0.06252077221870422, "max_score_of_first_answer": 0.9976792931556702, "min_score_of_first_answer": 0.0007048422121442854, "avg_score": 0.06252077221870422, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4592885798988871, "std_of_first_answer": 0.4592885798988871, "avg_entropy_of_first_answer": 2.151212029159069, "lex_sim": 1.0}
{"question": "The 53.5 km Milford Track is a long-distance walk in which country?", "answer": ["Milford Track"], "scores": [[0.002004212001338601, 0.9872205853462219, 0.2788916528224945]], "normalized_score": [0.08202214539051056], "tokens": [["\u2581Mil", "ford", "\u2581Track"]], "entropy": [[0.6651060581207275, 0.12977546453475952, 2.4706499576568604]], "avg_entropy": [1.0885104934374492], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "N Z", "score_of_biggest_cluster": 0.08202214539051056, "score_of_first_answer": 0.08202214539051056, "max_score_of_first_answer": 0.9872205853462219, "min_score_of_first_answer": 0.002004212001338601, "avg_score": 0.08202214539051056, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41486916730746937, "std_of_first_answer": 0.41486916730746937, "avg_entropy_of_first_answer": 1.0885104934374492, "lex_sim": 1.0}
{"question": "In South Africa, who replaced Nelson Mandela as President of South Africa in 1999?", "answer": ["Mandela Thabo"], "scores": [[0.00033010810147970915, 0.7227028608322144, 0.10057100653648376, 0.9835972189903259, 0.9700130820274353]], "normalized_score": [0.118014857172966], "tokens": [["\u2581Mand", "ela", "\u2581Th", "ab", "o"]], "entropy": [[0.6119924783706665, 1.4538583755493164, 2.253329038619995, 0.15404734015464783, 0.20765627920627594]], "avg_entropy": [0.9361767023801804], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mbeki, Thabo", "score_of_biggest_cluster": 0.118014857172966, "score_of_first_answer": 0.118014857172966, "max_score_of_first_answer": 0.9835972189903259, "min_score_of_first_answer": 0.00033010810147970915, "avg_score": 0.118014857172966, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4238440992597087, "std_of_first_answer": 0.4238440992597087, "avg_entropy_of_first_answer": 0.9361767023801804, "lex_sim": 1.0}
{"question": "What are the grades of proficiency in judo called?", "answer": ["Belts"], "scores": [[0.006903181318193674, 0.9989957213401794]], "normalized_score": [0.0830436572432518], "tokens": [["\u2581Bel", "ts"]], "entropy": [[2.9215340614318848, 0.011981976218521595]], "avg_entropy": [1.4667580188252032], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dansereau", "score_of_biggest_cluster": 0.0830436572432518, "score_of_first_answer": 0.0830436572432518, "max_score_of_first_answer": 0.9989957213401794, "min_score_of_first_answer": 0.006903181318193674, "avg_score": 0.0830436572432518, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4960462700109929, "std_of_first_answer": 0.4960462700109929, "avg_entropy_of_first_answer": 1.4667580188252032, "lex_sim": 1.0}
{"question": "What is traditionally written in a ledger?", "answer": ["Ledger"], "scores": [[0.11571753770112991, 0.8099636435508728]], "normalized_score": [0.3061486482620239], "tokens": [["\u2581Led", "ger"]], "entropy": [[4.767454624176025, 0.9411776065826416]], "avg_entropy": [2.8543161153793335], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Account", "score_of_biggest_cluster": 0.3061486482620239, "score_of_first_answer": 0.3061486482620239, "max_score_of_first_answer": 0.8099636435508728, "min_score_of_first_answer": 0.11571753770112991, "avg_score": 0.3061486482620239, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34712305292487144, "std_of_first_answer": 0.34712305292487144, "avg_entropy_of_first_answer": 2.8543161153793335, "lex_sim": 1.0}
{"question": "\"Which famous children's novel begins with the words \"\"The mole had been working very hard all morning, spring cleaning his little home\"\"\"", "answer": ["Wind in the Willows"], "scores": [[0.27944767475128174, 0.8698675632476807, 0.9862815737724304, 0.9005033373832703, 0.9930524230003357]], "normalized_score": [0.7349239587783813], "tokens": [["\u2581Wind", "\u2581in", "\u2581the", "\u2581Will", "ows"]], "entropy": [[2.1568264961242676, 0.6240283250808716, 0.08940571546554565, 0.37213289737701416, 0.045906439423561096]], "avg_entropy": [0.657659974694252], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wild Wood (novel)", "score_of_biggest_cluster": 0.7349239587783813, "score_of_first_answer": 0.7349239587783813, "max_score_of_first_answer": 0.9930524230003357, "min_score_of_first_answer": 0.27944767475128174, "avg_score": 0.7349239587783813, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26749101248527024, "std_of_first_answer": 0.26749101248527024, "avg_entropy_of_first_answer": 0.657659974694252, "lex_sim": 1.0}
{"question": "Terry Molloy, David Gooderson and Julian Bleach have all portrayed which villain in the UK television series Dr Who?", "answer": ["The Master"], "scores": [[0.5787180066108704, 0.8215550184249878]], "normalized_score": [0.6895278692245483], "tokens": [["\u2581The", "\u2581Master"]], "entropy": [[1.3985896110534668, 0.7795668840408325]], "avg_entropy": [1.0890782475471497], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Davros", "score_of_biggest_cluster": 0.6895278692245483, "score_of_first_answer": 0.6895278692245483, "max_score_of_first_answer": 0.8215550184249878, "min_score_of_first_answer": 0.5787180066108704, "avg_score": 0.6895278692245483, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12141850590705872, "std_of_first_answer": 0.12141850590705872, "avg_entropy_of_first_answer": 1.0890782475471497, "lex_sim": 1.0}
{"question": "Who did Friedrich Engels famously work with?", "answer": ["Mary Shelley"], "scores": [[0.005153427831828594, 0.21605317294597626, 0.8824002146720886, 0.9995939135551453]], "normalized_score": [0.17702576518058777], "tokens": [["\u2581Mary", "\u2581Sh", "elle", "y"]], "entropy": [[0.8297637701034546, 3.1007299423217773, 0.4132123291492462, 0.0048432732000947]], "avg_entropy": [1.0871373286936432], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Karl Marx", "score_of_biggest_cluster": 0.17702576518058777, "score_of_first_answer": 0.17702576518058777, "max_score_of_first_answer": 0.9995939135551453, "min_score_of_first_answer": 0.005153427831828594, "avg_score": 0.17702576518058777, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4238691825334939, "std_of_first_answer": 0.4238691825334939, "avg_entropy_of_first_answer": 1.0871373286936432, "lex_sim": 1.0}
{"question": "Two counties in the Irish province of Munster begin with the letter C. One is Cork. Name the other.", "answer": ["Clare"], "scores": [[0.8183265924453735, 0.9928593635559082]], "normalized_score": [0.9013785123825073], "tokens": [["\u2581Cla", "re"]], "entropy": [[0.9848275184631348, 0.05791178718209267]], "avg_entropy": [0.5213696528226137], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "CLARE", "score_of_biggest_cluster": 0.9013785123825073, "score_of_first_answer": 0.9013785123825073, "max_score_of_first_answer": 0.9928593635559082, "min_score_of_first_answer": 0.8183265924453735, "avg_score": 0.9013785123825073, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08726638555526733, "std_of_first_answer": 0.08726638555526733, "avg_entropy_of_first_answer": 0.5213696528226137, "lex_sim": 1.0}
{"question": "The fictitious South American city/country El Dorado was abundant with?", "answer": ["Gold"], "scores": [[0.7647513747215271]], "normalized_score": [0.7647513747215271], "tokens": [["\u2581Gold"]], "entropy": [[1.3764760494232178]], "avg_entropy": [1.3764760494232178], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gold compound", "score_of_biggest_cluster": 0.7647513747215271, "score_of_first_answer": 0.7647513747215271, "max_score_of_first_answer": 0.7647513747215271, "min_score_of_first_answer": 0.7647513747215271, "avg_score": 0.7647513747215271, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.3764760494232178, "lex_sim": 1.0}
{"question": "Pipistrellus is a genus of which mammal?", "answer": ["Bat"], "scores": [[0.3472476601600647]], "normalized_score": [0.3472476303577423], "tokens": [["\u2581Bat"]], "entropy": [[2.5714359283447266]], "avg_entropy": [2.5714359283447266], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "BAT", "score_of_biggest_cluster": 0.3472476303577423, "score_of_first_answer": 0.3472476303577423, "max_score_of_first_answer": 0.3472476601600647, "min_score_of_first_answer": 0.3472476601600647, "avg_score": 0.3472476303577423, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.5714359283447266, "lex_sim": 1.0}
{"question": "Which Italian fashion designer was shot dead on the steps of his Miami mansion in 1997?", "answer": ["Gianni Versace"], "scores": [[0.8882389664649963, 0.9790706634521484, 0.9645803570747375, 0.9806506037712097]], "normalized_score": [0.9523556232452393], "tokens": [["\u2581Gian", "ni", "\u2581Vers", "ace"]], "entropy": [[0.8471989631652832, 0.16549156606197357, 0.2798925042152405, 0.15835314989089966]], "avg_entropy": [0.36273404583334923], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Giani Versaci", "score_of_biggest_cluster": 0.9523556232452393, "score_of_first_answer": 0.9523556232452393, "max_score_of_first_answer": 0.9806506037712097, "min_score_of_first_answer": 0.8882389664649963, "avg_score": 0.9523556232452393, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.03798769029782751, "std_of_first_answer": 0.03798769029782751, "avg_entropy_of_first_answer": 0.36273404583334923, "lex_sim": 1.0}
{"question": "The Pilgrimage of Grace was a popular uprising in 1536, led by Robert Aske, in which city did it take place?", "answer": ["Lincoln"], "scores": [[0.09966018050909042]], "normalized_score": [0.09966018795967102], "tokens": [["\u2581Lincoln"]], "entropy": [[2.270651340484619]], "avg_entropy": [2.270651340484619], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Park Grove (1895)", "score_of_biggest_cluster": 0.09966018795967102, "score_of_first_answer": 0.09966018795967102, "max_score_of_first_answer": 0.09966018050909042, "min_score_of_first_answer": 0.09966018050909042, "avg_score": 0.09966018795967102, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.270651340484619, "lex_sim": 1.0}
{"question": "Who is the protagonist and narrator of Robert Louis Stevenson's novels 'Kidnapped' and 'Catriona'?", "answer": ["David Balfour"], "scores": [[0.9277969002723694, 0.9761816263198853, 0.9981967806816101, 0.9883646965026855]], "normalized_score": [0.9722528457641602], "tokens": [["\u2581David", "\u2581B", "alf", "our"]], "entropy": [[0.515915036201477, 0.23521414399147034, 0.01911550760269165, 0.11523377895355225]], "avg_entropy": [0.22136961668729782], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "David Balfour", "score_of_biggest_cluster": 0.9722528457641602, "score_of_first_answer": 0.9722528457641602, "max_score_of_first_answer": 0.9981967806816101, "min_score_of_first_answer": 0.9277969002723694, "avg_score": 0.9722528457641602, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.027036372394975538, "std_of_first_answer": 0.027036372394975538, "avg_entropy_of_first_answer": 0.22136961668729782, "lex_sim": 1.0}
{"question": "What is the acronym referring to a scale of readiness in the US military, derived from 'defense readiness condition'?", "answer": ["DRC"], "scores": [[0.02648467943072319, 0.8077423572540283]], "normalized_score": [0.1462627649307251], "tokens": [["\u2581D", "RC"]], "entropy": [[1.1238813400268555, 1.3109071254730225]], "avg_entropy": [1.217394232749939], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "DEFCON-1", "score_of_biggest_cluster": 0.1462627649307251, "score_of_first_answer": 0.1462627649307251, "max_score_of_first_answer": 0.8077423572540283, "min_score_of_first_answer": 0.02648467943072319, "avg_score": 0.1462627649307251, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39062883891165257, "std_of_first_answer": 0.39062883891165257, "avg_entropy_of_first_answer": 1.217394232749939, "lex_sim": 1.0}
{"question": "Who beat Tim Henman in his first Wimbledon singles semifinal?", "answer": ["Roger Federer"], "scores": [[0.168327197432518, 0.9954007267951965, 0.9936815500259399]], "normalized_score": [0.5501314997673035], "tokens": [["\u2581Roger", "\u2581Feder", "er"]], "entropy": [[3.8837499618530273, 0.05340740457177162, 0.05903063714504242]], "avg_entropy": [1.3320626678566139], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sampras", "score_of_biggest_cluster": 0.5501314997673035, "score_of_first_answer": 0.5501314997673035, "max_score_of_first_answer": 0.9954007267951965, "min_score_of_first_answer": 0.168327197432518, "avg_score": 0.5501314997673035, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3894816192904655, "std_of_first_answer": 0.3894816192904655, "avg_entropy_of_first_answer": 1.3320626678566139, "lex_sim": 1.0}
{"question": "Which American singer was born Eleanora Fagan?", "answer": ["Billie Holiday"], "scores": [[0.8889080286026001, 0.9986864924430847, 0.8454708456993103, 0.9945336580276489]], "normalized_score": [0.9295035004615784], "tokens": [["\u2581Bill", "ie", "\u2581Hol", "iday"]], "entropy": [[0.6873779892921448, 0.016673574224114418, 0.7046061754226685, 0.04865594580769539]], "avg_entropy": [0.36432842118665576], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Billie halliday", "score_of_biggest_cluster": 0.9295035004615784, "score_of_first_answer": 0.9295035004615784, "max_score_of_first_answer": 0.9986864924430847, "min_score_of_first_answer": 0.8454708456993103, "avg_score": 0.9295035004615784, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06652390361423516, "std_of_first_answer": 0.06652390361423516, "avg_entropy_of_first_answer": 0.36432842118665576, "lex_sim": 1.0}
{"question": "\"Who plays the part of Ric Griffin in TV's \"\"Holby City\"\"?\"", "answer": ["Joe Irvine"], "scores": [[0.023058725520968437, 0.0006736310897395015, 0.2500169575214386, 0.9987472295761108]], "normalized_score": [0.04437830671668053], "tokens": [["\u2581Joe", "\u2581Ir", "v", "ine"]], "entropy": [[4.609577655792236, 1.5389008522033691, 1.9343960285186768, 0.01417829841375351]], "avg_entropy": [2.024263208732009], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hugh Quarshie", "score_of_biggest_cluster": 0.04437830671668053, "score_of_first_answer": 0.04437830671668053, "max_score_of_first_answer": 0.9987472295761108, "min_score_of_first_answer": 0.0006736310897395015, "avg_score": 0.04437830671668053, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4048841818342514, "std_of_first_answer": 0.4048841818342514, "avg_entropy_of_first_answer": 2.024263208732009, "lex_sim": 1.0}
{"question": "What type of oil is traditionally used to protect cricket bats ?", "answer": ["Linseed"], "scores": [[0.3481330871582031, 0.9938547015190125]], "normalized_score": [0.5882123112678528], "tokens": [["\u2581Lin", "seed"]], "entropy": [[4.090737342834473, 0.054377391934394836]], "avg_entropy": [2.0725573673844337], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Linseed meal", "score_of_biggest_cluster": 0.5882123112678528, "score_of_first_answer": 0.5882123112678528, "max_score_of_first_answer": 0.9938547015190125, "min_score_of_first_answer": 0.3481330871582031, "avg_score": 0.5882123112678528, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32286080718040466, "std_of_first_answer": 0.32286080718040466, "avg_entropy_of_first_answer": 2.0725573673844337, "lex_sim": 1.0}
{"question": "Which country left the Commonwealthin 1972 and rejoined in 1989?", "answer": ["Malaysia"], "scores": [[0.00600298261269927, 0.9887009859085083]], "normalized_score": [0.0770399421453476], "tokens": [["\u2581Malays", "ia"]], "entropy": [[3.531132698059082, 0.08310547471046448]], "avg_entropy": [1.8071190863847733], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Union of Pakistan", "score_of_biggest_cluster": 0.0770399421453476, "score_of_first_answer": 0.0770399421453476, "max_score_of_first_answer": 0.9887009859085083, "min_score_of_first_answer": 0.00600298261269927, "avg_score": 0.0770399421453476, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4913490016479045, "std_of_first_answer": 0.4913490016479045, "avg_entropy_of_first_answer": 1.8071190863847733, "lex_sim": 1.0}
{"question": "Red Grant was a villain in which James Bond movie?", "answer": ["OSS"], "scores": [[0.0043386975303292274, 0.000954075192566961]], "normalized_score": [0.0020345624070614576], "tokens": [["\u2581O", "SS"]], "entropy": [[3.1351020336151123, 3.3947157859802246]], "avg_entropy": [3.2649089097976685], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "From Russia With Love (album)", "score_of_biggest_cluster": 0.0020345624070614576, "score_of_first_answer": 0.0020345624070614576, "max_score_of_first_answer": 0.0043386975303292274, "min_score_of_first_answer": 0.000954075192566961, "avg_score": 0.0020345624070614576, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0016923111688811332, "std_of_first_answer": 0.0016923111688811332, "avg_entropy_of_first_answer": 3.2649089097976685, "lex_sim": 1.0}
{"question": "On the London Underground, which is the only line to connect at some point with every other line on the system?", "answer": ["Central line"], "scores": [[0.10945925116539001, 0.31554582715034485]], "normalized_score": [0.18584780395030975], "tokens": [["\u2581Central", "\u2581line"]], "entropy": [[2.65323543548584, 1.4832332134246826]], "avg_entropy": [2.0682343244552612], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jubilee line", "score_of_biggest_cluster": 0.18584780395030975, "score_of_first_answer": 0.18584780395030975, "max_score_of_first_answer": 0.31554582715034485, "min_score_of_first_answer": 0.10945925116539001, "avg_score": 0.18584780395030975, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10304328799247742, "std_of_first_answer": 0.10304328799247742, "avg_entropy_of_first_answer": 2.0682343244552612, "lex_sim": 1.0}
{"question": "What is the disease that Stephen Hawking has?", "answer": ["Motor neurone disease"], "scores": [[0.24687770009040833, 0.7537755370140076, 0.5652007460594177, 0.7357942461967468]], "normalized_score": [0.5274372100830078], "tokens": [["\u2581Motor", "\u2581neur", "one", "\u2581disease"]], "entropy": [[2.4205079078674316, 0.9034830331802368, 0.7730576395988464, 1.1314347982406616]], "avg_entropy": [1.3071208447217941], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Motor neuron disease", "score_of_biggest_cluster": 0.5274372100830078, "score_of_first_answer": 0.5274372100830078, "max_score_of_first_answer": 0.7537755370140076, "min_score_of_first_answer": 0.24687770009040833, "avg_score": 0.5274372100830078, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.20345458470834013, "std_of_first_answer": 0.20345458470834013, "avg_entropy_of_first_answer": 1.3071208447217941, "lex_sim": 1.0}
{"question": "Which flower is also known as the Lent Lily?", "answer": ["Hellebore"], "scores": [[0.008744810707867146, 0.7711962461471558, 0.7367215752601624, 0.9718276262283325]], "normalized_score": [0.26360398530960083], "tokens": [["\u2581H", "elle", "b", "ore"]], "entropy": [[4.450268745422363, 1.2705295085906982, 0.6915788054466248, 0.1791505664587021]], "avg_entropy": [1.647881906479597], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Daphodyl", "score_of_biggest_cluster": 0.26360398530960083, "score_of_first_answer": 0.26360398530960083, "max_score_of_first_answer": 0.9718276262283325, "min_score_of_first_answer": 0.008744810707867146, "avg_score": 0.26360398530960083, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3653360763983433, "std_of_first_answer": 0.3653360763983433, "avg_entropy_of_first_answer": 1.647881906479597, "lex_sim": 1.0}
{"question": "Whose birthday is celebrated by a public holiday on the third Monday in January in the USA?", "answer": ["Franklin D. Roosevelt"], "scores": [[0.0048527163453400135, 0.22623389959335327, 0.6559743881225586, 0.9638646841049194, 0.9871024489402771, 0.9978101849555969, 0.9959949254989624]], "normalized_score": [0.3528479337692261], "tokens": [["\u2581Franklin", "\u2581D", ".", "\u2581Ro", "ose", "vel", "t"]], "entropy": [[2.4691529273986816, 1.2863314151763916, 0.9285035133361816, 0.24930283427238464, 0.0813809484243393, 0.02326026000082493, 0.037838228046894073]], "avg_entropy": [0.7251100180936711], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Reverend Doctor Martin Luther King, Junior", "score_of_biggest_cluster": 0.3528479337692261, "score_of_first_answer": 0.3528479337692261, "max_score_of_first_answer": 0.9978101849555969, "min_score_of_first_answer": 0.0048527163453400135, "avg_score": 0.3528479337692261, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3849517211093379, "std_of_first_answer": 0.3849517211093379, "avg_entropy_of_first_answer": 0.7251100180936711, "lex_sim": 1.0}
{"question": "What successful, no-frills airline is headquartered at Love Field in Dallas, and is one of the few that doesn't charge a baggage handling fee?", "answer": ["Big Daddy's airline"], "scores": [[0.0005320155178196728, 0.11539101600646973, 0.28456753492355347, 0.26464226841926575, 0.9962292313575745, 0.012940621934831142, 0.3652741014957428]], "normalized_score": [0.08042861521244049], "tokens": [["\u2581Big", "\u2581D", "addy", "'", "s", "\u2581air", "line"]], "entropy": [[2.384402275085449, 4.767822265625, 2.197173833847046, 2.8500726222991943, 0.04565312713384628, 2.371588706970215, 1.6033154726028442]], "avg_entropy": [2.317146900509085], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "RedBellyRadio", "score_of_biggest_cluster": 0.08042861521244049, "score_of_first_answer": 0.08042861521244049, "max_score_of_first_answer": 0.9962292313575745, "min_score_of_first_answer": 0.0005320155178196728, "avg_score": 0.08042861521244049, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31535178814060294, "std_of_first_answer": 0.31535178814060294, "avg_entropy_of_first_answer": 2.317146900509085, "lex_sim": 1.0}
{"question": "What board game for two players is known as wiq in Chinese and baduk in Korean?", "answer": ["Go"], "scores": [[0.6301113963127136]], "normalized_score": [0.6301113963127136], "tokens": [["\u2581Go"]], "entropy": [[2.539844036102295]], "avg_entropy": [2.539844036102295], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Go! (magazine)", "score_of_biggest_cluster": 0.6301113963127136, "score_of_first_answer": 0.6301113963127136, "max_score_of_first_answer": 0.6301113963127136, "min_score_of_first_answer": 0.6301113963127136, "avg_score": 0.6301113963127136, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.539844036102295, "lex_sim": 1.0}
{"question": "Who was the last woman hanged in Britain", "answer": ["Ruth Ellis"], "scores": [[0.636141836643219, 0.9952353835105896, 0.999071478843689]], "normalized_score": [0.8584047555923462], "tokens": [["\u2581Ruth", "\u2581Ell", "is"]], "entropy": [[2.2877020835876465, 0.05230478197336197, 0.010789886116981506]], "avg_entropy": [0.7835989172259966], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "David Blakely", "score_of_biggest_cluster": 0.8584047555923462, "score_of_first_answer": 0.8584047555923462, "max_score_of_first_answer": 0.999071478843689, "min_score_of_first_answer": 0.636141836643219, "avg_score": 0.8584047555923462, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17018970337601844, "std_of_first_answer": 0.17018970337601844, "avg_entropy_of_first_answer": 0.7835989172259966, "lex_sim": 1.0}
{"question": "\"From which Shakespeare play does the line \"\"A plague on both your houses\"\" come?\"", "answer": ["The Merchant of Venice"], "scores": [[0.20062623918056488, 0.8367711901664734, 0.8363968729972839, 0.9694376587867737, 0.9981644749641418, 0.9958798885345459]], "normalized_score": [0.7165105938911438], "tokens": [["\u2581The", "\u2581Mer", "chant", "\u2581of", "\u2581Ven", "ice"]], "entropy": [[2.8305535316467285, 0.8138257265090942, 0.47246360778808594, 0.17197579145431519, 0.019951486960053444, 0.04267696291208267]], "avg_entropy": [0.72524118454506], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Tragedy of Romeo and Juliet", "score_of_biggest_cluster": 0.7165105938911438, "score_of_first_answer": 0.7165105938911438, "max_score_of_first_answer": 0.9981644749641418, "min_score_of_first_answer": 0.20062623918056488, "avg_score": 0.7165105938911438, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2792971478067814, "std_of_first_answer": 0.2792971478067814, "avg_entropy_of_first_answer": 0.72524118454506, "lex_sim": 1.0}
{"question": "US singer Steveland Hardaway Morris is better known by what name?", "answer": ["Stevie Wonder"], "scores": [[0.6775597333908081, 0.6911163926124573, 0.9589347839355469]], "normalized_score": [0.7657657861709595], "tokens": [["\u2581Ste", "vie", "\u2581Wonder"]], "entropy": [[1.7754273414611816, 0.912839412689209, 0.2826274037361145]], "avg_entropy": [0.9902980526288351], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Steve Wonder", "score_of_biggest_cluster": 0.7657657861709595, "score_of_first_answer": 0.7657657861709595, "max_score_of_first_answer": 0.9589347839355469, "min_score_of_first_answer": 0.6775597333908081, "avg_score": 0.7657657861709595, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1295643953189103, "std_of_first_answer": 0.1295643953189103, "avg_entropy_of_first_answer": 0.9902980526288351, "lex_sim": 1.0}
{"question": "\"From her first US film musical \"\"Down Argentina Way\"\" (1940), who became famous for extravagant hats, jewellery and dresses?\"", "answer": ["Lucille Ball"], "scores": [[0.055254630744457245, 0.9963845014572144, 0.8219739198684692]], "normalized_score": [0.3563563823699951], "tokens": [["\u2581Luc", "ille", "\u2581Ball"]], "entropy": [[4.03794002532959, 0.03389276564121246, 1.122258186340332]], "avg_entropy": [1.7313636591037114], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Maria do Carmo Miranda da Cunha", "score_of_biggest_cluster": 0.3563563823699951, "score_of_first_answer": 0.3563563823699951, "max_score_of_first_answer": 0.9963845014572144, "min_score_of_first_answer": 0.055254630744457245, "avg_score": 0.3563563823699951, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40879266040823264, "std_of_first_answer": 0.40879266040823264, "avg_entropy_of_first_answer": 1.7313636591037114, "lex_sim": 1.0}
{"question": "\"Which Prime Minister said \"\"the War Office kept three sets of figures, one to mislead the public, one to mislead the Cabinet and one to mislead itself\"\"?\"", "answer": ["Winston Churchill"], "scores": [[0.20711767673492432, 0.9824857711791992, 0.9974168539047241, 0.8993056416511536, 0.9904060959815979]], "normalized_score": [0.7102776765823364], "tokens": [["\u2581W", "inst", "on", "\u2581Church", "ill"]], "entropy": [[3.475111961364746, 0.13769064843654633, 0.026846352964639664, 0.5842126607894897, 0.07283571362495422]], "avg_entropy": [0.8593394674360753], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Herbert Asquith (disambiguation)", "score_of_biggest_cluster": 0.7102776765823364, "score_of_first_answer": 0.7102776765823364, "max_score_of_first_answer": 0.9974168539047241, "min_score_of_first_answer": 0.20711767673492432, "avg_score": 0.7102776765823364, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3061772201858657, "std_of_first_answer": 0.3061772201858657, "avg_entropy_of_first_answer": 0.8593394674360753, "lex_sim": 1.0}
{"question": "Who wrote the 1963 novel \u2018The Spy Who Came in From the Cold\u2019?", "answer": ["John le Carre"], "scores": [[0.9422846436500549, 0.5624032020568848, 0.9852584004402161, 0.6023639440536499]], "normalized_score": [0.7488757371902466], "tokens": [["\u2581John", "\u2581le", "\u2581Car", "re"]], "entropy": [[0.5114547610282898, 1.4826059341430664, 0.12001572549343109, 0.8375630974769592]], "avg_entropy": [0.7379098795354366], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John le Carre", "score_of_biggest_cluster": 0.7488757371902466, "score_of_first_answer": 0.7488757371902466, "max_score_of_first_answer": 0.9852584004402161, "min_score_of_first_answer": 0.5624032020568848, "avg_score": 0.7488757371902466, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.19181929645599674, "std_of_first_answer": 0.19181929645599674, "avg_entropy_of_first_answer": 0.7379098795354366, "lex_sim": 1.0}
{"question": "In 1483, who was appointed the first grand inquisitor of the Spanish Inquisition?", "answer": ["Tomas de Torquemada"], "scores": [[0.6053807735443115, 0.8730413913726807, 0.7273188233375549, 0.9480178356170654, 0.9975564479827881, 0.9983482360839844, 0.990814745426178]], "normalized_score": [0.8640627264976501], "tokens": [["\u2581Tom", "as", "\u2581de", "\u2581Tor", "qu", "em", "ada"]], "entropy": [[1.8863234519958496, 0.753063440322876, 1.0912247896194458, 0.39563292264938354, 0.025038842111825943, 0.01700228452682495, 0.08080917596817017]], "avg_entropy": [0.6070135581706252], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Torquemada (disambiguation)", "score_of_biggest_cluster": 0.8640627264976501, "score_of_first_answer": 0.8640627264976501, "max_score_of_first_answer": 0.9983482360839844, "min_score_of_first_answer": 0.6053807735443115, "avg_score": 0.8640627264976501, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14315496001418485, "std_of_first_answer": 0.14315496001418485, "avg_entropy_of_first_answer": 0.6070135581706252, "lex_sim": 1.0}
{"question": "\"Which \"\"Bond girl\"\" met Ringo Starr when they starred in the 1981 film \"\"Caveman\"\", and later married him?\"", "answer": ["Barbara Bach"], "scores": [[0.9244857430458069, 0.9952591061592102]], "normalized_score": [0.9592199325561523], "tokens": [["\u2581Barbara", "\u2581Bach"]], "entropy": [[0.5594695806503296, 0.04416569322347641]], "avg_entropy": [0.301817636936903], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Barbara Bach", "score_of_biggest_cluster": 0.9592199325561523, "score_of_first_answer": 0.9592199325561523, "max_score_of_first_answer": 0.9952591061592102, "min_score_of_first_answer": 0.9244857430458069, "avg_score": 0.9592199325561523, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03538668155670166, "std_of_first_answer": 0.03538668155670166, "avg_entropy_of_first_answer": 0.301817636936903, "lex_sim": 1.0}
{"question": "According to Colossians, Ch 4 v 14, what profession was followed by St Luke?", "answer": ["Physician"], "scores": [[0.5574596524238586, 0.9910956621170044]], "normalized_score": [0.7433006763458252], "tokens": [["\u2581Phys", "ician"]], "entropy": [[2.5376858711242676, 0.06901902705430984]], "avg_entropy": [1.3033524490892887], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Physician", "score_of_biggest_cluster": 0.7433006763458252, "score_of_first_answer": 0.7433006763458252, "max_score_of_first_answer": 0.9910956621170044, "min_score_of_first_answer": 0.5574596524238586, "avg_score": 0.7433006763458252, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.21681800484657288, "std_of_first_answer": 0.21681800484657288, "avg_entropy_of_first_answer": 1.3033524490892887, "lex_sim": 1.0}
{"question": "Which animal is the emblem of the USA.'s Republican Party?", "answer": ["Donkey"], "scores": [[0.06675506383180618, 0.9904120564460754]], "normalized_score": [0.25712841749191284], "tokens": [["\u2581Don", "key"]], "entropy": [[3.589142322540283, 0.07766297459602356]], "avg_entropy": [1.8334026485681534], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Elephantinely", "score_of_biggest_cluster": 0.25712841749191284, "score_of_first_answer": 0.25712841749191284, "max_score_of_first_answer": 0.9904120564460754, "min_score_of_first_answer": 0.06675506383180618, "avg_score": 0.25712841749191284, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46182849630713463, "std_of_first_answer": 0.46182849630713463, "avg_entropy_of_first_answer": 1.8334026485681534, "lex_sim": 1.0}
{"question": "The youngest gold medalist for team GB at the 2012 Olympics, Jade Jones, won in which sport?", "answer": ["Judo"], "scores": [[0.1868996024131775, 0.9850003719329834]], "normalized_score": [0.4290643334388733], "tokens": [["\u2581J", "udo"]], "entropy": [[1.622185230255127, 0.12199532985687256]], "avg_entropy": [0.8720902800559998], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Punch (Taekwondo)", "score_of_biggest_cluster": 0.4290643334388733, "score_of_first_answer": 0.4290643334388733, "max_score_of_first_answer": 0.9850003719329834, "min_score_of_first_answer": 0.1868996024131775, "avg_score": 0.4290643334388733, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39905038475990295, "std_of_first_answer": 0.39905038475990295, "avg_entropy_of_first_answer": 0.8720902800559998, "lex_sim": 1.0}
{"question": "What would a Scotsman do with a spurtle?", "answer": ["Beat butter"], "scores": [[0.0777062326669693, 0.01380463968962431, 0.9935505390167236]], "normalized_score": [0.10214654356241226], "tokens": [["\u2581Beat", "\u2581but", "ter"]], "entropy": [[4.693634033203125, 3.456573009490967, 0.05884399637579918]], "avg_entropy": [2.7363503463566303], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eat porridge", "score_of_biggest_cluster": 0.10214654356241226, "score_of_first_answer": 0.10214654356241226, "max_score_of_first_answer": 0.9935505390167236, "min_score_of_first_answer": 0.01380463968962431, "avg_score": 0.10214654356241226, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4475558604870299, "std_of_first_answer": 0.4475558604870299, "avg_entropy_of_first_answer": 2.7363503463566303, "lex_sim": 1.0}
{"question": "Which author also writes under the name Barbara Vine?", "answer": ["Ruth Rendell"], "scores": [[0.9521334767341614, 0.9394434094429016, 0.9979173541069031, 0.9716882705688477]], "normalized_score": [0.9650448560714722], "tokens": [["\u2581Ruth", "\u2581R", "end", "ell"]], "entropy": [[0.36829423904418945, 0.5664249658584595, 0.022940296679735184, 0.19018298387527466]], "avg_entropy": [0.2869606213644147], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Baroness Rendell", "score_of_biggest_cluster": 0.9650448560714722, "score_of_first_answer": 0.9650448560714722, "max_score_of_first_answer": 0.9979173541069031, "min_score_of_first_answer": 0.9394434094429016, "avg_score": 0.9650448560714722, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.022060271550207875, "std_of_first_answer": 0.022060271550207875, "avg_entropy_of_first_answer": 0.2869606213644147, "lex_sim": 1.0}
{"question": "The adjective leporine relates to which animal?", "answer": ["rabbit"], "scores": [[0.05974171683192253, 0.8374180197715759]], "normalized_score": [0.22367118299007416], "tokens": [["\u2581rabb", "it"]], "entropy": [[2.9996988773345947, 0.5047920346260071]], "avg_entropy": [1.752245455980301], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Leveret", "score_of_biggest_cluster": 0.22367118299007416, "score_of_first_answer": 0.22367118299007416, "max_score_of_first_answer": 0.8374180197715759, "min_score_of_first_answer": 0.05974171683192253, "avg_score": 0.22367118299007416, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3888381514698267, "std_of_first_answer": 0.3888381514698267, "avg_entropy_of_first_answer": 1.752245455980301, "lex_sim": 1.0}
{"question": "Shirley Temple served as US Ambassador to which African country from 1974 \u2013 1976?", "answer": ["Eithopia"], "scores": [[0.0013523825909942389, 0.017635276541113853, 0.3077869415283203, 0.9845779538154602]], "normalized_score": [0.051849614828825], "tokens": [["\u2581E", "ith", "op", "ia"]], "entropy": [[2.5422050952911377, 3.667187213897705, 0.7639217376708984, 0.11740842461585999]], "avg_entropy": [1.7726806178689003], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ghana", "score_of_biggest_cluster": 0.051849614828825, "score_of_first_answer": 0.051849614828825, "max_score_of_first_answer": 0.9845779538154602, "min_score_of_first_answer": 0.0013523825909942389, "avg_score": 0.051849614828825, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3982863291338094, "std_of_first_answer": 0.3982863291338094, "avg_entropy_of_first_answer": 1.7726806178689003, "lex_sim": 1.0}
{"question": "'Roquefort Cheese' is made from what sort of milk?", "answer": ["Goat"], "scores": [[0.4956457018852234, 0.8387232422828674]], "normalized_score": [0.6447554230690002], "tokens": [["\u2581Go", "at"]], "entropy": [[2.4477295875549316, 0.5043925642967224]], "avg_entropy": [1.476061075925827], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sheep milk", "score_of_biggest_cluster": 0.6447554230690002, "score_of_first_answer": 0.6447554230690002, "max_score_of_first_answer": 0.8387232422828674, "min_score_of_first_answer": 0.4956457018852234, "avg_score": 0.6447554230690002, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17153877019882202, "std_of_first_answer": 0.17153877019882202, "avg_entropy_of_first_answer": 1.476061075925827, "lex_sim": 1.0}
{"question": "Which country defeated the Ivory Coast to win football's Africa Cup of Nations?", "answer": ["Togo"], "scores": [[0.014347822405397892, 0.9349176287651062]], "normalized_score": [0.11581894010305405], "tokens": [["\u2581T", "ogo"]], "entropy": [[2.6130478382110596, 0.4444235563278198]], "avg_entropy": [1.5287356972694397], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Zamibia", "score_of_biggest_cluster": 0.11581894010305405, "score_of_first_answer": 0.11581894010305405, "max_score_of_first_answer": 0.9349176287651062, "min_score_of_first_answer": 0.014347822405397892, "avg_score": 0.11581894010305405, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46028490317985415, "std_of_first_answer": 0.46028490317985415, "avg_entropy_of_first_answer": 1.5287356972694397, "lex_sim": 1.0}
{"question": "What company advertised itself with the slogan \u201cWe\u2019ll leave a light on for you\u201d?", "answer": ["The Home Depot"], "scores": [[0.036699019372463226, 0.041574448347091675, 0.9172847270965576, 0.9982730150222778]], "normalized_score": [0.19333414733409882], "tokens": [["\u2581The", "\u2581Home", "\u2581Dep", "ot"]], "entropy": [[5.4031476974487305, 5.979229927062988, 0.6269142627716064, 0.019629299640655518]], "avg_entropy": [3.007230296730995], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Motel 6", "score_of_biggest_cluster": 0.19333414733409882, "score_of_first_answer": 0.19333414733409882, "max_score_of_first_answer": 0.9982730150222778, "min_score_of_first_answer": 0.036699019372463226, "avg_score": 0.19333414733409882, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4602159309859709, "std_of_first_answer": 0.4602159309859709, "avg_entropy_of_first_answer": 3.007230296730995, "lex_sim": 1.0}
{"question": "Astrophobia is the fear of what?", "answer": ["The night sky"], "scores": [[0.11666486412286758, 0.28958266973495483, 0.3836505115032196]], "normalized_score": [0.2348998486995697], "tokens": [["\u2581The", "\u2581night", "\u2581sky"]], "entropy": [[4.0031938552856445, 3.184877395629883, 1.251710295677185]], "avg_entropy": [2.8132605155309043], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Forked lightning", "score_of_biggest_cluster": 0.2348998486995697, "score_of_first_answer": 0.2348998486995697, "max_score_of_first_answer": 0.3836505115032196, "min_score_of_first_answer": 0.11666486412286758, "avg_score": 0.2348998486995697, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1105695669701732, "std_of_first_answer": 0.1105695669701732, "avg_entropy_of_first_answer": 2.8132605155309043, "lex_sim": 1.0}
{"question": "Where did Richard III \u2018imprison\u2019 his two young nephews in order to eliminate them from succession to the throne so that he could gain the crown of England?", "answer": ["The Tower of London"], "scores": [[0.4067588746547699, 0.9535085558891296, 0.9002148509025574, 0.9805454015731812]], "normalized_score": [0.7649250626564026], "tokens": [["\u2581The", "\u2581Tower", "\u2581of", "\u2581London"]], "entropy": [[3.3701813220977783, 0.3300986886024475, 0.4526779055595398, 0.17042016983032227]], "avg_entropy": [1.080844521522522], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "London Tower", "score_of_biggest_cluster": 0.7649250626564026, "score_of_first_answer": 0.7649250626564026, "max_score_of_first_answer": 0.9805454015731812, "min_score_of_first_answer": 0.4067588746547699, "avg_score": 0.7649250626564026, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23474576677285475, "std_of_first_answer": 0.23474576677285475, "avg_entropy_of_first_answer": 1.080844521522522, "lex_sim": 1.0}
{"question": "In the Jewish calendar, Sivan is which month of the religious year?", "answer": ["Shavuos"], "scores": [[0.002903787652030587, 0.3878040313720703, 0.9168436527252197, 0.059618059545755386]], "normalized_score": [0.08857529610395432], "tokens": [["\u2581Sh", "av", "u", "os"]], "entropy": [[3.942326545715332, 1.9720820188522339, 0.4725763201713562, 0.8242528438568115]], "avg_entropy": [1.8028094321489334], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Third (music) (disambiguation)", "score_of_biggest_cluster": 0.08857529610395432, "score_of_first_answer": 0.08857529610395432, "max_score_of_first_answer": 0.9168436527252197, "min_score_of_first_answer": 0.002903787652030587, "avg_score": 0.08857529610395432, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36306640696719755, "std_of_first_answer": 0.36306640696719755, "avg_entropy_of_first_answer": 1.8028094321489334, "lex_sim": 1.0}
{"question": "What is the favourite drink of a vampire?", "answer": ["Bottled blood"], "scores": [[0.017842940986156464, 0.02662876434624195, 0.9104890823364258, 0.45842593908309937]], "normalized_score": [0.11866983771324158], "tokens": [["\u2581B", "ott", "led", "\u2581blood"]], "entropy": [[4.885107517242432, 3.4976329803466797, 0.3775562345981598, 3.2715282440185547]], "avg_entropy": [3.0079562440514565], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Oxygen capacity", "score_of_biggest_cluster": 0.11866983771324158, "score_of_first_answer": 0.11866983771324158, "max_score_of_first_answer": 0.9104890823364258, "min_score_of_first_answer": 0.017842940986156464, "avg_score": 0.11866983771324158, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3676807931045835, "std_of_first_answer": 0.3676807931045835, "avg_entropy_of_first_answer": 3.0079562440514565, "lex_sim": 1.0}
{"question": "Last month, what word was named by Oxford Dictionaries as the word of the year?", "answer": ["Selfie"], "scores": [[0.0253724604845047, 0.9816921949386597]], "normalized_score": [0.15782251954078674], "tokens": [["\u2581Self", "ie"]], "entropy": [[5.2063188552856445, 0.14174439013004303]], "avg_entropy": [2.674031622707844], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Selfies", "score_of_biggest_cluster": 0.15782251954078674, "score_of_first_answer": 0.15782251954078674, "max_score_of_first_answer": 0.9816921949386597, "min_score_of_first_answer": 0.0253724604845047, "avg_score": 0.15782251954078674, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4781598672270775, "std_of_first_answer": 0.4781598672270775, "avg_entropy_of_first_answer": 2.674031622707844, "lex_sim": 1.0}
{"question": "JSC. What phrase was trending worldwide on Twitter after the events of 7th Jan. 2015?", "answer": ["Justice for Sangeeta"], "scores": [[0.012121370993554592, 0.4027262330055237, 0.050558142364025116, 0.001981159904971719, 0.649314284324646]], "normalized_score": [0.05015861988067627], "tokens": [["\u2581Justice", "\u2581for", "\u2581S", "ange", "eta"]], "entropy": [[5.3613667488098145, 3.134065628051758, 4.761974334716797, 3.7195489406585693, 1.106542944908142]], "avg_entropy": [3.616699719429016], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Je suis Charlie", "score_of_biggest_cluster": 0.05015861988067627, "score_of_first_answer": 0.05015861988067627, "max_score_of_first_answer": 0.649314284324646, "min_score_of_first_answer": 0.001981159904971719, "avg_score": 0.05015861988067627, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25965366438902415, "std_of_first_answer": 0.25965366438902415, "avg_entropy_of_first_answer": 3.616699719429016, "lex_sim": 1.0}
{"question": "Who appears on the reverse of the current Bank of England \u00a310 note?", "answer": ["Jane Austen"], "scores": [[0.06964331865310669, 0.9144501686096191, 0.9986063838005066]], "normalized_score": [0.3991577923297882], "tokens": [["\u2581Jane", "\u2581Aust", "en"]], "entropy": [[3.8489131927490234, 0.6346129179000854, 0.01615404337644577]], "avg_entropy": [1.499893384675185], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charles R. Darwin", "score_of_biggest_cluster": 0.3991577923297882, "score_of_first_answer": 0.3991577923297882, "max_score_of_first_answer": 0.9986063838005066, "min_score_of_first_answer": 0.06964331865310669, "avg_score": 0.3991577923297882, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41949086318969947, "std_of_first_answer": 0.41949086318969947, "avg_entropy_of_first_answer": 1.499893384675185, "lex_sim": 1.0}
{"question": "Who played \u0091Aunty Entity' in the film 'Mad Max Beyond the Thunderdome'?", "answer": ["Tina Turner"], "scores": [[0.7868925333023071, 0.9956114888191223, 0.9836786389350891]], "normalized_score": [0.916824460029602], "tokens": [["\u2581T", "ina", "\u2581Turner"]], "entropy": [[1.4908244609832764, 0.04447120428085327, 0.14282402396202087]], "avg_entropy": [0.5593732297420502], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Anna Bullock", "score_of_biggest_cluster": 0.916824460029602, "score_of_first_answer": 0.916824460029602, "max_score_of_first_answer": 0.9956114888191223, "min_score_of_first_answer": 0.7868925333023071, "avg_score": 0.916824460029602, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.09570252909029078, "std_of_first_answer": 0.09570252909029078, "avg_entropy_of_first_answer": 0.5593732297420502, "lex_sim": 1.0}
{"question": "The Earth's Equator is approximately how many millions of metres in length?", "answer": ["30,000,000"], "scores": [[0.43208643794059753, 0.02859356999397278, 0.4353524446487427, 0.40666744112968445, 0.8649125695228577, 0.833170473575592, 0.9801511764526367, 0.7401764392852783, 0.9767448306083679, 0.9958357810974121, 0.9966791868209839]], "normalized_score": [0.5386956334114075], "tokens": [["\u2581", "3", "0", ",", "0", "0", "0", ",", "0", "0", "0"]], "entropy": [[2.9731838703155518, 0.9129736423492432, 1.994941234588623, 2.268160820007324, 0.7145404815673828, 0.8168305158615112, 0.14709678292274475, 1.0809744596481323, 0.17242291569709778, 0.03744611144065857, 0.03064664453268051]], "avg_entropy": [1.013565225357359], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "forty", "score_of_biggest_cluster": 0.5386956334114075, "score_of_first_answer": 0.5386956334114075, "max_score_of_first_answer": 0.9966791868209839, "min_score_of_first_answer": 0.02859356999397278, "avg_score": 0.5386956334114075, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.309808235221574, "std_of_first_answer": 0.309808235221574, "avg_entropy_of_first_answer": 1.013565225357359, "lex_sim": 1.0}
{"question": "The sweet red wine 'Commandaria' is produced on which island?", "answer": ["Cyprus"], "scores": [[0.9183605909347534, 0.9787200689315796, 0.9988013505935669]], "normalized_score": [0.9646807909011841], "tokens": [["\u2581Cy", "pr", "us"]], "entropy": [[0.6371281743049622, 0.12782657146453857, 0.012709439732134342]], "avg_entropy": [0.2592213951672117], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Culture of Cyprus", "score_of_biggest_cluster": 0.9646807909011841, "score_of_first_answer": 0.9646807909011841, "max_score_of_first_answer": 0.9988013505935669, "min_score_of_first_answer": 0.9183605909347534, "avg_score": 0.9646807909011841, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.03418453235622542, "std_of_first_answer": 0.03418453235622542, "avg_entropy_of_first_answer": 0.2592213951672117, "lex_sim": 1.0}
{"question": "A spoof of fraternal organizations such as the Masons, Shriners, Elks Club, or Moose Lodge, of what lodge was Fred Flintstone elected Grand Imperial Poobah?", "answer": ["Royal Order of Water Buffaloes"], "scores": [[0.00249377079308033, 0.9822703003883362, 0.9577286243438721, 0.11870062351226807, 0.9782664179801941, 0.9277559518814087, 0.7821787595748901]], "normalized_score": [0.2957020103931427], "tokens": [["\u2581Royal", "\u2581Order", "\u2581of", "\u2581Water", "\u2581Buff", "alo", "es"]], "entropy": [[3.4537885189056396, 0.17802570760250092, 0.2493184506893158, 4.500491142272949, 0.1884695589542389, 0.30261749029159546, 0.8326083421707153]], "avg_entropy": [1.3864741729838508], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Loyal Order of Water Buffalo", "score_of_biggest_cluster": 0.2957020103931427, "score_of_first_answer": 0.2957020103931427, "max_score_of_first_answer": 0.9822703003883362, "min_score_of_first_answer": 0.00249377079308033, "avg_score": 0.2957020103931427, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3970143038779739, "std_of_first_answer": 0.3970143038779739, "avg_entropy_of_first_answer": 1.3864741729838508, "lex_sim": 1.0}
{"question": "US President Harry S. Truman famously had a sign on his desk saying the what stops here?", "answer": ["The Buck Stops Here"], "scores": [[0.49058768153190613, 0.053420983254909515, 0.794972836971283, 0.9981393814086914, 0.9288805723190308]], "normalized_score": [0.454136461019516], "tokens": [["\u2581The", "\u2581Buck", "\u2581St", "ops", "\u2581Here"]], "entropy": [[3.122446060180664, 0.6649381518363953, 0.8874501585960388, 0.014674767851829529, 0.34522905945777893]], "avg_entropy": [1.0069476395845414], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Buckey Lasek", "score_of_biggest_cluster": 0.454136461019516, "score_of_first_answer": 0.454136461019516, "max_score_of_first_answer": 0.9981393814086914, "min_score_of_first_answer": 0.053420983254909515, "avg_score": 0.454136461019516, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3467737380911356, "std_of_first_answer": 0.3467737380911356, "avg_entropy_of_first_answer": 1.0069476395845414, "lex_sim": 1.0}
{"question": "What type of 'helpful' businesses operate on the basis of the Rochdale Principles that were formulated in 1844?", "answer": ["Co-operative retail"], "scores": [[0.35867828130722046, 0.9373469948768616, 0.8634595274925232, 0.6308112144470215, 0.007641650270670652, 0.9902456998825073]], "normalized_score": [0.3338972330093384], "tokens": [["\u2581Co", "-", "oper", "ative", "\u2581ret", "ail"]], "entropy": [[2.926086902618408, 0.36211514472961426, 0.6149550676345825, 0.7316285371780396, 1.8618243932724, 0.06254172325134277]], "avg_entropy": [1.093191961447398], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Co-operative enterprises", "score_of_biggest_cluster": 0.3338972330093384, "score_of_first_answer": 0.3338972330093384, "max_score_of_first_answer": 0.9902456998825073, "min_score_of_first_answer": 0.007641650270670652, "avg_score": 0.3338972330093384, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35111189523404995, "std_of_first_answer": 0.35111189523404995, "avg_entropy_of_first_answer": 1.093191961447398, "lex_sim": 1.0}
{"question": "Of which country was Adrian Nastase Prime Minister from 2000 to 2004?", "answer": ["Romania"], "scores": [[0.8754763603210449, 0.9797724485397339]], "normalized_score": [0.9261574745178223], "tokens": [["\u2581Roman", "ia"]], "entropy": [[0.8868683576583862, 0.11963468790054321]], "avg_entropy": [0.5032515227794647], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ISO 3166-1:RO", "score_of_biggest_cluster": 0.9261574745178223, "score_of_first_answer": 0.9261574745178223, "max_score_of_first_answer": 0.9797724485397339, "min_score_of_first_answer": 0.8754763603210449, "avg_score": 0.9261574745178223, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.05214804410934448, "std_of_first_answer": 0.05214804410934448, "avg_entropy_of_first_answer": 0.5032515227794647, "lex_sim": 1.0}
{"question": "Amerigo Vespucci international airport serves which European city?", "answer": ["Lisbon"], "scores": [[0.05086703598499298, 0.9625405073165894]], "normalized_score": [0.22127263247966766], "tokens": [["\u2581Lis", "bon"]], "entropy": [[3.5332112312316895, 0.2590198516845703]], "avg_entropy": [1.8961155414581299], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "UN/LOCODE:ITFLR", "score_of_biggest_cluster": 0.22127263247966766, "score_of_first_answer": 0.22127263247966766, "max_score_of_first_answer": 0.9625405073165894, "min_score_of_first_answer": 0.05086703598499298, "avg_score": 0.22127263247966766, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4558367356657982, "std_of_first_answer": 0.4558367356657982, "avg_entropy_of_first_answer": 1.8961155414581299, "lex_sim": 1.0}
{"question": "What is the name of the note that is 1/8 the length of a semi-breve?", "answer": ["Diapente"], "scores": [[0.005393410567194223, 0.7304395437240601, 0.4325747489929199]], "normalized_score": [0.11944547295570374], "tokens": [["\u2581Dia", "p", "ente"]], "entropy": [[4.639989376068115, 1.2045170068740845, 1.2004950046539307]], "avg_entropy": [2.348333795865377], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eighth-part-note", "score_of_biggest_cluster": 0.11944547295570374, "score_of_first_answer": 0.11944547295570374, "max_score_of_first_answer": 0.7304395437240601, "min_score_of_first_answer": 0.005393410567194223, "avg_score": 0.11944547295570374, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29756404110808043, "std_of_first_answer": 0.29756404110808043, "avg_entropy_of_first_answer": 2.348333795865377, "lex_sim": 1.0}
{"question": "What is the name of the strong fibrous tissue that joins one bone to another at a joint?", "answer": ["Tendon"], "scores": [[0.054941143840551376, 0.9333180785179138, 0.7469552159309387]], "normalized_score": [0.33708593249320984], "tokens": [["\u2581T", "end", "on"]], "entropy": [[3.4458718299865723, 0.46134740114212036, 0.8762767314910889]], "avg_entropy": [1.5944986542065938], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ligamentum", "score_of_biggest_cluster": 0.33708593249320984, "score_of_first_answer": 0.33708593249320984, "max_score_of_first_answer": 0.9333180785179138, "min_score_of_first_answer": 0.054941143840551376, "avg_score": 0.33708593249320984, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3778830847844536, "std_of_first_answer": 0.3778830847844536, "avg_entropy_of_first_answer": 1.5944986542065938, "lex_sim": 1.0}
{"question": "Fidel Castro, former President of Cuba, ordered which board game to be destroyed when he took power?", "answer": ["Monopoly"], "scores": [[0.7355074882507324, 0.9922183752059937, 0.9981886744499207]], "normalized_score": [0.8997786045074463], "tokens": [["\u2581Mon", "opol", "y"]], "entropy": [[1.6960338354110718, 0.05216319486498833, 0.019575638696551323]], "avg_entropy": [0.5892575563242038], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "List of possible monopolies", "score_of_biggest_cluster": 0.8997786045074463, "score_of_first_answer": 0.8997786045074463, "max_score_of_first_answer": 0.9981886744499207, "min_score_of_first_answer": 0.7355074882507324, "avg_score": 0.8997786045074463, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1224461466829087, "std_of_first_answer": 0.1224461466829087, "avg_entropy_of_first_answer": 0.5892575563242038, "lex_sim": 1.0}
{"question": "Which actor has been portrayed most on screen by other actors?", "answer": ["Marlon Brando"], "scores": [[0.02183961309492588, 0.8561373353004456, 0.9887235164642334, 0.9843865036964417]], "normalized_score": [0.36728841066360474], "tokens": [["\u2581Mar", "lon", "\u2581Br", "ando"]], "entropy": [[4.919857025146484, 0.6550004482269287, 0.11349402368068695, 0.10458780825138092]], "avg_entropy": [1.4482348263263702], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Roy Export Company Establishment", "score_of_biggest_cluster": 0.36728841066360474, "score_of_first_answer": 0.36728841066360474, "max_score_of_first_answer": 0.9887235164642334, "min_score_of_first_answer": 0.02183961309492588, "avg_score": 0.36728841066360474, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4024502642243699, "std_of_first_answer": 0.4024502642243699, "avg_entropy_of_first_answer": 1.4482348263263702, "lex_sim": 1.0}
{"question": "Which country has the international car registration code HKJ?", "answer": ["Vatican City"], "scores": [[0.001175054581835866, 0.8216427564620972, 0.9957446455955505, 0.7760770320892334]], "normalized_score": [0.16527168452739716], "tokens": [["\u2581V", "at", "ican", "\u2581City"]], "entropy": [[1.0545412302017212, 1.0991706848144531, 0.045732855796813965, 0.788591742515564]], "avg_entropy": [0.7470091283321381], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jordan (country)", "score_of_biggest_cluster": 0.16527168452739716, "score_of_first_answer": 0.16527168452739716, "max_score_of_first_answer": 0.9957446455955505, "min_score_of_first_answer": 0.001175054581835866, "avg_score": 0.16527168452739716, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38270832230660357, "std_of_first_answer": 0.38270832230660357, "avg_entropy_of_first_answer": 0.7470091283321381, "lex_sim": 1.0}
{"question": "Although they are best known for their only top five hit ,'Amateur Hour' and 'Beat The Clock' were other chart singles for which '70's group?", "answer": ["Dave Clark 5"], "scores": [[0.007446283008903265, 0.023773333057761192, 0.14087064564228058, 0.8803099989891052]], "normalized_score": [0.06844968348741531], "tokens": [["\u2581Dave", "\u2581Clark", "\u2581", "5"]], "entropy": [[3.8551149368286133, 2.5154292583465576, 0.7732605934143066, 0.45644238591194153]], "avg_entropy": [1.9000617936253548], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "'SPARKS'", "score_of_biggest_cluster": 0.06844968348741531, "score_of_first_answer": 0.06844968348741531, "max_score_of_first_answer": 0.8803099989891052, "min_score_of_first_answer": 0.007446283008903265, "avg_score": 0.06844968348741531, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36004314826138, "std_of_first_answer": 0.36004314826138, "avg_entropy_of_first_answer": 1.9000617936253548, "lex_sim": 1.0}
{"question": "A rabbit punch is a blow to what part of the body?", "answer": ["Ribs"], "scores": [[0.009284228086471558, 0.6698832511901855]], "normalized_score": [0.07886284589767456], "tokens": [["\u2581Rib", "s"]], "entropy": [[3.3028063774108887, 1.1320046186447144]], "avg_entropy": [2.2174054980278015], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Necke", "score_of_biggest_cluster": 0.07886284589767456, "score_of_first_answer": 0.07886284589767456, "max_score_of_first_answer": 0.6698832511901855, "min_score_of_first_answer": 0.009284228086471558, "avg_score": 0.07886284589767456, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.330299511551857, "std_of_first_answer": 0.330299511551857, "avg_entropy_of_first_answer": 2.2174054980278015, "lex_sim": 1.0}
{"question": "What notable vastly wealthy Budapest-born financier announced his retirement from managing external investment funds in 2011?", "answer": ["George Soros"], "scores": [[0.7888706922531128, 0.9672597050666809, 0.9964176416397095]], "normalized_score": [0.912704348564148], "tokens": [["\u2581George", "\u2581Sor", "os"]], "entropy": [[1.666353464126587, 0.3253278136253357, 0.03811224550008774]], "avg_entropy": [0.6765978410840034], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gyorgy Soros", "score_of_biggest_cluster": 0.912704348564148, "score_of_first_answer": 0.912704348564148, "max_score_of_first_answer": 0.9964176416397095, "min_score_of_first_answer": 0.7888706922531128, "avg_score": 0.912704348564148, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0917415217744813, "std_of_first_answer": 0.0917415217744813, "avg_entropy_of_first_answer": 0.6765978410840034, "lex_sim": 1.0}
{"question": "What is the name of the sport in which your opponent tries to knock you off a floating spinning log", "answer": ["Highland Games"], "scores": [[0.0021370688918977976, 0.7025480270385742, 0.15880778431892395]], "normalized_score": [0.06200908124446869], "tokens": [["\u2581High", "land", "\u2581Games"]], "entropy": [[4.018983840942383, 1.6707541942596436, 4.3872175216674805]], "avg_entropy": [3.358985185623169], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Birling", "score_of_biggest_cluster": 0.06200908124446869, "score_of_first_answer": 0.06200908124446869, "max_score_of_first_answer": 0.7025480270385742, "min_score_of_first_answer": 0.0021370688918977976, "avg_score": 0.06200908124446869, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30014342403390815, "std_of_first_answer": 0.30014342403390815, "avg_entropy_of_first_answer": 3.358985185623169, "lex_sim": 1.0}
{"question": "Was James II known as: James The Bald, Fiery Face, or The Red King?", "answer": ["James II"], "scores": [[0.3628937602043152, 0.12170282006263733]], "normalized_score": [0.2101551741361618], "tokens": [["\u2581James", "\u2581II"]], "entropy": [[1.8089970350265503, 1.310861587524414]], "avg_entropy": [1.5599293112754822], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fiery Face", "score_of_biggest_cluster": 0.2101551741361618, "score_of_first_answer": 0.2101551741361618, "max_score_of_first_answer": 0.3628937602043152, "min_score_of_first_answer": 0.12170282006263733, "avg_score": 0.2101551741361618, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12059547007083893, "std_of_first_answer": 0.12059547007083893, "avg_entropy_of_first_answer": 1.5599293112754822, "lex_sim": 1.0}
{"question": "Which group recorded the albums called 'Raintown', 'Fellow Hoodlums' and 'Homesick'?", "answer": ["The Alarm"], "scores": [[0.31185242533683777, 0.0009011881775222719, 0.94068843126297]], "normalized_score": [0.06418056041002274], "tokens": [["\u2581The", "\u2581Al", "arm"]], "entropy": [[2.7638866901397705, 3.1572601795196533, 0.4484978914260864]], "avg_entropy": [2.123214920361837], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Deacon Blue", "score_of_biggest_cluster": 0.06418056041002274, "score_of_first_answer": 0.06418056041002274, "max_score_of_first_answer": 0.94068843126297, "min_score_of_first_answer": 0.0009011881775222719, "avg_score": 0.06418056041002274, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3909142356218284, "std_of_first_answer": 0.3909142356218284, "avg_entropy_of_first_answer": 2.123214920361837, "lex_sim": 1.0}
{"question": "Which teeth are the third molars, and are always the last to0 erupt?", "answer": ["Third molars"], "scores": [[0.12666474282741547, 0.41663774847984314, 0.9619162678718567]], "normalized_score": [0.37026888132095337], "tokens": [["\u2581Third", "\u2581mol", "ars"]], "entropy": [[3.6574478149414062, 1.8351693153381348, 0.17708861827850342]], "avg_entropy": [1.8899019161860149], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wisdom-tooth", "score_of_biggest_cluster": 0.37026888132095337, "score_of_first_answer": 0.37026888132095337, "max_score_of_first_answer": 0.9619162678718567, "min_score_of_first_answer": 0.12666474282741547, "avg_score": 0.37026888132095337, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3462590741380593, "std_of_first_answer": 0.3462590741380593, "avg_entropy_of_first_answer": 1.8899019161860149, "lex_sim": 1.0}
{"question": "Who was the Defence Lawyer in the Leopold and Loeb murder trial of 1924?", "answer": ["Defense Lawyer Daniel Grodner"], "scores": [[0.0012022109003737569, 0.3074057102203369, 0.24767310917377472, 0.9860597848892212, 0.010944252833724022, 0.033797089010477066, 0.05110520124435425, 0.3786003887653351]], "normalized_score": [0.07100244611501694], "tokens": [["\u2581Def", "ense", "\u2581Law", "yer", "\u2581Daniel", "\u2581G", "rod", "ner"]], "entropy": [[3.767518997192383, 1.097424864768982, 2.325866222381592, 0.09812354296445847, 3.247821569442749, 4.415440559387207, 4.487540245056152, 2.098998785018921]], "avg_entropy": [2.6923418482765555], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Clarence S. Darrow", "score_of_biggest_cluster": 0.07100244611501694, "score_of_first_answer": 0.07100244611501694, "max_score_of_first_answer": 0.9860597848892212, "min_score_of_first_answer": 0.0012022109003737569, "avg_score": 0.07100244611501694, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3096244697530253, "std_of_first_answer": 0.3096244697530253, "avg_entropy_of_first_answer": 2.6923418482765555, "lex_sim": 1.0}
{"question": "Which African country's capital is named after an American president?", "answer": ["Burkina Faso"], "scores": [[0.01679428294301033, 0.8453891277313232, 0.951793909072876, 0.8666165471076965, 0.9802534580230713]], "normalized_score": [0.4092468321323395], "tokens": [["\u2581Bur", "k", "ina", "\u2581F", "aso"]], "entropy": [[4.659675598144531, 0.9013747572898865, 0.2647528350353241, 0.6351499557495117, 0.13971947133541107]], "avg_entropy": [1.320134523510933], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Liberia", "score_of_biggest_cluster": 0.4092468321323395, "score_of_first_answer": 0.4092468321323395, "max_score_of_first_answer": 0.9802534580230713, "min_score_of_first_answer": 0.01679428294301033, "avg_score": 0.4092468321323395, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36123037664292035, "std_of_first_answer": 0.36123037664292035, "avg_entropy_of_first_answer": 1.320134523510933, "lex_sim": 1.0}
{"question": "What type of creature is a bufflehead?", "answer": ["Pygmy-Opossum"], "scores": [[0.005629899445921183, 0.009895691648125648, 0.715464174747467, 0.019869394600391388, 0.00944597739726305, 0.007201153319329023, 0.994524359703064]], "normalized_score": [0.034096669405698776], "tokens": [["\u2581P", "yg", "my", "-", "O", "pos", "sum"]], "entropy": [[3.244230270385742, 3.829862356185913, 1.589526891708374, 2.941087484359741, 4.284400463104248, 1.0104639530181885, 0.05306549742817879]], "avg_entropy": [2.4218052737414837], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Quack (sound)", "score_of_biggest_cluster": 0.034096669405698776, "score_of_first_answer": 0.034096669405698776, "max_score_of_first_answer": 0.994524359703064, "min_score_of_first_answer": 0.005629899445921183, "avg_score": 0.034096669405698776, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.388788811884973, "std_of_first_answer": 0.388788811884973, "avg_entropy_of_first_answer": 2.4218052737414837, "lex_sim": 1.0}
{"question": "In which English city would you find Winson Green Prison?", "answer": ["Bishopsgate Street"], "scores": [[0.8882384896278381, 1.8286160411662422e-05, 0.7105820178985596, 0.8339609503746033, 0.09765318036079407]], "normalized_score": [0.06231888383626938], "tokens": [["\u2581B", "ish", "ops", "gate", "\u2581Street"]], "entropy": [[0.8510061502456665, 0.10332038998603821, 1.610630989074707, 1.0939127206802368, 1.5010206699371338]], "avg_entropy": [1.0319781839847564], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "County Borough of Birmingham", "score_of_biggest_cluster": 0.06231888383626938, "score_of_first_answer": 0.06231888383626938, "max_score_of_first_answer": 0.8882384896278381, "min_score_of_first_answer": 1.8286160411662422e-05, "avg_score": 0.06231888383626938, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37902058751271683, "std_of_first_answer": 0.37902058751271683, "avg_entropy_of_first_answer": 1.0319781839847564, "lex_sim": 1.0}
{"question": "In humans, otalgia, is the medical term for what?", "answer": ["Earache"], "scores": [[0.5040315389633179, 0.30398330092430115]], "normalized_score": [0.3914296627044678], "tokens": [["\u2581Ear", "ache"]], "entropy": [[2.044981002807617, 1.3984061479568481]], "avg_entropy": [1.7216935753822327], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Otalgia", "score_of_biggest_cluster": 0.3914296627044678, "score_of_first_answer": 0.3914296627044678, "max_score_of_first_answer": 0.5040315389633179, "min_score_of_first_answer": 0.30398330092430115, "avg_score": 0.3914296627044678, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10002411901950836, "std_of_first_answer": 0.10002411901950836, "avg_entropy_of_first_answer": 1.7216935753822327, "lex_sim": 1.0}
{"question": "Nine of the ten tallest buildings in Germany are in which city?", "answer": ["Berlin"], "scores": [[0.1310352236032486]], "normalized_score": [0.1310352385044098], "tokens": [["\u2581Berlin"]], "entropy": [[2.1941120624542236]], "avg_entropy": [2.1941120624542236], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Frankfort-on-the-Main", "score_of_biggest_cluster": 0.1310352385044098, "score_of_first_answer": 0.1310352385044098, "max_score_of_first_answer": 0.1310352236032486, "min_score_of_first_answer": 0.1310352236032486, "avg_score": 0.1310352385044098, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.1941120624542236, "lex_sim": 1.0}
{"question": "What colour is \u2018Mr Man\u2019 Mr Strong\u2019s hat?", "answer": ["Yellow"], "scores": [[0.09502359479665756, 0.9850303530693054]], "normalized_score": [0.30594301223754883], "tokens": [["\u2581Y", "ellow"]], "entropy": [[3.6802096366882324, 0.119804747402668]], "avg_entropy": [1.9000071920454502], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Greenishly", "score_of_biggest_cluster": 0.30594301223754883, "score_of_first_answer": 0.30594301223754883, "max_score_of_first_answer": 0.9850303530693054, "min_score_of_first_answer": 0.09502359479665756, "avg_score": 0.30594301223754883, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44500337913632393, "std_of_first_answer": 0.44500337913632393, "avg_entropy_of_first_answer": 1.9000071920454502, "lex_sim": 1.0}
{"question": "In which European city was the 1968 Eurovision Song Contest held?", "answer": ["London"], "scores": [[0.04164004325866699]], "normalized_score": [0.041640039533376694], "tokens": [["\u2581London"]], "entropy": [[3.5659842491149902]], "avg_entropy": [3.5659842491149902], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "London (UK)", "score_of_biggest_cluster": 0.041640039533376694, "score_of_first_answer": 0.041640039533376694, "max_score_of_first_answer": 0.04164004325866699, "min_score_of_first_answer": 0.04164004325866699, "avg_score": 0.041640039533376694, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.5659842491149902, "lex_sim": 1.0}
{"question": "Which fire-breathing creature of Greek mythology has the head of a lion, body of a goat and the rear of a dragon?", "answer": ["Griffon"], "scores": [[0.030575796961784363, 0.12062323838472366]], "normalized_score": [0.060730159282684326], "tokens": [["\u2581Griff", "on"]], "entropy": [[3.9069995880126953, 0.5809440612792969]], "avg_entropy": [2.243971824645996], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chimaeras", "score_of_biggest_cluster": 0.060730159282684326, "score_of_first_answer": 0.060730159282684326, "max_score_of_first_answer": 0.12062323838472366, "min_score_of_first_answer": 0.030575796961784363, "avg_score": 0.060730159282684326, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04502372071146965, "std_of_first_answer": 0.04502372071146965, "avg_entropy_of_first_answer": 2.243971824645996, "lex_sim": 1.0}
{"question": "Which Hollywood actor was married to Uma Thurman 1998 - 2004?", "answer": ["Ethan Hawke"], "scores": [[0.826361894607544, 0.9951470494270325, 0.8245530128479004, 0.9683641195297241]], "normalized_score": [0.9001786708831787], "tokens": [["\u2581Eth", "an", "\u2581Haw", "ke"]], "entropy": [[1.1063450574874878, 0.0455879345536232, 1.3525447845458984, 0.2028953731060028]], "avg_entropy": [0.6768432874232531], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ETHAN HAWKE", "score_of_biggest_cluster": 0.9001786708831787, "score_of_first_answer": 0.9001786708831787, "max_score_of_first_answer": 0.9951470494270325, "min_score_of_first_answer": 0.8245530128479004, "avg_score": 0.9001786708831787, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.07872325636319526, "std_of_first_answer": 0.07872325636319526, "avg_entropy_of_first_answer": 0.6768432874232531, "lex_sim": 1.0}
{"question": "What was the first name of the character played by John Travolta in Saturday Night Fever.", "answer": ["Tony Manero"], "scores": [[0.7688939571380615, 0.35897180438041687, 0.9650163054466248]], "normalized_score": [0.643409013748169], "tokens": [["\u2581Tony", "\u2581Man", "ero"]], "entropy": [[1.3993476629257202, 1.0983461141586304, 0.305753231048584]], "avg_entropy": [0.9344823360443115], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tony", "score_of_biggest_cluster": 0.643409013748169, "score_of_first_answer": 0.643409013748169, "max_score_of_first_answer": 0.9650163054466248, "min_score_of_first_answer": 0.35897180438041687, "avg_score": 0.643409013748169, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2524964431212879, "std_of_first_answer": 0.2524964431212879, "avg_entropy_of_first_answer": 0.9344823360443115, "lex_sim": 1.0}
{"question": "Afghanistan's controversial Parwan Detention Facility is better known in the 'western world' by what name?", "answer": ["Abu Ghurayb"], "scores": [[0.01794063113629818, 0.9493725299835205, 0.8082162141799927, 0.007681214716285467, 0.4803438186645508, 0.9774333834648132]], "normalized_score": [0.1917101889848709], "tokens": [["\u2581Ab", "u", "\u2581Gh", "ur", "ay", "b"]], "entropy": [[2.820361614227295, 0.5008966326713562, 0.9414685964584351, 0.22628013789653778, 1.363006591796875, 0.16344454884529114]], "avg_entropy": [1.0025763536492984], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Begram", "score_of_biggest_cluster": 0.1917101889848709, "score_of_first_answer": 0.1917101889848709, "max_score_of_first_answer": 0.9774333834648132, "min_score_of_first_answer": 0.007681214716285467, "avg_score": 0.1917101889848709, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40627202339426766, "std_of_first_answer": 0.40627202339426766, "avg_entropy_of_first_answer": 1.0025763536492984, "lex_sim": 1.0}
{"question": "What do the Americans call what we call a 'sofa'?", "answer": ["Couch"], "scores": [[0.1468576192855835, 0.9569388031959534]], "normalized_score": [0.37487828731536865], "tokens": [["\u2581C", "ouch"]], "entropy": [[3.8515286445617676, 0.318320095539093]], "avg_entropy": [2.0849243700504303], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Davenport (disambiguation)", "score_of_biggest_cluster": 0.37487828731536865, "score_of_first_answer": 0.37487828731536865, "max_score_of_first_answer": 0.9569388031959534, "min_score_of_first_answer": 0.1468576192855835, "avg_score": 0.37487828731536865, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40504059195518494, "std_of_first_answer": 0.40504059195518494, "avg_entropy_of_first_answer": 2.0849243700504303, "lex_sim": 1.0}
{"question": "In English what day of the week is the German dienstag?", "answer": ["Sunday"], "scores": [[0.05882486701011658]], "normalized_score": [0.05882486328482628], "tokens": [["\u2581Sunday"]], "entropy": [[3.2349302768707275]], "avg_entropy": [3.2349302768707275], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "TUESDAY", "score_of_biggest_cluster": 0.05882486328482628, "score_of_first_answer": 0.05882486328482628, "max_score_of_first_answer": 0.05882486701011658, "min_score_of_first_answer": 0.05882486701011658, "avg_score": 0.05882486328482628, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.2349302768707275, "lex_sim": 1.0}
{"question": "At the time the Spice Girls became famous, what was Scary Spice\u2019s surname?", "answer": ["Ginger Spicer"], "scores": [[0.04391877353191376, 0.0864376574754715, 0.39431336522102356, 0.006672999355942011, 0.8140485286712646]], "normalized_score": [0.09594741463661194], "tokens": [["\u2581G", "inger", "\u2581Sp", "ic", "er"]], "entropy": [[4.61760139465332, 4.241037845611572, 1.8031671047210693, 0.15374580025672913, 1.1222586631774902]], "avg_entropy": [2.3875621616840363], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Orangeish blacks", "score_of_biggest_cluster": 0.09594741463661194, "score_of_first_answer": 0.09594741463661194, "max_score_of_first_answer": 0.8140485286712646, "min_score_of_first_answer": 0.006672999355942011, "avg_score": 0.09594741463661194, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30515154601152594, "std_of_first_answer": 0.30515154601152594, "avg_entropy_of_first_answer": 2.3875621616840363, "lex_sim": 1.0}
{"question": "Which river meets the Rhine at the 'German Comer'?", "answer": ["Moselle Posted by Andrew at 2:17 pm No comments: Links to this post Labels: AI, game show, quiz, research, tech, tv The following is a guest post by a"], "scores": [[0.08344025909900665, 0.46454688906669617, 1.3024161660268874e-07, 0.41935038566589355, 0.4805850386619568, 0.0011312953429296613, 0.041322801262140274, 0.7159356474876404, 0.13730600476264954, 0.2617613971233368, 0.162849560379982, 0.09549631923437119, 0.14935772120952606, 0.05961323902010918, 0.8736673593521118, 0.9825164079666138, 0.29803547263145447, 0.9971039891242981, 0.9941142201423645, 0.9976917505264282, 0.9946556091308594, 0.012314656749367714, 0.9994957447052002, 0.998221218585968, 0.12710708379745483, 0.8829508423805237, 0.6205388903617859, 0.01035680528730154, 0.08632449805736542, 0.6091136932373047, 0.10061054676771164, 0.981773316860199, 0.3958751857280731, 0.0297758337110281, 0.1617438644170761, 0.0042987908236682415, 0.9133954048156738, 0.21780544519424438, 0.016640333458781242, 0.00015004353190306574, 0.00885606650263071, 0.026364367455244064, 0.5780388712882996, 0.010335823521018028, 0.7834175825119019, 0.4632461667060852, 0.024089694023132324]], "normalized_score": [0.1049635112285614], "tokens": [["\u2581Mos", "elle", "\u2581Post", "ed", "\u2581by", "\u2581Andrew", "\u2581at", "\u2581", "2", ":", "1", "7", "\u2581pm", "\u2581No", "\u2581comments", ":", "\u2581Link", "s", "\u2581to", "\u2581this", "\u2581post", "\u2581Label", "s", ":", "\u2581A", "I", ",", "\u2581game", "\u2581show", ",", "\u2581qu", "iz", ",", "\u2581research", ",", "\u2581te", "ch", ",", "\u2581tv", "\u2581The", "\u2581following", "\u2581is", "\u2581a", "\u2581guest", "\u2581post", "\u2581by", "\u2581a"]], "entropy": [[3.3025124073028564, 0.8309454917907715, 0.4646347761154175, 3.3523337841033936, 2.2933452129364014, 5.709136962890625, 5.757031440734863, 1.6913042068481445, 1.8922808170318604, 1.9157559871673584, 1.8036174774169922, 2.3050055503845215, 1.9388573169708252, 2.8836889266967773, 0.6097900867462158, 0.15023791790008545, 1.682995080947876, 0.029933461919426918, 0.05548020452260971, 0.026602141559123993, 0.05173157900571823, 0.7663559913635254, 0.005551355425268412, 0.020625339820981026, 5.998342990875244, 0.9337233901023865, 2.2951107025146484, 6.167707443237305, 2.047043800354004, 1.5792734622955322, 5.838315010070801, 0.1175251454114914, 2.1132490634918213, 4.733386039733887, 1.5348867177963257, 4.765290260314941, 0.5864459276199341, 2.754887580871582, 3.906928300857544, 1.7896842956542969, 6.841949462890625, 2.245253324508667, 1.9358736276626587, 4.914175987243652, 1.0044806003570557, 1.830723524093628, 5.578219413757324]], "avg_entropy": [2.362813395517383], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mozel", "score_of_biggest_cluster": 0.1049635112285614, "score_of_first_answer": 0.1049635112285614, "max_score_of_first_answer": 0.9994957447052002, "min_score_of_first_answer": 1.3024161660268874e-07, "avg_score": 0.1049635112285614, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.378450124683434, "std_of_first_answer": 0.378450124683434, "avg_entropy_of_first_answer": 2.362813395517383, "lex_sim": 1.0}
{"question": "Which American state has the most airports", "answer": ["California"], "scores": [[0.09345295280218124]], "normalized_score": [0.09345294535160065], "tokens": [["\u2581California"]], "entropy": [[4.035951614379883]], "avg_entropy": [4.035951614379883], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Texas (U.S. State)", "score_of_biggest_cluster": 0.09345294535160065, "score_of_first_answer": 0.09345294535160065, "max_score_of_first_answer": 0.09345295280218124, "min_score_of_first_answer": 0.09345295280218124, "avg_score": 0.09345294535160065, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.035951614379883, "lex_sim": 1.0}
{"question": "Little Miss Muffet sat on a tuffet eating her curds and what?", "answer": ["Whey"], "scores": [[0.048518549650907516, 0.997750461101532]], "normalized_score": [0.22002138197422028], "tokens": [["\u2581Whe", "y"]], "entropy": [[4.4879655838012695, 0.021447574719786644]], "avg_entropy": [2.254706579260528], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Whey powder", "score_of_biggest_cluster": 0.22002138197422028, "score_of_first_answer": 0.22002138197422028, "max_score_of_first_answer": 0.997750461101532, "min_score_of_first_answer": 0.048518549650907516, "avg_score": 0.22002138197422028, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.47461595572531223, "std_of_first_answer": 0.47461595572531223, "avg_entropy_of_first_answer": 2.254706579260528, "lex_sim": 1.0}
{"question": "Herman Van Rompuy became first Permanent President of what?", "answer": ["Eurozone Troika"], "scores": [[0.012875105254352093, 0.22784902155399323, 0.0004130798333790153, 0.9614943265914917]], "normalized_score": [0.032854482531547546], "tokens": [["\u2581Euro", "zone", "\u2581Tro", "ika"]], "entropy": [[2.0381834506988525, 3.719346046447754, 0.9915729761123657, 0.251403272151947]], "avg_entropy": [1.7501264363527298], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "European summits", "score_of_biggest_cluster": 0.032854482531547546, "score_of_first_answer": 0.032854482531547546, "max_score_of_first_answer": 0.9614943265914917, "min_score_of_first_answer": 0.0004130798333790153, "avg_score": 0.032854482531547546, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3921006900272169, "std_of_first_answer": 0.3921006900272169, "avg_entropy_of_first_answer": 1.7501264363527298, "lex_sim": 1.0}
{"question": "What was singer Jack Jones father Allan's theme song", "answer": ["Allan Jones"], "scores": [[0.09081798046827316, 0.36507704854011536, 0.5171101689338684]], "normalized_score": [0.2578575015068054], "tokens": [["\u2581All", "an", "\u2581Jones"]], "entropy": [[4.669849872589111, 2.968350410461426, 2.364964246749878]], "avg_entropy": [3.334388176600138], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Donkey Serenade", "score_of_biggest_cluster": 0.2578575015068054, "score_of_first_answer": 0.2578575015068054, "max_score_of_first_answer": 0.5171101689338684, "min_score_of_first_answer": 0.09081798046827316, "avg_score": 0.2578575015068054, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17640141598798126, "std_of_first_answer": 0.17640141598798126, "avg_entropy_of_first_answer": 3.334388176600138, "lex_sim": 1.0}
