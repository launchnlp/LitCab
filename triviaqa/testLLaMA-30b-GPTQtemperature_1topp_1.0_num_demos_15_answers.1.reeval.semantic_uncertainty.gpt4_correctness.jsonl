{"question": "Who was the third President of the USA?", "answer": ["AOL Search: \u3302 234"], "scores": [[0.010776788927614689, 0.005914958193898201, 0.0023784409277141094, 0.14546434581279755, 0.059445757418870926, 0.1634923666715622, 0.016539564356207848, 0.022124886512756348, 0.034827858209609985, 0.02547520399093628, 0.04936092719435692, 0.08197230100631714]], "normalized_score": [0.028453387320041656], "tokens": [["\u2581A", "OL", "\u2581Search", ":", "\u2581", "<0xE3>", "<0x8C>", "<0x82>", "\u2581", "2", "3", "4"]], "entropy": [[5.006460189819336, 6.665959358215332, 3.6139636039733887, 2.807743549346924, 5.50553560256958, 2.4732866287231445, 1.8722811937332153, 3.616269826889038, 1.943064570426941, 1.9579896926879883, 3.0563623905181885, 3.7718749046325684]], "avg_entropy": [3.5242326259613037], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Thomas Jefforson", "score_of_biggest_cluster": 0.028453387320041656, "score_of_first_answer": 0.028453387320041656, "max_score_of_first_answer": 0.1634923666715622, "min_score_of_first_answer": 0.0023784409277141094, "avg_score": 0.028453387320041656, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.051281526249684954, "std_of_first_answer": 0.051281526249684954, "avg_entropy_of_first_answer": 3.5242326259613037, "lex_sim": 1.0}
{"question": "Which famous novelist also wrote under the pseudonym Mary Westmacott?", "answer": ["Agatha Christie"], "scores": [[0.4679470956325531, 0.9878113865852356, 0.9933890104293823, 0.8423730731010437, 0.9503293037414551]], "normalized_score": [0.8186037540435791], "tokens": [["\u2581Ag", "ath", "a", "\u2581Christ", "ie"]], "entropy": [[3.9400200843811035, 0.11549607664346695, 0.06931379437446594, 1.100326418876648, 0.32001742720603943]], "avg_entropy": [1.1090347602963448], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Christie Estate", "score_of_biggest_cluster": 0.8186037540435791, "score_of_first_answer": 0.8186037540435791, "max_score_of_first_answer": 0.9933890104293823, "min_score_of_first_answer": 0.4679470956325531, "avg_score": 0.8186037540435791, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1978017505472408, "std_of_first_answer": 0.1978017505472408, "avg_entropy_of_first_answer": 1.1090347602963448, "lex_sim": 1.0}
{"question": "What ancient monument and tourist attraction would you see at Carnac in northern France?", "answer": ["92 menhirs"], "scores": [[0.039907682687044144, 0.0183199904859066, 0.025711026042699814, 0.021280335262417793, 0.8621741533279419, 0.962623119354248]], "normalized_score": [0.0832124799489975], "tokens": [["\u2581", "9", "2", "\u2581men", "h", "irs"]], "entropy": [[4.942668914794922, 4.4535322189331055, 3.4036219120025635, 4.541823387145996, 0.9130173921585083, 0.17192542552947998]], "avg_entropy": [3.071098208427429], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bautasten", "score_of_biggest_cluster": 0.0832124799489975, "score_of_first_answer": 0.0832124799489975, "max_score_of_first_answer": 0.962623119354248, "min_score_of_first_answer": 0.0183199904859066, "avg_score": 0.0832124799489975, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4187685394202875, "std_of_first_answer": 0.4187685394202875, "avg_entropy_of_first_answer": 3.071098208427429, "lex_sim": 1.0}
{"question": "What German battleship was sunk in November 1944 by Barnes-Wallis' Tallboy bombs, dropped by Lancaster bombers?", "answer": ["Tirpitz (ship)"], "scores": [[0.2292502522468567, 0.9746370911598206, 0.9840255379676819, 0.27248960733413696, 0.18784037232398987, 0.9742853045463562]], "normalized_score": [0.47133609652519226], "tokens": [["\u2581Tir", "p", "itz", "\u2581(", "ship", ")"]], "entropy": [[4.951706886291504, 0.219747394323349, 0.1272149235010147, 1.2164530754089355, 2.212200880050659, 0.18812167644500732]], "avg_entropy": [1.4859074726700783], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "TIRPITZ", "score_of_biggest_cluster": 0.47133609652519226, "score_of_first_answer": 0.47133609652519226, "max_score_of_first_answer": 0.9840255379676819, "min_score_of_first_answer": 0.18784037232398987, "avg_score": 0.47133609652519226, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3747059865544705, "std_of_first_answer": 0.3747059865544705, "avg_entropy_of_first_answer": 1.4859074726700783, "lex_sim": 1.0}
{"question": "Who did Claus von Stauffenberg plot to kill?", "answer": ["Chancellor Franz von Papen"], "scores": [[0.0030732157174497843, 0.9889386892318726, 0.9938777089118958, 0.023945175111293793, 0.747470498085022, 0.9851093292236328, 0.9919713735580444]], "normalized_score": [0.2448975294828415], "tokens": [["\u2581Chan", "cell", "or", "\u2581Franz", "\u2581von", "\u2581Pap", "en"]], "entropy": [[4.380023002624512, 0.09719434380531311, 0.050926804542541504, 3.0188302993774414, 1.5099895000457764, 0.15031832456588745, 0.06852178275585175]], "avg_entropy": [1.325114865388189], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hitlerian", "score_of_biggest_cluster": 0.2448975294828415, "score_of_first_answer": 0.2448975294828415, "max_score_of_first_answer": 0.9938777089118958, "min_score_of_first_answer": 0.0030732157174497843, "avg_score": 0.2448975294828415, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4271962609932482, "std_of_first_answer": 0.4271962609932482, "avg_entropy_of_first_answer": 1.325114865388189, "lex_sim": 1.0}
{"question": "In which year did John Stonehouse disappear and the Flixborough chemical plant exploded?", "answer": ["1974"], "scores": [[0.55314701795578, 0.7494415640830994, 0.9206731915473938, 0.9306058883666992, 0.9279221892356873]], "normalized_score": [0.8009257316589355], "tokens": [["\u2581", "1", "9", "7", "4"]], "entropy": [[3.5510056018829346, 1.4758455753326416, 0.5385745763778687, 0.467104971408844, 0.4384278953075409]], "avg_entropy": [1.294191724061966], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "1974", "score_of_biggest_cluster": 0.8009257316589355, "score_of_first_answer": 0.8009257316589355, "max_score_of_first_answer": 0.9306058883666992, "min_score_of_first_answer": 0.55314701795578, "avg_score": 0.8009257316589355, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.14841741000848546, "std_of_first_answer": 0.14841741000848546, "avg_entropy_of_first_answer": 1.294191724061966, "lex_sim": 1.0}
{"question": "What term is given to a piece of artwork that is painted directly onto a wall or architectural feature ?", "answer": ["mural"], "scores": [[0.025147520005702972, 0.9408080577850342]], "normalized_score": [0.15381479263305664], "tokens": [["\u2581m", "ural"]], "entropy": [[4.518013954162598, 0.5628222823143005]], "avg_entropy": [2.540418118238449], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wallpaint", "score_of_biggest_cluster": 0.15381479263305664, "score_of_first_answer": 0.15381479263305664, "max_score_of_first_answer": 0.9408080577850342, "min_score_of_first_answer": 0.025147520005702972, "avg_score": 0.15381479263305664, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4578302688896656, "std_of_first_answer": 0.4578302688896656, "avg_entropy_of_first_answer": 2.540418118238449, "lex_sim": 1.0}
{"question": "\"What single word sounds like \"\"Youth in Asia\"\"?\"", "answer": ["Gong sound: A"], "scores": [[0.0051076412200927734, 0.03746192529797554, 0.006584892515093088, 0.007368989288806915, 0.02068125642836094]], "normalized_score": [0.011393808759748936], "tokens": [["\u2581G", "ong", "\u2581sound", ":", "\u2581A"]], "entropy": [[5.520307540893555, 5.7576141357421875, 3.599249839782715, 2.41751766204834, 5.893136978149414]], "avg_entropy": [4.637565231323242], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Euthenasia", "score_of_biggest_cluster": 0.011393808759748936, "score_of_first_answer": 0.011393808759748936, "max_score_of_first_answer": 0.03746192529797554, "min_score_of_first_answer": 0.0051076412200927734, "avg_score": 0.011393808759748936, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.012351095002279664, "std_of_first_answer": 0.012351095002279664, "avg_entropy_of_first_answer": 4.637565231323242, "lex_sim": 1.0}
{"question": "Which unit of measurement is equal to one-tenth of a nautical mile?", "answer": ["1 7 8 7 (i.e."], "scores": [[0.05251414328813553, 0.3798612654209137, 0.005143332295119762, 0.014029588550329208, 0.030833207070827484, 0.07571925967931747, 0.4663879871368408, 0.07329465448856354, 0.0035381740890443325, 0.003762627486139536, 0.5328336954116821, 0.9689828753471375, 0.8627007603645325, 0.25053873658180237, 0.027049724012613297, 0.03551400080323219, 0.032990626990795135, 0.03762158006429672, 0.031144987791776657, 0.9650529623031616, 0.515360414981842, 0.353554368019104, 0.0076734512113034725, 0.9521310925483704, 0.6482939720153809, 0.026010874658823013, 0.10402087867259979, 0.9609076976776123, 0.9850022196769714, 0.5221813917160034, 0.5897865891456604, 0.08623003214597702, 0.0029804566875100136, 0.023136651143431664, 0.042569708079099655, 0.1430458426475525, 0.021255530416965485, 0.2640247046947479, 0.09355369210243225, 0.04066295549273491, 0.9826887845993042, 0.021339794620871544, 0.08630300313234329, 0.059108667075634, 0.017009465023875237, 0.0031861199531704187, 0.024333300068974495, 0.025799157097935677, 0.20746038854122162, 0.0953722596168518]], "normalized_score": [0.08077933639287949], "tokens": [["\u2581", "1", "\u2581", "7", "\u2581", "8", "\u2581", "7", "\u2581(", "i", ".", "e", ".", "\u2581", "2", "6", "2", "\u2581", "\u00f7", "\u2581", "1", "0", "\u2581\u00d7", "\u2581", "1", "2", "\u2581", "\u00f7", "\u2581", "1", "0", "0", ").", "\u2581(", "The", "\u2581answer", "\u2581given", "\u2581was", "\u2581\"", "na", "ut", "ic", "\"", "\u2581and", "\u2581that", ",", "\u2581when", "\u2581asked", "\u2581to", "\u2581clarify"]], "entropy": [[4.827054977416992, 2.8277039527893066, 2.922487258911133, 3.0378377437591553, 3.213149070739746, 2.7048823833465576, 2.5415728092193604, 2.3605823516845703, 1.9610486030578613, 5.985894203186035, 2.5494837760925293, 0.21493038535118103, 0.6764951944351196, 4.328812599182129, 1.0508131980895996, 2.987489700317383, 3.3345255851745605, 3.467158794403076, 2.587325096130371, 0.29955828189849854, 1.5750781297683716, 2.038602828979492, 1.8180110454559326, 0.40292179584503174, 1.4032235145568848, 1.4826207160949707, 2.5513734817504883, 0.29897812008857727, 0.12843644618988037, 1.61266028881073, 1.7069298028945923, 1.9499558210372925, 1.8944478034973145, 3.1335301399230957, 5.5014519691467285, 4.865931510925293, 3.0340094566345215, 2.786328077316284, 4.051468849182129, 4.399483680725098, 0.15610827505588531, 0.48786845803260803, 4.434558868408203, 3.495757579803467, 3.5689644813537598, 3.177725315093994, 4.510115146636963, 4.285924911499023, 2.420776844024658, 3.4091742038726807]], "avg_entropy": [2.609224470555782], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Data Cable", "score_of_biggest_cluster": 0.08077933639287949, "score_of_first_answer": 0.08077933639287949, "max_score_of_first_answer": 0.9850022196769714, "min_score_of_first_answer": 0.0029804566875100136, "avg_score": 0.08077933639287949, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3308976322177533, "std_of_first_answer": 0.3308976322177533, "avg_entropy_of_first_answer": 2.609224470555782, "lex_sim": 1.0}
{"question": "To which family of birds do sparrows belong?", "answer": ["Passeridae"], "scores": [[0.2987779676914215, 0.9640693068504333, 0.5032432675361633]], "normalized_score": [0.5253050327301025], "tokens": [["\u2581Pass", "er", "idae"]], "entropy": [[4.8447346687316895, 0.3251492977142334, 2.104903221130371]], "avg_entropy": [2.424929062525431], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sparow", "score_of_biggest_cluster": 0.5253050327301025, "score_of_first_answer": 0.5253050327301025, "max_score_of_first_answer": 0.9640693068504333, "min_score_of_first_answer": 0.2987779676914215, "avg_score": 0.5253050327301025, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27824434971016004, "std_of_first_answer": 0.27824434971016004, "avg_entropy_of_first_answer": 2.424929062525431, "lex_sim": 1.0}
{"question": "Dancer and actress Josie Marcus was the third wife of whom?", "answer": ["Singer/songwriter Ira Gershwin"], "scores": [[0.004691157955676317, 0.13696713745594025, 0.047903526574373245, 0.1632901281118393, 0.9797248840332031, 0.004074631724506617, 0.6969046592712402, 0.948351263999939, 0.9639960527420044, 0.9923225045204163]], "normalized_score": [0.16230377554893494], "tokens": [["\u2581S", "inger", "/", "song", "writer", "\u2581Ira", "\u2581G", "ers", "h", "win"]], "entropy": [[5.760994911193848, 5.261569976806641, 3.655349016189575, 2.757359743118286, 0.14626654982566833, 4.305553436279297, 1.887940764427185, 0.42233943939208984, 0.2607339024543762, 0.07066600769758224]], "avg_entropy": [2.452877374738455], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wyatt Earp (lawman)", "score_of_biggest_cluster": 0.16230377554893494, "score_of_first_answer": 0.16230377554893494, "max_score_of_first_answer": 0.9923225045204163, "min_score_of_first_answer": 0.004074631724506617, "avg_score": 0.16230377554893494, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4322093110386954, "std_of_first_answer": 0.4322093110386954, "avg_entropy_of_first_answer": 2.452877374738455, "lex_sim": 1.0}
{"question": "Which Dutch admiral boasted in 1667 that he had swept the British from the sea?", "answer": ["De Ruyter's"], "scores": [[0.10730858147144318, 0.8456734418869019, 0.9933751225471497, 0.9804679155349731, 0.004899300169199705, 0.9332504272460938]], "normalized_score": [0.271906316280365], "tokens": [["\u2581De", "\u2581R", "uy", "ter", "'", "s"]], "entropy": [[4.513834476470947, 1.144655704498291, 0.07553411275148392, 0.13199427723884583, 0.9052751064300537, 0.35338395833969116]], "avg_entropy": [1.1874462726215522], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Maarten Harpertsz. Tromp", "score_of_biggest_cluster": 0.271906316280365, "score_of_first_answer": 0.271906316280365, "max_score_of_first_answer": 0.9933751225471497, "min_score_of_first_answer": 0.004899300169199705, "avg_score": 0.271906316280365, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41954317341233804, "std_of_first_answer": 0.41954317341233804, "avg_entropy_of_first_answer": 1.1874462726215522, "lex_sim": 1.0}
{"question": "Who was the friend of Little Jackie Paper?", "answer": ["The Pied Piper of Hamelin"], "scores": [[0.07539930194616318, 0.23850131034851074, 0.7029387354850769, 0.9574354290962219, 0.9986503720283508, 0.4612475037574768, 0.9709253907203674, 0.8600412011146545]], "normalized_score": [0.5110839009284973], "tokens": [["\u2581The", "\u2581P", "ied", "\u2581Pi", "per", "\u2581of", "\u2581Ham", "elin"]], "entropy": [[4.811168193817139, 4.88676118850708, 1.0903401374816895, 0.3378615081310272, 0.012319962494075298, 1.4251713752746582, 0.293434739112854, 0.6508083343505859]], "avg_entropy": [1.6884831798961386], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Honalee", "score_of_biggest_cluster": 0.5110839009284973, "score_of_first_answer": 0.5110839009284973, "max_score_of_first_answer": 0.9986503720283508, "min_score_of_first_answer": 0.07539930194616318, "avg_score": 0.5110839009284973, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3355184040683317, "std_of_first_answer": 0.3355184040683317, "avg_entropy_of_first_answer": 1.6884831798961386, "lex_sim": 1.0}
{"question": "Which horse race is held at Churchill Downs?", "answer": ["The Kentucky Derby"], "scores": [[0.16663342714309692, 0.5866823792457581, 0.914598822593689]], "normalized_score": [0.4471624195575714], "tokens": [["\u2581The", "\u2581Kentucky", "\u2581Derby"]], "entropy": [[4.023166179656982, 3.0687270164489746, 0.6125980615615845]], "avg_entropy": [2.568163752555847], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "The Kentucky Derby", "score_of_biggest_cluster": 0.4471624195575714, "score_of_first_answer": 0.4471624195575714, "max_score_of_first_answer": 0.914598822593689, "min_score_of_first_answer": 0.16663342714309692, "avg_score": 0.4471624195575714, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.30612679812494203, "std_of_first_answer": 0.30612679812494203, "avg_entropy_of_first_answer": 2.568163752555847, "lex_sim": 1.0}
{"question": "What was Moses elder brother called", "answer": ["Aaron (bullingdon_club)"], "scores": [[0.456641286611557, 0.877687931060791, 0.06206700950860977, 0.018972350284457207, 0.010882051661610603, 0.0031960378400981426, 0.45011210441589355, 0.011249332688748837, 0.33360010385513306, 0.7300682663917542]], "normalized_score": [0.08523651957511902], "tokens": [["\u2581A", "aron", "\u2581(", "b", "ull", "ing", "don", "_", "club", ")"]], "entropy": [[4.106775283813477, 0.9274454116821289, 1.2389366626739502, 4.746216297149658, 1.8165836334228516, 3.1622936725616455, 2.874749183654785, 3.214143991470337, 4.736541748046875, 1.716820478439331]], "avg_entropy": [2.854050636291504], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "\u05d0\u05b7\u05d4\u05b2\u05e8\u05b9\u05df", "score_of_biggest_cluster": 0.08523651957511902, "score_of_first_answer": 0.08523651957511902, "max_score_of_first_answer": 0.877687931060791, "min_score_of_first_answer": 0.0031960378400981426, "avg_score": 0.08523651957511902, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.30940609231247446, "std_of_first_answer": 0.30940609231247446, "avg_entropy_of_first_answer": 2.854050636291504, "lex_sim": 1.0}
{"question": "What did St. George slay?", "answer": ["Jabberwock"], "scores": [[0.0020396788604557514, 0.20543348789215088, 0.8265790939331055, 0.8723390102386475, 0.987804651260376]], "normalized_score": [0.19723080098628998], "tokens": [["\u2581J", "ab", "ber", "wo", "ck"]], "entropy": [[4.6705121994018555, 5.164030075073242, 1.1447738409042358, 0.919323205947876, 0.09590353071689606]], "avg_entropy": [2.398908570408821], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Dragon (disambiguation)", "score_of_biggest_cluster": 0.19723080098628998, "score_of_first_answer": 0.19723080098628998, "max_score_of_first_answer": 0.987804651260376, "min_score_of_first_answer": 0.0020396788604557514, "avg_score": 0.19723080098628998, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39671134276758147, "std_of_first_answer": 0.39671134276758147, "avg_entropy_of_first_answer": 2.398908570408821, "lex_sim": 1.0}
{"question": "What type of oil is traditionally used to protect cricket bats ?", "answer": ["Beef tallow"], "scores": [[0.02438591793179512, 0.08053235709667206, 0.2513168454170227, 0.9987425208091736]], "normalized_score": [0.14900341629981995], "tokens": [["\u2581Be", "ef", "\u2581t", "allow"]], "entropy": [[4.845567226409912, 2.437494993209839, 2.6737265586853027, 0.015800055116415024]], "avg_entropy": [2.493147208355367], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Linseed meal", "score_of_biggest_cluster": 0.14900341629981995, "score_of_first_answer": 0.14900341629981995, "max_score_of_first_answer": 0.9987425208091736, "min_score_of_first_answer": 0.02438591793179512, "avg_score": 0.14900341629981995, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3901076169214514, "std_of_first_answer": 0.3901076169214514, "avg_entropy_of_first_answer": 2.493147208355367, "lex_sim": 1.0}
{"question": "Yarg and gevrik are cheeses from which county?", "answer": ["Cornwall"], "scores": [[0.40034785866737366, 0.8245007991790771]], "normalized_score": [0.5745320916175842], "tokens": [["\u2581Corn", "wall"]], "entropy": [[4.218211650848389, 0.9441816806793213]], "avg_entropy": [2.581196665763855], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Curnow", "score_of_biggest_cluster": 0.5745320916175842, "score_of_first_answer": 0.5745320916175842, "max_score_of_first_answer": 0.8245007991790771, "min_score_of_first_answer": 0.40034785866737366, "avg_score": 0.5745320916175842, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21207647025585175, "std_of_first_answer": 0.21207647025585175, "avg_entropy_of_first_answer": 2.581196665763855, "lex_sim": 1.0}
{"question": "On average, a Big Mac contains 178 of what ingredient?", "answer": ["g of fat"], "scores": [[0.0030191901605576277, 0.24609902501106262, 0.2137206643819809]], "normalized_score": [0.054152145981788635], "tokens": [["\u2581g", "\u2581of", "\u2581fat"]], "entropy": [[5.394701957702637, 3.78299617767334, 4.388097763061523]], "avg_entropy": [4.521931966145833], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Till (seed)", "score_of_biggest_cluster": 0.054152145981788635, "score_of_first_answer": 0.054152145981788635, "max_score_of_first_answer": 0.24609902501106262, "min_score_of_first_answer": 0.0030191901605576277, "avg_score": 0.054152145981788635, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10777098949318242, "std_of_first_answer": 0.10777098949318242, "avg_entropy_of_first_answer": 4.521931966145833, "lex_sim": 1.0}
{"question": "The River Yarra flows through which city?", "answer": ["Melbourne"], "scores": [[0.30550894141197205]], "normalized_score": [0.30550894141197205], "tokens": [["\u2581Melbourne"]], "entropy": [[4.958710670471191]], "avg_entropy": [4.958710670471191], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "MELBOURNE", "score_of_biggest_cluster": 0.30550894141197205, "score_of_first_answer": 0.30550894141197205, "max_score_of_first_answer": 0.30550894141197205, "min_score_of_first_answer": 0.30550894141197205, "avg_score": 0.30550894141197205, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.958710670471191, "lex_sim": 1.0}
{"question": "Horatio was a legendary Roman hero who defended a bridge against which army?", "answer": ["Horatius at the Bridge"], "scores": [[0.011270584538578987, 0.4632323086261749, 0.825121283531189, 0.2200285941362381, 0.9722899794578552, 0.4756257236003876]], "normalized_score": [0.27561354637145996], "tokens": [["\u2581Hor", "ati", "us", "\u2581at", "\u2581the", "\u2581Bridge"]], "entropy": [[5.58099889755249, 2.0404043197631836, 0.8266419172286987, 3.2347207069396973, 0.23015321791172028, 1.2485144138336182]], "avg_entropy": [2.1935722455382347], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Etruscan (disambiguation)", "score_of_biggest_cluster": 0.27561354637145996, "score_of_first_answer": 0.27561354637145996, "max_score_of_first_answer": 0.9722899794578552, "min_score_of_first_answer": 0.011270584538578987, "avg_score": 0.27561354637145996, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3285759098420524, "std_of_first_answer": 0.3285759098420524, "avg_entropy_of_first_answer": 2.1935722455382347, "lex_sim": 1.0}
{"question": "Which religion uses a domed structure called a Stupa as a shrine?", "answer": ["Buddhism"], "scores": [[0.4481279253959656, 0.8128600120544434]], "normalized_score": [0.6035439372062683], "tokens": [["\u2581Buddh", "ism"]], "entropy": [[3.8408143520355225, 0.6294441819190979]], "avg_entropy": [2.23512926697731], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "B\u00fadachas", "score_of_biggest_cluster": 0.6035439372062683, "score_of_first_answer": 0.6035439372062683, "max_score_of_first_answer": 0.8128600120544434, "min_score_of_first_answer": 0.4481279253959656, "avg_score": 0.6035439372062683, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1823660433292389, "std_of_first_answer": 0.1823660433292389, "avg_entropy_of_first_answer": 2.23512926697731, "lex_sim": 1.0}
{"question": "Real Love in 1996 was whose last top 10 hit in the UK?", "answer": ["A hit with Tom Petty & the Heartbreakers"], "scores": [[0.00798024982213974, 0.002916095545515418, 0.024724196642637253, 0.014749825932085514, 0.1052083820104599, 0.9968807697296143, 0.03866802155971527, 0.5694422721862793, 0.9888955354690552, 0.9758467674255371, 0.998485267162323]], "normalized_score": [0.10594965517520905], "tokens": [["\u2581A", "\u2581hit", "\u2581with", "\u2581Tom", "\u2581Pet", "ty", "\u2581&", "\u2581the", "\u2581Heart", "break", "ers"]], "entropy": [[5.071497917175293, 5.738201141357422, 3.301015853881836, 4.870072364807129, 3.5608787536621094, 0.030152974650263786, 1.723679780960083, 1.0554959774017334, 0.11620841175317764, 0.1678970754146576, 0.014505391009151936]], "avg_entropy": [2.3317823310975325], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fab 4", "score_of_biggest_cluster": 0.10594965517520905, "score_of_first_answer": 0.10594965517520905, "max_score_of_first_answer": 0.998485267162323, "min_score_of_first_answer": 0.002916095545515418, "avg_score": 0.10594965517520905, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45026211519955883, "std_of_first_answer": 0.45026211519955883, "avg_entropy_of_first_answer": 2.3317823310975325, "lex_sim": 1.0}
{"question": "What spirit is added to tomato juice to make a Bloody Maria?", "answer": ["Bier"], "scores": [[0.005044883582741022, 0.0042868722230196]], "normalized_score": [0.004650458227843046], "tokens": [["\u2581B", "ier"]], "entropy": [[4.934493064880371, 5.503083229064941]], "avg_entropy": [5.218788146972656], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tequila Manhattan", "score_of_biggest_cluster": 0.004650458227843046, "score_of_first_answer": 0.004650458227843046, "max_score_of_first_answer": 0.005044883582741022, "min_score_of_first_answer": 0.0042868722230196, "avg_score": 0.004650458227843046, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0003790056798607111, "std_of_first_answer": 0.0003790056798607111, "avg_entropy_of_first_answer": 5.218788146972656, "lex_sim": 1.0}
{"question": "What are the last four words of Gone With the Wind?", "answer": ["Died at last"], "scores": [[0.0038106143474578857, 0.15613138675689697, 0.039931751787662506, 0.1295996606349945]], "normalized_score": [0.04188917577266693], "tokens": [["\u2581D", "ied", "\u2581at", "\u2581last"]], "entropy": [[5.2020039558410645, 4.524258613586426, 2.3987531661987305, 3.12642240524292]], "avg_entropy": [3.812859535217285], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tomorrow is another day", "score_of_biggest_cluster": 0.04188917577266693, "score_of_first_answer": 0.04188917577266693, "max_score_of_first_answer": 0.15613138675689697, "min_score_of_first_answer": 0.0038106143474578857, "avg_score": 0.04188917577266693, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06253791848668432, "std_of_first_answer": 0.06253791848668432, "avg_entropy_of_first_answer": 3.812859535217285, "lex_sim": 1.0}
{"question": "\"\"\"Holding Back the Years\"\", was a US No 1 hit record for which British pop group?\"", "answer": ["Simply the Best"], "scores": [[0.40833359956741333, 0.926967203617096, 0.02072322554886341, 0.5854685306549072]], "normalized_score": [0.2603215277194977], "tokens": [["\u2581Sim", "ply", "\u2581the", "\u2581Best"]], "entropy": [[4.398253440856934, 0.6139776706695557, 1.9036251306533813, 1.9117825031280518]], "avg_entropy": [2.2069096863269806], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Simply Red", "score_of_biggest_cluster": 0.2603215277194977, "score_of_first_answer": 0.2603215277194977, "max_score_of_first_answer": 0.926967203617096, "min_score_of_first_answer": 0.02072322554886341, "avg_score": 0.2603215277194977, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32667225090361246, "std_of_first_answer": 0.32667225090361246, "avg_entropy_of_first_answer": 2.2069096863269806, "lex_sim": 1.0}
{"question": "In genetics, who is the woman who lived approximately 190,000-200,000 years ago and who was the most recent woman from whom all living humans today descend?", "answer": ["47.124.347.273815.12.23"], "scores": [[0.03647541627287865, 0.027200495824217796, 0.04072633385658264, 0.021550385281443596, 0.16755060851573944, 0.06862220913171768, 0.0388450063765049, 0.33521217107772827, 0.07546867430210114, 0.10111260414123535, 0.024606771767139435, 0.44413673877716064, 0.22284378111362457, 0.04416852816939354, 0.07189350575208664, 0.005155883729457855, 0.038459453731775284, 0.04857572913169861, 0.12934400141239166, 0.2659015953540802, 0.2190336287021637, 0.02219156175851822, 0.19537770748138428, 0.1140599474310875]], "normalized_score": [0.07107371091842651], "tokens": [["\u2581", "4", "7", ".", "1", "2", "4", ".", "3", "4", "7", ".", "2", "7", "3", "8", "1", "5", ".", "1", "2", ".", "2", "3"]], "entropy": [[5.125837802886963, 4.483855247497559, 3.8265466690063477, 3.739391326904297, 2.8357322216033936, 3.759202718734741, 3.3501784801483154, 2.965273380279541, 1.9083749055862427, 2.534876823425293, 2.0725715160369873, 1.8685228824615479, 2.1874301433563232, 2.404245615005493, 2.6546576023101807, 1.6513762474060059, 2.564344882965088, 2.7184839248657227, 2.792153835296631, 2.2413272857666016, 2.44963002204895, 2.510910987854004, 2.0477521419525146, 2.5900731086730957]], "avg_entropy": [2.80344790716966], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eve (genetics)", "score_of_biggest_cluster": 0.07107371091842651, "score_of_first_answer": 0.07107371091842651, "max_score_of_first_answer": 0.44413673877716064, "min_score_of_first_answer": 0.005155883729457855, "avg_score": 0.07107371091842651, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1109248925770121, "std_of_first_answer": 0.1109248925770121, "avg_entropy_of_first_answer": 2.80344790716966, "lex_sim": 1.0}
{"question": "What is the historical subject matter of Arthur Miller\u2019s 1953 play The Crucible?", "answer": ["Ayn Rand\u2019s 'The Fountainhead'"], "scores": [[0.016320010647177696, 0.004340017680078745, 0.8932833075523376, 0.06527063250541687, 0.9979754090309143, 0.005543040111660957, 0.6170516610145569, 0.8019751310348511, 0.9964311122894287, 0.9986698627471924, 0.980984091758728, 0.9204792380332947]], "normalized_score": [0.2157253473997116], "tokens": [["\u2581A", "yn", "\u2581Rand", "\u2019", "s", "\u2581'", "The", "\u2581F", "ount", "ain", "head", "'"]], "entropy": [[4.430374622344971, 6.435207843780518, 0.9418412446975708, 2.3991880416870117, 0.024117102846503258, 3.9666080474853516, 2.361832857131958, 1.4849879741668701, 0.04274813458323479, 0.01397719606757164, 0.1334308236837387, 0.5173999071121216]], "avg_entropy": [1.8959761496322851], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Salem Witch Trials Memorial", "score_of_biggest_cluster": 0.2157253473997116, "score_of_first_answer": 0.2157253473997116, "max_score_of_first_answer": 0.9986698627471924, "min_score_of_first_answer": 0.004340017680078745, "avg_score": 0.2157253473997116, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42655963200012187, "std_of_first_answer": 0.42655963200012187, "avg_entropy_of_first_answer": 1.8959761496322851, "lex_sim": 1.0}
{"question": "Marking the separation between the Atlantic and Pacific Oceans, what name is given to the southernmost piece of South America?", "answer": ["Ataulfo Sivirano Alvarez"], "scores": [[0.009687473997473717, 0.003452953416854143, 0.06861425191164017, 0.8267457485198975, 0.06047581881284714, 0.25287070870399475, 0.7688964009284973, 0.011419589631259441, 0.0031580752693116665, 0.12218864262104034, 0.9777806401252747]], "normalized_score": [0.06555799394845963], "tokens": [["\u2581A", "ta", "ul", "fo", "\u2581S", "iv", "ir", "ano", "\u2581Al", "vare", "z"]], "entropy": [[5.2324113845825195, 7.3111162185668945, 5.36364221572876, 1.2955355644226074, 3.097263813018799, 2.7725799083709717, 1.3007580041885376, 3.6052911281585693, 1.7287575006484985, 4.245517730712891, 0.18909505009651184]], "avg_entropy": [3.2856335016814144], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cape Hoorn", "score_of_biggest_cluster": 0.06555799394845963, "score_of_first_answer": 0.06555799394845963, "max_score_of_first_answer": 0.9777806401252747, "min_score_of_first_answer": 0.0031580752693116665, "avg_score": 0.06555799394845963, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3620182721515255, "std_of_first_answer": 0.3620182721515255, "avg_entropy_of_first_answer": 3.2856335016814144, "lex_sim": 1.0}
{"question": "Which country scored the first goal in the 2010 World Cup tournament?", "answer": ["Mexico"], "scores": [[0.06164078414440155]], "normalized_score": [0.06164078414440155], "tokens": [["\u2581Mexico"]], "entropy": [[4.971724510192871]], "avg_entropy": [4.971724510192871], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "South africa", "score_of_biggest_cluster": 0.06164078414440155, "score_of_first_answer": 0.06164078414440155, "max_score_of_first_answer": 0.06164078414440155, "min_score_of_first_answer": 0.06164078414440155, "avg_score": 0.06164078414440155, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.971724510192871, "lex_sim": 1.0}
{"question": "Which sport had previously been known as Whiff Whaff and Gossima?", "answer": ["Table tennis"], "scores": [[0.2798680365085602, 0.6484307646751404]], "normalized_score": [0.42599886655807495], "tokens": [["\u2581Table", "\u2581tennis"]], "entropy": [[3.429346799850464, 0.8523113131523132]], "avg_entropy": [2.1408290565013885], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Ping-pang Ch'iu", "score_of_biggest_cluster": 0.42599886655807495, "score_of_first_answer": 0.42599886655807495, "max_score_of_first_answer": 0.6484307646751404, "min_score_of_first_answer": 0.2798680365085602, "avg_score": 0.42599886655807495, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1842813640832901, "std_of_first_answer": 0.1842813640832901, "avg_entropy_of_first_answer": 2.1408290565013885, "lex_sim": 1.0}
{"question": "\"Who published \"\"A Book of Nonsense\"\" (a volume of limericks that went through three editions) in 1846, \"\"The History of the Seven Families of the Lake Pipple-Popple\"\" in 1865, and \"\"The Owl and the Pussycat\"\" in 1867?\"", "answer": ["Edward Lear"], "scores": [[0.5592576861381531, 0.9449639916419983]], "normalized_score": [0.7269651889801025], "tokens": [["\u2581Edward", "\u2581Lear"]], "entropy": [[3.2685160636901855, 0.49703288078308105]], "avg_entropy": [1.8827744722366333], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Edward Lear", "score_of_biggest_cluster": 0.7269651889801025, "score_of_first_answer": 0.7269651889801025, "max_score_of_first_answer": 0.9449639916419983, "min_score_of_first_answer": 0.5592576861381531, "avg_score": 0.7269651889801025, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1928531527519226, "std_of_first_answer": 0.1928531527519226, "avg_entropy_of_first_answer": 1.8827744722366333, "lex_sim": 1.0}
{"question": "Which hymn, written by John Newton, 1727-1807, was the inspiration for a number one hit of 1973?", "answer": ["Amazing Grace"], "scores": [[0.4186413884162903, 0.9923728108406067, 0.9389752745628357]], "normalized_score": [0.730674147605896], "tokens": [["\u2581Ama", "zing", "\u2581Grace"]], "entropy": [[4.331029891967773, 0.06361450999975204, 0.4152553677558899]], "avg_entropy": [1.6032999232411385], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Amazing Grace", "score_of_biggest_cluster": 0.730674147605896, "score_of_first_answer": 0.730674147605896, "max_score_of_first_answer": 0.9923728108406067, "min_score_of_first_answer": 0.4186413884162903, "avg_score": 0.730674147605896, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.25879343865758, "std_of_first_answer": 0.25879343865758, "avg_entropy_of_first_answer": 1.6032999232411385, "lex_sim": 1.0}
{"question": "Which former Neighbours star links the films Memento and The Time Machine?", "answer": ["Guy pearce"], "scores": [[0.7095722556114197, 0.0025045068468898535, 0.9462039470672607, 0.9170002341270447]], "normalized_score": [0.1981610208749771], "tokens": [["\u2581Guy", "\u2581p", "ear", "ce"]], "entropy": [[2.3461458683013916, 1.684906005859375, 0.48409271240234375, 0.5343472361564636]], "avg_entropy": [1.2623729556798935], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Guy Pearce", "score_of_biggest_cluster": 0.1981610208749771, "score_of_first_answer": 0.1981610208749771, "max_score_of_first_answer": 0.9462039470672607, "min_score_of_first_answer": 0.0025045068468898535, "avg_score": 0.1981610208749771, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.38133725645341376, "std_of_first_answer": 0.38133725645341376, "avg_entropy_of_first_answer": 1.2623729556798935, "lex_sim": 1.0}
{"question": "Which canal joins the North Sea at Brunsb\u00fcttel?", "answer": ["206 Canal du Nord"], "scores": [[0.02287455089390278, 0.05415628105401993, 0.39490818977355957, 0.009097378700971603, 0.003924136981368065, 0.006262053269892931, 0.3870943486690521]], "normalized_score": [0.03296845778822899], "tokens": [["\u2581", "2", "0", "6", "\u2581Canal", "\u2581du", "\u2581Nord"]], "entropy": [[4.55649995803833, 4.768826484680176, 2.993866443634033, 2.059176445007324, 3.9393935203552246, 2.5342445373535156, 2.0336620807647705]], "avg_entropy": [3.269381352833339], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nord-Ostsee-Kanal", "score_of_biggest_cluster": 0.03296845778822899, "score_of_first_answer": 0.03296845778822899, "max_score_of_first_answer": 0.39490818977355957, "min_score_of_first_answer": 0.003924136981368065, "avg_score": 0.03296845778822899, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1686849256018155, "std_of_first_answer": 0.1686849256018155, "avg_entropy_of_first_answer": 3.269381352833339, "lex_sim": 1.0}
{"question": "How many black keys are there on a standard modern piano?", "answer": ["Seven (7)"], "scores": [[0.023849446326494217, 0.03816985711455345, 0.05690212547779083, 0.9469200968742371]], "normalized_score": [0.08368741720914841], "tokens": [["\u2581Seven", "\u2581(", "7", ")"]], "entropy": [[3.7277071475982666, 1.3582338094711304, 5.430775165557861, 0.4581102132797241]], "avg_entropy": [2.7437065839767456], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "36", "score_of_biggest_cluster": 0.08368741720914841, "score_of_first_answer": 0.08368741720914841, "max_score_of_first_answer": 0.9469200968742371, "min_score_of_first_answer": 0.023849446326494217, "avg_score": 0.08368741720914841, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3930383938956227, "std_of_first_answer": 0.3930383938956227, "avg_entropy_of_first_answer": 2.7437065839767456, "lex_sim": 1.0}
{"question": "\"What musical has the characters Roxie Hart, Velma Kelly, Billy Flynn, Matron \"\"Mama\"\" Morton, Amos Hart, Mary Sunshine, Fred Casely, Mrs. Borusewicz, Harrison and Go-to-Hell Kitty Baxter?\"", "answer": ["Chicoine's Music Theory 3"], "scores": [[0.012194950133562088, 0.011146798729896545, 0.06894112378358841, 0.026407767087221146, 0.9740028977394104, 0.0069784983061254025, 0.011240523308515549, 0.04847915470600128, 0.06352056562900543]], "normalized_score": [0.03384212404489517], "tokens": [["\u2581Ch", "ico", "ine", "'", "s", "\u2581Music", "\u2581Theory", "\u2581", "3"]], "entropy": [[2.42897367477417, 4.1232991218566895, 3.7220630645751953, 3.442866325378418, 0.2275303602218628, 6.078873634338379, 4.350442409515381, 3.9353909492492676, 1.4472131729125977]], "avg_entropy": [3.3062947458691068], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chi-Beria", "score_of_biggest_cluster": 0.03384212404489517, "score_of_first_answer": 0.03384212404489517, "max_score_of_first_answer": 0.9740028977394104, "min_score_of_first_answer": 0.0069784983061254025, "avg_score": 0.03384212404489517, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2971712430916108, "std_of_first_answer": 0.2971712430916108, "avg_entropy_of_first_answer": 3.3062947458691068, "lex_sim": 1.0}
{"question": "What is the capital of Australia's Northern Territory?", "answer": ["Pleiades 3"], "scores": [[0.005152667872607708, 0.0139106260612607, 0.9303242564201355, 0.040636319667100906, 0.0160402562469244]], "normalized_score": [0.03369984030723572], "tokens": [["\u2581P", "lei", "ades", "\u2581", "3"]], "entropy": [[4.930013179779053, 6.049437522888184, 0.44158437848091125, 2.272615909576416, 0.7531588077545166]], "avg_entropy": [2.889361959695816], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Darvino", "score_of_biggest_cluster": 0.03369984030723572, "score_of_first_answer": 0.03369984030723572, "max_score_of_first_answer": 0.9303242564201355, "min_score_of_first_answer": 0.005152667872607708, "avg_score": 0.03369984030723572, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3647461782256705, "std_of_first_answer": 0.3647461782256705, "avg_entropy_of_first_answer": 2.889361959695816, "lex_sim": 1.0}
{"question": "Which local radio station broadcasts to Wigan and St. Helens on 102.4 FM?", "answer": ["Audience"], "scores": [[0.0052243005484342575, 0.025198206305503845, 0.19849687814712524]], "normalized_score": [0.029674526304006577], "tokens": [["\u2581A", "udi", "ence"]], "entropy": [[4.5733184814453125, 6.415962219238281, 4.182476997375488]], "avg_entropy": [5.057252566019694], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "102.4 Wish FM", "score_of_biggest_cluster": 0.029674526304006577, "score_of_first_answer": 0.029674526304006577, "max_score_of_first_answer": 0.19849687814712524, "min_score_of_first_answer": 0.0052243005484342575, "avg_score": 0.029674526304006577, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08678560801378707, "std_of_first_answer": 0.08678560801378707, "avg_entropy_of_first_answer": 5.057252566019694, "lex_sim": 1.0}
{"question": "In which city were the 1984 Summer Olympics held?", "answer": ["The Game of Thrones"], "scores": [[0.028254816308617592, 0.001946748117916286, 0.36885976791381836, 0.10317005962133408, 0.9424716830253601, 0.9994550347328186]], "normalized_score": [0.11198023706674576], "tokens": [["\u2581The", "\u2581Game", "\u2581of", "\u2581Th", "ron", "es"]], "entropy": [[4.517921447753906, 5.590480804443359, 2.496044158935547, 4.259002685546875, 0.4001791477203369, 0.006939783692359924]], "avg_entropy": [2.878428004682064], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "City of Los Angeles, California", "score_of_biggest_cluster": 0.11198023706674576, "score_of_first_answer": 0.11198023706674576, "max_score_of_first_answer": 0.9994550347328186, "min_score_of_first_answer": 0.001946748117916286, "avg_score": 0.11198023706674576, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4161360907736884, "std_of_first_answer": 0.4161360907736884, "avg_entropy_of_first_answer": 2.878428004682064, "lex_sim": 1.0}
{"question": "Who finished bottom of County Cricket\u2019s Division One this year?", "answer": ["David Willey"], "scores": [[0.002404874423518777, 0.08607267588376999, 0.97364342212677]], "normalized_score": [0.05862990394234657], "tokens": [["\u2581David", "\u2581Wille", "y"]], "entropy": [[5.300951957702637, 5.468500137329102, 0.17073772847652435]], "avg_entropy": [3.6467299411694207], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Worcestershire county", "score_of_biggest_cluster": 0.05862990394234657, "score_of_first_answer": 0.05862990394234657, "max_score_of_first_answer": 0.97364342212677, "min_score_of_first_answer": 0.002404874423518777, "avg_score": 0.05862990394234657, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4394550219852196, "std_of_first_answer": 0.4394550219852196, "avg_entropy_of_first_answer": 3.6467299411694207, "lex_sim": 1.0}
{"question": "What TV show takes place at the fictional Seattle Grace Hospital?", "answer": ["Grey's Anatomy"], "scores": [[0.3573519289493561, 0.8848189115524292, 0.9986476302146912, 0.9335373640060425, 0.9983934760093689, 0.9973199963569641]], "normalized_score": [0.8152143359184265], "tokens": [["\u2581Grey", "'", "s", "\u2581An", "atom", "y"]], "entropy": [[4.431129455566406, 0.7573164701461792, 0.01588490791618824, 0.4812554717063904, 0.017457326874136925, 0.029166091233491898]], "avg_entropy": [0.9553682872404655], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Seattle Grace Hospital", "score_of_biggest_cluster": 0.8152143359184265, "score_of_first_answer": 0.8152143359184265, "max_score_of_first_answer": 0.9986476302146912, "min_score_of_first_answer": 0.3573519289493561, "avg_score": 0.8152143359184265, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22945391987689848, "std_of_first_answer": 0.22945391987689848, "avg_entropy_of_first_answer": 0.9553682872404655, "lex_sim": 1.0}
{"question": "Which company merged with BOAC in 1974 to form British Airways?", "answer": ["Air Holdings"], "scores": [[0.022391367703676224, 0.03613928332924843, 0.9382070899009705]], "normalized_score": [0.09122621268033981], "tokens": [["\u2581Air", "\u2581Hold", "ings"]], "entropy": [[4.788590431213379, 4.875577449798584, 0.253905713558197]], "avg_entropy": [3.3060245315233865], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bea (disambiguation)", "score_of_biggest_cluster": 0.09122621268033981, "score_of_first_answer": 0.09122621268033981, "max_score_of_first_answer": 0.9382070899009705, "min_score_of_first_answer": 0.022391367703676224, "avg_score": 0.09122621268033981, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4285160142036594, "std_of_first_answer": 0.4285160142036594, "avg_entropy_of_first_answer": 3.3060245315233865, "lex_sim": 1.0}
{"question": "What is the pen name of Jim Grant, the British author who writes stories about the former U.S. Military Police Major, Jack Reacher?", "answer": ["Hesu Reachere"], "scores": [[0.003284931182861328, 0.004716399125754833, 0.0034142322838306427, 0.0057600983418524265, 0.026066388934850693, 0.00199639773927629]], "normalized_score": [0.00501223374158144], "tokens": [["\u2581H", "es", "u", "\u2581Re", "ach", "ere"]], "entropy": [[5.168935775756836, 5.658064842224121, 5.745343208312988, 4.630859851837158, 2.6218557357788086, 2.9987568855285645]], "avg_entropy": [4.470636049906413], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lee Child.", "score_of_biggest_cluster": 0.00501223374158144, "score_of_first_answer": 0.00501223374158144, "max_score_of_first_answer": 0.026066388934850693, "min_score_of_first_answer": 0.00199639773927629, "avg_score": 0.00501223374158144, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.008368860478831548, "std_of_first_answer": 0.008368860478831548, "avg_entropy_of_first_answer": 4.470636049906413, "lex_sim": 1.0}
{"question": "Who was European Footballer of the Year for 1973 and 1974?", "answer": ["Georgi Asparuhov"], "scores": [[0.005354076623916626, 0.3576847016811371, 0.5503337979316711, 0.9869396686553955, 0.4771381616592407, 0.9958726763725281]], "normalized_score": [0.2811846435070038], "tokens": [["\u2581Georg", "i", "\u2581As", "par", "uh", "ov"]], "entropy": [[5.321084022521973, 3.5545194149017334, 2.7605607509613037, 0.09727641940116882, 1.5416994094848633, 0.04461664706468582]], "avg_entropy": [2.2199594440559545], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hendrik Johannes Cruijff", "score_of_biggest_cluster": 0.2811846435070038, "score_of_first_answer": 0.2811846435070038, "max_score_of_first_answer": 0.9958726763725281, "min_score_of_first_answer": 0.005354076623916626, "avg_score": 0.2811846435070038, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3482709640871501, "std_of_first_answer": 0.3482709640871501, "avg_entropy_of_first_answer": 2.2199594440559545, "lex_sim": 1.0}
{"question": "How many square stickers are on each face of a classic Rubik\u2019s Cube?", "answer": ["9"], "scores": [[0.5043691992759705, 0.3264331817626953]], "normalized_score": [0.40576204657554626], "tokens": [["\u2581", "9"]], "entropy": [[3.4408321380615234, 2.2523226737976074]], "avg_entropy": [2.8465774059295654], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "9", "score_of_biggest_cluster": 0.40576204657554626, "score_of_first_answer": 0.40576204657554626, "max_score_of_first_answer": 0.5043691992759705, "min_score_of_first_answer": 0.3264331817626953, "avg_score": 0.40576204657554626, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08896800875663757, "std_of_first_answer": 0.08896800875663757, "avg_entropy_of_first_answer": 2.8465774059295654, "lex_sim": 1.0}
{"question": "Which internet site was founded in 1999 by Steve and Julie Pankhurst?", "answer": ["EGGDROP"], "scores": [[0.023882795125246048, 0.002736703958362341, 0.003662255359813571, 0.3408581614494324]], "normalized_score": [0.016900865361094475], "tokens": [["\u2581E", "GG", "D", "ROP"]], "entropy": [[6.075669288635254, 4.780427932739258, 3.8785877227783203, 2.5332233905792236]], "avg_entropy": [4.316977083683014], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "FRIENDS REUNITED", "score_of_biggest_cluster": 0.016900865361094475, "score_of_first_answer": 0.016900865361094475, "max_score_of_first_answer": 0.3408581614494324, "min_score_of_first_answer": 0.002736703958362341, "avg_score": 0.016900865361094475, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14347418452488173, "std_of_first_answer": 0.14347418452488173, "avg_entropy_of_first_answer": 4.316977083683014, "lex_sim": 1.0}
{"question": "For how many years was Richard III king of England?", "answer": ["I have found no results matching your query."], "scores": [[0.004133600275963545, 0.046986449509859085, 0.04147889465093613, 0.07216603308916092, 0.08319678902626038, 0.05983789265155792, 0.37249553203582764, 0.4056837260723114, 0.6289574503898621]], "normalized_score": [0.08664016425609589], "tokens": [["\u2581I", "\u2581have", "\u2581found", "\u2581no", "\u2581results", "\u2581matching", "\u2581your", "\u2581query", "."]], "entropy": [[3.956709861755371, 4.9805192947387695, 3.589632272720337, 3.825030565261841, 4.184167861938477, 2.141409158706665, 2.688933849334717, 2.1671791076660156, 1.5091880559921265]], "avg_entropy": [3.226974447568258], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2", "score_of_biggest_cluster": 0.08664016425609589, "score_of_first_answer": 0.08664016425609589, "max_score_of_first_answer": 0.6289574503898621, "min_score_of_first_answer": 0.004133600275963545, "avg_score": 0.08664016425609589, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20864554774892366, "std_of_first_answer": 0.20864554774892366, "avg_entropy_of_first_answer": 3.226974447568258, "lex_sim": 1.0}
{"question": "Which famous composer, though better known for other works, wrote the music for several films including 'First of the Few' and 'Henry the Fifth'?", "answer": ["Hubert de Burgh"], "scores": [[0.002461735624819994, 0.7419683933258057, 0.00758423050865531, 0.7074427008628845, 0.9801695942878723]], "normalized_score": [0.09919872879981995], "tokens": [["\u2581Hub", "ert", "\u2581de", "\u2581Burg", "h"]], "entropy": [[5.08188533782959, 1.6304049491882324, 2.8565452098846436, 1.9024560451507568, 0.16910523176193237]], "avg_entropy": [2.328079354763031], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Will Walton", "score_of_biggest_cluster": 0.09919872879981995, "score_of_first_answer": 0.09919872879981995, "max_score_of_first_answer": 0.9801695942878723, "min_score_of_first_answer": 0.002461735624819994, "avg_score": 0.09919872879981995, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4053227462610615, "std_of_first_answer": 0.4053227462610615, "avg_entropy_of_first_answer": 2.328079354763031, "lex_sim": 1.0}
{"question": "Which northern Duchies were questionable until the 1860s?", "answer": ["Schleswig-Holstein Question"], "scores": [[0.5372406840324402, 0.9821861982345581, 0.6246085166931152, 0.9774660468101501, 0.9923128485679626, 0.21184679865837097]], "normalized_score": [0.6384454369544983], "tokens": [["\u2581Schles", "wig", "-", "Hol", "stein", "\u2581Question"]], "entropy": [[3.4619994163513184, 0.17142944037914276, 1.480891466140747, 0.18584105372428894, 0.05809181183576584, 1.5501084327697754]], "avg_entropy": [1.1513936035335064], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "L\u00fctt-Witt Moor", "score_of_biggest_cluster": 0.6384454369544983, "score_of_first_answer": 0.6384454369544983, "max_score_of_first_answer": 0.9923128485679626, "min_score_of_first_answer": 0.21184679865837097, "avg_score": 0.6384454369544983, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2915188652171913, "std_of_first_answer": 0.2915188652171913, "avg_entropy_of_first_answer": 1.1513936035335064, "lex_sim": 1.0}
{"question": "JP Morgan, Barclays, RBS, Citigroup, Bank of America, and UBS were fined $6bn in 2015 for rigging what market?", "answer": ["LIBOR"], "scores": [[0.09010802954435349, 0.9764429926872253, 0.9943654537200928]], "normalized_score": [0.4439344108104706], "tokens": [["\u2581L", "IB", "OR"]], "entropy": [[4.572295188903809, 0.2354302704334259, 0.057096973061561584]], "avg_entropy": [1.6216074774662654], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Foreign exchanges", "score_of_biggest_cluster": 0.4439344108104706, "score_of_first_answer": 0.4439344108104706, "max_score_of_first_answer": 0.9943654537200928, "min_score_of_first_answer": 0.09010802954435349, "avg_score": 0.4439344108104706, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42211009236347247, "std_of_first_answer": 0.42211009236347247, "avg_entropy_of_first_answer": 1.6216074774662654, "lex_sim": 1.0}
{"question": "\"Who dubbed the singing voice for Audrey Hepburn in the film version of \"\"My Fair Lady\"\"?\"", "answer": ["MANCIPATE"], "scores": [[0.012311002239584923, 0.00885624997317791, 0.04060845077037811, 0.03108927235007286, 0.1652001589536667]], "normalized_score": [0.029604390263557434], "tokens": [["\u2581M", "AN", "C", "IP", "ATE"]], "entropy": [[4.156614303588867, 4.331578254699707, 4.589817047119141, 2.5702476501464844, 3.7554128170013428]], "avg_entropy": [3.880734014511108], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marnie Nixon", "score_of_biggest_cluster": 0.029604390263557434, "score_of_first_answer": 0.029604390263557434, "max_score_of_first_answer": 0.1652001589536667, "min_score_of_first_answer": 0.00885624997317791, "avg_score": 0.029604390263557434, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0579950909653107, "std_of_first_answer": 0.0579950909653107, "avg_entropy_of_first_answer": 3.880734014511108, "lex_sim": 1.0}
{"question": "English economist and physician Nicholas Barbon helped to pioneer which type of insurance in 1666?", "answer": ["Fire"], "scores": [[0.3685060441493988]], "normalized_score": [0.3685060441493988], "tokens": [["\u2581Fire"]], "entropy": [[3.7934799194335938]], "avg_entropy": [3.7934799194335938], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Buildings insurance", "score_of_biggest_cluster": 0.3685060441493988, "score_of_first_answer": 0.3685060441493988, "max_score_of_first_answer": 0.3685060441493988, "min_score_of_first_answer": 0.3685060441493988, "avg_score": 0.3685060441493988, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.7934799194335938, "lex_sim": 1.0}
{"question": "Which football team are nicknamed the Blaugrana?", "answer": ["Barrca"], "scores": [[0.05999210476875305, 0.002110424917191267, 0.10978558659553528]], "normalized_score": [0.02404380589723587], "tokens": [["\u2581Bar", "r", "ca"]], "entropy": [[3.597350597381592, 3.0998406410217285, 5.154012680053711]], "avg_entropy": [3.9504013061523438], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The weather in Barcelona", "score_of_biggest_cluster": 0.02404380589723587, "score_of_first_answer": 0.02404380589723587, "max_score_of_first_answer": 0.10978558659553528, "min_score_of_first_answer": 0.002110424917191267, "avg_score": 0.02404380589723587, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04399952037280294, "std_of_first_answer": 0.04399952037280294, "avg_entropy_of_first_answer": 3.9504013061523438, "lex_sim": 1.0}
{"question": "How many players are there in a Curling team?", "answer": ["Two"], "scores": [[0.04096301645040512]], "normalized_score": [0.04096301272511482], "tokens": [["\u2581Two"]], "entropy": [[4.058987140655518]], "avg_entropy": [4.058987140655518], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "FOUR (New Zealand TV Channel)", "score_of_biggest_cluster": 0.04096301272511482, "score_of_first_answer": 0.04096301272511482, "max_score_of_first_answer": 0.04096301645040512, "min_score_of_first_answer": 0.04096301645040512, "avg_score": 0.04096301272511482, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.058987140655518, "lex_sim": 1.0}
{"question": "Who in the Bible is the mother of Mary, the mother of Jesus?", "answer": ["St Anne"], "scores": [[0.09101051837205887, 0.23566624522209167]], "normalized_score": [0.14645171165466309], "tokens": [["\u2581St", "\u2581Anne"]], "entropy": [[5.044666290283203, 2.23230242729187]], "avg_entropy": [3.6384843587875366], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Anne", "score_of_biggest_cluster": 0.14645171165466309, "score_of_first_answer": 0.14645171165466309, "max_score_of_first_answer": 0.23566624522209167, "min_score_of_first_answer": 0.09101051837205887, "avg_score": 0.14645171165466309, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0723278634250164, "std_of_first_answer": 0.0723278634250164, "avg_entropy_of_first_answer": 3.6384843587875366, "lex_sim": 1.0}
{"question": "What creature's eye did the witches of Macbeth put into their cauldron?", "answer": ["Newt"], "scores": [[0.06727500259876251, 0.9556114673614502]], "normalized_score": [0.25355228781700134], "tokens": [["\u2581New", "t"]], "entropy": [[5.58332633972168, 0.3291693329811096]], "avg_entropy": [2.9562478363513947], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Newts", "score_of_biggest_cluster": 0.25355228781700134, "score_of_first_answer": 0.25355228781700134, "max_score_of_first_answer": 0.9556114673614502, "min_score_of_first_answer": 0.06727500259876251, "avg_score": 0.25355228781700134, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.44416823238134384, "std_of_first_answer": 0.44416823238134384, "avg_entropy_of_first_answer": 2.9562478363513947, "lex_sim": 1.0}
{"question": "What sport is known as \u201cthe sport of kings\u201d?", "answer": ["Horse racing"], "scores": [[0.30653461813926697, 0.8385138511657715, 0.6157939434051514]], "normalized_score": [0.5409308075904846], "tokens": [["\u2581Hor", "se", "\u2581racing"]], "entropy": [[4.2966814041137695, 0.534555196762085, 1.7012262344360352]], "avg_entropy": [2.17748761177063], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Scratch sheet", "score_of_biggest_cluster": 0.5409308075904846, "score_of_first_answer": 0.5409308075904846, "max_score_of_first_answer": 0.8385138511657715, "min_score_of_first_answer": 0.30653461813926697, "avg_score": 0.5409308075904846, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21813537895034033, "std_of_first_answer": 0.21813537895034033, "avg_entropy_of_first_answer": 2.17748761177063, "lex_sim": 1.0}
{"question": "The world's first publicly subscribed passenger railway, the Stockton and Darlington, opened in England with steam engines when?", "answer": ["1825 (157 year)"], "scores": [[0.5531473755836487, 0.7622699737548828, 0.8173345923423767, 0.8634958267211914, 0.9243409633636475, 0.033085305243730545, 0.1063324585556984, 0.027190983295440674, 0.014777625910937786, 0.00610222015529871, 0.11735627055168152]], "normalized_score": [0.13531392812728882], "tokens": [["\u2581", "1", "8", "2", "5", "\u2581(", "1", "5", "7", "\u2581year", ")"]], "entropy": [[3.3558778762817383, 1.1815797090530396, 1.047118902206421, 0.8705836534500122, 0.44125017523765564, 1.4979133605957031, 4.899789810180664, 2.3556270599365234, 3.478086233139038, 2.3834147453308105, 2.6721229553222656]], "avg_entropy": [2.1984876800667155], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "1825", "score_of_biggest_cluster": 0.13531392812728882, "score_of_first_answer": 0.13531392812728882, "max_score_of_first_answer": 0.9243409633636475, "min_score_of_first_answer": 0.00610222015529871, "avg_score": 0.13531392812728882, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3764844711623807, "std_of_first_answer": 0.3764844711623807, "avg_entropy_of_first_answer": 2.1984876800667155, "lex_sim": 1.0}
{"question": "When Alexander the Great had an Empire to which Asian Country did it stretch?", "answer": ["It"], "scores": [[0.0033782741520553827]], "normalized_score": [0.0033782748505473137], "tokens": [["\u2581It"]], "entropy": [[5.72043514251709]], "avg_entropy": [5.72043514251709], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u0d2d\u0d3e\u0d30\u0d24 \u0d2e\u0d39\u0d3e\u0d30\u0d3e\u0d1c\u0d4d\u0d2f\u0d02", "score_of_biggest_cluster": 0.0033782748505473137, "score_of_first_answer": 0.0033782748505473137, "max_score_of_first_answer": 0.0033782741520553827, "min_score_of_first_answer": 0.0033782741520553827, "avg_score": 0.0033782748505473137, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.72043514251709, "lex_sim": 1.0}
{"question": "What is the name of the enzyme usually added to milk to coagulate it, to make \"curds and whey\"?", "answer": ["Cheese"], "scores": [[0.004434246104210615, 0.8185809254646301]], "normalized_score": [0.06024772673845291], "tokens": [["\u2581Che", "ese"]], "entropy": [[4.534269332885742, 0.8471761345863342]], "avg_entropy": [2.690722733736038], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rennet.", "score_of_biggest_cluster": 0.06024772673845291, "score_of_first_answer": 0.06024772673845291, "max_score_of_first_answer": 0.8185809254646301, "min_score_of_first_answer": 0.004434246104210615, "avg_score": 0.06024772673845291, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40707333968020976, "std_of_first_answer": 0.40707333968020976, "avg_entropy_of_first_answer": 2.690722733736038, "lex_sim": 1.0}
{"question": "What Ford automobile came in any color you want - as long as it was black?", "answer": ["Model T"], "scores": [[0.25763455033302307, 0.9218741655349731]], "normalized_score": [0.4873465299606323], "tokens": [["\u2581Model", "\u2581T"]], "entropy": [[4.703039169311523, 0.5184386372566223]], "avg_entropy": [2.610738903284073], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Model-T", "score_of_biggest_cluster": 0.4873465299606323, "score_of_first_answer": 0.4873465299606323, "max_score_of_first_answer": 0.9218741655349731, "min_score_of_first_answer": 0.25763455033302307, "avg_score": 0.4873465299606323, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.33211980760097504, "std_of_first_answer": 0.33211980760097504, "avg_entropy_of_first_answer": 2.610738903284073, "lex_sim": 1.0}
{"question": "\"Whose autobiography was entitled \"\"Step Inside Love\"\"?\"", "answer": ["Cilla Black"], "scores": [[0.2854646146297455, 0.8588456511497498, 0.8832014203071594]], "normalized_score": [0.6004945635795593], "tokens": [["\u2581C", "illa", "\u2581Black"]], "entropy": [[4.8141021728515625, 0.7901883721351624, 0.7854629755020142]], "avg_entropy": [2.129917840162913], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Priscilla Maria Veronica Willis", "score_of_biggest_cluster": 0.6004945635795593, "score_of_first_answer": 0.6004945635795593, "max_score_of_first_answer": 0.8832014203071594, "min_score_of_first_answer": 0.2854646146297455, "avg_score": 0.6004945635795593, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2762141490851381, "std_of_first_answer": 0.2762141490851381, "avg_entropy_of_first_answer": 2.129917840162913, "lex_sim": 1.0}
{"question": "In which country is Lake Bala", "answer": ["WALES"], "scores": [[0.016577325761318207, 0.2521430253982544, 0.9542096853256226]], "normalized_score": [0.15858733654022217], "tokens": [["\u2581W", "A", "LES"]], "entropy": [[4.531503677368164, 4.227103233337402, 0.30762046575546265]], "avg_entropy": [3.0220757921536765], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Welsh nation", "score_of_biggest_cluster": 0.15858733654022217, "score_of_first_answer": 0.15858733654022217, "max_score_of_first_answer": 0.9542096853256226, "min_score_of_first_answer": 0.016577325761318207, "avg_score": 0.15858733654022217, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3982661359678887, "std_of_first_answer": 0.3982661359678887, "avg_entropy_of_first_answer": 3.0220757921536765, "lex_sim": 1.0}
{"question": "Who was the Georgia Peach?", "answer": ["Ty Cobb/Tyrus Cobb"], "scores": [[0.42894187569618225, 0.903427004814148, 0.9870941042900085, 0.00024444289738312364, 0.15857051312923431, 0.9868523478507996, 0.1272813081741333, 0.9949160814285278]], "normalized_score": [0.19207976758480072], "tokens": [["\u2581Ty", "\u2581C", "obb", "/", "T", "yrus", "\u2581C", "obb"]], "entropy": [[3.9588348865509033, 0.8618797063827515, 0.13399556279182434, 0.7607440948486328, 4.126009941101074, 0.12771405279636383, 1.6008522510528564, 0.05594142526388168]], "avg_entropy": [1.453246490098536], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Cobb, Ty", "score_of_biggest_cluster": 0.19207976758480072, "score_of_first_answer": 0.19207976758480072, "max_score_of_first_answer": 0.9949160814285278, "min_score_of_first_answer": 0.00024444289738312364, "avg_score": 0.19207976758480072, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4106641141687854, "std_of_first_answer": 0.4106641141687854, "avg_entropy_of_first_answer": 1.453246490098536, "lex_sim": 1.0}
{"question": "Oil from oil wells was first used for what?", "answer": ["To kill whales by drifting, and they would come to a frolic; it happened at Newfoundland."], "scores": [[0.014701778069138527, 0.014757344499230385, 0.05195513740181923, 0.9437538385391235, 0.006825133226811886, 0.004177771043032408, 0.09687883406877518, 0.006642403546720743, 0.060340654104948044, 0.006340938154608011, 0.06948887556791306, 0.02673931047320366, 0.29649561643600464, 0.007405465468764305, 0.004871569573879242, 0.04348896071314812, 0.004772441927343607, 0.031186413019895554, 0.0012088132789358497, 0.0741349384188652, 0.02395482547581196, 0.2873660624027252, 0.9883835911750793, 0.1999664157629013]], "normalized_score": [0.031831976026296616], "tokens": [["\u2581To", "\u2581kill", "\u2581wh", "ales", "\u2581by", "\u2581dr", "ifting", ",", "\u2581and", "\u2581they", "\u2581would", "\u2581come", "\u2581to", "\u2581a", "\u2581fro", "lic", ";", "\u2581it", "\u2581happened", "\u2581at", "\u2581New", "found", "land", "."]], "entropy": [[5.531834602355957, 4.391597270965576, 4.83437442779541, 0.32520774006843567, 1.3153624534606934, 5.029183387756348, 0.5011451244354248, 4.02295446395874, 5.521328926086426, 4.700241565704346, 4.720035552978516, 4.901475429534912, 2.7121596336364746, 2.3465890884399414, 4.660775184631348, 1.6007804870605469, 3.4077205657958984, 4.264533996582031, 2.6969614028930664, 3.5802156925201416, 4.699121475219727, 3.723546266555786, 0.11467397212982178, 2.770695686340332]], "avg_entropy": [3.432188099871079], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Evaporate brine to produce salt", "score_of_biggest_cluster": 0.031831976026296616, "score_of_first_answer": 0.031831976026296616, "max_score_of_first_answer": 0.9883835911750793, "min_score_of_first_answer": 0.0012088132789358497, "avg_score": 0.031831976026296616, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2633544346684121, "std_of_first_answer": 0.2633544346684121, "avg_entropy_of_first_answer": 3.432188099871079, "lex_sim": 1.0}
{"question": "In medicine, epiphora affects which part of the human body?", "answer": ["Lens (anatomy)"], "scores": [[0.011466925032436848, 0.020184297114610672, 0.14887107908725739, 0.5493683218955994, 0.9949654936790466, 0.9854491353034973, 0.9899654388427734]], "normalized_score": [0.21059919893741608], "tokens": [["\u2581L", "ens", "\u2581(", "an", "atom", "y", ")"]], "entropy": [[4.4703369140625, 4.221404075622559, 2.199387550354004, 2.2693681716918945, 0.04483875632286072, 0.07962941378355026, 0.0835469663143158]], "avg_entropy": [1.9097874068788119], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eye (anatomy)", "score_of_biggest_cluster": 0.21059919893741608, "score_of_first_answer": 0.21059919893741608, "max_score_of_first_answer": 0.9949654936790466, "min_score_of_first_answer": 0.011466925032436848, "avg_score": 0.21059919893741608, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43253480360453883, "std_of_first_answer": 0.43253480360453883, "avg_entropy_of_first_answer": 1.9097874068788119, "lex_sim": 1.0}
{"question": "What was the name of the British teacher jailed in Sudan in 2007 for allowing a teddy bear to be called Mohammed?", "answer": ["Gillian Gibbons"], "scores": [[0.1029420718550682, 0.9564712643623352, 0.7567754983901978, 0.9269453287124634, 0.98748779296875]], "normalized_score": [0.5844717621803284], "tokens": [["\u2581Gill", "ian", "\u2581Gib", "b", "ons"]], "entropy": [[5.9444580078125, 0.3933762311935425, 2.0394434928894043, 0.5710057020187378, 0.10916094481945038]], "avg_entropy": [1.811488875746727], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Muhammad (teddy bear)", "score_of_biggest_cluster": 0.5844717621803284, "score_of_first_answer": 0.5844717621803284, "max_score_of_first_answer": 0.98748779296875, "min_score_of_first_answer": 0.1029420718550682, "avg_score": 0.5844717621803284, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3313593976961401, "std_of_first_answer": 0.3313593976961401, "avg_entropy_of_first_answer": 1.811488875746727, "lex_sim": 1.0}
{"question": "The Scottish football club Ferranti Thistle joined the Scottish Football League in 1974 as Meadowbank Thistle; in 1995 they relocated and changed their name to what?", "answer": ["Dunfermline Athletic"], "scores": [[0.004650387447327375, 0.5855008959770203, 0.9346111416816711, 0.9885077476501465, 0.5925264358520508, 0.9988023042678833]], "normalized_score": [0.3379111588001251], "tokens": [["\u2581Dun", "fer", "ml", "ine", "\u2581Athlet", "ic"]], "entropy": [[6.0674543380737305, 2.612567663192749, 0.4353654384613037, 0.09515734016895294, 1.5795421600341797, 0.013195721432566643]], "avg_entropy": [1.800547110227247], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Livingston (disambiguation)", "score_of_biggest_cluster": 0.3379111588001251, "score_of_first_answer": 0.3379111588001251, "max_score_of_first_answer": 0.9988023042678833, "min_score_of_first_answer": 0.004650387447327375, "avg_score": 0.3379111588001251, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.349813456528447, "std_of_first_answer": 0.349813456528447, "avg_entropy_of_first_answer": 1.800547110227247, "lex_sim": 1.0}
{"question": "What was the middle name of the character 'Rodney Trotter' from the t.v. series 'Only Fools And Horses'?", "answer": ["Tracy"], "scores": [[0.09547042101621628, 0.004942477680742741]], "normalized_score": [0.021722348406910896], "tokens": [["\u2581T", "racy"]], "entropy": [[5.317953586578369, 1.4796736240386963]], "avg_entropy": [3.3988136053085327], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charlton (disambiguation)", "score_of_biggest_cluster": 0.021722348406910896, "score_of_first_answer": 0.021722348406910896, "max_score_of_first_answer": 0.09547042101621628, "min_score_of_first_answer": 0.004942477680742741, "avg_score": 0.021722348406910896, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04526397166773677, "std_of_first_answer": 0.04526397166773677, "avg_entropy_of_first_answer": 3.3988136053085327, "lex_sim": 1.0}
{"question": "Which group recorded the 90s albums 'Nevermind' and 'In Utero'?", "answer": ["Grunge"], "scores": [[0.019999206066131592, 0.8385407328605652]], "normalized_score": [0.129499614238739], "tokens": [["\u2581Gr", "unge"]], "entropy": [[4.8084588050842285, 1.066823124885559]], "avg_entropy": [2.937640964984894], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nirvana (Philosophical concept)", "score_of_biggest_cluster": 0.129499614238739, "score_of_first_answer": 0.129499614238739, "max_score_of_first_answer": 0.8385407328605652, "min_score_of_first_answer": 0.019999206066131592, "avg_score": 0.129499614238739, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4092707633972168, "std_of_first_answer": 0.4092707633972168, "avg_entropy_of_first_answer": 2.937640964984894, "lex_sim": 1.0}
{"question": "In which film does psychopath Annie Wilkes wield a sledgehammer to horrific effect?", "answer": ["Misery movie"], "scores": [[0.4738866984844208, 0.9947342872619629, 0.0017784781521186233]], "normalized_score": [0.09429240971803665], "tokens": [["\u2581Mis", "ery", "\u2581movie"]], "entropy": [[3.5792300701141357, 0.06034890562295914, 1.1661508083343506]], "avg_entropy": [1.6019099280238152], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Misery (disambiguation)", "score_of_biggest_cluster": 0.09429240971803665, "score_of_first_answer": 0.09429240971803665, "max_score_of_first_answer": 0.9947342872619629, "min_score_of_first_answer": 0.0017784781521186233, "avg_score": 0.09429240971803665, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4055352596378299, "std_of_first_answer": 0.4055352596378299, "avg_entropy_of_first_answer": 1.6019099280238152, "lex_sim": 1.0}
{"question": "The international multi-million dollar prizes set up in 2012 to recognise important recent achievements by scientists in (by 2015) Mathematics, Life Sciences, Fundamental Physics are known as what?", "answer": ["Breakthrough prizes"], "scores": [[0.08140642940998077, 0.972383975982666, 0.09312757849693298, 0.9920638203620911]], "normalized_score": [0.2924344837665558], "tokens": [["\u2581Break", "through", "\u2581pri", "zes"]], "entropy": [[6.013934135437012, 0.23425602912902832, 2.422757387161255, 0.07642732560634613]], "avg_entropy": [2.1868437193334103], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "The Breakthrough Prizes", "score_of_biggest_cluster": 0.2924344837665558, "score_of_first_answer": 0.2924344837665558, "max_score_of_first_answer": 0.9920638203620911, "min_score_of_first_answer": 0.08140642940998077, "avg_score": 0.2924344837665558, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.44755172419378986, "std_of_first_answer": 0.44755172419378986, "avg_entropy_of_first_answer": 2.1868437193334103, "lex_sim": 1.0}
{"question": "In musical terminology, what word describes sliding between two notes?", "answer": ["Legato"], "scores": [[0.01290551945567131, 0.9199820756912231]], "normalized_score": [0.10896259546279907], "tokens": [["\u2581Leg", "ato"]], "entropy": [[3.9479031562805176, 0.6524205207824707]], "avg_entropy": [2.300161838531494], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Glissando", "score_of_biggest_cluster": 0.10896259546279907, "score_of_first_answer": 0.10896259546279907, "max_score_of_first_answer": 0.9199820756912231, "min_score_of_first_answer": 0.01290551945567131, "avg_score": 0.10896259546279907, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4535382781177759, "std_of_first_answer": 0.4535382781177759, "avg_entropy_of_first_answer": 2.300161838531494, "lex_sim": 1.0}
{"question": "What is the first name of James bond villain Dr No?", "answer": ["Emile No"], "scores": [[0.03153374418616295, 0.06025055795907974, 0.005947270430624485]], "normalized_score": [0.0224397461861372], "tokens": [["\u2581Em", "ile", "\u2581No"]], "entropy": [[5.858172416687012, 3.2139837741851807, 2.6742725372314453]], "avg_entropy": [3.9154762427012124], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gens Iulia", "score_of_biggest_cluster": 0.0224397461861372, "score_of_first_answer": 0.0224397461861372, "max_score_of_first_answer": 0.06025055795907974, "min_score_of_first_answer": 0.005947270430624485, "avg_score": 0.0224397461861372, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02218149896976038, "std_of_first_answer": 0.02218149896976038, "avg_entropy_of_first_answer": 3.9154762427012124, "lex_sim": 1.0}
{"question": "Which country was once led by F.W. de Klerk?", "answer": ["South Africa"], "scores": [[0.43000340461730957, 0.8861806988716125]], "normalized_score": [0.6173011660575867], "tokens": [["\u2581South", "\u2581Africa"]], "entropy": [[3.9254071712493896, 0.6617845892906189]], "avg_entropy": [2.2935958802700043], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "South africa", "score_of_biggest_cluster": 0.6173011660575867, "score_of_first_answer": 0.6173011660575867, "max_score_of_first_answer": 0.8861806988716125, "min_score_of_first_answer": 0.43000340461730957, "avg_score": 0.6173011660575867, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2280886471271515, "std_of_first_answer": 0.2280886471271515, "avg_entropy_of_first_answer": 2.2935958802700043, "lex_sim": 1.0}
{"question": "\"From which musical does the song \"\"If I Ruled The World\"\" come from?\"", "answer": ["\"\"If I Ruled The World\"\", from the 1960 musical Pickwick by Cyril Ornadel and Leslie Bricusse, is based on The Pickwick Papers by Charles Dickens."], "scores": [[0.014725025743246078, 0.4756140112876892, 0.9856356382369995, 0.8791621327400208, 0.9998664855957031, 0.791278600692749, 0.9952995777130127, 0.23472219705581665, 0.12548266351222992, 0.18790870904922485, 0.6726078987121582, 0.07385440915822983, 0.9662913680076599, 0.9965929388999939, 0.8777317404747009, 0.0512857511639595, 0.5552911162376404, 0.2357034832239151, 0.9902237057685852, 0.07457288354635239, 0.8650654554367065, 0.8712368607521057, 0.9852033853530884, 0.9834461808204651, 0.9919956922531128, 0.7972022294998169, 0.69417804479599, 0.998664140701294, 0.9551612138748169, 0.9693909287452698, 0.999940037727356, 0.20501138269901276, 0.12593939900398254, 0.11896426975727081, 0.962898850440979, 0.347578227519989, 0.9446051120758057, 0.9992130994796753, 0.9962843060493469, 0.9997852444648743, 0.6527718305587769, 0.9394663572311401, 0.9981116056442261, 0.9975124597549438, 0.5340449810028076]], "normalized_score": [0.5091540217399597], "tokens": [["\u2581\"\"", "If", "\u2581I", "\u2581Ru", "led", "\u2581The", "\u2581World", "\"", "\",", "\u2581from", "\u2581the", "\u2581", "1", "9", "6", "0", "\u2581musical", "\u2581Pick", "wick", "\u2581by", "\u2581Cy", "ril", "\u2581Or", "n", "adel", "\u2581and", "\u2581Les", "lie", "\u2581Br", "icus", "se", ",", "\u2581is", "\u2581based", "\u2581on", "\u2581The", "\u2581Pick", "wick", "\u2581Pap", "ers", "\u2581by", "\u2581Charles", "\u2581Dick", "ens", "."]], "entropy": [[4.996314525604248, 2.4058847427368164, 0.12609946727752686, 0.6362345218658447, 0.0017925635911524296, 0.5354452133178711, 0.04347366839647293, 0.968392550945282, 3.1519508361816406, 4.400181770324707, 1.6831947565078735, 0.9999834299087524, 0.18694956600666046, 0.026846088469028473, 0.5961793661117554, 1.354607343673706, 1.6159086227416992, 2.9362354278564453, 0.08560418337583542, 2.047189235687256, 0.7631576061248779, 0.7891148328781128, 0.13431978225708008, 0.14931319653987885, 0.09051342308521271, 0.9439330101013184, 1.422342300415039, 0.01715962588787079, 0.2837943434715271, 0.18316000699996948, 0.0008501408156007528, 1.8209612369537354, 3.558258533477783, 3.135601282119751, 0.21949845552444458, 2.086660385131836, 0.4217185378074646, 0.008582236245274544, 0.034979820251464844, 0.002655980410054326, 1.4584271907806396, 0.3036957383155823, 0.01954043284058571, 0.023711057379841805, 1.4639670848846436]], "avg_entropy": [1.0696529804728925], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "PICKWICK", "score_of_biggest_cluster": 0.5091540217399597, "score_of_first_answer": 0.5091540217399597, "max_score_of_first_answer": 0.999940037727356, "min_score_of_first_answer": 0.014725025743246078, "avg_score": 0.5091540217399597, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.35488559740892756, "std_of_first_answer": 0.35488559740892756, "avg_entropy_of_first_answer": 1.0696529804728925, "lex_sim": 1.0}
{"question": "What is the only natural food that never goes bad?", "answer": ["Water (it can be filtered and distilled but never goes bad)"], "scores": [[0.0049975099973380566, 0.024716928601264954, 0.020808368921279907, 0.033665116876363754, 0.2600838243961334, 0.0076994942501187325, 0.17359541356563568, 0.03425430878996849, 0.9864016175270081, 0.017267227172851562, 0.12962889671325684, 0.24615482985973358, 0.8922538161277771, 0.708512008190155]], "normalized_score": [0.07876953482627869], "tokens": [["\u2581Water", "\u2581(", "it", "\u2581can", "\u2581be", "\u2581filtered", "\u2581and", "\u2581dist", "illed", "\u2581but", "\u2581never", "\u2581goes", "\u2581bad", ")"]], "entropy": [[4.631400108337402, 2.4558300971984863, 5.6351213455200195, 3.191573143005371, 3.382967472076416, 4.726258277893066, 3.06840181350708, 3.9005041122436523, 0.11170460283756256, 2.6679060459136963, 3.5717666149139404, 3.1996023654937744, 0.6964190006256104, 1.7042070627212524]], "avg_entropy": [3.0674044330205237], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Finland honey", "score_of_biggest_cluster": 0.07876953482627869, "score_of_first_answer": 0.07876953482627869, "max_score_of_first_answer": 0.9864016175270081, "min_score_of_first_answer": 0.0049975099973380566, "avg_score": 0.07876953482627869, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3332596520313137, "std_of_first_answer": 0.3332596520313137, "avg_entropy_of_first_answer": 3.0674044330205237, "lex_sim": 1.0}
{"question": "Which Disney character is brought to life by the Blue Fairy ?", "answer": ["Pinocchio (character)"], "scores": [[0.6305103898048401, 0.890684187412262, 0.9726501703262329, 0.20591627061367035, 0.2780021131038666, 0.9755202531814575]], "normalized_score": [0.5589736104011536], "tokens": [["\u2581Pin", "oc", "chio", "\u2581(", "character", ")"]], "entropy": [[2.922872543334961, 0.6445802450180054, 0.1876070499420166, 1.2502509355545044, 3.1143178939819336, 0.17456193268299103]], "avg_entropy": [1.3823651000857353], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Pin-Occhio", "score_of_biggest_cluster": 0.5589736104011536, "score_of_first_answer": 0.5589736104011536, "max_score_of_first_answer": 0.9755202531814575, "min_score_of_first_answer": 0.20591627061367035, "avg_score": 0.5589736104011536, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.31714741332435664, "std_of_first_answer": 0.31714741332435664, "avg_entropy_of_first_answer": 1.3823651000857353, "lex_sim": 1.0}
{"question": "Who wrote the novels About A Boy, How To Be Good and High Fidelity?", "answer": ["Nick Hornby"], "scores": [[0.36088162660598755, 0.8183183670043945, 0.910171627998352]], "normalized_score": [0.6453620791435242], "tokens": [["\u2581Nick", "\u2581Horn", "by"]], "entropy": [[4.758512020111084, 1.518622636795044, 0.49339374899864197]], "avg_entropy": [2.2568428019682565], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Nick Hornby", "score_of_biggest_cluster": 0.6453620791435242, "score_of_first_answer": 0.6453620791435242, "max_score_of_first_answer": 0.910171627998352, "min_score_of_first_answer": 0.36088162660598755, "avg_score": 0.6453620791435242, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.24023250280860264, "std_of_first_answer": 0.24023250280860264, "avg_entropy_of_first_answer": 2.2568428019682565, "lex_sim": 1.0}
{"question": "In 2003, what character was voted by the American Film Institute to be the greatest hero in American film?", "answer": ["Will Turner"], "scores": [[0.006673399358987808, 0.06993035227060318]], "normalized_score": [0.02160261943936348], "tokens": [["\u2581Will", "\u2581Turner"]], "entropy": [[6.039372444152832, 4.51354455947876]], "avg_entropy": [5.276458501815796], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Atticus Finch (character)", "score_of_biggest_cluster": 0.02160261943936348, "score_of_first_answer": 0.02160261943936348, "max_score_of_first_answer": 0.06993035227060318, "min_score_of_first_answer": 0.006673399358987808, "avg_score": 0.02160261943936348, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.031628476455807686, "std_of_first_answer": 0.031628476455807686, "avg_entropy_of_first_answer": 5.276458501815796, "lex_sim": 1.0}
{"question": "What type of animal is a bonobo?", "answer": ["Monkey"], "scores": [[0.027494003996253014, 0.8165618777275085]], "normalized_score": [0.14983507990837097], "tokens": [["\u2581Mon", "key"]], "entropy": [[4.428472995758057, 0.7036105990409851]], "avg_entropy": [2.566041797399521], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Apes", "score_of_biggest_cluster": 0.14983507990837097, "score_of_first_answer": 0.14983507990837097, "max_score_of_first_answer": 0.8165618777275085, "min_score_of_first_answer": 0.027494003996253014, "avg_score": 0.14983507990837097, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39453393686562777, "std_of_first_answer": 0.39453393686562777, "avg_entropy_of_first_answer": 2.566041797399521, "lex_sim": 1.0}
{"question": "Which fictional cartoon character, and leader of a gang, lives in Hoagy\u2019s Alley?", "answer": ["Hoodwinked!"], "scores": [[0.01906108483672142, 0.08504805713891983, 0.2011009007692337, 0.9700852036476135, 0.8452084064483643, 0.24402685463428497, 0.43965256214141846, 0.6835319399833679, 0.9532503485679626]], "normalized_score": [0.298272043466568], "tokens": [["\u2581H", "ood", "w", "ink", "ed", "!", "\u2581(", "film", ")"]], "entropy": [[5.485747337341309, 3.996325731277466, 3.1347341537475586, 0.18353673815727234, 0.9276524782180786, 2.4020187854766846, 1.6514477729797363, 1.6489332914352417, 0.2765597403049469]], "avg_entropy": [2.1896617809931436], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brain (Top Cat character", "score_of_biggest_cluster": 0.298272043466568, "score_of_first_answer": 0.298272043466568, "max_score_of_first_answer": 0.9700852036476135, "min_score_of_first_answer": 0.01906108483672142, "avg_score": 0.298272043466568, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3560389574293085, "std_of_first_answer": 0.3560389574293085, "avg_entropy_of_first_answer": 2.1896617809931436, "lex_sim": 1.0}
{"question": "\"What purple berry from South America has been promoted as a \"\"superfood\"\"?\"", "answer": ["\"Durian\""], "scores": [[0.034914977848529816, 0.00785070564597845, 0.024575771763920784, 0.8864890336990356, 0.502953052520752]], "normalized_score": [0.07861866801977158], "tokens": [["\u2581\"", "D", "ur", "ian", "\""]], "entropy": [[4.535348415374756, 4.139263153076172, 4.7342329025268555, 0.8950701355934143, 2.322171688079834]], "avg_entropy": [3.3252172589302065], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Assai palm", "score_of_biggest_cluster": 0.07861866801977158, "score_of_first_answer": 0.07861866801977158, "max_score_of_first_answer": 0.8864890336990356, "min_score_of_first_answer": 0.00785070564597845, "avg_score": 0.07861866801977158, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3510742315748408, "std_of_first_answer": 0.3510742315748408, "avg_entropy_of_first_answer": 3.3252172589302065, "lex_sim": 1.0}
{"question": "Who's home runs record did Mark McGwire break in the 1998 season?", "answer": ["RBI"], "scores": [[0.006898854859173298, 0.023273034021258354]], "normalized_score": [0.01267111673951149], "tokens": [["\u2581R", "BI"]], "entropy": [[4.547527313232422, 5.712454795837402]], "avg_entropy": [5.129991054534912], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Roger Eugene Maras", "score_of_biggest_cluster": 0.01267111673951149, "score_of_first_answer": 0.01267111673951149, "max_score_of_first_answer": 0.023273034021258354, "min_score_of_first_answer": 0.006898854859173298, "avg_score": 0.01267111673951149, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.008187089581042528, "std_of_first_answer": 0.008187089581042528, "avg_entropy_of_first_answer": 5.129991054534912, "lex_sim": 1.0}
{"question": "Rugby League - which country are the current holders of the European Cup?", "answer": ["Wales (Nationality)"], "scores": [[0.055792730301618576, 0.044683340936899185, 0.004160305485129356, 0.021942220628261566, 0.9402852058410645]], "normalized_score": [0.04635298624634743], "tokens": [["\u2581Wales", "\u2581(", "National", "ity", ")"]], "entropy": [[5.381534576416016, 1.254128336906433, 3.3449018001556396, 1.7043567895889282, 0.484390527009964]], "avg_entropy": [2.433862406015396], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Welsh nation", "score_of_biggest_cluster": 0.04635298624634743, "score_of_first_answer": 0.04635298624634743, "max_score_of_first_answer": 0.9402852058410645, "min_score_of_first_answer": 0.004160305485129356, "avg_score": 0.04635298624634743, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.36389690636511846, "std_of_first_answer": 0.36389690636511846, "avg_entropy_of_first_answer": 2.433862406015396, "lex_sim": 1.0}
{"question": "By what first name is the wife of Tommy Beresford known in the Partners in Crime stories by Agatha Christie, recently adapted by BBC TV?", "answer": ["Touie Mallow"], "scores": [[0.020282136276364326, 0.035944532603025436, 0.11110842972993851, 0.0007015933515504003, 0.05033262446522713]], "normalized_score": [0.019556263461709023], "tokens": [["\u2581T", "ou", "ie", "\u2581M", "allow"]], "entropy": [[5.721927642822266, 5.440037727355957, 4.571903228759766, 1.4432915449142456, 4.58204460144043]], "avg_entropy": [4.351840949058532], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tuppence", "score_of_biggest_cluster": 0.019556263461709023, "score_of_first_answer": 0.019556263461709023, "max_score_of_first_answer": 0.11110842972993851, "min_score_of_first_answer": 0.0007015933515504003, "avg_score": 0.019556263461709023, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.03753742473996646, "std_of_first_answer": 0.03753742473996646, "avg_entropy_of_first_answer": 4.351840949058532, "lex_sim": 1.0}
{"question": "George Bush, American president 2001 to 2009, had what middle name?", "answer": ["Walker"], "scores": [[0.2824324667453766]], "normalized_score": [0.2824324667453766], "tokens": [["\u2581Walker"]], "entropy": [[5.268779277801514]], "avg_entropy": [5.268779277801514], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alick Walker", "score_of_biggest_cluster": 0.2824324667453766, "score_of_first_answer": 0.2824324667453766, "max_score_of_first_answer": 0.2824324667453766, "min_score_of_first_answer": 0.2824324667453766, "avg_score": 0.2824324667453766, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.268779277801514, "lex_sim": 1.0}
{"question": "Which actress played The Sculptress on TV?", "answer": ["Caron Joghires"], "scores": [[0.03605521097779274, 0.0038844533264636993, 0.004905030131340027, 0.002512171398848295, 0.18336427211761475, 0.0034654694609344006]], "normalized_score": [0.01015494391322136], "tokens": [["\u2581Car", "on", "\u2581J", "og", "h", "ires"]], "entropy": [[5.745431423187256, 3.228425979614258, 4.929983139038086, 3.983598232269287, 4.528916358947754, 2.578186273574829]], "avg_entropy": [4.165756901105245], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "PAULINE QUIRKE", "score_of_biggest_cluster": 0.01015494391322136, "score_of_first_answer": 0.01015494391322136, "max_score_of_first_answer": 0.18336427211761475, "min_score_of_first_answer": 0.002512171398848295, "avg_score": 0.01015494391322136, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06562436301646145, "std_of_first_answer": 0.06562436301646145, "avg_entropy_of_first_answer": 4.165756901105245, "lex_sim": 1.0}
{"question": "The UK TV series, All Ceatures Great and Small, concerned what area of activity?", "answer": ["RSPCA"], "scores": [[0.014127278700470924, 0.11625442653894424, 0.9302370548248291]], "normalized_score": [0.11517384648323059], "tokens": [["\u2581R", "SP", "CA"]], "entropy": [[4.777817726135254, 1.7244787216186523, 0.5277512073516846]], "avg_entropy": [2.3433492183685303], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Veterinary Medical Doctor", "score_of_biggest_cluster": 0.11517384648323059, "score_of_first_answer": 0.11517384648323059, "max_score_of_first_answer": 0.9302370548248291, "min_score_of_first_answer": 0.014127278700470924, "avg_score": 0.11517384648323059, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4099125645776236, "std_of_first_answer": 0.4099125645776236, "avg_entropy_of_first_answer": 2.3433492183685303, "lex_sim": 1.0}
{"question": "Who is the famous brother of Emilio Estevez?", "answer": ["Shia Saide"], "scores": [[0.004260520450770855, 0.11453398317098618, 0.032409679144620895, 0.5565174221992493]], "normalized_score": [0.054467517882585526], "tokens": [["\u2581Sh", "ia", "\u2581Sa", "ide"]], "entropy": [[4.787365913391113, 4.564117431640625, 2.392965316772461, 2.3088579177856445]], "avg_entropy": [3.513326644897461], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bi-winning", "score_of_biggest_cluster": 0.054467517882585526, "score_of_first_answer": 0.054467517882585526, "max_score_of_first_answer": 0.5565174221992493, "min_score_of_first_answer": 0.004260520450770855, "avg_score": 0.054467517882585526, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2228680923300244, "std_of_first_answer": 0.2228680923300244, "avg_entropy_of_first_answer": 3.513326644897461, "lex_sim": 1.0}
{"question": "Which actress left Eastenders for Hollywood only to flop as The Bionic Woman?", "answer": ["Jane Lynch"], "scores": [[0.005728449672460556, 0.017152123153209686, 0.9135369658470154]], "normalized_score": [0.0447741262614727], "tokens": [["\u2581Jane", "\u2581Lyn", "ch"]], "entropy": [[4.452741622924805, 4.437675476074219, 0.4891633987426758]], "avg_entropy": [3.1265268325805664], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Michelle Ryan", "score_of_biggest_cluster": 0.0447741262614727, "score_of_first_answer": 0.0447741262614727, "max_score_of_first_answer": 0.9135369658470154, "min_score_of_first_answer": 0.005728449672460556, "avg_score": 0.0447741262614727, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42527802521357727, "std_of_first_answer": 0.42527802521357727, "avg_entropy_of_first_answer": 3.1265268325805664, "lex_sim": 1.0}
{"question": "In Greek mythology what was the name of the goatlike man with pointed ears, horns and a tail?", "answer": ["Pan"], "scores": [[0.08915167301893234]], "normalized_score": [0.08915168046951294], "tokens": [["\u2581Pan"]], "entropy": [[5.187376022338867]], "avg_entropy": [5.187376022338867], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Saturos", "score_of_biggest_cluster": 0.08915168046951294, "score_of_first_answer": 0.08915168046951294, "max_score_of_first_answer": 0.08915167301893234, "min_score_of_first_answer": 0.08915167301893234, "avg_score": 0.08915168046951294, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.187376022338867, "lex_sim": 1.0}
{"question": "When did the Canadian Charter of Rights and Freedoms, the bill of rights entrenched in Canadian law as part of the Constitution Act, come into force?", "answer": ["December 26, 1982"], "scores": [[0.014849514700472355, 0.9514617323875427, 0.0682939738035202, 0.07141906768083572, 0.7080524563789368, 0.9827908277511597, 0.7999024987220764, 0.9759212136268616, 0.5694414973258972, 0.8348900675773621]], "normalized_score": [0.3349929749965668], "tokens": [["\u2581December", "\u2581", "2", "6", ",", "\u2581", "1", "9", "8", "2"]], "entropy": [[3.669757127761841, 0.3484184741973877, 0.7115080952644348, 2.397343873977661, 1.1448371410369873, 0.15223437547683716, 0.5305415391921997, 0.15270549058914185, 1.4846340417861938, 0.7965096831321716]], "avg_entropy": [1.1388489842414855], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and eighty-two", "score_of_biggest_cluster": 0.3349929749965668, "score_of_first_answer": 0.3349929749965668, "max_score_of_first_answer": 0.9827908277511597, "min_score_of_first_answer": 0.014849514700472355, "avg_score": 0.3349929749965668, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37730250861396303, "std_of_first_answer": 0.37730250861396303, "avg_entropy_of_first_answer": 1.1388489842414855, "lex_sim": 1.0}
{"question": "At the beginning of 'Two Towers', the second book in 'The Lord Of The Rings' trilogy, who was apparently killed in the 'Mines of Moria'?", "answer": ["Boromir"], "scores": [[0.07570349425077438, 0.925990879535675, 0.9757319092750549]], "normalized_score": [0.4089633524417877], "tokens": [["\u2581Bor", "om", "ir"]], "entropy": [[5.164278507232666, 0.6015141010284424, 0.2274697721004486]], "avg_entropy": [1.9977541267871857], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tharkun", "score_of_biggest_cluster": 0.4089633524417877, "score_of_first_answer": 0.4089633524417877, "max_score_of_first_answer": 0.9757319092750549, "min_score_of_first_answer": 0.07570349425077438, "avg_score": 0.4089633524417877, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41305285584095053, "std_of_first_answer": 0.41305285584095053, "avg_entropy_of_first_answer": 1.9977541267871857, "lex_sim": 1.0}
{"question": "Also called The Roodee, which, officially, is Britain\u2019s oldest and shortest racecourse?", "answer": ["\ud83c\udf69"], "scores": [[0.03988727182149887, 0.026083074510097504, 0.8476679921150208, 0.055929433554410934, 0.013171101920306683]], "normalized_score": [0.057881034910678864], "tokens": [["\u2581", "<0xF0>", "<0x9F>", "<0x8D>", "<0xA9>"]], "entropy": [[4.214346885681152, 4.203457832336426, 0.7631567120552063, 3.0512003898620605, 3.915846824645996]], "avg_entropy": [3.229601728916168], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "County Borough of Chester", "score_of_biggest_cluster": 0.057881034910678864, "score_of_first_answer": 0.057881034910678864, "max_score_of_first_answer": 0.8476679921150208, "min_score_of_first_answer": 0.013171101920306683, "avg_score": 0.057881034910678864, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32587076784696417, "std_of_first_answer": 0.32587076784696417, "avg_entropy_of_first_answer": 3.229601728916168, "lex_sim": 0.0}
{"question": "In which London magazine did Sherlock Holmes first appear?", "answer": ["The Strand Magazine"], "scores": [[0.1927681714296341, 0.8303494453430176, 0.996980607509613, 0.5666686296463013]], "normalized_score": [0.5483754873275757], "tokens": [["\u2581The", "\u2581Str", "and", "\u2581Magazine"]], "entropy": [[4.093747138977051, 1.4582462310791016, 0.030533574521541595, 1.3459687232971191]], "avg_entropy": [1.7321239169687033], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Strand (disambiguation)", "score_of_biggest_cluster": 0.5483754873275757, "score_of_first_answer": 0.5483754873275757, "max_score_of_first_answer": 0.996980607509613, "min_score_of_first_answer": 0.1927681714296341, "avg_score": 0.5483754873275757, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3036785914941004, "std_of_first_answer": 0.3036785914941004, "avg_entropy_of_first_answer": 1.7321239169687033, "lex_sim": 1.0}
{"question": "\"Director James Cameron, collected the 'Oscar' for \"\"Best Director\"\" in 1998, for which film?\"", "answer": ["Leonardo DiCaprio"], "scores": [[0.002346351044252515, 0.9188323616981506, 0.6441465616226196, 0.934901773929596, 0.9898860454559326]], "normalized_score": [0.26411500573158264], "tokens": [["\u2581Leon", "ardo", "\u2581Di", "Cap", "rio"]], "entropy": [[3.942460060119629, 0.7156732082366943, 1.919323444366455, 0.35643911361694336, 0.09068775177001953]], "avg_entropy": [1.4049167156219482], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "S.S. Titanic", "score_of_biggest_cluster": 0.26411500573158264, "score_of_first_answer": 0.26411500573158264, "max_score_of_first_answer": 0.9898860454559326, "min_score_of_first_answer": 0.002346351044252515, "avg_score": 0.26411500573158264, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36794592960611705, "std_of_first_answer": 0.36794592960611705, "avg_entropy_of_first_answer": 1.4049167156219482, "lex_sim": 1.0}
{"question": "Which London football club won the last FA cup final held at the old Wembley Stadium?", "answer": ["West Ham"], "scores": [[0.03305605798959732, 0.8147990703582764]], "normalized_score": [0.16411595046520233], "tokens": [["\u2581West", "\u2581Ham"]], "entropy": [[4.368605613708496, 1.2998936176300049]], "avg_entropy": [2.8342496156692505], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Pensioners", "score_of_biggest_cluster": 0.16411595046520233, "score_of_first_answer": 0.16411595046520233, "max_score_of_first_answer": 0.8147990703582764, "min_score_of_first_answer": 0.03305605798959732, "avg_score": 0.16411595046520233, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3908715061843395, "std_of_first_answer": 0.3908715061843395, "avg_entropy_of_first_answer": 2.8342496156692505, "lex_sim": 1.0}
{"question": "What is the offspring of a male lion and a tigress called?", "answer": ["Lionking"], "scores": [[0.1581832319498062, 0.002134916605427861]], "normalized_score": [0.018376832827925682], "tokens": [["\u2581Lion", "king"]], "entropy": [[4.524846076965332, 4.656795024871826]], "avg_entropy": [4.590820550918579], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ligers", "score_of_biggest_cluster": 0.018376832827925682, "score_of_first_answer": 0.018376832827925682, "max_score_of_first_answer": 0.1581832319498062, "min_score_of_first_answer": 0.002134916605427861, "avg_score": 0.018376832827925682, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07802415767218918, "std_of_first_answer": 0.07802415767218918, "avg_entropy_of_first_answer": 4.590820550918579, "lex_sim": 1.0}
{"question": "Born in Kingston, Texas in 1925, who is generally recognized as the most decorated American soldier of WWII, before he launched a successful film career?", "answer": ["Audie Murphy - a World War II hero and actor"], "scores": [[0.6788711547851562, 0.9688039422035217, 0.6774962544441223, 0.9964711666107178, 0.0013303544837981462, 0.010249710641801357, 0.009826157242059708, 0.9689896106719971, 0.9267826080322266, 0.24240079522132874, 0.12513577938079834, 0.2359369546175003]], "normalized_score": [0.1641366332769394], "tokens": [["\u2581Aud", "ie", "\u2581Mur", "phy", "\u2581-", "\u2581a", "\u2581World", "\u2581War", "\u2581II", "\u2581hero", "\u2581and", "\u2581actor"]], "entropy": [[2.562238931655884, 0.24358205497264862, 1.5939218997955322, 0.03371450677514076, 0.5925626158714294, 5.5281476974487305, 5.518830299377441, 0.2721371650695801, 0.43790021538734436, 3.055795669555664, 2.640515089035034, 3.5331954956054688]], "avg_entropy": [2.167711803379158], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Audie Murphy", "score_of_biggest_cluster": 0.1641366332769394, "score_of_first_answer": 0.1641366332769394, "max_score_of_first_answer": 0.9964711666107178, "min_score_of_first_answer": 0.0013303544837981462, "avg_score": 0.1641366332769394, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4015791342876905, "std_of_first_answer": 0.4015791342876905, "avg_entropy_of_first_answer": 2.167711803379158, "lex_sim": 1.0}
{"question": "Prior to Viginia Wade in 1977 who was the last British player to win a tennis Grand Slam title?", "answer": ["Dorothy Round"], "scores": [[0.0063193063251674175, 0.7324428558349609, 0.3435354232788086]], "normalized_score": [0.11671808362007141], "tokens": [["\u2581Dor", "othy", "\u2581Round"]], "entropy": [[4.862652778625488, 0.9148344993591309, 3.956416130065918]], "avg_entropy": [3.244634469350179], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sue Barker", "score_of_biggest_cluster": 0.11671808362007141, "score_of_first_answer": 0.11671808362007141, "max_score_of_first_answer": 0.7324428558349609, "min_score_of_first_answer": 0.0063193063251674175, "avg_score": 0.11671808362007141, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2966889710310954, "std_of_first_answer": 0.2966889710310954, "avg_entropy_of_first_answer": 3.244634469350179, "lex_sim": 1.0}
{"question": "Charlotte of Mecklenburg-Strelitz was the wife of which King of Great Britain?", "answer": ["A"], "scores": [[0.0053643761202692986]], "normalized_score": [0.005364375188946724], "tokens": [["\u2581A"]], "entropy": [[3.169112205505371]], "avg_entropy": [3.169112205505371], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George III, King of Great Britain and Ireland", "score_of_biggest_cluster": 0.005364375188946724, "score_of_first_answer": 0.005364375188946724, "max_score_of_first_answer": 0.0053643761202692986, "min_score_of_first_answer": 0.0053643761202692986, "avg_score": 0.005364375188946724, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.169112205505371, "lex_sim": 1.0}
{"question": "In chemistry, what are elements called when they have a greater atomic number than uranium?", "answer": ["Transactinides"], "scores": [[0.382049560546875, 0.13145899772644043, 0.9373328685760498, 0.31810253858566284]], "normalized_score": [0.34981846809387207], "tokens": [["\u2581Trans", "act", "in", "ides"]], "entropy": [[3.9411301612854004, 1.635798692703247, 0.4143645167350769, 0.9507827162742615]], "avg_entropy": [1.7355190217494965], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Transuranium", "score_of_biggest_cluster": 0.34981846809387207, "score_of_first_answer": 0.34981846809387207, "max_score_of_first_answer": 0.9373328685760498, "min_score_of_first_answer": 0.13145899772644043, "avg_score": 0.34981846809387207, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.30030600150209263, "std_of_first_answer": 0.30030600150209263, "avg_entropy_of_first_answer": 1.7355190217494965, "lex_sim": 1.0}
{"question": "What sporting term, meaning a second chance, comes from the French word for fishing again?", "answer": ["Mulligan?"], "scores": [[0.23873066902160645, 0.8093616962432861, 0.9438266158103943, 0.0012159703765064478]], "normalized_score": [0.12202996015548706], "tokens": [["\u2581M", "ull", "igan", "?"]], "entropy": [[5.0363054275512695, 1.5672178268432617, 0.4324291944503784, 0.8653197288513184]], "avg_entropy": [1.975318044424057], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Rep\u00eachage", "score_of_biggest_cluster": 0.12202996015548706, "score_of_first_answer": 0.12202996015548706, "max_score_of_first_answer": 0.9438266158103943, "min_score_of_first_answer": 0.0012159703765064478, "avg_score": 0.12202996015548706, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.39042352429723215, "std_of_first_answer": 0.39042352429723215, "avg_entropy_of_first_answer": 1.975318044424057, "lex_sim": 1.0}
{"question": "Which RAF fighter-bomber was built by the Panavia partnership?", "answer": ["Tornado"], "scores": [[0.2721109092235565, 0.844914972782135, 0.9661860466003418]], "normalized_score": [0.6056288480758667], "tokens": [["\u2581T", "orn", "ado"]], "entropy": [[4.573195457458496, 1.2415952682495117, 0.28911715745925903]], "avg_entropy": [2.0346359610557556], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "The Tornado", "score_of_biggest_cluster": 0.6056288480758667, "score_of_first_answer": 0.6056288480758667, "max_score_of_first_answer": 0.9661860466003418, "min_score_of_first_answer": 0.2721109092235565, "avg_score": 0.6056288480758667, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.302682720737747, "std_of_first_answer": 0.302682720737747, "avg_entropy_of_first_answer": 2.0346359610557556, "lex_sim": 1.0}
{"question": "Bonar Law is the only Prime Minister not born in the UK. In which country was he born?", "answer": ["Canada, Toronto"], "scores": [[0.06808897852897644, 0.006125719752162695, 0.02146332710981369]], "normalized_score": [0.02076396532356739], "tokens": [["\u2581Canada", ",", "\u2581Toronto"]], "entropy": [[5.7439141273498535, 0.6870903372764587, 5.508185386657715]], "avg_entropy": [3.979729950428009], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Canada", "score_of_biggest_cluster": 0.02076396532356739, "score_of_first_answer": 0.02076396532356739, "max_score_of_first_answer": 0.06808897852897644, "min_score_of_first_answer": 0.006125719752162695, "avg_score": 0.02076396532356739, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.026349444455373097, "std_of_first_answer": 0.026349444455373097, "avg_entropy_of_first_answer": 3.979729950428009, "lex_sim": 1.0}
{"question": "Which Jane Austen character rents Netherfleld Park?", "answer": ["Mr. Bingley"], "scores": [[0.1727214753627777, 0.40054851770401, 0.07443472743034363, 0.6879383325576782, 0.9811263680458069]], "normalized_score": [0.3222629427909851], "tokens": [["\u2581Mr", ".", "\u2581B", "ing", "ley"]], "entropy": [[5.161795139312744, 2.372741222381592, 3.1958580017089844, 1.9573214054107666, 0.1447082757949829]], "avg_entropy": [2.566484808921814], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pride prejudice", "score_of_biggest_cluster": 0.3222629427909851, "score_of_first_answer": 0.3222629427909851, "max_score_of_first_answer": 0.9811263680458069, "min_score_of_first_answer": 0.07443472743034363, "avg_score": 0.3222629427909851, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3341246297720722, "std_of_first_answer": 0.3341246297720722, "avg_entropy_of_first_answer": 2.566484808921814, "lex_sim": 1.0}
{"question": "The Dales Way is a path that runs from Bowness in Cumbria to which town in Yorkshire on the River Wharfe?", "answer": ["\u3153\u3156\u3152\u314d\u3147\u3142\u314b\u314e\u3141\u3153"], "scores": [[0.0697263553738594, 0.32886815071105957, 0.07696153223514557, 0.025549959391355515, 0.3150836229324341, 0.9007243514060974, 0.03423637896776199, 0.7056459188461304, 0.944876492023468, 0.07662680000066757, 0.9041104316711426, 0.907836377620697, 0.013784934766590595, 0.8808392286300659, 0.9569490551948547, 0.04330422356724739, 0.8910117149353027, 0.9625087380409241, 0.01821337454020977, 0.8108901381492615, 0.968400776386261, 0.017194677144289017, 0.928680956363678, 0.9627918601036072, 0.052956800907850266, 0.8653429746627808, 0.9498842358589172, 0.049636099487543106, 0.8634588718414307, 0.9519891142845154, 0.03771451115608215]], "normalized_score": [0.24505791068077087], "tokens": [["\u2581", "<0xE3>", "<0x85>", "<0x93>", "<0xE3>", "<0x85>", "<0x96>", "<0xE3>", "<0x85>", "<0x92>", "<0xE3>", "<0x85>", "<0x8D>", "<0xE3>", "<0x85>", "<0x87>", "<0xE3>", "<0x85>", "<0x82>", "<0xE3>", "<0x85>", "<0x8B>", "<0xE3>", "<0x85>", "<0x8E>", "<0xE3>", "<0x85>", "<0x81>", "<0xE3>", "<0x85>", "<0x93>"]], "entropy": [[5.783495903015137, 4.331635475158691, 2.636854887008667, 3.4549853801727295, 1.4168643951416016, 0.5239529013633728, 2.644413709640503, 1.2402889728546143, 0.2864982485771179, 3.376218795776367, 0.5016039609909058, 0.3880707621574402, 3.3570878505706787, 0.5978443622589111, 0.21263138949871063, 3.3619065284729004, 0.5303783416748047, 0.19517995417118073, 3.4773380756378174, 0.9733235239982605, 0.18521255254745483, 3.4356136322021484, 0.44681841135025024, 0.1896596997976303, 3.464677572250366, 0.63926100730896, 0.24122469127178192, 3.2839536666870117, 0.6986017823219299, 0.23326458036899567, 3.502764940261841]], "avg_entropy": [1.79392341788738], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bingley College", "score_of_biggest_cluster": 0.24505791068077087, "score_of_first_answer": 0.24505791068077087, "max_score_of_first_answer": 0.968400776386261, "min_score_of_first_answer": 0.013784934766590595, "avg_score": 0.24505791068077087, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4165714050977472, "std_of_first_answer": 0.4165714050977472, "avg_entropy_of_first_answer": 1.79392341788738, "lex_sim": 0.0}
{"question": "What year were all these comedians born: Harry Enfield, Eddie Murphy and Rory Bremner?", "answer": ["Births in 1957"], "scores": [[0.008990182541310787, 0.3366182744503021, 0.31192225217819214, 0.1840764433145523, 0.8837533593177795, 0.9871664643287659, 0.9966141581535339, 0.21359460055828094, 0.8255156874656677]], "normalized_score": [0.310258686542511], "tokens": [["\u2581B", "irth", "s", "\u2581in", "\u2581", "1", "9", "5", "7"]], "entropy": [[3.076706886291504, 3.831238269805908, 2.7000296115875244, 2.3437891006469727, 0.7661025524139404, 0.0950019583106041, 0.030931076034903526, 0.7526419758796692, 0.8017598986625671]], "avg_entropy": [1.599800147737066], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and sixty-one", "score_of_biggest_cluster": 0.310258686542511, "score_of_first_answer": 0.310258686542511, "max_score_of_first_answer": 0.9966141581535339, "min_score_of_first_answer": 0.008990182541310787, "avg_score": 0.310258686542511, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36747327392812806, "std_of_first_answer": 0.36747327392812806, "avg_entropy_of_first_answer": 1.599800147737066, "lex_sim": 1.0}
{"question": "Which April fool's joke did Richard Dimbleby report on in 1957?", "answer": ["Spaghetti trees"], "scores": [[0.24960918724536896, 0.9991382360458374, 0.9918415546417236, 0.09157559275627136]], "normalized_score": [0.38795116543769836], "tokens": [["\u2581Sp", "agh", "etti", "\u2581trees"]], "entropy": [[4.697376728057861, 0.008025070652365685, 0.05332065373659134, 2.9497828483581543]], "avg_entropy": [1.9271263252012432], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Spaghetti weevil", "score_of_biggest_cluster": 0.38795116543769836, "score_of_first_answer": 0.38795116543769836, "max_score_of_first_answer": 0.9991382360458374, "min_score_of_first_answer": 0.09157559275627136, "avg_score": 0.38795116543769836, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41622404531525087, "std_of_first_answer": 0.41622404531525087, "avg_entropy_of_first_answer": 1.9271263252012432, "lex_sim": 1.0}
{"question": "What 1841 invention of the art world that replaced the usage of pig bladders was called by Renoir as indispensable for Impressionism?", "answer": ["Impressionist movement"], "scores": [[0.014798269607126713, 0.9304831624031067, 0.19584213197231293, 0.01638037897646427]], "normalized_score": [0.08152434229850769], "tokens": [["\u2581Im", "pression", "ist", "\u2581movement"]], "entropy": [[5.038381099700928, 0.46146446466445923, 1.42838454246521, 4.406930923461914]], "avg_entropy": [2.8337902575731277], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paint tube", "score_of_biggest_cluster": 0.08152434229850769, "score_of_first_answer": 0.08152434229850769, "max_score_of_first_answer": 0.9304831624031067, "min_score_of_first_answer": 0.014798269607126713, "avg_score": 0.08152434229850769, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37738790298528074, "std_of_first_answer": 0.37738790298528074, "avg_entropy_of_first_answer": 2.8337902575731277, "lex_sim": 1.0}
{"question": "The 'Vuvuzela' is traditionally made from the horn of which antelope?", "answer": ["Kudu Horn"], "scores": [[0.15269257128238678, 0.9183981418609619, 0.9782654643058777, 0.03006194531917572]], "normalized_score": [0.2534140348434448], "tokens": [["\u2581K", "ud", "u", "\u2581Horn"]], "entropy": [[5.574079513549805, 0.7146607637405396, 0.17085354030132294, 1.2948944568634033]], "avg_entropy": [1.9386220686137676], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "KUDU", "score_of_biggest_cluster": 0.2534140348434448, "score_of_first_answer": 0.2534140348434448, "max_score_of_first_answer": 0.9782654643058777, "min_score_of_first_answer": 0.03006194531917572, "avg_score": 0.2534140348434448, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.43118507487621993, "std_of_first_answer": 0.43118507487621993, "avg_entropy_of_first_answer": 1.9386220686137676, "lex_sim": 1.0}
{"question": "\"Which opera features the \"\"Humming Chorus\"\"?\"", "answer": ["Madam Butterfly"], "scores": [[0.47169724106788635, 0.2952258586883545, 0.9371187090873718, 0.9956887364387512, 0.9842271208763123]], "normalized_score": [0.6627752184867859], "tokens": [["\u2581Mad", "am", "\u2581But", "ter", "fly"]], "entropy": [[2.975761890411377, 0.7355956435203552, 0.549260139465332, 0.04360996186733246, 0.1331431269645691]], "avg_entropy": [0.8874741524457932], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Madam Butterfly", "score_of_biggest_cluster": 0.6627752184867859, "score_of_first_answer": 0.6627752184867859, "max_score_of_first_answer": 0.9956887364387512, "min_score_of_first_answer": 0.2952258586883545, "avg_score": 0.6627752184867859, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.29449561556007253, "std_of_first_answer": 0.29449561556007253, "avg_entropy_of_first_answer": 0.8874741524457932, "lex_sim": 1.0}
{"question": "Ivan the Terrible was a czar of what country?", "answer": ["Russia"], "scores": [[0.3304142951965332]], "normalized_score": [0.3304142653942108], "tokens": [["\u2581Russia"]], "entropy": [[4.459379196166992]], "avg_entropy": [4.459379196166992], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Russia (1991-1993)", "score_of_biggest_cluster": 0.3304142653942108, "score_of_first_answer": 0.3304142653942108, "max_score_of_first_answer": 0.3304142951965332, "min_score_of_first_answer": 0.3304142951965332, "avg_score": 0.3304142653942108, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.459379196166992, "lex_sim": 1.0}
{"question": "What is a nattatorium better known as", "answer": ["Swimming pool"], "scores": [[0.34635964035987854, 0.9736083745956421, 0.7780696153640747]], "normalized_score": [0.6401916742324829], "tokens": [["\u2581Sw", "imming", "\u2581pool"]], "entropy": [[3.2852582931518555, 0.15840604901313782, 0.9421401023864746]], "avg_entropy": [1.4619348148504894], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Spa pool", "score_of_biggest_cluster": 0.6401916742324829, "score_of_first_answer": 0.6401916742324829, "max_score_of_first_answer": 0.9736083745956421, "min_score_of_first_answer": 0.34635964035987854, "avg_score": 0.6401916742324829, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2620538289657112, "std_of_first_answer": 0.2620538289657112, "avg_entropy_of_first_answer": 1.4619348148504894, "lex_sim": 1.0}
{"question": "Mike Joyce, Johnny Marr, Stephen Morrisey and Andy Rourke were members of which pop group?", "answer": ["Smiths"], "scores": [[0.06327519565820694, 0.8082539439201355]], "normalized_score": [0.22614693641662598], "tokens": [["\u2581Smith", "s"]], "entropy": [[4.149306297302246, 1.3046672344207764]], "avg_entropy": [2.7269867658615112], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dale Hibbert", "score_of_biggest_cluster": 0.22614693641662598, "score_of_first_answer": 0.22614693641662598, "max_score_of_first_answer": 0.8082539439201355, "min_score_of_first_answer": 0.06327519565820694, "avg_score": 0.22614693641662598, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3724893741309643, "std_of_first_answer": 0.3724893741309643, "avg_entropy_of_first_answer": 2.7269867658615112, "lex_sim": 1.0}
{"question": "\"Who painted \"\"Dedham Vale\"\" and \"\"The Valley Farm\"\"?\"", "answer": ["John Constable"], "scores": [[0.6455645561218262, 0.8830794095993042, 0.9851522445678711]], "normalized_score": [0.8250512480735779], "tokens": [["\u2581John", "\u2581Const", "able"]], "entropy": [[2.5206422805786133, 0.9232997298240662, 0.10498227179050446]], "avg_entropy": [1.1829747607310612], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "J Constable", "score_of_biggest_cluster": 0.8250512480735779, "score_of_first_answer": 0.8250512480735779, "max_score_of_first_answer": 0.9851522445678711, "min_score_of_first_answer": 0.6455645561218262, "avg_score": 0.8250512480735779, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.14226421717864585, "std_of_first_answer": 0.14226421717864585, "avg_entropy_of_first_answer": 1.1829747607310612, "lex_sim": 1.0}
{"question": "Whose single I saw mommy kissing Santa Claus sold three million copies in 1952 ?", "answer": ["Jimmy Boyd (singer)"], "scores": [[0.4849799573421478, 0.9438516497612, 0.9874059557914734, 0.10072925686836243, 0.7336918115615845, 0.9965242743492126, 0.9773748517036438]], "normalized_score": [0.6130238175392151], "tokens": [["\u2581Jimmy", "\u2581Bo", "yd", "\u2581(", "sing", "er", ")"]], "entropy": [[3.666533946990967, 0.5039923191070557, 0.10273430496454239, 0.7978804707527161, 1.758171558380127, 0.03217317909002304, 0.18220627307891846]], "avg_entropy": [1.0062417217663355], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bwana Jimmy", "score_of_biggest_cluster": 0.6130238175392151, "score_of_first_answer": 0.6130238175392151, "max_score_of_first_answer": 0.9965242743492126, "min_score_of_first_answer": 0.10072925686836243, "avg_score": 0.6130238175392151, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31586054707018424, "std_of_first_answer": 0.31586054707018424, "avg_entropy_of_first_answer": 1.0062417217663355, "lex_sim": 1.0}
{"question": "What system was added in 1858 to boost communication via the transatlantic telegraph cable between the USA and the UK?", "answer": ["\u3148"], "scores": [[0.023537825793027878, 0.19582194089889526, 0.06445298343896866, 0.030224163085222244]], "normalized_score": [0.054740190505981445], "tokens": [["\u2581", "<0xE3>", "<0x85>", "<0x88>"]], "entropy": [[5.93269157409668, 4.408730983734131, 2.5340168476104736, 3.3672256469726562]], "avg_entropy": [4.060666263103485], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marine galvanometer", "score_of_biggest_cluster": 0.054740190505981445, "score_of_first_answer": 0.054740190505981445, "max_score_of_first_answer": 0.19582194089889526, "min_score_of_first_answer": 0.023537825793027878, "avg_score": 0.054740190505981445, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06948588556909086, "std_of_first_answer": 0.06948588556909086, "avg_entropy_of_first_answer": 4.060666263103485, "lex_sim": 0.0}
{"question": "What is the US state capital of Kansas?", "answer": ["Kansas City"], "scores": [[0.038326963782310486, 0.3753024935722351]], "normalized_score": [0.11993414908647537], "tokens": [["\u2581Kansas", "\u2581City"]], "entropy": [[5.291431427001953, 2.8142824172973633]], "avg_entropy": [4.052856922149658], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Northern Hills Junior High", "score_of_biggest_cluster": 0.11993414908647537, "score_of_first_answer": 0.11993414908647537, "max_score_of_first_answer": 0.3753024935722351, "min_score_of_first_answer": 0.038326963782310486, "avg_score": 0.11993414908647537, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1684877648949623, "std_of_first_answer": 0.1684877648949623, "avg_entropy_of_first_answer": 4.052856922149658, "lex_sim": 1.0}
{"question": "Who was born in a cross-fire hurricane and raised by a toothless bearded hag?", "answer": ["The Rolling Stones"], "scores": [[0.09846854954957962, 0.6914375424385071, 0.9963340759277344, 0.9330133199691772, 0.9987941980361938]], "normalized_score": [0.575657069683075], "tokens": [["\u2581The", "\u2581Rol", "ling", "\u2581St", "ones"]], "entropy": [[5.13386869430542, 2.4784903526306152, 0.03064385987818241, 0.372547447681427, 0.013025154359638691]], "avg_entropy": [1.6057151017710567], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jumpin' Jack Flash (song)", "score_of_biggest_cluster": 0.575657069683075, "score_of_first_answer": 0.575657069683075, "max_score_of_first_answer": 0.9987941980361938, "min_score_of_first_answer": 0.09846854954957962, "avg_score": 0.575657069683075, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3416991388670613, "std_of_first_answer": 0.3416991388670613, "avg_entropy_of_first_answer": 1.6057151017710567, "lex_sim": 1.0}
{"question": "Ninette de Valois was involved professionally in what activity?", "answer": ["Ballerina"], "scores": [[0.03478445112705231, 0.6274592876434326, 0.9289171099662781]], "normalized_score": [0.2726774513721466], "tokens": [["\u2581B", "aller", "ina"]], "entropy": [[5.219840049743652, 2.514465093612671, 0.4049053490161896]], "avg_entropy": [2.713070164124171], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Ballet", "score_of_biggest_cluster": 0.2726774513721466, "score_of_first_answer": 0.2726774513721466, "max_score_of_first_answer": 0.9289171099662781, "min_score_of_first_answer": 0.03478445112705231, "avg_score": 0.2726774513721466, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.37142570545912135, "std_of_first_answer": 0.37142570545912135, "avg_entropy_of_first_answer": 2.713070164124171, "lex_sim": 1.0}
{"question": "Cable News Network (CNN), the first channel to provide 24-hour television news coverage, was founded by Ted Turner in what year?", "answer": ["100,000 in 1980"], "scores": [[0.5872834920883179, 0.6894584894180298, 0.010401509702205658, 0.30862581729888916, 0.034088049083948135, 0.8480998277664185, 0.9710150957107544, 0.9768179655075073, 0.00165132200345397, 0.4534904956817627, 0.7331438064575195, 0.7646058201789856, 0.7040429711341858, 0.8825562596321106]], "normalized_score": [0.26687633991241455], "tokens": [["\u2581", "1", "0", "0", ",", "0", "0", "0", "\u2581in", "\u2581", "1", "9", "8", "0"]], "entropy": [[3.1283915042877197, 1.5668017864227295, 0.5263053178787231, 3.294377326965332, 2.986860752105713, 0.8607140183448792, 0.20150160789489746, 0.16217295825481415, 3.022566556930542, 3.8327534198760986, 0.9642186760902405, 1.098961353302002, 1.2030792236328125, 0.5991701483726501]], "avg_entropy": [1.674848189311368], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and eighty", "score_of_biggest_cluster": 0.26687633991241455, "score_of_first_answer": 0.26687633991241455, "max_score_of_first_answer": 0.9768179655075073, "min_score_of_first_answer": 0.00165132200345397, "avg_score": 0.26687633991241455, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3384435541215594, "std_of_first_answer": 0.3384435541215594, "avg_entropy_of_first_answer": 1.674848189311368, "lex_sim": 1.0}
{"question": "Nankey Poo, Poo Bah and Pish Tush are characters in which show?", "answer": ["The Mikado (operetta)"], "scores": [[0.5882564783096313, 0.9300805926322937, 0.9971689581871033, 0.2624519169330597, 0.1351148933172226, 0.9880290627479553, 0.9703474640846252]], "normalized_score": [0.5657377243041992], "tokens": [["\u2581The", "\u2581Mik", "ado", "\u2581(", "oper", "etta", ")"]], "entropy": [[2.9831745624542236, 0.6824033856391907, 0.027609284967184067, 1.038077473640442, 2.739561080932617, 0.09120168536901474, 0.207660511136055]], "avg_entropy": [1.1099554263055325], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Katisha", "score_of_biggest_cluster": 0.5657377243041992, "score_of_first_answer": 0.5657377243041992, "max_score_of_first_answer": 0.9971689581871033, "min_score_of_first_answer": 0.1351148933172226, "avg_score": 0.5657377243041992, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34230492591580614, "std_of_first_answer": 0.34230492591580614, "avg_entropy_of_first_answer": 1.1099554263055325, "lex_sim": 1.0}
{"question": "What animated character does Gene Kelly dance with in the film Anchors Aweigh?", "answer": ["Jerry Mouse"], "scores": [[0.21916766464710236, 0.8109220862388611]], "normalized_score": [0.4215778708457947], "tokens": [["\u2581Jerry", "\u2581Mouse"]], "entropy": [[5.142288684844971, 0.9999262094497681]], "avg_entropy": [3.0711074471473694], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Jerry Mouse", "score_of_biggest_cluster": 0.4215778708457947, "score_of_first_answer": 0.4215778708457947, "max_score_of_first_answer": 0.8109220862388611, "min_score_of_first_answer": 0.21916766464710236, "avg_score": 0.4215778708457947, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.29587721079587936, "std_of_first_answer": 0.29587721079587936, "avg_entropy_of_first_answer": 3.0711074471473694, "lex_sim": 1.0}
{"question": "Who directed the first two Beatles' films 'A Hard Day's Night' and 'Help! '?", "answer": ["Richard Lester"], "scores": [[0.27386781573295593, 0.746482789516449, 0.9251770377159119]], "normalized_score": [0.5740220546722412], "tokens": [["\u2581Richard", "\u2581L", "ester"]], "entropy": [[5.320650577545166, 2.0443594455718994, 0.6913467049598694]], "avg_entropy": [2.6854522426923118], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Richard Lester", "score_of_biggest_cluster": 0.5740220546722412, "score_of_first_answer": 0.5740220546722412, "max_score_of_first_answer": 0.9251770377159119, "min_score_of_first_answer": 0.27386781573295593, "avg_score": 0.5740220546722412, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.27477268391324383, "std_of_first_answer": 0.27477268391324383, "avg_entropy_of_first_answer": 2.6854522426923118, "lex_sim": 1.0}
{"question": "On what date does official grouse shooting start", "answer": ["12 August"], "scores": [[0.36795586347579956, 0.7944261431694031, 0.8115829229354858, 0.5724459290504456]], "normalized_score": [0.6070565581321716], "tokens": [["\u2581", "1", "2", "\u2581August"]], "entropy": [[3.8221452236175537, 1.204927921295166, 1.0359965562820435, 1.273768663406372]], "avg_entropy": [1.8342095911502838], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Aug 12", "score_of_biggest_cluster": 0.6070565581321716, "score_of_first_answer": 0.6070565581321716, "max_score_of_first_answer": 0.8115829229354858, "min_score_of_first_answer": 0.36795586347579956, "avg_score": 0.6070565581321716, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.18153066660039244, "std_of_first_answer": 0.18153066660039244, "avg_entropy_of_first_answer": 1.8342095911502838, "lex_sim": 1.0}
{"question": "Which famous family lived at '50 Wimpole Street, London'?", "answer": ["The Forsyte Saga"], "scores": [[0.10611207783222198, 0.019306227564811707, 0.8855206370353699, 0.9207301735877991, 0.49555110931396484, 0.8681262135505676, 0.9923661351203918]], "normalized_score": [0.35517963767051697], "tokens": [["\u2581The", "\u2581F", "ors", "y", "te", "\u2581S", "aga"]], "entropy": [[5.372997283935547, 5.565546989440918, 0.8503063321113586, 0.3310360908508301, 0.9142137765884399, 0.7083256244659424, 0.05501389503479004]], "avg_entropy": [1.971062856061118], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Barrett\u2019s esophagus", "score_of_biggest_cluster": 0.35517963767051697, "score_of_first_answer": 0.35517963767051697, "max_score_of_first_answer": 0.9923661351203918, "min_score_of_first_answer": 0.019306227564811707, "avg_score": 0.35517963767051697, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.378186228836925, "std_of_first_answer": 0.378186228836925, "avg_entropy_of_first_answer": 1.971062856061118, "lex_sim": 1.0}
{"question": "'Roquefort Cheese' is made from what sort of milk?", "answer": ["It is made from goat's milk and named after the town of Roquefort in Southern France."], "scores": [[0.0027988688088953495, 0.4865712523460388, 0.3332391679286957, 0.7306243181228638, 0.07383695244789124, 0.8169084191322327, 0.6732416152954102, 0.9991816878318787, 0.9717320203781128, 0.014021378010511398, 0.03675559163093567, 0.900658369064331, 0.4851405620574951, 0.15951162576675415, 0.9439557194709778, 0.9599841237068176, 0.9890562295913696, 0.9983477592468262, 0.11435851454734802, 0.032853733748197556, 0.9837506413459778, 0.39541104435920715]], "normalized_score": [0.27856045961380005], "tokens": [["\u2581It", "\u2581is", "\u2581made", "\u2581from", "\u2581go", "at", "'", "s", "\u2581milk", "\u2581and", "\u2581named", "\u2581after", "\u2581the", "\u2581town", "\u2581of", "\u2581Ro", "que", "fort", "\u2581in", "\u2581Southern", "\u2581France", "."]], "entropy": [[4.3897223472595215, 2.224766254425049, 3.3665175437927246, 1.1975144147872925, 2.456972599029541, 0.4845172166824341, 1.092499852180481, 0.00928420014679432, 0.2061346024274826, 1.2711701393127441, 4.010684013366699, 0.46638962626457214, 1.1375869512557983, 2.1290478706359863, 0.37348902225494385, 0.27838823199272156, 0.08073566854000092, 0.017246965318918228, 1.6161110401153564, 1.9144065380096436, 0.1412891447544098, 1.4613075256347656]], "avg_entropy": [1.3784446258267218], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sheep milk", "score_of_biggest_cluster": 0.27856045961380005, "score_of_first_answer": 0.27856045961380005, "max_score_of_first_answer": 0.9991816878318787, "min_score_of_first_answer": 0.0027988688088953495, "avg_score": 0.27856045961380005, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3868592573031017, "std_of_first_answer": 0.3868592573031017, "avg_entropy_of_first_answer": 1.3784446258267218, "lex_sim": 1.0}
{"question": "When was Uluru (Ayers Rock) in the Northern Territory of Australia declared a World Heritage Site?", "answer": ["Ayer's rock"], "scores": [[0.012464816682040691, 0.18512089550495148, 0.37085041403770447, 0.9963112473487854, 0.10499074310064316]], "normalized_score": [0.1550162434577942], "tokens": [["\u2581A", "yer", "'", "s", "\u2581rock"]], "entropy": [[3.4328720569610596, 5.5461344718933105, 3.369030237197876, 0.046806879341602325, 2.8676939010620117]], "avg_entropy": [3.052507509291172], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1987", "score_of_biggest_cluster": 0.1550162434577942, "score_of_first_answer": 0.1550162434577942, "max_score_of_first_answer": 0.9963112473487854, "min_score_of_first_answer": 0.012464816682040691, "avg_score": 0.1550162434577942, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3515705417907012, "std_of_first_answer": 0.3515705417907012, "avg_entropy_of_first_answer": 3.052507509291172, "lex_sim": 1.0}
{"question": "Toussaint Louverture was the founding father of what country?", "answer": ["Haiti"], "scores": [[0.4655461013317108, 0.9607540965080261, 0.9326794147491455]], "normalized_score": [0.7471981048583984], "tokens": [["\u2581H", "ait", "i"]], "entropy": [[3.9453518390655518, 0.36514419317245483, 0.3453342318534851]], "avg_entropy": [1.5519434213638306], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Haitian Ancestry", "score_of_biggest_cluster": 0.7471981048583984, "score_of_first_answer": 0.7471981048583984, "max_score_of_first_answer": 0.9607540965080261, "min_score_of_first_answer": 0.4655461013317108, "avg_score": 0.7471981048583984, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2271154084372963, "std_of_first_answer": 0.2271154084372963, "avg_entropy_of_first_answer": 1.5519434213638306, "lex_sim": 1.0}
{"question": "Which Scot was Glenn Hoddle's assistant in the 'England job'?", "answer": ["Ian McGregor"], "scores": [[0.005193261429667473, 0.020365728065371513, 0.06379283964633942, 0.9871249794960022]], "normalized_score": [0.0508008636534214], "tokens": [["\u2581Ian", "\u2581McG", "reg", "or"]], "entropy": [[5.351543426513672, 5.4230804443359375, 2.186126708984375, 0.11805802583694458]], "avg_entropy": [3.2697021514177322], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Gorman (disambiguation)", "score_of_biggest_cluster": 0.0508008636534214, "score_of_first_answer": 0.0508008636534214, "max_score_of_first_answer": 0.9871249794960022, "min_score_of_first_answer": 0.005193261429667473, "avg_score": 0.0508008636534214, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41509830296434175, "std_of_first_answer": 0.41509830296434175, "avg_entropy_of_first_answer": 3.2697021514177322, "lex_sim": 1.0}
{"question": "What genus of flowering plants, native to an area from Brazil to Peru to southern Argentina discovered in 1768 by French botanist Philibert Commeron, is a thorny, woody vine reaching from 1 to 12 metres tall by growing over other plants with hooked thorns tipped with a black, waxy substance?", "answer": ["Pisonia"], "scores": [[0.010229318402707577, 0.029908185824751854, 0.9226028919219971]], "normalized_score": [0.06559700518846512], "tokens": [["\u2581P", "ison", "ia"]], "entropy": [[5.262841701507568, 5.391123294830322, 0.5797445178031921]], "avg_entropy": [3.7445698380470276], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bougonvillea", "score_of_biggest_cluster": 0.06559700518846512, "score_of_first_answer": 0.06559700518846512, "max_score_of_first_answer": 0.9226028919219971, "min_score_of_first_answer": 0.010229318402707577, "avg_score": 0.06559700518846512, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42553451795926545, "std_of_first_answer": 0.42553451795926545, "avg_entropy_of_first_answer": 3.7445698380470276, "lex_sim": 1.0}
{"question": "\"Heinz' 1963 song \"\"Just Like Eddie\"\" was a tribute to whom?\"", "answer": ["Eddie Cochran"], "scores": [[0.44636473059654236, 0.9696322083473206, 0.7758437991142273, 0.996385931968689, 0.8768018484115601]], "normalized_score": [0.7824922800064087], "tokens": [["\u2581Edd", "ie", "\u2581C", "och", "ran"]], "entropy": [[3.850917339324951, 0.19056253135204315, 1.7193877696990967, 0.03896840289235115, 0.5071699023246765]], "avg_entropy": [1.2614011891186236], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Cochran, Eddie", "score_of_biggest_cluster": 0.7824922800064087, "score_of_first_answer": 0.7824922800064087, "max_score_of_first_answer": 0.996385931968689, "min_score_of_first_answer": 0.44636473059654236, "avg_score": 0.7824922800064087, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.19901594331543607, "std_of_first_answer": 0.19901594331543607, "avg_entropy_of_first_answer": 1.2614011891186236, "lex_sim": 1.0}
{"question": "Who won the award for the Best Actor at the 2015 BAFTA film awards", "answer": ["Eddie Redmayne"], "scores": [[0.397113561630249, 0.931997537612915, 0.746040403842926, 0.9189198613166809, 0.9811674356460571]], "normalized_score": [0.7572207450866699], "tokens": [["\u2581Edd", "ie", "\u2581Red", "may", "ne"]], "entropy": [[4.4017767906188965, 0.4399179220199585, 1.9610214233398438, 0.6615864634513855, 0.15879029035568237]], "avg_entropy": [1.5246185779571533], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Eddie redmayne", "score_of_biggest_cluster": 0.7572207450866699, "score_of_first_answer": 0.7572207450866699, "max_score_of_first_answer": 0.9811674356460571, "min_score_of_first_answer": 0.397113561630249, "avg_score": 0.7572207450866699, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.21423978151833198, "std_of_first_answer": 0.21423978151833198, "avg_entropy_of_first_answer": 1.5246185779571533, "lex_sim": 1.0}
{"question": "Who was the youngest brother in the Beach Boys?", "answer": ["Beach Boys (band)"], "scores": [[0.009078675881028175, 0.3718116283416748, 0.10905345529317856, 0.4934528172016144, 0.9680142402648926]], "normalized_score": [0.17742809653282166], "tokens": [["\u2581Beach", "\u2581Boys", "\u2581(", "band", ")"]], "entropy": [[5.813835144042969, 3.5513272285461426, 3.4703071117401123, 2.8906002044677734, 0.24823077023029327]], "avg_entropy": [3.194860091805458], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Carl Wilson", "score_of_biggest_cluster": 0.17742809653282166, "score_of_first_answer": 0.17742809653282166, "max_score_of_first_answer": 0.9680142402648926, "min_score_of_first_answer": 0.009078675881028175, "avg_score": 0.17742809653282166, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3373915773311107, "std_of_first_answer": 0.3373915773311107, "avg_entropy_of_first_answer": 3.194860091805458, "lex_sim": 1.0}
{"question": "After a short war with Norway during the Napoleonic wars of 1814, what country remained uninvolved in any conflict ever since and is now the oldest neutral country in the world?", "answer": ["Switzerland"], "scores": [[0.15857309103012085]], "normalized_score": [0.15857309103012085], "tokens": [["\u2581Switzerland"]], "entropy": [[5.301043510437012]], "avg_entropy": [5.301043510437012], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Swedish sin", "score_of_biggest_cluster": 0.15857309103012085, "score_of_first_answer": 0.15857309103012085, "max_score_of_first_answer": 0.15857309103012085, "min_score_of_first_answer": 0.15857309103012085, "avg_score": 0.15857309103012085, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.301043510437012, "lex_sim": 1.0}
{"question": "\"In the James Bond film \"\"Goldfinger\"\" who played Bond girl Pussy Galore?\"", "answer": ["Honor Blackman"], "scores": [[0.2066602259874344, 0.8869134783744812, 0.965630292892456]], "normalized_score": [0.5614567995071411], "tokens": [["\u2581Honor", "\u2581Black", "man"]], "entropy": [[4.903654098510742, 1.0101919174194336, 0.3122715651988983]], "avg_entropy": [2.0753725270430246], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Honor Blackman", "score_of_biggest_cluster": 0.5614567995071411, "score_of_first_answer": 0.5614567995071411, "max_score_of_first_answer": 0.965630292892456, "min_score_of_first_answer": 0.2066602259874344, "avg_score": 0.5614567995071411, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3407469551064455, "std_of_first_answer": 0.3407469551064455, "avg_entropy_of_first_answer": 2.0753725270430246, "lex_sim": 1.0}
{"question": "The Paragons originally recorded which 1980 hit single by Blondie?", "answer": ["The Tide Is High"], "scores": [[0.5565283298492432, 0.8232717514038086, 0.9930971264839172, 0.29449740052223206, 0.9828102588653564]], "normalized_score": [0.6666757464408875], "tokens": [["\u2581The", "\u2581T", "ide", "\u2581Is", "\u2581High"]], "entropy": [[3.2144570350646973, 1.1933372020721436, 0.053085267543792725, 0.6993781328201294, 0.1618460714817047]], "avg_entropy": [1.0644207417964935], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tide Is High (Get The Feeling)", "score_of_biggest_cluster": 0.6666757464408875, "score_of_first_answer": 0.6666757464408875, "max_score_of_first_answer": 0.9930971264839172, "min_score_of_first_answer": 0.29449740052223206, "avg_score": 0.6666757464408875, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2689191962892479, "std_of_first_answer": 0.2689191962892479, "avg_entropy_of_first_answer": 1.0644207417964935, "lex_sim": 1.0}
{"question": "What modern popular pet population originated entirely from single female found in Syria in 1930?", "answer": ["Cats"], "scores": [[0.014315088279545307, 0.2765446603298187]], "normalized_score": [0.06291868537664413], "tokens": [["\u2581C", "ats"]], "entropy": [[5.420035362243652, 4.247982501983643]], "avg_entropy": [4.8340089321136475], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "H\u00f6rcs\u00f6g", "score_of_biggest_cluster": 0.06291868537664413, "score_of_first_answer": 0.06291868537664413, "max_score_of_first_answer": 0.2765446603298187, "min_score_of_first_answer": 0.014315088279545307, "avg_score": 0.06291868537664413, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1311147860251367, "std_of_first_answer": 0.1311147860251367, "avg_entropy_of_first_answer": 4.8340089321136475, "lex_sim": 1.0}
{"question": "Who was the 8th President of the United States from 1837- 1841?", "answer": ["45th President of the United States"], "scores": [[0.04697004705667496, 0.03811907023191452, 0.18902617692947388, 0.5184061527252197, 0.4780994653701782, 0.7772191166877747, 0.9407637715339661, 0.8507962822914124, 0.9934333562850952]], "normalized_score": [0.33408069610595703], "tokens": [["\u2581", "4", "5", "th", "\u2581President", "\u2581of", "\u2581the", "\u2581United", "\u2581States"]], "entropy": [[4.8047871589660645, 3.705533504486084, 3.171877861022949, 2.4896750450134277, 2.205707311630249, 1.1273821592330933, 0.3658936619758606, 0.6229828000068665, 0.05908862501382828]], "avg_entropy": [2.0614364585942693], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Martin Van Buren", "score_of_biggest_cluster": 0.33408069610595703, "score_of_first_answer": 0.33408069610595703, "max_score_of_first_answer": 0.9934333562850952, "min_score_of_first_answer": 0.03811907023191452, "avg_score": 0.33408069610595703, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35612766783822747, "std_of_first_answer": 0.35612766783822747, "avg_entropy_of_first_answer": 2.0614364585942693, "lex_sim": 1.0}
{"question": "Which US state was admitted to the Union in 1876, hence its nickname of 'the Centennial State'?", "answer": ["Colorado"], "scores": [[0.22938895225524902]], "normalized_score": [0.22938893735408783], "tokens": [["\u2581Colorado"]], "entropy": [[5.085653305053711]], "avg_entropy": [5.085653305053711], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Colorado (State)", "score_of_biggest_cluster": 0.22938893735408783, "score_of_first_answer": 0.22938893735408783, "max_score_of_first_answer": 0.22938895225524902, "min_score_of_first_answer": 0.22938895225524902, "avg_score": 0.22938893735408783, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.085653305053711, "lex_sim": 1.0}
{"question": "The original Ferris Wheel was erected in which US state for the 1893 World\u2019s Columbian Exposition?", "answer": ["Pennsylvania"], "scores": [[0.0018039867281913757]], "normalized_score": [0.0018039868446066976], "tokens": [["\u2581Pennsylvania"]], "entropy": [[4.659017086029053]], "avg_entropy": [4.659017086029053], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Transportation in Illinois", "score_of_biggest_cluster": 0.0018039868446066976, "score_of_first_answer": 0.0018039868446066976, "max_score_of_first_answer": 0.0018039867281913757, "min_score_of_first_answer": 0.0018039867281913757, "avg_score": 0.0018039868446066976, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.659017086029053, "lex_sim": 1.0}
{"question": "The port of Mocha is in which country?", "answer": ["Coffee port"], "scores": [[0.017003318294882774, 0.7163631319999695, 0.8806281089782715, 0.02750992961227894]], "normalized_score": [0.13106516003608704], "tokens": [["\u2581C", "off", "ee", "\u2581port"]], "entropy": [[4.014898300170898, 2.1207685470581055, 0.5787893533706665, 3.680544853210449]], "avg_entropy": [2.59875026345253], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yemen AR", "score_of_biggest_cluster": 0.13106516003608704, "score_of_first_answer": 0.13106516003608704, "max_score_of_first_answer": 0.8806281089782715, "min_score_of_first_answer": 0.017003318294882774, "avg_score": 0.13106516003608704, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3924581715017633, "std_of_first_answer": 0.3924581715017633, "avg_entropy_of_first_answer": 2.59875026345253, "lex_sim": 1.0}
{"question": "Summerisle is the setting for which 1973 horror film?", "answer": ["The Wicker Man"], "scores": [[0.6662397384643555, 0.9327157735824585, 0.9802635312080383, 0.9402887225151062]], "normalized_score": [0.8699533939361572], "tokens": [["\u2581The", "\u2581W", "icker", "\u2581Man"]], "entropy": [[2.733139991760254, 0.7088685035705566, 0.12109827995300293, 0.502477765083313]], "avg_entropy": [1.0163961350917816], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Wicker Man (disambiguation)", "score_of_biggest_cluster": 0.8699533939361572, "score_of_first_answer": 0.8699533939361572, "max_score_of_first_answer": 0.9802635312080383, "min_score_of_first_answer": 0.6662397384643555, "avg_score": 0.8699533939361572, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12465938329640355, "std_of_first_answer": 0.12465938329640355, "avg_entropy_of_first_answer": 1.0163961350917816, "lex_sim": 1.0}
{"question": "Which cartoon character created by Al Capp lives in Dog Patch?", "answer": ["Horsemen of the apocalypse"], "scores": [[0.004920046776533127, 0.00387605931609869, 0.002485026139765978, 0.5100451707839966, 0.740389883518219, 0.0478229895234108, 0.9340617656707764, 0.981203019618988, 0.9982602000236511]], "normalized_score": [0.09731888771057129], "tokens": [["\u2581H", "ors", "emen", "\u2581of", "\u2581the", "\u2581ap", "ocal", "yp", "se"]], "entropy": [[4.705996036529541, 5.074193000793457, 3.239875316619873, 2.3588099479675293, 1.1095600128173828, 0.759685754776001, 0.317119836807251, 0.13757368922233582, 0.020654378458857536]], "avg_entropy": [1.96927421933247], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "L'ABNER", "score_of_biggest_cluster": 0.09731888771057129, "score_of_first_answer": 0.09731888771057129, "max_score_of_first_answer": 0.9982602000236511, "min_score_of_first_answer": 0.002485026139765978, "avg_score": 0.09731888771057129, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42957656389085114, "std_of_first_answer": 0.42957656389085114, "avg_entropy_of_first_answer": 1.96927421933247, "lex_sim": 1.0}
{"question": "Which planet has a diameter that is closest to that of the Earth?", "answer": ["Solanum aethiopicum"], "scores": [[0.002460385672748089, 0.004506811499595642, 0.9053647518157959, 0.0038487801793962717, 0.7602919936180115, 0.9693992137908936, 0.9859372973442078]], "normalized_score": [0.08340518921613693], "tokens": [["\u2581Sol", "an", "um", "\u2581a", "eth", "iop", "icum"]], "entropy": [[4.5759429931640625, 2.250469923019409, 0.7375782132148743, 3.1742653846740723, 1.3898473978042603, 0.22156625986099243, 0.12319160997867584]], "avg_entropy": [1.7818373973880495], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "VENUS", "score_of_biggest_cluster": 0.08340518921613693, "score_of_first_answer": 0.08340518921613693, "max_score_of_first_answer": 0.9859372973442078, "min_score_of_first_answer": 0.002460385672748089, "avg_score": 0.08340518921613693, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4512342486319634, "std_of_first_answer": 0.4512342486319634, "avg_entropy_of_first_answer": 1.7818373973880495, "lex_sim": 1.0}
{"question": "What is the capital of Venezuela", "answer": ["Capital: Caracas"], "scores": [[0.01515020988881588, 0.03171980381011963, 0.6994296908378601, 0.9761478900909424, 0.9852146506309509]], "normalized_score": [0.20040476322174072], "tokens": [["\u2581Capital", ":", "\u2581Car", "ac", "as"]], "entropy": [[4.383269786834717, 2.2994349002838135, 2.1725106239318848, 0.2287694811820984, 0.11311377584934235]], "avg_entropy": [1.8394197136163712], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Metropolitan District of Caracas", "score_of_biggest_cluster": 0.20040476322174072, "score_of_first_answer": 0.20040476322174072, "max_score_of_first_answer": 0.9852146506309509, "min_score_of_first_answer": 0.01515020988881588, "avg_score": 0.20040476322174072, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4353534682314987, "std_of_first_answer": 0.4353534682314987, "avg_entropy_of_first_answer": 1.8394197136163712, "lex_sim": 1.0}
{"question": "Which capital city is known as Praha by its inhabitants?", "answer": ["Prague"], "scores": [[0.3217787742614746, 0.9737820029258728]], "normalized_score": [0.5597699284553528], "tokens": [["\u2581Prag", "ue"]], "entropy": [[4.510616779327393, 0.19696959853172302]], "avg_entropy": [2.353793188929558], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "City of Prague", "score_of_biggest_cluster": 0.5597699284553528, "score_of_first_answer": 0.5597699284553528, "max_score_of_first_answer": 0.9737820029258728, "min_score_of_first_answer": 0.3217787742614746, "avg_score": 0.5597699284553528, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3260016143321991, "std_of_first_answer": 0.3260016143321991, "avg_entropy_of_first_answer": 2.353793188929558, "lex_sim": 1.0}
{"question": "Han of Iceland, Oedipus in Athens & The Ballet of Unhatched Chicks are all works by which composer?", "answer": ["Stravinsky (music)"], "scores": [[0.06236286833882332, 0.979548990726471, 0.9733051061630249, 0.02795329876244068, 0.007910693064332008, 0.5618988275527954]], "normalized_score": [0.1395568698644638], "tokens": [["\u2581Stra", "vin", "sky", "\u2581(", "music", ")"]], "entropy": [[5.611177444458008, 0.1440359503030777, 0.15862008929252625, 0.7610259056091309, 2.9975712299346924, 1.5097169876098633]], "avg_entropy": [1.863691267867883], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Modest Petrovich Mussorgsky", "score_of_biggest_cluster": 0.1395568698644638, "score_of_first_answer": 0.1395568698644638, "max_score_of_first_answer": 0.979548990726471, "min_score_of_first_answer": 0.007910693064332008, "avg_score": 0.1395568698644638, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4260982962071461, "std_of_first_answer": 0.4260982962071461, "avg_entropy_of_first_answer": 1.863691267867883, "lex_sim": 1.0}
{"question": "Who in 1476 brought his painting press from Bruges to London?", "answer": ["William Caxton"], "scores": [[0.5845341086387634, 0.9140429496765137, 0.9998032450675964, 0.9966838955879211]], "normalized_score": [0.8542050123214722], "tokens": [["\u2581William", "\u2581Ca", "xt", "on"]], "entropy": [[3.2794604301452637, 0.7545872926712036, 0.002837075386196375, 0.033300574868917465]], "avg_entropy": [1.0175463432678953], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Caxton", "score_of_biggest_cluster": 0.8542050123214722, "score_of_first_answer": 0.8542050123214722, "max_score_of_first_answer": 0.9998032450675964, "min_score_of_first_answer": 0.5845341086387634, "avg_score": 0.8542050123214722, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.17049304667318924, "std_of_first_answer": 0.17049304667318924, "avg_entropy_of_first_answer": 1.0175463432678953, "lex_sim": 1.0}
{"question": "What religious text was recently adapted into a hit Broadway musical?", "answer": ["Book of Mormon"], "scores": [[0.25109046697616577, 0.9818865060806274, 0.9728891253471375, 0.9971292614936829, 0.9112374782562256]], "normalized_score": [0.7373396158218384], "tokens": [["\u2581Book", "\u2581of", "\u2581M", "orm", "on"]], "entropy": [[3.126372814178467, 0.12249059975147247, 0.2414034605026245, 0.03442500904202461, 0.35662922263145447]], "avg_entropy": [0.7762642212212085], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Book of mormon", "score_of_biggest_cluster": 0.7373396158218384, "score_of_first_answer": 0.7373396158218384, "max_score_of_first_answer": 0.9971292614936829, "min_score_of_first_answer": 0.25109046697616577, "avg_score": 0.7373396158218384, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2873669849670532, "std_of_first_answer": 0.2873669849670532, "avg_entropy_of_first_answer": 0.7762642212212085, "lex_sim": 1.0}
{"question": "What is the common name of the cooking ingredient from the hard fat of kidneys and loins of oxen, famously sold under the Atora brand?", "answer": ["Suet"], "scores": [[0.12976785004138947, 0.9586732983589172]], "normalized_score": [0.35271090269088745], "tokens": [["\u2581Su", "et"]], "entropy": [[5.3189263343811035, 0.37377113103866577]], "avg_entropy": [2.8463487327098846], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Beef suet", "score_of_biggest_cluster": 0.35271090269088745, "score_of_first_answer": 0.35271090269088745, "max_score_of_first_answer": 0.9586732983589172, "min_score_of_first_answer": 0.12976785004138947, "avg_score": 0.35271090269088745, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4144527241587639, "std_of_first_answer": 0.4144527241587639, "avg_entropy_of_first_answer": 2.8463487327098846, "lex_sim": 1.0}
{"question": "Harvey Littleton, Dominick Labino, Dale Chihuly, Dante Marioni, Fritz Driesbach and Marvin Lipofsky are associated with producing art from what material?", "answer": ["Glass"], "scores": [[0.43679818511009216, 0.979530394077301]], "normalized_score": [0.6541078686714172], "tokens": [["\u2581G", "lass"]], "entropy": [[4.190474510192871, 0.20582956075668335]], "avg_entropy": [2.198152035474777], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Optical glass", "score_of_biggest_cluster": 0.6541078686714172, "score_of_first_answer": 0.6541078686714172, "max_score_of_first_answer": 0.979530394077301, "min_score_of_first_answer": 0.43679818511009216, "avg_score": 0.6541078686714172, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27136610448360443, "std_of_first_answer": 0.27136610448360443, "avg_entropy_of_first_answer": 2.198152035474777, "lex_sim": 1.0}
{"question": "Which island of just over 30 square miles lies between North Uist and South Uist?", "answer": ["Benbecula Island"], "scores": [[0.14774851500988007, 0.8947447538375854, 0.9857022166252136, 0.05727554112672806]], "normalized_score": [0.2939234972000122], "tokens": [["\u2581Ben", "bec", "ula", "\u2581Island"]], "entropy": [[5.051865100860596, 0.8972861170768738, 0.12468751519918442, 0.8092221021652222]], "avg_entropy": [1.720765208825469], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Benbecula", "score_of_biggest_cluster": 0.2939234972000122, "score_of_first_answer": 0.2939234972000122, "max_score_of_first_answer": 0.9857022166252136, "min_score_of_first_answer": 0.05727554112672806, "avg_score": 0.2939234972000122, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.42130446086598966, "std_of_first_answer": 0.42130446086598966, "avg_entropy_of_first_answer": 1.720765208825469, "lex_sim": 1.0}
{"question": "Who commissioned the Domesday Book?", "answer": ["King William II"], "scores": [[0.1269351989030838, 0.8688739538192749, 0.034624677151441574]], "normalized_score": [0.156305730342865], "tokens": [["\u2581King", "\u2581William", "\u2581II"]], "entropy": [[3.9978837966918945, 0.859018862247467, 1.4484918117523193]], "avg_entropy": [2.101798156897227], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "King William I of England", "score_of_biggest_cluster": 0.156305730342865, "score_of_first_answer": 0.156305730342865, "max_score_of_first_answer": 0.8688739538192749, "min_score_of_first_answer": 0.034624677151441574, "avg_score": 0.156305730342865, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37341758038918954, "std_of_first_answer": 0.37341758038918954, "avg_entropy_of_first_answer": 2.101798156897227, "lex_sim": 1.0}
{"question": "Where are New Zealand's parliament buildings?", "answer": ["Wellington"], "scores": [[0.3933987319469452, 0.9721999764442444]], "normalized_score": [0.6184353232383728], "tokens": [["\u2581Well", "ington"]], "entropy": [[4.134952068328857, 0.22367338836193085]], "avg_entropy": [2.179312728345394], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "UN/LOCODE:NZWLG", "score_of_biggest_cluster": 0.6184353232383728, "score_of_first_answer": 0.6184353232383728, "max_score_of_first_answer": 0.9721999764442444, "min_score_of_first_answer": 0.3933987319469452, "avg_score": 0.6184353232383728, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2894006222486496, "std_of_first_answer": 0.2894006222486496, "avg_entropy_of_first_answer": 2.179312728345394, "lex_sim": 1.0}
{"question": "In which city was Pablo Picasso born, in 1881?", "answer": ["M\u00e1laga"], "scores": [[0.12449551373720169, 0.7012658715248108, 0.989162802696228]], "normalized_score": [0.4420126676559448], "tokens": [["\u2581M", "\u00e1l", "aga"]], "entropy": [[5.228816986083984, 2.2399919033050537, 0.09921940416097641]], "avg_entropy": [2.5226760978500047], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pedregalego", "score_of_biggest_cluster": 0.4420126676559448, "score_of_first_answer": 0.4420126676559448, "max_score_of_first_answer": 0.989162802696228, "min_score_of_first_answer": 0.12449551373720169, "avg_score": 0.4420126676559448, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3595055569337356, "std_of_first_answer": 0.3595055569337356, "avg_entropy_of_first_answer": 2.5226760978500047, "lex_sim": 1.0}
{"question": "Wookey Hole is part of which range of hills?", "answer": ["Aggas/Mendips"], "scores": [[0.00949899572879076, 0.04393356293439865, 0.11425454169511795, 0.0017847217386588454, 0.430868923664093, 0.9901746511459351, 0.7086007595062256]], "normalized_score": [0.08236981183290482], "tokens": [["\u2581A", "gg", "as", "/", "M", "end", "ips"]], "entropy": [[3.684048652648926, 6.009312629699707, 4.099315643310547, 3.126117467880249, 3.574984550476074, 0.10407533496618271, 0.6786623001098633]], "avg_entropy": [3.03950236844165], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "The Mendip Hills", "score_of_biggest_cluster": 0.08236981183290482, "score_of_first_answer": 0.08236981183290482, "max_score_of_first_answer": 0.9901746511459351, "min_score_of_first_answer": 0.0017847217386588454, "avg_score": 0.08236981183290482, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.36413360854702953, "std_of_first_answer": 0.36413360854702953, "avg_entropy_of_first_answer": 3.03950236844165, "lex_sim": 1.0}
{"question": "Which actor played the father of the Waltons children?", "answer": ["Walten Family"], "scores": [[0.01838940568268299, 0.0017182108713313937, 0.05603752285242081]], "normalized_score": [0.012097836472094059], "tokens": [["\u2581Wal", "ten", "\u2581Family"]], "entropy": [[5.650282382965088, 0.6023896932601929, 4.230454444885254]], "avg_entropy": [3.4943755070368447], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ralph Waite", "score_of_biggest_cluster": 0.012097836472094059, "score_of_first_answer": 0.012097836472094059, "max_score_of_first_answer": 0.05603752285242081, "min_score_of_first_answer": 0.0017182108713313937, "avg_score": 0.012097836472094059, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02272027268264394, "std_of_first_answer": 0.02272027268264394, "avg_entropy_of_first_answer": 3.4943755070368447, "lex_sim": 1.0}
{"question": "\"What word refers to the period before \"\"The Great Flood\"\" as described in the Old Testament of the Bible?\"", "answer": ["Antediluvian"], "scores": [[0.12244304269552231, 0.9700561165809631, 0.91046142578125, 0.9622910022735596, 0.9717061519622803]], "normalized_score": [0.632363498210907], "tokens": [["\u2581Ant", "ed", "il", "uv", "ian"]], "entropy": [[5.336153984069824, 0.22323748469352722, 0.42659202218055725, 0.18895503878593445, 0.17675358057022095]], "avg_entropy": [1.270338422060013], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Antediluvian", "score_of_biggest_cluster": 0.632363498210907, "score_of_first_answer": 0.632363498210907, "max_score_of_first_answer": 0.9717061519622803, "min_score_of_first_answer": 0.12244304269552231, "avg_score": 0.632363498210907, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.33323587636314955, "std_of_first_answer": 0.33323587636314955, "avg_entropy_of_first_answer": 1.270338422060013, "lex_sim": 1.0}
{"question": "\"In the film 'Casablanca', Humphrey Bogart said \"\"Play it again, Sam\"\". True or False?\"", "answer": ["False - [I\u2019ll] play it again...\u201d as Sam played it"], "scores": [[0.4016455113887787, 0.013328398577868938, 0.002022871747612953, 0.019550353288650513, 0.00671588908880949, 0.2321605086326599, 0.567078173160553, 0.314425528049469, 0.9318491816520691, 0.3035782277584076, 0.009199108928442001, 0.0031613637693226337, 0.0014115970116108656, 0.1343781054019928, 0.10539694875478745, 0.2118566930294037]], "normalized_score": [0.05093265324831009], "tokens": [["\u2581False", "\u2581-", "\u2581[", "I", "\u2019", "ll", "]", "\u2581play", "\u2581it", "\u2581again", "...", "\u201d", "\u2581as", "\u2581Sam", "\u2581played", "\u2581it"]], "entropy": [[3.3510711193084717, 1.5564589500427246, 4.125096797943115, 5.934776782989502, 3.558666467666626, 1.089382290840149, 2.5105133056640625, 3.080637216567993, 0.5025310516357422, 3.1454601287841797, 1.7225908041000366, 3.3495845794677734, 1.6495847702026367, 4.454184532165527, 4.038547992706299, 2.652191162109375]], "avg_entropy": [2.9200798720121384], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "False (disambiguation)", "score_of_biggest_cluster": 0.05093265324831009, "score_of_first_answer": 0.05093265324831009, "max_score_of_first_answer": 0.9318491816520691, "min_score_of_first_answer": 0.0014115970116108656, "avg_score": 0.05093265324831009, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2509487441960685, "std_of_first_answer": 0.2509487441960685, "avg_entropy_of_first_answer": 2.9200798720121384, "lex_sim": 1.0}
{"question": "To whom is Gwyneth Paltrow married?", "answer": ["Coldplay's Chris Martin"], "scores": [[0.06362566351890564, 0.932803213596344, 0.1019362360239029, 0.9977083206176758, 0.5284948945045471, 0.9062390327453613]], "normalized_score": [0.37743285298347473], "tokens": [["\u2581Cold", "play", "'", "s", "\u2581Chris", "\u2581Martin"]], "entropy": [[5.001793384552002, 0.6125906705856323, 2.612781047821045, 0.026271995157003403, 1.7478623390197754, 0.5313349366188049]], "avg_entropy": [1.7554390622923772], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Moses Bruce Martin", "score_of_biggest_cluster": 0.37743285298347473, "score_of_first_answer": 0.37743285298347473, "max_score_of_first_answer": 0.9977083206176758, "min_score_of_first_answer": 0.06362566351890564, "avg_score": 0.37743285298347473, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38789865890828557, "std_of_first_answer": 0.38789865890828557, "avg_entropy_of_first_answer": 1.7554390622923772, "lex_sim": 1.0}
{"question": "What is the name of the Mafia's vow of silence?", "answer": ["Omert\u00e0"], "scores": [[0.6128869652748108, 0.42872628569602966, 0.8372707962989807]], "normalized_score": [0.603682816028595], "tokens": [["\u2581Om", "ert", "\u00e0"]], "entropy": [[2.630744218826294, 0.7208662629127502, 1.1243807077407837]], "avg_entropy": [1.4919970631599426], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Omereta%60", "score_of_biggest_cluster": 0.603682816028595, "score_of_first_answer": 0.603682816028595, "max_score_of_first_answer": 0.8372707962989807, "min_score_of_first_answer": 0.42872628569602966, "avg_score": 0.603682816028595, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.16705683552871275, "std_of_first_answer": 0.16705683552871275, "avg_entropy_of_first_answer": 1.4919970631599426, "lex_sim": 1.0}
{"question": "Which teeth are the third molars, and are always the last to0 erupt?", "answer": ["Wisdom"], "scores": [[0.35151565074920654, 0.995665967464447]], "normalized_score": [0.5916013717651367], "tokens": [["\u2581Wis", "dom"]], "entropy": [[4.51728630065918, 0.04562380909919739]], "avg_entropy": [2.2814550548791885], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Wisdom-tooth", "score_of_biggest_cluster": 0.5916013717651367, "score_of_first_answer": 0.5916013717651367, "max_score_of_first_answer": 0.995665967464447, "min_score_of_first_answer": 0.35151565074920654, "avg_score": 0.5916013717651367, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.32207515835762024, "std_of_first_answer": 0.32207515835762024, "avg_entropy_of_first_answer": 2.2814550548791885, "lex_sim": 1.0}
{"question": "Who is the patron saint of dancers?", "answer": ["St. Genevieve"], "scores": [[0.1772509217262268, 0.5605615973472595, 0.11287043988704681, 0.8267530798912048, 0.9887346625328064]], "normalized_score": [0.39124566316604614], "tokens": [["\u2581St", ".", "\u2581Gene", "v", "ieve"]], "entropy": [[4.955696105957031, 2.7337522506713867, 4.395074844360352, 0.6618432402610779, 0.09052912890911102]], "avg_entropy": [2.567379114031792], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "St Vitus", "score_of_biggest_cluster": 0.39124566316604614, "score_of_first_answer": 0.39124566316604614, "max_score_of_first_answer": 0.9887346625328064, "min_score_of_first_answer": 0.11287043988704681, "avg_score": 0.39124566316604614, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3457775516405853, "std_of_first_answer": 0.3457775516405853, "avg_entropy_of_first_answer": 2.567379114031792, "lex_sim": 1.0}
{"question": "Which comedy duo were born Bartholomew and Wiseman?", "answer": ["The Marx Brothers"], "scores": [[0.04564222693443298, 0.012453999370336533, 0.8762064576148987]], "normalized_score": [0.07926730811595917], "tokens": [["\u2581The", "\u2581Marx", "\u2581Brothers"]], "entropy": [[5.497198104858398, 4.363457679748535, 0.6111881136894226]], "avg_entropy": [3.490614632765452], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Morcambe and Wise", "score_of_biggest_cluster": 0.07926730811595917, "score_of_first_answer": 0.07926730811595917, "max_score_of_first_answer": 0.8762064576148987, "min_score_of_first_answer": 0.012453999370336533, "avg_score": 0.07926730811595917, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39958404884999604, "std_of_first_answer": 0.39958404884999604, "avg_entropy_of_first_answer": 3.490614632765452, "lex_sim": 1.0}
{"question": "How many valves does a trumpet have?", "answer": ["Trumpet"], "scores": [[0.03876011446118355, 0.8655248284339905]], "normalized_score": [0.1831606924533844], "tokens": [["\u2581Trump", "et"]], "entropy": [[4.228568077087402, 0.6404086351394653]], "avg_entropy": [2.434488356113434], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "3", "score_of_biggest_cluster": 0.1831606924533844, "score_of_first_answer": 0.1831606924533844, "max_score_of_first_answer": 0.8655248284339905, "min_score_of_first_answer": 0.03876011446118355, "avg_score": 0.1831606924533844, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41338235698640347, "std_of_first_answer": 0.41338235698640347, "avg_entropy_of_first_answer": 2.434488356113434, "lex_sim": 1.0}
{"question": "Haematoma and Ecchymosis are medical terms which more commonly are called a?", "answer": ["Coffin stain (bruise, the color of old blood in a fresh bruise is the color of the inside surface of a coffin of fresh wood)"], "scores": [[0.0028508047107607126, 0.04044589400291443, 0.1499479115009308, 0.008687401190400124, 0.8017339706420898, 0.024686159566044807, 0.021923640742897987, 0.8775475025177002, 0.011535778641700745, 0.003658105619251728, 0.024120744317770004, 0.7105547189712524, 0.012380045838654041, 0.7757015228271484, 0.015634329989552498, 0.6516175270080566, 0.00865580327808857, 0.4611213505268097, 0.9967765212059021, 0.06509947031736374, 0.05352624133229256, 0.3516732454299927, 0.9661850333213806, 0.08629921823740005, 0.1592799574136734, 0.0018506089691072702, 0.9749386310577393, 0.7352490425109863, 0.4016984701156616, 0.9989520311355591, 0.002343247877433896, 0.013261044397950172, 0.22300735116004944, 0.47442272305488586]], "normalized_score": [0.08535277843475342], "tokens": [["\u2581C", "off", "in", "\u2581st", "ain", "\u2581(", "bru", "ise", ",", "\u2581the", "\u2581color", "\u2581of", "\u2581old", "\u2581blood", "\u2581in", "\u2581a", "\u2581fresh", "\u2581bru", "ise", "\u2581is", "\u2581the", "\u2581color", "\u2581of", "\u2581the", "\u2581inside", "\u2581surface", "\u2581of", "\u2581a", "\u2581coff", "in", "\u2581of", "\u2581fresh", "\u2581wood", ")"]], "entropy": [[3.4235620498657227, 5.321810722351074, 1.0274631977081299, 4.973545074462891, 1.116425633430481, 1.000708818435669, 5.613682270050049, 0.48363521695137024, 0.7305564880371094, 3.866260528564453, 5.431690216064453, 1.7765707969665527, 2.4475507736206055, 1.4651103019714355, 1.7482521533966064, 1.8863556385040283, 3.315680503845215, 2.162489652633667, 0.028081513941287994, 1.7219457626342773, 3.8092122077941895, 1.5768113136291504, 0.3010859489440918, 3.1680450439453125, 4.476746559143066, 0.22630366683006287, 0.21171993017196655, 1.0984115600585938, 3.4559051990509033, 0.011915819719433784, 2.3678762912750244, 3.292496681213379, 2.037217140197754, 2.391603946685791]], "avg_entropy": [2.2931390771204057], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Contuse", "score_of_biggest_cluster": 0.08535277843475342, "score_of_first_answer": 0.08535277843475342, "max_score_of_first_answer": 0.9989520311355591, "min_score_of_first_answer": 0.0018506089691072702, "avg_score": 0.08535277843475342, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36664597309611585, "std_of_first_answer": 0.36664597309611585, "avg_entropy_of_first_answer": 2.2931390771204057, "lex_sim": 1.0}
{"question": "In October 2003, which cricketer, playing in a Test series against Zimbabwe, scored a record 380 not out?", "answer": ["Matthew Hayden"], "scores": [[0.05797399953007698, 0.48890650272369385, 0.9015125632286072]], "normalized_score": [0.29453954100608826], "tokens": [["\u2581Matthew", "\u2581Hay", "den"]], "entropy": [[5.174322128295898, 3.5971031188964844, 0.6009923815727234]], "avg_entropy": [3.124139209588369], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Mathew Hayden", "score_of_biggest_cluster": 0.29453954100608826, "score_of_first_answer": 0.29453954100608826, "max_score_of_first_answer": 0.9015125632286072, "min_score_of_first_answer": 0.05797399953007698, "avg_score": 0.29453954100608826, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3444002664730999, "std_of_first_answer": 0.3444002664730999, "avg_entropy_of_first_answer": 3.124139209588369, "lex_sim": 1.0}
{"question": "In which city are the Oscar statuettes made?", "answer": ["Rodeo Drive, Los Angeles"], "scores": [[0.030640479177236557, 0.08265527337789536, 0.8830459713935852, 0.7288851737976074, 0.10566024482250214, 0.47152912616729736, 0.989010751247406]], "normalized_score": [0.26000136137008667], "tokens": [["\u2581R", "ode", "o", "\u2581Drive", ",", "\u2581Los", "\u2581Angeles"]], "entropy": [[5.109949111938477, 4.401551723480225, 0.8066748380661011, 1.6820114850997925, 1.1450763940811157, 1.990506887435913, 0.09408324211835861]], "avg_entropy": [2.1756933831742833], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chi-Beria", "score_of_biggest_cluster": 0.26000136137008667, "score_of_first_answer": 0.26000136137008667, "max_score_of_first_answer": 0.989010751247406, "min_score_of_first_answer": 0.030640479177236557, "avg_score": 0.26000136137008667, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3747034823704872, "std_of_first_answer": 0.3747034823704872, "avg_entropy_of_first_answer": 2.1756933831742833, "lex_sim": 1.0}
{"question": "Metz is the capital of which region of France?", "answer": ["Grand Est (Grand East)"], "scores": [[0.03964976221323013, 0.7955911159515381, 0.04956620931625366, 0.04195612668991089, 0.9908660054206848, 0.6966092586517334, 0.914426863193512]], "normalized_score": [0.2365192025899887], "tokens": [["\u2581Grand", "\u2581Est", "\u2581(", "Gr", "and", "\u2581East", ")"]], "entropy": [[4.431853294372559, 1.1619621515274048, 0.8663517832756042, 3.7629621028900146, 0.0777578353881836, 1.5171890258789062, 0.5684047341346741]], "avg_entropy": [1.769497275352478], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lorraine (France)", "score_of_biggest_cluster": 0.2365192025899887, "score_of_first_answer": 0.2365192025899887, "max_score_of_first_answer": 0.9908660054206848, "min_score_of_first_answer": 0.03964976221323013, "avg_score": 0.2365192025899887, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4076462250805089, "std_of_first_answer": 0.4076462250805089, "avg_entropy_of_first_answer": 1.769497275352478, "lex_sim": 1.0}
{"question": "In the TV series Cracker what is Fitz's christian name", "answer": ["Richard"], "scores": [[0.002577249426394701]], "normalized_score": [0.0025772498920559883], "tokens": [["\u2581Richard"]], "entropy": [[5.345732688903809]], "avg_entropy": [5.345732688903809], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eddie", "score_of_biggest_cluster": 0.0025772498920559883, "score_of_first_answer": 0.0025772498920559883, "max_score_of_first_answer": 0.002577249426394701, "min_score_of_first_answer": 0.002577249426394701, "avg_score": 0.0025772498920559883, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.345732688903809, "lex_sim": 1.0}
{"question": "What is prussic acid, found in bitter almonds and laurel leaves, also known as?", "answer": ["Hydrogen cyanide"], "scores": [[0.3134959638118744, 0.9286698698997498, 0.7157769203186035, 0.800041139125824, 0.9977660179138184, 0.9959222078323364]], "normalized_score": [0.7410933375358582], "tokens": [["\u2581H", "ydro", "gen", "\u2581cy", "an", "ide"]], "entropy": [[4.1852006912231445, 0.46664702892303467, 0.7742576003074646, 0.5851144194602966, 0.02530781365931034, 0.03673070669174194]], "avg_entropy": [1.0122097100441654], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Formic anammonide", "score_of_biggest_cluster": 0.7410933375358582, "score_of_first_answer": 0.7410933375358582, "max_score_of_first_answer": 0.9977660179138184, "min_score_of_first_answer": 0.3134959638118744, "avg_score": 0.7410933375358582, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2371508967018518, "std_of_first_answer": 0.2371508967018518, "avg_entropy_of_first_answer": 1.0122097100441654, "lex_sim": 1.0}
{"question": "Probably best remembered as being chief presenter of ITV's News At Ten between 1976 and 1991which British journalist and broadcaster sadly passed away on July 20th 2012?", "answer": ["Jonathan Charles William Mair Peel"], "scores": [[0.004793793894350529, 0.4980117082595825, 0.067163847386837, 0.026382001116871834, 0.02167356014251709, 0.006131456233561039, 0.8074223399162292]], "normalized_score": [0.04626787081360817], "tokens": [["\u2581Jonathan", "\u2581Charles", "\u2581William", "\u2581M", "air", "\u2581Pe", "el"]], "entropy": [[5.818580627441406, 3.5262651443481445, 4.618932247161865, 3.887495756149292, 3.4237403869628906, 4.076170921325684, 0.735283374786377]], "avg_entropy": [3.7266383511679515], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ALASTAIR BURNETT", "score_of_biggest_cluster": 0.04626787081360817, "score_of_first_answer": 0.04626787081360817, "max_score_of_first_answer": 0.8074223399162292, "min_score_of_first_answer": 0.004793793894350529, "avg_score": 0.04626787081360817, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2959029438697178, "std_of_first_answer": 0.2959029438697178, "avg_entropy_of_first_answer": 3.7266383511679515, "lex_sim": 1.0}
{"question": "Which Gilbert and Sullivan operetta is sub titled 'Bunthornes's Bride'?", "answer": ["Iolanthe"], "scores": [[0.019643930718302727, 0.800635576248169, 0.9418602585792542, 0.9973098039627075]], "normalized_score": [0.3486342132091522], "tokens": [["\u2581I", "ol", "ant", "he"]], "entropy": [[4.201257228851318, 1.3992819786071777, 0.38880228996276855, 0.02946300059556961]], "avg_entropy": [1.5047011245042086], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Impatience", "score_of_biggest_cluster": 0.3486342132091522, "score_of_first_answer": 0.3486342132091522, "max_score_of_first_answer": 0.9973098039627075, "min_score_of_first_answer": 0.019643930718302727, "avg_score": 0.3486342132091522, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.393538536891353, "std_of_first_answer": 0.393538536891353, "avg_entropy_of_first_answer": 1.5047011245042086, "lex_sim": 1.0}
{"question": "Which cardinal point does not appear in the names of the American states", "answer": ["N"], "scores": [[0.05734929442405701]], "normalized_score": [0.057349298149347305], "tokens": [["\u2581N"]], "entropy": [[4.569519996643066]], "avg_entropy": [4.569519996643066], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u6771", "score_of_biggest_cluster": 0.057349298149347305, "score_of_first_answer": 0.057349298149347305, "max_score_of_first_answer": 0.05734929442405701, "min_score_of_first_answer": 0.05734929442405701, "avg_score": 0.057349298149347305, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.569519996643066, "lex_sim": 1.0}
{"question": "Who invented the telescope in 1608?", "answer": ["Hans Lippershey"], "scores": [[0.2898717224597931, 0.857714831829071, 0.8785338997840881, 0.8881969451904297, 0.9901277422904968]], "normalized_score": [0.7189547419548035], "tokens": [["\u2581Hans", "\u2581Li", "ppers", "he", "y"]], "entropy": [[4.327436447143555, 1.1832208633422852, 0.46717357635498047, 0.6637437343597412, 0.08507063239812851]], "avg_entropy": [1.345329050719738], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Hans Lipperhey", "score_of_biggest_cluster": 0.7189547419548035, "score_of_first_answer": 0.7189547419548035, "max_score_of_first_answer": 0.9901277422904968, "min_score_of_first_answer": 0.2898717224597931, "avg_score": 0.7189547419548035, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.24973207567686098, "std_of_first_answer": 0.24973207567686098, "avg_entropy_of_first_answer": 1.345329050719738, "lex_sim": 1.0}
{"question": "When were the first Winter Paralympics held?", "answer": ["1976 (Stoke Mandeville Games)"], "scores": [[0.5472689270973206, 0.7264573574066162, 0.9514646530151367, 0.7014837265014648, 0.9710707068443298, 0.05370267108082771, 0.0067313970066607, 0.8803231120109558, 0.921734094619751, 0.9949702620506287, 0.982529878616333, 0.26106131076812744, 0.8370056748390198]], "normalized_score": [0.4279453456401825], "tokens": [["\u2581", "1", "9", "7", "6", "\u2581(", "Sto", "ke", "\u2581Man", "dev", "ille", "\u2581Games", ")"]], "entropy": [[3.3153533935546875, 1.2800050973892212, 0.3399995267391205, 1.0418565273284912, 0.19112801551818848, 2.5808284282684326, 4.457668304443359, 0.4025212228298187, 0.4764537811279297, 0.053889669477939606, 0.12504109740257263, 1.8543033599853516, 1.0289621353149414]], "avg_entropy": [1.3190777353369272], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "1976", "score_of_biggest_cluster": 0.4279453456401825, "score_of_first_answer": 0.4279453456401825, "max_score_of_first_answer": 0.9949702620506287, "min_score_of_first_answer": 0.0067313970066607, "avg_score": 0.4279453456401825, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3412247074202113, "std_of_first_answer": 0.3412247074202113, "avg_entropy_of_first_answer": 1.3190777353369272, "lex_sim": 1.0}
{"question": "What is the name of the town in which the Meerkats live in the adverts for comparethemarket.com?", "answer": ["Drayton-on-Five"], "scores": [[0.007272206712514162, 0.013056691735982895, 0.6817524433135986, 0.05543001741170883, 0.12260576337575912, 0.9906587600708008, 0.019808843731880188, 0.009971634484827518]], "normalized_score": [0.05519077181816101], "tokens": [["\u2581D", "ray", "ton", "-", "on", "-", "F", "ive"]], "entropy": [[5.475380897521973, 6.169492721557617, 2.1729178428649902, 3.58461332321167, 2.907900094985962, 0.10368295013904572, 4.513732433319092, 4.284778118133545]], "avg_entropy": [3.651562297716737], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Compare the Market", "score_of_biggest_cluster": 0.05519077181816101, "score_of_first_answer": 0.05519077181816101, "max_score_of_first_answer": 0.9906587600708008, "min_score_of_first_answer": 0.007272206712514162, "avg_score": 0.05519077181816101, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35593129758360004, "std_of_first_answer": 0.35593129758360004, "avg_entropy_of_first_answer": 3.651562297716737, "lex_sim": 1.0}
{"question": "Which US state is nicknamed The Equality State?", "answer": ["Equality State - Wyoming"], "scores": [[0.044693078845739365, 0.8455550670623779, 0.49709680676460266, 0.004900785628706217, 0.5681456327438354, 0.9886572957038879]], "normalized_score": [0.19301855564117432], "tokens": [["\u2581Equ", "ality", "\u2581State", "\u2581-", "\u2581Wy", "oming"]], "entropy": [[3.9623899459838867, 0.6393985748291016, 2.404086112976074, 1.6812989711761475, 3.0795512199401855, 0.09785100072622299]], "avg_entropy": [1.9774293042719364], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "State of Wyoming", "score_of_biggest_cluster": 0.19301855564117432, "score_of_first_answer": 0.19301855564117432, "max_score_of_first_answer": 0.9886572957038879, "min_score_of_first_answer": 0.004900785628706217, "avg_score": 0.19301855564117432, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3685198918810623, "std_of_first_answer": 0.3685198918810623, "avg_entropy_of_first_answer": 1.9774293042719364, "lex_sim": 1.0}
{"question": "Which artist, a member of the Order of Merit, was born in Bradford in July 1937?", "answer": ["Howard Hodgkin"], "scores": [[0.0028747334145009518, 0.5752270817756653, 0.9654533863067627, 0.9701201915740967, 0.8918991088867188]], "normalized_score": [0.2679550051689148], "tokens": [["\u2581Howard", "\u2581H", "od", "g", "kin"]], "entropy": [[4.537478446960449, 3.2549796104431152, 0.3223305940628052, 0.20766617357730865, 0.5136700868606567]], "avg_entropy": [1.767224982380867], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Hockney", "score_of_biggest_cluster": 0.2679550051689148, "score_of_first_answer": 0.2679550051689148, "max_score_of_first_answer": 0.9701201915740967, "min_score_of_first_answer": 0.0028747334145009518, "avg_score": 0.2679550051689148, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.36878838639178557, "std_of_first_answer": 0.36878838639178557, "avg_entropy_of_first_answer": 1.767224982380867, "lex_sim": 1.0}
{"question": "In which stadium was the 2014 Commonwealth Games Rugby Sevens tournament held?", "answer": ["Ibrox Stadium"], "scores": [[0.2454393357038498, 0.9353936910629272, 0.9949889779090881, 0.509955108165741]], "normalized_score": [0.5842143297195435], "tokens": [["\u2581I", "bro", "x", "\u2581Stadium"]], "entropy": [[4.729965686798096, 0.5744368433952332, 0.05456717312335968, 1.4219956398010254]], "avg_entropy": [1.6952413357794285], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Hunnery", "score_of_biggest_cluster": 0.5842143297195435, "score_of_first_answer": 0.5842143297195435, "max_score_of_first_answer": 0.9949889779090881, "min_score_of_first_answer": 0.2454393357038498, "avg_score": 0.5842143297195435, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30899410525527427, "std_of_first_answer": 0.30899410525527427, "avg_entropy_of_first_answer": 1.6952413357794285, "lex_sim": 1.0}
{"question": "From the Latin meaning of clip/cut, what hairstyle symbolizes religious devotion?", "answer": ["Nazarite Vow"], "scores": [[0.014496101066470146, 0.4969571530818939, 0.7686437964439392, 0.05511302873492241, 0.9754945039749146]], "normalized_score": [0.19713094830513], "tokens": [["\u2581Naz", "ar", "ite", "\u2581V", "ow"]], "entropy": [[4.899359703063965, 1.350804090499878, 1.1692581176757812, 1.525721788406372, 0.15012140572071075]], "avg_entropy": [1.8190530210733413], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Tonsuring", "score_of_biggest_cluster": 0.19713094830513, "score_of_first_answer": 0.19713094830513, "max_score_of_first_answer": 0.9754945039749146, "min_score_of_first_answer": 0.014496101066470146, "avg_score": 0.19713094830513, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.38072186008161907, "std_of_first_answer": 0.38072186008161907, "avg_entropy_of_first_answer": 1.8190530210733413, "lex_sim": 1.0}
{"question": "In which city is the majority of Shakespeare's Romeo and Juliet set?", "answer": ["Shakespeare's Romeo and Juliet"], "scores": [[0.007764441426843405, 0.6011757850646973, 0.9982323050498962, 0.32822057604789734, 0.9949111342430115, 0.8752449750900269, 0.9934525489807129, 0.9970560073852539]], "normalized_score": [0.4365522563457489], "tokens": [["\u2581Shakespeare", "'", "s", "\u2581Rome", "o", "\u2581and", "\u2581Jul", "iet"]], "entropy": [[5.254485130310059, 2.290274143218994, 0.02421894110739231, 3.8422183990478516, 0.04793769493699074, 0.7429441809654236, 0.0730481967329979, 0.0268178079277277]], "avg_entropy": [1.5377430617809296], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "San Michele Extra", "score_of_biggest_cluster": 0.4365522563457489, "score_of_first_answer": 0.4365522563457489, "max_score_of_first_answer": 0.9982323050498962, "min_score_of_first_answer": 0.007764441426843405, "avg_score": 0.4365522563457489, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3541457960666769, "std_of_first_answer": 0.3541457960666769, "avg_entropy_of_first_answer": 1.5377430617809296, "lex_sim": 1.0}
{"question": "In 2008 government in Bhutan was changed from an absolute monarchy to what?", "answer": ["A parliamentary one-party system; also called \"constitutional monarchy\""], "scores": [[0.07526351511478424, 0.06979978829622269, 0.9512752890586853, 0.0045950040221214294, 0.026762668043375015, 0.8809307813644409, 0.49294593930244446, 0.001866992679424584, 0.005755132529884577, 0.18294744193553925, 0.03399522602558136, 0.2223627269268036, 0.9946731925010681, 0.9971319437026978, 0.8588141202926636, 0.9939582347869873, 0.7678638696670532]], "normalized_score": [0.13862468302249908], "tokens": [["\u2581A", "\u2581parliament", "ary", "\u2581one", "-", "party", "\u2581system", ";", "\u2581also", "\u2581called", "\u2581\"", "const", "itution", "al", "\u2581mon", "archy", "\""]], "entropy": [[3.8526856899261475, 2.404486656188965, 0.325465589761734, 1.5825601816177368, 0.9388917684555054, 0.8128962516784668, 2.3751959800720215, 1.4213277101516724, 4.901759147644043, 2.801912784576416, 2.656355857849121, 3.711937189102173, 0.04543613642454147, 0.028450001031160355, 0.8335311412811279, 0.05934803560376167, 1.025299072265625]], "avg_entropy": [1.7516199525664835], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Monarchic republic", "score_of_biggest_cluster": 0.13862468302249908, "score_of_first_answer": 0.13862468302249908, "max_score_of_first_answer": 0.9971319437026978, "min_score_of_first_answer": 0.001866992679424584, "avg_score": 0.13862468302249908, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4169428540794694, "std_of_first_answer": 0.4169428540794694, "avg_entropy_of_first_answer": 1.7516199525664835, "lex_sim": 1.0}
{"question": "Who was the last monarch to use Hampton Court as an official residence?", "answer": ["King William 1V"], "scores": [[0.07569113373756409, 0.3833855092525482, 0.0057136304676532745, 0.20758159458637238, 0.21181626617908478]], "normalized_score": [0.09387462586164474], "tokens": [["\u2581King", "\u2581William", "\u2581", "1", "V"]], "entropy": [[4.485649108886719, 1.737825632095337, 1.7076269388198853, 1.9465454816818237, 1.8565349578857422]], "avg_entropy": [2.3468364238739015], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "GEORGE II", "score_of_biggest_cluster": 0.09387462586164474, "score_of_first_answer": 0.09387462586164474, "max_score_of_first_answer": 0.3833855092525482, "min_score_of_first_answer": 0.0057136304676532745, "avg_score": 0.09387462586164474, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12988052948250223, "std_of_first_answer": 0.12988052948250223, "avg_entropy_of_first_answer": 2.3468364238739015, "lex_sim": 1.0}
{"question": "Associated with salmon fishing and whisky production, which is the second longest river in Scotland?", "answer": ["Dilution"], "scores": [[0.007092085666954517, 0.009209315292537212, 0.1210494413971901]], "normalized_score": [0.019921470433473587], "tokens": [["\u2581D", "il", "ution"]], "entropy": [[5.026514530181885, 6.009099006652832, 4.463242530822754]], "avg_entropy": [5.166285355885823], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Spey (disambiguation)", "score_of_biggest_cluster": 0.019921470433473587, "score_of_first_answer": 0.019921470433473587, "max_score_of_first_answer": 0.1210494413971901, "min_score_of_first_answer": 0.007092085666954517, "avg_score": 0.019921470433473587, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.053227995342279504, "std_of_first_answer": 0.053227995342279504, "avg_entropy_of_first_answer": 5.166285355885823, "lex_sim": 1.0}
{"question": "A set of traditional laws called Kanun influence life in what mountainous European country?", "answer": ["Albania"], "scores": [[0.41620323061943054, 0.9115453362464905]], "normalized_score": [0.6159449219703674], "tokens": [["\u2581Alban", "ia"]], "entropy": [[4.087738990783691, 0.4297681450843811]], "avg_entropy": [2.2587535679340363], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Republic of Albania", "score_of_biggest_cluster": 0.6159449219703674, "score_of_first_answer": 0.6159449219703674, "max_score_of_first_answer": 0.9115453362464905, "min_score_of_first_answer": 0.41620323061943054, "avg_score": 0.6159449219703674, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.24767105281352997, "std_of_first_answer": 0.24767105281352997, "avg_entropy_of_first_answer": 2.2587535679340363, "lex_sim": 1.0}
{"question": "The painting by Millais entitled 'Bubbles' was used for many years in an advertisement for which product?", "answer": ["Ariel Washing Powder"], "scores": [[0.15322892367839813, 0.7214457392692566, 0.059126995503902435, 0.9917232394218445, 0.9623181223869324, 0.8126561045646667, 0.9967865943908691]], "normalized_score": [0.46982431411743164], "tokens": [["\u2581A", "riel", "\u2581W", "ash", "ing", "\u2581Pow", "der"]], "entropy": [[4.667756080627441, 1.7627426385879517, 2.1251816749572754, 0.0826355367898941, 0.2688272297382355, 1.0168819427490234, 0.028350310400128365]], "avg_entropy": [1.4217679162642785], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pears' Soap", "score_of_biggest_cluster": 0.46982431411743164, "score_of_first_answer": 0.46982431411743164, "max_score_of_first_answer": 0.9967865943908691, "min_score_of_first_answer": 0.059126995503902435, "avg_score": 0.46982431411743164, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3701260835051539, "std_of_first_answer": 0.3701260835051539, "avg_entropy_of_first_answer": 1.4217679162642785, "lex_sim": 1.0}
{"question": "The English Football Association banned (What?) from 1921 to 1971 at its member clubs?", "answer": ["The wearing of black kits"], "scores": [[0.049208059906959534, 0.04201415553689003, 0.9995176792144775, 0.9850999712944031, 0.157365620136261, 0.004663486965000629, 0.4907878637313843]], "normalized_score": [0.13292372226715088], "tokens": [["\u2581The", "\u2581we", "aring", "\u2581of", "\u2581black", "\u2581k", "its"]], "entropy": [[6.065024375915527, 5.58922004699707, 0.006070820614695549, 0.1337105929851532, 4.071723461151123, 2.5242528915405273, 0.7917733788490295]], "avg_entropy": [2.740253652579018], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Women's Football games", "score_of_biggest_cluster": 0.13292372226715088, "score_of_first_answer": 0.13292372226715088, "max_score_of_first_answer": 0.9995176792144775, "min_score_of_first_answer": 0.004663486965000629, "avg_score": 0.13292372226715088, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4098170306385878, "std_of_first_answer": 0.4098170306385878, "avg_entropy_of_first_answer": 2.740253652579018, "lex_sim": 1.0}
{"question": "What was the name of the Russian Submarine that sank in the Barents Sea in the year 2000 ?", "answer": ["Gurza-class patrol boat"], "scores": [[0.0032532887998968363, 0.005600426811724901, 0.018335970118641853, 0.34231212735176086, 0.7731986045837402, 0.22386813163757324, 0.9999655485153198, 0.8337748646736145]], "normalized_score": [0.10646291822195053], "tokens": [["\u2581G", "ur", "za", "-", "class", "\u2581pat", "rol", "\u2581boat"]], "entropy": [[5.247305870056152, 5.3641462326049805, 5.710363388061523, 2.1334924697875977, 1.3282335996627808, 3.0197243690490723, 0.0004788601363543421, 0.8485302329063416]], "avg_entropy": [2.9565343777831004], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kursk, Russia", "score_of_biggest_cluster": 0.10646291822195053, "score_of_first_answer": 0.10646291822195053, "max_score_of_first_answer": 0.9999655485153198, "min_score_of_first_answer": 0.0032532887998968363, "avg_score": 0.10646291822195053, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38411801779318305, "std_of_first_answer": 0.38411801779318305, "avg_entropy_of_first_answer": 2.9565343777831004, "lex_sim": 1.0}
{"question": "What is the second event of a men\u2019s decathlon?", "answer": ["1500 metres"], "scores": [[0.3453470468521118, 0.6716647744178772, 0.017070159316062927, 0.9452749490737915, 0.9704164862632751, 0.27344080805778503]], "normalized_score": [0.31586700677871704], "tokens": [["\u2581", "1", "5", "0", "0", "\u2581metres"]], "entropy": [[3.972393751144409, 1.668923258781433, 0.7175140380859375, 0.45946604013442993, 0.24204227328300476, 2.127882242202759]], "avg_entropy": [1.5313702672719955], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Longjumper", "score_of_biggest_cluster": 0.31586700677871704, "score_of_first_answer": 0.31586700677871704, "max_score_of_first_answer": 0.9704164862632751, "min_score_of_first_answer": 0.017070159316062927, "avg_score": 0.31586700677871704, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35336335918637635, "std_of_first_answer": 0.35336335918637635, "avg_entropy_of_first_answer": 1.5313702672719955, "lex_sim": 1.0}
{"question": "Anaheim, Bell, Hungarian Wax, Poblano, and Scotch Bonnet are all types of what?", "answer": ["Chilli peppers"], "scores": [[0.3323911726474762, 0.41308802366256714, 0.3536914587020874, 0.4021795988082886]], "normalized_score": [0.3738386332988739], "tokens": [["\u2581Ch", "illi", "\u2581pe", "ppers"]], "entropy": [[4.273034572601318, 1.418927788734436, 1.7061110734939575, 0.7090498208999634]], "avg_entropy": [2.026780813932419], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Chili Pepper", "score_of_biggest_cluster": 0.3738386332988739, "score_of_first_answer": 0.3738386332988739, "max_score_of_first_answer": 0.41308802366256714, "min_score_of_first_answer": 0.3323911726474762, "avg_score": 0.3738386332988739, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03338614399027287, "std_of_first_answer": 0.03338614399027287, "avg_entropy_of_first_answer": 2.026780813932419, "lex_sim": 1.0}
{"question": "In 2012 Harvard professor Jonathan Tilley announced research apparently proving (contrary to all previous medical thinking) what human feature can be enabled to be replenished in the host body?", "answer": ["Limbs"], "scores": [[0.0031793357338756323, 0.4499359726905823]], "normalized_score": [0.037821922451257706], "tokens": [["\u2581Lim", "bs"]], "entropy": [[5.599278450012207, 1.003652811050415]], "avg_entropy": [3.301465630531311], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Egg (biology)", "score_of_biggest_cluster": 0.037821922451257706, "score_of_first_answer": 0.037821922451257706, "max_score_of_first_answer": 0.4499359726905823, "min_score_of_first_answer": 0.0031793357338756323, "avg_score": 0.037821922451257706, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22337831847835332, "std_of_first_answer": 0.22337831847835332, "avg_entropy_of_first_answer": 3.301465630531311, "lex_sim": 1.0}
{"question": "Which black and white marine creature is the largest member of the Dolphin family?", "answer": ["Killer Whale"], "scores": [[0.12409751862287521, 0.9417674541473389, 0.36422526836395264, 0.8598639369010925]], "normalized_score": [0.437397837638855], "tokens": [["\u2581K", "iller", "\u2581Wh", "ale"]], "entropy": [[5.050065994262695, 0.5819080471992493, 1.4716722965240479, 0.4695349335670471]], "avg_entropy": [1.8932953178882599], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Killer Whales in the Kwakiutl Culture", "score_of_biggest_cluster": 0.437397837638855, "score_of_first_answer": 0.437397837638855, "max_score_of_first_answer": 0.9417674541473389, "min_score_of_first_answer": 0.12409751862287521, "avg_score": 0.437397837638855, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3403599688352279, "std_of_first_answer": 0.3403599688352279, "avg_entropy_of_first_answer": 1.8932953178882599, "lex_sim": 1.0}
{"question": "Which islands were ceded to Britain by Spain in 1771?", "answer": ["Falkland"], "scores": [[0.2715848982334137, 0.9739073514938354, 0.7518014907836914]], "normalized_score": [0.5836809277534485], "tokens": [["\u2581F", "alk", "land"]], "entropy": [[4.788993835449219, 0.2678152024745941, 0.7129566073417664]], "avg_entropy": [1.9232552150885265], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Malvinas/Falklands", "score_of_biggest_cluster": 0.5836809277534485, "score_of_first_answer": 0.5836809277534485, "max_score_of_first_answer": 0.9739073514938354, "min_score_of_first_answer": 0.2715848982334137, "avg_score": 0.5836809277534485, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2931051797328678, "std_of_first_answer": 0.2931051797328678, "avg_entropy_of_first_answer": 1.9232552150885265, "lex_sim": 1.0}
{"question": "Ford recently announced that they were shutting down production of another of their marques. Active for the last 81 years, what brand is being discontinued?", "answer": ["Mercury"], "scores": [[0.38982954621315, 0.9560238122940063]], "normalized_score": [0.6104804277420044], "tokens": [["\u2581Mercur", "y"]], "entropy": [[4.395212173461914, 0.3479130268096924]], "avg_entropy": [2.3715626001358032], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Mercury", "score_of_biggest_cluster": 0.6104804277420044, "score_of_first_answer": 0.6104804277420044, "max_score_of_first_answer": 0.9560238122940063, "min_score_of_first_answer": 0.38982954621315, "avg_score": 0.6104804277420044, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.28309713304042816, "std_of_first_answer": 0.28309713304042816, "avg_entropy_of_first_answer": 2.3715626001358032, "lex_sim": 1.0}
{"question": "Which group had a number one hit in 2009 with 'I Gotta Feeling'?", "answer": ["The Black Eyed Peas."], "scores": [[0.2691875398159027, 0.6957158446311951, 0.9217400550842285, 0.9527013897895813, 0.7931729555130005, 0.9688413143157959, 0.0030308288987725973]], "normalized_score": [0.3250053822994232], "tokens": [["\u2581The", "\u2581Black", "\u2581E", "yed", "\u2581Pe", "as", "."]], "entropy": [[4.333733081817627, 2.7390804290771484, 0.6773199439048767, 0.2628396153450012, 1.6697165966033936, 0.21477502584457397, 0.761648416519165]], "avg_entropy": [1.5227304441588265], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "The Black Eyed Peas", "score_of_biggest_cluster": 0.3250053822994232, "score_of_first_answer": 0.3250053822994232, "max_score_of_first_answer": 0.9688413143157959, "min_score_of_first_answer": 0.0030308288987725973, "avg_score": 0.3250053822994232, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.34905661335295063, "std_of_first_answer": 0.34905661335295063, "avg_entropy_of_first_answer": 1.5227304441588265, "lex_sim": 1.0}
{"question": "Charcot-Marie-Tooth disease is an inherited muscle wasting disease affecting which part of the body?", "answer": ["hand"], "scores": [[0.003925197757780552]], "normalized_score": [0.003925197757780552], "tokens": [["\u2581hand"]], "entropy": [[5.098762512207031]], "avg_entropy": [5.098762512207031], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Legs", "score_of_biggest_cluster": 0.003925197757780552, "score_of_first_answer": 0.003925197757780552, "max_score_of_first_answer": 0.003925197757780552, "min_score_of_first_answer": 0.003925197757780552, "avg_score": 0.003925197757780552, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.098762512207031, "lex_sim": 1.0}
{"question": "What was the first name of the older sister of German-born diarist Anne Frank?", "answer": ["Dana"], "scores": [[0.007492563221603632, 0.007181392051279545]], "normalized_score": [0.007335326634347439], "tokens": [["\u2581D", "ana"]], "entropy": [[5.753100395202637, 5.328856468200684]], "avg_entropy": [5.54097843170166], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Margot (name)", "score_of_biggest_cluster": 0.007335326634347439, "score_of_first_answer": 0.007335326634347439, "max_score_of_first_answer": 0.007492563221603632, "min_score_of_first_answer": 0.007181392051279545, "avg_score": 0.007335326634347439, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.00015558558516204357, "std_of_first_answer": 0.00015558558516204357, "avg_entropy_of_first_answer": 5.54097843170166, "lex_sim": 1.0}
{"question": "Who was the first Spanish actor to win an Oscar, for his role in the film No Country for Old Men?", "answer": ["Jos\u00e9 Vicente Ferrer"], "scores": [[0.002218174049630761, 0.2375725507736206, 0.9882247447967529, 0.9305342435836792, 0.9498306512832642]], "normalized_score": [0.2150825709104538], "tokens": [["\u2581Jos\u00e9", "\u2581Vic", "ente", "\u2581Fer", "rer"]], "entropy": [[4.763058662414551, 3.7943954467773438, 0.09343263506889343, 0.5127198696136475, 0.3168013095855713]], "avg_entropy": [1.8960815846920014], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Javier \u00c1ngel Encinas Bardem", "score_of_biggest_cluster": 0.2150825709104538, "score_of_first_answer": 0.2150825709104538, "max_score_of_first_answer": 0.9882247447967529, "min_score_of_first_answer": 0.002218174049630761, "avg_score": 0.2150825709104538, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41682138379388467, "std_of_first_answer": 0.41682138379388467, "avg_entropy_of_first_answer": 1.8960815846920014, "lex_sim": 1.0}
{"question": "How many avenues radiate from the Arc de Triomphe?", "answer": ["365"], "scores": [[0.3151606321334839, 0.017905190587043762, 0.19335757195949554, 0.19764308631420135]], "normalized_score": [0.12118209153413773], "tokens": [["\u2581", "3", "6", "5"]], "entropy": [[4.6302056312561035, 1.6910394430160522, 3.665374994277954, 2.874110698699951]], "avg_entropy": [3.2151826918125153], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "twelve", "score_of_biggest_cluster": 0.12118209153413773, "score_of_first_answer": 0.12118209153413773, "max_score_of_first_answer": 0.3151606321334839, "min_score_of_first_answer": 0.017905190587043762, "avg_score": 0.12118209153413773, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1060998266654686, "std_of_first_answer": 0.1060998266654686, "avg_entropy_of_first_answer": 3.2151826918125153, "lex_sim": 1.0}
{"question": "What is the surname of the title character in Kingsley Amis' novel 'Lucky Jim'?", "answer": ["Dixon"], "scores": [[0.31548044085502625, 0.9841392636299133]], "normalized_score": [0.5572043657302856], "tokens": [["\u2581D", "ixon"]], "entropy": [[4.856344223022461, 0.15660488605499268]], "avg_entropy": [2.506474554538727], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "'DIXON'", "score_of_biggest_cluster": 0.5572043657302856, "score_of_first_answer": 0.5572043657302856, "max_score_of_first_answer": 0.9841392636299133, "min_score_of_first_answer": 0.31548044085502625, "avg_score": 0.5572043657302856, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.33432941138744354, "std_of_first_answer": 0.33432941138744354, "avg_entropy_of_first_answer": 2.506474554538727, "lex_sim": 1.0}
{"question": "Octavian, who became Augustus, and Marcus Aemilius Lepidus were two members of the 'Second Triumvirate' in 43 BC. Who was the third?", "answer": ["Mark Antony"], "scores": [[0.10763988643884659, 0.5072749853134155, 0.9885791540145874]], "normalized_score": [0.3779282569885254], "tokens": [["\u2581Mark", "\u2581Anton", "y"]], "entropy": [[5.988034248352051, 1.703866958618164, 0.09111393988132477]], "avg_entropy": [2.5943383822838464], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Mark antony", "score_of_biggest_cluster": 0.3779282569885254, "score_of_first_answer": 0.3779282569885254, "max_score_of_first_answer": 0.9885791540145874, "min_score_of_first_answer": 0.10763988643884659, "avg_score": 0.3779282569885254, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.36015674183741714, "std_of_first_answer": 0.36015674183741714, "avg_entropy_of_first_answer": 2.5943383822838464, "lex_sim": 1.0}
{"question": "\"\"\"The Great Gate of Kiev\"\" and \"\"Catacombs\"\" are two sections of which musical work by Mussorgsky?\"", "answer": ["Pictures at an Exhibition (composition)"], "scores": [[0.3847227096557617, 0.9572855234146118, 0.990545392036438, 0.9547287821769714, 0.989723801612854, 0.9973223805427551, 0.17461934685707092, 0.026533599942922592, 0.9895997047424316, 0.983018159866333]], "normalized_score": [0.5236248970031738], "tokens": [["\u2581Pictures", "\u2581at", "\u2581an", "\u2581Ex", "hib", "ition", "\u2581(", "com", "position", ")"]], "entropy": [[4.035593032836914, 0.27506309747695923, 0.07384075969457626, 0.2140062153339386, 0.07624150812625885, 0.02289554476737976, 1.0070592164993286, 3.5100531578063965, 0.07001800835132599, 0.13090915977954865]], "avg_entropy": [0.9415679700672627], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Bilder einer Ausstellung", "score_of_biggest_cluster": 0.5236248970031738, "score_of_first_answer": 0.5236248970031738, "max_score_of_first_answer": 0.9973223805427551, "min_score_of_first_answer": 0.026533599942922592, "avg_score": 0.5236248970031738, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3688779269705404, "std_of_first_answer": 0.3688779269705404, "avg_entropy_of_first_answer": 0.9415679700672627, "lex_sim": 1.0}
{"question": "Name the pioneering Game Theory US mathematician featured in the film A Beautiful Mind?", "answer": ["John Nash"], "scores": [[0.38199204206466675, 0.6360756158828735]], "normalized_score": [0.4929257929325104], "tokens": [["\u2581John", "\u2581Nash"]], "entropy": [[4.377488136291504, 1.7627766132354736]], "avg_entropy": [3.0701323747634888], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Nash (disambiguation)", "score_of_biggest_cluster": 0.4929257929325104, "score_of_first_answer": 0.4929257929325104, "max_score_of_first_answer": 0.6360756158828735, "min_score_of_first_answer": 0.38199204206466675, "avg_score": 0.4929257929325104, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1270417869091034, "std_of_first_answer": 0.1270417869091034, "avg_entropy_of_first_answer": 3.0701323747634888, "lex_sim": 1.0}
{"question": "What did Davey Crocket call his rifle", "answer": ["Babe"], "scores": [[0.04990166425704956, 0.022640302777290344]], "normalized_score": [0.03361232951283455], "tokens": [["\u2581B", "abe"]], "entropy": [[5.768238544464111, 1.769965648651123]], "avg_entropy": [3.769102096557617], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Old Betsy", "score_of_biggest_cluster": 0.03361232951283455, "score_of_first_answer": 0.03361232951283455, "max_score_of_first_answer": 0.04990166425704956, "min_score_of_first_answer": 0.022640302777290344, "avg_score": 0.03361232951283455, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.013630680739879608, "std_of_first_answer": 0.013630680739879608, "avg_entropy_of_first_answer": 3.769102096557617, "lex_sim": 1.0}
{"question": "What island chain includes the islands of Fuerteventura, Lanzarote, and La Palma?", "answer": ["The Hawaii Volcano Observatory"], "scores": [[0.07904395461082458, 0.001204738044179976, 0.10090303421020508, 0.03928179666399956, 0.6865005493164062, 0.2108374536037445, 0.9892191290855408]], "normalized_score": [0.09158400446176529], "tokens": [["\u2581The", "\u2581Hawai", "i", "\u2581Vol", "cano", "\u2581Observ", "atory"]], "entropy": [[4.7464399337768555, 3.0485265254974365, 0.5295904874801636, 2.9542236328125, 0.6598598957061768, 2.015878677368164, 0.07609772682189941]], "avg_entropy": [2.0043738399233137], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Canary islands", "score_of_biggest_cluster": 0.09158400446176529, "score_of_first_answer": 0.09158400446176529, "max_score_of_first_answer": 0.9892191290855408, "min_score_of_first_answer": 0.001204738044179976, "avg_score": 0.09158400446176529, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3541715538255358, "std_of_first_answer": 0.3541715538255358, "avg_entropy_of_first_answer": 2.0043738399233137, "lex_sim": 1.0}
{"question": "In British legend, what is the name of King Arthur\u2019s sword?", "answer": ["\ud83d\udd25"], "scores": [[0.027061911299824715, 0.02550605870783329, 0.8460385203361511, 0.13778774440288544, 0.10789956897497177]], "normalized_score": [0.0972130224108696], "tokens": [["\u2581", "<0xF0>", "<0x9F>", "<0x94>", "<0xA5>"]], "entropy": [[4.856540679931641, 4.860467910766602, 0.7334959506988525, 3.068699359893799, 2.964123010635376]], "avg_entropy": [3.296665382385254], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rhongomiant", "score_of_biggest_cluster": 0.0972130224108696, "score_of_first_answer": 0.0972130224108696, "max_score_of_first_answer": 0.8460385203361511, "min_score_of_first_answer": 0.02550605870783329, "avg_score": 0.0972130224108696, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3117403411854329, "std_of_first_answer": 0.3117403411854329, "avg_entropy_of_first_answer": 3.296665382385254, "lex_sim": 0.0}
{"question": "Which city is the destination of the yacht race which leaves Sydney every Boxing Day?", "answer": ["Pulling Out"], "scores": [[0.002550908364355564, 0.005258242599666119, 0.1734081208705902, 0.016139542683959007]], "normalized_score": [0.013919512741267681], "tokens": [["\u2581P", "ull", "ing", "\u2581Out"]], "entropy": [[4.764117240905762, 5.99569034576416, 4.479844093322754, 5.199468612670898]], "avg_entropy": [5.1097800731658936], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "UN/LOCODE:AUHBA", "score_of_biggest_cluster": 0.013919512741267681, "score_of_first_answer": 0.013919512741267681, "max_score_of_first_answer": 0.1734081208705902, "min_score_of_first_answer": 0.002550908364355564, "avg_score": 0.013919512741267681, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07181154052693661, "std_of_first_answer": 0.07181154052693661, "avg_entropy_of_first_answer": 5.1097800731658936, "lex_sim": 1.0}
{"question": "In which part of the human body is the limbic system?", "answer": ["The human brain"], "scores": [[0.16371548175811768, 0.07551155984401703, 0.6340263485908508]], "normalized_score": [0.1986416131258011], "tokens": [["\u2581The", "\u2581human", "\u2581brain"]], "entropy": [[4.8987202644348145, 4.501481056213379, 1.9139271974563599]], "avg_entropy": [3.7713761727015176], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brain cell", "score_of_biggest_cluster": 0.1986416131258011, "score_of_first_answer": 0.1986416131258011, "max_score_of_first_answer": 0.6340263485908508, "min_score_of_first_answer": 0.07551155984401703, "avg_score": 0.1986416131258011, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2451555090483739, "std_of_first_answer": 0.2451555090483739, "avg_entropy_of_first_answer": 3.7713761727015176, "lex_sim": 1.0}
{"question": "King Louie, Akela and Ziggy are all characters in which Disney film?", "answer": ["The jungle book"], "scores": [[0.5201582908630371, 0.011796118691563606, 0.9817970991134644, 0.9973623156547546, 0.7892306447029114]], "normalized_score": [0.3429182767868042], "tokens": [["\u2581The", "\u2581j", "ung", "le", "\u2581book"]], "entropy": [[3.099236488342285, 1.024733304977417, 0.1809084415435791, 0.028513485565781593, 0.8863658905029297]], "avg_entropy": [1.0439515221863984], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "The Jungle Book", "score_of_biggest_cluster": 0.3429182767868042, "score_of_first_answer": 0.3429182767868042, "max_score_of_first_answer": 0.9973623156547546, "min_score_of_first_answer": 0.011796118691563606, "avg_score": 0.3429182767868042, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3670943247117818, "std_of_first_answer": 0.3670943247117818, "avg_entropy_of_first_answer": 1.0439515221863984, "lex_sim": 1.0}
{"question": "What do we call the vegetable called the rutabaga in America?", "answer": ["Swedes"], "scores": [[0.31675252318382263, 0.10905179381370544]], "normalized_score": [0.1858559548854828], "tokens": [["\u2581Sw", "edes"]], "entropy": [[4.493222236633301, 0.4654954671859741]], "avg_entropy": [2.4793588519096375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Svenskar", "score_of_biggest_cluster": 0.1858559548854828, "score_of_first_answer": 0.1858559548854828, "max_score_of_first_answer": 0.31675252318382263, "min_score_of_first_answer": 0.10905179381370544, "avg_score": 0.1858559548854828, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1038503646850586, "std_of_first_answer": 0.1038503646850586, "avg_entropy_of_first_answer": 2.4793588519096375, "lex_sim": 1.0}
{"question": "What successful racehorse trainer died on 1 June 2009?", "answer": ["Henry Cecil"], "scores": [[0.22615830600261688, 0.911448061466217, 0.9987465143203735]], "normalized_score": [0.5904728174209595], "tokens": [["\u2581Henry", "\u2581Cec", "il"]], "entropy": [[4.991742134094238, 0.781684935092926, 0.012811661697924137]], "avg_entropy": [1.9287462436283629], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Vincent O'Brien", "score_of_biggest_cluster": 0.5904728174209595, "score_of_first_answer": 0.5904728174209595, "max_score_of_first_answer": 0.9987465143203735, "min_score_of_first_answer": 0.22615830600261688, "avg_score": 0.5904728174209595, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34546837907168093, "std_of_first_answer": 0.34546837907168093, "avg_entropy_of_first_answer": 1.9287462436283629, "lex_sim": 1.0}
{"question": "In which 1955 film does Boccherini\u2019s String Quartet in E play a significant part?", "answer": ["Les Diaboliques"], "scores": [[0.042636141180992126, 0.6305183172225952, 0.9917207956314087, 0.9620848298072815]], "normalized_score": [0.40019309520721436], "tokens": [["\u2581Les", "\u2581Di", "abol", "iques"]], "entropy": [[4.195789337158203, 2.0836260318756104, 0.06828980892896652, 0.19430311024188995]], "avg_entropy": [1.6355020720511675], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Ladykillers", "score_of_biggest_cluster": 0.40019309520721436, "score_of_first_answer": 0.40019309520721436, "max_score_of_first_answer": 0.9917207956314087, "min_score_of_first_answer": 0.042636141180992126, "avg_score": 0.40019309520721436, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38185688569016346, "std_of_first_answer": 0.38185688569016346, "avg_entropy_of_first_answer": 1.6355020720511675, "lex_sim": 1.0}
{"question": "Which 137-mile canal joins London and Birmingham?", "answer": ["Grand Union Canal"], "scores": [[0.22343112528324127, 0.6071075797080994, 0.8105852603912354]], "normalized_score": [0.47907406091690063], "tokens": [["\u2581Grand", "\u2581Union", "\u2581Canal"]], "entropy": [[5.195311546325684, 2.1934351921081543, 0.9028233885765076]], "avg_entropy": [2.7638567090034485], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Warwick and Napton Canal", "score_of_biggest_cluster": 0.47907406091690063, "score_of_first_answer": 0.47907406091690063, "max_score_of_first_answer": 0.8105852603912354, "min_score_of_first_answer": 0.22343112528324127, "avg_score": 0.47907406091690063, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24343850845533757, "std_of_first_answer": 0.24343850845533757, "avg_entropy_of_first_answer": 2.7638567090034485, "lex_sim": 1.0}
{"question": "What is the distinctive colour of the soles of shoes designed by Christian Louboutin?", "answer": ["Red"], "scores": [[0.31732890009880066]], "normalized_score": [0.31732890009880066], "tokens": [["\u2581Red"]], "entropy": [[4.752706050872803]], "avg_entropy": [4.752706050872803], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Red (colour)", "score_of_biggest_cluster": 0.31732890009880066, "score_of_first_answer": 0.31732890009880066, "max_score_of_first_answer": 0.31732890009880066, "min_score_of_first_answer": 0.31732890009880066, "avg_score": 0.31732890009880066, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.752706050872803, "lex_sim": 1.0}
{"question": "In which town is the University of Surrey", "answer": ["Staggaforde"], "scores": [[0.044573262333869934, 0.5079084634780884, 0.05506599694490433, 0.0017889870796352625, 0.21488532423973083]], "normalized_score": [0.05446424335241318], "tokens": [["\u2581St", "ag", "ga", "for", "de"]], "entropy": [[4.915951728820801, 2.5731520652770996, 4.631508827209473, 2.775546073913574, 4.0884199142456055]], "avg_entropy": [3.7969157218933107], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dennisville", "score_of_biggest_cluster": 0.05446424335241318, "score_of_first_answer": 0.05446424335241318, "max_score_of_first_answer": 0.5079084634780884, "min_score_of_first_answer": 0.0017889870796352625, "avg_score": 0.05446424335241318, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18617226311366955, "std_of_first_answer": 0.18617226311366955, "avg_entropy_of_first_answer": 3.7969157218933107, "lex_sim": 1.0}
{"question": "In \u2018Dad\u2019s Army\u2019, what was the surname of the air-raid warden?", "answer": ["Captain Mainwaring"], "scores": [[0.015583300963044167, 0.5370900630950928, 0.9280396699905396, 0.9504078030586243]], "normalized_score": [0.29312020540237427], "tokens": [["\u2581Captain", "\u2581Main", "w", "aring"]], "entropy": [[5.782839298248291, 2.8016457557678223, 0.5687868595123291, 0.34494659304618835]], "avg_entropy": [2.3745546266436577], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hodges (disambiguation)", "score_of_biggest_cluster": 0.29312020540237427, "score_of_first_answer": 0.29312020540237427, "max_score_of_first_answer": 0.9504078030586243, "min_score_of_first_answer": 0.015583300963044167, "avg_score": 0.29312020540237427, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37935934981918534, "std_of_first_answer": 0.37935934981918534, "avg_entropy_of_first_answer": 2.3745546266436577, "lex_sim": 1.0}
{"question": "What was the name of the space shuttle, which disintegrated over Texas, on re-entry into the earth's atmosphere in February 2003, with the loss of all seven crew members?", "answer": ["Space Shuttle Columbia"], "scores": [[0.07401179522275925, 0.6824682950973511, 0.9746306538581848, 0.9924315214157104, 0.5326312184333801]], "normalized_score": [0.4820256531238556], "tokens": [["\u2581Space", "\u2581Sh", "utt", "le", "\u2581Columbia"]], "entropy": [[5.249207973480225, 1.963725209236145, 0.18554484844207764, 0.05605146661400795, 2.5178463459014893]], "avg_entropy": [1.9944751687347888], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Columbia", "score_of_biggest_cluster": 0.4820256531238556, "score_of_first_answer": 0.4820256531238556, "max_score_of_first_answer": 0.9924315214157104, "min_score_of_first_answer": 0.07401179522275925, "avg_score": 0.4820256531238556, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.33741037892096143, "std_of_first_answer": 0.33741037892096143, "avg_entropy_of_first_answer": 1.9944751687347888, "lex_sim": 1.0}
{"question": "The German film 'The Blue Angel' launched the career of which famous actress?", "answer": ["Marlene Dietrich?"], "scores": [[0.4286625385284424, 0.9312054514884949, 0.9693741202354431, 0.7493689060211182, 0.9837487936019897, 0.0007162717520259321]], "normalized_score": [0.24268989264965057], "tokens": [["\u2581Mar", "l", "ene", "\u2581Diet", "rich", "?"]], "entropy": [[4.24295711517334, 0.5340824723243713, 0.21690276265144348, 1.687859058380127, 0.154380202293396, 0.3995457887649536]], "avg_entropy": [1.2059545665979385], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marie M. Dietrich", "score_of_biggest_cluster": 0.24268989264965057, "score_of_first_answer": 0.24268989264965057, "max_score_of_first_answer": 0.9837487936019897, "min_score_of_first_answer": 0.0007162717520259321, "avg_score": 0.24268989264965057, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35787942605391104, "std_of_first_answer": 0.35787942605391104, "avg_entropy_of_first_answer": 1.2059545665979385, "lex_sim": 1.0}
{"question": "Beloved in 1999 was whose first movie since The Color Purple in 1985?", "answer": ["Oprah Winfrey"], "scores": [[0.4182812571525574, 0.9809888005256653, 0.7416958808898926, 0.9957494735717773, 0.9931580424308777]], "normalized_score": [0.786512017250061], "tokens": [["\u2581Op", "rah", "\u2581W", "inf", "rey"]], "entropy": [[4.214359283447266, 0.14497210085391998, 1.2115765810012817, 0.04586688429117203, 0.05575164407491684]], "avg_entropy": [1.1345052987337112], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Oprah", "score_of_biggest_cluster": 0.786512017250061, "score_of_first_answer": 0.786512017250061, "max_score_of_first_answer": 0.9957494735717773, "min_score_of_first_answer": 0.4182812571525574, "avg_score": 0.786512017250061, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2254417786395788, "std_of_first_answer": 0.2254417786395788, "avg_entropy_of_first_answer": 1.1345052987337112, "lex_sim": 1.0}
{"question": "Tina Turner Highway, Tennessee, links Brownsville and which town?", "answer": ["Memphis"], "scores": [[0.0218523982912302, 0.930565595626831, 0.9897778630256653]], "normalized_score": [0.27201610803604126], "tokens": [["\u2581M", "emph", "is"]], "entropy": [[4.527735710144043, 0.6230406165122986, 0.1045980155467987]], "avg_entropy": [1.7517914474010468], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nutbush", "score_of_biggest_cluster": 0.27201610803604126, "score_of_first_answer": 0.27201610803604126, "max_score_of_first_answer": 0.9897778630256653, "min_score_of_first_answer": 0.0218523982912302, "avg_score": 0.27201610803604126, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44298802001656556, "std_of_first_answer": 0.44298802001656556, "avg_entropy_of_first_answer": 1.7517914474010468, "lex_sim": 1.0}
{"question": "Also a plumber, what is the name of 'Super Mario's' older brother?", "answer": ["Herman"], "scores": [[0.002881742315366864, 0.032738931477069855]], "normalized_score": [0.009713142178952694], "tokens": [["\u2581H", "erman"]], "entropy": [[4.257737159729004, 5.520840644836426]], "avg_entropy": [4.889288902282715], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mr. L", "score_of_biggest_cluster": 0.009713142178952694, "score_of_first_answer": 0.009713142178952694, "max_score_of_first_answer": 0.032738931477069855, "min_score_of_first_answer": 0.002881742315366864, "avg_score": 0.009713142178952694, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.014928594580851495, "std_of_first_answer": 0.014928594580851495, "avg_entropy_of_first_answer": 4.889288902282715, "lex_sim": 1.0}
{"question": "Which group had UK top ten hits in 1987 with 'Sweet Little Mystery' and 'Angel Eyes'?", "answer": ["Wet Wet Wet"], "scores": [[0.6182542443275452, 0.9005688428878784, 0.8425515294075012, 0.9979506134986877, 0.954807698726654, 0.996051013469696]], "normalized_score": [0.873838484287262], "tokens": [["\u2581W", "et", "\u2581W", "et", "\u2581W", "et"]], "entropy": [[3.0453734397888184, 0.730988621711731, 0.9756396412849426, 0.023776154965162277, 0.3266875147819519, 0.03690999373793602]], "avg_entropy": [0.8565625610450903], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Wet wet wet", "score_of_biggest_cluster": 0.873838484287262, "score_of_first_answer": 0.873838484287262, "max_score_of_first_answer": 0.9979506134986877, "min_score_of_first_answer": 0.6182542443275452, "avg_score": 0.873838484287262, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.13112388387738338, "std_of_first_answer": 0.13112388387738338, "avg_entropy_of_first_answer": 0.8565625610450903, "lex_sim": 1.0}
{"question": "In Channel 4's 1987 adaptation of Tom Sharpe's novel 'Porterhouse Blue', who played the part of Skullion the Head Porter?", "answer": ["Geoffrey Palmer"], "scores": [[0.003235948970541358, 0.9688807129859924, 0.7576578855514526, 0.989051878452301]], "normalized_score": [0.2201613187789917], "tokens": [["\u2581Geoff", "rey", "\u2581Pal", "mer"]], "entropy": [[5.781384468078613, 0.2848394513130188, 1.6834203004837036, 0.1032198965549469]], "avg_entropy": [1.9632160291075706], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "David Jason filmography", "score_of_biggest_cluster": 0.2201613187789917, "score_of_first_answer": 0.2201613187789917, "max_score_of_first_answer": 0.989051878452301, "min_score_of_first_answer": 0.003235948970541358, "avg_score": 0.2201613187789917, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4009379981821724, "std_of_first_answer": 0.4009379981821724, "avg_entropy_of_first_answer": 1.9632160291075706, "lex_sim": 1.0}
{"question": "What commonly used English Latin expression means literally 'so' or 'thus' or 'in this manner', and generally refers to a quoted error?", "answer": ["Mistake (idiom)"], "scores": [[0.015964338555932045, 0.5203638076782227, 0.6763426065444946, 0.03313794732093811, 0.01232510432600975, 0.9594559669494629, 0.9497815370559692]], "normalized_score": [0.15439283847808838], "tokens": [["\u2581M", "ist", "ake", "\u2581(", "idi", "om", ")"]], "entropy": [[5.251717567443848, 3.369788646697998, 1.2327508926391602, 2.4896154403686523, 5.478558540344238, 0.23990607261657715, 0.329708993434906]], "avg_entropy": [2.627435164792197], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sic, recte", "score_of_biggest_cluster": 0.15439283847808838, "score_of_first_answer": 0.15439283847808838, "max_score_of_first_answer": 0.9594559669494629, "min_score_of_first_answer": 0.01232510432600975, "avg_score": 0.15439283847808838, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39985464140554794, "std_of_first_answer": 0.39985464140554794, "avg_entropy_of_first_answer": 2.627435164792197, "lex_sim": 1.0}
{"question": "In which race did Zola Budd and Mary Decker famously collide at the 1984 Olympics?", "answer": ["Tiny Tim"], "scores": [[0.002310860902070999, 0.022201135754585266, 0.05965719372034073]], "normalized_score": [0.014519023708999157], "tokens": [["\u2581T", "iny", "\u2581Tim"]], "entropy": [[3.402705192565918, 5.769670009613037, 6.458079814910889]], "avg_entropy": [5.210151672363281], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "3000 metres", "score_of_biggest_cluster": 0.014519023708999157, "score_of_first_answer": 0.014519023708999157, "max_score_of_first_answer": 0.05965719372034073, "min_score_of_first_answer": 0.002310860902070999, "avg_score": 0.014519023708999157, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.023774826088267663, "std_of_first_answer": 0.023774826088267663, "avg_entropy_of_first_answer": 5.210151672363281, "lex_sim": 1.0}
{"question": "In the NATO phonetic alphabet, which word represents the letter I?", "answer": ["India"], "scores": [[0.47120699286460876]], "normalized_score": [0.47120699286460876], "tokens": [["\u2581India"]], "entropy": [[3.1961519718170166]], "avg_entropy": [3.1961519718170166], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u0d2d\u0d3e\u0d30\u0d24 \u0d2e\u0d39\u0d3e\u0d30\u0d3e\u0d1c\u0d4d\u0d2f\u0d02", "score_of_biggest_cluster": 0.47120699286460876, "score_of_first_answer": 0.47120699286460876, "max_score_of_first_answer": 0.47120699286460876, "min_score_of_first_answer": 0.47120699286460876, "avg_score": 0.47120699286460876, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.1961519718170166, "lex_sim": 1.0}
{"question": "What is the capital and largest city of the Islamic Republic of Mauritania?", "answer": ["Nouakchott"], "scores": [[0.4393725097179413, 0.8677974939346313, 0.8642475605010986, 0.9839391708374023, 0.9796072244644165]], "normalized_score": [0.7950270771980286], "tokens": [["\u2581N", "ou", "ak", "ch", "ott"]], "entropy": [[4.083422660827637, 1.2084629535675049, 0.9199555516242981, 0.1640925407409668, 0.14927008748054504]], "avg_entropy": [1.3050407588481903], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Noukchott", "score_of_biggest_cluster": 0.7950270771980286, "score_of_first_answer": 0.7950270771980286, "max_score_of_first_answer": 0.9839391708374023, "min_score_of_first_answer": 0.4393725097179413, "avg_score": 0.7950270771980286, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20061194158826975, "std_of_first_answer": 0.20061194158826975, "avg_entropy_of_first_answer": 1.3050407588481903, "lex_sim": 1.0}
{"question": "What is the project/brand name for Google's computerized spectacles?", "answer": ["Google Glass"], "scores": [[0.2518436312675476, 0.6127250790596008, 0.9300374984741211]], "normalized_score": [0.5235590934753418], "tokens": [["\u2581Google", "\u2581G", "lass"]], "entropy": [[4.53394889831543, 2.481379985809326, 0.4033201038837433]], "avg_entropy": [2.472882996002833], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Optical glass", "score_of_biggest_cluster": 0.5235590934753418, "score_of_first_answer": 0.5235590934753418, "max_score_of_first_answer": 0.9300374984741211, "min_score_of_first_answer": 0.2518436312675476, "avg_score": 0.5235590934753418, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27706186882110573, "std_of_first_answer": 0.27706186882110573, "avg_entropy_of_first_answer": 2.472882996002833, "lex_sim": 1.0}
{"question": "National Flag Day is celebrated in Estonia during which month of the year?", "answer": ["February 24th"], "scores": [[0.2389652132987976, 0.08061759173870087, 0.8618965148925781, 0.9557560682296753, 0.2250080704689026]], "normalized_score": [0.32400593161582947], "tokens": [["\u2581February", "\u2581", "2", "4", "th"]], "entropy": [[4.498048305511475, 0.919975757598877, 0.7275916337966919, 0.2789563536643982, 1.2648612260818481]], "avg_entropy": [1.537886655330658], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u32c5", "score_of_biggest_cluster": 0.32400593161582947, "score_of_first_answer": 0.32400593161582947, "max_score_of_first_answer": 0.9557560682296753, "min_score_of_first_answer": 0.08061759173870087, "avg_score": 0.32400593161582947, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36180896445938776, "std_of_first_answer": 0.36180896445938776, "avg_entropy_of_first_answer": 1.537886655330658, "lex_sim": 1.0}
{"question": "Give a year in the reign of King Richard I of England (The Lionheart)?", "answer": ["1189-1199"], "scores": [[0.6628998517990112, 0.7853360176086426, 0.8458360433578491, 0.47403207421302795, 0.8776308298110962, 0.24179500341415405, 0.911781907081604, 0.9800962805747986, 0.9951313734054565, 0.9304731488227844]], "normalized_score": [0.7184723615646362], "tokens": [["\u2581", "1", "1", "8", "9", "-", "1", "1", "9", "9"]], "entropy": [[2.588179111480713, 1.1971991062164307, 0.8581705093383789, 1.1455326080322266, 0.6357851028442383, 2.2555317878723145, 0.42478927969932556, 0.13170458376407623, 0.03926770016551018, 0.3859255909919739]], "avg_entropy": [0.9662085380405188], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "1189-99", "score_of_biggest_cluster": 0.7184723615646362, "score_of_first_answer": 0.7184723615646362, "max_score_of_first_answer": 0.9951313734054565, "min_score_of_first_answer": 0.24179500341415405, "avg_score": 0.7184723615646362, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23158464504743406, "std_of_first_answer": 0.23158464504743406, "avg_entropy_of_first_answer": 0.9662085380405188, "lex_sim": 1.0}
{"question": "What is the name of the long running science program, produced by WGBH in Boston, on PBS, running from 1974 to the present?", "answer": ["H2O"], "scores": [[0.003092677565291524, 0.05403309687972069, 0.6521840691566467]], "normalized_score": [0.047766294330358505], "tokens": [["\u2581H", "2", "O"]], "entropy": [[5.374445915222168, 5.626209259033203, 1.9608204364776611]], "avg_entropy": [4.320491870244344], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Novas", "score_of_biggest_cluster": 0.047766294330358505, "score_of_first_answer": 0.047766294330358505, "max_score_of_first_answer": 0.6521840691566467, "min_score_of_first_answer": 0.003092677565291524, "avg_score": 0.047766294330358505, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29471250521930764, "std_of_first_answer": 0.29471250521930764, "avg_entropy_of_first_answer": 4.320491870244344, "lex_sim": 1.0}
{"question": "In what country is vodou religion commonly practiced?", "answer": ["Haiti"], "scores": [[0.3211323916912079, 0.9723765850067139, 0.925178050994873]], "normalized_score": [0.6610708236694336], "tokens": [["\u2581H", "ait", "i"]], "entropy": [[4.203353404998779, 0.26853373646736145, 0.3782718777656555]], "avg_entropy": [1.6167196730772655], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Haitian Ancestry", "score_of_biggest_cluster": 0.6610708236694336, "score_of_first_answer": 0.6610708236694336, "max_score_of_first_answer": 0.9723765850067139, "min_score_of_first_answer": 0.3211323916912079, "avg_score": 0.6610708236694336, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2965014257907691, "std_of_first_answer": 0.2965014257907691, "avg_entropy_of_first_answer": 1.6167196730772655, "lex_sim": 1.0}
{"question": "Lucrezia, Cesare, and Rodrigo were part of what papal family?", "answer": ["Borgia family"], "scores": [[0.5060960054397583, 0.7630376219749451, 0.15283584594726562]], "normalized_score": [0.38934507966041565], "tokens": [["\u2581Borg", "ia", "\u2581family"]], "entropy": [[3.1773319244384766, 0.6634238362312317, 1.5688707828521729]], "avg_entropy": [1.803208847840627], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "House of Borgia", "score_of_biggest_cluster": 0.38934507966041565, "score_of_first_answer": 0.38934507966041565, "max_score_of_first_answer": 0.7630376219749451, "min_score_of_first_answer": 0.15283584594726562, "avg_score": 0.38934507966041565, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.25014616661724237, "std_of_first_answer": 0.25014616661724237, "avg_entropy_of_first_answer": 1.803208847840627, "lex_sim": 1.0}
{"question": "Who hosted the 2006 Academy Awards ceremony?", "answer": ["OH BUGS BUNNEY"], "scores": [[0.011530738323926926, 0.0014144235756248236, 0.006896537728607655, 0.025306448340415955, 0.02484549768269062, 0.17445842921733856, 0.869339108467102, 0.0036142810713499784, 0.9348814487457275]], "normalized_score": [0.03210543468594551], "tokens": [["\u2581O", "H", "\u2581B", "U", "GS", "\u2581B", "UN", "NE", "Y"]], "entropy": [[6.111451148986816, 1.4737706184387207, 4.950941562652588, 4.224518775939941, 3.4246318340301514, 2.5131118297576904, 0.878473162651062, 0.15562278032302856, 0.4291468560695648]], "avg_entropy": [2.684629840983285], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stew beef", "score_of_biggest_cluster": 0.03210543468594551, "score_of_first_answer": 0.03210543468594551, "max_score_of_first_answer": 0.9348814487457275, "min_score_of_first_answer": 0.0014144235756248236, "avg_score": 0.03210543468594551, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3641818851989243, "std_of_first_answer": 0.3641818851989243, "avg_entropy_of_first_answer": 2.684629840983285, "lex_sim": 1.0}
{"question": "Which artist has a daughter and two sons with Jane Asher, whom he married in 1981?", "answer": ["RINGO STARR"], "scores": [[0.010169493965804577, 0.0035252543166279793, 0.6958476305007935, 0.37122419476509094, 0.968330442905426, 0.7208220958709717]], "normalized_score": [0.1364840865135193], "tokens": [["\u2581R", "ING", "O", "\u2581ST", "AR", "R"]], "entropy": [[5.416281700134277, 4.871626853942871, 1.9720457792282104, 2.4405884742736816, 0.274922251701355, 1.129795789718628]], "avg_entropy": [2.6842101414998374], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gerald Anthony Scarfe", "score_of_biggest_cluster": 0.1364840865135193, "score_of_first_answer": 0.1364840865135193, "max_score_of_first_answer": 0.968330442905426, "min_score_of_first_answer": 0.0035252543166279793, "avg_score": 0.1364840865135193, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3652929264670173, "std_of_first_answer": 0.3652929264670173, "avg_entropy_of_first_answer": 2.6842101414998374, "lex_sim": 1.0}
{"question": "What cocktail typically consists of 3 parts vodka, 1 part apple schnapps, and 1 part cointreau?", "answer": ["White Cosmopolitan"], "scores": [[0.06868250668048859, 0.006265612319111824, 0.7277687788009644, 0.9945656061172485, 0.9901680946350098]], "normalized_score": [0.1985313445329666], "tokens": [["\u2581White", "\u2581Cos", "m", "opol", "itan"]], "entropy": [[5.785597801208496, 2.358088970184326, 0.801133394241333, 0.04738829284906387, 0.07926039397716522]], "avg_entropy": [1.814293770492077], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Appletini", "score_of_biggest_cluster": 0.1985313445329666, "score_of_first_answer": 0.1985313445329666, "max_score_of_first_answer": 0.9945656061172485, "min_score_of_first_answer": 0.006265612319111824, "avg_score": 0.1985313445329666, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43589479754846655, "std_of_first_answer": 0.43589479754846655, "avg_entropy_of_first_answer": 1.814293770492077, "lex_sim": 1.0}
{"question": "What part of the human body is affected by 'iritis'?", "answer": ["\uffe1776"], "scores": [[0.051873449236154556, 0.016641559079289436, 0.07113789021968842, 0.09995708614587784, 0.011704502627253532, 0.04328974336385727, 0.052093859761953354]], "normalized_score": [0.03993635252118111], "tokens": [["\u2581", "<0xEF>", "<0xBF>", "<0xA1>", "7", "7", "6"]], "entropy": [[4.343226909637451, 5.033698081970215, 3.0414938926696777, 3.33591365814209, 3.0642075538635254, 2.8445050716400146, 2.9090497493743896]], "avg_entropy": [3.5102992738996233], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eye (anatomy)", "score_of_biggest_cluster": 0.03993635252118111, "score_of_first_answer": 0.03993635252118111, "max_score_of_first_answer": 0.09995708614587784, "min_score_of_first_answer": 0.011704502627253532, "avg_score": 0.03993635252118111, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.028216549471199122, "std_of_first_answer": 0.028216549471199122, "avg_entropy_of_first_answer": 3.5102992738996233, "lex_sim": 1.0}
{"question": "The UK television series \u2018Blackadder Goes Forth\u2019 is set during which war?", "answer": ["WWF"], "scores": [[0.08041450381278992, 0.002027479698881507]], "normalized_score": [0.012768665328621864], "tokens": [["\u2581W", "WF"]], "entropy": [[4.432026386260986, 1.2782433032989502]], "avg_entropy": [2.8551348447799683], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "World war i", "score_of_biggest_cluster": 0.012768665328621864, "score_of_first_answer": 0.012768665328621864, "max_score_of_first_answer": 0.08041450381278992, "min_score_of_first_answer": 0.002027479698881507, "avg_score": 0.012768665328621864, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.039193512056954205, "std_of_first_answer": 0.039193512056954205, "avg_entropy_of_first_answer": 2.8551348447799683, "lex_sim": 1.0}
{"question": "To what note is the top string on a six-string guitar normally tuned?", "answer": ["Guitar, top, E"], "scores": [[0.00390481180511415, 0.968798816204071, 0.004565558396279812, 0.03644264489412308, 0.03186476603150368, 0.13344880938529968]], "normalized_score": [0.037261515855789185], "tokens": [["\u2581Gu", "itar", ",", "\u2581top", ",", "\u2581E"]], "entropy": [[4.303796768188477, 0.2454816699028015, 3.378411054611206, 5.714754104614258, 1.8095241785049438, 3.9631552696228027]], "avg_entropy": [3.235853840907415], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "\u24a0", "score_of_biggest_cluster": 0.037261515855789185, "score_of_first_answer": 0.037261515855789185, "max_score_of_first_answer": 0.968798816204071, "min_score_of_first_answer": 0.00390481180511415, "avg_score": 0.037261515855789185, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.34810816509083853, "std_of_first_answer": 0.34810816509083853, "avg_entropy_of_first_answer": 3.235853840907415, "lex_sim": 1.0}
{"question": "Who was Poet Laureate from 1850 to 1892?", "answer": ["Alfred Tinkly Touched"], "scores": [[0.401009738445282, 0.31108251214027405, 0.00017867230053525418, 0.007253548130393028, 0.0009652086300775409, 0.10120239853858948]], "normalized_score": [0.015839507803320885], "tokens": [["\u2581Alfred", "\u2581T", "ink", "ly", "\u2581Touch", "ed"]], "entropy": [[4.088474750518799, 2.784743309020996, 0.5773600339889526, 2.5395188331604004, 3.0967259407043457, 3.3756518363952637]], "avg_entropy": [2.7437457839647927], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A. tennyson", "score_of_biggest_cluster": 0.015839507803320885, "score_of_first_answer": 0.015839507803320885, "max_score_of_first_answer": 0.401009738445282, "min_score_of_first_answer": 0.00017867230053525418, "avg_score": 0.015839507803320885, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.16090732189888016, "std_of_first_answer": 0.16090732189888016, "avg_entropy_of_first_answer": 2.7437457839647927, "lex_sim": 1.0}
{"question": "Who wrote the Buddy Holly hit 'It Doesn't Matter Anymore'?", "answer": ["Paul Anka"], "scores": [[0.38996538519859314, 0.8761479258537292, 0.99722820520401]], "normalized_score": [0.698445737361908], "tokens": [["\u2581Paul", "\u2581An", "ka"]], "entropy": [[4.269898891448975, 1.1112154722213745, 0.033056750893592834]], "avg_entropy": [1.8047237048546474], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "\u0628\u0648\u0644 \u0645\u0635\u0637\u0641\u0649 \u0639\u0628\u062f\u064a \u0623\u0646\u0643\u0627", "score_of_biggest_cluster": 0.698445737361908, "score_of_first_answer": 0.698445737361908, "max_score_of_first_answer": 0.99722820520401, "min_score_of_first_answer": 0.38996538519859314, "avg_score": 0.698445737361908, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.26242502197905265, "std_of_first_answer": 0.26242502197905265, "avg_entropy_of_first_answer": 1.8047237048546474, "lex_sim": 1.0}
{"question": "What fairy tale features a wolf who tries to \u201chuff and puff and blow your house in\u201d?", "answer": ["The Three Little Pigs"], "scores": [[0.2641913592815399, 0.6936299204826355, 0.9029566645622253, 0.9570719003677368, 0.9714915752410889]], "normalized_score": [0.6877322196960449], "tokens": [["\u2581The", "\u2581Three", "\u2581Little", "\u2581P", "igs"]], "entropy": [[3.678520679473877, 1.86216402053833, 0.5931243300437927, 0.3856753706932068, 0.16492211818695068]], "avg_entropy": [1.3368813037872314], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "The 3 Little Pigs", "score_of_biggest_cluster": 0.6877322196960449, "score_of_first_answer": 0.6877322196960449, "max_score_of_first_answer": 0.9714915752410889, "min_score_of_first_answer": 0.2641913592815399, "avg_score": 0.6877322196960449, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2661620194000992, "std_of_first_answer": 0.2661620194000992, "avg_entropy_of_first_answer": 1.3368813037872314, "lex_sim": 1.0}
{"question": "\"The \"\"Hall of Mirrors\"\" is in which building?\"", "answer": ["Versailles Palace"], "scores": [[0.15028373897075653, 0.9424715042114258, 0.9964273571968079, 0.3362663686275482]], "normalized_score": [0.46674248576164246], "tokens": [["\u2581Vers", "a", "illes", "\u2581Palace"]], "entropy": [[4.187504291534424, 0.37775975465774536, 0.03197305276989937, 1.63331937789917]], "avg_entropy": [1.5576391192153096], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Palais de Versailles", "score_of_biggest_cluster": 0.46674248576164246, "score_of_first_answer": 0.46674248576164246, "max_score_of_first_answer": 0.9964273571968079, "min_score_of_first_answer": 0.15028373897075653, "avg_score": 0.46674248576164246, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.36948599837342655, "std_of_first_answer": 0.36948599837342655, "avg_entropy_of_first_answer": 1.5576391192153096, "lex_sim": 1.0}
{"question": "Next Sunday, Sept 19, is International what day?", "answer": ["Talk like a Pirate Day"], "scores": [[0.3240300416946411, 0.9427908658981323, 0.5757112503051758, 0.9049056172370911, 0.16287612915039062, 0.9987046718597412, 0.6294849514961243]], "normalized_score": [0.5553721189498901], "tokens": [["\u2581T", "alk", "\u2581like", "\u2581a", "\u2581Pir", "ate", "\u2581Day"]], "entropy": [[4.819311141967773, 0.4952923059463501, 1.0673848390579224, 0.7022525072097778, 1.1789271831512451, 0.013479463756084442, 1.2032287120819092]], "avg_entropy": [1.3542680218815804], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Talk like a pirate day", "score_of_biggest_cluster": 0.5553721189498901, "score_of_first_answer": 0.5553721189498901, "max_score_of_first_answer": 0.9987046718597412, "min_score_of_first_answer": 0.16287612915039062, "avg_score": 0.5553721189498901, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2980328832711854, "std_of_first_answer": 0.2980328832711854, "avg_entropy_of_first_answer": 1.3542680218815804, "lex_sim": 1.0}
{"question": "In 1973, which Chicago building became the tallest in the world?", "answer": ["Willis Building (The Sears Tower)"], "scores": [[0.12299146503210068, 0.9618497490882874, 0.02447604201734066, 0.0619455985724926, 0.009014250710606575, 0.6944177150726318, 0.9784859418869019, 0.944570004940033, 0.8874979019165039]], "normalized_score": [0.21348141133785248], "tokens": [["\u2581Will", "is", "\u2581Building", "\u2581(", "The", "\u2581S", "ears", "\u2581Tower", ")"]], "entropy": [[4.287933349609375, 0.3404746651649475, 1.1538273096084595, 0.8111413717269897, 3.958860397338867, 1.9039000272750854, 0.12252671271562576, 0.3913114070892334, 0.7615909576416016]], "avg_entropy": [1.525729577574465], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Sears Tower (Willis Tower)", "score_of_biggest_cluster": 0.21348141133785248, "score_of_first_answer": 0.21348141133785248, "max_score_of_first_answer": 0.9784859418869019, "min_score_of_first_answer": 0.009014250710606575, "avg_score": 0.21348141133785248, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.42495143192876167, "std_of_first_answer": 0.42495143192876167, "avg_entropy_of_first_answer": 1.525729577574465, "lex_sim": 1.0}
{"question": "What artist designed the logo for Chupa Chups lollipops?", "answer": ["Salvador Dali"], "scores": [[0.3667966425418854, 0.6206480860710144, 0.9313243627548218]], "normalized_score": [0.5962896347045898], "tokens": [["\u2581Salvador", "\u2581D", "ali"]], "entropy": [[4.384088516235352, 1.5197688341140747, 0.5942016243934631]], "avg_entropy": [2.1660196582476297], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Salvador Felipe Jacinto DalA", "score_of_biggest_cluster": 0.5962896347045898, "score_of_first_answer": 0.5962896347045898, "max_score_of_first_answer": 0.9313243627548218, "min_score_of_first_answer": 0.3667966425418854, "avg_score": 0.5962896347045898, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23085634089064316, "std_of_first_answer": 0.23085634089064316, "avg_entropy_of_first_answer": 2.1660196582476297, "lex_sim": 1.0}
{"question": "\"Who was the female member of the 1980's pop music duo, \"\"Eurythmics\"\"?\"", "answer": ["Marvin Humes"], "scores": [[0.007990186102688313, 0.026769671589136124, 0.16903136670589447, 0.5790004134178162]], "normalized_score": [0.06764122098684311], "tokens": [["\u2581Mar", "vin", "\u2581H", "umes"]], "entropy": [[4.631579875946045, 3.5692906379699707, 2.478804588317871, 1.77414870262146]], "avg_entropy": [3.1134559512138367], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Annie lennox", "score_of_biggest_cluster": 0.06764122098684311, "score_of_first_answer": 0.06764122098684311, "max_score_of_first_answer": 0.5790004134178162, "min_score_of_first_answer": 0.007990186102688313, "avg_score": 0.06764122098684311, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22989282992349305, "std_of_first_answer": 0.22989282992349305, "avg_entropy_of_first_answer": 3.1134559512138367, "lex_sim": 1.0}
{"question": "\"Who is the principal actor who plays a character named Michael Blomqvist in the 2011 film \"\"The Girl with the Dragon Tattoo\"\"?\"", "answer": ["Stellan Skarsgard"], "scores": [[0.010032177902758121, 0.4855285584926605, 0.6116393208503723, 0.9521620869636536, 0.9441209435462952, 0.395374059677124]], "normalized_score": [0.3192582130432129], "tokens": [["\u2581St", "ellan", "\u2581Sk", "ars", "g", "ard"]], "entropy": [[5.103975296020508, 2.4433350563049316, 2.433434247970581, 0.3794437646865845, 0.3590118885040283, 1.3250586986541748]], "avg_entropy": [2.007376492023468], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dan Craig", "score_of_biggest_cluster": 0.3192582130432129, "score_of_first_answer": 0.3192582130432129, "max_score_of_first_answer": 0.9521620869636536, "min_score_of_first_answer": 0.010032177902758121, "avg_score": 0.3192582130432129, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32630948398528503, "std_of_first_answer": 0.32630948398528503, "avg_entropy_of_first_answer": 2.007376492023468, "lex_sim": 1.0}
{"question": "In which disaster film do the heroes take off to the song Leaving on a Jet Plane?", "answer": ["Airport '75"], "scores": [[0.06776294857263565, 0.12407202273607254, 0.9853693842887878, 0.40144839882850647]], "normalized_score": [0.24014508724212646], "tokens": [["\u2581Airport", "\u2581'", "7", "5"]], "entropy": [[4.558414936065674, 2.107558488845825, 0.11821942776441574, 1.0879507064819336]], "avg_entropy": [1.968035889789462], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Amargeddon", "score_of_biggest_cluster": 0.24014508724212646, "score_of_first_answer": 0.24014508724212646, "max_score_of_first_answer": 0.9853693842887878, "min_score_of_first_answer": 0.06776294857263565, "avg_score": 0.24014508724212646, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36368365191348423, "std_of_first_answer": 0.36368365191348423, "avg_entropy_of_first_answer": 1.968035889789462, "lex_sim": 1.0}
{"question": "The Red Setter is more properly known by which name?", "answer": ["Lurcher (the dog's name is Red Setter)"], "scores": [[0.0026424454990774393, 0.04959239065647125, 0.9034020900726318, 0.045813143253326416, 0.010439346544444561, 0.11166036128997803, 0.01043230015784502, 0.9968103766441345, 0.3645870089530945, 0.3514632284641266, 0.010471345856785774, 0.4633454382419586, 0.9915995597839355, 0.7184208631515503]], "normalized_score": [0.10793301463127136], "tokens": [["\u2581L", "urch", "er", "\u2581(", "the", "\u2581dog", "'", "s", "\u2581name", "\u2581is", "\u2581Red", "\u2581Set", "ter", ")"]], "entropy": [[3.216585159301758, 4.9404778480529785, 0.5519213080406189, 0.9052457809448242, 3.6256983280181885, 4.976048946380615, 1.6619194746017456, 0.0333639420568943, 3.9516818523406982, 1.8834140300750732, 4.381295204162598, 2.2572498321533203, 0.06123112514615059, 1.7064284086227417]], "avg_entropy": [2.439468659992729], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Irish Setter", "score_of_biggest_cluster": 0.10793301463127136, "score_of_first_answer": 0.10793301463127136, "max_score_of_first_answer": 0.9968103766441345, "min_score_of_first_answer": 0.0026424454990774393, "avg_score": 0.10793301463127136, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37701540027087704, "std_of_first_answer": 0.37701540027087704, "avg_entropy_of_first_answer": 2.439468659992729, "lex_sim": 1.0}
{"question": "Who became the second Astronomer Royal upon the death of John Flamstead in 1619?", "answer": ["John Wallis"], "scores": [[0.049063801765441895, 0.03810262680053711, 0.8317881226539612]], "normalized_score": [0.11585363000631332], "tokens": [["\u2581John", "\u2581Wall", "is"]], "entropy": [[5.820832252502441, 5.230588436126709, 0.9167224168777466]], "avg_entropy": [3.989381035168966], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sir Edmond Halley", "score_of_biggest_cluster": 0.11585363000631332, "score_of_first_answer": 0.11585363000631332, "max_score_of_first_answer": 0.8317881226539612, "min_score_of_first_answer": 0.03810262680053711, "avg_score": 0.11585363000631332, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3715903024980156, "std_of_first_answer": 0.3715903024980156, "avg_entropy_of_first_answer": 3.989381035168966, "lex_sim": 1.0}
{"question": "In 2001 which movie was the first winner of the Best Animated Movie Oscar?", "answer": ["Shrek"], "scores": [[0.4231625199317932, 0.9607862830162048]], "normalized_score": [0.6376274228096008], "tokens": [["\u2581Sh", "rek"]], "entropy": [[3.9539623260498047, 0.3472170829772949]], "avg_entropy": [2.15058970451355], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Shrek One", "score_of_biggest_cluster": 0.6376274228096008, "score_of_first_answer": 0.6376274228096008, "max_score_of_first_answer": 0.9607862830162048, "min_score_of_first_answer": 0.4231625199317932, "avg_score": 0.6376274228096008, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2688118815422058, "std_of_first_answer": 0.2688118815422058, "avg_entropy_of_first_answer": 2.15058970451355, "lex_sim": 1.0}
{"question": "In which country can you find Westmorland in Cornwall and Manchester in Middlesex?", "answer": ["US"], "scores": [[0.014409953728318214]], "normalized_score": [0.014409951865673065], "tokens": [["\u2581US"]], "entropy": [[5.462152481079102]], "avg_entropy": [5.462152481079102], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jamica", "score_of_biggest_cluster": 0.014409951865673065, "score_of_first_answer": 0.014409951865673065, "max_score_of_first_answer": 0.014409953728318214, "min_score_of_first_answer": 0.014409953728318214, "avg_score": 0.014409951865673065, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.462152481079102, "lex_sim": 1.0}
{"question": "By what name is James Osterberg popularly known?", "answer": ["Iggy Pop"], "scores": [[0.4228486716747284, 0.951440155506134, 0.9527636170387268, 0.7125288248062134]], "normalized_score": [0.7229171395301819], "tokens": [["\u2581I", "gg", "y", "\u2581Pop"]], "entropy": [[3.9088714122772217, 0.4767063856124878, 0.43496546149253845, 2.0189666748046875]], "avg_entropy": [1.7098774835467339], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "James Osterberg", "score_of_biggest_cluster": 0.7229171395301819, "score_of_first_answer": 0.7229171395301819, "max_score_of_first_answer": 0.9527636170387268, "min_score_of_first_answer": 0.4228486716747284, "avg_score": 0.7229171395301819, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21779097319788807, "std_of_first_answer": 0.21779097319788807, "avg_entropy_of_first_answer": 1.7098774835467339, "lex_sim": 1.0}
{"question": "Who wrote the 1930\u2019s novel \u2018Tender is the Night\u2019?", "answer": ["F. Scott Fitzgerald"], "scores": [[0.2248672991991043, 0.811308741569519, 0.9025821089744568, 0.7987133264541626, 0.98859041929245, 0.9958840012550354]], "normalized_score": [0.7112722992897034], "tokens": [["\u2581F", ".", "\u2581Scott", "\u2581Fitz", "ger", "ald"]], "entropy": [[5.0308380126953125, 0.9987040162086487, 0.5652488470077515, 1.248716115951538, 0.10228873789310455, 0.038652800023555756]], "avg_entropy": [1.330741421629985], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Francis Scott Key Fitzgerald", "score_of_biggest_cluster": 0.7112722992897034, "score_of_first_answer": 0.7112722992897034, "max_score_of_first_answer": 0.9958840012550354, "min_score_of_first_answer": 0.2248672991991043, "avg_score": 0.7112722992897034, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2627900736332352, "std_of_first_answer": 0.2627900736332352, "avg_entropy_of_first_answer": 1.330741421629985, "lex_sim": 1.0}
{"question": "What is the only country in which the Komodo Dragon is found in the wild?", "answer": ["Teggana"], "scores": [[0.0033771453890949488, 0.00831438135355711, 0.08073630928993225, 0.00928315706551075]], "normalized_score": [0.012044421397149563], "tokens": [["\u2581T", "eg", "g", "ana"]], "entropy": [[4.734606742858887, 5.973593711853027, 5.008709907531738, 4.745615005493164]], "avg_entropy": [5.115631341934204], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Unitary state of republic of indonesia", "score_of_biggest_cluster": 0.012044421397149563, "score_of_first_answer": 0.012044421397149563, "max_score_of_first_answer": 0.08073630928993225, "min_score_of_first_answer": 0.0033771453890949488, "avg_score": 0.012044421397149563, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.032010862070099816, "std_of_first_answer": 0.032010862070099816, "avg_entropy_of_first_answer": 5.115631341934204, "lex_sim": 1.0}
{"question": "The Fan, Westhead, Becher's Brook, Foinavon's, Canal Turn, Valentine's Brook, the Booth, the Anchor Bridge and the Chair are features of a venue for what sport?", "answer": ["Aintree Racecourse"], "scores": [[0.1260623186826706, 0.847058892250061, 0.9993160963058472, 0.34987229108810425, 0.978756844997406]], "normalized_score": [0.5158901810646057], "tokens": [["\u2581A", "int", "ree", "\u2581Race", "course"]], "entropy": [[4.154356956481934, 1.4156843423843384, 0.008825762197375298, 2.371445655822754, 0.1542951464653015]], "avg_entropy": [1.6209215726703405], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Scratch sheet", "score_of_biggest_cluster": 0.5158901810646057, "score_of_first_answer": 0.5158901810646057, "max_score_of_first_answer": 0.9993160963058472, "min_score_of_first_answer": 0.1260623186826706, "avg_score": 0.5158901810646057, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35580915798218404, "std_of_first_answer": 0.35580915798218404, "avg_entropy_of_first_answer": 1.6209215726703405, "lex_sim": 1.0}
{"question": "Luzon is an island of which country?", "answer": ["PHILIPPINES"], "scores": [[0.014539790339767933, 0.23201481997966766, 0.6968249678611755, 0.9127309322357178, 0.7665557861328125, 0.9800125956535339]], "normalized_score": [0.34241488575935364], "tokens": [["\u2581P", "HI", "LI", "PP", "IN", "ES"]], "entropy": [[4.6408371925354, 3.855893135070801, 1.2159274816513062, 0.4337327480316162, 0.6766697764396667, 0.1760103702545166]], "avg_entropy": [1.8331784506638844], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Islas Filipinas", "score_of_biggest_cluster": 0.34241488575935364, "score_of_first_answer": 0.34241488575935364, "max_score_of_first_answer": 0.9800125956535339, "min_score_of_first_answer": 0.014539790339767933, "avg_score": 0.34241488575935364, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3553171510216271, "std_of_first_answer": 0.3553171510216271, "avg_entropy_of_first_answer": 1.8331784506638844, "lex_sim": 1.0}
{"question": "Who was the defending champion when Martina Navratilova first won Wimbledon singles?", "answer": ["C"], "scores": [[0.0031583458185195923]], "normalized_score": [0.0031583465170115232], "tokens": [["\u2581C"]], "entropy": [[5.095895290374756]], "avg_entropy": [5.095895290374756], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Virginia Wade", "score_of_biggest_cluster": 0.0031583465170115232, "score_of_first_answer": 0.0031583465170115232, "max_score_of_first_answer": 0.0031583458185195923, "min_score_of_first_answer": 0.0031583458185195923, "avg_score": 0.0031583465170115232, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.095895290374756, "lex_sim": 1.0}
{"question": "Which island group in the South Atlantic was once known as the Sebald Islands?", "answer": ["Falkland Islands"], "scores": [[0.3645959496498108, 0.975276529788971, 0.7497219443321228, 0.7045940160751343]], "normalized_score": [0.658331573009491], "tokens": [["\u2581F", "alk", "land", "\u2581Islands"]], "entropy": [[3.617225408554077, 0.24292942881584167, 0.6699811816215515, 1.2456774711608887]], "avg_entropy": [1.4439533725380898], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Malvinas/Falklands", "score_of_biggest_cluster": 0.658331573009491, "score_of_first_answer": 0.658331573009491, "max_score_of_first_answer": 0.975276529788971, "min_score_of_first_answer": 0.3645959496498108, "avg_score": 0.658331573009491, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.21837924467424782, "std_of_first_answer": 0.21837924467424782, "avg_entropy_of_first_answer": 1.4439533725380898, "lex_sim": 1.0}
{"question": "Which Frenchman, who never learned to speak Swedish, became King Charles XIV John of Sweden and Norway?", "answer": ["Jean-Baptiste Bernadotte"], "scores": [[0.384583443403244, 0.7617616057395935, 0.9918076395988464, 0.9896983504295349, 0.9829923510551453, 0.44022873044013977, 0.9990712404251099, 0.9959035515785217]], "normalized_score": [0.7701898217201233], "tokens": [["\u2581Jean", "-", "B", "apt", "iste", "\u2581Bern", "ad", "otte"]], "entropy": [[3.6678452491760254, 1.0177087783813477, 0.08007480204105377, 0.08689209818840027, 0.11395031213760376, 1.237152338027954, 0.010461458936333656, 0.03651399910449982]], "avg_entropy": [0.7813248794991523], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "King Charles XIV", "score_of_biggest_cluster": 0.7701898217201233, "score_of_first_answer": 0.7701898217201233, "max_score_of_first_answer": 0.9990712404251099, "min_score_of_first_answer": 0.384583443403244, "avg_score": 0.7701898217201233, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24623962914653924, "std_of_first_answer": 0.24623962914653924, "avg_entropy_of_first_answer": 0.7813248794991523, "lex_sim": 1.0}
{"question": "Peter Schmeiche\u2019s son is currently the regular goalkeeper at which English football league team?", "answer": ["Leicester City"], "scores": [[0.0643736869096756, 0.700873076915741, 0.9560410380363464, 0.734702467918396]], "normalized_score": [0.4219236969947815], "tokens": [["\u2581Le", "ic", "ester", "\u2581City"]], "entropy": [[5.245519638061523, 1.225385308265686, 0.29079627990722656, 1.2102887630462646]], "avg_entropy": [1.9929974973201752], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Leicester Fosse FC", "score_of_biggest_cluster": 0.4219236969947815, "score_of_first_answer": 0.4219236969947815, "max_score_of_first_answer": 0.9560410380363464, "min_score_of_first_answer": 0.0643736869096756, "avg_score": 0.4219236969947815, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33211337408066505, "std_of_first_answer": 0.33211337408066505, "avg_entropy_of_first_answer": 1.9929974973201752, "lex_sim": 1.0}
{"question": "Mordant is the general term for a chemical which allows what to work properly?", "answer": ["Dyes"], "scores": [[0.20712436735630035, 0.3421141803264618]], "normalized_score": [0.26619577407836914], "tokens": [["\u2581D", "yes"]], "entropy": [[4.948831558227539, 0.9589138627052307]], "avg_entropy": [2.953872710466385], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "DYE", "score_of_biggest_cluster": 0.26619577407836914, "score_of_first_answer": 0.26619577407836914, "max_score_of_first_answer": 0.3421141803264618, "min_score_of_first_answer": 0.20712436735630035, "avg_score": 0.26619577407836914, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06749490648508072, "std_of_first_answer": 0.06749490648508072, "avg_entropy_of_first_answer": 2.953872710466385, "lex_sim": 1.0}
{"question": "Which country will host the 2014 FIFA World Cup?", "answer": ["Brazil"], "scores": [[0.3024951219558716]], "normalized_score": [0.3024951219558716], "tokens": [["\u2581Brazil"]], "entropy": [[4.408601760864258]], "avg_entropy": [4.408601760864258], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Brazil", "score_of_biggest_cluster": 0.3024951219558716, "score_of_first_answer": 0.3024951219558716, "max_score_of_first_answer": 0.3024951219558716, "min_score_of_first_answer": 0.3024951219558716, "avg_score": 0.3024951219558716, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.408601760864258, "lex_sim": 1.0}
{"question": "If you were kicked on the tarsus where would it hurt", "answer": ["Tarsus"], "scores": [[0.09341258555650711, 0.9056748747825623, 0.6839373111724854]], "normalized_score": [0.38678058981895447], "tokens": [["\u2581T", "ars", "us"]], "entropy": [[4.998728275299072, 0.8232945799827576, 1.1436851024627686]], "avg_entropy": [2.321902652581533], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ancle", "score_of_biggest_cluster": 0.38678058981895447, "score_of_first_answer": 0.38678058981895447, "max_score_of_first_answer": 0.9056748747825623, "min_score_of_first_answer": 0.09341258555650711, "avg_score": 0.38678058981895447, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34280817849350464, "std_of_first_answer": 0.34280817849350464, "avg_entropy_of_first_answer": 2.321902652581533, "lex_sim": 1.0}
{"question": "Which European capital city is located 40 miles east of Vienna?", "answer": ["Czeck capital in Prague"], "scores": [[0.004796378780156374, 0.010089515708386898, 0.16411983966827393, 0.12396913766860962, 0.0022070836275815964, 0.23673753440380096, 0.9725012183189392]], "normalized_score": [0.04691581055521965], "tokens": [["\u2581C", "ze", "ck", "\u2581capital", "\u2581in", "\u2581Prag", "ue"]], "entropy": [[4.196875095367432, 5.503353595733643, 2.791233539581299, 3.7458577156066895, 1.8910834789276123, 3.7315149307250977, 0.19877701997756958]], "avg_entropy": [3.151242196559906], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Istropolis", "score_of_biggest_cluster": 0.04691581055521965, "score_of_first_answer": 0.04691581055521965, "max_score_of_first_answer": 0.9725012183189392, "min_score_of_first_answer": 0.0022070836275815964, "avg_score": 0.04691581055521965, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31995491656504244, "std_of_first_answer": 0.31995491656504244, "avg_entropy_of_first_answer": 3.151242196559906, "lex_sim": 1.0}
{"question": "Which King of England had two wives, both named Isabella?", "answer": ["Edward II"], "scores": [[0.4520847499370575, 0.8518282771110535]], "normalized_score": [0.6205630898475647], "tokens": [["\u2581Edward", "\u2581II"]], "entropy": [[2.889469623565674, 0.7661911845207214]], "avg_entropy": [1.8278304040431976], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John (disambiguation)", "score_of_biggest_cluster": 0.6205630898475647, "score_of_first_answer": 0.6205630898475647, "max_score_of_first_answer": 0.8518282771110535, "min_score_of_first_answer": 0.4520847499370575, "avg_score": 0.6205630898475647, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19987176358699799, "std_of_first_answer": 0.19987176358699799, "avg_entropy_of_first_answer": 1.8278304040431976, "lex_sim": 1.0}
{"question": "What name is given to a positive whole number that can only be divided by 1 or itself?", "answer": ["Prime number"], "scores": [[0.39453989267349243, 0.4505617916584015]], "normalized_score": [0.4216214120388031], "tokens": [["\u2581Prime", "\u2581number"]], "entropy": [[3.9311513900756836, 1.8999967575073242]], "avg_entropy": [2.915574073791504], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "PRIME", "score_of_biggest_cluster": 0.4216214120388031, "score_of_first_answer": 0.4216214120388031, "max_score_of_first_answer": 0.4505617916584015, "min_score_of_first_answer": 0.39453989267349243, "avg_score": 0.4216214120388031, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.02801094949245453, "std_of_first_answer": 0.02801094949245453, "avg_entropy_of_first_answer": 2.915574073791504, "lex_sim": 1.0}
{"question": "A member of which profession would reside in a manse", "answer": ["Manse"], "scores": [[0.06381350010633469, 0.8650293350219727]], "normalized_score": [0.23494796454906464], "tokens": [["\u2581Man", "se"]], "entropy": [[4.615987777709961, 0.7928298115730286]], "avg_entropy": [2.7044087946414948], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Clergyman", "score_of_biggest_cluster": 0.23494796454906464, "score_of_first_answer": 0.23494796454906464, "max_score_of_first_answer": 0.8650293350219727, "min_score_of_first_answer": 0.06381350010633469, "avg_score": 0.23494796454906464, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.400607917457819, "std_of_first_answer": 0.400607917457819, "avg_entropy_of_first_answer": 2.7044087946414948, "lex_sim": 1.0}
{"question": "\"What type of clothing is a \"\"sou-wester\"\"?\"", "answer": ["Hoods - coats - and a belt with a buckle"], "scores": [[0.00476969825103879, 0.0870455801486969, 0.033598098903894424, 0.004281926900148392, 0.005840246099978685, 0.9411006569862366, 0.2323496788740158, 0.009727190248668194, 0.0035431813448667526, 0.004446284379810095, 0.907617449760437, 0.059214185923337936, 0.5507550239562988, 0.2207118570804596, 0.9982319474220276, 0.993559718132019]], "normalized_score": [0.06382005661725998], "tokens": [["\u2581H", "ood", "s", "\u2581-", "\u2581co", "ats", "\u2581-", "\u2581and", "\u2581a", "\u2581bel", "t", "\u2581with", "\u2581a", "\u2581bu", "ck", "le"]], "entropy": [[4.823616027832031, 3.881890296936035, 1.3509509563446045, 2.562974691390991, 5.759936332702637, 0.4442600607872009, 2.2522144317626953, 5.578573703765869, 4.130608558654785, 5.88881254196167, 0.49361780285835266, 3.3108816146850586, 3.156585693359375, 4.982185363769531, 0.018968529999256134, 0.05314219743013382]], "avg_entropy": [3.043076175265014], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Titfer", "score_of_biggest_cluster": 0.06382005661725998, "score_of_first_answer": 0.06382005661725998, "max_score_of_first_answer": 0.9982319474220276, "min_score_of_first_answer": 0.0035431813448667526, "avg_score": 0.06382005661725998, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3963360465538279, "std_of_first_answer": 0.3963360465538279, "avg_entropy_of_first_answer": 3.043076175265014, "lex_sim": 1.0}
{"question": "What is the line called on a ship which is the limit to which it may be loaded?", "answer": ["Load Line"], "scores": [[0.10533693432807922, 0.3499365746974945]], "normalized_score": [0.19199281930923462], "tokens": [["\u2581Load", "\u2581Line"]], "entropy": [[4.914100646972656, 2.051814079284668]], "avg_entropy": [3.482957363128662], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "International load line", "score_of_biggest_cluster": 0.19199281930923462, "score_of_first_answer": 0.19199281930923462, "max_score_of_first_answer": 0.3499365746974945, "min_score_of_first_answer": 0.10533693432807922, "avg_score": 0.19199281930923462, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.12229982018470764, "std_of_first_answer": 0.12229982018470764, "avg_entropy_of_first_answer": 3.482957363128662, "lex_sim": 1.0}
{"question": "What movie featured a talking candelabra named Lumi\u00e8re?", "answer": ["Beauty and the Beast (1991 film)"], "scores": [[0.5753933787345886, 0.9992952346801758, 0.9456719160079956, 0.9707221388816833, 0.9822465181350708, 0.9976288676261902, 0.5961269736289978, 0.5316041111946106, 0.999645471572876, 0.9847593903541565, 0.9905114769935608, 0.7866030335426331, 0.9935036301612854]], "normalized_score": [0.8521292805671692], "tokens": [["\u2581Beaut", "y", "\u2581and", "\u2581the", "\u2581Be", "ast", "\u2581(", "1", "9", "9", "1", "\u2581film", ")"]], "entropy": [[2.978900909423828, 0.008816746063530445, 0.3346765637397766, 0.16295002400875092, 0.14181320369243622, 0.023531101644039154, 0.9812450408935547, 1.6028077602386475, 0.004386086016893387, 0.09217716753482819, 0.06737382709980011, 0.9693809747695923, 0.06203430891036987]], "avg_entropy": [0.5715456703104652], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Beauty and the Beast & Other Stories", "score_of_biggest_cluster": 0.8521292805671692, "score_of_first_answer": 0.8521292805671692, "max_score_of_first_answer": 0.999645471572876, "min_score_of_first_answer": 0.5316041111946106, "avg_score": 0.8521292805671692, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17636283182132576, "std_of_first_answer": 0.17636283182132576, "avg_entropy_of_first_answer": 0.5715456703104652, "lex_sim": 1.0}
{"question": "'Christie Mahon' is the title character in which play?", "answer": ["The Shadow of a Gunman"], "scores": [[0.32386860251426697, 0.38922351598739624, 0.8902479410171509, 0.9647188782691956, 0.9266971349716187, 0.791698694229126, 0.9963890314102173]], "normalized_score": [0.6960324645042419], "tokens": [["\u2581The", "\u2581Sh", "adow", "\u2581of", "\u2581a", "\u2581Gun", "man"]], "entropy": [[3.8806653022766113, 1.836893916130066, 0.4671100974082947, 0.2493063360452652, 0.36940649151802063, 1.0379540920257568, 0.03431235998868942]], "avg_entropy": [1.125092656484672], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sara Tansey", "score_of_biggest_cluster": 0.6960324645042419, "score_of_first_answer": 0.6960324645042419, "max_score_of_first_answer": 0.9963890314102173, "min_score_of_first_answer": 0.32386860251426697, "avg_score": 0.6960324645042419, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2594046569234606, "std_of_first_answer": 0.2594046569234606, "avg_entropy_of_first_answer": 1.125092656484672, "lex_sim": 1.0}
{"question": "With regard to book titles, if TDVC is 'The Da Vinci Code', which E L James book is F S 0 G?", "answer": ["Fire in the Fifth Fifth Fifth Fifth Fifth Fifth Fifth Fifth Fifth Fifth Fifth Fifth Fifth Fifth Fifth Fifth Fifth Fifth Fifth Fifth Fifth Fifth Fifth Fif"], "scores": [[0.0022878709714859724, 0.026184918358922005, 0.48456433415412903, 0.26560789346694946, 0.21286849677562714, 0.004082339815795422, 0.23092372715473175, 0.15890344977378845, 0.7484151721000671, 0.47084739804267883, 0.9439669847488403, 0.7127992510795593, 0.9690569043159485, 0.7743431925773621, 0.9820963144302368, 0.8505324125289917, 0.9886153936386108, 0.8774491548538208, 0.9908031225204468, 0.8927963376045227, 0.9917308688163757, 0.896937906742096, 0.9924613833427429, 0.8998138308525085, 0.9931597709655762, 0.9065454602241516, 0.994361400604248, 0.9125124216079712, 0.9952926635742188, 0.9205284118652344, 0.9959281086921692, 0.9262898564338684, 0.9964576363563538, 0.9332952499389648, 0.9967855215072632, 0.9407458305358887, 0.9972705245018005, 0.9475879669189453, 0.9977728724479675, 0.9525742530822754, 0.9979885816574097, 0.9576443433761597, 0.9980499744415283, 0.960634708404541, 0.9981434345245361, 0.9632046222686768, 0.9983232617378235, 0.964552640914917, 0.998475968837738, 0.9662014245986938]], "normalized_score": [0.6023877263069153], "tokens": [["\u2581Fire", "\u2581in", "\u2581the", "\u2581Fif", "th", "\u2581Fif", "th", "\u2581Fif", "th", "\u2581Fif", "th", "\u2581Fif", "th", "\u2581Fif", "th", "\u2581Fif", "th", "\u2581Fif", "th", "\u2581Fif", "th", "\u2581Fif", "th", "\u2581Fif", "th", "\u2581Fif", "th", "\u2581Fif", "th", "\u2581Fif", "th", "\u2581Fif", "th", "\u2581Fif", "th", "\u2581Fif", "th", "\u2581Fif", "th", "\u2581Fif", "th", "\u2581Fif", "th", "\u2581Fif", "th", "\u2581Fif", "th", "\u2581Fif", "th", "\u2581Fif"]], "entropy": [[2.697850465774536, 3.7276477813720703, 3.257797956466675, 4.370813369750977, 0.5911077260971069, 4.390127182006836, 0.6888539791107178, 3.6918601989746094, 0.6508752107620239, 2.668132781982422, 0.2654276490211487, 1.7195934057235718, 0.17664477229118347, 1.367218017578125, 0.12139800190925598, 0.9936034679412842, 0.0886663943529129, 0.8444141745567322, 0.07474023103713989, 0.7508164644241333, 0.06680586189031601, 0.7182121276855469, 0.0609959177672863, 0.6918560862541199, 0.05556713044643402, 0.6519261598587036, 0.0470389649271965, 0.6185349225997925, 0.040003858506679535, 0.5708109140396118, 0.03496154770255089, 0.5359078049659729, 0.030589766800403595, 0.49431777000427246, 0.027853747829794884, 0.447108656167984, 0.02419516071677208, 0.40268608927726746, 0.020276643335819244, 0.370574951171875, 0.01849575713276863, 0.33667606115341187, 0.01805352419614792, 0.31655916571617126, 0.017277099192142487, 0.29860055446624756, 0.016060099005699158, 0.28917235136032104, 0.01496012695133686, 0.2772277593612671]], "avg_entropy": [0.8130179162323475], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fifty shades of Grey", "score_of_biggest_cluster": 0.6023877263069153, "score_of_first_answer": 0.6023877263069153, "max_score_of_first_answer": 0.998475968837738, "min_score_of_first_answer": 0.0022878709714859724, "avg_score": 0.6023877263069153, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29943631586452696, "std_of_first_answer": 0.29943631586452696, "avg_entropy_of_first_answer": 0.8130179162323475, "lex_sim": 1.0}
{"question": "What sort of plants belong to the genus Ilex?", "answer": ["Holly"], "scores": [[0.3555970788002014, 0.7550856471061707]], "normalized_score": [0.5181758999824524], "tokens": [["\u2581Hol", "ly"]], "entropy": [[4.530364036560059, 0.645290195941925]], "avg_entropy": [2.587827116250992], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ilex species", "score_of_biggest_cluster": 0.5181758999824524, "score_of_first_answer": 0.5181758999824524, "max_score_of_first_answer": 0.7550856471061707, "min_score_of_first_answer": 0.3555970788002014, "avg_score": 0.5181758999824524, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19974428415298462, "std_of_first_answer": 0.19974428415298462, "avg_entropy_of_first_answer": 2.587827116250992, "lex_sim": 1.0}
{"question": "Which British politician became Member of Parliament for Epping in October 1924?", "answer": ["Winston Churchill"], "scores": [[0.282819002866745, 0.9713088870048523, 0.9941616654396057, 0.7783139944076538, 0.977536678314209]], "normalized_score": [0.7303350567817688], "tokens": [["\u2581W", "inst", "on", "\u2581Church", "ill"]], "entropy": [[4.766530990600586, 0.262917697429657, 0.05828487500548363, 1.3605901002883911, 0.17720942199230194]], "avg_entropy": [1.325106617063284], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tango (cat)", "score_of_biggest_cluster": 0.7303350567817688, "score_of_first_answer": 0.7303350567817688, "max_score_of_first_answer": 0.9941616654396057, "min_score_of_first_answer": 0.282819002866745, "avg_score": 0.7303350567817688, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2707426074989423, "std_of_first_answer": 0.2707426074989423, "avg_entropy_of_first_answer": 1.325106617063284, "lex_sim": 1.0}
{"question": "How meny times have Liverpool won the Europion Cup?", "answer": ["Europe Cup Champions: 5"], "scores": [[0.0055307247675955296, 0.13869830965995789, 0.016788287088274956, 0.05520744249224663, 0.2632008492946625, 0.7626866102218628]], "normalized_score": [0.0722905844449997], "tokens": [["\u2581Europe", "\u2581Cup", "\u2581Champions", ":", "\u2581", "5"]], "entropy": [[4.189492225646973, 4.157976150512695, 3.405482769012451, 2.6063826084136963, 2.414928674697876, 1.0403966903686523]], "avg_entropy": [2.969109853108724], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "five", "score_of_biggest_cluster": 0.0722905844449997, "score_of_first_answer": 0.0722905844449997, "max_score_of_first_answer": 0.7626866102218628, "min_score_of_first_answer": 0.0055307247675955296, "avg_score": 0.0722905844449997, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2634459489635462, "std_of_first_answer": 0.2634459489635462, "avg_entropy_of_first_answer": 2.969109853108724, "lex_sim": 1.0}
{"question": "\"Which artist was known as \"\"Jack the Dripper'?\"", "answer": ["Jackson Pollock"], "scores": [[0.5528582334518433, 0.9540179371833801, 0.9168735146522522]], "normalized_score": [0.7849221229553223], "tokens": [["\u2581Jackson", "\u2581Pol", "lock"]], "entropy": [[3.1368508338928223, 0.42798084020614624, 0.43572503328323364]], "avg_entropy": [1.3335189024607341], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Paul Jackson Pollock", "score_of_biggest_cluster": 0.7849221229553223, "score_of_first_answer": 0.7849221229553223, "max_score_of_first_answer": 0.9540179371833801, "min_score_of_first_answer": 0.5528582334518433, "avg_score": 0.7849221229553223, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.18098985268380785, "std_of_first_answer": 0.18098985268380785, "avg_entropy_of_first_answer": 1.3335189024607341, "lex_sim": 1.0}
{"question": "The legal year is divided into four terms. Which term runs from January to April?", "answer": ["The Legal Year"], "scores": [[0.05408376827836037, 0.10387241840362549, 0.9904947280883789, 0.7437419891357422]], "normalized_score": [0.2536357343196869], "tokens": [["\u2581The", "\u2581Leg", "al", "\u2581Year"]], "entropy": [[4.432497978210449, 4.164130210876465, 0.09120753407478333, 1.3283019065856934]], "avg_entropy": [2.5040344074368477], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hilary (disambiguation)", "score_of_biggest_cluster": 0.2536357343196869, "score_of_first_answer": 0.2536357343196869, "max_score_of_first_answer": 0.9904947280883789, "min_score_of_first_answer": 0.05408376827836037, "avg_score": 0.2536357343196869, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4039950462791935, "std_of_first_answer": 0.4039950462791935, "avg_entropy_of_first_answer": 2.5040344074368477, "lex_sim": 1.0}
{"question": "100.4 Smooth Radio, and 105.4 Century FM, etc, are based where?", "answer": ["North East"], "scores": [[0.050074733793735504, 0.22452418506145477]], "normalized_score": [0.10603296756744385], "tokens": [["\u2581North", "\u2581East"]], "entropy": [[4.680307388305664, 1.9521596431732178]], "avg_entropy": [3.316233515739441], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Salford Quays", "score_of_biggest_cluster": 0.10603296756744385, "score_of_first_answer": 0.10603296756744385, "max_score_of_first_answer": 0.22452418506145477, "min_score_of_first_answer": 0.050074733793735504, "avg_score": 0.10603296756744385, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08722472563385963, "std_of_first_answer": 0.08722472563385963, "avg_entropy_of_first_answer": 3.316233515739441, "lex_sim": 1.0}
{"question": "Chang Beer originates from which country?", "answer": ["Thailand"], "scores": [[0.35041528940200806, 0.8448247313499451]], "normalized_score": [0.5440950989723206], "tokens": [["\u2581Th", "ailand"]], "entropy": [[4.358068466186523, 0.7135939002037048]], "avg_entropy": [2.535831183195114], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Muang Thai", "score_of_biggest_cluster": 0.5440950989723206, "score_of_first_answer": 0.5440950989723206, "max_score_of_first_answer": 0.8448247313499451, "min_score_of_first_answer": 0.35041528940200806, "avg_score": 0.5440950989723206, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2472047209739685, "std_of_first_answer": 0.2472047209739685, "avg_entropy_of_first_answer": 2.535831183195114, "lex_sim": 1.0}
{"question": "Which supermarket chain started selling straight croissants in its stores in February?", "answer": ["Morison"], "scores": [[0.019465094432234764, 0.006228582467883825]], "normalized_score": [0.01101089920848608], "tokens": [["\u2581Mor", "ison"]], "entropy": [[5.525517463684082, 1.5479652881622314]], "avg_entropy": [3.5367413759231567], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tesco direct", "score_of_biggest_cluster": 0.01101089920848608, "score_of_first_answer": 0.01101089920848608, "max_score_of_first_answer": 0.019465094432234764, "min_score_of_first_answer": 0.006228582467883825, "avg_score": 0.01101089920848608, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.006618255982175469, "std_of_first_answer": 0.006618255982175469, "avg_entropy_of_first_answer": 3.5367413759231567, "lex_sim": 1.0}
{"question": "In which decade was the English League Cup first contested?", "answer": ["Sevenies"], "scores": [[0.0014411697629839182, 0.007565815467387438]], "normalized_score": [0.003302063327282667], "tokens": [["\u2581Seven", "ies"]], "entropy": [[3.7169103622436523, 4.45003604888916]], "avg_entropy": [4.083473205566406], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sixties Revolution", "score_of_biggest_cluster": 0.003302063327282667, "score_of_first_answer": 0.003302063327282667, "max_score_of_first_answer": 0.007565815467387438, "min_score_of_first_answer": 0.0014411697629839182, "avg_score": 0.003302063327282667, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.00306232285220176, "std_of_first_answer": 0.00306232285220176, "avg_entropy_of_first_answer": 4.083473205566406, "lex_sim": 1.0}
{"question": "What type of creature is a bufflehead?", "answer": ["A duck"], "scores": [[0.0684112161397934, 0.15817303955554962, 0.9990159273147583]], "normalized_score": [0.22111119329929352], "tokens": [["\u2581A", "\u2581du", "ck"]], "entropy": [[4.6764092445373535, 4.318192005157471, 0.011185446754097939]], "avg_entropy": [3.0019288988163075], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Quack (sound)", "score_of_biggest_cluster": 0.22111119329929352, "score_of_first_answer": 0.22111119329929352, "max_score_of_first_answer": 0.9990159273147583, "min_score_of_first_answer": 0.0684112161397934, "avg_score": 0.22111119329929352, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4191392072250521, "std_of_first_answer": 0.4191392072250521, "avg_entropy_of_first_answer": 3.0019288988163075, "lex_sim": 1.0}
{"question": "Which country, formerly known as Pleasant Island, is the smallest island country in the world?", "answer": ["Republic of Nauru"], "scores": [[0.0663638785481453, 0.9763597846031189, 0.7421025633811951, 0.970517098903656]], "normalized_score": [0.464785099029541], "tokens": [["\u2581Republic", "\u2581of", "\u2581Na", "uru"]], "entropy": [[5.024470329284668, 0.19713768362998962, 1.0272142887115479, 0.2176733762025833]], "avg_entropy": [1.6166239194571972], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "List of islands of Nauru", "score_of_biggest_cluster": 0.464785099029541, "score_of_first_answer": 0.464785099029541, "max_score_of_first_answer": 0.9763597846031189, "min_score_of_first_answer": 0.0663638785481453, "avg_score": 0.464785099029541, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3715921948020348, "std_of_first_answer": 0.3715921948020348, "avg_entropy_of_first_answer": 1.6166239194571972, "lex_sim": 1.0}
{"question": "Which family of insects has species named 'drivers', 'weavers', 'and even individuals called 'soldiers'?", "answer": ["Spongilla spongilliformis"], "scores": [[0.051396433264017105, 0.008210532367229462, 0.20504820346832275, 0.014052655547857285, 0.1749170571565628, 0.3549875319004059, 0.14462055265903473, 0.8578314185142517]], "normalized_score": [0.09918581694364548], "tokens": [["\u2581Sp", "ong", "illa", "\u2581sp", "ong", "ill", "iform", "is"]], "entropy": [[5.110459804534912, 1.268630862236023, 2.7548084259033203, 3.1690449714660645, 2.2240734100341797, 2.8749284744262695, 3.691218852996826, 0.8083858489990234]], "avg_entropy": [2.7376938313245773], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "History of ants", "score_of_biggest_cluster": 0.09918581694364548, "score_of_first_answer": 0.09918581694364548, "max_score_of_first_answer": 0.8578314185142517, "min_score_of_first_answer": 0.008210532367229462, "avg_score": 0.09918581694364548, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26181074863979376, "std_of_first_answer": 0.26181074863979376, "avg_entropy_of_first_answer": 2.7376938313245773, "lex_sim": 1.0}
{"question": "'Captain Yossarian' is the central character of which 20th century novel?", "answer": ["Journey to the Centre of the Earth"], "scores": [[0.0019968627020716667, 0.04025908187031746, 0.8638644814491272, 0.43090128898620605, 0.7883632183074951, 0.46404385566711426, 0.9950760006904602, 0.9852749109268188, 0.9763782024383545]], "normalized_score": [0.2797086238861084], "tokens": [["\u2581J", "our", "ney", "\u2581to", "\u2581the", "\u2581Centre", "\u2581of", "\u2581the", "\u2581Earth"]], "entropy": [[3.8286869525909424, 4.834113121032715, 0.5882066488265991, 2.5467658042907715, 1.6461374759674072, 2.3285536766052246, 0.04322575777769089, 0.13293419778347015, 0.21595463156700134]], "avg_entropy": [1.796064251826869], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Catch-22 (novel)", "score_of_biggest_cluster": 0.2797086238861084, "score_of_first_answer": 0.2797086238861084, "max_score_of_first_answer": 0.9950760006904602, "min_score_of_first_answer": 0.0019968627020716667, "avg_score": 0.2797086238861084, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3751903288506591, "std_of_first_answer": 0.3751903288506591, "avg_entropy_of_first_answer": 1.796064251826869, "lex_sim": 1.0}
{"question": "Who was the narrator in the novel `Moby Dick'?", "answer": ["Penn"], "scores": [[0.00852763932198286, 0.00263979216106236]], "normalized_score": [0.0047445958480238914], "tokens": [["\u2581P", "enn"]], "entropy": [[4.859590530395508, 3.2404181957244873]], "avg_entropy": [4.050004363059998], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kedemah", "score_of_biggest_cluster": 0.0047445958480238914, "score_of_first_answer": 0.0047445958480238914, "max_score_of_first_answer": 0.00852763932198286, "min_score_of_first_answer": 0.00263979216106236, "avg_score": 0.0047445958480238914, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0029439235804602504, "std_of_first_answer": 0.0029439235804602504, "avg_entropy_of_first_answer": 4.050004363059998, "lex_sim": 1.0}
{"question": "To within a year either side, in which year was the complete English FA Cup Final first shown live on television?", "answer": ["1953 \u2013 1956"], "scores": [[0.6164901852607727, 0.7206332087516785, 0.9429137706756592, 0.42315611243247986, 0.31812915205955505, 0.0089429235085845, 0.8984791040420532, 0.9062490463256836, 0.9920835494995117, 0.8945059776306152, 0.040819309651851654]], "normalized_score": [0.36402544379234314], "tokens": [["\u2581", "1", "9", "5", "3", "\u2581\u2013", "\u2581", "1", "9", "5", "6"]], "entropy": [[2.9144861698150635, 1.560495376586914, 0.4272056221961975, 1.5241713523864746, 1.9784187078475952, 3.418921947479248, 0.8947536945343018, 0.45248860120773315, 0.07456482201814651, 0.4303949177265167, 1.3816386461257935]], "avg_entropy": [1.3688672598112712], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and thirty-eight", "score_of_biggest_cluster": 0.36402544379234314, "score_of_first_answer": 0.36402544379234314, "max_score_of_first_answer": 0.9920835494995117, "min_score_of_first_answer": 0.0089429235085845, "avg_score": 0.36402544379234314, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3472115480299068, "std_of_first_answer": 0.3472115480299068, "avg_entropy_of_first_answer": 1.3688672598112712, "lex_sim": 1.0}
{"question": "The Reign Of Terror was a period in the revolution of which country?", "answer": ["Southeast Asia"], "scores": [[0.0023896426428109407, 0.004948474932461977, 0.7729507684707642, 0.6448804140090942]], "normalized_score": [0.04927298426628113], "tokens": [["\u2581S", "out", "heast", "\u2581Asia"]], "entropy": [[4.086773872375488, 6.209609031677246, 0.9907714128494263, 1.8667783737182617]], "avg_entropy": [3.2884831726551056], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "La Republique francaise", "score_of_biggest_cluster": 0.04927298426628113, "score_of_first_answer": 0.04927298426628113, "max_score_of_first_answer": 0.7729507684707642, "min_score_of_first_answer": 0.0023896426428109407, "avg_score": 0.04927298426628113, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3555196735742253, "std_of_first_answer": 0.3555196735742253, "avg_entropy_of_first_answer": 3.2884831726551056, "lex_sim": 1.0}
{"question": "Which type of fish is used to make an Arbroath Smokie?", "answer": ["Haddock"], "scores": [[0.20223908126354218, 0.9544299840927124, 0.964699387550354]], "normalized_score": [0.571040689945221], "tokens": [["\u2581H", "add", "ock"]], "entropy": [[4.787288665771484, 0.409181147813797, 0.2640882134437561]], "avg_entropy": [1.820186009009679], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Haddock", "score_of_biggest_cluster": 0.571040689945221, "score_of_first_answer": 0.571040689945221, "max_score_of_first_answer": 0.964699387550354, "min_score_of_first_answer": 0.20223908126354218, "avg_score": 0.571040689945221, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3570313297191651, "std_of_first_answer": 0.3570313297191651, "avg_entropy_of_first_answer": 1.820186009009679, "lex_sim": 1.0}
{"question": "What are the international registration letters of a vehicle from Iceland?", "answer": ["IS"], "scores": [[0.33945685625076294]], "normalized_score": [0.33945685625076294], "tokens": [["\u2581IS"]], "entropy": [[3.8416149616241455]], "avg_entropy": [3.8416149616241455], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "I S", "score_of_biggest_cluster": 0.33945685625076294, "score_of_first_answer": 0.33945685625076294, "max_score_of_first_answer": 0.33945685625076294, "min_score_of_first_answer": 0.33945685625076294, "avg_score": 0.33945685625076294, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.8416149616241455, "lex_sim": 1.0}
{"question": "How was Monday October 19, 1987 known in the world of finance", "answer": ["Black Monday"], "scores": [[0.7790066599845886, 0.9048135876655579]], "normalized_score": [0.8395569324493408], "tokens": [["\u2581Black", "\u2581Monday"]], "entropy": [[1.5479938983917236, 0.6989501118659973]], "avg_entropy": [1.1234720051288605], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "List of Black Mondays", "score_of_biggest_cluster": 0.8395569324493408, "score_of_first_answer": 0.8395569324493408, "max_score_of_first_answer": 0.9048135876655579, "min_score_of_first_answer": 0.7790066599845886, "avg_score": 0.8395569324493408, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06290346384048462, "std_of_first_answer": 0.06290346384048462, "avg_entropy_of_first_answer": 1.1234720051288605, "lex_sim": 1.0}
{"question": "Lake Neuchatel is in which country?", "answer": ["\uff08^\u203f\u21bc^\uff09"], "scores": [[0.040651869028806686, 0.001545910257846117, 0.0067903450690209866, 0.030467748641967773, 0.5983918309211731, 0.33993178606033325, 0.050621502101421356, 0.15921205282211304, 0.22400495409965515, 0.46356016397476196, 0.699855387210846]], "normalized_score": [0.08440513908863068], "tokens": [["\u2581", "\uff08", "^", "<0xE2>", "<0x80>", "<0xBF>", "<0xE2>", "<0x86>", "<0xBC>", "^", "\uff09"]], "entropy": [[4.251117706298828, 4.489137649536133, 5.6819682121276855, 3.647740364074707, 1.447569489479065, 1.802558422088623, 0.717939019203186, 1.8201152086257935, 1.9328875541687012, 2.3620684146881104, 0.859844446182251]], "avg_entropy": [2.637540589679371], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ISO 3166-1:CH", "score_of_biggest_cluster": 0.08440513908863068, "score_of_first_answer": 0.08440513908863068, "max_score_of_first_answer": 0.699855387210846, "min_score_of_first_answer": 0.001545910257846117, "avg_score": 0.08440513908863068, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24071065462194124, "std_of_first_answer": 0.24071065462194124, "avg_entropy_of_first_answer": 2.637540589679371, "lex_sim": 0.0}
{"question": "\"Which American writer chronicled the lives of the Glass family in works such as \"\"Franny and Zooey\"\", and \"\"Raise the Roofbeam, Carpenters\"\"?\"", "answer": ["J D Salinger"], "scores": [[0.3624134361743927, 0.1466350257396698, 0.8243502378463745, 0.9739710092544556]], "normalized_score": [0.45449066162109375], "tokens": [["\u2581J", "\u2581D", "\u2581Sal", "inger"]], "entropy": [[4.476154327392578, 1.4067332744598389, 1.325924277305603, 0.22107675671577454]], "avg_entropy": [1.8574721589684486], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Salingerian", "score_of_biggest_cluster": 0.45449066162109375, "score_of_first_answer": 0.45449066162109375, "max_score_of_first_answer": 0.9739710092544556, "min_score_of_first_answer": 0.1466350257396698, "avg_score": 0.45449066162109375, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3354211634196081, "std_of_first_answer": 0.3354211634196081, "avg_entropy_of_first_answer": 1.8574721589684486, "lex_sim": 1.0}
{"question": "Thomas Hardy's famous novel is 'Far from the Madding (What?)'?", "answer": ["Crowd"], "scores": [[0.31135597825050354, 0.9443005323410034]], "normalized_score": [0.5422302484512329], "tokens": [["\u2581Crow", "d"]], "entropy": [[3.710184097290039, 0.35829612612724304]], "avg_entropy": [2.034240111708641], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Croud", "score_of_biggest_cluster": 0.5422302484512329, "score_of_first_answer": 0.5422302484512329, "max_score_of_first_answer": 0.9443005323410034, "min_score_of_first_answer": 0.31135597825050354, "avg_score": 0.5422302484512329, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31647227704524994, "std_of_first_answer": 0.31647227704524994, "avg_entropy_of_first_answer": 2.034240111708641, "lex_sim": 1.0}
{"question": "Who has been President of France twice, but never been elected to the position?", "answer": ["Adolphe Thiers"], "scores": [[0.002852433593943715, 0.6931458115577698, 0.9620673656463623, 0.772503137588501, 0.9610397219657898]], "normalized_score": [0.26913946866989136], "tokens": [["\u2581Ad", "ol", "phe", "\u2581Th", "iers"]], "entropy": [[5.349262237548828, 1.9117857217788696, 0.298520565032959, 1.7504963874816895, 0.26626527309417725]], "avg_entropy": [1.9152660369873047], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Alain Poher", "score_of_biggest_cluster": 0.26913946866989136, "score_of_first_answer": 0.26913946866989136, "max_score_of_first_answer": 0.9620673656463623, "min_score_of_first_answer": 0.002852433593943715, "avg_score": 0.26913946866989136, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3537766838123156, "std_of_first_answer": 0.3537766838123156, "avg_entropy_of_first_answer": 1.9152660369873047, "lex_sim": 1.0}
{"question": "What product is the Singer company well-known for making?", "answer": ["Sewing machine"], "scores": [[0.2900773882865906, 0.6115249991416931, 0.9788581728935242, 0.5129646062850952]], "normalized_score": [0.5463032722473145], "tokens": [["\u2581S", "ew", "ing", "\u2581machine"]], "entropy": [[5.2612762451171875, 1.4756250381469727, 0.20094822347164154, 1.5014474391937256]], "avg_entropy": [2.109824236482382], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Foot (sewing)", "score_of_biggest_cluster": 0.5463032722473145, "score_of_first_answer": 0.5463032722473145, "max_score_of_first_answer": 0.9788581728935242, "min_score_of_first_answer": 0.2900773882865906, "avg_score": 0.5463032722473145, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24863764732802832, "std_of_first_answer": 0.24863764732802832, "avg_entropy_of_first_answer": 2.109824236482382, "lex_sim": 1.0}
{"question": "Who in the Bible, became the first drunken man after planting a vineyard?", "answer": ["Noaa"], "scores": [[0.5462899208068848, 0.00026768725365400314]], "normalized_score": [0.012092760764062405], "tokens": [["\u2581No", "aa"]], "entropy": [[3.385819911956787, 0.3136826455593109]], "avg_entropy": [1.849751278758049], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Noah (album)", "score_of_biggest_cluster": 0.012092760764062405, "score_of_first_answer": 0.012092760764062405, "max_score_of_first_answer": 0.5462899208068848, "min_score_of_first_answer": 0.00026768725365400314, "avg_score": 0.012092760764062405, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2730111167766154, "std_of_first_answer": 0.2730111167766154, "avg_entropy_of_first_answer": 1.849751278758049, "lex_sim": 1.0}
{"question": "Who wrote, produced and directed epic film, Avatar?", "answer": ["James Cameron"], "scores": [[0.4462391138076782, 0.7330725193023682, 0.9952940344810486]], "normalized_score": [0.6879475712776184], "tokens": [["\u2581James", "\u2581Camer", "on"]], "entropy": [[4.161464691162109, 1.9555106163024902, 0.048113394528627396]], "avg_entropy": [2.0550295673310757], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "James cameron", "score_of_biggest_cluster": 0.6879475712776184, "score_of_first_answer": 0.6879475712776184, "max_score_of_first_answer": 0.9952940344810486, "min_score_of_first_answer": 0.4462391138076782, "avg_score": 0.6879475712776184, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.22422578686224223, "std_of_first_answer": 0.22422578686224223, "avg_entropy_of_first_answer": 2.0550295673310757, "lex_sim": 1.0}
{"question": "In Greek mythology who was the god of the north winds?", "answer": ["Boreas"], "scores": [[0.3516453206539154, 0.9287149906158447, 0.8452117443084717]], "normalized_score": [0.6511048674583435], "tokens": [["\u2581B", "ore", "as"]], "entropy": [[4.571600914001465, 0.6804206371307373, 1.0328681468963623]], "avg_entropy": [2.094963232676188], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Boreas (disambiguation)", "score_of_biggest_cluster": 0.6511048674583435, "score_of_first_answer": 0.6511048674583435, "max_score_of_first_answer": 0.9287149906158447, "min_score_of_first_answer": 0.3516453206539154, "avg_score": 0.6511048674583435, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25464354404037653, "std_of_first_answer": 0.25464354404037653, "avg_entropy_of_first_answer": 2.094963232676188, "lex_sim": 1.0}
{"question": "Mrs Darrell Waters books were translated into 120 languages. What was her pen name?", "answer": ["\u36e8"], "scores": [[0.021999076008796692, 0.10056004673242569, 0.006929012015461922, 0.01899043284356594]], "normalized_score": [0.02322784624993801], "tokens": [["\u2581", "<0xE3>", "<0x9B>", "<0xA8>"]], "entropy": [[5.40150260925293, 4.983705520629883, 2.576033353805542, 4.031140327453613]], "avg_entropy": [4.248095452785492], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Little lose hen", "score_of_biggest_cluster": 0.02322784624993801, "score_of_first_answer": 0.02322784624993801, "max_score_of_first_answer": 0.10056004673242569, "min_score_of_first_answer": 0.006929012015461922, "avg_score": 0.02322784624993801, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.03705893560266314, "std_of_first_answer": 0.03705893560266314, "avg_entropy_of_first_answer": 4.248095452785492, "lex_sim": 0.0}
{"question": "In which country was singer Chris de Burgh born?", "answer": ["Trinidad"], "scores": [[0.012054295279085636, 0.43266868591308594, 0.9832903146743774]], "normalized_score": [0.17244862020015717], "tokens": [["\u2581Tr", "in", "idad"]], "entropy": [[4.879570960998535, 2.4446067810058594, 0.1559215933084488]], "avg_entropy": [2.493366445104281], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Arxintina", "score_of_biggest_cluster": 0.17244862020015717, "score_of_first_answer": 0.17244862020015717, "max_score_of_first_answer": 0.9832903146743774, "min_score_of_first_answer": 0.012054295279085636, "avg_score": 0.17244862020015717, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39768776823574664, "std_of_first_answer": 0.39768776823574664, "avg_entropy_of_first_answer": 2.493366445104281, "lex_sim": 1.0}
{"question": "Which spy was sentenced to 42 years in prison in 1961, but escaped from Wormwood Scrubs in 1966?", "answer": ["George Blake"], "scores": [[0.46037545800209045, 0.9245655536651611, 0.9967511892318726]], "normalized_score": [0.7514132857322693], "tokens": [["\u2581George", "\u2581Bla", "ke"]], "entropy": [[3.693242073059082, 0.6959917545318604, 0.034950193017721176]], "avg_entropy": [1.4747280068695545], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "GEORGE BLAKE", "score_of_biggest_cluster": 0.7514132857322693, "score_of_first_answer": 0.7514132857322693, "max_score_of_first_answer": 0.9967511892318726, "min_score_of_first_answer": 0.46037545800209045, "avg_score": 0.7514132857322693, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23766973722797785, "std_of_first_answer": 0.23766973722797785, "avg_entropy_of_first_answer": 1.4747280068695545, "lex_sim": 1.0}
{"question": "In which country would you find the wine-producing region known as 'Casablanca Valley'?", "answer": ["Chile"], "scores": [[0.3010556399822235]], "normalized_score": [0.3010556399822235], "tokens": [["\u2581Chile"]], "entropy": [[4.828114986419678]], "avg_entropy": [4.828114986419678], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "ISO 3166-1:CL", "score_of_biggest_cluster": 0.3010556399822235, "score_of_first_answer": 0.3010556399822235, "max_score_of_first_answer": 0.3010556399822235, "min_score_of_first_answer": 0.3010556399822235, "avg_score": 0.3010556399822235, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.828114986419678, "lex_sim": 1.0}
{"question": "What language was first to be represented in dictionaries and related language theory?", "answer": ["Cuneiform"], "scores": [[0.016186999157071114, 0.08620740473270416, 0.9836318492889404]], "normalized_score": [0.11113420873880386], "tokens": [["\u2581C", "une", "iform"]], "entropy": [[5.931754112243652, 5.258306503295898, 0.13629093766212463]], "avg_entropy": [3.775450517733892], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chine\u017fe", "score_of_biggest_cluster": 0.11113420873880386, "score_of_first_answer": 0.11113420873880386, "max_score_of_first_answer": 0.9836318492889404, "min_score_of_first_answer": 0.016186999157071114, "avg_score": 0.11113420873880386, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44048244118500596, "std_of_first_answer": 0.44048244118500596, "avg_entropy_of_first_answer": 3.775450517733892, "lex_sim": 1.0}
{"question": "In 2005, Dan Wheldon became the first British competitor to win what event in over 30 years?", "answer": ["US-centric"], "scores": [[0.017626818269491196, 0.018677718937397003, 0.006530763115733862, 0.805504560470581]], "normalized_score": [0.036277081817388535], "tokens": [["\u2581US", "-", "cent", "ric"]], "entropy": [[4.292301654815674, 4.26099967956543, 3.454287052154541, 0.7210797071456909]], "avg_entropy": [3.182167023420334], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Indianapolis 500-Mile Race", "score_of_biggest_cluster": 0.036277081817388535, "score_of_first_answer": 0.036277081817388535, "max_score_of_first_answer": 0.805504560470581, "min_score_of_first_answer": 0.006530763115733862, "avg_score": 0.036277081817388535, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3426440134673777, "std_of_first_answer": 0.3426440134673777, "avg_entropy_of_first_answer": 3.182167023420334, "lex_sim": 1.0}
{"question": "Which US Olympic swimmer is nicknamed the \u2018Baltimore Bullet\u2019?", "answer": ["Olympic Swimmer Michael Phelps"], "scores": [[0.009089156985282898, 0.1259869933128357, 0.8695582747459412, 0.7398272752761841, 0.9204305410385132, 0.9980977177619934, 0.9934513568878174]], "normalized_score": [0.35220733284950256], "tokens": [["\u2581Olympic", "\u2581Sw", "immer", "\u2581Michael", "\u2581Ph", "el", "ps"]], "entropy": [[3.6741995811462402, 3.3991098403930664, 0.5409857034683228, 1.63331937789917, 0.5982128381729126, 0.021688250824809074, 0.057147540152072906]], "avg_entropy": [1.4178090188652277], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Phelps II", "score_of_biggest_cluster": 0.35220733284950256, "score_of_first_answer": 0.35220733284950256, "max_score_of_first_answer": 0.9980977177619934, "min_score_of_first_answer": 0.009089156985282898, "avg_score": 0.35220733284950256, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3877178856921559, "std_of_first_answer": 0.3877178856921559, "avg_entropy_of_first_answer": 1.4178090188652277, "lex_sim": 1.0}
{"question": "\"Which snooker player had the nickname \"\"The Thunder from Down Under\"\"?\"", "answer": ["The Thunder from Down Under"], "scores": [[0.06340895593166351, 0.5591835975646973, 0.9874388575553894, 0.871417224407196, 0.9851462244987488, 0.984988808631897]], "normalized_score": [0.5561972856521606], "tokens": [["\u2581The", "\u2581Th", "under", "\u2581from", "\u2581Down", "\u2581Under"]], "entropy": [[5.315319061279297, 3.380157470703125, 0.09748728573322296, 0.6764681935310364, 0.1332797408103943, 0.11726399511098862]], "avg_entropy": [1.619995957861344], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Neil Robertson (disambiguation)", "score_of_biggest_cluster": 0.5561972856521606, "score_of_first_answer": 0.5561972856521606, "max_score_of_first_answer": 0.9874388575553894, "min_score_of_first_answer": 0.06340895593166351, "avg_score": 0.5561972856521606, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33888550465218653, "std_of_first_answer": 0.33888550465218653, "avg_entropy_of_first_answer": 1.619995957861344, "lex_sim": 1.0}
{"question": "Who painted Luncheon of the Boating Party?", "answer": ["French 19th century artist Pierre-Auguste Renoir, Luncheon of the Boating Party (French: Le d\u00e9jeuner des canotiers) 1881, National Gallery, Washington, D."], "scores": [[0.0250542014837265, 0.005144285969436169, 0.47853031754493713, 0.4601391553878784, 0.9444637298583984, 0.5161925554275513, 0.1446298509836197, 0.17481809854507446, 0.8050517439842224, 0.9872255921363831, 0.9666385054588318, 0.9503079652786255, 0.9901037216186523, 0.009961478412151337, 0.10356318950653076, 0.985561192035675, 0.9983363747596741, 0.9981799125671387, 0.9675559997558594, 0.9916587471961975, 0.9689118266105652, 0.993694007396698, 0.9920912981033325, 0.2006763219833374, 0.10649679601192474, 0.9721212387084961, 0.7688390612602234, 0.9841923713684082, 0.5672247409820557, 0.9981821775436401, 0.9949942231178284, 0.9988155364990234, 0.9922550916671753, 0.9859858751296997, 0.9935504198074341, 0.9977211356163025, 0.628404438495636, 0.05766528099775314, 0.973355233669281, 0.9867941737174988, 0.9917519688606262, 0.6651126146316528, 0.1814202219247818, 0.007702451664954424, 0.8047693371772766, 0.08087526261806488, 0.8401037454605103, 0.3580445349216461, 0.4415891766548157, 0.9916418790817261]], "normalized_score": [0.4313908517360687], "tokens": [["\u2581French", "\u2581", "1", "9", "th", "\u2581century", "\u2581artist", "\u2581Pierre", "-", "August", "e", "\u2581Ren", "oir", ",", "\u2581L", "un", "che", "on", "\u2581of", "\u2581the", "\u2581Bo", "ating", "\u2581Party", "\u2581(", "F", "rench", ":", "\u2581Le", "\u2581d\u00e9", "je", "un", "er", "\u2581des", "\u2581can", "ot", "iers", ")", "\u2581", "1", "8", "8", "1", ",", "\u2581National", "\u2581Gallery", ",", "\u2581Washington", ",", "\u2581D", "."]], "entropy": [[5.068273067474365, 3.136362314224243, 2.1493406295776367, 0.9430609941482544, 0.38894879817962646, 1.2278279066085815, 2.6656837463378906, 2.629298686981201, 1.0056540966033936, 0.11282235383987427, 0.18147951364517212, 0.3313761055469513, 0.08083853870630264, 0.707115650177002, 4.657314300537109, 0.10965417325496674, 0.01910734921693802, 0.020386110991239548, 0.24865812063217163, 0.07251334190368652, 0.2545793056488037, 0.040926214307546616, 0.0639798492193222, 1.8967370986938477, 2.976217746734619, 0.2105238139629364, 1.346631646156311, 0.12350771576166153, 0.8953253030776978, 0.0161571241915226, 0.04300311952829361, 0.013488199561834335, 0.05393229424953461, 0.09437994658946991, 0.049621839076280594, 0.02138596400618553, 1.333870530128479, 1.5315971374511719, 0.23011785745620728, 0.1060248389840126, 0.07498142868280411, 0.7003620266914368, 1.7500226497650146, 2.1813642978668213, 0.8174903392791748, 0.448965847492218, 0.9052551984786987, 1.6411199569702148, 1.4216923713684082, 0.07778376340866089]], "avg_entropy": [0.9415352244675159], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Pierre Auguste Renoir", "score_of_biggest_cluster": 0.4313908517360687, "score_of_first_answer": 0.4313908517360687, "max_score_of_first_answer": 0.9988155364990234, "min_score_of_first_answer": 0.005144285969436169, "avg_score": 0.4313908517360687, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.37376747146731565, "std_of_first_answer": 0.37376747146731565, "avg_entropy_of_first_answer": 0.9415352244675159, "lex_sim": 1.0}
{"question": "Which country has the greatest number of ancient pyramids? (hint: They are called the Nubian Pyramids)", "answer": ["Sudan"], "scores": [[0.34595733880996704, 0.9873435497283936]], "normalized_score": [0.5844473838806152], "tokens": [["\u2581Sud", "an"]], "entropy": [[3.6608779430389404, 0.12951084971427917]], "avg_entropy": [1.8951943963766098], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Sudan", "score_of_biggest_cluster": 0.5844473838806152, "score_of_first_answer": 0.5844473838806152, "max_score_of_first_answer": 0.9873435497283936, "min_score_of_first_answer": 0.34595733880996704, "avg_score": 0.5844473838806152, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.32069310545921326, "std_of_first_answer": 0.32069310545921326, "avg_entropy_of_first_answer": 1.8951943963766098, "lex_sim": 1.0}
{"question": "In 1996, Dolly the Sheep was successfully cloned in which European country?", "answer": ["United Kingdom"], "scores": [[0.03932187706232071, 0.935516893863678]], "normalized_score": [0.19179749488830566], "tokens": [["\u2581United", "\u2581Kingdom"]], "entropy": [[5.271050453186035, 0.4315904974937439]], "avg_entropy": [2.8513204753398895], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "The Scottish Nation", "score_of_biggest_cluster": 0.19179749488830566, "score_of_first_answer": 0.19179749488830566, "max_score_of_first_answer": 0.935516893863678, "min_score_of_first_answer": 0.03932187706232071, "avg_score": 0.19179749488830566, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.44809750840067863, "std_of_first_answer": 0.44809750840067863, "avg_entropy_of_first_answer": 2.8513204753398895, "lex_sim": 1.0}
{"question": "Edith Cresson became the first female Prime Minister of which European country?", "answer": ["France [2nd Female PM]"], "scores": [[0.33148571848869324, 0.0008609492797404528, 0.03443417325615883, 0.03172336891293526, 0.08446121960878372, 0.9885925054550171, 0.3653687834739685, 0.43189436197280884]], "normalized_score": [0.08947451412677765], "tokens": [["\u2581France", "\u2581[", "2", "nd", "\u2581Fem", "ale", "\u2581PM", "]"]], "entropy": [[4.7462639808654785, 0.783974289894104, 5.545295238494873, 1.7190614938735962, 3.25770902633667, 0.10278390347957611, 2.1480510234832764, 2.232235908508301]], "avg_entropy": [2.5669218581169844], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "La Republique francaise", "score_of_biggest_cluster": 0.08947451412677765, "score_of_first_answer": 0.08947451412677765, "max_score_of_first_answer": 0.9885925054550171, "min_score_of_first_answer": 0.0008609492797404528, "avg_score": 0.08947451412677765, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3108673144674656, "std_of_first_answer": 0.3108673144674656, "avg_entropy_of_first_answer": 2.5669218581169844, "lex_sim": 1.0}
{"question": "Mosses are classified as belonging to which part of the plant kingdom?", "answer": ["Bryophyta"], "scores": [[0.2651030421257019, 0.9582816958427429, 0.9760901927947998, 0.5898711085319519]], "normalized_score": [0.6184273958206177], "tokens": [["\u2581Bry", "oph", "y", "ta"]], "entropy": [[4.7008514404296875, 0.342881441116333, 0.1906067579984665, 0.9999988079071045]], "avg_entropy": [1.5585846118628979], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bryophyta (disambiguation)", "score_of_biggest_cluster": 0.6184273958206177, "score_of_first_answer": 0.6184273958206177, "max_score_of_first_answer": 0.9760901927947998, "min_score_of_first_answer": 0.2651030421257019, "avg_score": 0.6184273958206177, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2933302698961949, "std_of_first_answer": 0.2933302698961949, "avg_entropy_of_first_answer": 1.5585846118628979, "lex_sim": 1.0}
{"question": "Which famous London Building was completed in 1951 from a design by Hugh Casson?", "answer": ["Kew Bridge Railway Station"], "scores": [[0.006981952115893364, 0.26538538932800293, 0.04214585945010185, 0.008635562844574451, 0.7248699069023132]], "normalized_score": [0.054680436849594116], "tokens": [["\u2581K", "ew", "\u2581Bridge", "\u2581Railway", "\u2581Station"]], "entropy": [[5.566305160522461, 3.81681489944458, 2.702711582183838, 2.252915382385254, 1.2607146501541138]], "avg_entropy": [3.1198923349380494], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Royal Festival Hall", "score_of_biggest_cluster": 0.054680436849594116, "score_of_first_answer": 0.054680436849594116, "max_score_of_first_answer": 0.7248699069023132, "min_score_of_first_answer": 0.006981952115893364, "avg_score": 0.054680436849594116, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2749897285604483, "std_of_first_answer": 0.2749897285604483, "avg_entropy_of_first_answer": 3.1198923349380494, "lex_sim": 1.0}
{"question": "Which ship did Drake command against the Spanish Armada?", "answer": ["The Spanish Armada"], "scores": [[0.12452173978090286, 0.007703222334384918, 0.6546986699104309, 0.9816110134124756]], "normalized_score": [0.15757040679454803], "tokens": [["\u2581The", "\u2581Spanish", "\u2581Arm", "ada"]], "entropy": [[5.121987819671631, 4.409150123596191, 2.6698660850524902, 0.13334491848945618]], "avg_entropy": [3.083587236702442], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Retaliatory force", "score_of_biggest_cluster": 0.15757040679454803, "score_of_first_answer": 0.15757040679454803, "max_score_of_first_answer": 0.9816110134124756, "min_score_of_first_answer": 0.007703222334384918, "avg_score": 0.15757040679454803, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3955460893874286, "std_of_first_answer": 0.3955460893874286, "avg_entropy_of_first_answer": 3.083587236702442, "lex_sim": 1.0}
{"question": "A pressure hull is part of which type of vessel?", "answer": ["Submarine"], "scores": [[0.38509440422058105, 0.7195631265640259]], "normalized_score": [0.5264026522636414], "tokens": [["\u2581Sub", "marine"]], "entropy": [[3.992957830429077, 0.9141711592674255]], "avg_entropy": [2.4535644948482513], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Underwater vehicles", "score_of_biggest_cluster": 0.5264026522636414, "score_of_first_answer": 0.5264026522636414, "max_score_of_first_answer": 0.7195631265640259, "min_score_of_first_answer": 0.38509440422058105, "avg_score": 0.5264026522636414, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1672343611717224, "std_of_first_answer": 0.1672343611717224, "avg_entropy_of_first_answer": 2.4535644948482513, "lex_sim": 1.0}
{"question": "What award is given for a wound in the American forces", "answer": ["Purple Hearst"], "scores": [[0.7357535362243652, 0.9974155426025391, 0.003624115139245987, 0.6425161361694336, 0.00957627221941948]], "normalized_score": [0.11035146564245224], "tokens": [["\u2581Pur", "ple", "\u2581H", "ear", "st"]], "entropy": [[2.0622782707214355, 0.02983950637280941, 0.9420550465583801, 1.9141994714736938, 2.2775650024414062]], "avg_entropy": [1.445187459513545], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Purple Heart", "score_of_biggest_cluster": 0.11035146564245224, "score_of_first_answer": 0.11035146564245224, "max_score_of_first_answer": 0.9974155426025391, "min_score_of_first_answer": 0.003624115139245987, "avg_score": 0.11035146564245224, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40193232537386975, "std_of_first_answer": 0.40193232537386975, "avg_entropy_of_first_answer": 1.445187459513545, "lex_sim": 1.0}
{"question": "The Crossbill belongs to which bird family?", "answer": ["Cardinalidae"], "scores": [[0.025059614330530167, 0.49195238947868347, 0.7592337727546692]], "normalized_score": [0.2107451856136322], "tokens": [["\u2581Card", "inal", "idae"]], "entropy": [[5.3144378662109375, 2.2014522552490234, 1.5239499807357788]], "avg_entropy": [3.0132800340652466], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fringillidae", "score_of_biggest_cluster": 0.2107451856136322, "score_of_first_answer": 0.2107451856136322, "max_score_of_first_answer": 0.7592337727546692, "min_score_of_first_answer": 0.025059614330530167, "avg_score": 0.2107451856136322, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3033955790033975, "std_of_first_answer": 0.3033955790033975, "avg_entropy_of_first_answer": 3.0132800340652466, "lex_sim": 1.0}
{"question": "What do the Americans call what we call a 'sofa'?", "answer": ["What's your name?"], "scores": [[0.0029782166238874197, 0.031174644827842712, 0.9657548069953918, 0.045002736151218414, 0.14666394889354706, 0.7729535102844238]], "normalized_score": [0.08777890354394913], "tokens": [["\u2581What", "'", "s", "\u2581your", "\u2581name", "?"]], "entropy": [[4.906736373901367, 2.8933024406433105, 0.25458207726478577, 3.193056583404541, 4.569005012512207, 1.282745122909546]], "avg_entropy": [2.8499046017726264], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Davenport (disambiguation)", "score_of_biggest_cluster": 0.08777890354394913, "score_of_first_answer": 0.08777890354394913, "max_score_of_first_answer": 0.9657548069953918, "min_score_of_first_answer": 0.0029782166238874197, "avg_score": 0.08777890354394913, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3897493803267854, "std_of_first_answer": 0.3897493803267854, "avg_entropy_of_first_answer": 2.8499046017726264, "lex_sim": 1.0}
{"question": "On the London Underground, which is the only line to connect at some point with every other line on the system?", "answer": ["District"], "scores": [[0.060838498175144196]], "normalized_score": [0.060838501900434494], "tokens": [["\u2581District"]], "entropy": [[4.630764007568359]], "avg_entropy": [4.630764007568359], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jubilee line", "score_of_biggest_cluster": 0.060838501900434494, "score_of_first_answer": 0.060838501900434494, "max_score_of_first_answer": 0.060838498175144196, "min_score_of_first_answer": 0.060838498175144196, "avg_score": 0.060838501900434494, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.630764007568359, "lex_sim": 1.0}
{"question": "What game, created in 1904 by a quaker, Elizabeth (Lizzie) Phillips, when revised by Charles Darrow and others became one of the world's most played commercial board games?", "answer": ["Scrapbook (card game)"], "scores": [[0.013971616514027119, 0.0010194294154644012, 0.07605333626270294, 0.05645306035876274, 0.011168234050273895, 0.9267553091049194, 0.9809451103210449]], "normalized_score": [0.04838545247912407], "tokens": [["\u2581Sc", "rap", "book", "\u2581(", "card", "\u2581game", ")"]], "entropy": [[4.217343330383301, 0.6324936151504517, 4.900092124938965, 2.585815906524658, 2.818681240081787, 0.498288631439209, 0.16913050413131714]], "avg_entropy": [2.2602636218070984], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "List of possible monopolies", "score_of_biggest_cluster": 0.04838545247912407, "score_of_first_answer": 0.04838545247912407, "max_score_of_first_answer": 0.9809451103210449, "min_score_of_first_answer": 0.0010194294154644012, "avg_score": 0.04838545247912407, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4175505926394621, "std_of_first_answer": 0.4175505926394621, "avg_entropy_of_first_answer": 2.2602636218070984, "lex_sim": 1.0}
{"question": "What is the current occupation of rock singer Bruce Dickinson?", "answer": ["Pilot of Iron Maiden during their world tours"], "scores": [[0.10252917557954788, 0.8857120275497437, 0.12414732575416565, 0.11763136088848114, 0.968622088432312, 0.9896893501281738, 0.0007832966512069106, 0.10350387543439865, 0.13541515171527863, 0.7610689997673035, 0.9998522996902466]], "normalized_score": [0.18841083347797394], "tokens": [["\u2581Pil", "ot", "\u2581of", "\u2581Iron", "\u2581Ma", "iden", "\u2581during", "\u2581their", "\u2581world", "\u2581t", "ours"]], "entropy": [[4.893463134765625, 0.47794830799102783, 1.9397480487823486, 3.4591457843780518, 0.24655357003211975, 0.06588691473007202, 1.6229802370071411, 3.6949143409729004, 3.0007357597351074, 0.8420966267585754, 0.0019621788524091244]], "avg_entropy": [1.840494082182307], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Airline Captain", "score_of_biggest_cluster": 0.18841083347797394, "score_of_first_answer": 0.18841083347797394, "max_score_of_first_answer": 0.9998522996902466, "min_score_of_first_answer": 0.0007832966512069106, "avg_score": 0.18841083347797394, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41584445721536817, "std_of_first_answer": 0.41584445721536817, "avg_entropy_of_first_answer": 1.840494082182307, "lex_sim": 1.0}
{"question": "\"Which car company, in 1986, comissioned \"\"Cars\"\", a series of artworks to track the evolution of its designs, from Andy Warhol?\"", "answer": ["BMW"], "scores": [[0.19684259593486786, 0.8525767922401428]], "normalized_score": [0.4096625745296478], "tokens": [["\u2581B", "MW"]], "entropy": [[4.495553493499756, 1.1824804544448853]], "avg_entropy": [2.8390169739723206], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mercedes benz", "score_of_biggest_cluster": 0.4096625745296478, "score_of_first_answer": 0.4096625745296478, "max_score_of_first_answer": 0.8525767922401428, "min_score_of_first_answer": 0.19684259593486786, "avg_score": 0.4096625745296478, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3278670981526375, "std_of_first_answer": 0.3278670981526375, "avg_entropy_of_first_answer": 2.8390169739723206, "lex_sim": 1.0}
{"question": "Who is the current presenter of BBC Radio 4 programme Desert Island Discs?", "answer": ["The Island Desert"], "scores": [[0.021659715101122856, 0.007891197688877583, 0.02529803104698658, 0.903333306312561]], "normalized_score": [0.044456224888563156], "tokens": [["\u2581The", "\u2581Island", "\u2581Des", "ert"]], "entropy": [[5.374600410461426, 6.897355079650879, 3.854726791381836, 0.6468954086303711]], "avg_entropy": [4.193394422531128], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kirsty Young", "score_of_biggest_cluster": 0.044456224888563156, "score_of_first_answer": 0.044456224888563156, "max_score_of_first_answer": 0.903333306312561, "min_score_of_first_answer": 0.007891197688877583, "avg_score": 0.044456224888563156, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38329302133036924, "std_of_first_answer": 0.38329302133036924, "avg_entropy_of_first_answer": 4.193394422531128, "lex_sim": 1.0}
{"question": "In which year was the storming of the Bastille, which started the French Revolution?", "answer": ["BSE 1856"], "scores": [[0.004230003338307142, 0.0030437735840678215, 0.03886649012565613, 0.3823402523994446, 0.05632627755403519, 0.0584857240319252, 0.0812460333108902]], "normalized_score": [0.03387707844376564], "tokens": [["\u2581B", "SE", "\u2581", "1", "8", "5", "6"]], "entropy": [[3.565669059753418, 4.636702537536621, 3.012219190597534, 2.385063409805298, 2.3818814754486084, 2.925718307495117, 2.7015862464904785]], "avg_entropy": [3.086977175303868], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1789", "score_of_biggest_cluster": 0.03387707844376564, "score_of_first_answer": 0.03387707844376564, "max_score_of_first_answer": 0.3823402523994446, "min_score_of_first_answer": 0.0030437735840678215, "avg_score": 0.03387707844376564, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12258803058287701, "std_of_first_answer": 0.12258803058287701, "avg_entropy_of_first_answer": 3.086977175303868, "lex_sim": 1.0}
{"question": "What comes next in the sequence kilo, mega, giga?", "answer": ["Frog"], "scores": [[0.0025071410927921534, 0.006870996206998825]], "normalized_score": [0.00415048748254776], "tokens": [["\u2581F", "rog"]], "entropy": [[5.291913986206055, 6.008570671081543]], "avg_entropy": [5.650242328643799], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tera (disambiguation)", "score_of_biggest_cluster": 0.00415048748254776, "score_of_first_answer": 0.00415048748254776, "max_score_of_first_answer": 0.006870996206998825, "min_score_of_first_answer": 0.0025071410927921534, "avg_score": 0.00415048748254776, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.002181927557103336, "std_of_first_answer": 0.002181927557103336, "avg_entropy_of_first_answer": 5.650242328643799, "lex_sim": 1.0}
{"question": "Which book begins 'When he was nearly thirteen my brother Jem got his arm badly broken'?", "answer": ["The Middlesex Money Tramp (novel)"], "scores": [[0.03427653759717941, 0.0038312135729938745, 0.002898073522374034, 0.43003401160240173, 0.5454872846603394, 0.002618630649521947, 0.003237989032641053, 0.011318407952785492, 0.004693584516644478, 0.11260125786066055, 0.6566916704177856, 0.9852012991905212, 0.9713423848152161]], "normalized_score": [0.04470137879252434], "tokens": [["\u2581The", "\u2581M", "idd", "les", "ex", "\u2581M", "oney", "\u2581Tr", "amp", "\u2581(", "nov", "el", ")"]], "entropy": [[3.8043527603149414, 5.954094886779785, 4.2722392082214355, 2.0820045471191406, 2.3388891220092773, 4.048473358154297, 4.3632025718688965, 4.581289291381836, 2.061007499694824, 1.882075548171997, 2.2259178161621094, 0.08797745406627655, 0.21593868732452393]], "avg_entropy": [2.916727903943795], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "To Assassinate a Mockingbird", "score_of_biggest_cluster": 0.04470137879252434, "score_of_first_answer": 0.04470137879252434, "max_score_of_first_answer": 0.9852012991905212, "min_score_of_first_answer": 0.002618630649521947, "avg_score": 0.04470137879252434, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36710346134480704, "std_of_first_answer": 0.36710346134480704, "avg_entropy_of_first_answer": 2.916727903943795, "lex_sim": 1.0}
{"question": "Which now obsolete unit originated as the distance a person could walk in an hour?", "answer": ["Foot-pound-second system"], "scores": [[0.006343277636915445, 0.03696523979306221, 0.39371877908706665, 0.9653908014297485, 0.1745057851076126, 0.7996307015419006, 0.6266763806343079]], "normalized_score": [0.18631526827812195], "tokens": [["\u2581Foot", "-", "p", "ound", "-", "second", "\u2581system"]], "entropy": [[5.13340950012207, 2.0428175926208496, 2.5407016277313232, 0.16523873805999756, 1.7291851043701172, 0.7814006805419922, 1.1608833074569702]], "avg_entropy": [1.9362337929861886], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "League", "score_of_biggest_cluster": 0.18631526827812195, "score_of_first_answer": 0.18631526827812195, "max_score_of_first_answer": 0.9653908014297485, "min_score_of_first_answer": 0.006343277636915445, "avg_score": 0.18631526827812195, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3510189617906052, "std_of_first_answer": 0.3510189617906052, "avg_entropy_of_first_answer": 1.9362337929861886, "lex_sim": 1.0}
{"question": "What is potamology the study of?", "answer": ["Waterways"], "scores": [[0.019723521545529366, 0.23398244380950928]], "normalized_score": [0.06793349236249924], "tokens": [["\u2581Water", "ways"]], "entropy": [[4.841190814971924, 3.7389626502990723]], "avg_entropy": [4.290076732635498], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Riveriness", "score_of_biggest_cluster": 0.06793349236249924, "score_of_first_answer": 0.06793349236249924, "max_score_of_first_answer": 0.23398244380950928, "min_score_of_first_answer": 0.019723521545529366, "avg_score": 0.06793349236249924, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10712946113198996, "std_of_first_answer": 0.10712946113198996, "avg_entropy_of_first_answer": 4.290076732635498, "lex_sim": 1.0}
{"question": "Who was the first Hanover King of England?", "answer": ["The Sun never sets on the British Empire"], "scores": [[0.031272631138563156, 0.010102812200784683, 0.01618095487356186, 0.7448770999908447, 0.6803674697875977, 0.813805341720581, 0.8252027630805969, 0.9358841776847839]], "normalized_score": [0.1890026181936264], "tokens": [["\u2581The", "\u2581Sun", "\u2581never", "\u2581sets", "\u2581on", "\u2581the", "\u2581British", "\u2581Empire"]], "entropy": [[4.0039286613464355, 5.807742118835449, 1.8100464344024658, 1.2479190826416016, 1.1324102878570557, 1.3047869205474854, 1.2131965160369873, 0.4800095856189728]], "avg_entropy": [2.1250049509108067], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George the First", "score_of_biggest_cluster": 0.1890026181936264, "score_of_first_answer": 0.1890026181936264, "max_score_of_first_answer": 0.9358841776847839, "min_score_of_first_answer": 0.010102812200784683, "avg_score": 0.1890026181936264, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3840723027125957, "std_of_first_answer": 0.3840723027125957, "avg_entropy_of_first_answer": 2.1250049509108067, "lex_sim": 1.0}
{"question": "Which car manufacturer makes the Jazz model?", "answer": ["Fit & Vezel"], "scores": [[0.007820588536560535, 0.048960763961076736, 0.0017960895784199238, 0.007837454788386822, 0.5464385747909546, 0.8716841340065002]], "normalized_score": [0.03700399771332741], "tokens": [["\u2581F", "it", "\u2581&", "\u2581V", "ez", "el"]], "entropy": [[4.687995910644531, 4.910140037536621, 2.738050699234009, 5.212490081787109, 2.880186080932617, 0.8580505847930908]], "avg_entropy": [3.547818899154663], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Honda Technologies", "score_of_biggest_cluster": 0.03700399771332741, "score_of_first_answer": 0.03700399771332741, "max_score_of_first_answer": 0.8716841340065002, "min_score_of_first_answer": 0.0017960895784199238, "avg_score": 0.03700399771332741, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34001045687568743, "std_of_first_answer": 0.34001045687568743, "avg_entropy_of_first_answer": 3.547818899154663, "lex_sim": 1.0}
{"question": "Which country since the late 1900s has governed/overseen the increasing independence of the New Caledonia archipelago, east of Australia?", "answer": ["France (nation)"], "scores": [[0.24864378571510315, 0.03707391396164894, 0.017573542892932892, 0.9040743112564087, 0.8721498250961304]], "normalized_score": [0.1664409190416336], "tokens": [["\u2581France", "\u2581(", "n", "ation", ")"]], "entropy": [[4.620989799499512, 1.2700436115264893, 4.486684799194336, 0.5059723854064941, 0.7134053707122803]], "avg_entropy": [2.319419193267822], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "La Republique francaise", "score_of_biggest_cluster": 0.1664409190416336, "score_of_first_answer": 0.1664409190416336, "max_score_of_first_answer": 0.9040743112564087, "min_score_of_first_answer": 0.017573542892932892, "avg_score": 0.1664409190416336, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.39411314926937824, "std_of_first_answer": 0.39411314926937824, "avg_entropy_of_first_answer": 2.319419193267822, "lex_sim": 1.0}
{"question": "What is the name of the island that separates the two largest waterfalls at Niagara ?", "answer": ["Goat Island"], "scores": [[0.4006025493144989, 0.9647652506828308, 0.7516704201698303]], "normalized_score": [0.6622992753982544], "tokens": [["\u2581Go", "at", "\u2581Island"]], "entropy": [[4.0569376945495605, 0.2596343159675598, 1.2722076177597046]], "avg_entropy": [1.8629265427589417], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "GOAT ISLAND", "score_of_biggest_cluster": 0.6622992753982544, "score_of_first_answer": 0.6622992753982544, "max_score_of_first_answer": 0.9647652506828308, "min_score_of_first_answer": 0.4006025493144989, "avg_score": 0.6622992753982544, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23260305012652727, "std_of_first_answer": 0.23260305012652727, "avg_entropy_of_first_answer": 1.8629265427589417, "lex_sim": 1.0}
{"question": "The Great Sphinx of Giza in Egypt is a statue of a reclining mythical creature with a lion's body and the head of a what?", "answer": ["Falcon"], "scores": [[0.005188969429582357, 0.9028181433677673]], "normalized_score": [0.06844483315944672], "tokens": [["\u2581Fal", "con"]], "entropy": [[5.815975666046143, 0.6975398063659668]], "avg_entropy": [3.2567577362060547], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Human species", "score_of_biggest_cluster": 0.06844483315944672, "score_of_first_answer": 0.06844483315944672, "max_score_of_first_answer": 0.9028181433677673, "min_score_of_first_answer": 0.005188969429582357, "avg_score": 0.06844483315944672, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4488145869690925, "std_of_first_answer": 0.4488145869690925, "avg_entropy_of_first_answer": 3.2567577362060547, "lex_sim": 1.0}
{"question": "The TV characters Jack Ford, Trevor Chaplin and Jack Halford link which actor?", "answer": ["Paul Nichols"], "scores": [[0.0451325923204422, 0.14006386697292328, 0.07116898894309998]], "normalized_score": [0.07662474364042282], "tokens": [["\u2581Paul", "\u2581Nich", "ols"]], "entropy": [[5.706585884094238, 4.673009872436523, 0.37874066829681396]], "avg_entropy": [3.586112141609192], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "James Christopher Bolam", "score_of_biggest_cluster": 0.07662474364042282, "score_of_first_answer": 0.07662474364042282, "max_score_of_first_answer": 0.14006386697292328, "min_score_of_first_answer": 0.0451325923204422, "avg_score": 0.07662474364042282, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04005044745898099, "std_of_first_answer": 0.04005044745898099, "avg_entropy_of_first_answer": 3.586112141609192, "lex_sim": 1.0}
{"question": "Which trade is historically associated with Flamenco music and dance?", "answer": ["Cigar maker"], "scores": [[0.0199743639677763, 0.31513452529907227, 0.3250570595264435, 0.2020552009344101, 0.4365980923175812]], "normalized_score": [0.17835943400859833], "tokens": [["\u2581C", "ig", "ar", "\u2581mak", "er"]], "entropy": [[5.368194580078125, 3.8086814880371094, 1.071617841720581, 3.4016342163085938, 0.7116854190826416]], "avg_entropy": [2.8723627090454102], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Blacksmith shop", "score_of_biggest_cluster": 0.17835943400859833, "score_of_first_answer": 0.17835943400859833, "max_score_of_first_answer": 0.4365980923175812, "min_score_of_first_answer": 0.0199743639677763, "avg_score": 0.17835943400859833, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14101684292164943, "std_of_first_answer": 0.14101684292164943, "avg_entropy_of_first_answer": 2.8723627090454102, "lex_sim": 1.0}
{"question": "Juba is the capital of which country?", "answer": ["South Sudan"], "scores": [[0.2936251163482666, 0.9304443597793579, 0.9953007698059082]], "normalized_score": [0.6478572487831116], "tokens": [["\u2581South", "\u2581Sud", "an"]], "entropy": [[4.7705488204956055, 0.5911468267440796, 0.051505718380212784]], "avg_entropy": [1.8044004552066326], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Al-Istiw\u0101\u02be\u012byah", "score_of_biggest_cluster": 0.6478572487831116, "score_of_first_answer": 0.6478572487831116, "max_score_of_first_answer": 0.9953007698059082, "min_score_of_first_answer": 0.2936251163482666, "avg_score": 0.6478572487831116, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31659539994217084, "std_of_first_answer": 0.31659539994217084, "avg_entropy_of_first_answer": 1.8044004552066326, "lex_sim": 1.0}
{"question": "Fleurie, Morgon and Brouilly are wines from which French wine region?", "answer": ["Bouchard Pere & Fils"], "scores": [[0.019065922126173973, 0.06783755123615265, 0.12989886105060577, 0.22825342416763306, 0.24157583713531494, 0.9860959649085999, 0.9945291876792908]], "normalized_score": [0.19044211506843567], "tokens": [["\u2581B", "ouch", "ard", "\u2581Pere", "\u2581&", "\u2581Fil", "s"]], "entropy": [[4.410663604736328, 5.774516582489014, 3.2650203704833984, 3.144599437713623, 1.6171252727508545, 0.13434574007987976, 0.05030862241983414]], "avg_entropy": [2.6280828043818474], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Julienas wine", "score_of_biggest_cluster": 0.19044211506843567, "score_of_first_answer": 0.19044211506843567, "max_score_of_first_answer": 0.9945291876792908, "min_score_of_first_answer": 0.019065922126173973, "avg_score": 0.19044211506843567, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3923279555803212, "std_of_first_answer": 0.3923279555803212, "avg_entropy_of_first_answer": 2.6280828043818474, "lex_sim": 1.0}
{"question": "Gordon Ramsay opened a restaurant in 2001 in which London hotel?", "answer": ["Ritz"], "scores": [[0.032283347100019455, 0.10895731300115585]], "normalized_score": [0.05930858477950096], "tokens": [["\u2581R", "itz"]], "entropy": [[4.347382545471191, 2.335536003112793]], "avg_entropy": [3.341459274291992], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Claridges Hotel", "score_of_biggest_cluster": 0.05930858477950096, "score_of_first_answer": 0.05930858477950096, "max_score_of_first_answer": 0.10895731300115585, "min_score_of_first_answer": 0.032283347100019455, "avg_score": 0.05930858477950096, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0383369829505682, "std_of_first_answer": 0.0383369829505682, "avg_entropy_of_first_answer": 3.341459274291992, "lex_sim": 1.0}
{"question": "For what novel did J. K. Rowling win the 1999 Whitbread Children's book of the year award?", "answer": ["Chamber of Secrets"], "scores": [[0.006502089556306601, 0.954811155796051, 0.8661045432090759, 0.9950783252716064]], "normalized_score": [0.27045780420303345], "tokens": [["\u2581Chamber", "\u2581of", "\u2581Sec", "rets"]], "entropy": [[2.898406982421875, 0.3710649013519287, 0.6623340845108032, 0.03938727825880051]], "avg_entropy": [0.9927983116358519], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "HPPOA", "score_of_biggest_cluster": 0.27045780420303345, "score_of_first_answer": 0.27045780420303345, "max_score_of_first_answer": 0.9950783252716064, "min_score_of_first_answer": 0.006502089556306601, "avg_score": 0.27045780420303345, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4063260683992853, "std_of_first_answer": 0.4063260683992853, "avg_entropy_of_first_answer": 0.9927983116358519, "lex_sim": 1.0}
{"question": "Which actress Dame was made a Companion of Honour in the 2014 Queen\u2019s Birthday Honours List?", "answer": ["She who entangles men"], "scores": [[0.005115334875881672, 0.49510398507118225, 0.03684690222144127, 0.8411306142807007, 0.9163050651550293]], "normalized_score": [0.1483798623085022], "tokens": [["\u2581She", "\u2581who", "\u2581ent", "angles", "\u2581men"]], "entropy": [[5.110605239868164, 3.042360544204712, 5.613793849945068, 0.8315713405609131, 0.641266942024231]], "avg_entropy": [3.047919583320618], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dame Maggie Smith", "score_of_biggest_cluster": 0.1483798623085022, "score_of_first_answer": 0.1483798623085022, "max_score_of_first_answer": 0.9163050651550293, "min_score_of_first_answer": 0.005115334875881672, "avg_score": 0.1483798623085022, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3848843214015025, "std_of_first_answer": 0.3848843214015025, "avg_entropy_of_first_answer": 3.047919583320618, "lex_sim": 1.0}
{"question": "In which Verdi opera do the gypsies sing the 'Anvil Chorus'?", "answer": ["Il Trovatore"], "scores": [[0.5385038256645203, 0.7194614410400391, 0.9896134734153748, 0.9710268974304199]], "normalized_score": [0.7811299562454224], "tokens": [["\u2581Il", "\u2581Tro", "v", "atore"]], "entropy": [[2.906475067138672, 1.0124478340148926, 0.1002855896949768, 0.20210078358650208]], "avg_entropy": [1.0553273186087608], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dei miei bollenti spiriti", "score_of_biggest_cluster": 0.7811299562454224, "score_of_first_answer": 0.7811299562454224, "max_score_of_first_answer": 0.9896134734153748, "min_score_of_first_answer": 0.5385038256645203, "avg_score": 0.7811299562454224, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18707193481358536, "std_of_first_answer": 0.18707193481358536, "avg_entropy_of_first_answer": 1.0553273186087608, "lex_sim": 1.0}
{"question": "Which Nobel Prize did Patrick White win in 1973?", "answer": ["Nobel prize in Literature"], "scores": [[0.28772589564323425, 0.033091459423303604, 0.47852978110313416, 0.24350357055664062, 0.9948568344116211]], "normalized_score": [0.2561969459056854], "tokens": [["\u2581Nobel", "\u2581prize", "\u2581in", "\u2581Liter", "ature"]], "entropy": [[3.845945358276367, 1.0212006568908691, 1.7032475471496582, 1.2275912761688232, 0.04925215244293213]], "avg_entropy": [1.56944739818573], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ltierature", "score_of_biggest_cluster": 0.2561969459056854, "score_of_first_answer": 0.2561969459056854, "max_score_of_first_answer": 0.9948568344116211, "min_score_of_first_answer": 0.033091459423303604, "avg_score": 0.2561969459056854, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3260232176763927, "std_of_first_answer": 0.3260232176763927, "avg_entropy_of_first_answer": 1.56944739818573, "lex_sim": 1.0}
{"question": "Elvis Presley sang a few lines in German on which US hit song?", "answer": ["\"Wooden Heart\""], "scores": [[0.012276722118258476, 0.6020002365112305, 0.8364949226379395, 0.9711158275604248, 0.8856160044670105, 0.7971487045288086]], "normalized_score": [0.4022842347621918], "tokens": [["\u2581\"", "W", "ood", "en", "\u2581Heart", "\""]], "entropy": [[5.266274929046631, 2.5886566638946533, 0.8582018613815308, 0.31330591440200806, 0.5229282975196838, 0.8274378776550293]], "avg_entropy": [1.7294675906499226], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Wooden Heart", "score_of_biggest_cluster": 0.4022842347621918, "score_of_first_answer": 0.4022842347621918, "max_score_of_first_answer": 0.9711158275604248, "min_score_of_first_answer": 0.012276722118258476, "avg_score": 0.4022842347621918, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.32069661020419044, "std_of_first_answer": 0.32069661020419044, "avg_entropy_of_first_answer": 1.7294675906499226, "lex_sim": 1.0}
{"question": "Which football manager as the nickname the bald eagle?", "answer": ["Alan Pas"], "scores": [[0.01776367612183094, 0.0010899066692218184]], "normalized_score": [0.004400085657835007], "tokens": [["\u2581Alan", "\u2581Pas"]], "entropy": [[5.291217803955078, 1.7279893159866333]], "avg_entropy": [3.5096035599708557], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jim Smith", "score_of_biggest_cluster": 0.004400085657835007, "score_of_first_answer": 0.004400085657835007, "max_score_of_first_answer": 0.01776367612183094, "min_score_of_first_answer": 0.0010899066692218184, "avg_score": 0.004400085657835007, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.008336884726304561, "std_of_first_answer": 0.008336884726304561, "avg_entropy_of_first_answer": 3.5096035599708557, "lex_sim": 1.0}
{"question": "Badminton debuted at the 1992 Summer Olympics. How many different people have won gold medals for the men's singles event in its first 5 Olympiads to 2008?", "answer": ["3 - Taufik Hidayat in 2004, Lin Dan in 2008 and Lee Chong Wei in 2012"], "scores": [[0.3418695330619812, 0.17774051427841187, 0.010857345536351204, 0.02248833328485489, 0.576382040977478, 0.8950690031051636, 0.8429197072982788, 0.9872927665710449, 0.985672116279602, 0.01075759157538414, 0.9184978604316711, 0.7244164347648621, 0.9993127584457397, 0.999382734298706, 0.6303197741508484, 0.6763643622398376, 0.8016253113746643, 0.9664930701255798, 0.9718725681304932, 0.987297534942627, 0.9994844198226929, 0.9999096393585205, 0.9993627667427063, 0.9922898411750793, 0.5243135690689087, 0.5078554749488831, 0.987429678440094, 0.9941306710243225, 0.9898697733879089, 0.9095308184623718, 0.9768760800361633, 0.9995608925819397, 0.9997870326042175, 0.7460059523582458, 0.9932548403739929]], "normalized_score": [0.5671780109405518], "tokens": [["\u2581", "3", "\u2581-", "\u2581T", "auf", "ik", "\u2581H", "iday", "at", "\u2581in", "\u2581", "2", "0", "0", "4", ",", "\u2581Lin", "\u2581Dan", "\u2581in", "\u2581", "2", "0", "0", "8", "\u2581and", "\u2581Lee", "\u2581Ch", "ong", "\u2581Wei", "\u2581in", "\u2581", "2", "0", "1", "2"]], "entropy": [[4.265350341796875, 2.5168204307556152, 3.401881217956543, 4.673508644104004, 2.4124083518981934, 0.5152978301048279, 0.7729334831237793, 0.09388530254364014, 0.11910302191972733, 1.5743930339813232, 0.47795945405960083, 0.5955018997192383, 0.008757034316658974, 0.006107780616730452, 0.7570443153381348, 1.2816755771636963, 1.1312875747680664, 0.26503419876098633, 0.21205297112464905, 0.08869646489620209, 0.004855690523982048, 0.0013761688023805618, 0.005634498316794634, 0.05713176727294922, 0.9624843597412109, 2.253220558166504, 0.11129181832075119, 0.04988294094800949, 0.07270129024982452, 0.5888530015945435, 0.15515384078025818, 0.004456658381968737, 0.002924933098256588, 0.5686609148979187, 0.050675369799137115]], "avg_entropy": [0.8588286497097994], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "five", "score_of_biggest_cluster": 0.5671780109405518, "score_of_first_answer": 0.5671780109405518, "max_score_of_first_answer": 0.9999096393585205, "min_score_of_first_answer": 0.01075759157538414, "avg_score": 0.5671780109405518, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3108750211010117, "std_of_first_answer": 0.3108750211010117, "avg_entropy_of_first_answer": 0.8588286497097994, "lex_sim": 1.0}
{"question": "What was the name of the second wife of Johnny Cash, who he married in 1968?", "answer": ["Vivian Liberto Cash"], "scores": [[0.010112195275723934, 0.7665278315544128, 0.3494284152984619, 0.9774941205978394, 0.059196215122938156, 0.9975372552871704]], "normalized_score": [0.23210155963897705], "tokens": [["\u2581Viv", "ian", "\u2581Li", "berto", "\u2581C", "ash"]], "entropy": [[5.660305976867676, 1.4695332050323486, 3.5769143104553223, 0.19284547865390778, 0.8407787680625916, 0.02665858343243599]], "avg_entropy": [1.9611727204173803], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "June Carter Cash", "score_of_biggest_cluster": 0.23210155963897705, "score_of_first_answer": 0.23210155963897705, "max_score_of_first_answer": 0.9975372552871704, "min_score_of_first_answer": 0.010112195275723934, "avg_score": 0.23210155963897705, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40809892815953874, "std_of_first_answer": 0.40809892815953874, "avg_entropy_of_first_answer": 1.9611727204173803, "lex_sim": 1.0}
{"question": "Breve, Mocha and Americano are all types of which drink?", "answer": ["Coffea"], "scores": [[0.38671135902404785, 0.8246501684188843, 0.006652236450463533]], "normalized_score": [0.1284916251897812], "tokens": [["\u2581C", "off", "ea"]], "entropy": [[4.090987205505371, 0.8179675340652466, 0.3664473295211792]], "avg_entropy": [1.7584673563639324], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Kawha", "score_of_biggest_cluster": 0.1284916251897812, "score_of_first_answer": 0.1284916251897812, "max_score_of_first_answer": 0.8246501684188843, "min_score_of_first_answer": 0.006652236450463533, "avg_score": 0.1284916251897812, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3342248002850415, "std_of_first_answer": 0.3342248002850415, "avg_entropy_of_first_answer": 1.7584673563639324, "lex_sim": 1.0}
{"question": "What is the oldest vegetable known to man", "answer": ["Lettuce"], "scores": [[0.01147222239524126, 0.4714445471763611, 0.9538567066192627]], "normalized_score": [0.1727907359600067], "tokens": [["\u2581L", "ett", "uce"]], "entropy": [[5.988783836364746, 2.9100170135498047, 0.3193308115005493]], "avg_entropy": [3.072710553805033], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pisum sativum", "score_of_biggest_cluster": 0.1727907359600067, "score_of_first_answer": 0.1727907359600067, "max_score_of_first_answer": 0.9538567066192627, "min_score_of_first_answer": 0.01147222239524126, "avg_score": 0.1727907359600067, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38476320961355015, "std_of_first_answer": 0.38476320961355015, "avg_entropy_of_first_answer": 3.072710553805033, "lex_sim": 1.0}
{"question": "Who was the youngest person to become president of the USA?", "answer": ["Theodore Roosevelt"], "scores": [[0.19454559683799744, 0.9854340553283691, 0.7414944171905518, 0.9896670579910278, 0.9932138919830322, 0.9960907101631165]], "normalized_score": [0.7198874354362488], "tokens": [["\u2581The", "odore", "\u2581Ro", "ose", "vel", "t"]], "entropy": [[4.39787483215332, 0.15525871515274048, 1.8302184343338013, 0.08207278698682785, 0.06933602690696716, 0.03784424811601639]], "avg_entropy": [1.0954341739416122], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "T Ros", "score_of_biggest_cluster": 0.7198874354362488, "score_of_first_answer": 0.7198874354362488, "max_score_of_first_answer": 0.9960907101631165, "min_score_of_first_answer": 0.19454559683799744, "avg_score": 0.7198874354362488, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.29281932328348803, "std_of_first_answer": 0.29281932328348803, "avg_entropy_of_first_answer": 1.0954341739416122, "lex_sim": 1.0}
{"question": "In the Bible, which bird was the first to leave the Ark?", "answer": ["First bird from the Ark"], "scores": [[0.0034098573960363865, 0.1502702832221985, 0.010870366357266903, 0.5984957814216614, 0.5530646443367004]], "normalized_score": [0.0713079571723938], "tokens": [["\u2581First", "\u2581bird", "\u2581from", "\u2581the", "\u2581Ark"]], "entropy": [[5.029293537139893, 5.099984169006348, 2.5162084102630615, 1.7450757026672363, 1.6558377742767334]], "avg_entropy": [3.209279918670654], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Corbies", "score_of_biggest_cluster": 0.0713079571723938, "score_of_first_answer": 0.0713079571723938, "max_score_of_first_answer": 0.5984957814216614, "min_score_of_first_answer": 0.0034098573960363865, "avg_score": 0.0713079571723938, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2609057507048736, "std_of_first_answer": 0.2609057507048736, "avg_entropy_of_first_answer": 3.209279918670654, "lex_sim": 1.0}
{"question": "Papua is a province of which country?", "answer": ["Indonesia"], "scores": [[0.336628794670105, 0.9359787106513977]], "normalized_score": [0.5613175630569458], "tokens": [["\u2581Indones", "ia"]], "entropy": [[4.170715808868408, 0.29699254035949707]], "avg_entropy": [2.2338541746139526], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Unitary state of republic of indonesia", "score_of_biggest_cluster": 0.5613175630569458, "score_of_first_answer": 0.5613175630569458, "max_score_of_first_answer": 0.9359787106513977, "min_score_of_first_answer": 0.336628794670105, "avg_score": 0.5613175630569458, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.29967495799064636, "std_of_first_answer": 0.29967495799064636, "avg_entropy_of_first_answer": 2.2338541746139526, "lex_sim": 1.0}
{"question": "'90210' is a spinoff from a TV series set in which city in Los Angeles County California?", "answer": ["Beverly Hills California"], "scores": [[0.22844834625720978, 0.9535284042358398, 0.9092692732810974, 0.8761156797409058, 0.024169351905584335]], "normalized_score": [0.33460167050361633], "tokens": [["\u2581B", "ever", "ly", "\u2581Hills", "\u2581California"]], "entropy": [[5.151183128356934, 0.43914371728897095, 0.3669204115867615, 0.8595449924468994, 1.5353035926818848]], "avg_entropy": [1.67041916847229], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Beverly Hills, California, USA", "score_of_biggest_cluster": 0.33460167050361633, "score_of_first_answer": 0.33460167050361633, "max_score_of_first_answer": 0.9535284042358398, "min_score_of_first_answer": 0.024169351905584335, "avg_score": 0.33460167050361633, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3915320948978562, "std_of_first_answer": 0.3915320948978562, "avg_entropy_of_first_answer": 1.67041916847229, "lex_sim": 1.0}
{"question": "What is the name of the London Underground station in EastEnders?", "answer": ["Walford"], "scores": [[0.26860785484313965, 0.9718471765518188, 0.9228023290634155]], "normalized_score": [0.6222168803215027], "tokens": [["\u2581W", "alf", "ord"]], "entropy": [[4.8576979637146, 0.2815502882003784, 0.679892897605896]], "avg_entropy": [1.939713716506958], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Walford East tube station", "score_of_biggest_cluster": 0.6222168803215027, "score_of_first_answer": 0.6222168803215027, "max_score_of_first_answer": 0.9718471765518188, "min_score_of_first_answer": 0.26860785484313965, "avg_score": 0.6222168803215027, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3205761048076463, "std_of_first_answer": 0.3205761048076463, "avg_entropy_of_first_answer": 1.939713716506958, "lex_sim": 1.0}
{"question": "Name either of the detectives featured in novels by Reginald Hill and in a TV series based on the novels?", "answer": ["Dalziel, Pascoe"], "scores": [[0.284758597612381, 0.890221118927002, 0.01611281931400299, 0.6644046902656555, 0.965649425983429, 0.9531182050704956]], "normalized_score": [0.36834731698036194], "tokens": [["\u2581Dal", "ziel", ",", "\u2581Pas", "co", "e"]], "entropy": [[4.545805931091309, 0.7145719528198242, 1.6989901065826416, 2.277344226837158, 0.31969720125198364, 0.2566617727279663]], "avg_entropy": [1.6355118652184804], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Dalziel & Pascoe", "score_of_biggest_cluster": 0.36834731698036194, "score_of_first_answer": 0.36834731698036194, "max_score_of_first_answer": 0.965649425983429, "min_score_of_first_answer": 0.01611281931400299, "avg_score": 0.36834731698036194, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.36101898371480984, "std_of_first_answer": 0.36101898371480984, "avg_entropy_of_first_answer": 1.6355118652184804, "lex_sim": 1.0}
{"question": "What word describes making beautiful with ornaments, or to decorate, etc?", "answer": ["Glorify"], "scores": [[0.011872253380715847, 0.07579672336578369, 0.5323558449745178]], "normalized_score": [0.07824596017599106], "tokens": [["\u2581G", "lor", "ify"]], "entropy": [[4.767651557922363, 3.625156879425049, 1.8679205179214478]], "avg_entropy": [3.42024298508962], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Embellishment", "score_of_biggest_cluster": 0.07824596017599106, "score_of_first_answer": 0.07824596017599106, "max_score_of_first_answer": 0.5323558449745178, "min_score_of_first_answer": 0.011872253380715847, "avg_score": 0.07824596017599106, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23176514417865973, "std_of_first_answer": 0.23176514417865973, "avg_entropy_of_first_answer": 3.42024298508962, "lex_sim": 1.0}
{"question": "Washington Irving's classic story Rip Van Winkle is set in which geographic region of New York state?", "answer": ["The Catskills"], "scores": [[0.11220327764749527, 0.6152507066726685, 0.9941552877426147, 0.6074650287628174]], "normalized_score": [0.4518646001815796], "tokens": [["\u2581The", "\u2581Cat", "sk", "ills"]], "entropy": [[3.9459593296051025, 2.2830727100372314, 0.06802833080291748, 0.7270232439041138]], "avg_entropy": [1.7560209035873413], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Catskill Mountains, New York", "score_of_biggest_cluster": 0.4518646001815796, "score_of_first_answer": 0.4518646001815796, "max_score_of_first_answer": 0.9941552877426147, "min_score_of_first_answer": 0.11220327764749527, "avg_score": 0.4518646001815796, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.31318314538018766, "std_of_first_answer": 0.31318314538018766, "avg_entropy_of_first_answer": 1.7560209035873413, "lex_sim": 1.0}
{"question": "Which is the main sport played at the Basin Reserve in Wellington, New Zealand?", "answer": ["New Zealand Cricket (sport)"], "scores": [[0.0510527566075325, 0.9460277557373047, 0.23658105731010437, 0.987090528011322, 0.023018693551421165, 0.289887934923172, 0.9096063375473022, 0.9369040727615356]], "normalized_score": [0.29915061593055725], "tokens": [["\u2581New", "\u2581Zealand", "\u2581Cr", "icket", "\u2581(", "s", "port", ")"]], "entropy": [[4.045897006988525, 0.5020108222961426, 3.109744071960449, 0.08125584572553635, 2.240851402282715, 3.517364025115967, 0.3357394337654114, 0.44202548265457153]], "avg_entropy": [1.7843610113486648], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Cricket games", "score_of_biggest_cluster": 0.29915061593055725, "score_of_first_answer": 0.29915061593055725, "max_score_of_first_answer": 0.987090528011322, "min_score_of_first_answer": 0.023018693551421165, "avg_score": 0.29915061593055725, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4061089453108802, "std_of_first_answer": 0.4061089453108802, "avg_entropy_of_first_answer": 1.7843610113486648, "lex_sim": 1.0}
{"question": "Which French vessel was the first ocean-going liner to exceed 1,000 feet in length ?", "answer": ["QE3"], "scores": [[0.005986684933304787, 0.4951620101928711, 0.15005582571029663]], "normalized_score": [0.07633590698242188], "tokens": [["\u2581Q", "E", "3"]], "entropy": [[5.787337779998779, 1.9109299182891846, 1.9270093441009521]], "avg_entropy": [3.208425680796305], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Normandie (disambiguation)", "score_of_biggest_cluster": 0.07633590698242188, "score_of_first_answer": 0.07633590698242188, "max_score_of_first_answer": 0.4951620101928711, "min_score_of_first_answer": 0.005986684933304787, "avg_score": 0.07633590698242188, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.205249629783167, "std_of_first_answer": 0.205249629783167, "avg_entropy_of_first_answer": 3.208425680796305, "lex_sim": 1.0}
{"question": "Which sea lies between Italy and the former Yugoslavia?", "answer": ["Adriatic Sea"], "scores": [[0.24676768481731415, 0.9626626968383789, 0.3725827634334564]], "normalized_score": [0.44565117359161377], "tokens": [["\u2581Adri", "atic", "\u2581Sea"]], "entropy": [[4.843082427978516, 0.2699222266674042, 1.4944157600402832]], "avg_entropy": [2.2024734715620675], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Adriatic sea", "score_of_biggest_cluster": 0.44565117359161377, "score_of_first_answer": 0.44565117359161377, "max_score_of_first_answer": 0.9626626968383789, "min_score_of_first_answer": 0.24676768481731415, "avg_score": 0.44565117359161377, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3120771681724791, "std_of_first_answer": 0.3120771681724791, "avg_entropy_of_first_answer": 2.2024734715620675, "lex_sim": 1.0}
{"question": "What kind of song is a Brindisi?", "answer": ["Synchronized"], "scores": [[0.00572397094219923, 0.004081625025719404, 0.2774446904659271]], "normalized_score": [0.018645280972123146], "tokens": [["\u2581S", "ynchron", "ized"]], "entropy": [[4.737787246704102, 3.9596376419067383, 2.4664411544799805]], "avg_entropy": [3.7212886810302734], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Drink, Drink, Drink", "score_of_biggest_cluster": 0.018645280972123146, "score_of_first_answer": 0.018645280972123146, "max_score_of_first_answer": 0.2774446904659271, "min_score_of_first_answer": 0.004081625025719404, "avg_score": 0.018645280972123146, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12847922973749884, "std_of_first_answer": 0.12847922973749884, "avg_entropy_of_first_answer": 3.7212886810302734, "lex_sim": 1.0}
{"question": "In humans, Gorham disease affects which part of the body?", "answer": ["The bones"], "scores": [[0.0652943029999733, 0.32329508662223816, 0.524910569190979]], "normalized_score": [0.22293925285339355], "tokens": [["\u2581The", "\u2581b", "ones"]], "entropy": [[4.838164329528809, 4.350092887878418, 0.7944235801696777]], "avg_entropy": [3.3275602658589682], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bone lining cell", "score_of_biggest_cluster": 0.22293925285339355, "score_of_first_answer": 0.22293925285339355, "max_score_of_first_answer": 0.524910569190979, "min_score_of_first_answer": 0.0652943029999733, "avg_score": 0.22293925285339355, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1881076285618571, "std_of_first_answer": 0.1881076285618571, "avg_entropy_of_first_answer": 3.3275602658589682, "lex_sim": 1.0}
{"question": "Which British Prime Minister said of Downing Street \u2018One of the most precious jewels in the national heritage\u2019?", "answer": ["Harold McMillan"], "scores": [[0.0905812606215477, 0.06052298843860626, 0.7879586219787598, 0.8850632905960083]], "normalized_score": [0.2486618310213089], "tokens": [["\u2581Harold", "\u2581Mc", "Mill", "an"]], "entropy": [[4.9239959716796875, 2.1243605613708496, 0.6959308981895447, 0.5728781819343567]], "avg_entropy": [2.0792914032936096], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Baroness Margaret Thatcher", "score_of_biggest_cluster": 0.2486618310213089, "score_of_first_answer": 0.2486618310213089, "max_score_of_first_answer": 0.8850632905960083, "min_score_of_first_answer": 0.06052298843860626, "avg_score": 0.2486618310213089, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38217298171979347, "std_of_first_answer": 0.38217298171979347, "avg_entropy_of_first_answer": 2.0792914032936096, "lex_sim": 1.0}
{"question": "What name is given to the bridge design, such as Tower Bridge, with a divided lifting roadway?", "answer": ["Bascule bridge"], "scores": [[0.31884947419166565, 0.9357620477676392, 0.9992786049842834, 0.30053287744522095]], "normalized_score": [0.5471197962760925], "tokens": [["\u2581Bas", "c", "ule", "\u2581bridge"]], "entropy": [[4.276120185852051, 0.4017995595932007, 0.009337213821709156, 1.6712621450424194]], "avg_entropy": [1.589629776077345], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Bascule", "score_of_biggest_cluster": 0.5471197962760925, "score_of_first_answer": 0.5471197962760925, "max_score_of_first_answer": 0.9992786049842834, "min_score_of_first_answer": 0.30053287744522095, "avg_score": 0.5471197962760925, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3297438842123305, "std_of_first_answer": 0.3297438842123305, "avg_entropy_of_first_answer": 1.589629776077345, "lex_sim": 1.0}
{"question": "On 30 September 1399, Henry Bolingbroke, Duke of Lancaster, became better known as whom?", "answer": ["Henry IV (2 August 1367 \u2013 20 March 1413), also known as Henry Bolingbroke, was king of England and France, and Lord of Ireland, from 1399 to"], "scores": [[0.30983394384384155, 0.8343804478645325, 0.1349586844444275, 0.007238614372909069, 0.09518057107925415, 0.9915958046913147, 0.9997363686561584, 0.9987519979476929, 0.9664917588233948, 0.8330163359642029, 0.8863155245780945, 0.9910483956336975, 0.9821637868881226, 0.9943588376045227, 0.9986090064048767, 0.9983955025672913, 0.999643087387085, 0.9997958540916443, 0.9988707900047302, 0.9996718168258667, 0.2595101296901703, 0.5832185745239258, 0.3851163983345032, 0.9859573841094971, 0.9173563122749329, 0.0948866456747055, 0.999764621257782, 0.9998162388801575, 0.9991294741630554, 0.6928763389587402, 0.9044069647789001, 0.017045628279447556, 0.9953879714012146, 0.9957219362258911, 0.6436697244644165, 0.008995515294373035, 0.06389985978603363, 0.9104712605476379, 0.480631023645401, 0.9988688826560974, 0.9958620071411133, 0.8043136596679688, 0.9889554381370544, 0.9956948757171631, 0.9463831782341003, 0.9487441778182983, 0.9993475079536438, 0.999269425868988, 0.9687560796737671]], "normalized_score": [0.542397677898407], "tokens": [["\u2581Henry", "\u2581IV", "\u2581(", "2", "\u2581August", "\u2581", "1", "3", "6", "7", "\u2581\u2013", "\u2581", "2", "0", "\u2581March", "\u2581", "1", "4", "1", "3", "),", "\u2581also", "\u2581known", "\u2581as", "\u2581Henry", "\u2581Bol", "ing", "bro", "ke", ",", "\u2581was", "\u2581king", "\u2581of", "\u2581England", "\u2581and", "\u2581France", ",", "\u2581and", "\u2581Lord", "\u2581of", "\u2581Ireland", ",", "\u2581from", "\u2581", "1", "3", "9", "9", "\u2581to"]], "entropy": [[3.9420151710510254, 1.1492096185684204, 1.421492338180542, 3.062472343444824, 2.4927895069122314, 0.0735606923699379, 0.0028961747884750366, 0.0116996755823493, 0.160255566239357, 0.4545154571533203, 0.6240963935852051, 0.07159610092639923, 0.10501118749380112, 0.043295975774526596, 0.014962883666157722, 0.01742984727025032, 0.003789798356592655, 0.002329941838979721, 0.009509792551398277, 0.003537931013852358, 0.9456517696380615, 1.9987496137619019, 0.7358679175376892, 0.1227874830365181, 0.37721627950668335, 0.35492995381355286, 0.0026691961102187634, 0.0024872361682355404, 0.008091021329164505, 1.304072618484497, 0.6441650390625, 0.8844687938690186, 0.041504357010126114, 0.0390133261680603, 0.8860248327255249, 0.8913309574127197, 1.4101828336715698, 0.6215585470199585, 1.3489810228347778, 0.01196213811635971, 0.03354490548372269, 0.7325989603996277, 0.10089589655399323, 0.033720891922712326, 0.24702796339988708, 0.20875728130340576, 0.007313684094697237, 0.007883220911026001, 0.2011750191450119]], "avg_entropy": [0.5687979419848749], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Henry IV (England)", "score_of_biggest_cluster": 0.542397677898407, "score_of_first_answer": 0.542397677898407, "max_score_of_first_answer": 0.9998162388801575, "min_score_of_first_answer": 0.007238614372909069, "avg_score": 0.542397677898407, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.34263523023891224, "std_of_first_answer": 0.34263523023891224, "avg_entropy_of_first_answer": 0.5687979419848749, "lex_sim": 1.0}
{"question": "Macbeth belonged to which royal house or dynasty?", "answer": ["Banquo's"], "scores": [[0.02669931761920452, 0.736941933631897, 0.9343762993812561, 0.14150337874889374, 0.9969637989997864]], "normalized_score": [0.3039346933364868], "tokens": [["\u2581Ban", "qu", "o", "'", "s"]], "entropy": [[4.810125350952148, 1.530068278312683, 0.5326634645462036, 2.485293388366699, 0.03329536318778992]], "avg_entropy": [1.878289169073105], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "House of Dunkeld", "score_of_biggest_cluster": 0.3039346933364868, "score_of_first_answer": 0.3039346933364868, "max_score_of_first_answer": 0.9969637989997864, "min_score_of_first_answer": 0.02669931761920452, "avg_score": 0.3039346933364868, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.405385307034562, "std_of_first_answer": 0.405385307034562, "avg_entropy_of_first_answer": 1.878289169073105, "lex_sim": 1.0}
{"question": "The German company Recaro famously produces what high-performance product?", "answer": ["Racing car seating"], "scores": [[0.02338450774550438, 0.383210152387619, 0.028529010713100433, 0.9706825017929077]], "normalized_score": [0.12551116943359375], "tokens": [["\u2581Racing", "\u2581car", "\u2581se", "ating"]], "entropy": [[5.312319755554199, 2.5995428562164307, 1.0794159173965454, 0.23762615025043488]], "avg_entropy": [2.3072261698544025], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Seat", "score_of_biggest_cluster": 0.12551116943359375, "score_of_first_answer": 0.12551116943359375, "max_score_of_first_answer": 0.9706825017929077, "min_score_of_first_answer": 0.02338450774550438, "avg_score": 0.12551116943359375, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3861225618495626, "std_of_first_answer": 0.3861225618495626, "avg_entropy_of_first_answer": 2.3072261698544025, "lex_sim": 1.0}
{"question": "Where is the Headquarters of Boeing?", "answer": ["Bing"], "scores": [[0.006895261816680431, 0.00688527524471283]], "normalized_score": [0.0068902671337127686], "tokens": [["\u2581B", "ing"]], "entropy": [[4.48784065246582, 5.7786641120910645]], "avg_entropy": [5.133252382278442], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Seattle, Washington.", "score_of_biggest_cluster": 0.0068902671337127686, "score_of_first_answer": 0.0068902671337127686, "max_score_of_first_answer": 0.006895261816680431, "min_score_of_first_answer": 0.00688527524471283, "avg_score": 0.0068902671337127686, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 4.993285983800888e-06, "std_of_first_answer": 4.993285983800888e-06, "avg_entropy_of_first_answer": 5.133252382278442, "lex_sim": 1.0}
{"question": "The particle physics unit of reactionary particle decay is?", "answer": ["Sigma (particle physics)"], "scores": [[0.011530736461281776, 0.26693448424339294, 0.056724295020103455, 0.37472784519195557, 0.9838908314704895, 0.7984995245933533, 0.9595686793327332]], "normalized_score": [0.24250498414039612], "tokens": [["\u2581S", "igma", "\u2581(", "part", "icle", "\u2581physics", ")"]], "entropy": [[6.0438923835754395, 4.737524509429932, 3.281561851501465, 3.72068452835083, 0.11408516764640808, 0.9307050108909607, 0.2627829611301422]], "avg_entropy": [2.7273194875035967], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Strange particle", "score_of_biggest_cluster": 0.24250498414039612, "score_of_first_answer": 0.24250498414039612, "max_score_of_first_answer": 0.9838908314704895, "min_score_of_first_answer": 0.011530736461281776, "avg_score": 0.24250498414039612, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38530749376568385, "std_of_first_answer": 0.38530749376568385, "avg_entropy_of_first_answer": 2.7273194875035967, "lex_sim": 1.0}
{"question": "What is the deepest male singing voice?", "answer": ["Chthonic"], "scores": [[0.0052756331861019135, 0.0057324920780956745, 0.9473495483398438]], "normalized_score": [0.030599158257246017], "tokens": [["\u2581Ch", "th", "onic"]], "entropy": [[4.590864181518555, 2.789213180541992, 0.3169446289539337]], "avg_entropy": [2.565673997004827], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bass (musical term)", "score_of_biggest_cluster": 0.030599158257246017, "score_of_first_answer": 0.030599158257246017, "max_score_of_first_answer": 0.9473495483398438, "min_score_of_first_answer": 0.0052756331861019135, "avg_score": 0.030599158257246017, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44399025902430017, "std_of_first_answer": 0.44399025902430017, "avg_entropy_of_first_answer": 2.565673997004827, "lex_sim": 1.0}
{"question": "In which country was Rita Ora born", "answer": ["Kosovo"], "scores": [[0.34347569942474365, 0.8695825338363647]], "normalized_score": [0.5465166568756104], "tokens": [["\u2581Kos", "ovo"]], "entropy": [[4.2138519287109375, 0.6201441287994385]], "avg_entropy": [2.416998028755188], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Republika e Kosoves", "score_of_biggest_cluster": 0.5465166568756104, "score_of_first_answer": 0.5465166568756104, "max_score_of_first_answer": 0.8695825338363647, "min_score_of_first_answer": 0.34347569942474365, "avg_score": 0.5465166568756104, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.26305341720581055, "std_of_first_answer": 0.26305341720581055, "avg_entropy_of_first_answer": 2.416998028755188, "lex_sim": 1.0}
{"question": "Who was known as 'The Great One' in North American ice hockey?", "answer": ["Wayne Gretzky"], "scores": [[0.2561049163341522, 0.46278977394104004, 0.9945582151412964, 0.9481130838394165, 0.9916945099830627]], "normalized_score": [0.6440713405609131], "tokens": [["\u2581Wayne", "\u2581G", "ret", "z", "ky"]], "entropy": [[5.041123390197754, 2.3429269790649414, 0.06615923345088959, 0.2884594798088074, 0.06905800849199295]], "avg_entropy": [1.561545418202877], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gretzky's Office", "score_of_biggest_cluster": 0.6440713405609131, "score_of_first_answer": 0.6440713405609131, "max_score_of_first_answer": 0.9945582151412964, "min_score_of_first_answer": 0.2561049163341522, "avg_score": 0.6440713405609131, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3104912598312706, "std_of_first_answer": 0.3104912598312706, "avg_entropy_of_first_answer": 1.561545418202877, "lex_sim": 1.0}
{"question": "\"Who plays the part of Ric Griffin in TV's \"\"Holby City\"\"?\"", "answer": ["Wade Headley"], "scores": [[0.003154573030769825, 0.009995348751544952, 0.006588922813534737, 0.7226598262786865]], "normalized_score": [0.01968437433242798], "tokens": [["\u2581W", "ade", "\u2581Head", "ley"]], "entropy": [[5.316516876220703, 5.594954013824463, 5.401308059692383, 1.4376513957977295]], "avg_entropy": [4.43760758638382], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hugh Quarshie", "score_of_biggest_cluster": 0.01968437433242798, "score_of_first_answer": 0.01968437433242798, "max_score_of_first_answer": 0.7226598262786865, "min_score_of_first_answer": 0.003154573030769825, "avg_score": 0.01968437433242798, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3100812595593235, "std_of_first_answer": 0.3100812595593235, "avg_entropy_of_first_answer": 4.43760758638382, "lex_sim": 1.0}
{"question": "Perhaps the greatest thing since something, what now common innovation was introduced by the Chillicothe Baking Company of Chillicothe, MO on July 7, 1928?", "answer": ["Pies"], "scores": [[0.011740480549633503, 0.003325666068121791]], "normalized_score": [0.006248593330383301], "tokens": [["\u2581P", "ies"]], "entropy": [[3.8918561935424805, 3.7888808250427246]], "avg_entropy": [3.8403685092926025], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pane in cassetta", "score_of_biggest_cluster": 0.006248593330383301, "score_of_first_answer": 0.006248593330383301, "max_score_of_first_answer": 0.011740480549633503, "min_score_of_first_answer": 0.003325666068121791, "avg_score": 0.006248593330383301, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.004207407240755856, "std_of_first_answer": 0.004207407240755856, "avg_entropy_of_first_answer": 3.8403685092926025, "lex_sim": 1.0}
{"question": "In a film who married Princess Fiona, the daughter of King Harold and Queen Lillian of Far, Far Away?", "answer": ["The Shrek 3D"], "scores": [[0.033537186682224274, 0.19237275421619415, 0.9607378840446472, 0.04722875356674194, 0.1287894994020462, 0.12826338410377502]], "normalized_score": [0.13004015386104584], "tokens": [["\u2581The", "\u2581Sh", "rek", "\u2581", "3", "D"]], "entropy": [[4.719003677368164, 5.727440357208252, 0.28849583864212036, 3.3187050819396973, 1.321750283241272, 3.1930134296417236]], "avg_entropy": [3.0947347780068717], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Shrek One", "score_of_biggest_cluster": 0.13004015386104584, "score_of_first_answer": 0.13004015386104584, "max_score_of_first_answer": 0.9607378840446472, "min_score_of_first_answer": 0.033537186682224274, "avg_score": 0.13004015386104584, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3229911360662713, "std_of_first_answer": 0.3229911360662713, "avg_entropy_of_first_answer": 3.0947347780068717, "lex_sim": 1.0}
{"question": "Gene Hackman played the sheriff of Big Whiskey and received an Oscar in which 1992 film?", "answer": ["Unforgiven (film)"], "scores": [[0.6062965393066406, 0.991304874420166, 0.998422384262085, 0.9249887466430664, 0.44324249029159546, 0.6313868165016174, 0.9868637323379517]], "normalized_score": [0.7649766206741333], "tokens": [["\u2581Un", "f", "org", "iven", "\u2581(", "film", ")"]], "entropy": [[2.966487169265747, 0.08687247335910797, 0.01868142932653427, 0.35426944494247437, 1.0936000347137451, 1.3586502075195312, 0.11348028481006622]], "avg_entropy": [0.8560058634196009], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Unforgiven (film)", "score_of_biggest_cluster": 0.7649766206741333, "score_of_first_answer": 0.7649766206741333, "max_score_of_first_answer": 0.998422384262085, "min_score_of_first_answer": 0.44324249029159546, "avg_score": 0.7649766206741333, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.21370636523229292, "std_of_first_answer": 0.21370636523229292, "avg_entropy_of_first_answer": 0.8560058634196009, "lex_sim": 1.0}
{"question": "Who played Bond villain Scaramanga in The Man with the Golden Gun?", "answer": ["Lee"], "scores": [[0.0027818612288683653]], "normalized_score": [0.002781861461699009], "tokens": [["\u2581Lee"]], "entropy": [[5.525230884552002]], "avg_entropy": [5.525230884552002], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Christopher Frank Lee", "score_of_biggest_cluster": 0.002781861461699009, "score_of_first_answer": 0.002781861461699009, "max_score_of_first_answer": 0.0027818612288683653, "min_score_of_first_answer": 0.0027818612288683653, "avg_score": 0.002781861461699009, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.525230884552002, "lex_sim": 1.0}
{"question": "In which State is USA's highest mountain, Mount McKinley?", "answer": ["Hawaii"], "scores": [[0.007555694784969091, 0.8976582884788513]], "normalized_score": [0.08235551416873932], "tokens": [["\u2581Hawai", "i"]], "entropy": [[3.540095329284668, 0.5183829665184021]], "avg_entropy": [2.029239147901535], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Forty-ninth State", "score_of_biggest_cluster": 0.08235551416873932, "score_of_first_answer": 0.08235551416873932, "max_score_of_first_answer": 0.8976582884788513, "min_score_of_first_answer": 0.007555694784969091, "avg_score": 0.08235551416873932, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4450512968469411, "std_of_first_answer": 0.4450512968469411, "avg_entropy_of_first_answer": 2.029239147901535, "lex_sim": 1.0}
{"question": "Who composed the musical theme for the Pink Panther?", "answer": ["AHDKS"], "scores": [[0.011042982339859009, 0.01306988950818777, 0.004142121411859989, 0.07410693913698196]], "normalized_score": [0.014508082531392574], "tokens": [["\u2581A", "H", "DK", "S"]], "entropy": [[4.071048736572266, 6.18221378326416, 5.331470012664795, 3.9802396297454834]], "avg_entropy": [4.891243040561676], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Henry Mancini", "score_of_biggest_cluster": 0.014508082531392574, "score_of_first_answer": 0.014508082531392574, "max_score_of_first_answer": 0.07410693913698196, "min_score_of_first_answer": 0.004142121411859989, "avg_score": 0.014508082531392574, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02820582283745374, "std_of_first_answer": 0.02820582283745374, "avg_entropy_of_first_answer": 4.891243040561676, "lex_sim": 1.0}
{"question": "Wednesday Friday is the youngest member of which TV family from the 1960s?", "answer": ["The Flintstones (band)"], "scores": [[0.37235862016677856, 0.08859962224960327, 0.9793927073478699, 0.9846861362457275, 0.9988777041435242, 0.15082503855228424, 0.007345047779381275, 0.9306666851043701]], "normalized_score": [0.2750602662563324], "tokens": [["\u2581The", "\u2581Fl", "int", "st", "ones", "\u2581(", "band", ")"]], "entropy": [[4.031494140625, 2.6638779640197754, 0.1188284158706665, 0.10093263536691666, 0.009621352888643742, 0.9846293330192566, 2.784446954727173, 0.5196638107299805]], "avg_entropy": [1.4016868259059265], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Addams Family, The", "score_of_biggest_cluster": 0.2750602662563324, "score_of_first_answer": 0.2750602662563324, "max_score_of_first_answer": 0.9988777041435242, "min_score_of_first_answer": 0.007345047779381275, "avg_score": 0.2750602662563324, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4207732693649842, "std_of_first_answer": 0.4207732693649842, "avg_entropy_of_first_answer": 1.4016868259059265, "lex_sim": 1.0}
{"question": "Dsseldorf is a major city in which country?", "answer": ["Denmark"], "scores": [[0.002840764820575714, 0.9136185646057129]], "normalized_score": [0.05094483494758606], "tokens": [["\u2581Den", "mark"]], "entropy": [[4.4351806640625, 0.7809839248657227]], "avg_entropy": [2.6080822944641113], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "United States of Germany", "score_of_biggest_cluster": 0.05094483494758606, "score_of_first_answer": 0.05094483494758606, "max_score_of_first_answer": 0.9136185646057129, "min_score_of_first_answer": 0.002840764820575714, "avg_score": 0.05094483494758606, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4553888998925686, "std_of_first_answer": 0.4553888998925686, "avg_entropy_of_first_answer": 2.6080822944641113, "lex_sim": 1.0}
{"question": "The oak furniture maker Robert Thompson included a carving of which animal on almost every piece he made from the 1920s until his death in 1955?", "answer": ["King Rat"], "scores": [[0.004052233882248402, 0.014134343713521957]], "normalized_score": [0.007568070199340582], "tokens": [["\u2581King", "\u2581Rat"]], "entropy": [[5.449885368347168, 2.4014291763305664]], "avg_entropy": [3.925657272338867], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\ud83d\udc01", "score_of_biggest_cluster": 0.007568070199340582, "score_of_first_answer": 0.007568070199340582, "max_score_of_first_answer": 0.014134343713521957, "min_score_of_first_answer": 0.004052233882248402, "avg_score": 0.007568070199340582, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.005041054915636778, "std_of_first_answer": 0.005041054915636778, "avg_entropy_of_first_answer": 3.925657272338867, "lex_sim": 1.0}
{"question": "American businessman and philanthropist Solomon Guggenheim (1861-1949) established a famous eponymous international network of?", "answer": ["Solomon Guggenheim Museum"], "scores": [[0.16793940961360931, 0.932439923286438, 0.42163434624671936, 0.973405122756958, 0.988400936126709, 0.9835131764411926, 0.4414190351963043]], "normalized_score": [0.5987202525138855], "tokens": [["\u2581Sol", "omon", "\u2581G", "ug", "gen", "heim", "\u2581Museum"]], "entropy": [[3.8653652667999268, 0.5551140904426575, 1.7584519386291504, 0.22550390660762787, 0.09728281199932098, 0.11923245340585709, 2.280134677886963]], "avg_entropy": [1.2715835922530718], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "History of museums", "score_of_biggest_cluster": 0.5987202525138855, "score_of_first_answer": 0.5987202525138855, "max_score_of_first_answer": 0.988400936126709, "min_score_of_first_answer": 0.16793940961360931, "avg_score": 0.5987202525138855, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3206606088057217, "std_of_first_answer": 0.3206606088057217, "avg_entropy_of_first_answer": 1.2715835922530718, "lex_sim": 1.0}
{"question": "The Luas Tram system serves which European capital city?", "answer": ["Dublin"], "scores": [[0.39796194434165955]], "normalized_score": [0.39796194434165955], "tokens": [["\u2581Dublin"]], "entropy": [[4.3898210525512695]], "avg_entropy": [4.3898210525512695], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "UN/LOCODE:IEDUB", "score_of_biggest_cluster": 0.39796194434165955, "score_of_first_answer": 0.39796194434165955, "max_score_of_first_answer": 0.39796194434165955, "min_score_of_first_answer": 0.39796194434165955, "avg_score": 0.39796194434165955, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.3898210525512695, "lex_sim": 1.0}
{"question": "Who in 1975 became the first black man to win a Wimbledon singles title?", "answer": ["Arthur Ashe"], "scores": [[0.3956995904445648, 0.8529558181762695, 0.985937774181366]], "normalized_score": [0.6929691433906555], "tokens": [["\u2581Arthur", "\u2581As", "he"]], "entropy": [[4.4792160987854, 1.1242213249206543, 0.1319180727005005]], "avg_entropy": [1.9117851654688518], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Arthur Ashe, Jr.", "score_of_biggest_cluster": 0.6929691433906555, "score_of_first_answer": 0.6929691433906555, "max_score_of_first_answer": 0.985937774181366, "min_score_of_first_answer": 0.3956995904445648, "avg_score": 0.6929691433906555, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.25279516784307754, "std_of_first_answer": 0.25279516784307754, "avg_entropy_of_first_answer": 1.9117851654688518, "lex_sim": 1.0}
{"question": "What often poisonous compound, CN, found in fruit seeds and stones, named from Greek for dark blue, comprises Carbon and Nitrogen?", "answer": ["Prussic Acid"], "scores": [[0.09185050427913666, 0.9585953950881958, 0.7990677356719971, 0.146128311753273, 0.9986459612846375]], "normalized_score": [0.40021124482154846], "tokens": [["\u2581Pr", "uss", "ic", "\u2581Ac", "id"]], "entropy": [[4.72084379196167, 0.29095137119293213, 0.8396778702735901, 0.8283772468566895, 0.013579823076725006]], "avg_entropy": [1.3386860206723212], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "CN-", "score_of_biggest_cluster": 0.40021124482154846, "score_of_first_answer": 0.40021124482154846, "max_score_of_first_answer": 0.9986459612846375, "min_score_of_first_answer": 0.09185050427913666, "avg_score": 0.40021124482154846, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3978306466342338, "std_of_first_answer": 0.3978306466342338, "avg_entropy_of_first_answer": 1.3386860206723212, "lex_sim": 1.0}
{"question": "What colour cape does Batman\u2019s sidekick Robin wear in the 1960\u2019s television series?", "answer": ["yellow"], "scores": [[0.007201419677585363]], "normalized_score": [0.007201421074569225], "tokens": [["\u2581yellow"]], "entropy": [[5.2480974197387695]], "avg_entropy": [5.2480974197387695], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yellowest", "score_of_biggest_cluster": 0.007201421074569225, "score_of_first_answer": 0.007201421074569225, "max_score_of_first_answer": 0.007201419677585363, "min_score_of_first_answer": 0.007201419677585363, "avg_score": 0.007201421074569225, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.2480974197387695, "lex_sim": 1.0}
{"question": "Quantrill's Raiders, a loosely organised force of Partisan rangers under the leadership of William Clarke Quantril, fought in which war?", "answer": ["American Civil War"], "scores": [[0.4309910833835602, 0.9329474568367004, 0.9670292139053345]], "normalized_score": [0.729885995388031], "tokens": [["\u2581American", "\u2581Civil", "\u2581War"]], "entropy": [[3.5145583152770996, 0.50773024559021, 0.22348511219024658]], "avg_entropy": [1.4152578910191853], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Civil War in the United States", "score_of_biggest_cluster": 0.729885995388031, "score_of_first_answer": 0.729885995388031, "max_score_of_first_answer": 0.9670292139053345, "min_score_of_first_answer": 0.4309910833835602, "avg_score": 0.729885995388031, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.24505297485972125, "std_of_first_answer": 0.24505297485972125, "avg_entropy_of_first_answer": 1.4152578910191853, "lex_sim": 1.0}
{"question": "In which North African country was Mohammed Al-Fayed, owner of Harrods department store in London, born?", "answer": ["RASMAIL"], "scores": [[0.002756201196461916, 0.008707147091627121, 0.0035710663069039583, 0.002856062026694417]], "normalized_score": [0.00395538005977869], "tokens": [["\u2581R", "AS", "MA", "IL"]], "entropy": [[4.645721435546875, 5.811303615570068, 4.301220893859863, 3.457749843597412]], "avg_entropy": [4.553998947143555], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "EGY", "score_of_biggest_cluster": 0.00395538005977869, "score_of_first_answer": 0.00395538005977869, "max_score_of_first_answer": 0.008707147091627121, "min_score_of_first_answer": 0.002756201196461916, "avg_score": 0.00395538005977869, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0024649225110261208, "std_of_first_answer": 0.0024649225110261208, "avg_entropy_of_first_answer": 4.553998947143555, "lex_sim": 1.0}
{"question": "Which character created by Keith Waterhouse was played on film by Tom Courtneyand on TV by Jeff Rawle?", "answer": ["The Singing Dwarf"], "scores": [[0.03479547053575516, 0.007639250718057156, 0.9347605109214783, 0.1312706619501114, 0.0033707537222653627, 0.8167047500610352]], "normalized_score": [0.06691735982894897], "tokens": [["\u2581The", "\u2581Sing", "ing", "\u2581D", "war", "f"]], "entropy": [[4.505136489868164, 6.6500349044799805, 0.43205955624580383, 1.791682481765747, 2.6406564712524414, 0.626239538192749]], "avg_entropy": [2.7743015736341476], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "'BILLY LIAR'", "score_of_biggest_cluster": 0.06691735982894897, "score_of_first_answer": 0.06691735982894897, "max_score_of_first_answer": 0.9347605109214783, "min_score_of_first_answer": 0.0033707537222653627, "avg_score": 0.06691735982894897, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39568877484569304, "std_of_first_answer": 0.39568877484569304, "avg_entropy_of_first_answer": 2.7743015736341476, "lex_sim": 1.0}
{"question": "Who was known as 'The Preston Plumber'?", "answer": ["Samuel Irving Newhouse, Jr."], "scores": [[0.009627188555896282, 0.0074485535733401775, 0.41395747661590576, 0.7656500935554504, 0.9862381219863892, 0.3438936471939087, 0.616130530834198, 0.7993975877761841]], "normalized_score": [0.21009938418865204], "tokens": [["\u2581Samuel", "\u2581Ir", "ving", "\u2581New", "house", ",", "\u2581Jr", "."]], "entropy": [[4.764837265014648, 3.928151845932007, 1.7578988075256348, 1.2920496463775635, 0.10161750763654709, 1.7282161712646484, 1.0467920303344727, 0.647449791431427]], "avg_entropy": [1.9083766331896186], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tom Finney", "score_of_biggest_cluster": 0.21009938418865204, "score_of_first_answer": 0.21009938418865204, "max_score_of_first_answer": 0.9862381219863892, "min_score_of_first_answer": 0.0074485535733401775, "avg_score": 0.21009938418865204, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3399544255691209, "std_of_first_answer": 0.3399544255691209, "avg_entropy_of_first_answer": 1.9083766331896186, "lex_sim": 1.0}
{"question": "Which female politician and aristocrat said \u2018I married beneath me, all women do\u2019?", "answer": ["Margaret Thatcher"], "scores": [[0.19913022220134735, 0.8333180546760559, 0.9876247644424438, 0.9966752529144287]], "normalized_score": [0.635731041431427], "tokens": [["\u2581Margaret", "\u2581Th", "atch", "er"]], "entropy": [[4.164186954498291, 1.0641082525253296, 0.10531099885702133, 0.03412996977567673]], "avg_entropy": [1.3419340439140797], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nancy astor", "score_of_biggest_cluster": 0.635731041431427, "score_of_first_answer": 0.635731041431427, "max_score_of_first_answer": 0.9966752529144287, "min_score_of_first_answer": 0.19913022220134735, "avg_score": 0.635731041431427, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3269722821064975, "std_of_first_answer": 0.3269722821064975, "avg_entropy_of_first_answer": 1.3419340439140797, "lex_sim": 1.0}
{"question": "The Melrose Cup is awarded in which ball sport?", "answer": ["Rugby Union"], "scores": [[0.2871685326099396, 0.24242155253887177]], "normalized_score": [0.2638481557369232], "tokens": [["\u2581Rugby", "\u2581Union"]], "entropy": [[4.593127250671387, 2.5718894004821777]], "avg_entropy": [3.5825083255767822], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Borders Game", "score_of_biggest_cluster": 0.2638481557369232, "score_of_first_answer": 0.2638481557369232, "max_score_of_first_answer": 0.2871685326099396, "min_score_of_first_answer": 0.24242155253887177, "avg_score": 0.2638481557369232, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.022373490035533905, "std_of_first_answer": 0.022373490035533905, "avg_entropy_of_first_answer": 3.5825083255767822, "lex_sim": 1.0}
{"question": "What is another name for shingles?", "answer": ["Herpes zoster"], "scores": [[0.3643355667591095, 0.9366117715835571, 0.6410673260688782, 0.9696689248085022]], "normalized_score": [0.6786519289016724], "tokens": [["\u2581Her", "pes", "\u2581z", "oster"]], "entropy": [[4.16959810256958, 0.37030303478240967, 1.2898225784301758, 0.24283833801746368]], "avg_entropy": [1.5181405134499073], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Disseminated herpes zoster", "score_of_biggest_cluster": 0.6786519289016724, "score_of_first_answer": 0.6786519289016724, "max_score_of_first_answer": 0.9696689248085022, "min_score_of_first_answer": 0.3643355667591095, "avg_score": 0.6786519289016724, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2458311540639302, "std_of_first_answer": 0.2458311540639302, "avg_entropy_of_first_answer": 1.5181405134499073, "lex_sim": 1.0}
{"question": "Which famous UK television duo provide the voices for the main characters in the children\u2019s program \u2018Engie Benjy\u2019?", "answer": ["Ant and Dec"], "scores": [[0.07459709048271179, 0.5952109694480896, 0.848054826259613]], "normalized_score": [0.335175484418869], "tokens": [["\u2581Ant", "\u2581and", "\u2581Dec"]], "entropy": [[5.671416282653809, 1.4160757064819336, 1.244299054145813]], "avg_entropy": [2.7772636810938516], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Declan Donnely", "score_of_biggest_cluster": 0.335175484418869, "score_of_first_answer": 0.335175484418869, "max_score_of_first_answer": 0.848054826259613, "min_score_of_first_answer": 0.07459709048271179, "avg_score": 0.335175484418869, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.322008574063182, "std_of_first_answer": 0.322008574063182, "avg_entropy_of_first_answer": 2.7772636810938516, "lex_sim": 1.0}
{"question": "Areas on the Canadian eastern coast share their name with two breeds of dog, Newfoundland and which other?", "answer": ["Labrador Retriever Dog / Poodle Dog"], "scores": [[0.4157920479774475, 0.9425914883613586, 0.9988840222358704, 0.2035006582736969, 0.9130290746688843, 0.013963314704596996, 0.0002901261323131621, 0.005162199959158897, 0.2830003798007965, 0.9647506475448608, 0.37490612268447876]], "normalized_score": [0.12834857404232025], "tokens": [["\u2581Lab", "r", "ador", "\u2581Retrie", "ver", "\u2581Dog", "\u2581/", "\u2581P", "ood", "le", "\u2581Dog"]], "entropy": [[3.6439926624298096, 0.3906211853027344, 0.013477858155965805, 1.3963570594787598, 0.328179270029068, 0.6959823966026306, 1.004745602607727, 4.998010635375977, 3.865385055541992, 0.17326872050762177, 1.9149225950241089]], "avg_entropy": [1.6749948219142177], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Labrador boundary dispute", "score_of_biggest_cluster": 0.12834857404232025, "score_of_first_answer": 0.12834857404232025, "max_score_of_first_answer": 0.9988840222358704, "min_score_of_first_answer": 0.0002901261323131621, "avg_score": 0.12834857404232025, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39387246771336176, "std_of_first_answer": 0.39387246771336176, "avg_entropy_of_first_answer": 1.6749948219142177, "lex_sim": 1.0}
{"question": "The mountain massif called Maxwell Montes contains the highest point on the surface of which planet?", "answer": ["The Moon"], "scores": [[0.036157604306936264, 0.080814890563488]], "normalized_score": [0.05405619740486145], "tokens": [["\u2581The", "\u2581Moon"]], "entropy": [[4.402073860168457, 5.956203460693359]], "avg_entropy": [5.179138660430908], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sol 2", "score_of_biggest_cluster": 0.05405619740486145, "score_of_first_answer": 0.05405619740486145, "max_score_of_first_answer": 0.080814890563488, "min_score_of_first_answer": 0.036157604306936264, "avg_score": 0.05405619740486145, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02232864312827587, "std_of_first_answer": 0.02232864312827587, "avg_entropy_of_first_answer": 5.179138660430908, "lex_sim": 1.0}
{"question": "Retailer Fortnum and Mason opened its first new UK store for more than 300 years at which London station in November 2013?", "answer": ["Kings Cross Station"], "scores": [[0.029236214235424995, 0.8127658367156982, 0.17669649422168732]], "normalized_score": [0.16132615506649017], "tokens": [["\u2581Kings", "\u2581Cross", "\u2581Station"]], "entropy": [[4.8292741775512695, 1.4730427265167236, 2.2612171173095703]], "avg_entropy": [2.854511340459188], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Saint Pancras railway station", "score_of_biggest_cluster": 0.16132615506649017, "score_of_first_answer": 0.16132615506649017, "max_score_of_first_answer": 0.8127658367156982, "min_score_of_first_answer": 0.029236214235424995, "avg_score": 0.16132615506649017, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3399750663959654, "std_of_first_answer": 0.3399750663959654, "avg_entropy_of_first_answer": 2.854511340459188, "lex_sim": 1.0}
{"question": "What pattern would be on material described as Tattersall", "answer": ["Rust and White"], "scores": [[0.004554769955575466, 0.03232560306787491, 0.01935148984193802, 0.023674355819821358]], "normalized_score": [0.016115786507725716], "tokens": [["\u2581R", "ust", "\u2581and", "\u2581White"]], "entropy": [[5.274819374084473, 4.532573223114014, 3.051274061203003, 4.371530055999756]], "avg_entropy": [4.307549178600311], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Check (disambiguation)", "score_of_biggest_cluster": 0.016115786507725716, "score_of_first_answer": 0.016115786507725716, "max_score_of_first_answer": 0.03232560306787491, "min_score_of_first_answer": 0.004554769955575466, "avg_score": 0.016115786507725716, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.010054785918830089, "std_of_first_answer": 0.010054785918830089, "avg_entropy_of_first_answer": 4.307549178600311, "lex_sim": 1.0}
{"question": "What word makes new words as a prefix with the words cover, line, code and ship?", "answer": ["Crypto"], "scores": [[0.07747956365346909, 0.43506354093551636]], "normalized_score": [0.1835988312959671], "tokens": [["\u2581C", "rypto"]], "entropy": [[6.459244251251221, 2.7220492362976074]], "avg_entropy": [4.590646743774414], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hard (album)", "score_of_biggest_cluster": 0.1835988312959671, "score_of_first_answer": 0.1835988312959671, "max_score_of_first_answer": 0.43506354093551636, "min_score_of_first_answer": 0.07747956365346909, "avg_score": 0.1835988312959671, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17879198864102364, "std_of_first_answer": 0.17879198864102364, "avg_entropy_of_first_answer": 4.590646743774414, "lex_sim": 1.0}
{"question": "The Greek god of marriage is?", "answer": ["\u32ce"], "scores": [[0.05113467946648598, 0.1881236433982849, 0.38249292969703674, 0.0325634591281414]], "normalized_score": [0.10462324321269989], "tokens": [["\u2581", "<0xE3>", "<0x8B>", "<0x8E>"]], "entropy": [[4.498442649841309, 5.090107440948486, 2.489372491836548, 3.5311062335968018]], "avg_entropy": [3.902257204055786], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Microperforate hymen", "score_of_biggest_cluster": 0.10462324321269989, "score_of_first_answer": 0.10462324321269989, "max_score_of_first_answer": 0.38249292969703674, "min_score_of_first_answer": 0.0325634591281414, "avg_score": 0.10462324321269989, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1399415461047241, "std_of_first_answer": 0.1399415461047241, "avg_entropy_of_first_answer": 3.902257204055786, "lex_sim": 0.0}
{"question": "The king of what country became Iceland's ruler by treaty in 1262?", "answer": ["Denmark"], "scores": [[0.058962300419807434, 0.9820969700813293]], "normalized_score": [0.2406381070613861], "tokens": [["\u2581Den", "mark"]], "entropy": [[3.880673408508301, 0.18328729271888733]], "avg_entropy": [2.031980350613594], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Norv\u00e8ge", "score_of_biggest_cluster": 0.2406381070613861, "score_of_first_answer": 0.2406381070613861, "max_score_of_first_answer": 0.9820969700813293, "min_score_of_first_answer": 0.058962300419807434, "avg_score": 0.2406381070613861, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46156733483076096, "std_of_first_answer": 0.46156733483076096, "avg_entropy_of_first_answer": 2.031980350613594, "lex_sim": 1.0}
{"question": "In Roman mythology, on which part of the body were talaria worn?", "answer": ["Feet"], "scores": [[0.09241606295108795, 0.9068320393562317]], "normalized_score": [0.2894923985004425], "tokens": [["\u2581Fe", "et"]], "entropy": [[5.145743370056152, 0.530253529548645]], "avg_entropy": [2.8379984498023987], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Foot fracture", "score_of_biggest_cluster": 0.2894923985004425, "score_of_first_answer": 0.2894923985004425, "max_score_of_first_answer": 0.9068320393562317, "min_score_of_first_answer": 0.09241606295108795, "avg_score": 0.2894923985004425, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40720798820257187, "std_of_first_answer": 0.40720798820257187, "avg_entropy_of_first_answer": 2.8379984498023987, "lex_sim": 1.0}
{"question": "What hill does a bill sit on before it becomes a law?", "answer": ["Capitol Hill"], "scores": [[0.09183996170759201, 0.9782522320747375, 0.8340756893157959]], "normalized_score": [0.42159557342529297], "tokens": [["\u2581Capit", "ol", "\u2581Hill"]], "entropy": [[5.593397617340088, 0.15223920345306396, 0.8068745136260986]], "avg_entropy": [2.184170444806417], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Capitol Hill, Washington, DC", "score_of_biggest_cluster": 0.42159557342529297, "score_of_first_answer": 0.42159557342529297, "max_score_of_first_answer": 0.9782522320747375, "min_score_of_first_answer": 0.09183996170759201, "avg_score": 0.42159557342529297, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3883622970425422, "std_of_first_answer": 0.3883622970425422, "avg_entropy_of_first_answer": 2.184170444806417, "lex_sim": 1.0}
{"question": "\"\"\"Belshazzar's Feast\u2019\u2019, \"\"Portsmouth Point\u201d and \"\"Facade\u201d are among the works of which composer?\"", "answer": ["William Walton"], "scores": [[0.41980019211769104, 0.8836473226547241, 0.9861071109771729]], "normalized_score": [0.7151798009872437], "tokens": [["\u2581William", "\u2581Wal", "ton"]], "entropy": [[3.738229751586914, 1.0157430171966553, 0.09045174717903137]], "avg_entropy": [1.6148081719875336], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Will Walton", "score_of_biggest_cluster": 0.7151798009872437, "score_of_first_answer": 0.7151798009872437, "max_score_of_first_answer": 0.9861071109771729, "min_score_of_first_answer": 0.41980019211769104, "avg_score": 0.7151798009872437, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24638625842784156, "std_of_first_answer": 0.24638625842784156, "avg_entropy_of_first_answer": 1.6148081719875336, "lex_sim": 1.0}
{"question": "What is the screen name of the actor Bernard Schwarz?", "answer": ["Tony Curtis"], "scores": [[0.5396586656570435, 0.5784799456596375, 0.9834534525871277]], "normalized_score": [0.674611508846283], "tokens": [["\u2581Tony", "\u2581Curt", "is"]], "entropy": [[3.4608211517333984, 2.495359420776367, 0.13413435220718384]], "avg_entropy": [2.0301049749056497], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bernard Schwartz (actor)", "score_of_biggest_cluster": 0.674611508846283, "score_of_first_answer": 0.674611508846283, "max_score_of_first_answer": 0.9834534525871277, "min_score_of_first_answer": 0.5396586656570435, "avg_score": 0.674611508846283, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20068340074203908, "std_of_first_answer": 0.20068340074203908, "avg_entropy_of_first_answer": 2.0301049749056497, "lex_sim": 1.0}
{"question": "Which British university was the first to throw open all degrees to women (in 1878)?", "answer": ["University of Bristol"], "scores": [[0.09644123166799545, 0.8620008826255798, 0.2223709523677826, 0.9921342134475708]], "normalized_score": [0.3680058419704437], "tokens": [["\u2581University", "\u2581of", "\u2581Br", "istol"]], "entropy": [[5.227684020996094, 0.5951471924781799, 2.6581358909606934, 0.06840194016695023]], "avg_entropy": [2.1373422611504793], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "London (UK)", "score_of_biggest_cluster": 0.3680058419704437, "score_of_first_answer": 0.3680058419704437, "max_score_of_first_answer": 0.9921342134475708, "min_score_of_first_answer": 0.09644123166799545, "avg_score": 0.3680058419704437, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3891338459418255, "std_of_first_answer": 0.3891338459418255, "avg_entropy_of_first_answer": 2.1373422611504793, "lex_sim": 1.0}
{"question": "A \u2018gricer\u2019 is a slang term for a what?", "answer": ["Rail enthusiast"], "scores": [[0.13186249136924744, 0.1899694800376892, 0.9851958751678467]], "normalized_score": [0.2911449074745178], "tokens": [["\u2581Rail", "\u2581enthus", "iast"]], "entropy": [[4.3922882080078125, 1.9722577333450317, 0.1041504442691803]], "avg_entropy": [2.1562321285406747], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Train spotting", "score_of_biggest_cluster": 0.2911449074745178, "score_of_first_answer": 0.2911449074745178, "max_score_of_first_answer": 0.9851958751678467, "min_score_of_first_answer": 0.13186249136924744, "avg_score": 0.2911449074745178, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3892927070054938, "std_of_first_answer": 0.3892927070054938, "avg_entropy_of_first_answer": 2.1562321285406747, "lex_sim": 1.0}
{"question": "In what year was the most recent census in the UK ?", "answer": ["21"], "scores": [[0.569807231426239, 0.8479415774345398, 0.004176316317170858]], "normalized_score": [0.12636564671993256], "tokens": [["\u2581", "2", "1"]], "entropy": [[3.180068016052246, 0.8326610922813416, 0.2538864016532898]], "avg_entropy": [1.4222051699956257], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2001", "score_of_biggest_cluster": 0.12636564671993256, "score_of_first_answer": 0.12636564671993256, "max_score_of_first_answer": 0.8479415774345398, "min_score_of_first_answer": 0.004176316317170858, "avg_score": 0.12636564671993256, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35106771464982345, "std_of_first_answer": 0.35106771464982345, "avg_entropy_of_first_answer": 1.4222051699956257, "lex_sim": 1.0}
{"question": "In most constellations, which letter of the Greek alphabet was designated by German astronomer Johann Bayer to the brightest star?", "answer": ["Lambda"], "scores": [[0.00864759087562561, 0.7885919809341431]], "normalized_score": [0.08257977664470673], "tokens": [["\u2581L", "ambda"]], "entropy": [[4.675350189208984, 1.6091890335083008]], "avg_entropy": [3.1422696113586426], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u0391\u03b1", "score_of_biggest_cluster": 0.08257977664470673, "score_of_first_answer": 0.08257977664470673, "max_score_of_first_answer": 0.7885919809341431, "min_score_of_first_answer": 0.00864759087562561, "avg_score": 0.08257977664470673, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38997219502925873, "std_of_first_answer": 0.38997219502925873, "avg_entropy_of_first_answer": 3.1422696113586426, "lex_sim": 1.0}
{"question": "Which famous writer of children's books also made serious natural history studies of fungi and lichens?", "answer": ["Gertrude Jekyll"], "scores": [[0.004077447112649679, 0.1406978964805603, 0.9571405053138733, 0.12476649135351181, 0.9849748015403748, 0.9846232533454895]], "normalized_score": [0.20125225186347961], "tokens": [["\u2581Ger", "tr", "ude", "\u2581J", "ek", "yll"]], "entropy": [[5.196234703063965, 2.05954909324646, 0.2131262868642807, 4.0861945152282715, 0.13498812913894653, 0.12349162995815277]], "avg_entropy": [1.9689307262500126], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Beatrix Potter bibliography", "score_of_biggest_cluster": 0.20125225186347961, "score_of_first_answer": 0.20125225186347961, "max_score_of_first_answer": 0.9849748015403748, "min_score_of_first_answer": 0.004077447112649679, "avg_score": 0.20125225186347961, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4450569372859429, "std_of_first_answer": 0.4450569372859429, "avg_entropy_of_first_answer": 1.9689307262500126, "lex_sim": 1.0}
{"question": "Founded in 1775, George Washington was the Commander-in-Chief of what Army?", "answer": ["Continental Army"], "scores": [[0.2452450543642044, 0.9865161180496216, 0.845794141292572]], "normalized_score": [0.5892817974090576], "tokens": [["\u2581Cont", "inental", "\u2581Army"]], "entropy": [[4.3223981857299805, 0.11521217226982117, 0.8286006450653076]], "avg_entropy": [1.7554036676883698], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "American Continental Army", "score_of_biggest_cluster": 0.5892817974090576, "score_of_first_answer": 0.5892817974090576, "max_score_of_first_answer": 0.9865161180496216, "min_score_of_first_answer": 0.2452450543642044, "avg_score": 0.5892817974090576, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3214454628722192, "std_of_first_answer": 0.3214454628722192, "avg_entropy_of_first_answer": 1.7554036676883698, "lex_sim": 1.0}
{"question": "Which actress married Dennis Quaid on Valentine's Day in 1991?", "answer": ["Bark Mask"], "scores": [[0.004844642709940672, 0.02157989703118801, 0.0011113116052001715, 0.050330810248851776]], "normalized_score": [0.008744709193706512], "tokens": [["\u2581B", "ark", "\u2581M", "ask"]], "entropy": [[5.127642631530762, 5.464324474334717, 2.9874067306518555, 5.304191589355469]], "avg_entropy": [4.720891356468201], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Margaret Hyra", "score_of_biggest_cluster": 0.008744709193706512, "score_of_first_answer": 0.008744709193706512, "max_score_of_first_answer": 0.050330810248851776, "min_score_of_first_answer": 0.0011113116052001715, "avg_score": 0.008744709193706512, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.019415095580880094, "std_of_first_answer": 0.019415095580880094, "avg_entropy_of_first_answer": 4.720891356468201, "lex_sim": 1.0}
{"question": "What is the only letter in the alphabet that does not appear in the name of any of the United States?", "answer": ["P"], "scores": [[0.009865248575806618]], "normalized_score": [0.009865250438451767], "tokens": [["\u2581P"]], "entropy": [[5.391639232635498]], "avg_entropy": [5.391639232635498], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u24e0", "score_of_biggest_cluster": 0.009865250438451767, "score_of_first_answer": 0.009865250438451767, "max_score_of_first_answer": 0.009865248575806618, "min_score_of_first_answer": 0.009865248575806618, "avg_score": 0.009865250438451767, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.391639232635498, "lex_sim": 1.0}
{"question": "Which choral work by Handel, still used at coronations, was written originally for the Coronation of King George II in 1727?", "answer": ["Zadok The Priest"], "scores": [[0.4964466989040375, 0.9052625298500061, 0.8624056577682495, 0.04195750504732132, 0.9610510468482971, 0.9993485808372498]], "normalized_score": [0.4999638795852661], "tokens": [["\u2581Z", "ad", "ok", "\u2581The", "\u2581Pri", "est"]], "entropy": [[3.4867072105407715, 0.8504348993301392, 0.722877025604248, 0.6610438227653503, 0.3731653690338135, 0.00750074302777648]], "avg_entropy": [1.0169548450503498], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Zadok The Priest", "score_of_biggest_cluster": 0.4999638795852661, "score_of_first_answer": 0.4999638795852661, "max_score_of_first_answer": 0.9993485808372498, "min_score_of_first_answer": 0.04195750504732132, "avg_score": 0.4999638795852661, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.34156087201227875, "std_of_first_answer": 0.34156087201227875, "avg_entropy_of_first_answer": 1.0169548450503498, "lex_sim": 1.0}
{"question": "A Long Island Iced Tea is a cocktail based on vodka, gin, tequila, and which other spirit?", "answer": ["Gin & Tonic"], "scores": [[0.005259829107671976, 0.47161179780960083, 0.02111843414604664, 0.43848273158073425, 0.9764903783798218]], "normalized_score": [0.11753511428833008], "tokens": [["\u2581G", "in", "\u2581&", "\u2581T", "onic"]], "entropy": [[4.897867202758789, 3.424410820007324, 1.865264892578125, 2.675851821899414, 0.2196541130542755]], "avg_entropy": [2.6166097700595854], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Spiced Rum", "score_of_biggest_cluster": 0.11753511428833008, "score_of_first_answer": 0.11753511428833008, "max_score_of_first_answer": 0.9764903783798218, "min_score_of_first_answer": 0.005259829107671976, "avg_score": 0.11753511428833008, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3568770903603514, "std_of_first_answer": 0.3568770903603514, "avg_entropy_of_first_answer": 2.6166097700595854, "lex_sim": 1.0}
{"question": "Which original surname is shared by Mel Brookes and Danny Kaye?", "answer": ["Batchelder"], "scores": [[0.031950391829013824, 0.011434693820774555, 0.24249912798404694, 0.8904076814651489]], "normalized_score": [0.09424321353435516], "tokens": [["\u2581B", "atch", "el", "der"]], "entropy": [[5.095682144165039, 5.5060529708862305, 3.0657479763031006, 0.6460005044937134]], "avg_entropy": [3.578370898962021], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kami\u0144ski", "score_of_biggest_cluster": 0.09424321353435516, "score_of_first_answer": 0.09424321353435516, "max_score_of_first_answer": 0.8904076814651489, "min_score_of_first_answer": 0.011434693820774555, "avg_score": 0.09424321353435516, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35597316978498456, "std_of_first_answer": 0.35597316978498456, "avg_entropy_of_first_answer": 3.578370898962021, "lex_sim": 1.0}
{"question": "Which 1962 ecological classic was written by Rachel Carson?", "answer": ["The Sound of Music"], "scores": [[0.13124589622020721, 0.008084511384367943, 0.9188951849937439, 0.1572626531124115]], "normalized_score": [0.11127759516239166], "tokens": [["\u2581The", "\u2581Sound", "\u2581of", "\u2581Music"]], "entropy": [[4.860878944396973, 5.2099103927612305, 0.5669071674346924, 2.930605888366699]], "avg_entropy": [3.3920755982398987], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Slient spring", "score_of_biggest_cluster": 0.11127759516239166, "score_of_first_answer": 0.11127759516239166, "max_score_of_first_answer": 0.9188951849937439, "min_score_of_first_answer": 0.008084511384367943, "avg_score": 0.11127759516239166, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3595267127774978, "std_of_first_answer": 0.3595267127774978, "avg_entropy_of_first_answer": 3.3920755982398987, "lex_sim": 1.0}
{"question": "The 1810 betrothal of Princess Theresa to Crown Prince Ludwig not only resulted in the meadow where the event took place to be named Theresienwiese but also started what?", "answer": ["The Munich Oktoberfest"], "scores": [[0.10207336395978928, 0.05741295590996742, 0.9965820908546448, 0.8243458867073059, 0.9779345393180847]], "normalized_score": [0.3424292802810669], "tokens": [["\u2581The", "\u2581Mun", "ich", "\u2581Oktober", "fest"]], "entropy": [[2.84445858001709, 2.9648447036743164, 0.030160631984472275, 0.9315094947814941, 0.1781158447265625]], "avg_entropy": [1.3898178510367871], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Oktoberfest", "score_of_biggest_cluster": 0.3424292802810669, "score_of_first_answer": 0.3424292802810669, "max_score_of_first_answer": 0.9965820908546448, "min_score_of_first_answer": 0.05741295590996742, "avg_score": 0.3424292802810669, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.42247547126226453, "std_of_first_answer": 0.42247547126226453, "avg_entropy_of_first_answer": 1.3898178510367871, "lex_sim": 1.0}
{"question": "Who is the current (Jan 2014) Secretary of State for Education?", "answer": ["Michael Gove"], "scores": [[0.23378396034240723, 0.6930408477783203, 0.9770673513412476]], "normalized_score": [0.5409610867500305], "tokens": [["\u2581Michael", "\u2581G", "ove"]], "entropy": [[5.269311428070068, 2.078280448913574, 0.22624753415584564]], "avg_entropy": [2.524613137046496], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gabriel Milland", "score_of_biggest_cluster": 0.5409610867500305, "score_of_first_answer": 0.5409610867500305, "max_score_of_first_answer": 0.9770673513412476, "min_score_of_first_answer": 0.23378396034240723, "avg_score": 0.5409610867500305, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3062421232943051, "std_of_first_answer": 0.3062421232943051, "avg_entropy_of_first_answer": 2.524613137046496, "lex_sim": 1.0}
{"question": "Who is the author of the 'Harry Potter' books?", "answer": ["J K Rowling"], "scores": [[0.2859060764312744, 0.14234000444412231, 0.7685563564300537, 0.9517780542373657]], "normalized_score": [0.41537508368492126], "tokens": [["\u2581J", "\u2581K", "\u2581Row", "ling"]], "entropy": [[4.870847702026367, 1.5571835041046143, 1.727030873298645, 0.3271857500076294]], "avg_entropy": [2.120561957359314], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "J.K.Rowlings", "score_of_biggest_cluster": 0.41537508368492126, "score_of_first_answer": 0.41537508368492126, "max_score_of_first_answer": 0.9517780542373657, "min_score_of_first_answer": 0.14234000444412231, "avg_score": 0.41537508368492126, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3333405791314612, "std_of_first_answer": 0.3333405791314612, "avg_entropy_of_first_answer": 2.120561957359314, "lex_sim": 1.0}
{"question": "Anna Nicole Smith was controversially married to which oil billionaire?", "answer": ["Smith"], "scores": [[0.0150948166847229]], "normalized_score": [0.01509481854736805], "tokens": [["\u2581Smith"]], "entropy": [[5.281416893005371]], "avg_entropy": [5.281416893005371], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "J. Howard Marshall II", "score_of_biggest_cluster": 0.01509481854736805, "score_of_first_answer": 0.01509481854736805, "max_score_of_first_answer": 0.0150948166847229, "min_score_of_first_answer": 0.0150948166847229, "avg_score": 0.01509481854736805, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.281416893005371, "lex_sim": 1.0}
{"question": "According to a popular Christmas song, who was a jolly happy soul?", "answer": ["Saint Nicolas"], "scores": [[0.008860277011990547, 0.029639683663845062]], "normalized_score": [0.016205422580242157], "tokens": [["\u2581Saint", "\u2581Nicolas"]], "entropy": [[5.764978408813477, 1.7198677062988281]], "avg_entropy": [3.7424230575561523], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Frosty The Snowman", "score_of_biggest_cluster": 0.016205422580242157, "score_of_first_answer": 0.016205422580242157, "max_score_of_first_answer": 0.029639683663845062, "min_score_of_first_answer": 0.008860277011990547, "avg_score": 0.016205422580242157, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.010389703325927258, "std_of_first_answer": 0.010389703325927258, "avg_entropy_of_first_answer": 3.7424230575561523, "lex_sim": 1.0}
{"question": "Which creature is mythically said to sing before it dies, hence an expression for a grand final performance before retirement?", "answer": ["Cossack"], "scores": [[0.03161336109042168, 0.01135191973298788, 0.7147104144096375]], "normalized_score": [0.06353651732206345], "tokens": [["\u2581C", "oss", "ack"]], "entropy": [[5.107170581817627, 3.9748916625976562, 1.7490453720092773]], "avg_entropy": [3.6103692054748535], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cob (bird)", "score_of_biggest_cluster": 0.06353651732206345, "score_of_first_answer": 0.06353651732206345, "max_score_of_first_answer": 0.7147104144096375, "min_score_of_first_answer": 0.01135191973298788, "avg_score": 0.06353651732206345, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3268953761090132, "std_of_first_answer": 0.3268953761090132, "avg_entropy_of_first_answer": 3.6103692054748535, "lex_sim": 1.0}
{"question": "Which other colour, along with yellow and green appears on the flag of Gabon?", "answer": ["Ocean blue"], "scores": [[0.0028497264720499516, 0.19790902733802795]], "normalized_score": [0.023748399689793587], "tokens": [["\u2581Ocean", "\u2581blue"]], "entropy": [[5.599696159362793, 3.122258186340332]], "avg_entropy": [4.3609771728515625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "BLUE", "score_of_biggest_cluster": 0.023748399689793587, "score_of_first_answer": 0.023748399689793587, "max_score_of_first_answer": 0.19790902733802795, "min_score_of_first_answer": 0.0028497264720499516, "avg_score": 0.023748399689793587, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.097529650432989, "std_of_first_answer": 0.097529650432989, "avg_entropy_of_first_answer": 4.3609771728515625, "lex_sim": 1.0}
{"question": "What is the name of the A1 Pacific steam locomotive which was completed in 2009 and is the first of its kind built for over 50 years?", "answer": ["Nimble"], "scores": [[0.0042800926603376865, 0.056736964732408524, 0.13230696320533752]], "normalized_score": [0.031790729612112045], "tokens": [["\u2581N", "im", "ble"]], "entropy": [[4.642744541168213, 4.766445159912109, 2.951435089111328]], "avg_entropy": [4.120208263397217], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Tornado", "score_of_biggest_cluster": 0.031790729612112045, "score_of_first_answer": 0.031790729612112045, "max_score_of_first_answer": 0.13230696320533752, "min_score_of_first_answer": 0.0042800926603376865, "avg_score": 0.031790729612112045, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05254989975498446, "std_of_first_answer": 0.05254989975498446, "avg_entropy_of_first_answer": 4.120208263397217, "lex_sim": 1.0}
{"question": "OJ presenter Stephen Fry stepped down earlier this year, who has replaced him, she is currently filming the latest series?", "answer": ["Sandi Toksvig"], "scores": [[0.2869100868701935, 0.7682732939720154, 0.6928468942642212, 0.9901492595672607, 0.9930728077888489]], "normalized_score": [0.6844096183776855], "tokens": [["\u2581Sand", "i", "\u2581Tok", "sv", "ig"]], "entropy": [[4.321541786193848, 1.1638504266738892, 2.2101902961730957, 0.08833282440900803, 0.06584679335355759]], "avg_entropy": [1.5699524253606796], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sandy Toksvig", "score_of_biggest_cluster": 0.6844096183776855, "score_of_first_answer": 0.6844096183776855, "max_score_of_first_answer": 0.9930728077888489, "min_score_of_first_answer": 0.2869100868701935, "avg_score": 0.6844096183776855, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2587425052962248, "std_of_first_answer": 0.2587425052962248, "avg_entropy_of_first_answer": 1.5699524253606796, "lex_sim": 1.0}
{"question": "Which former Taoiseach of the Republic of Ireland died in August", "answer": ["Albert, the Prince of Wales"], "scores": [[0.08074090629816055, 0.0013632329646497965, 0.010996272787451744, 0.22816304862499237, 0.16376349329948425, 0.25445428490638733]], "normalized_score": [0.04751494526863098], "tokens": [["\u2581Albert", ",", "\u2581the", "\u2581Prince", "\u2581of", "\u2581Wales"]], "entropy": [[5.003046989440918, 0.6799354553222656, 2.6787192821502686, 4.986600399017334, 1.2189997434616089, 1.691023826599121]], "avg_entropy": [2.709720949331919], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Reynolds, Albert", "score_of_biggest_cluster": 0.04751494526863098, "score_of_first_answer": 0.04751494526863098, "max_score_of_first_answer": 0.25445428490638733, "min_score_of_first_answer": 0.0013632329646497965, "avg_score": 0.04751494526863098, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09926980199523691, "std_of_first_answer": 0.09926980199523691, "avg_entropy_of_first_answer": 2.709720949331919, "lex_sim": 1.0}
{"question": "On which subject do Carol Kirkwood and Helen Willets report on BBC TV?", "answer": ["BBC TV"], "scores": [[0.10138455033302307, 0.023958832025527954]], "normalized_score": [0.04928545281291008], "tokens": [["\u2581BBC", "\u2581TV"]], "entropy": [[5.140319347381592, 2.8285388946533203]], "avg_entropy": [3.984429121017456], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Weather conditions", "score_of_biggest_cluster": 0.04928545281291008, "score_of_first_answer": 0.04928545281291008, "max_score_of_first_answer": 0.10138455033302307, "min_score_of_first_answer": 0.023958832025527954, "avg_score": 0.04928545281291008, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.03871285915374756, "std_of_first_answer": 0.03871285915374756, "avg_entropy_of_first_answer": 3.984429121017456, "lex_sim": 1.0}
{"question": "Who played 'Peter Pan' in Spielberg's 'Hook'?", "answer": ["Robin Williams"], "scores": [[0.22338387370109558, 0.7452350854873657]], "normalized_score": [0.40801164507865906], "tokens": [["\u2581Robin", "\u2581Williams"]], "entropy": [[4.7848405838012695, 1.7288093566894531]], "avg_entropy": [3.2568249702453613], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Robin Willaims", "score_of_biggest_cluster": 0.40801164507865906, "score_of_first_answer": 0.40801164507865906, "max_score_of_first_answer": 0.7452350854873657, "min_score_of_first_answer": 0.22338387370109558, "avg_score": 0.40801164507865906, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.26092560589313507, "std_of_first_answer": 0.26092560589313507, "avg_entropy_of_first_answer": 3.2568249702453613, "lex_sim": 1.0}
{"question": "What is the next in this series: Conrad Hilton, Michael Wilding, Michael Todd, Eddie Fisher, Richard Burton?", "answer": ["Elizabeth Taylor"], "scores": [[0.34658095240592957, 0.944211483001709]], "normalized_score": [0.5720539689064026], "tokens": [["\u2581Elizabeth", "\u2581Taylor"]], "entropy": [[4.011882305145264, 0.4689692258834839]], "avg_entropy": [2.240425765514374], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Richard Burton", "score_of_biggest_cluster": 0.5720539689064026, "score_of_first_answer": 0.5720539689064026, "max_score_of_first_answer": 0.944211483001709, "min_score_of_first_answer": 0.34658095240592957, "avg_score": 0.5720539689064026, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2988152652978897, "std_of_first_answer": 0.2988152652978897, "avg_entropy_of_first_answer": 2.240425765514374, "lex_sim": 1.0}
{"question": "\"The US TV series \"\"Mad Men\"\" centres around what industry or profession?\"", "answer": ["The US (country) TV (television) series Mad Men (name)"], "scores": [[0.05288856849074364, 0.24511663615703583, 0.0005471864133141935, 0.017471278086304665, 0.9406943321228027, 0.18016494810581207, 0.1266486495733261, 0.1386430859565735, 0.9246675372123718, 0.8369542360305786, 0.8832521438598633, 0.15550558269023895, 0.6861950755119324, 0.41174739599227905, 0.04850463196635246, 0.7221891283988953]], "normalized_score": [0.1676110476255417], "tokens": [["\u2581The", "\u2581US", "\u2581(", "country", ")", "\u2581TV", "\u2581(", "tele", "vision", ")", "\u2581series", "\u2581Mad", "\u2581Men", "\u2581(", "name", ")"]], "entropy": [[4.643643856048584, 4.334216117858887, 0.8053018450737, 4.542632102966309, 0.46803829073905945, 1.3915914297103882, 0.9896473288536072, 4.265276908874512, 0.4157178997993469, 1.066202998161316, 0.7452220916748047, 1.8584752082824707, 0.9969479441642761, 1.6138267517089844, 4.348432540893555, 0.8729153871536255]], "avg_entropy": [2.084880543872714], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pervasive advertising", "score_of_biggest_cluster": 0.1676110476255417, "score_of_first_answer": 0.1676110476255417, "max_score_of_first_answer": 0.9406943321228027, "min_score_of_first_answer": 0.0005471864133141935, "avg_score": 0.1676110476255417, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35381279863722487, "std_of_first_answer": 0.35381279863722487, "avg_entropy_of_first_answer": 2.084880543872714, "lex_sim": 1.0}
{"question": "Which musician founded the Red Hot Peppers?", "answer": ["Chili Peppers"], "scores": [[0.007408232428133488, 0.5316428542137146, 0.402758926153183, 0.838907778263092]], "normalized_score": [0.19099566340446472], "tokens": [["\u2581Ch", "ili", "\u2581Pe", "ppers"]], "entropy": [[5.538074493408203, 1.882566213607788, 2.7705376148223877, 0.5592330694198608]], "avg_entropy": [2.68760284781456], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ferdinand Joseph La Menthe", "score_of_biggest_cluster": 0.19099566340446472, "score_of_first_answer": 0.19099566340446472, "max_score_of_first_answer": 0.838907778263092, "min_score_of_first_answer": 0.007408232428133488, "avg_score": 0.19099566340446472, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2983039794081141, "std_of_first_answer": 0.2983039794081141, "avg_entropy_of_first_answer": 2.68760284781456, "lex_sim": 1.0}
{"question": "Which is the first written single word number in which the letter \u2018A\u2019 appears?", "answer": ["BBBB"], "scores": [[0.011084073223173618, 0.011948756873607635, 0.1670452207326889]], "normalized_score": [0.0280727781355381], "tokens": [["\u2581B", "BB", "B"]], "entropy": [[5.321443557739258, 4.871430397033691, 3.131985664367676]], "avg_entropy": [4.441619873046875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Thousand", "score_of_biggest_cluster": 0.0280727781355381, "score_of_first_answer": 0.0280727781355381, "max_score_of_first_answer": 0.1670452207326889, "min_score_of_first_answer": 0.011084073223173618, "avg_score": 0.0280727781355381, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07331783193125016, "std_of_first_answer": 0.07331783193125016, "avg_entropy_of_first_answer": 4.441619873046875, "lex_sim": 1.0}
{"question": "Which mountain range extends almost the length of Italy?", "answer": ["Alps, France"], "scores": [[0.12291457504034042, 0.7929439544677734, 0.009527081623673439, 0.003338090144097805]], "normalized_score": [0.04195907339453697], "tokens": [["\u2581Al", "ps", ",", "\u2581France"]], "entropy": [[4.606409072875977, 1.062579870223999, 1.036005973815918, 4.81453800201416]], "avg_entropy": [2.8798832297325134], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Apennine mountains", "score_of_biggest_cluster": 0.04195907339453697, "score_of_first_answer": 0.04195907339453697, "max_score_of_first_answer": 0.7929439544677734, "min_score_of_first_answer": 0.003338090144097805, "avg_score": 0.04195907339453697, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32723771462291645, "std_of_first_answer": 0.32723771462291645, "avg_entropy_of_first_answer": 2.8798832297325134, "lex_sim": 1.0}
{"question": "What was once the most abundant bird in North America but was driven to extinction in 1914?", "answer": ["The Lord of the Rings?"], "scores": [[0.06343203037977219, 0.0018142109038308263, 0.35204654932022095, 0.8962031006813049, 0.7305477857589722, 0.993591845035553, 0.00324905663728714]], "normalized_score": [0.0978078544139862], "tokens": [["\u2581The", "\u2581Lord", "\u2581of", "\u2581the", "\u2581R", "ings", "?"]], "entropy": [[4.550824165344238, 4.574646949768066, 3.190492630004883, 0.8683744072914124, 1.7335925102233887, 0.06696788966655731, 2.3398053646087646]], "avg_entropy": [2.47495770241533], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Columba canadensis", "score_of_biggest_cluster": 0.0978078544139862, "score_of_first_answer": 0.0978078544139862, "max_score_of_first_answer": 0.993591845035553, "min_score_of_first_answer": 0.0018142109038308263, "avg_score": 0.0978078544139862, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4019740680254799, "std_of_first_answer": 0.4019740680254799, "avg_entropy_of_first_answer": 2.47495770241533, "lex_sim": 1.0}
{"question": "The name of which chemical element takes its name from the Greek for light-bringing?", "answer": ["I want you to give me this information"], "scores": [[0.023675983771681786, 0.0015808705938979983, 0.07136034965515137, 0.7820245623588562, 0.017910350114107132, 0.7246866226196289, 0.007084299810230732, 0.15197992324829102]], "normalized_score": [0.04821169748902321], "tokens": [["\u2581I", "\u2581want", "\u2581you", "\u2581to", "\u2581give", "\u2581me", "\u2581this", "\u2581information"]], "entropy": [[5.733434677124023, 3.5942320823669434, 2.09420108795166, 1.305298089981079, 5.165139198303223, 1.6291487216949463, 3.294621706008911, 4.349677085876465]], "avg_entropy": [3.3957190811634064], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Phospho", "score_of_biggest_cluster": 0.04821169748902321, "score_of_first_answer": 0.04821169748902321, "max_score_of_first_answer": 0.7820245623588562, "min_score_of_first_answer": 0.0015808705938979983, "avg_score": 0.04821169748902321, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3101758580576789, "std_of_first_answer": 0.3101758580576789, "avg_entropy_of_first_answer": 3.3957190811634064, "lex_sim": 1.0}
{"question": "What was the name of the Liverpool nightclub where the Beatles regularly played?", "answer": ["The Kaiserkeller"], "scores": [[0.18923959136009216, 0.0036731320433318615, 0.9923751950263977, 0.9138072729110718]], "normalized_score": [0.1584509164094925], "tokens": [["\u2581The", "\u2581Kais", "erk", "eller"]], "entropy": [[5.484843730926514, 3.0618715286254883, 0.07493236660957336, 0.497755229473114]], "avg_entropy": [2.2798507139086723], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cavern club", "score_of_biggest_cluster": 0.1584509164094925, "score_of_first_answer": 0.1584509164094925, "max_score_of_first_answer": 0.9923751950263977, "min_score_of_first_answer": 0.0036731320433318615, "avg_score": 0.1584509164094925, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43420249279235423, "std_of_first_answer": 0.43420249279235423, "avg_entropy_of_first_answer": 2.2798507139086723, "lex_sim": 1.0}
{"question": "What according to the commercial cleaned a big, big carpet for less than half a crown", "answer": ["Windex."], "scores": [[0.01152221392840147, 0.01400662399828434, 0.005456902086734772, 0.016391703858971596, 0.031559817492961884]], "normalized_score": [0.013542992062866688], "tokens": [["\u2581W", "index", ".", "\u2581I", "\u2581think"]], "entropy": [[5.958047866821289, 3.867457628250122, 1.550485610961914, 3.506166696548462, 4.026331901550293]], "avg_entropy": [3.781697940826416], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "One Thousand and One", "score_of_biggest_cluster": 0.013542992062866688, "score_of_first_answer": 0.013542992062866688, "max_score_of_first_answer": 0.031559817492961884, "min_score_of_first_answer": 0.005456902086734772, "avg_score": 0.013542992062866688, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.008685810258602445, "std_of_first_answer": 0.008685810258602445, "avg_entropy_of_first_answer": 3.781697940826416, "lex_sim": 1.0}
{"question": "Which actor died in September 1955 after a collision in Cholame, California?", "answer": ["James Deen"], "scores": [[0.49195584654808044, 0.025693969801068306, 0.42243778705596924]], "normalized_score": [0.1747860312461853], "tokens": [["\u2581James", "\u2581De", "en"]], "entropy": [[3.7391467094421387, 1.9102180004119873, 2.595268726348877]], "avg_entropy": [2.748211145401001], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "James Dean", "score_of_biggest_cluster": 0.1747860312461853, "score_of_first_answer": 0.1747860312461853, "max_score_of_first_answer": 0.49195584654808044, "min_score_of_first_answer": 0.025693969801068306, "avg_score": 0.1747860312461853, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20538271993556725, "std_of_first_answer": 0.20538271993556725, "avg_entropy_of_first_answer": 2.748211145401001, "lex_sim": 1.0}
{"question": "Who presents the BBC quiz show \u2018Perfection\u2019?", "answer": ["Matthew Amroliwala"], "scores": [[0.01161582488566637, 0.10215020179748535, 0.9720879793167114, 0.9640788435935974, 0.8903394341468811, 0.9076338410377502]], "normalized_score": [0.3106435239315033], "tokens": [["\u2581Matthew", "\u2581Am", "ro", "li", "w", "ala"]], "entropy": [[5.769280433654785, 4.934932708740234, 0.22727185487747192, 0.2972574532032013, 0.673137903213501, 0.6591624617576599]], "avg_entropy": [2.093507135907809], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nick Knowles", "score_of_biggest_cluster": 0.3106435239315033, "score_of_first_answer": 0.3106435239315033, "max_score_of_first_answer": 0.9720879793167114, "min_score_of_first_answer": 0.01161582488566637, "avg_score": 0.3106435239315033, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4150794878069192, "std_of_first_answer": 0.4150794878069192, "avg_entropy_of_first_answer": 2.093507135907809, "lex_sim": 1.0}
{"question": "Herb of Grace is the name given to which strong- smelling herb?", "answer": ["The scrofula tree or plant"], "scores": [[0.015495149418711662, 0.005141825880855322, 0.002806629054248333, 0.8208525776863098, 0.37534958124160767, 0.0023368794936686754, 0.01526420097798109, 0.012371376156806946]], "normalized_score": [0.020434588193893433], "tokens": [["\u2581The", "\u2581sc", "ro", "ful", "a", "\u2581tree", "\u2581or", "\u2581plant"]], "entropy": [[5.790258884429932, 6.843161582946777, 1.8752259016036987, 1.2211575508117676, 1.0230993032455444, 2.932008981704712, 1.176546573638916, 5.5478291511535645]], "avg_entropy": [3.301160991191864], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rue", "score_of_biggest_cluster": 0.020434588193893433, "score_of_first_answer": 0.020434588193893433, "max_score_of_first_answer": 0.8208525776863098, "min_score_of_first_answer": 0.0023368794936686754, "avg_score": 0.020434588193893433, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27842390645199033, "std_of_first_answer": 0.27842390645199033, "avg_entropy_of_first_answer": 3.301160991191864, "lex_sim": 1.0}
{"question": "Who's missing: Bashful, Doc, Dopey, Grumpy, Happy, Sleepy", "answer": ["Snow White or Cinderella's Stepsisters"], "scores": [[0.20428155362606049, 0.898690402507782, 0.00101913430262357, 0.009719115681946278, 0.628925085067749, 0.9690931439399719, 0.05368815362453461, 0.9949859380722046, 0.10205737501382828, 0.6060993075370789, 0.9954337477684021, 0.785787045955658]], "normalized_score": [0.1941281408071518], "tokens": [["\u2581Snow", "\u2581White", "\u2581or", "\u2581C", "inder", "ella", "'", "s", "\u2581Ste", "ps", "ister", "s"]], "entropy": [[4.278401851654053, 0.8209754824638367, 1.7814624309539795, 3.681105613708496, 2.313323497772217, 0.27140697836875916, 1.7325820922851562, 0.05092991888523102, 4.0215864181518555, 0.8612472414970398, 0.04524146020412445, 0.9561538696289062]], "avg_entropy": [1.734534737964471], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Snow White and the Seven Dwarfs (Disney film)", "score_of_biggest_cluster": 0.1941281408071518, "score_of_first_answer": 0.1941281408071518, "max_score_of_first_answer": 0.9954337477684021, "min_score_of_first_answer": 0.00101913430262357, "avg_score": 0.1941281408071518, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39876696869523265, "std_of_first_answer": 0.39876696869523265, "avg_entropy_of_first_answer": 1.734534737964471, "lex_sim": 1.0}
{"question": "Who said that there are only three true sports, bullfighting, mountaineering and motor racing, the rest are merely games?", "answer": ["Enzo Ferrari"], "scores": [[0.11676173657178879, 0.9718202948570251, 0.9530181884765625, 0.9886937737464905]], "normalized_score": [0.5718239545822144], "tokens": [["\u2581En", "zo", "\u2581Ferr", "ari"]], "entropy": [[4.731161117553711, 0.21828147768974304, 0.391335666179657, 0.0831126719713211]], "avg_entropy": [1.355972733348608], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Famous at Twenty-Five Thirty a Master", "score_of_biggest_cluster": 0.5718239545822144, "score_of_first_answer": 0.5718239545822144, "max_score_of_first_answer": 0.9886937737464905, "min_score_of_first_answer": 0.11676173657178879, "avg_score": 0.5718239545822144, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37018799698424687, "std_of_first_answer": 0.37018799698424687, "avg_entropy_of_first_answer": 1.355972733348608, "lex_sim": 1.0}
{"question": "Who is the next in this series: John L. Sullivan, James J. Corbett, Bob Fitzsimmons, James J. Jeffries, Marvin Hart, ... ?", "answer": ["Gene Tunney"], "scores": [[0.1814001202583313, 0.9913867115974426, 0.9875203371047974]], "normalized_score": [0.56209397315979], "tokens": [["\u2581Gene", "\u2581Tun", "ney"]], "entropy": [[2.7250137329101562, 0.0909462720155716, 0.0919116884469986]], "avg_entropy": [0.9692905644575754], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tommy Burns (disambiguation)", "score_of_biggest_cluster": 0.56209397315979, "score_of_first_answer": 0.56209397315979, "max_score_of_first_answer": 0.9913867115974426, "min_score_of_first_answer": 0.1814001202583313, "avg_score": 0.56209397315979, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38092329805493846, "std_of_first_answer": 0.38092329805493846, "avg_entropy_of_first_answer": 0.9692905644575754, "lex_sim": 1.0}
{"question": "In which American state is Camp David, the mountain retreat of the President of the U.S.A.?", "answer": ["Macy's"], "scores": [[0.017309971153736115, 0.0028559842612594366, 0.47325006127357483, 0.9962242841720581]], "normalized_score": [0.06948240101337433], "tokens": [["\u2581M", "acy", "'", "s"]], "entropy": [[4.916971206665039, 5.08378791809082, 3.049954891204834, 0.04243557155132294]], "avg_entropy": [3.273287396878004], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Education in Maryland", "score_of_biggest_cluster": 0.06948240101337433, "score_of_first_answer": 0.06948240101337433, "max_score_of_first_answer": 0.9962242841720581, "min_score_of_first_answer": 0.0028559842612594366, "avg_score": 0.06948240101337433, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40681051489912956, "std_of_first_answer": 0.40681051489912956, "avg_entropy_of_first_answer": 3.273287396878004, "lex_sim": 1.0}
{"question": "What decoration, a Cross, was first awarded in 1995 to Corporal Wayne Mills for his actions in Bosnia?", "answer": ["The Victoria Cross of George Cross"], "scores": [[0.030760305002331734, 0.21881744265556335, 0.9895059466362, 0.004188894294202328, 0.027469459921121597, 0.5919045209884644]], "normalized_score": [0.08765605837106705], "tokens": [["\u2581The", "\u2581Victoria", "\u2581Cross", "\u2581of", "\u2581George", "\u2581Cross"]], "entropy": [[3.961699962615967, 3.694828987121582, 0.09545809775590897, 1.6726891994476318, 3.2107315063476562, 2.046693801879883]], "avg_entropy": [2.4470169258614383], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "CONSPICUOUS GALLANTRY", "score_of_biggest_cluster": 0.08765605837106705, "score_of_first_answer": 0.08765605837106705, "max_score_of_first_answer": 0.9895059466362, "min_score_of_first_answer": 0.004188894294202328, "avg_score": 0.08765605837106705, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3653378179976879, "std_of_first_answer": 0.3653378179976879, "avg_entropy_of_first_answer": 2.4470169258614383, "lex_sim": 1.0}
{"question": "The melody for which famous song was written by sisters and school teachers Patty and Mildred Hill in Kentucky in 1893?", "answer": ["Happy Birthday to You"], "scores": [[0.49723881483078003, 0.896114706993103, 0.9949862957000732, 0.9957737326622009, 0.5587238073348999, 0.8574134111404419]], "normalized_score": [0.7718785405158997], "tokens": [["\u2581Happy", "\u2581B", "irth", "day", "\u2581to", "\u2581You"]], "entropy": [[3.729248523712158, 0.5433826446533203, 0.04755362868309021, 0.042472392320632935, 1.5401555299758911, 0.49704509973526]], "avg_entropy": [1.0666429698467255], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Happy Birthday (song)", "score_of_biggest_cluster": 0.7718785405158997, "score_of_first_answer": 0.7718785405158997, "max_score_of_first_answer": 0.9957737326622009, "min_score_of_first_answer": 0.49723881483078003, "avg_score": 0.7718785405158997, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.19948261098054626, "std_of_first_answer": 0.19948261098054626, "avg_entropy_of_first_answer": 1.0666429698467255, "lex_sim": 1.0}
{"question": "What type of creature is a cichlid?", "answer": ["A perch"], "scores": [[0.0682159960269928, 0.00534231448546052, 0.8288143277168274]], "normalized_score": [0.06709512323141098], "tokens": [["\u2581A", "\u2581per", "ch"]], "entropy": [[4.621786117553711, 4.450470924377441, 0.9960614442825317]], "avg_entropy": [3.356106162071228], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fish proteins", "score_of_biggest_cluster": 0.06709512323141098, "score_of_first_answer": 0.06709512323141098, "max_score_of_first_answer": 0.8288143277168274, "min_score_of_first_answer": 0.00534231448546052, "avg_score": 0.06709512323141098, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37425022505378314, "std_of_first_answer": 0.37425022505378314, "avg_entropy_of_first_answer": 3.356106162071228, "lex_sim": 1.0}
{"question": "In medicine the MMR vaccination gives protection against which diseases?", "answer": ["Mucocutaneous measles & acute convolsive meningitis"], "scores": [[0.18276415765285492, 0.004016718361526728, 0.03137939050793648, 0.9449331164360046, 0.9908027648925781, 0.12761522829532623, 0.9896467328071594, 0.0022505763918161392, 0.0013315866235643625, 0.9231560230255127, 0.021564535796642303, 0.024013858288526535, 0.13686978816986084, 0.7411888837814331, 0.042972929775714874, 0.8860629796981812, 0.9083027243614197]], "normalized_score": [0.10186528414487839], "tokens": [["\u2581M", "uc", "oc", "ut", "aneous", "\u2581meas", "les", "\u2581&", "\u2581ac", "ute", "\u2581con", "vol", "s", "ive", "\u2581men", "ing", "itis"]], "entropy": [[4.076657295227051, 1.9762814044952393, 2.9101271629333496, 0.45878487825393677, 0.0765000581741333, 4.669322967529297, 0.09627361595630646, 2.2430148124694824, 3.66129994392395, 0.558967113494873, 4.144495964050293, 2.5262107849121094, 3.3163506984710693, 1.3299992084503174, 3.141376495361328, 0.7239470481872559, 0.6377420425415039]], "avg_entropy": [2.149844205554794], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Measles, Mumps and Rubella", "score_of_biggest_cluster": 0.10186528414487839, "score_of_first_answer": 0.10186528414487839, "max_score_of_first_answer": 0.9908027648925781, "min_score_of_first_answer": 0.0013315866235643625, "avg_score": 0.10186528414487839, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42629899924883535, "std_of_first_answer": 0.42629899924883535, "avg_entropy_of_first_answer": 2.149844205554794, "lex_sim": 1.0}
{"question": "Which conflict began in England in 1455 after King Henry VI, following a troubled reign, had a long period of mental withdrawal?", "answer": ["The War of the Roses"], "scores": [[0.14563338458538055, 0.3344433903694153, 0.9844439029693604, 0.9413551688194275, 0.9841758608818054, 0.9987529516220093]], "normalized_score": [0.5949901938438416], "tokens": [["\u2581The", "\u2581War", "\u2581of", "\u2581the", "\u2581Ros", "es"]], "entropy": [[4.056070327758789, 2.672245979309082, 0.1280517578125, 0.3116951584815979, 0.15966454148292542, 0.01391537580639124]], "avg_entropy": [1.2236071901085477], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "War of the roses", "score_of_biggest_cluster": 0.5949901938438416, "score_of_first_answer": 0.5949901938438416, "max_score_of_first_answer": 0.9987529516220093, "min_score_of_first_answer": 0.14563338458538055, "avg_score": 0.5949901938438416, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3521795651609391, "std_of_first_answer": 0.3521795651609391, "avg_entropy_of_first_answer": 1.2236071901085477, "lex_sim": 1.0}
{"question": "What is the title of Simon and Garfunkel\u2019s only UK number one hit single?", "answer": ["A Hazy Shade Of Winter"], "scores": [[0.043109942227602005, 0.07642867416143417, 0.8805904984474182, 0.9415360689163208, 0.9435338973999023, 0.07263167202472687, 0.8648706674575806]], "normalized_score": [0.2873879075050354], "tokens": [["\u2581A", "\u2581H", "azy", "\u2581Sh", "ade", "\u2581Of", "\u2581Winter"]], "entropy": [[5.266030311584473, 5.053985595703125, 0.6007227897644043, 0.5558598041534424, 0.3358430862426758, 0.37807798385620117, 1.0873808860778809]], "avg_entropy": [1.8968429224831718], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bridge Over Troubled Waters", "score_of_biggest_cluster": 0.2873879075050354, "score_of_first_answer": 0.2873879075050354, "max_score_of_first_answer": 0.9435338973999023, "min_score_of_first_answer": 0.043109942227602005, "avg_score": 0.2873879075050354, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41842979092139765, "std_of_first_answer": 0.41842979092139765, "avg_entropy_of_first_answer": 1.8968429224831718, "lex_sim": 1.0}
{"question": "'Picket's Charge' occurred in which battle of the American Civil War?", "answer": ["Battle of Gettysburg"], "scores": [[0.1650087982416153, 0.970197856426239, 0.8787586688995361, 0.9829251766204834, 0.9983245730400085, 0.9711713194847107]], "normalized_score": [0.7154086828231812], "tokens": [["\u2581Battle", "\u2581of", "\u2581G", "ett", "ys", "burg"]], "entropy": [[4.0382609367370605, 0.254533588886261, 1.0715022087097168, 0.17823919653892517, 0.019151270389556885, 0.1696072518825531]], "avg_entropy": [0.955215742190679], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "GETTYSBURG", "score_of_biggest_cluster": 0.7154086828231812, "score_of_first_answer": 0.7154086828231812, "max_score_of_first_answer": 0.9983245730400085, "min_score_of_first_answer": 0.1650087982416153, "avg_score": 0.7154086828231812, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.29884886641636915, "std_of_first_answer": 0.29884886641636915, "avg_entropy_of_first_answer": 0.955215742190679, "lex_sim": 1.0}
{"question": "Who was the first of these to become US President?", "answer": ["Harry S. Truman"], "scores": [[0.003566536121070385, 0.5660766363143921, 0.7118625640869141, 0.90761399269104, 0.9904679656028748]], "normalized_score": [0.26439419388771057], "tokens": [["\u2581Harry", "\u2581S", ".", "\u2581Tr", "uman"]], "entropy": [[5.195122718811035, 1.8065541982650757, 1.014333724975586, 0.6654084920883179, 0.0787486732006073]], "avg_entropy": [1.7520335614681244], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Father of the Constitution", "score_of_biggest_cluster": 0.26439419388771057, "score_of_first_answer": 0.26439419388771057, "max_score_of_first_answer": 0.9904679656028748, "min_score_of_first_answer": 0.003566536121070385, "avg_score": 0.26439419388771057, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.349295908168976, "std_of_first_answer": 0.349295908168976, "avg_entropy_of_first_answer": 1.7520335614681244, "lex_sim": 1.0}
{"question": "Who had an August 1969 No.1 with 'In The Year 2525 (Exordium And Terminus)'?", "answer": ["Rare Earth (band)"], "scores": [[0.010423893108963966, 0.01848026178777218, 0.7018117904663086, 0.25399380922317505, 0.750125527381897, 0.9921280145645142]], "normalized_score": [0.1716250330209732], "tokens": [["\u2581R", "are", "\u2581Earth", "\u2581(", "band", ")"]], "entropy": [[5.932564735412598, 5.691270351409912, 2.2577831745147705, 1.1086211204528809, 1.6003973484039307, 0.07512824982404709]], "avg_entropy": [2.7776274966696897], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Zager and Evans", "score_of_biggest_cluster": 0.1716250330209732, "score_of_first_answer": 0.1716250330209732, "max_score_of_first_answer": 0.9921280145645142, "min_score_of_first_answer": 0.010423893108963966, "avg_score": 0.1716250330209732, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37971941161106587, "std_of_first_answer": 0.37971941161106587, "avg_entropy_of_first_answer": 2.7776274966696897, "lex_sim": 1.0}
{"question": "George Cukor directed which 1964 film musical?", "answer": ["\u315b"], "scores": [[0.02079319767653942, 0.19954147934913635, 0.10880716890096664, 0.01208658330142498]], "normalized_score": [0.04833134263753891], "tokens": [["\u2581", "<0xE3>", "<0x85>", "<0x9B>"]], "entropy": [[3.73801851272583, 4.337978363037109, 2.6692333221435547, 3.4276061058044434]], "avg_entropy": [3.5432090759277344], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "My Fair Lady (2010 film)", "score_of_biggest_cluster": 0.04833134263753891, "score_of_first_answer": 0.04833134263753891, "max_score_of_first_answer": 0.19954147934913635, "min_score_of_first_answer": 0.01208658330142498, "avg_score": 0.04833134263753891, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07603458805412125, "std_of_first_answer": 0.07603458805412125, "avg_entropy_of_first_answer": 3.5432090759277344, "lex_sim": 0.0}
{"question": "The 'Colossus of Rhodes' was a bronze statue of who?", "answer": ["Apollonius of Rhodes"], "scores": [[0.0021446493919938803, 0.8343150615692139, 0.7005434036254883, 0.28523582220077515, 0.7381624579429626, 0.9516277313232422, 0.9924536347389221]], "normalized_score": [0.30565840005874634], "tokens": [["\u2581Ap", "oll", "on", "ius", "\u2581of", "\u2581Rh", "odes"]], "entropy": [[5.4736857414245605, 1.186018705368042, 1.4159117937088013, 2.374382495880127, 1.2336974143981934, 0.32183653116226196, 0.05388832837343216]], "avg_entropy": [1.7227744300450598], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Apollo (ballet)", "score_of_biggest_cluster": 0.30565840005874634, "score_of_first_answer": 0.30565840005874634, "max_score_of_first_answer": 0.9924536347389221, "min_score_of_first_answer": 0.0021446493919938803, "avg_score": 0.30565840005874634, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33912209419023137, "std_of_first_answer": 0.33912209419023137, "avg_entropy_of_first_answer": 1.7227744300450598, "lex_sim": 1.0}
{"question": "An isohel is a line on a map connecting two points which have the same duration of what?", "answer": ["Sunset"], "scores": [[0.13401328027248383, 0.15072256326675415]], "normalized_score": [0.1421225666999817], "tokens": [["\u2581Sun", "set"]], "entropy": [[5.36396598815918, 2.0133681297302246]], "avg_entropy": [3.688667058944702], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Solar Irradiation", "score_of_biggest_cluster": 0.1421225666999817, "score_of_first_answer": 0.1421225666999817, "max_score_of_first_answer": 0.15072256326675415, "min_score_of_first_answer": 0.13401328027248383, "avg_score": 0.1421225666999817, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.008354641497135162, "std_of_first_answer": 0.008354641497135162, "avg_entropy_of_first_answer": 3.688667058944702, "lex_sim": 1.0}
{"question": "Which superhero film, produced by Marvel Studios and written and directed by Joss whedon, was the highest grossing film world-wide in 2012 ?", "answer": ["The avengers (films)"], "scores": [[0.4994182884693146, 0.059771548956632614, 0.9927425384521484, 0.9439685940742493, 0.3440413475036621, 0.0019239656394347548, 0.8851428627967834, 0.9717886447906494]], "normalized_score": [0.25134387612342834], "tokens": [["\u2581The", "\u2581a", "ven", "gers", "\u2581(", "fil", "ms", ")"]], "entropy": [[3.1778383255004883, 1.5928981304168701, 0.08149416744709015, 0.2885744571685791, 1.580923080444336, 1.6683290004730225, 0.6962031722068787, 0.25389257073402405]], "avg_entropy": [1.167519113048911], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Avengers (cartoon)", "score_of_biggest_cluster": 0.25134387612342834, "score_of_first_answer": 0.25134387612342834, "max_score_of_first_answer": 0.9927425384521484, "min_score_of_first_answer": 0.0019239656394347548, "avg_score": 0.25134387612342834, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38986974469005214, "std_of_first_answer": 0.38986974469005214, "avg_entropy_of_first_answer": 1.167519113048911, "lex_sim": 1.0}
{"question": "In what month is Hogmanay celebrated?", "answer": ["Hogmanay"], "scores": [[0.027294384315609932, 0.5357867479324341, 0.903957188129425]], "normalized_score": [0.23644912242889404], "tokens": [["\u2581Hog", "man", "ay"]], "entropy": [[4.511260032653809, 3.1322433948516846, 0.8222199082374573]], "avg_entropy": [2.8219077785809836], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "December", "score_of_biggest_cluster": 0.23644912242889404, "score_of_first_answer": 0.23644912242889404, "max_score_of_first_answer": 0.903957188129425, "min_score_of_first_answer": 0.027294384315609932, "avg_score": 0.23644912242889404, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.35942108212484797, "std_of_first_answer": 0.35942108212484797, "avg_entropy_of_first_answer": 2.8219077785809836, "lex_sim": 1.0}
{"question": "On what river is the town of Colchester", "answer": ["COLCHESTER"], "scores": [[0.004821275360882282, 0.41867703199386597, 0.8529847860336304, 0.9827433824539185, 0.9833366274833679]], "normalized_score": [0.27811530232429504], "tokens": [["\u2581COL", "CH", "E", "ST", "ER"]], "entropy": [[5.114572525024414, 3.326821804046631, 0.9582517147064209, 0.15152081847190857, 0.1530378758907318]], "avg_entropy": [1.9408409476280213], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Colne", "score_of_biggest_cluster": 0.27811530232429504, "score_of_first_answer": 0.27811530232429504, "max_score_of_first_answer": 0.9833366274833679, "min_score_of_first_answer": 0.004821275360882282, "avg_score": 0.27811530232429504, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38282871624727305, "std_of_first_answer": 0.38282871624727305, "avg_entropy_of_first_answer": 1.9408409476280213, "lex_sim": 1.0}
{"question": "What in law, does the term consanguinity mean?", "answer": ["An immediate male heir"], "scores": [[0.004484010860323906, 0.008417900651693344, 0.0065726665779948235, 0.013251271098852158, 0.9886515736579895]], "normalized_score": [0.02006237953901291], "tokens": [["\u2581An", "\u2581immediate", "\u2581male", "\u2581he", "ir"]], "entropy": [[4.535863876342773, 3.953878879547119, 2.1532557010650635, 2.607726573944092, 0.07560577988624573]], "avg_entropy": [2.665266162157059], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "RELATED BY BLOOD", "score_of_biggest_cluster": 0.02006237953901291, "score_of_first_answer": 0.02006237953901291, "max_score_of_first_answer": 0.9886515736579895, "min_score_of_first_answer": 0.004484010860323906, "avg_score": 0.02006237953901291, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39219875819605066, "std_of_first_answer": 0.39219875819605066, "avg_entropy_of_first_answer": 2.665266162157059, "lex_sim": 1.0}
{"question": "Which is the sixth book of the New Testament, following Acts?", "answer": ["6th Book of New Test"], "scores": [[0.08759467303752899, 0.05480225756764412, 0.33288225531578064, 0.15767258405685425, 0.7432140707969666, 0.270773321390152, 0.038037654012441635]], "normalized_score": [0.15261885523796082], "tokens": [["\u2581", "6", "th", "\u2581Book", "\u2581of", "\u2581New", "\u2581Test"]], "entropy": [[4.342518329620361, 3.6352038383483887, 3.7249534130096436, 3.222294807434082, 1.1865298748016357, 1.5634658336639404, 0.31455492973327637]], "avg_entropy": [2.5699315752301897], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Epistles of St. Paul to the Romans", "score_of_biggest_cluster": 0.15261885523796082, "score_of_first_answer": 0.15261885523796082, "max_score_of_first_answer": 0.7432140707969666, "min_score_of_first_answer": 0.038037654012441635, "avg_score": 0.15261885523796082, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.22924944412633208, "std_of_first_answer": 0.22924944412633208, "avg_entropy_of_first_answer": 2.5699315752301897, "lex_sim": 1.0}
{"question": "Jackpot Counter, Ghost Drop and Drop Zone are all terms used in which UK television game show?", "answer": ["Deal or No Deal"], "scores": [[0.13981926441192627, 0.9911171197891235, 0.8783544898033142, 0.7966862916946411, 0.9885987043380737, 0.998820960521698]], "normalized_score": [0.6763831973075867], "tokens": [["\u2581De", "al", "\u2581or", "\u2581No", "\u2581De", "al"]], "entropy": [[4.640754699707031, 0.09507766366004944, 0.5895901918411255, 0.5516164898872375, 0.09991388767957687, 0.01157805323600769]], "avg_entropy": [0.9980884976685047], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Tipping Point (album)", "score_of_biggest_cluster": 0.6763831973075867, "score_of_first_answer": 0.6763831973075867, "max_score_of_first_answer": 0.998820960521698, "min_score_of_first_answer": 0.13981926441192627, "avg_score": 0.6763831973075867, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3037561364262174, "std_of_first_answer": 0.3037561364262174, "avg_entropy_of_first_answer": 0.9980884976685047, "lex_sim": 1.0}
{"question": "\"In what year did Glen Campbell have a hit with \"\"It's Only Make Believe\"\"?\"", "answer": ["2004"], "scores": [[0.44749289751052856, 0.10580236464738846, 0.8160476088523865, 0.46491116285324097, 0.09428045898675919]], "normalized_score": [0.27909862995147705], "tokens": [["\u2581", "2", "0", "0", "4"]], "entropy": [[3.531754970550537, 1.5851187705993652, 1.0897116661071777, 1.208519697189331, 2.4051151275634766]], "avg_entropy": [1.9640440464019775], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and seventy", "score_of_biggest_cluster": 0.27909862995147705, "score_of_first_answer": 0.27909862995147705, "max_score_of_first_answer": 0.8160476088523865, "min_score_of_first_answer": 0.09428045898675919, "avg_score": 0.27909862995147705, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26779091465943206, "std_of_first_answer": 0.26779091465943206, "avg_entropy_of_first_answer": 1.9640440464019775, "lex_sim": 1.0}
{"question": "The characters Denisov and Dolokhov appear in which famous novel?", "answer": ["War and Peace (novel)"], "scores": [[0.43934082984924316, 0.9374321699142456, 0.9724613428115845, 0.4448879659175873, 0.9429246187210083, 0.9991023540496826, 0.9697687029838562]], "normalized_score": [0.7715680003166199], "tokens": [["\u2581War", "\u2581and", "\u2581Peace", "\u2581(", "nov", "el", ")"]], "entropy": [[4.062090873718262, 0.369284063577652, 0.1972714215517044, 1.0243937969207764, 0.4671862721443176, 0.008926240727305412, 0.1801399141550064]], "avg_entropy": [0.9013275118278605], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "War+Peace", "score_of_biggest_cluster": 0.7715680003166199, "score_of_first_answer": 0.7715680003166199, "max_score_of_first_answer": 0.9991023540496826, "min_score_of_first_answer": 0.43934082984924316, "avg_score": 0.7715680003166199, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2366727869204226, "std_of_first_answer": 0.2366727869204226, "avg_entropy_of_first_answer": 0.9013275118278605, "lex_sim": 1.0}
{"question": "Who was the last woman hanged in Britain", "answer": ["Ruth Ellis"], "scores": [[0.18346141278743744, 0.8475205302238464, 0.9972853660583496]], "normalized_score": [0.5372439026832581], "tokens": [["\u2581Ruth", "\u2581Ell", "is"]], "entropy": [[5.16111946105957, 1.1417157649993896, 0.027662405744194984]], "avg_entropy": [2.1101658772677183], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "David Blakely", "score_of_biggest_cluster": 0.5372439026832581, "score_of_first_answer": 0.5372439026832581, "max_score_of_first_answer": 0.9972853660583496, "min_score_of_first_answer": 0.18346141278743744, "avg_score": 0.5372439026832581, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35366547980015334, "std_of_first_answer": 0.35366547980015334, "avg_entropy_of_first_answer": 2.1101658772677183, "lex_sim": 1.0}
{"question": "Kenneth Wolstenholme was a commentator for which sport?", "answer": ["Association Football"], "scores": [[0.06541518867015839, 0.1966015249490738]], "normalized_score": [0.11340513825416565], "tokens": [["\u2581Association", "\u2581Football"]], "entropy": [[5.065580368041992, 0.7682541608810425]], "avg_entropy": [2.9169172644615173], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "FOOTBALL", "score_of_biggest_cluster": 0.11340513825416565, "score_of_first_answer": 0.11340513825416565, "max_score_of_first_answer": 0.1966015249490738, "min_score_of_first_answer": 0.06541518867015839, "avg_score": 0.11340513825416565, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0655931681394577, "std_of_first_answer": 0.0655931681394577, "avg_entropy_of_first_answer": 2.9169172644615173, "lex_sim": 1.0}
{"question": "\"Who played the part of the tragic Jennifer Cavalleri in the 1970 film, \"\"Love Story\"\"?\"", "answer": ["Ali McGraw"], "scores": [[0.1522725522518158, 0.154693141579628, 0.9904036521911621]], "normalized_score": [0.2857381999492645], "tokens": [["\u2581Ali", "\u2581McG", "raw"]], "entropy": [[5.140551567077637, 1.7575736045837402, 0.08848535269498825]], "avg_entropy": [2.328870174785455], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Ali McGraw", "score_of_biggest_cluster": 0.2857381999492645, "score_of_first_answer": 0.2857381999492645, "max_score_of_first_answer": 0.9904036521911621, "min_score_of_first_answer": 0.1522725522518158, "avg_score": 0.2857381999492645, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.39452948875630794, "std_of_first_answer": 0.39452948875630794, "avg_entropy_of_first_answer": 2.328870174785455, "lex_sim": 1.0}
{"question": "Terry Molloy, David Gooderson and Julian Bleach have all portrayed which villain in the UK television series Dr Who?", "answer": ["Davros, the \"Creator of the Daleks\""], "scores": [[0.8518927097320557, 0.9855658411979675, 0.010896079242229462, 0.2898387908935547, 0.001092779915779829, 0.1211046427488327, 0.9990130662918091, 0.7334519624710083, 0.6273530721664429, 0.9357099533081055, 0.9973830580711365, 0.9863789081573486, 0.8656426668167114]], "normalized_score": [0.2950702905654907], "tokens": [["\u2581Dav", "ros", ",", "\u2581the", "\u2581\"", "Cre", "ator", "\u2581of", "\u2581the", "\u2581D", "ale", "ks", "\""]], "entropy": [[1.068182110786438, 0.13828612864017487, 0.961251437664032, 3.681826591491699, 2.4502782821655273, 2.345977306365967, 0.00939575769007206, 0.7452565431594849, 1.2117116451263428, 0.5579801797866821, 0.030463196337223053, 0.07522891461849213, 0.715122640132904]], "avg_entropy": [1.0762277487665415], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Davros", "score_of_biggest_cluster": 0.2950702905654907, "score_of_first_answer": 0.2950702905654907, "max_score_of_first_answer": 0.9990130662918091, "min_score_of_first_answer": 0.001092779915779829, "avg_score": 0.2950702905654907, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.38063078109529597, "std_of_first_answer": 0.38063078109529597, "avg_entropy_of_first_answer": 1.0762277487665415, "lex_sim": 1.0}
{"question": "What is the common name of the garden weed urtica dioica?", "answer": ["Nettle"], "scores": [[0.18300800025463104, 0.9946948885917664, 0.7845337986946106]], "normalized_score": [0.5227056741714478], "tokens": [["\u2581N", "ett", "le"]], "entropy": [[4.979971885681152, 0.05913396179676056, 0.8029987812042236]], "avg_entropy": [1.947368209560712], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Nettle", "score_of_biggest_cluster": 0.5227056741714478, "score_of_first_answer": 0.5227056741714478, "max_score_of_first_answer": 0.9946948885917664, "min_score_of_first_answer": 0.18300800025463104, "avg_score": 0.5227056741714478, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.34396972970727085, "std_of_first_answer": 0.34396972970727085, "avg_entropy_of_first_answer": 1.947368209560712, "lex_sim": 1.0}
{"question": "\u2018Make Them Wait\u2019 is rhyming slang for which Bingo number?", "answer": ["Seventy-eight"], "scores": [[0.05067131295800209, 0.9856510758399963, 0.8936336636543274, 0.41436171531677246, 0.0640401765704155, 0.9964626431465149]], "normalized_score": [0.3250795602798462], "tokens": [["\u2581Se", "vent", "y", "-", "e", "ight"]], "entropy": [[5.176223278045654, 0.1365601122379303, 0.3727770149707794, 2.4686784744262695, 2.73125958442688, 0.030427072197198868]], "avg_entropy": [1.819320922717452], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "58", "score_of_biggest_cluster": 0.3250795602798462, "score_of_first_answer": 0.3250795602798462, "max_score_of_first_answer": 0.9964626431465149, "min_score_of_first_answer": 0.05067131295800209, "avg_score": 0.3250795602798462, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4101336067751177, "std_of_first_answer": 0.4101336067751177, "avg_entropy_of_first_answer": 1.819320922717452, "lex_sim": 1.0}
{"question": "Which English football league club is based in Burslem?", "answer": ["Stoke City"], "scores": [[0.12129709124565125, 0.981503963470459, 0.6228182911872864]], "normalized_score": [0.42011478543281555], "tokens": [["\u2581Sto", "ke", "\u2581City"]], "entropy": [[3.2218973636627197, 0.16003325581550598, 1.7426979541778564]], "avg_entropy": [1.7082095245520275], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Port Vale F.C", "score_of_biggest_cluster": 0.42011478543281555, "score_of_first_answer": 0.42011478543281555, "max_score_of_first_answer": 0.981503963470459, "min_score_of_first_answer": 0.12129709124565125, "avg_score": 0.42011478543281555, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.352788068032497, "std_of_first_answer": 0.352788068032497, "avg_entropy_of_first_answer": 1.7082095245520275, "lex_sim": 1.0}
{"question": "Which man won a record 26 Academy Award 'Oscars'?", "answer": ["Daniel Daniel"], "scores": [[0.003445844864472747, 0.0028348169289529324]], "normalized_score": [0.003125434974208474], "tokens": [["\u2581Daniel", "\u2581Daniel"]], "entropy": [[5.2646355628967285, 4.237781524658203]], "avg_entropy": [4.751208543777466], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Walt DisneyY", "score_of_biggest_cluster": 0.003125434974208474, "score_of_first_answer": 0.003125434974208474, "max_score_of_first_answer": 0.003445844864472747, "min_score_of_first_answer": 0.0028348169289529324, "avg_score": 0.003125434974208474, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.00030551396775990725, "std_of_first_answer": 0.00030551396775990725, "avg_entropy_of_first_answer": 4.751208543777466, "lex_sim": 1.0}
{"question": "Which British Victorian writer lived with George Lewes for 24 years but never married him?", "answer": ["2d 74a"], "scores": [[0.03646009787917137, 0.06358496844768524, 0.0010357731953263283, 0.02656957320868969, 0.019984742626547813, 0.011287085711956024, 0.014331653714179993]], "normalized_score": [0.015408884733915329], "tokens": [["\u2581", "2", "d", "\u2581", "7", "4", "a"]], "entropy": [[4.933994293212891, 4.754745006561279, 2.551856517791748, 4.45516300201416, 2.566310405731201, 4.000475883483887, 4.368431568145752]], "avg_entropy": [3.947282382420131], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mary Ann Evans", "score_of_biggest_cluster": 0.015408884733915329, "score_of_first_answer": 0.015408884733915329, "max_score_of_first_answer": 0.06358496844768524, "min_score_of_first_answer": 0.0010357731953263283, "avg_score": 0.015408884733915329, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.018986672929055256, "std_of_first_answer": 0.018986672929055256, "avg_entropy_of_first_answer": 3.947282382420131, "lex_sim": 1.0}
{"question": "What nationality is singer Rhianna?", "answer": ["Barbadian"], "scores": [[0.3024061322212219, 0.9187321662902832, 0.9760742783546448]], "normalized_score": [0.6472729444503784], "tokens": [["\u2581Bar", "bad", "ian"]], "entropy": [[4.589770793914795, 0.678074300289154, 0.19427338242530823]], "avg_entropy": [1.820706158876419], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Barbadian", "score_of_biggest_cluster": 0.6472729444503784, "score_of_first_answer": 0.6472729444503784, "max_score_of_first_answer": 0.9760742783546448, "min_score_of_first_answer": 0.3024061322212219, "avg_score": 0.6472729444503784, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.30495439899582477, "std_of_first_answer": 0.30495439899582477, "avg_entropy_of_first_answer": 1.820706158876419, "lex_sim": 1.0}
{"question": "\"What movie featured the song \"\"Moon River\"\"?\"", "answer": ["Breakfast at Tiffany's (film)"], "scores": [[0.33428269624710083, 0.9851825833320618, 0.9276618957519531, 0.9857117533683777, 0.995274543762207, 0.9453445076942444, 0.9084247946739197, 0.998316764831543, 0.4471995532512665, 0.4969463348388672, 0.9841795563697815]], "normalized_score": [0.769733726978302], "tokens": [["\u2581Break", "fast", "\u2581at", "\u2581T", "iff", "any", "'", "s", "\u2581(", "film", ")"]], "entropy": [[4.032878398895264, 0.1506451666355133, 0.36913609504699707, 0.13948173820972443, 0.040497004985809326, 0.24999433755874634, 0.5318934321403503, 0.018352339044213295, 1.1919403076171875, 1.8126220703125, 0.136470764875412]], "avg_entropy": [0.7885374232110652], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Breakfast At Tiffanys", "score_of_biggest_cluster": 0.769733726978302, "score_of_first_answer": 0.769733726978302, "max_score_of_first_answer": 0.998316764831543, "min_score_of_first_answer": 0.33428269624710083, "avg_score": 0.769733726978302, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.24468775614708035, "std_of_first_answer": 0.24468775614708035, "avg_entropy_of_first_answer": 0.7885374232110652, "lex_sim": 1.0}
{"question": "Cleeve Hill is the highest point in which range of English hills?", "answer": ["The Chilterns"], "scores": [[0.05274899676442146, 0.13665816187858582, 0.6707996129989624, 0.9868178367614746, 0.7255448698997498]], "normalized_score": [0.3220095634460449], "tokens": [["\u2581The", "\u2581Ch", "il", "tern", "s"]], "entropy": [[4.683113098144531, 4.183764934539795, 2.10782527923584, 0.11604715138673782, 0.8583921790122986]], "avg_entropy": [2.3898285284638403], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cotteswold Hills", "score_of_biggest_cluster": 0.3220095634460449, "score_of_first_answer": 0.3220095634460449, "max_score_of_first_answer": 0.9868178367614746, "min_score_of_first_answer": 0.05274899676442146, "avg_score": 0.3220095634460449, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3600089131441753, "std_of_first_answer": 0.3600089131441753, "avg_entropy_of_first_answer": 2.3898285284638403, "lex_sim": 1.0}
{"question": "What is the generic word for plants that grow in water or water-logged conditions?", "answer": ["Aquatic plant"], "scores": [[0.2351657897233963, 0.7945023775100708, 0.15675553679466248]], "normalized_score": [0.308245986700058], "tokens": [["\u2581Aqu", "atic", "\u2581plant"]], "entropy": [[3.823786735534668, 1.0563838481903076, 2.502115249633789]], "avg_entropy": [2.4607619444529214], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Hydrophyte", "score_of_biggest_cluster": 0.308245986700058, "score_of_first_answer": 0.308245986700058, "max_score_of_first_answer": 0.7945023775100708, "min_score_of_first_answer": 0.15675553679466248, "avg_score": 0.308245986700058, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.28396529894487554, "std_of_first_answer": 0.28396529894487554, "avg_entropy_of_first_answer": 2.4607619444529214, "lex_sim": 1.0}
{"question": "What abolitionist raided Harpers Ferry in 1859?", "answer": ["John Brown's Body's raids"], "scores": [[0.41601401567459106, 0.8610832095146179, 0.12375783920288086, 0.9985223412513733, 0.0930885374546051, 0.000477106892503798, 0.7594287395477295, 0.06347603350877762, 0.010884733870625496]], "normalized_score": [0.10034625232219696], "tokens": [["\u2581John", "\u2581Brown", "'", "s", "\u2581Body", "'", "s", "\u2581ra", "ids"]], "entropy": [[4.467795372009277, 1.1887993812561035, 1.4583606719970703, 0.016906723380088806, 2.510981321334839, 1.1032822132110596, 0.8743467330932617, 4.562342643737793, 0.5586062669754028]], "avg_entropy": [1.8601579252216551], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John H. Brown", "score_of_biggest_cluster": 0.10034625232219696, "score_of_first_answer": 0.10034625232219696, "max_score_of_first_answer": 0.9985223412513733, "min_score_of_first_answer": 0.000477106892503798, "avg_score": 0.10034625232219696, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37811769823621827, "std_of_first_answer": 0.37811769823621827, "avg_entropy_of_first_answer": 1.8601579252216551, "lex_sim": 1.0}
{"question": "What Canadian-born teen idol of the 1950s-60s wrote the lyrics to Sinatra's song My Way?", "answer": ["Paul Anka"], "scores": [[0.49755287170410156, 0.7613593935966492, 0.9789111614227295]], "normalized_score": [0.7184404134750366], "tokens": [["\u2581Paul", "\u2581An", "ka"]], "entropy": [[3.803250551223755, 1.9864298105239868, 0.19765713810920715]], "avg_entropy": [1.995779166618983], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "\u0628\u0648\u0644 \u0645\u0635\u0637\u0641\u0649 \u0639\u0628\u062f\u064a \u0623\u0646\u0643\u0627", "score_of_biggest_cluster": 0.7184404134750366, "score_of_first_answer": 0.7184404134750366, "max_score_of_first_answer": 0.9789111614227295, "min_score_of_first_answer": 0.49755287170410156, "avg_score": 0.7184404134750366, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.19681589134070857, "std_of_first_answer": 0.19681589134070857, "avg_entropy_of_first_answer": 1.995779166618983, "lex_sim": 1.0}
{"question": "Who bought Chelsea football club for \u00a31 in 1982?", "answer": ["Ken Bates"], "scores": [[0.3264613449573517, 0.7309473752975464, 0.9782710671424866]], "normalized_score": [0.6157329082489014], "tokens": [["\u2581Ken", "\u2581B", "ates"]], "entropy": [[4.6483612060546875, 2.1147301197052, 0.18943466246128082]], "avg_entropy": [2.3175086627403894], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Ken Bates", "score_of_biggest_cluster": 0.6157329082489014, "score_of_first_answer": 0.6157329082489014, "max_score_of_first_answer": 0.9782710671424866, "min_score_of_first_answer": 0.3264613449573517, "avg_score": 0.6157329082489014, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.268666226472601, "std_of_first_answer": 0.268666226472601, "avg_entropy_of_first_answer": 2.3175086627403894, "lex_sim": 1.0}
{"question": "Which English cricketer had figures for bowling 10 for 53 against Australia 1956?", "answer": ["Peter Brian Tayfield"], "scores": [[0.020349958911538124, 0.0198516882956028, 0.2953602373600006, 0.10519073903560638, 0.08166181296110153]], "normalized_score": [0.06340768933296204], "tokens": [["\u2581Peter", "\u2581Brian", "\u2581T", "ay", "field"]], "entropy": [[4.680976867675781, 4.469826698303223, 3.4832935333251953, 2.7831788063049316, 2.6233272552490234]], "avg_entropy": [3.608120632171631], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jim Laker", "score_of_biggest_cluster": 0.06340768933296204, "score_of_first_answer": 0.06340768933296204, "max_score_of_first_answer": 0.2953602373600006, "min_score_of_first_answer": 0.0198516882956028, "avg_score": 0.06340768933296204, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10118918394584381, "std_of_first_answer": 0.10118918394584381, "avg_entropy_of_first_answer": 3.608120632171631, "lex_sim": 1.0}
{"question": "\"Who was the fourth (and the first female) Prime Minister of the State of Israel, who was described as the \"\"Iron Lady\"\" (before Margaret Thatcher), \"\"the best man in the government\"\" and the \"\"strong-willed, straight-talking, grey-bunned grandmother of the Jewish people\"\"?\"", "answer": ["Golda Meir"], "scores": [[0.5542712211608887, 0.9590549468994141, 0.8669114708900452, 0.9866676330566406]], "normalized_score": [0.8211601376533508], "tokens": [["\u2581Gold", "a", "\u2581Me", "ir"]], "entropy": [[3.155181407928467, 0.3448140025138855, 0.9486511945724487, 0.10990040004253387]], "avg_entropy": [1.1396367512643337], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Golda Meir", "score_of_biggest_cluster": 0.8211601376533508, "score_of_first_answer": 0.8211601376533508, "max_score_of_first_answer": 0.9866676330566406, "min_score_of_first_answer": 0.5542712211608887, "avg_score": 0.8211601376533508, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.17178381515340918, "std_of_first_answer": 0.17178381515340918, "avg_entropy_of_first_answer": 1.1396367512643337, "lex_sim": 1.0}
{"question": "Which opera by Rossini , his last , is based on a play by Schiller ?", "answer": ["Gioacchino Rossini in The Assedio di Corinto"], "scores": [[0.009875440038740635, 0.4898757040500641, 0.20391707122325897, 0.9906875491142273, 0.9907795190811157, 0.7275491952896118, 0.9770454168319702, 0.001195749850012362, 0.00987696647644043, 0.0050980569794774055, 0.6029584407806396, 0.9772623777389526, 0.8959236741065979, 0.8415752053260803, 0.9861578941345215]], "normalized_score": [0.1922815591096878], "tokens": [["\u2581G", "io", "ac", "ch", "ino", "\u2581Ross", "ini", "\u2581in", "\u2581The", "\u2581Ass", "ed", "io", "\u2581di", "\u2581Cor", "into"]], "entropy": [[4.492368698120117, 3.4950389862060547, 1.852276086807251, 0.08683644980192184, 0.07987901568412781, 0.912825345993042, 0.17607200145721436, 2.4803338050842285, 4.423300743103027, 4.010704040527344, 1.6222255229949951, 0.23175780475139618, 0.547555148601532, 1.1060688495635986, 0.09814520925283432]], "avg_entropy": [1.7076925138632457], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "There But for the Grace of Grace", "score_of_biggest_cluster": 0.1922815591096878, "score_of_first_answer": 0.1922815591096878, "max_score_of_first_answer": 0.9907795190811157, "min_score_of_first_answer": 0.001195749850012362, "avg_score": 0.1922815591096878, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4060805549840812, "std_of_first_answer": 0.4060805549840812, "avg_entropy_of_first_answer": 1.7076925138632457, "lex_sim": 1.0}
{"question": "What name links an American singer and a Nottinghamshire and England cricketer?", "answer": ["Jolly Holiday"], "scores": [[0.009016342461109161, 0.007004063576459885, 0.0166633278131485, 0.8184105157852173]], "normalized_score": [0.03046340122818947], "tokens": [["\u2581J", "olly", "\u2581Hol", "iday"]], "entropy": [[5.846498489379883, 4.179081916809082, 4.211060047149658, 0.7766956090927124]], "avg_entropy": [3.753334015607834], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Taylor, James", "score_of_biggest_cluster": 0.03046340122818947, "score_of_first_answer": 0.03046340122818947, "max_score_of_first_answer": 0.8184105157852173, "min_score_of_first_answer": 0.007004063576459885, "avg_score": 0.03046340122818947, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.349683226198639, "std_of_first_answer": 0.349683226198639, "avg_entropy_of_first_answer": 3.753334015607834, "lex_sim": 1.0}
{"question": "The town of Keswick in Cumbria lies just north of which body of water?", "answer": ["Llangollen International Musical Eisteddfod"], "scores": [[0.0336042158305645, 0.004548691678792238, 0.6066932678222656, 0.9781433343887329, 0.023219717666506767, 0.49869146943092346, 0.9572781324386597, 0.9991264939308167, 0.9575623273849487, 0.9917747974395752]], "normalized_score": [0.25001034140586853], "tokens": [["\u2581L", "lang", "ol", "len", "\u2581International", "\u2581Musical", "\u2581E", "isted", "df", "od"]], "entropy": [[4.612651824951172, 3.414226531982422, 2.3846583366394043, 0.20389601588249207, 2.75327467918396, 1.1894855499267578, 0.31110215187072754, 0.008376847952604294, 0.22202128171920776, 0.062396422028541565]], "avg_entropy": [1.5162089642137289], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Derwentwater", "score_of_biggest_cluster": 0.25001034140586853, "score_of_first_answer": 0.25001034140586853, "max_score_of_first_answer": 0.9991264939308167, "min_score_of_first_answer": 0.004548691678792238, "avg_score": 0.25001034140586853, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4158574280608618, "std_of_first_answer": 0.4158574280608618, "avg_entropy_of_first_answer": 1.5162089642137289, "lex_sim": 1.0}
{"question": "In the human body what is the Axilla commonly known as?", "answer": ["The underarm area; specifically the area from the armpit to the elbow"], "scores": [[0.08154015243053436, 0.047409538179636, 0.796474814414978, 0.04153171554207802, 0.004111416172236204, 0.01128680445253849, 0.46060341596603394, 0.41418933868408203, 0.15639962255954742, 0.871174156665802, 0.3513881266117096, 0.9999949932098389, 0.9957591891288757, 0.9653946757316589, 0.921376645565033, 0.369762659072876, 0.9986814856529236]], "normalized_score": [0.2331136167049408], "tokens": [["\u2581The", "\u2581under", "arm", "\u2581area", ";", "\u2581specifically", "\u2581the", "\u2581area", "\u2581from", "\u2581the", "\u2581ar", "mp", "it", "\u2581to", "\u2581the", "\u2581el", "bow"]], "entropy": [[4.417146682739258, 4.0193634033203125, 0.8534349203109741, 0.8928011655807495, 1.3783843517303467, 3.6887922286987305, 1.7078351974487305, 2.8733527660369873, 2.3764028549194336, 0.7744580507278442, 3.201730728149414, 8.371176954824477e-05, 0.027826782315969467, 0.2563319802284241, 0.46880075335502625, 2.529736042022705, 0.010426992550492287]], "avg_entropy": [1.733935800700291], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Underarm", "score_of_biggest_cluster": 0.2331136167049408, "score_of_first_answer": 0.2331136167049408, "max_score_of_first_answer": 0.9999949932098389, "min_score_of_first_answer": 0.004111416172236204, "avg_score": 0.2331136167049408, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3914658476903647, "std_of_first_answer": 0.3914658476903647, "avg_entropy_of_first_answer": 1.733935800700291, "lex_sim": 1.0}
{"question": "Which American state has been nicknamed the Equality State because in 1869 it granted voting rights to women as well as men?", "answer": ["Wyoming"], "scores": [[0.5084124207496643, 0.958265483379364]], "normalized_score": [0.6979928612709045], "tokens": [["\u2581Wy", "oming"]], "entropy": [[3.6070642471313477, 0.3238690495491028]], "avg_entropy": [1.9654666483402252], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "State of Wyoming", "score_of_biggest_cluster": 0.6979928612709045, "score_of_first_answer": 0.6979928612709045, "max_score_of_first_answer": 0.958265483379364, "min_score_of_first_answer": 0.5084124207496643, "avg_score": 0.6979928612709045, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.22492653131484985, "std_of_first_answer": 0.22492653131484985, "avg_entropy_of_first_answer": 1.9654666483402252, "lex_sim": 1.0}
{"question": "What was the middle name of Senator Edward M Kennedy, who died in August 2009?", "answer": ["36!"], "scores": [[0.01665417104959488, 0.02560136653482914, 0.04581698030233383, 0.0014068027958273888]], "normalized_score": [0.012875420972704887], "tokens": [["\u2581", "3", "6", "!"]], "entropy": [[5.845410346984863, 4.779493808746338, 4.028165817260742, 3.5487968921661377]], "avg_entropy": [4.55046671628952], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Moore Theater Building", "score_of_biggest_cluster": 0.012875420972704887, "score_of_first_answer": 0.012875420972704887, "max_score_of_first_answer": 0.04581698030233383, "min_score_of_first_answer": 0.0014068027958273888, "avg_score": 0.012875420972704887, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.016064938161072982, "std_of_first_answer": 0.016064938161072982, "avg_entropy_of_first_answer": 4.55046671628952, "lex_sim": 1.0}
{"question": "What is the Latin motto of the film company Metro-Goldwyn-Mayer has the motto?", "answer": ["Art for art."], "scores": [[0.0898929312825203, 0.9028695821762085, 0.40501266717910767, 3.161043196087121e-06, 0.9129984378814697, 0.9865593910217285, 0.06145795062184334]], "normalized_score": [0.06650151312351227], "tokens": [["\u2581Art", "\u2581for", "\u2581art", ".'", "s", "\u2581sake", "."]], "entropy": [[4.9006147384643555, 0.7386301755905151, 1.995866298675537, 0.40350043773651123, 0.4610322117805481, 0.11657301336526871, 0.7538495063781738]], "avg_entropy": [1.338580911712987], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Art for art's sake", "score_of_biggest_cluster": 0.06650151312351227, "score_of_first_answer": 0.06650151312351227, "max_score_of_first_answer": 0.9865593910217285, "min_score_of_first_answer": 3.161043196087121e-06, "avg_score": 0.06650151312351227, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4116672680850565, "std_of_first_answer": 0.4116672680850565, "avg_entropy_of_first_answer": 1.338580911712987, "lex_sim": 1.0}
{"question": "Who is universally accepted to be the first Roman Emperor?", "answer": ["Gaius Octavianus Thurinus"], "scores": [[0.06009690836071968, 0.9024147391319275, 0.9975984692573547, 0.49999743700027466, 0.9961119294166565, 0.13635051250457764, 0.47687944769859314, 0.33616402745246887, 0.9909940361976624, 0.9568008780479431]], "normalized_score": [0.47282442450523376], "tokens": [["\u2581G", "ai", "us", "\u2581Oct", "av", "ian", "us", "\u2581Th", "ur", "inus"]], "entropy": [[4.749834060668945, 0.8199180960655212, 0.027529265731573105, 1.447835922241211, 0.03522191941738129, 0.5179859399795532, 1.8291544914245605, 1.8810722827911377, 0.09076111018657684, 0.2507163882255554]], "avg_entropy": [1.1650029476732016], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "GAIVS\u2022IVLIVS\u2022CAESAR\u2022OCTAVIANVS", "score_of_biggest_cluster": 0.47282442450523376, "score_of_first_answer": 0.47282442450523376, "max_score_of_first_answer": 0.9975984692573547, "min_score_of_first_answer": 0.06009690836071968, "avg_score": 0.47282442450523376, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.35712352640403316, "std_of_first_answer": 0.35712352640403316, "avg_entropy_of_first_answer": 1.1650029476732016, "lex_sim": 1.0}
{"question": "Which branch of the US federal police, the oldest, is charged with court security, prisoner transport, serving arrest warrants and seeking fugitives?", "answer": ["Federal Bureau of Prisons"], "scores": [[0.04787710681557655, 0.41151854395866394, 0.9878894686698914, 0.46373140811920166, 0.9497255086898804]], "normalized_score": [0.38602733612060547], "tokens": [["\u2581Federal", "\u2581Bureau", "\u2581of", "\u2581Pr", "isons"]], "entropy": [[4.402759552001953, 3.2131667137145996, 0.09964501857757568, 1.8037068843841553, 0.22079990804195404]], "avg_entropy": [1.9480156153440475], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "U.S. Marshals Service", "score_of_biggest_cluster": 0.38602733612060547, "score_of_first_answer": 0.38602733612060547, "max_score_of_first_answer": 0.9878894686698914, "min_score_of_first_answer": 0.04787710681557655, "avg_score": 0.38602733612060547, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3543505870809309, "std_of_first_answer": 0.3543505870809309, "avg_entropy_of_first_answer": 1.9480156153440475, "lex_sim": 1.0}
{"question": "In the book by James Hilton what was the full surname of the character Mr Chips?", "answer": ["2nd World War"], "scores": [[0.024367215111851692, 0.04453795775771141, 0.019407739862799644, 0.029719842597842216, 0.9055951237678528]], "normalized_score": [0.05632462725043297], "tokens": [["\u2581", "2", "nd", "\u2581World", "\u2581War"]], "entropy": [[5.279146194458008, 4.646320819854736, 3.139474391937256, 5.877284049987793, 0.7994499206542969]], "avg_entropy": [3.948335075378418], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chepying", "score_of_biggest_cluster": 0.05632462725043297, "score_of_first_answer": 0.05632462725043297, "max_score_of_first_answer": 0.9055951237678528, "min_score_of_first_answer": 0.019407739862799644, "avg_score": 0.05632462725043297, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3505358873162383, "std_of_first_answer": 0.3505358873162383, "avg_entropy_of_first_answer": 3.948335075378418, "lex_sim": 1.0}
{"question": "What type of creature is a discus?", "answer": ["Squamate"], "scores": [[0.008055451326072216, 0.1037805825471878, 0.06544844061136246, 0.5895177125930786]], "normalized_score": [0.07536167651414871], "tokens": [["\u2581S", "qu", "am", "ate"]], "entropy": [[4.99521541595459, 4.379725456237793, 1.7398667335510254, 1.5196200609207153]], "avg_entropy": [3.158606916666031], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fish proteins", "score_of_biggest_cluster": 0.07536167651414871, "score_of_first_answer": 0.07536167651414871, "max_score_of_first_answer": 0.5895177125930786, "min_score_of_first_answer": 0.008055451326072216, "avg_score": 0.07536167651414871, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23219255636354313, "std_of_first_answer": 0.23219255636354313, "avg_entropy_of_first_answer": 3.158606916666031, "lex_sim": 1.0}
{"question": "Which Japanese island that has its capital at Sapporo is the traditional home of the Ainu people?", "answer": ["Hokkaido"], "scores": [[0.4950083792209625, 0.9005655646324158, 0.9822521209716797, 0.8495057821273804]], "normalized_score": [0.7809609174728394], "tokens": [["\u2581H", "ok", "ka", "ido"]], "entropy": [[3.7481534481048584, 0.7928072214126587, 0.16793422400951385, 0.5120657086372375]], "avg_entropy": [1.3052401505410671], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Hokkaidou prefecture", "score_of_biggest_cluster": 0.7809609174728394, "score_of_first_answer": 0.7809609174728394, "max_score_of_first_answer": 0.9822521209716797, "min_score_of_first_answer": 0.4950083792209625, "avg_score": 0.7809609174728394, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.18615398339380382, "std_of_first_answer": 0.18615398339380382, "avg_entropy_of_first_answer": 1.3052401505410671, "lex_sim": 1.0}
{"question": "What was painter Rubens first name?", "answer": ["Botticelle"], "scores": [[0.005795971490442753, 0.02887907810509205, 0.4541565179824829, 0.005190693773329258]], "normalized_score": [0.02506311610341072], "tokens": [["\u2581B", "ott", "ic", "elle"]], "entropy": [[5.559926986694336, 6.069209098815918, 1.7538282871246338, 0.3936813473701477]], "avg_entropy": [3.444161430001259], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Peter (consul)", "score_of_biggest_cluster": 0.02506311610341072, "score_of_first_answer": 0.02506311610341072, "max_score_of_first_answer": 0.4541565179824829, "min_score_of_first_answer": 0.005190693773329258, "avg_score": 0.02506311610341072, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19114011999568878, "std_of_first_answer": 0.19114011999568878, "avg_entropy_of_first_answer": 3.444161430001259, "lex_sim": 1.0}
{"question": "'Eureka', the National Children's Museum is in which town?", "answer": ["Coatesville, Pennsylvania"], "scores": [[0.0054420544765889645, 0.008771083317697048, 0.4169653356075287, 0.28192687034606934, 0.6776735186576843]], "normalized_score": [0.0824165865778923], "tokens": [["\u2581Co", "ates", "ville", ",", "\u2581Pennsylvania"]], "entropy": [[4.913278579711914, 4.213427543640137, 2.432971715927124, 1.3826316595077515, 1.6710281372070312]], "avg_entropy": [2.9226675271987914], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Community of Halifax", "score_of_biggest_cluster": 0.0824165865778923, "score_of_first_answer": 0.0824165865778923, "max_score_of_first_answer": 0.6776735186576843, "min_score_of_first_answer": 0.0054420544765889645, "avg_score": 0.0824165865778923, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.255279309090252, "std_of_first_answer": 0.255279309090252, "avg_entropy_of_first_answer": 2.9226675271987914, "lex_sim": 1.0}
{"question": "Which model village did Samuel Greg build to house workers at his nearby Quarry Bank Mill?", "answer": ["Styal"], "scores": [[0.33577102422714233, 0.843629777431488]], "normalized_score": [0.5322278141975403], "tokens": [["\u2581St", "yal"]], "entropy": [[4.687224388122559, 1.2093265056610107]], "avg_entropy": [2.9482754468917847], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Styal", "score_of_biggest_cluster": 0.5322278141975403, "score_of_first_answer": 0.5322278141975403, "max_score_of_first_answer": 0.843629777431488, "min_score_of_first_answer": 0.33577102422714233, "avg_score": 0.5322278141975403, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.25392937660217285, "std_of_first_answer": 0.25392937660217285, "avg_entropy_of_first_answer": 2.9482754468917847, "lex_sim": 1.0}
{"question": "Where on a coin would 'reeding' be found?", "answer": ["Edge (of the coin)"], "scores": [[0.19524484872817993, 0.06919661909341812, 0.35322439670562744, 0.14524400234222412, 0.9732077717781067, 0.9681400060653687]], "normalized_score": [0.2945503294467926], "tokens": [["\u2581Edge", "\u2581(", "of", "\u2581the", "\u2581coin", ")"]], "entropy": [[4.6016998291015625, 1.2247469425201416, 3.427643299102783, 1.3376264572143555, 0.248977392911911, 0.24533045291900635]], "avg_entropy": [1.8476707289616268], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Edge foundation", "score_of_biggest_cluster": 0.2945503294467926, "score_of_first_answer": 0.2945503294467926, "max_score_of_first_answer": 0.9732077717781067, "min_score_of_first_answer": 0.06919661909341812, "avg_score": 0.2945503294467926, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37735163852583664, "std_of_first_answer": 0.37735163852583664, "avg_entropy_of_first_answer": 1.8476707289616268, "lex_sim": 1.0}
{"question": "Its capital is Valverde - which is the smallest of the Canary Islands?", "answer": ["El Hierro"], "scores": [[0.4034704267978668, 0.7092928886413574, 0.9712650179862976]], "normalized_score": [0.6526169180870056], "tokens": [["\u2581El", "\u2581Hier", "ro"]], "entropy": [[3.861588716506958, 2.495729923248291, 0.20178987085819244]], "avg_entropy": [2.1863695035378137], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "El hierro", "score_of_biggest_cluster": 0.6526169180870056, "score_of_first_answer": 0.6526169180870056, "max_score_of_first_answer": 0.9712650179862976, "min_score_of_first_answer": 0.4034704267978668, "avg_score": 0.6526169180870056, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2320314807435435, "std_of_first_answer": 0.2320314807435435, "avg_entropy_of_first_answer": 2.1863695035378137, "lex_sim": 1.0}
{"question": "Bernie Ecclestone and Flavio Briatore were at one time co-owners of which Premier League club?", "answer": ["Queen's Park Rangers"], "scores": [[0.06425122916698456, 0.6651821136474609, 0.9990952014923096, 0.9276193380355835, 0.9192715287208557, 0.9770830869674683]], "normalized_score": [0.5734941959381104], "tokens": [["\u2581Queen", "'", "s", "\u2581Park", "\u2581R", "angers"]], "entropy": [[4.200214862823486, 1.7110545635223389, 0.01287965476512909, 0.6395847201347351, 0.5188734531402588, 0.1540810912847519]], "avg_entropy": [1.20611472427845], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Queens' Park Rangers FC", "score_of_biggest_cluster": 0.5734941959381104, "score_of_first_answer": 0.5734941959381104, "max_score_of_first_answer": 0.9990952014923096, "min_score_of_first_answer": 0.06425122916698456, "avg_score": 0.5734941959381104, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3293434527714673, "std_of_first_answer": 0.3293434527714673, "avg_entropy_of_first_answer": 1.20611472427845, "lex_sim": 1.0}
{"question": "Usually made of plastic, which transparent instrument, in the shape of a triangle with one angle at ninety degrees, is used for drawing angles on paper?", "answer": ["Drawing triangle"], "scores": [[0.01789991371333599, 0.8994495272636414, 0.31391921639442444]], "normalized_score": [0.17161238193511963], "tokens": [["\u2581Draw", "ing", "\u2581triangle"]], "entropy": [[5.414061546325684, 0.8420537710189819, 3.5728566646575928]], "avg_entropy": [3.276323994000753], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "SET-SQUARE", "score_of_biggest_cluster": 0.17161238193511963, "score_of_first_answer": 0.17161238193511963, "max_score_of_first_answer": 0.8994495272636414, "min_score_of_first_answer": 0.01789991371333599, "avg_score": 0.17161238193511963, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.366303288676518, "std_of_first_answer": 0.366303288676518, "avg_entropy_of_first_answer": 3.276323994000753, "lex_sim": 1.0}
{"question": "What 2009 hit record by Lily Allen concerns her boyfriend's performance in the bedroom?", "answer": ["\"Friday night\" (song)"], "scores": [[0.0051714410074055195, 0.25854942202568054, 0.10698650777339935, 0.9986640214920044, 0.05503927543759346, 0.3107055723667145, 0.07821435481309891, 0.520369827747345, 0.9355975389480591]], "normalized_score": [0.16547590494155884], "tokens": [["\u2581\"", "F", "rid", "ay", "\u2581night", "\"", "\u2581(", "song", ")"]], "entropy": [[5.424757480621338, 3.6652824878692627, 2.671630859375, 0.0162002295255661, 1.0432181358337402, 2.286417007446289, 1.909745216369629, 3.1050758361816406, 0.3905118703842163]], "avg_entropy": [2.279204347067409], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Not Fair", "score_of_biggest_cluster": 0.16547590494155884, "score_of_first_answer": 0.16547590494155884, "max_score_of_first_answer": 0.9986640214920044, "min_score_of_first_answer": 0.0051714410074055195, "avg_score": 0.16547590494155884, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35588276059345325, "std_of_first_answer": 0.35588276059345325, "avg_entropy_of_first_answer": 2.279204347067409, "lex_sim": 1.0}
{"question": "What was said to have been guarded by the Fisher King?", "answer": ["Vindolanda Tablets"], "scores": [[0.0031659933738410473, 0.005661338567733765, 0.007943633943796158, 0.867202877998352, 0.27497997879981995, 0.9259960055351257]], "normalized_score": [0.05617974326014519], "tokens": [["\u2581V", "ind", "ol", "anda", "\u2581Table", "ts"]], "entropy": [[3.954895496368408, 3.7209482192993164, 3.0250208377838135, 1.1016817092895508, 2.1536598205566406, 0.26718223094940186]], "avg_entropy": [2.3705647190411887], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sangrael", "score_of_biggest_cluster": 0.05617974326014519, "score_of_first_answer": 0.05617974326014519, "max_score_of_first_answer": 0.9259960055351257, "min_score_of_first_answer": 0.0031659933738410473, "avg_score": 0.05617974326014519, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4001512145788406, "std_of_first_answer": 0.4001512145788406, "avg_entropy_of_first_answer": 2.3705647190411887, "lex_sim": 1.0}
{"question": "From which song does this lyric come? 'She calls out to the man in the street, sir can you help me'?", "answer": ["The man on the street"], "scores": [[0.09400960057973862, 0.017570726573467255, 0.19588594138622284, 0.9861246943473816, 0.5178850889205933]], "normalized_score": [0.17523711919784546], "tokens": [["\u2581The", "\u2581man", "\u2581on", "\u2581the", "\u2581street"]], "entropy": [[5.390639305114746, 5.295034408569336, 1.885901689529419, 0.1255335509777069, 2.7955384254455566]], "avg_entropy": [3.0985294759273527], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Another Day in Paradise (Jam Tronix song)", "score_of_biggest_cluster": 0.17523711919784546, "score_of_first_answer": 0.17523711919784546, "max_score_of_first_answer": 0.9861246943473816, "min_score_of_first_answer": 0.017570726573467255, "avg_score": 0.17523711919784546, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3554927898959156, "std_of_first_answer": 0.3554927898959156, "avg_entropy_of_first_answer": 3.0985294759273527, "lex_sim": 1.0}
{"question": "Who first hosted Family Feud?", "answer": ["Richard Dawson"], "scores": [[0.17978884279727936, 0.7605017423629761, 0.9443040490150452]], "normalized_score": [0.505426824092865], "tokens": [["\u2581Richard", "\u2581Daw", "son"]], "entropy": [[5.336956024169922, 1.8665142059326172, 0.29610806703567505]], "avg_entropy": [2.4998594323794046], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dick Dawson", "score_of_biggest_cluster": 0.505426824092865, "score_of_first_answer": 0.505426824092865, "max_score_of_first_answer": 0.9443040490150452, "min_score_of_first_answer": 0.17978884279727936, "avg_score": 0.505426824092865, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3258312890286787, "std_of_first_answer": 0.3258312890286787, "avg_entropy_of_first_answer": 2.4998594323794046, "lex_sim": 1.0}
{"question": "Whose horse was called Sleipnir?", "answer": ["Odin"], "scores": [[0.6381086111068726, 0.9601748585700989]], "normalized_score": [0.7827488780021667], "tokens": [["\u2581Od", "in"]], "entropy": [[2.4450173377990723, 0.29163652658462524]], "avg_entropy": [1.3683269321918488], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Odin (disambiguation)", "score_of_biggest_cluster": 0.7827488780021667, "score_of_first_answer": 0.7827488780021667, "max_score_of_first_answer": 0.9601748585700989, "min_score_of_first_answer": 0.6381086111068726, "avg_score": 0.7827488780021667, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.16103312373161316, "std_of_first_answer": 0.16103312373161316, "avg_entropy_of_first_answer": 1.3683269321918488, "lex_sim": 1.0}
{"question": "What is the name for the infection of living tissues such as skin, lung or bowel by bacteria?", "answer": ["Pestilence"], "scores": [[0.03456636518239975, 0.02968820556998253, 0.5002639293670654, 0.8236329555511475]], "normalized_score": [0.14339785277843475], "tokens": [["\u2581P", "est", "il", "ence"]], "entropy": [[5.115214824676514, 4.175069332122803, 2.264587640762329, 0.8063029050827026]], "avg_entropy": [3.090293675661087], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gonococcal septicaemia affecting skin", "score_of_biggest_cluster": 0.14339785277843475, "score_of_first_answer": 0.14339785277843475, "max_score_of_first_answer": 0.8236329555511475, "min_score_of_first_answer": 0.02968820556998253, "avg_score": 0.14339785277843475, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3350262495975073, "std_of_first_answer": 0.3350262495975073, "avg_entropy_of_first_answer": 3.090293675661087, "lex_sim": 1.0}
{"question": "By population, which is the second biggest city in France?", "answer": ["Marseilles"], "scores": [[0.09091269224882126, 0.35385704040527344]], "normalized_score": [0.17936024069786072], "tokens": [["\u2581Marse", "illes"]], "entropy": [[5.092198371887207, 1.1780734062194824]], "avg_entropy": [3.1351358890533447], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lyon (France)", "score_of_biggest_cluster": 0.17936024069786072, "score_of_first_answer": 0.17936024069786072, "max_score_of_first_answer": 0.35385704040527344, "min_score_of_first_answer": 0.09091269224882126, "avg_score": 0.17936024069786072, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1314721740782261, "std_of_first_answer": 0.1314721740782261, "avg_entropy_of_first_answer": 3.1351358890533447, "lex_sim": 1.0}
{"question": "Who created the Miss World pageant?", "answer": ["Mr Trump"], "scores": [[0.019159909337759018, 0.004610602278262377]], "normalized_score": [0.009398866444826126], "tokens": [["\u2581Mr", "\u2581Trump"]], "entropy": [[5.174152374267578, 3.583657741546631]], "avg_entropy": [4.3789050579071045], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eric Morley", "score_of_biggest_cluster": 0.009398866444826126, "score_of_first_answer": 0.009398866444826126, "max_score_of_first_answer": 0.019159909337759018, "min_score_of_first_answer": 0.004610602278262377, "avg_score": 0.009398866444826126, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.007274653529748321, "std_of_first_answer": 0.007274653529748321, "avg_entropy_of_first_answer": 4.3789050579071045, "lex_sim": 1.0}
{"question": "Where in the UK did the Queen unveil a memorial to John F Kennedy in May 1965?", "answer": ["Runnymede"], "scores": [[0.38453352451324463, 0.941555380821228, 0.9209844470024109, 0.9656642079353333]], "normalized_score": [0.7532942295074463], "tokens": [["\u2581R", "unn", "ym", "ede"]], "entropy": [[4.257746696472168, 0.46947556734085083, 0.39641842246055603, 0.2398960143327713]], "avg_entropy": [1.3408841751515865], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Runnymead", "score_of_biggest_cluster": 0.7532942295074463, "score_of_first_answer": 0.7532942295074463, "max_score_of_first_answer": 0.9656642079353333, "min_score_of_first_answer": 0.38453352451324463, "avg_score": 0.7532942295074463, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2422249073380605, "std_of_first_answer": 0.2422249073380605, "avg_entropy_of_first_answer": 1.3408841751515865, "lex_sim": 1.0}
{"question": "What is the Milky Way?", "answer": ["252-558-0227"], "scores": [[0.027880152687430382, 0.06031408905982971, 0.07684294134378433, 0.018616747111082077, 0.016185447573661804, 0.06610412150621414, 0.12993353605270386, 0.07234328985214233, 0.27290472388267517, 0.18232819437980652, 0.044164735823869705, 0.08820555359125137, 0.07392861694097519]], "normalized_score": [0.06467923521995544], "tokens": [["\u2581", "2", "5", "2", "-", "5", "5", "8", "-", "0", "2", "2", "7"]], "entropy": [[4.101835250854492, 4.742627143859863, 3.5225749015808105, 3.1386001110076904, 4.4191575050354, 4.1298370361328125, 2.5905628204345703, 2.540919065475464, 3.2127647399902344, 2.4161200523376465, 2.1894149780273438, 2.4172754287719727, 2.618098497390747]], "avg_entropy": [3.2338298100691576], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Age of the Milky Way", "score_of_biggest_cluster": 0.06467923521995544, "score_of_first_answer": 0.06467923521995544, "max_score_of_first_answer": 0.27290472388267517, "min_score_of_first_answer": 0.016185447573661804, "avg_score": 0.06467923521995544, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0691438083589031, "std_of_first_answer": 0.0691438083589031, "avg_entropy_of_first_answer": 3.2338298100691576, "lex_sim": 1.0}
{"question": "In which musical is the main male character called Billy Bigelow?", "answer": ["Carousel"], "scores": [[0.6463664770126343, 0.9621198177337646]], "normalized_score": [0.7885949611663818], "tokens": [["\u2581Car", "ousel"]], "entropy": [[2.844208240509033, 0.2858855128288269]], "avg_entropy": [1.56504687666893], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Carousels", "score_of_biggest_cluster": 0.7885949611663818, "score_of_first_answer": 0.7885949611663818, "max_score_of_first_answer": 0.9621198177337646, "min_score_of_first_answer": 0.6463664770126343, "avg_score": 0.7885949611663818, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.15787667036056519, "std_of_first_answer": 0.15787667036056519, "avg_entropy_of_first_answer": 1.56504687666893, "lex_sim": 1.0}
{"question": "When was the current theme for the BBC television programme Top Gear first released on an album?", "answer": ["1995"], "scores": [[0.3906789720058441, 0.24723969399929047, 0.6534693241119385, 0.4787690341472626, 0.32947731018066406]], "normalized_score": [0.3977617621421814], "tokens": [["\u2581", "1", "9", "9", "5"]], "entropy": [[4.526427268981934, 2.1301252841949463, 1.8117868900299072, 1.7754106521606445, 2.0902836322784424]], "avg_entropy": [2.466806745529175], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and seventy-three", "score_of_biggest_cluster": 0.3977617621421814, "score_of_first_answer": 0.3977617621421814, "max_score_of_first_answer": 0.6534693241119385, "min_score_of_first_answer": 0.24723969399929047, "avg_score": 0.3977617621421814, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13918469838157385, "std_of_first_answer": 0.13918469838157385, "avg_entropy_of_first_answer": 2.466806745529175, "lex_sim": 1.0}
{"question": "What was the name of the Prime Minister of Egypt who was assassinated in 1910?", "answer": ["Notorious"], "scores": [[0.007826977409422398, 0.13887685537338257]], "normalized_score": [0.03296947106719017], "tokens": [["\u2581Not", "orious"]], "entropy": [[5.40463924407959, 5.547768592834473]], "avg_entropy": [5.476203918457031], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Boutros Ghali", "score_of_biggest_cluster": 0.03296947106719017, "score_of_first_answer": 0.03296947106719017, "max_score_of_first_answer": 0.13887685537338257, "min_score_of_first_answer": 0.007826977409422398, "avg_score": 0.03296947106719017, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06552493898198009, "std_of_first_answer": 0.06552493898198009, "avg_entropy_of_first_answer": 5.476203918457031, "lex_sim": 1.0}
{"question": "What is the state capital of New South Wales, Australia?", "answer": ["The Cove"], "scores": [[0.020939799025654793, 0.0056495219469070435, 0.013488780707120895]], "normalized_score": [0.011685637757182121], "tokens": [["\u2581The", "\u2581C", "ove"]], "entropy": [[5.122261047363281, 6.376150131225586, 4.991490364074707]], "avg_entropy": [5.496633847554524], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sydney, New South Wales, Australia", "score_of_biggest_cluster": 0.011685637757182121, "score_of_first_answer": 0.011685637757182121, "max_score_of_first_answer": 0.020939799025654793, "min_score_of_first_answer": 0.0056495219469070435, "avg_score": 0.011685637757182121, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.006242900189972216, "std_of_first_answer": 0.006242900189972216, "avg_entropy_of_first_answer": 5.496633847554524, "lex_sim": 1.0}
{"question": "On which of the hills of ancient Rome were the main residences of the Caesars?", "answer": ["Palatine"], "scores": [[0.39779168367385864, 0.8936245441436768, 0.9281153082847595]], "normalized_score": [0.6909886598587036], "tokens": [["\u2581Pal", "at", "ine"]], "entropy": [[3.886678457260132, 0.6901819705963135, 0.4671246409416199]], "avg_entropy": [1.6813283562660217], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "The Palatine", "score_of_biggest_cluster": 0.6909886598587036, "score_of_first_answer": 0.6909886598587036, "max_score_of_first_answer": 0.9281153082847595, "min_score_of_first_answer": 0.39779168367385864, "avg_score": 0.6909886598587036, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2422769272164859, "std_of_first_answer": 0.2422769272164859, "avg_entropy_of_first_answer": 1.6813283562660217, "lex_sim": 1.0}
{"question": "Which Eastenders actor has played the policeman Nick Rowan on TV?", "answer": ["Nick Rowan"], "scores": [[0.05887630954384804, 0.2594418525695801, 0.9056652188301086]], "normalized_score": [0.24005798995494843], "tokens": [["\u2581Nick", "\u2581Row", "an"]], "entropy": [[6.028974533081055, 4.741014003753662, 0.7062673568725586]], "avg_entropy": [3.8254186312357583], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nick Berry", "score_of_biggest_cluster": 0.24005798995494843, "score_of_first_answer": 0.24005798995494843, "max_score_of_first_answer": 0.9056652188301086, "min_score_of_first_answer": 0.05887630954384804, "avg_score": 0.24005798995494843, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36130667691350377, "std_of_first_answer": 0.36130667691350377, "avg_entropy_of_first_answer": 3.8254186312357583, "lex_sim": 1.0}
{"question": "In which city is Nevil Road County Cricket Ground?", "answer": ["Nevil Road"], "scores": [[0.07985268533229828, 0.8153095245361328, 0.4248075783252716]], "normalized_score": [0.3024137020111084], "tokens": [["\u2581Ne", "vil", "\u2581Road"]], "entropy": [[5.951223373413086, 0.7865365147590637, 3.6307778358459473]], "avg_entropy": [3.4561792413393655], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bristol (England)", "score_of_biggest_cluster": 0.3024137020111084, "score_of_first_answer": 0.3024137020111084, "max_score_of_first_answer": 0.8153095245361328, "min_score_of_first_answer": 0.07985268533229828, "avg_score": 0.3024137020111084, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30044086320050095, "std_of_first_answer": 0.30044086320050095, "avg_entropy_of_first_answer": 3.4561792413393655, "lex_sim": 1.0}
{"question": "Congo, Sensation, President Grevy and Beauty of Moscow are all varieties of which garden plant?", "answer": ["Peony"], "scores": [[0.026209281757473946, 0.7807493805885315]], "normalized_score": [0.14304852485656738], "tokens": [["\u2581Pe", "ony"]], "entropy": [[5.187627792358398, 0.7687311172485352]], "avg_entropy": [2.978179454803467], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Syringa vulgaris", "score_of_biggest_cluster": 0.14304852485656738, "score_of_first_answer": 0.14304852485656738, "max_score_of_first_answer": 0.7807493805885315, "min_score_of_first_answer": 0.026209281757473946, "avg_score": 0.14304852485656738, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3772700494155288, "std_of_first_answer": 0.3772700494155288, "avg_entropy_of_first_answer": 2.978179454803467, "lex_sim": 1.0}
{"question": "Involvement with which corrupt architect led to the resignation of Home Secretary Reginald Maudling in July 1972?", "answer": ["John Allen"], "scores": [[0.7209591269493103, 0.0006557319429703057]], "normalized_score": [0.02174295112490654], "tokens": [["\u2581John", "\u2581Allen"]], "entropy": [[2.3612565994262695, 1.0729178190231323]], "avg_entropy": [1.717087209224701], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Poulson affair", "score_of_biggest_cluster": 0.02174295112490654, "score_of_first_answer": 0.02174295112490654, "max_score_of_first_answer": 0.7209591269493103, "min_score_of_first_answer": 0.0006557319429703057, "avg_score": 0.02174295112490654, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.36015169750317, "std_of_first_answer": 0.36015169750317, "avg_entropy_of_first_answer": 1.717087209224701, "lex_sim": 1.0}
{"question": "Which U.S. air base in Britain was the scene of a long running women's anti nuclear protest in the 1980s", "answer": ["The Lochs"], "scores": [[0.016766799613833427, 0.0074126096442341805, 0.03899737820029259, 0.2124793529510498]], "normalized_score": [0.03185615688562393], "tokens": [["\u2581The", "\u2581L", "och", "s"]], "entropy": [[4.352518558502197, 4.89415168762207, 3.2363362312316895, 2.227262496948242]], "avg_entropy": [3.67756724357605], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "RAF Greenham Common", "score_of_biggest_cluster": 0.03185615688562393, "score_of_first_answer": 0.03185615688562393, "max_score_of_first_answer": 0.2124793529510498, "min_score_of_first_answer": 0.0074126096442341805, "avg_score": 0.03185615688562393, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08367760505605518, "std_of_first_answer": 0.08367760505605518, "avg_entropy_of_first_answer": 3.67756724357605, "lex_sim": 1.0}
{"question": "What is the furcula of a bird more commonly called?", "answer": ["Wishbone"], "scores": [[0.32439836859703064, 0.86125648021698, 0.8407071828842163]], "normalized_score": [0.6170002222061157], "tokens": [["\u2581W", "ish", "bone"]], "entropy": [[4.746636867523193, 0.7972191572189331, 0.7907660007476807]], "avg_entropy": [2.111540675163269], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Wish bones", "score_of_biggest_cluster": 0.6170002222061157, "score_of_first_answer": 0.6170002222061157, "max_score_of_first_answer": 0.86125648021698, "min_score_of_first_answer": 0.32439836859703064, "avg_score": 0.6170002222061157, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2483755439167892, "std_of_first_answer": 0.2483755439167892, "avg_entropy_of_first_answer": 2.111540675163269, "lex_sim": 1.0}
{"question": "What substance, best known as a poison, was used in small doses in medications as a stimulant, as a laxative, and for enhancing performance in sports?", "answer": ["Methoxyethylone"], "scores": [[0.07299891859292984, 0.7692141532897949, 0.015954377129673958, 0.5628927946090698, 0.08979364484548569, 0.11334384977817535, 0.07532279193401337]], "normalized_score": [0.12130852788686752], "tokens": [["\u2581M", "eth", "ox", "y", "eth", "yl", "one"]], "entropy": [[5.790613174438477, 1.6628206968307495, 2.176734447479248, 1.8120687007904053, 3.7172958850860596, 1.3130812644958496, 1.2356019020080566]], "avg_entropy": [2.5297451530184065], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stricnine", "score_of_biggest_cluster": 0.12130852788686752, "score_of_first_answer": 0.12130852788686752, "max_score_of_first_answer": 0.7692141532897949, "min_score_of_first_answer": 0.015954377129673958, "avg_score": 0.12130852788686752, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2746637049385999, "std_of_first_answer": 0.2746637049385999, "avg_entropy_of_first_answer": 2.5297451530184065, "lex_sim": 1.0}
{"question": "What kind of sport is French Ring Sport?", "answer": ["Show sport"], "scores": [[0.002375619485974312, 0.05030665174126625]], "normalized_score": [0.01093203853815794], "tokens": [["\u2581Show", "\u2581sport"]], "entropy": [[4.852579593658447, 3.5817418098449707]], "avg_entropy": [4.217160701751709], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "List of dog sports", "score_of_biggest_cluster": 0.01093203853815794, "score_of_first_answer": 0.01093203853815794, "max_score_of_first_answer": 0.05030665174126625, "min_score_of_first_answer": 0.002375619485974312, "avg_score": 0.01093203853815794, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02396551612764597, "std_of_first_answer": 0.02396551612764597, "avg_entropy_of_first_answer": 4.217160701751709, "lex_sim": 1.0}
{"question": "On which island in the North Sea did both St Aidan and St Cuthbert live?", "answer": ["Holy Island, County"], "scores": [[0.08223126828670502, 0.6987143754959106, 0.09245503693819046, 0.00181152718141675]], "normalized_score": [0.055696506053209305], "tokens": [["\u2581Holy", "\u2581Island", ",", "\u2581County"]], "entropy": [[4.990139007568359, 1.593670129776001, 1.3911755084991455, 2.6262056827545166]], "avg_entropy": [2.6502975821495056], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Lindisfarne", "score_of_biggest_cluster": 0.055696506053209305, "score_of_first_answer": 0.055696506053209305, "max_score_of_first_answer": 0.6987143754959106, "min_score_of_first_answer": 0.00181152718141675, "avg_score": 0.055696506053209305, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2792919052171852, "std_of_first_answer": 0.2792919052171852, "avg_entropy_of_first_answer": 2.6502975821495056, "lex_sim": 1.0}
{"question": "What product is obtained from the tree Hevea Braziliensis' (amongst others)?", "answer": ["Natural latex (rubber)"], "scores": [[0.1370203047990799, 0.04699426889419556, 0.9922122955322266, 0.03500683233141899, 0.49651265144348145, 0.999413251876831, 0.9943302869796753, 0.8946748375892639]], "normalized_score": [0.31572380661964417], "tokens": [["\u2581Natural", "\u2581late", "x", "\u2581(", "r", "ub", "ber", ")"]], "entropy": [[4.896754741668701, 1.1906647682189941, 0.07233789563179016, 1.3164868354797363, 3.381869077682495, 0.005990057718008757, 0.047286830842494965, 0.7167913913726807]], "avg_entropy": [1.4535226998268627], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "LaTeX", "score_of_biggest_cluster": 0.31572380661964417, "score_of_first_answer": 0.31572380661964417, "max_score_of_first_answer": 0.999413251876831, "min_score_of_first_answer": 0.03500683233141899, "avg_score": 0.31572380661964417, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41841813881520395, "std_of_first_answer": 0.41841813881520395, "avg_entropy_of_first_answer": 1.4535226998268627, "lex_sim": 1.0}
{"question": "\u2018The Night of the Long Knives\u2019 refers to which leader eliminating many of his political enemies?", "answer": ["Adolf Hitler"], "scores": [[0.42020463943481445, 0.9256478548049927]], "normalized_score": [0.6236677765846252], "tokens": [["\u2581Adolf", "\u2581Hitler"]], "entropy": [[3.917263984680176, 0.5639619827270508]], "avg_entropy": [2.2406129837036133], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hitlerian", "score_of_biggest_cluster": 0.6236677765846252, "score_of_first_answer": 0.6236677765846252, "max_score_of_first_answer": 0.9256478548049927, "min_score_of_first_answer": 0.42020463943481445, "avg_score": 0.6236677765846252, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2527216076850891, "std_of_first_answer": 0.2527216076850891, "avg_entropy_of_first_answer": 2.2406129837036133, "lex_sim": 1.0}
{"question": "Which rust free cars were built in Dunmurry, Northern Ireland in the late 1970s?", "answer": ["British Leyland 1975-1990s"], "scores": [[0.025795435532927513, 0.6705472469329834, 0.9862539768218994, 0.006661880295723677, 0.5137357115745544, 0.27524200081825256, 0.8412056565284729, 0.06615563482046127, 0.22783517837524414, 0.6880244016647339, 0.9983397722244263, 0.05187378078699112, 0.11339650303125381, 0.05227839574217796]], "normalized_score": [0.18178625404834747], "tokens": [["\u2581British", "\u2581Ley", "land", "\u2581", "1", "9", "7", "5", "-", "1", "9", "9", "0", "s"]], "entropy": [[5.471762180328369, 2.3359012603759766, 0.08958815038204193, 2.483510971069336, 2.0375020503997803, 1.9449799060821533, 0.7943630218505859, 1.8905842304229736, 3.263244867324829, 1.2603142261505127, 0.016674863174557686, 0.8980290293693542, 1.9156644344329834, 1.9388413429260254]], "avg_entropy": [1.881497181020677], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "DeLorean", "score_of_biggest_cluster": 0.18178625404834747, "score_of_first_answer": 0.18178625404834747, "max_score_of_first_answer": 0.9983397722244263, "min_score_of_first_answer": 0.006661880295723677, "avg_score": 0.18178625404834747, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3625912060641852, "std_of_first_answer": 0.3625912060641852, "avg_entropy_of_first_answer": 1.881497181020677, "lex_sim": 1.0}
{"question": "How many fences do horses need to encounter at the Grand National?", "answer": ["99%"], "scores": [[0.38751688599586487, 0.005705753341317177, 0.051118265837430954, 0.02280394360423088]], "normalized_score": [0.04006798565387726], "tokens": [["\u2581", "9", "9", "%"]], "entropy": [[4.010987281799316, 1.3590455055236816, 3.7034666538238525, 4.079830646514893]], "avg_entropy": [3.288332521915436], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "30", "score_of_biggest_cluster": 0.04006798565387726, "score_of_first_answer": 0.04006798565387726, "max_score_of_first_answer": 0.38751688599586487, "min_score_of_first_answer": 0.005705753341317177, "avg_score": 0.04006798565387726, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.15714556314457548, "std_of_first_answer": 0.15714556314457548, "avg_entropy_of_first_answer": 3.288332521915436, "lex_sim": 1.0}
{"question": "\"What Latin expression means \"\"Seize the Day\"\"?\"", "answer": ["carpe d(i)em"], "scores": [[0.04498919099569321, 0.9752922058105469, 0.0032441127113997936, 0.0037756995297968388, 0.28467270731925964, 0.9016398191452026, 0.7229774594306946]], "normalized_score": [0.09996183216571808], "tokens": [["\u2581car", "pe", "\u2581d", "(", "i", ")", "em"]], "entropy": [[2.5582621097564697, 0.19170214235782623, 1.6940970420837402, 4.244475364685059, 4.785650730133057, 0.6514922380447388, 1.1986805200576782]], "avg_entropy": [2.189194306731224], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Carpe diem quam minimum credula postero", "score_of_biggest_cluster": 0.09996183216571808, "score_of_first_answer": 0.09996183216571808, "max_score_of_first_answer": 0.9752922058105469, "min_score_of_first_answer": 0.0032441127113997936, "avg_score": 0.09996183216571808, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40320054405010325, "std_of_first_answer": 0.40320054405010325, "avg_entropy_of_first_answer": 2.189194306731224, "lex_sim": 1.0}
{"question": "Which US President died in office in July 1850 to be succeeded by his Vice President Millard Fillmore?", "answer": ["Births by mothers in their 20s"], "scores": [[0.0028572222217917442, 0.04261580854654312, 0.01435520127415657, 0.012421326711773872, 0.002548971911892295, 0.9744358062744141, 0.05214206874370575, 0.11584210395812988, 0.41507720947265625, 0.23385272920131683, 0.9915675520896912, 0.872521162033081]], "normalized_score": [0.07408885657787323], "tokens": [["\u2581B", "irth", "s", "\u2581by", "\u2581moth", "ers", "\u2581in", "\u2581their", "\u2581", "2", "0", "s"]], "entropy": [[4.829387664794922, 6.219127178192139, 2.828136920928955, 3.2533493041992188, 5.233249664306641, 0.2411358803510666, 3.401660442352295, 4.379196643829346, 2.0766048431396484, 1.2414604425430298, 0.06340048462152481, 0.5306622982025146]], "avg_entropy": [2.858114313955108], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Zachary Talor", "score_of_biggest_cluster": 0.07408885657787323, "score_of_first_answer": 0.07408885657787323, "max_score_of_first_answer": 0.9915675520896912, "min_score_of_first_answer": 0.002548971911892295, "avg_score": 0.07408885657787323, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38511624452184356, "std_of_first_answer": 0.38511624452184356, "avg_entropy_of_first_answer": 2.858114313955108, "lex_sim": 1.0}
{"question": "What cereal is kid-tested and mother-approved?", "answer": ["Caped Crusader"], "scores": [[0.008628935553133488, 0.002028967486694455, 0.5083343386650085, 0.9680556654930115, 0.8874242305755615]], "normalized_score": [0.0947725847363472], "tokens": [["\u2581Cap", "ed", "\u2581Cr", "us", "ader"]], "entropy": [[5.836361885070801, 0.9506062865257263, 3.3078525066375732, 0.17493164539337158, 0.5560610890388489]], "avg_entropy": [2.165162682533264], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "KIX", "score_of_biggest_cluster": 0.0947725847363472, "score_of_first_answer": 0.0947725847363472, "max_score_of_first_answer": 0.9680556654930115, "min_score_of_first_answer": 0.002028967486694455, "avg_score": 0.0947725847363472, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41364533721465624, "std_of_first_answer": 0.41364533721465624, "avg_entropy_of_first_answer": 2.165162682533264, "lex_sim": 1.0}
{"question": "In which opera do the characters 'Ned Keene' and 'Bob Boles' appear?", "answer": ["The Pirates of Penzance"], "scores": [[0.50495845079422, 0.40527260303497314, 0.9921324849128723, 0.9805229306221008, 0.9799346327781677, 0.9938498139381409, 0.998603880405426]], "normalized_score": [0.7909234762191772], "tokens": [["\u2581The", "\u2581Pir", "ates", "\u2581of", "\u2581P", "enz", "ance"]], "entropy": [[3.770796060562134, 3.7993931770324707, 0.05111593380570412, 0.12223522365093231, 0.1687222272157669, 0.05820325389504433, 0.013979834504425526]], "avg_entropy": [1.1406351015237826], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ellen Orford", "score_of_biggest_cluster": 0.7909234762191772, "score_of_first_answer": 0.7909234762191772, "max_score_of_first_answer": 0.998603880405426, "min_score_of_first_answer": 0.40527260303497314, "avg_score": 0.7909234762191772, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2427376774191713, "std_of_first_answer": 0.2427376774191713, "avg_entropy_of_first_answer": 1.1406351015237826, "lex_sim": 1.0}
{"question": "Which US Naval Shipyard shares its name with an English county?", "answer": ["USS Portsmouth"], "scores": [[0.008374418132007122, 0.07190371304750443, 0.8998585939407349, 0.9831748604774475]], "normalized_score": [0.1519244760274887], "tokens": [["\u2581USS", "\u2581Port", "sm", "outh"]], "entropy": [[5.08052921295166, 4.917906761169434, 0.5341241955757141, 0.13712553679943085]], "avg_entropy": [2.6674214266240597], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Norfolkshire", "score_of_biggest_cluster": 0.1519244760274887, "score_of_first_answer": 0.1519244760274887, "max_score_of_first_answer": 0.9831748604774475, "min_score_of_first_answer": 0.008374418132007122, "avg_score": 0.1519244760274887, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45220860082968883, "std_of_first_answer": 0.45220860082968883, "avg_entropy_of_first_answer": 2.6674214266240597, "lex_sim": 1.0}
{"question": "'Professor Cuthbert Calculus' is an associate of which character?", "answer": ["Tintin character"], "scores": [[0.4844176471233368, 0.8762682676315308, 0.9897664189338684, 0.012329299002885818]], "normalized_score": [0.2682761549949646], "tokens": [["\u2581T", "int", "in", "\u2581character"]], "entropy": [[3.3379149436950684, 0.6023980379104614, 0.10052318871021271, 1.3794032335281372]], "avg_entropy": [1.35505985096097], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Tin-tin", "score_of_biggest_cluster": 0.2682761549949646, "score_of_first_answer": 0.2682761549949646, "max_score_of_first_answer": 0.9897664189338684, "min_score_of_first_answer": 0.012329299002885818, "avg_score": 0.2682761549949646, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3829529502495472, "std_of_first_answer": 0.3829529502495472, "avg_entropy_of_first_answer": 1.35505985096097, "lex_sim": 1.0}
{"question": "In cricket, what is the score of 111 known as?", "answer": ["Three figures"], "scores": [[0.0024900410789996386, 0.29951047897338867]], "normalized_score": [0.02730921469628811], "tokens": [["\u2581Three", "\u2581figures"]], "entropy": [[4.905889987945557, 4.2184343338012695]], "avg_entropy": [4.562162160873413], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "N\u00e9lson", "score_of_biggest_cluster": 0.02730921469628811, "score_of_first_answer": 0.02730921469628811, "max_score_of_first_answer": 0.29951047897338867, "min_score_of_first_answer": 0.0024900410789996386, "avg_score": 0.02730921469628811, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14851021894719452, "std_of_first_answer": 0.14851021894719452, "avg_entropy_of_first_answer": 4.562162160873413, "lex_sim": 1.0}
{"question": "In which town or city is the HQ of Colman's (of mustard fame)?", "answer": ["Northforks"], "scores": [[0.02491787262260914, 0.03668276593089104, 0.20671531558036804, 0.036162495613098145]], "normalized_score": [0.05112708359956741], "tokens": [["\u2581North", "f", "ork", "s"]], "entropy": [[4.829876899719238, 4.422349452972412, 3.5384764671325684, 2.0650815963745117]], "avg_entropy": [3.7139461040496826], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Norwich", "score_of_biggest_cluster": 0.05112708359956741, "score_of_first_answer": 0.05112708359956741, "max_score_of_first_answer": 0.20671531558036804, "min_score_of_first_answer": 0.02491787262260914, "avg_score": 0.05112708359956741, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07554583358259512, "std_of_first_answer": 0.07554583358259512, "avg_entropy_of_first_answer": 3.7139461040496826, "lex_sim": 1.0}
{"question": "What company purchased Skype in 2011?", "answer": ["Microsoft"], "scores": [[0.3809410333633423]], "normalized_score": [0.3809410333633423], "tokens": [["\u2581Microsoft"]], "entropy": [[4.119516849517822]], "avg_entropy": [4.119516849517822], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "'Microsoft Corp", "score_of_biggest_cluster": 0.3809410333633423, "score_of_first_answer": 0.3809410333633423, "max_score_of_first_answer": 0.3809410333633423, "min_score_of_first_answer": 0.3809410333633423, "avg_score": 0.3809410333633423, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.119516849517822, "lex_sim": 1.0}
{"question": "Who was the mother of Edward VI?", "answer": ["Henry VIII's widow"], "scores": [[0.21730075776576996, 0.7536630630493164, 0.2650396525859833, 0.994851291179657, 0.029694998636841774, 0.9784431457519531]], "normalized_score": [0.32841354608535767], "tokens": [["\u2581Henry", "\u2581VIII", "'", "s", "\u2581wid", "ow"]], "entropy": [[4.427193641662598, 1.3450888395309448, 2.0006895065307617, 0.0471772626042366, 3.010232925415039, 0.12637697160243988]], "avg_entropy": [1.8261265245576699], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jane St Maur", "score_of_biggest_cluster": 0.32841354608535767, "score_of_first_answer": 0.32841354608535767, "max_score_of_first_answer": 0.994851291179657, "min_score_of_first_answer": 0.029694998636841774, "avg_score": 0.32841354608535767, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3840405821920381, "std_of_first_answer": 0.3840405821920381, "avg_entropy_of_first_answer": 1.8261265245576699, "lex_sim": 1.0}
{"question": "In which sea is the The Grenadines island chain?", "answer": ["Atlantis"], "scores": [[0.002320450497791171, 0.8302233815193176, 0.6952390670776367]], "normalized_score": [0.11023016273975372], "tokens": [["\u2581Atl", "ant", "is"]], "entropy": [[4.590811729431152, 1.0682275295257568, 1.687788486480713]], "avg_entropy": [2.4489425818125405], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Carribean Islands", "score_of_biggest_cluster": 0.11023016273975372, "score_of_first_answer": 0.11023016273975372, "max_score_of_first_answer": 0.8302233815193176, "min_score_of_first_answer": 0.002320450497791171, "avg_score": 0.11023016273975372, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36267221771827884, "std_of_first_answer": 0.36267221771827884, "avg_entropy_of_first_answer": 2.4489425818125405, "lex_sim": 1.0}
{"question": "What did Arthur Wynne invent?", "answer": ["The crossword puzzle"], "scores": [[0.07959754765033722, 0.15748779475688934, 0.9813243746757507, 0.6390450596809387, 0.9929770827293396]], "normalized_score": [0.3788663446903229], "tokens": [["\u2581The", "\u2581cross", "word", "\u2581puzz", "le"]], "entropy": [[5.410040378570557, 4.9348554611206055, 0.15623363852500916, 1.0913403034210205, 0.04971693456172943]], "avg_entropy": [2.3284373432397842], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Cross word", "score_of_biggest_cluster": 0.3788663446903229, "score_of_first_answer": 0.3788663446903229, "max_score_of_first_answer": 0.9929770827293396, "min_score_of_first_answer": 0.07959754765033722, "avg_score": 0.3788663446903229, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.39077497909347714, "std_of_first_answer": 0.39077497909347714, "avg_entropy_of_first_answer": 2.3284373432397842, "lex_sim": 1.0}
{"question": "Saloth Sar was the real name of which infamous dictator?", "answer": ["Khmer"], "scores": [[0.03391968831419945, 0.9158576130867004]], "normalized_score": [0.17625436186790466], "tokens": [["\u2581Kh", "mer"]], "entropy": [[4.121099948883057, 0.7048049569129944]], "avg_entropy": [2.4129524528980255], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Polpot", "score_of_biggest_cluster": 0.17625436186790466, "score_of_first_answer": 0.17625436186790466, "max_score_of_first_answer": 0.9158576130867004, "min_score_of_first_answer": 0.03391968831419945, "avg_score": 0.17625436186790466, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4409689623862505, "std_of_first_answer": 0.4409689623862505, "avg_entropy_of_first_answer": 2.4129524528980255, "lex_sim": 1.0}
{"question": "Indians used wampum for money, what was wampum", "answer": ["The Indian belt"], "scores": [[0.023659367114305496, 0.020221516489982605, 0.008443559519946575, 0.6641638875007629]], "normalized_score": [0.04047197848558426], "tokens": [["\u2581The", "\u2581Indian", "\u2581bel", "t"]], "entropy": [[5.564426898956299, 5.8524017333984375, 4.853346824645996, 1.1898140907287598]], "avg_entropy": [4.364997386932373], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Beads or shells", "score_of_biggest_cluster": 0.04047197848558426, "score_of_first_answer": 0.04047197848558426, "max_score_of_first_answer": 0.6641638875007629, "min_score_of_first_answer": 0.008443559519946575, "avg_score": 0.04047197848558426, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28009585705542317, "std_of_first_answer": 0.28009585705542317, "avg_entropy_of_first_answer": 4.364997386932373, "lex_sim": 1.0}
{"question": "In which US city is the 1974 film Towering Inferno set?", "answer": ["San Francisco, California"], "scores": [[0.34101182222366333, 0.829164981842041, 0.06795915216207504, 0.677686870098114]], "normalized_score": [0.3378095030784607], "tokens": [["\u2581San", "\u2581Francisco", ",", "\u2581California"]], "entropy": [[4.473842620849609, 1.0456457138061523, 1.2366760969161987, 1.5634498596191406]], "avg_entropy": [2.0799035727977753], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "San Fransciso", "score_of_biggest_cluster": 0.3378095030784607, "score_of_first_answer": 0.3378095030784607, "max_score_of_first_answer": 0.829164981842041, "min_score_of_first_answer": 0.06795915216207504, "avg_score": 0.3378095030784607, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2958408756441324, "std_of_first_answer": 0.2958408756441324, "avg_entropy_of_first_answer": 2.0799035727977753, "lex_sim": 1.0}
{"question": "What is the nickname of baseball star Jimmie Foxx?", "answer": ["Beaver"], "scores": [[0.08929993957281113, 0.27414992451667786]], "normalized_score": [0.15646588802337646], "tokens": [["\u2581Be", "aver"]], "entropy": [[3.799982786178589, 2.8480260372161865]], "avg_entropy": [3.3240044116973877], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Double X (disambiguation)", "score_of_biggest_cluster": 0.15646588802337646, "score_of_first_answer": 0.15646588802337646, "max_score_of_first_answer": 0.27414992451667786, "min_score_of_first_answer": 0.08929993957281113, "avg_score": 0.15646588802337646, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09242499247193336, "std_of_first_answer": 0.09242499247193336, "avg_entropy_of_first_answer": 3.3240044116973877, "lex_sim": 1.0}
{"question": "What is the name of the agreement under which passport controls have been abolished in 22 EU and 4 non-EU states?", "answer": ["A single Schengen visa"], "scores": [[0.015041102655231953, 0.006460682488977909, 0.06850908696651459, 0.9933372735977173, 0.33939579129219055, 0.9977831244468689]], "normalized_score": [0.11438198387622833], "tokens": [["\u2581A", "\u2581single", "\u2581Sch", "engen", "\u2581vis", "a"]], "entropy": [[4.5891571044921875, 6.7735676765441895, 4.33824348449707, 0.052471987903118134, 2.7334654331207275, 0.023649141192436218]], "avg_entropy": [3.0850924712916217], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Schengen", "score_of_biggest_cluster": 0.11438198387622833, "score_of_first_answer": 0.11438198387622833, "max_score_of_first_answer": 0.9977831244468689, "min_score_of_first_answer": 0.006460682488977909, "avg_score": 0.11438198387622833, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4331953370043811, "std_of_first_answer": 0.4331953370043811, "avg_entropy_of_first_answer": 3.0850924712916217, "lex_sim": 1.0}
{"question": "'The Umbrellas' is the work of which French impressionist?", "answer": ["Luncheon of the Boating Party"], "scores": [[0.004795777145773172, 0.17178620398044586, 0.9608597159385681, 0.9512530565261841, 0.7266544699668884, 0.9602153897285461, 0.9195109009742737, 0.9973210692405701, 0.9769638180732727]], "normalized_score": [0.42688196897506714], "tokens": [["\u2581L", "un", "che", "on", "\u2581of", "\u2581the", "\u2581Bo", "ating", "\u2581Party"]], "entropy": [[5.330368518829346, 4.035701274871826, 0.2835656702518463, 0.3964325785636902, 1.4179611206054688, 0.3219258189201355, 0.5622577667236328, 0.026530850678682327, 0.15812095999717712]], "avg_entropy": [1.392540506604645], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pierre Auguste Renoir", "score_of_biggest_cluster": 0.42688196897506714, "score_of_first_answer": 0.42688196897506714, "max_score_of_first_answer": 0.9973210692405701, "min_score_of_first_answer": 0.004795777145773172, "avg_score": 0.42688196897506714, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3590190908627431, "std_of_first_answer": 0.3590190908627431, "avg_entropy_of_first_answer": 1.392540506604645, "lex_sim": 1.0}
{"question": "Who won the most of the first ten tournaments called the Women's Hockey World Cup (the field hockey World Cup competition for women), which was inaugurated in 1974?", "answer": ["The Netherlands Men"], "scores": [[0.1123243197798729, 0.9119792580604553, 0.0006409628549590707]], "normalized_score": [0.04034259542822838], "tokens": [["\u2581The", "\u2581Netherlands", "\u2581Men"]], "entropy": [[4.207734107971191, 0.7125715017318726, 1.6696133613586426]], "avg_entropy": [2.196639657020569], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "NETHERLANDS", "score_of_biggest_cluster": 0.04034259542822838, "score_of_first_answer": 0.04034259542822838, "max_score_of_first_answer": 0.9119792580604553, "min_score_of_first_answer": 0.0006409628549590707, "avg_score": 0.04034259542822838, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4058541994414853, "std_of_first_answer": 0.4058541994414853, "avg_entropy_of_first_answer": 2.196639657020569, "lex_sim": 1.0}
{"question": "What courtesy title is held by the heir to the Marquis of Bath?", "answer": ["Earl"], "scores": [[0.12495596706867218]], "normalized_score": [0.12495595961809158], "tokens": [["\u2581Earl"]], "entropy": [[4.607868671417236]], "avg_entropy": [4.607868671417236], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Viscount Weymouth", "score_of_biggest_cluster": 0.12495595961809158, "score_of_first_answer": 0.12495595961809158, "max_score_of_first_answer": 0.12495596706867218, "min_score_of_first_answer": 0.12495596706867218, "avg_score": 0.12495595961809158, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.607868671417236, "lex_sim": 1.0}
{"question": "\"On what \"\"railroad\"\" was Harriet Tubman a conductor?\"", "answer": ["Underground railroad"], "scores": [[0.5131538510322571, 0.9841212034225464, 0.12871983647346497, 0.9679205417633057]], "normalized_score": [0.5008357763290405], "tokens": [["\u2581Under", "ground", "\u2581rail", "road"]], "entropy": [[3.239790916442871, 0.13426291942596436, 1.1567949056625366, 0.24717864394187927]], "avg_entropy": [1.1945068463683128], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "National Underground Railroad Network to Freedom", "score_of_biggest_cluster": 0.5008357763290405, "score_of_first_answer": 0.5008357763290405, "max_score_of_first_answer": 0.9841212034225464, "min_score_of_first_answer": 0.12871983647346497, "avg_score": 0.5008357763290405, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3546692372250956, "std_of_first_answer": 0.3546692372250956, "avg_entropy_of_first_answer": 1.1945068463683128, "lex_sim": 1.0}
{"question": "In the 1971 Number One hit Ernie by Benny Hill, what was the name of the widow that Ernie loved who lived at 22 Linley?", "answer": ["Mrs Rogers"], "scores": [[0.05173340439796448, 0.001966971205547452, 0.8151596784591675]], "normalized_score": [0.043611783534288406], "tokens": [["\u2581Mrs", "\u2581Rog", "ers"]], "entropy": [[5.7343549728393555, 4.147894859313965, 1.0351226329803467]], "avg_entropy": [3.6391241550445557], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sue", "score_of_biggest_cluster": 0.043611783534288406, "score_of_first_answer": 0.043611783534288406, "max_score_of_first_answer": 0.8151596784591675, "min_score_of_first_answer": 0.001966971205547452, "avg_score": 0.043611783534288406, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37216763739687375, "std_of_first_answer": 0.37216763739687375, "avg_entropy_of_first_answer": 3.6391241550445557, "lex_sim": 1.0}
{"question": "The childhood disease 'Rickets' is caused by a lack of which vitamin?", "answer": ["Dihydrooxophytol"], "scores": [[0.0017933750059455633, 0.44861337542533875, 0.8067513704299927, 0.0010590627789497375, 0.019496556371450424, 0.06731453537940979, 0.32323864102363586]], "normalized_score": [0.04343380406498909], "tokens": [["\u2581Di", "h", "ydro", "ox", "oph", "yt", "ol"]], "entropy": [[4.070927619934082, 2.505033493041992, 0.5822169780731201, 1.9753053188323975, 2.6931817531585693, 3.399648904800415, 3.3277459144592285]], "avg_entropy": [2.6505799974714006], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Vitamin D1", "score_of_biggest_cluster": 0.04343380406498909, "score_of_first_answer": 0.04343380406498909, "max_score_of_first_answer": 0.8067513704299927, "min_score_of_first_answer": 0.0010590627789497375, "avg_score": 0.04343380406498909, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28382914055571085, "std_of_first_answer": 0.28382914055571085, "avg_entropy_of_first_answer": 2.6505799974714006, "lex_sim": 1.0}
{"question": "Which celebrity first found fame as the Joan Collins Fan Club", "answer": ["Michael Jackson"], "scores": [[0.004883291199803352, 0.06464972347021103]], "normalized_score": [0.017768047749996185], "tokens": [["\u2581Michael", "\u2581Jackson"]], "entropy": [[5.434735298156738, 3.8992273807525635]], "avg_entropy": [4.666981339454651], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joan Collins Fanclub", "score_of_biggest_cluster": 0.017768047749996185, "score_of_first_answer": 0.017768047749996185, "max_score_of_first_answer": 0.06464972347021103, "min_score_of_first_answer": 0.004883291199803352, "avg_score": 0.017768047749996185, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02988321613520384, "std_of_first_answer": 0.02988321613520384, "avg_entropy_of_first_answer": 4.666981339454651, "lex_sim": 1.0}
{"question": "Maria Fitzherbert was \u2018married\u2019 to which British monarch?", "answer": ["King George IV of the United Kingdom"], "scores": [[0.10407431423664093, 0.8969742655754089, 0.774795651435852, 0.19069281220436096, 0.798012375831604, 0.9513751864433289, 0.9917131662368774]], "normalized_score": [0.5207479000091553], "tokens": [["\u2581King", "\u2581George", "\u2581IV", "\u2581of", "\u2581the", "\u2581United", "\u2581Kingdom"]], "entropy": [[3.9396071434020996, 0.6997807025909424, 1.0601943731307983, 0.9345693588256836, 0.8475365042686462, 0.2613561451435089, 0.07293499261140823]], "avg_entropy": [1.1165684599961554], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "George, Prince Regent", "score_of_biggest_cluster": 0.5207479000091553, "score_of_first_answer": 0.5207479000091553, "max_score_of_first_answer": 0.9917131662368774, "min_score_of_first_answer": 0.10407431423664093, "avg_score": 0.5207479000091553, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3404872121667235, "std_of_first_answer": 0.3404872121667235, "avg_entropy_of_first_answer": 1.1165684599961554, "lex_sim": 1.0}
{"question": "Give a year in the life of botanist Carl Linnaeus.", "answer": ["A.D. 1758"], "scores": [[0.008114154450595379, 0.16084207594394684, 0.8744959235191345, 0.9245144724845886, 0.9084417223930359, 0.9568594694137573, 0.9048364758491516, 0.22473379969596863, 0.28048378229141235]], "normalized_score": [0.3344179391860962], "tokens": [["\u2581A", ".", "D", ".", "\u2581", "1", "7", "5", "8"]], "entropy": [[3.1558914184570312, 6.080463886260986, 0.7956994771957397, 0.35426294803619385, 0.6167030334472656, 0.26354533433914185, 0.488932341337204, 1.9467233419418335, 1.8097573518753052]], "avg_entropy": [1.7235532369878557], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1707 - 1778", "score_of_biggest_cluster": 0.3344179391860962, "score_of_first_answer": 0.3344179391860962, "max_score_of_first_answer": 0.9568594694137573, "min_score_of_first_answer": 0.008114154450595379, "avg_score": 0.3344179391860962, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3770417336777094, "std_of_first_answer": 0.3770417336777094, "avg_entropy_of_first_answer": 1.7235532369878557, "lex_sim": 1.0}
{"question": "What sea bird has the Latin name Corvus Marinus?", "answer": ["Corgo"], "scores": [[0.2016938030719757, 0.0007027129759080708, 0.011503909714519978]], "normalized_score": [0.011769878678023815], "tokens": [["\u2581C", "org", "o"]], "entropy": [[4.756302833557129, 1.7212121486663818, 1.9267826080322266]], "avg_entropy": [2.8014325300852456], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Phalacrocorax", "score_of_biggest_cluster": 0.011769878678023815, "score_of_first_answer": 0.011769878678023815, "max_score_of_first_answer": 0.2016938030719757, "min_score_of_first_answer": 0.0007027129759080708, "avg_score": 0.011769878678023815, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09230762559345436, "std_of_first_answer": 0.09230762559345436, "avg_entropy_of_first_answer": 2.8014325300852456, "lex_sim": 1.0}
{"question": "What is the name given to the official residence provided for the Olympic athletes?", "answer": ["Athletes Village"], "scores": [[0.05649540573358536, 0.9778477549552917, 0.26577430963516235]], "normalized_score": [0.24486824870109558], "tokens": [["\u2581Athlet", "es", "\u2581Village"]], "entropy": [[4.85648250579834, 0.17429575324058533, 1.756065845489502]], "avg_entropy": [2.2622813681761422], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paralympic Village", "score_of_biggest_cluster": 0.24486824870109558, "score_of_first_answer": 0.24486824870109558, "max_score_of_first_answer": 0.9778477549552917, "min_score_of_first_answer": 0.05649540573358536, "avg_score": 0.24486824870109558, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39436818704306176, "std_of_first_answer": 0.39436818704306176, "avg_entropy_of_first_answer": 2.2622813681761422, "lex_sim": 1.0}
{"question": "From which wood is a cricket bat traditionally made?", "answer": ["Willow"], "scores": [[0.2747344970703125, 0.9712629914283752]], "normalized_score": [0.5165650248527527], "tokens": [["\u2581Will", "ow"]], "entropy": [[4.814644813537598, 0.22324715554714203]], "avg_entropy": [2.51894598454237], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Booth willow", "score_of_biggest_cluster": 0.5165650248527527, "score_of_first_answer": 0.5165650248527527, "max_score_of_first_answer": 0.9712629914283752, "min_score_of_first_answer": 0.2747344970703125, "avg_score": 0.5165650248527527, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34826424717903137, "std_of_first_answer": 0.34826424717903137, "avg_entropy_of_first_answer": 2.51894598454237, "lex_sim": 1.0}
{"question": "The upper house of the Canadian parliament is known as the Senate; what is the lower house called?", "answer": ["House of Commons"], "scores": [[0.21024088561534882, 0.9508597254753113, 0.7899805307388306]], "normalized_score": [0.5405260920524597], "tokens": [["\u2581House", "\u2581of", "\u2581Commons"]], "entropy": [[4.590127944946289, 0.39980804920196533, 1.5636274814605713]], "avg_entropy": [2.1845211585362754], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "HOUSE OF COMMONS", "score_of_biggest_cluster": 0.5405260920524597, "score_of_first_answer": 0.5405260920524597, "max_score_of_first_answer": 0.9508597254753113, "min_score_of_first_answer": 0.21024088561534882, "avg_score": 0.5405260920524597, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.31806645660973915, "std_of_first_answer": 0.31806645660973915, "avg_entropy_of_first_answer": 2.1845211585362754, "lex_sim": 1.0}
{"question": "Can you name the singer of the title track for '1997 - Tomorrow Never Dies'?", "answer": ["Sheryl"], "scores": [[0.3271980881690979, 0.9827960133552551, 0.9894751310348511]], "normalized_score": [0.6826944351196289], "tokens": [["\u2581Sh", "ery", "l"]], "entropy": [[4.517378807067871, 0.12707854807376862, 0.0996011346578598]], "avg_entropy": [1.5813528299331665], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Sheryl Suzanne Crow", "score_of_biggest_cluster": 0.6826944351196289, "score_of_first_answer": 0.6826944351196289, "max_score_of_first_answer": 0.9894751310348511, "min_score_of_first_answer": 0.3271980881690979, "avg_score": 0.6826944351196289, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3106380765865244, "std_of_first_answer": 0.3106380765865244, "avg_entropy_of_first_answer": 1.5813528299331665, "lex_sim": 1.0}
{"question": "Which country is bordered by Cambodia and Laos to the west and China to the north?", "answer": ["Trip"], "scores": [[0.004351218696683645, 0.010201286524534225]], "normalized_score": [0.006662433035671711], "tokens": [["\u2581T", "rip"]], "entropy": [[4.19399881362915, 6.111174583435059]], "avg_entropy": [5.1525866985321045], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u0110\u1ed9c l\u1eadp - t\u1ef1 do - h\u1ea1nh ph\u00fac", "score_of_biggest_cluster": 0.006662433035671711, "score_of_first_answer": 0.006662433035671711, "max_score_of_first_answer": 0.010201286524534225, "min_score_of_first_answer": 0.004351218696683645, "avg_score": 0.006662433035671711, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.00292503391392529, "std_of_first_answer": 0.00292503391392529, "avg_entropy_of_first_answer": 5.1525866985321045, "lex_sim": 1.0}
{"question": "Beatrice Tinsley was well known in what field?", "answer": ["Astronomy and Cosmology"], "scores": [[0.3556007742881775, 0.7681870460510254, 0.03680388629436493, 0.22437848150730133, 0.9940788745880127, 0.9917958378791809]], "normalized_score": [0.36129170656204224], "tokens": [["\u2581Astronom", "y", "\u2581and", "\u2581Cos", "m", "ology"]], "entropy": [[4.04710578918457, 0.6682758331298828, 0.8832005262374878, 2.0648584365844727, 0.048236455768346786, 0.07113449275493622]], "avg_entropy": [1.2971352556099494], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Astronomy", "score_of_biggest_cluster": 0.36129170656204224, "score_of_first_answer": 0.36129170656204224, "max_score_of_first_answer": 0.9940788745880127, "min_score_of_first_answer": 0.03680388629436493, "avg_score": 0.36129170656204224, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.375577195722852, "std_of_first_answer": 0.375577195722852, "avg_entropy_of_first_answer": 1.2971352556099494, "lex_sim": 1.0}
{"question": "Kremlin in Russia technically means?", "answer": ["Citadels"], "scores": [[0.08036857098340988, 0.01632317528128624, 0.7532269954681396]], "normalized_score": [0.09960296005010605], "tokens": [["\u2581Cit", "ad", "els"]], "entropy": [[5.041653633117676, 0.2107495665550232, 1.184898853302002]], "avg_entropy": [2.145767350991567], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Forts", "score_of_biggest_cluster": 0.09960296005010605, "score_of_first_answer": 0.09960296005010605, "max_score_of_first_answer": 0.7532269954681396, "min_score_of_first_answer": 0.01632317528128624, "avg_score": 0.09960296005010605, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.33331125132135925, "std_of_first_answer": 0.33331125132135925, "avg_entropy_of_first_answer": 2.145767350991567, "lex_sim": 1.0}
{"question": "Franz Liszt ended up the father-in-law to which other very famous composer?", "answer": ["Camille Saint Saens"], "scores": [[0.018636537715792656, 0.988409161567688, 0.8998321890830994, 0.06678283214569092, 0.7311984300613403]], "normalized_score": [0.2407868355512619], "tokens": [["\u2581Camil", "le", "\u2581Saint", "\u2581Sa", "ens"]], "entropy": [[4.174620628356934, 0.0876329317688942, 0.7745746374130249, 0.4786916673183441, 0.9880062341690063]], "avg_entropy": [1.3007052198052407], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Wagner's", "score_of_biggest_cluster": 0.2407868355512619, "score_of_first_answer": 0.2407868355512619, "max_score_of_first_answer": 0.988409161567688, "min_score_of_first_answer": 0.018636537715792656, "avg_score": 0.2407868355512619, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.41541700630192396, "std_of_first_answer": 0.41541700630192396, "avg_entropy_of_first_answer": 1.3007052198052407, "lex_sim": 1.0}
{"question": "Between 20 and 30 miles thick above water (3 to 6 miles thick beneath the ocean), what is the geologic term for the part of the earth that sits on top of the mantle?", "answer": ["Crust (of the Earth)"], "scores": [[0.17108631134033203, 0.9786027073860168, 0.19202110171318054, 0.035993900150060654, 0.7883794903755188, 0.5590047240257263, 0.9866748452186584]], "normalized_score": [0.33792299032211304], "tokens": [["\u2581Cr", "ust", "\u2581(", "of", "\u2581the", "\u2581Earth", ")"]], "entropy": [[5.095414161682129, 0.1833343505859375, 1.2626698017120361, 1.9946054220199585, 1.138946294784546, 0.8524075746536255, 0.12066880613565445]], "avg_entropy": [1.521149487367698], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Cru\u017ft", "score_of_biggest_cluster": 0.33792299032211304, "score_of_first_answer": 0.33792299032211304, "max_score_of_first_answer": 0.9866748452186584, "min_score_of_first_answer": 0.035993900150060654, "avg_score": 0.33792299032211304, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.371192651676599, "std_of_first_answer": 0.371192651676599, "avg_entropy_of_first_answer": 1.521149487367698, "lex_sim": 1.0}
{"question": "Stalactites grow down from the top of cave. What grows up from the floor of a cave?", "answer": ["Staleaks, aka stalagmites"], "scores": [[0.5570718050003052, 0.0017599512357264757, 0.004192437510937452, 0.0034189485013484955, 0.018145492300391197, 0.17450778186321259, 0.2262280136346817, 0.9388912320137024, 0.939201831817627, 0.99913090467453]], "normalized_score": [0.07848556339740753], "tokens": [["\u2581St", "ale", "aks", ",", "\u2581a", "ka", "\u2581stal", "ag", "mit", "es"]], "entropy": [[3.5820460319519043, 0.3561738133430481, 6.327009201049805, 0.7894554734230042, 5.975302219390869, 5.756648063659668, 4.706084728240967, 0.4533694386482239, 0.3553069233894348, 0.010860481299459934]], "avg_entropy": [2.8312256374396383], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stalagmite", "score_of_biggest_cluster": 0.07848556339740753, "score_of_first_answer": 0.07848556339740753, "max_score_of_first_answer": 0.99913090467453, "min_score_of_first_answer": 0.0017599512357264757, "avg_score": 0.07848556339740753, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4077099552466243, "std_of_first_answer": 0.4077099552466243, "avg_entropy_of_first_answer": 2.8312256374396383, "lex_sim": 1.0}
{"question": "What were Pangaea and Gondwana?", "answer": ["Supercontinent"], "scores": [[0.3028397262096405, 0.9142758250236511, 0.13000765442848206]], "normalized_score": [0.33018168807029724], "tokens": [["\u2581Super", "cont", "inent"]], "entropy": [[4.21577787399292, 0.4429525136947632, 0.46017736196517944]], "avg_entropy": [1.7063025832176208], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Continant", "score_of_biggest_cluster": 0.33018168807029724, "score_of_first_answer": 0.33018168807029724, "max_score_of_first_answer": 0.9142758250236511, "min_score_of_first_answer": 0.13000765442848206, "avg_score": 0.33018168807029724, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33645233959619125, "std_of_first_answer": 0.33645233959619125, "avg_entropy_of_first_answer": 1.7063025832176208, "lex_sim": 1.0}
{"question": "Which film about a submarine had as one of its stars Denzil Washington, whose character had violent disagreements with his Captain played by Gene Hackman?", "answer": ["Crimson Tide"], "scores": [[0.48006755113601685, 0.9256767630577087, 0.9887004494667053, 0.8735036849975586, 0.9831095337867737]], "normalized_score": [0.8228840231895447], "tokens": [["\u2581C", "rim", "son", "\u2581T", "ide"]], "entropy": [[3.727928638458252, 0.49825286865234375, 0.10156138241291046, 1.136042594909668, 0.1607522964477539]], "avg_entropy": [1.1249075561761857], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "'CRIMSON TIDE'", "score_of_biggest_cluster": 0.8228840231895447, "score_of_first_answer": 0.8228840231895447, "max_score_of_first_answer": 0.9887004494667053, "min_score_of_first_answer": 0.48006755113601685, "avg_score": 0.8228840231895447, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.18978153292651434, "std_of_first_answer": 0.18978153292651434, "avg_entropy_of_first_answer": 1.1249075561761857, "lex_sim": 1.0}
{"question": "What is the Latin for wolf spider?", "answer": ["Arachnida taxonum wolfies"], "scores": [[0.02179109863936901, 0.6498579382896423, 0.31261634826660156, 0.001063006347976625, 0.6812918186187744, 0.0017698456067591906, 0.01061682403087616, 0.9688431024551392, 0.002320297295227647]], "normalized_score": [0.037169743329286575], "tokens": [["\u2581Ara", "chn", "ida", "\u2581tax", "on", "um", "\u2581w", "olf", "ies"]], "entropy": [[5.740649223327637, 2.0028324127197266, 2.736088752746582, 1.8976842164993286, 1.2246885299682617, 3.215996026992798, 2.365361213684082, 0.25399288535118103, 3.2141976356506348]], "avg_entropy": [2.516832321882248], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Www.lycos.com", "score_of_biggest_cluster": 0.037169743329286575, "score_of_first_answer": 0.037169743329286575, "max_score_of_first_answer": 0.9688431024551392, "min_score_of_first_answer": 0.001063006347976625, "avg_score": 0.037169743329286575, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3563829056205061, "std_of_first_answer": 0.3563829056205061, "avg_entropy_of_first_answer": 2.516832321882248, "lex_sim": 1.0}
{"question": "The ancient pre-scientific system of classifying all universal matter as Water/Air/Fire/Earth/Aether is called the Classical?", "answer": ["The ancient pre-scientific system of classifying all universal matter as Water/Air/Fire/Earth/Aether is called the Classical element theory"], "scores": [[0.052654627710580826, 0.1292066127061844, 0.902195930480957, 0.9986316561698914, 0.9981750249862671, 0.9984989166259766, 0.9992194175720215, 0.9931929707527161, 0.9929956793785095, 0.9939488172531128, 0.998791515827179, 0.9951510429382324, 0.9908666610717773, 0.9973056316375732, 0.9837453365325928, 0.9508329629898071, 0.9893962740898132, 0.9956796765327454, 0.9998948574066162, 0.9991095662117004, 0.9985532164573669, 0.9993225336074829, 0.9989443421363831, 0.9995319843292236, 0.9958350658416748, 0.9893302321434021, 0.9996165037155151, 0.9815529584884644, 0.983180820941925, 0.9191769957542419, 0.7610082030296326, 0.9977914094924927, 0.054032258689403534, 0.33228635787963867]], "normalized_score": [0.7528229355812073], "tokens": [["\u2581The", "\u2581ancient", "\u2581pre", "-", "s", "cient", "ific", "\u2581system", "\u2581of", "\u2581class", "ifying", "\u2581all", "\u2581universal", "\u2581matter", "\u2581as", "\u2581Water", "/", "A", "ir", "/", "Fire", "/", "E", "arth", "/", "A", "ether", "\u2581is", "\u2581called", "\u2581the", "\u2581Class", "ical", "\u2581element", "\u2581theory"]], "entropy": [[5.192121505737305, 4.623717308044434, 0.6729618310928345, 0.016989124938845634, 0.02084755338728428, 0.012494427151978016, 0.007581598125398159, 0.07825925946235657, 0.06362168490886688, 0.05663572996854782, 0.012355053797364235, 0.050285454839468, 0.08709659427404404, 0.027437200769782066, 0.13226163387298584, 0.4192264974117279, 0.08761774748563766, 0.036908168345689774, 0.0013814514968544245, 0.01037471555173397, 0.014870136976242065, 0.008311627432703972, 0.012171734124422073, 0.005076647270470858, 0.040948182344436646, 0.09218521416187286, 0.004582809284329414, 0.16231095790863037, 0.1517028510570526, 0.6660432815551758, 1.7268092632293701, 0.02307792566716671, 3.109945774078369, 1.9657270908355713]], "avg_entropy": [0.5762922951937927], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Elements (disambiguation)", "score_of_biggest_cluster": 0.7528229355812073, "score_of_first_answer": 0.7528229355812073, "max_score_of_first_answer": 0.9998948574066162, "min_score_of_first_answer": 0.052654627710580826, "avg_score": 0.7528229355812073, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27630617009292147, "std_of_first_answer": 0.27630617009292147, "avg_entropy_of_first_answer": 0.5762922951937927, "lex_sim": 1.0}
{"question": "Whose play about the Salem witch trials in the 17th century drew parallels to McCarthyism in the 1950s?", "answer": ["Arthur Miller"], "scores": [[0.3225873112678528, 0.7052813172340393]], "normalized_score": [0.4769850969314575], "tokens": [["\u2581Arthur", "\u2581Miller"]], "entropy": [[3.851870059967041, 2.1279525756835938]], "avg_entropy": [2.9899113178253174], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Agnes Barley", "score_of_biggest_cluster": 0.4769850969314575, "score_of_first_answer": 0.4769850969314575, "max_score_of_first_answer": 0.7052813172340393, "min_score_of_first_answer": 0.3225873112678528, "avg_score": 0.4769850969314575, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19134700298309326, "std_of_first_answer": 0.19134700298309326, "avg_entropy_of_first_answer": 2.9899113178253174, "lex_sim": 1.0}
{"question": "Who was the first Tory Prime Minister of Great Britain (May, 1762 to July, 1765)?", "answer": ["English Statesman"], "scores": [[0.0024996325373649597, 0.023175854235887527, 0.9652808308601379]], "normalized_score": [0.03824033960700035], "tokens": [["\u2581English", "\u2581States", "man"]], "entropy": [[4.8193488121032715, 5.2454833984375, 0.1846114993095398]], "avg_entropy": [3.4164812366167703], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Stuart, 3rd earl of Bute", "score_of_biggest_cluster": 0.03824033960700035, "score_of_first_answer": 0.03824033960700035, "max_score_of_first_answer": 0.9652808308601379, "min_score_of_first_answer": 0.0024996325373649597, "avg_score": 0.03824033960700035, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4490653168239487, "std_of_first_answer": 0.4490653168239487, "avg_entropy_of_first_answer": 3.4164812366167703, "lex_sim": 1.0}
{"question": "What word links take, write and stand", "answer": ["Action"], "scores": [[0.005397598259150982]], "normalized_score": [0.005397597793489695], "tokens": [["\u2581Action"]], "entropy": [[6.840227127075195]], "avg_entropy": [6.840227127075195], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Under (disambiguation)", "score_of_biggest_cluster": 0.005397597793489695, "score_of_first_answer": 0.005397597793489695, "max_score_of_first_answer": 0.005397598259150982, "min_score_of_first_answer": 0.005397598259150982, "avg_score": 0.005397597793489695, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.840227127075195, "lex_sim": 1.0}
{"question": "How many faces has an icosahedron?", "answer": ["20"], "scores": [[0.5065012574195862, 0.5986382365226746, 0.9499508142471313]], "normalized_score": [0.660412609577179], "tokens": [["\u2581", "2", "0"]], "entropy": [[3.372084617614746, 1.439044713973999, 0.40610605478286743]], "avg_entropy": [1.7390784621238708], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "twenty", "score_of_biggest_cluster": 0.660412609577179, "score_of_first_answer": 0.660412609577179, "max_score_of_first_answer": 0.9499508142471313, "min_score_of_first_answer": 0.5065012574195862, "avg_score": 0.660412609577179, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.19106638161068282, "std_of_first_answer": 0.19106638161068282, "avg_entropy_of_first_answer": 1.7390784621238708, "lex_sim": 1.0}
{"question": "Who was Prime Minister of China from 1949 to 1976?", "answer": ["Dangling Punishments"], "scores": [[0.027975479140877724, 0.010128023102879524, 0.11079678684473038, 0.003875727765262127, 0.014565249904990196, 0.06800713390111923, 0.5766202807426453]], "normalized_score": [0.035387441515922546], "tokens": [["\u2581D", "ang", "ling", "\u2581P", "un", "ish", "ments"]], "entropy": [[4.903475761413574, 2.9031927585601807, 5.462296485900879, 6.236184120178223, 4.924838066101074, 1.9297881126403809, 2.0080344676971436]], "avg_entropy": [4.052544253213065], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chao En-Lai", "score_of_biggest_cluster": 0.035387441515922546, "score_of_first_answer": 0.035387441515922546, "max_score_of_first_answer": 0.5766202807426453, "min_score_of_first_answer": 0.003875727765262127, "avg_score": 0.035387441515922546, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19135587050251412, "std_of_first_answer": 0.19135587050251412, "avg_entropy_of_first_answer": 4.052544253213065, "lex_sim": 1.0}
{"question": "A 'frontispiece' is traditionally at the beginning of a?", "answer": ["Book"], "scores": [[0.45560720562934875]], "normalized_score": [0.45560720562934875], "tokens": [["\u2581Book"]], "entropy": [[3.9718542098999023]], "avg_entropy": [3.9718542098999023], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Booke", "score_of_biggest_cluster": 0.45560720562934875, "score_of_first_answer": 0.45560720562934875, "max_score_of_first_answer": 0.45560720562934875, "min_score_of_first_answer": 0.45560720562934875, "avg_score": 0.45560720562934875, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.9718542098999023, "lex_sim": 1.0}
{"question": "What sort of cross can be seen on a former flag of Hungary and the current flag of Slovakia?", "answer": ["Croats, Slovenes and Serbs"], "scores": [[0.011557181365787983, 0.01432977244257927, 0.019891967996954918, 0.13138554990291595, 0.8676086664199829, 0.27927330136299133, 0.554893434047699, 0.9578105211257935]], "normalized_score": [0.12395665049552917], "tokens": [["\u2581Cro", "ats", ",", "\u2581Sloven", "es", "\u2581and", "\u2581Ser", "bs"]], "entropy": [[5.825047492980957, 2.7005631923675537, 1.9989938735961914, 4.6952595710754395, 0.5169376134872437, 1.2750160694122314, 2.3834280967712402, 0.2356393039226532]], "avg_entropy": [2.4538606517016888], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Two-barred cross", "score_of_biggest_cluster": 0.12395665049552917, "score_of_first_answer": 0.12395665049552917, "max_score_of_first_answer": 0.9578105211257935, "min_score_of_first_answer": 0.011557181365787983, "avg_score": 0.12395665049552917, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36540196293934707, "std_of_first_answer": 0.36540196293934707, "avg_entropy_of_first_answer": 2.4538606517016888, "lex_sim": 1.0}
{"question": "Which US President has a statue in Parliament Square?", "answer": ["Theodore Roosevelt (statue)"], "scores": [[0.03647231683135033, 0.4643995463848114, 0.7930587530136108, 0.9821717143058777, 0.9904195666313171, 0.9947380423545837, 0.038387443870306015, 0.14620313048362732, 0.9806498289108276, 0.7393379211425781]], "normalized_score": [0.3735406994819641], "tokens": [["\u2581The", "odore", "\u2581Ro", "ose", "vel", "t", "\u2581(", "stat", "ue", ")"]], "entropy": [[4.933440208435059, 3.8024048805236816, 1.5984902381896973, 0.13108056783676147, 0.09640173614025116, 0.04874083772301674, 0.7360805869102478, 3.937673568725586, 0.12555177509784698, 1.0296428203582764]], "avg_entropy": [1.6439507219940424], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Kennedy", "score_of_biggest_cluster": 0.3735406994819641, "score_of_first_answer": 0.3735406994819641, "max_score_of_first_answer": 0.9947380423545837, "min_score_of_first_answer": 0.03647231683135033, "avg_score": 0.3735406994819641, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3886415793072739, "std_of_first_answer": 0.3886415793072739, "avg_entropy_of_first_answer": 1.6439507219940424, "lex_sim": 1.0}
{"question": "'Cape Cod Salad' is an exotic summer dish centred", "answer": ["US salad"], "scores": [[0.004513180814683437, 0.02700231783092022, 0.9127494096755981]], "normalized_score": [0.04809261113405228], "tokens": [["\u2581US", "\u2581sal", "ad"]], "entropy": [[5.879669189453125, 5.132440567016602, 0.331432580947876]], "avg_entropy": [3.7811807791392007], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "LOBSTER", "score_of_biggest_cluster": 0.04809261113405228, "score_of_first_answer": 0.04809261113405228, "max_score_of_first_answer": 0.9127494096755981, "min_score_of_first_answer": 0.004513180814683437, "avg_score": 0.04809261113405228, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4229455861259139, "std_of_first_answer": 0.4229455861259139, "avg_entropy_of_first_answer": 3.7811807791392007, "lex_sim": 1.0}
{"question": "The character 'James Bond' was created by Ian Fleming, but who wrote the book 'Zero Minus Ten' featuring 'James Bond'?", "answer": ["A. R. Galdt"], "scores": [[0.01149940025061369, 0.0488884411752224, 0.011656304821372032, 0.7603532671928406, 0.024941641837358475, 0.018702475354075432, 0.005225601606070995]], "normalized_score": [0.02758234180510044], "tokens": [["\u2581A", ".", "\u2581R", ".", "\u2581G", "ald", "t"]], "entropy": [[5.359313011169434, 6.960162162780762, 4.780673980712891, 1.9031763076782227, 6.072057723999023, 2.8200151920318604, 4.5026164054870605]], "avg_entropy": [4.628287826265607], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Raymond Benson", "score_of_biggest_cluster": 0.02758234180510044, "score_of_first_answer": 0.02758234180510044, "max_score_of_first_answer": 0.7603532671928406, "min_score_of_first_answer": 0.005225601606070995, "avg_score": 0.02758234180510044, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2593527815824555, "std_of_first_answer": 0.2593527815824555, "avg_entropy_of_first_answer": 4.628287826265607, "lex_sim": 1.0}
{"question": "What was bandleader Joe Loss's signature tune", "answer": ["\ua54a\ub418\ub3fc"], "scores": [[0.032106913626194, 0.0036089259665459394, 0.013629976660013199, 0.009973311796784401, 0.004417513031512499, 0.03363591060042381, 0.15239360928535461, 0.11913198977708817, 0.037678223103284836, 0.2439013123512268]], "normalized_score": [0.02878453955054283], "tokens": [["\u2581", "<0xEA>", "<0x95>", "<0x8A>", "<0xEB>", "<0x90>", "<0x98>", "<0xEB>", "<0x8F>", "<0xBC>"]], "entropy": [[5.559760093688965, 4.252902984619141, 3.585885524749756, 4.022551536560059, 1.721449375152588, 3.9088876247406006, 3.1598803997039795, 2.8276655673980713, 3.7840888500213623, 2.8217215538024902]], "avg_entropy": [3.564479351043701], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "In The Mood", "score_of_biggest_cluster": 0.02878453955054283, "score_of_first_answer": 0.02878453955054283, "max_score_of_first_answer": 0.2439013123512268, "min_score_of_first_answer": 0.0036089259665459394, "avg_score": 0.02878453955054283, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07647982096109708, "std_of_first_answer": 0.07647982096109708, "avg_entropy_of_first_answer": 3.564479351043701, "lex_sim": 0.0}
{"question": "The moons of which planet are named after Shakespearian characters?", "answer": ["Uranus"], "scores": [[0.3397749662399292, 0.977893054485321, 0.9393661022186279]], "normalized_score": [0.6783271431922913], "tokens": [["\u2581U", "ran", "us"]], "entropy": [[4.3561296463012695, 0.21680666506290436, 0.3269672989845276]], "avg_entropy": [1.633301203449567], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Magnetosphere of Uranus", "score_of_biggest_cluster": 0.6783271431922913, "score_of_first_answer": 0.6783271431922913, "max_score_of_first_answer": 0.977893054485321, "min_score_of_first_answer": 0.3397749662399292, "avg_score": 0.6783271431922913, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2921545540256124, "std_of_first_answer": 0.2921545540256124, "avg_entropy_of_first_answer": 1.633301203449567, "lex_sim": 1.0}
{"question": "Who duetted with Lionel Ritchie for the hit song 'Endless Love'?", "answer": ["Dike 2000"], "scores": [[0.0029452925082296133, 0.011793326586484909, 0.002742599928751588, 0.14461649954319, 0.30759429931640625, 0.3229599893093109, 0.28423836827278137]], "normalized_score": [0.045259296894073486], "tokens": [["\u2581Di", "ke", "\u2581", "2", "0", "0", "0"]], "entropy": [[4.654187202453613, 5.192549705505371, 4.789892196655273, 3.891502857208252, 2.9039254188537598, 1.6947847604751587, 2.4835124015808105]], "avg_entropy": [3.658622077533177], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Diana Ernestine Earle Ross", "score_of_biggest_cluster": 0.045259296894073486, "score_of_first_answer": 0.045259296894073486, "max_score_of_first_answer": 0.3229599893093109, "min_score_of_first_answer": 0.002742599928751588, "avg_score": 0.045259296894073486, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13892837683030687, "std_of_first_answer": 0.13892837683030687, "avg_entropy_of_first_answer": 3.658622077533177, "lex_sim": 1.0}
{"question": "Acura is the luxury division of what major automaker?", "answer": ["Aircraft makers"], "scores": [[0.011331262066960335, 0.007953217253088951, 0.006041002459824085, 0.097978875041008]], "normalized_score": [0.015197272412478924], "tokens": [["\u2581A", "ircraft", "\u2581mak", "ers"]], "entropy": [[4.409280776977539, 5.576117038726807, 3.7288436889648438, 0.33324310183525085]], "avg_entropy": [3.51187115162611], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Honda Technologies", "score_of_biggest_cluster": 0.015197272412478924, "score_of_first_answer": 0.015197272412478924, "max_score_of_first_answer": 0.097978875041008, "min_score_of_first_answer": 0.006041002459824085, "avg_score": 0.015197272412478924, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.038816921976675384, "std_of_first_answer": 0.038816921976675384, "avg_entropy_of_first_answer": 3.51187115162611, "lex_sim": 1.0}
{"question": "Which island off Cape Cod was once the site of the world's biggest whaling port?", "answer": ["Nantucket"], "scores": [[0.4242655336856842, 0.951298177242279, 0.956168532371521, 0.9751245975494385]], "normalized_score": [0.7832262516021729], "tokens": [["\u2581N", "ant", "uck", "et"]], "entropy": [[4.136173725128174, 0.48577409982681274, 0.32350850105285645, 0.19997045397758484]], "avg_entropy": [1.286356694996357], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Town of Nantucket", "score_of_biggest_cluster": 0.7832262516021729, "score_of_first_answer": 0.7832262516021729, "max_score_of_first_answer": 0.9751245975494385, "min_score_of_first_answer": 0.4242655336856842, "avg_score": 0.7832262516021729, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23252428189020088, "std_of_first_answer": 0.23252428189020088, "avg_entropy_of_first_answer": 1.286356694996357, "lex_sim": 1.0}
{"question": "Nick Begs was lead singer with which 80\u2019s pop band?", "answer": ["ABC"], "scores": [[0.008037411607801914]], "normalized_score": [0.008037413470447063], "tokens": [["\u2581ABC"]], "entropy": [[5.4126081466674805]], "avg_entropy": [5.4126081466674805], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kaja Googoo", "score_of_biggest_cluster": 0.008037413470447063, "score_of_first_answer": 0.008037413470447063, "max_score_of_first_answer": 0.008037411607801914, "min_score_of_first_answer": 0.008037411607801914, "avg_score": 0.008037413470447063, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.4126081466674805, "lex_sim": 1.0}
{"question": "Name the ballet dancer who requested asylum in France while in Paris with the Kirov Ballet, in June 1961?", "answer": ["Rudolf Nureyev"], "scores": [[0.12285888195037842, 0.8654473423957825, 0.9204456210136414, 0.9624907970428467, 0.9944441914558411]], "normalized_score": [0.6227653622627258], "tokens": [["\u2581Rudolf", "\u2581N", "ure", "ye", "v"]], "entropy": [[5.264344692230225, 1.1022578477859497, 0.5857459902763367, 0.24978330731391907, 0.051727667450904846]], "avg_entropy": [1.450771901011467], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Nuriev", "score_of_biggest_cluster": 0.6227653622627258, "score_of_first_answer": 0.6227653622627258, "max_score_of_first_answer": 0.9944441914558411, "min_score_of_first_answer": 0.12285888195037842, "avg_score": 0.6227653622627258, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3279982906651399, "std_of_first_answer": 0.3279982906651399, "avg_entropy_of_first_answer": 1.450771901011467, "lex_sim": 1.0}
{"question": "The Archibald Fountain is in which Australian city?", "answer": ["Bondi"], "scores": [[0.0023230467922985554, 0.9367396831512451]], "normalized_score": [0.046648576855659485], "tokens": [["\u2581Bond", "i"]], "entropy": [[4.5593180656433105, 0.5636747479438782]], "avg_entropy": [2.5614964067935944], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sydney, New South Wales, Australia", "score_of_biggest_cluster": 0.046648576855659485, "score_of_first_answer": 0.046648576855659485, "max_score_of_first_answer": 0.9367396831512451, "min_score_of_first_answer": 0.0023230467922985554, "avg_score": 0.046648576855659485, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4672083181794733, "std_of_first_answer": 0.4672083181794733, "avg_entropy_of_first_answer": 2.5614964067935944, "lex_sim": 1.0}
{"question": "June 5, 2004 saw the death of what man, the 40th President of the United States?", "answer": ["Bill Clinton"], "scores": [[0.02797938697040081, 0.7609221339225769, 0.9818669557571411]], "normalized_score": [0.27547168731689453], "tokens": [["\u2581Bill", "\u2581Cl", "inton"]], "entropy": [[4.036926746368408, 1.770240068435669, 0.15502429008483887]], "avg_entropy": [1.987397034962972], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "President Regan", "score_of_biggest_cluster": 0.27547168731689453, "score_of_first_answer": 0.27547168731689453, "max_score_of_first_answer": 0.9818669557571411, "min_score_of_first_answer": 0.02797938697040081, "avg_score": 0.27547168731689453, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40769312785356193, "std_of_first_answer": 0.40769312785356193, "avg_entropy_of_first_answer": 1.987397034962972, "lex_sim": 1.0}
{"question": "Sherlock Holmes was proficient in which fictional martial art invented by Sir Arthur Conan Doyle?", "answer": ["Kumalahay"], "scores": [[0.0016299249837175012, 0.013269844464957714, 0.029576376080513, 0.08534203469753265, 0.042074572294950485]], "normalized_score": [0.018716823309659958], "tokens": [["\u2581K", "um", "al", "ah", "ay"]], "entropy": [[3.5388617515563965, 5.20667839050293, 4.977834224700928, 4.682901382446289, 3.8034305572509766]], "avg_entropy": [4.441941261291504], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Baritsu", "score_of_biggest_cluster": 0.018716823309659958, "score_of_first_answer": 0.018716823309659958, "max_score_of_first_answer": 0.08534203469753265, "min_score_of_first_answer": 0.0016299249837175012, "avg_score": 0.018716823309659958, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.028974527764418275, "std_of_first_answer": 0.028974527764418275, "avg_entropy_of_first_answer": 4.441941261291504, "lex_sim": 1.0}
{"question": "Which queen consort of England is associated with Hever Castle in Kent?", "answer": ["Anne Boyer"], "scores": [[0.32813760638237, 0.013439350761473179, 0.0011210378725081682]], "normalized_score": [0.01703537069261074], "tokens": [["\u2581Anne", "\u2581Bo", "yer"]], "entropy": [[4.307096481323242, 1.5968083143234253, 2.417433023452759]], "avg_entropy": [2.773779273033142], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Anne Boylen", "score_of_biggest_cluster": 0.01703537069261074, "score_of_first_answer": 0.01703537069261074, "max_score_of_first_answer": 0.32813760638237, "min_score_of_first_answer": 0.0011210378725081682, "avg_score": 0.01703537069261074, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.15133721324072974, "std_of_first_answer": 0.15133721324072974, "avg_entropy_of_first_answer": 2.773779273033142, "lex_sim": 1.0}
{"question": "Which famous English highwayman was hanged for horse-stealing in April 1739?", "answer": ["Jack Sheppard"], "scores": [[0.06425114721059799, 0.26929429173469543, 0.5021421313285828, 0.965865433216095]], "normalized_score": [0.30266544222831726], "tokens": [["\u2581Jack", "\u2581She", "pp", "ard"]], "entropy": [[5.259003639221191, 4.612722396850586, 1.0594934225082397, 0.2531531751155853]], "avg_entropy": [2.7960931584239006], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dick Turpin", "score_of_biggest_cluster": 0.30266544222831726, "score_of_first_answer": 0.30266544222831726, "max_score_of_first_answer": 0.965865433216095, "min_score_of_first_answer": 0.06425114721059799, "avg_score": 0.30266544222831726, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3355190287151243, "std_of_first_answer": 0.3355190287151243, "avg_entropy_of_first_answer": 2.7960931584239006, "lex_sim": 1.0}
{"question": "Which US state lends its name to a baked pudding, made with ice cream, sponge and meringue?", "answer": ["Mille-feuille-like"], "scores": [[0.014160212129354477, 0.015216845087707043, 0.4395507574081421, 0.706421971321106, 0.9866958856582642, 0.8954567313194275, 0.0021574883721768856, 0.0705576241016388]], "normalized_score": [0.09869007766246796], "tokens": [["\u2581M", "ille", "-", "fe", "u", "ille", "-", "like"]], "entropy": [[5.507957458496094, 4.193711280822754, 2.6606619358062744, 1.3073811531066895, 0.11195305734872818, 0.36148834228515625, 1.3217617273330688, 5.500425815582275]], "avg_entropy": [2.62066759634763], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Forty-ninth State", "score_of_biggest_cluster": 0.09869007766246796, "score_of_first_answer": 0.09869007766246796, "max_score_of_first_answer": 0.9866958856582642, "min_score_of_first_answer": 0.0021574883721768856, "avg_score": 0.09869007766246796, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3950127820222236, "std_of_first_answer": 0.3950127820222236, "avg_entropy_of_first_answer": 2.62066759634763, "lex_sim": 1.0}
{"question": "Born Jose Zapata, but known by his middle names, which Colombian goalkeeper who scored three goals for his country is best remembered for his 'scorpion kick' performed at Wembley in 1995?", "answer": ["Rene Higuan"], "scores": [[0.2902553677558899, 0.8670575022697449, 0.6421104073524475, 0.8507477641105652, 0.0057638101279735565]], "normalized_score": [0.2397671639919281], "tokens": [["\u2581R", "ene", "\u2581H", "igu", "an"]], "entropy": [[5.072345733642578, 1.020615816116333, 2.699535369873047, 1.2588751316070557, 0.6961815357208252]], "avg_entropy": [2.149510717391968], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jos\u00e9 Ren\u00e9 Higuita", "score_of_biggest_cluster": 0.2397671639919281, "score_of_first_answer": 0.2397671639919281, "max_score_of_first_answer": 0.8670575022697449, "min_score_of_first_answer": 0.0057638101279735565, "avg_score": 0.2397671639919281, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3350670555049058, "std_of_first_answer": 0.3350670555049058, "avg_entropy_of_first_answer": 2.149510717391968, "lex_sim": 1.0}
{"question": "Which World War 2 American general was known as 'Vinegar Joe'?", "answer": ["Joseph Stilwell"], "scores": [[0.15401701629161835, 0.32411280274391174, 0.9544598460197449, 0.994342565536499]], "normalized_score": [0.4665408134460449], "tokens": [["\u2581Joseph", "\u2581St", "il", "well"]], "entropy": [[5.1729736328125, 3.322493076324463, 0.2833366394042969, 0.049812402576208115]], "avg_entropy": [2.207153937779367], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Stilwell", "score_of_biggest_cluster": 0.4665408134460449, "score_of_first_answer": 0.4665408134460449, "max_score_of_first_answer": 0.994342565536499, "min_score_of_first_answer": 0.15401701629161835, "avg_score": 0.4665408134460449, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3728206916670323, "std_of_first_answer": 0.3728206916670323, "avg_entropy_of_first_answer": 2.207153937779367, "lex_sim": 1.0}
{"question": "Which Czech tennis player knocked out Andy Murray in the fourth round of the Men's Singles at the French Open?", "answer": ["Tomaz Ber"], "scores": [[0.2789675295352936, 0.00912952795624733, 0.12316831201314926]], "normalized_score": [0.06794649362564087], "tokens": [["\u2581Tom", "az", "\u2581Ber"]], "entropy": [[4.948271751403809, 1.2992734909057617, 4.231618881225586]], "avg_entropy": [3.4930547078450522], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tom\u00e1\u0161 Krupa", "score_of_biggest_cluster": 0.06794649362564087, "score_of_first_answer": 0.06794649362564087, "max_score_of_first_answer": 0.2789675295352936, "min_score_of_first_answer": 0.00912952795624733, "avg_score": 0.06794649362564087, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11059977272724415, "std_of_first_answer": 0.11059977272724415, "avg_entropy_of_first_answer": 3.4930547078450522, "lex_sim": 1.0}
{"question": "Man Ray was famous in what field?", "answer": ["Black and White photography"], "scores": [[0.0020359402988106012, 0.4656529724597931, 0.2742377817630768, 0.1428142637014389, 0.9597306251525879]], "normalized_score": [0.1289362907409668], "tokens": [["\u2581Black", "\u2581and", "\u2581White", "\u2581phot", "ography"]], "entropy": [[4.755127429962158, 3.4110846519470215, 0.8552998304367065, 3.3996903896331787, 0.22943264245986938]], "avg_entropy": [2.530126988887787], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pet photography", "score_of_biggest_cluster": 0.1289362907409668, "score_of_first_answer": 0.1289362907409668, "max_score_of_first_answer": 0.9597306251525879, "min_score_of_first_answer": 0.0020359402988106012, "avg_score": 0.1289362907409668, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33259792517577064, "std_of_first_answer": 0.33259792517577064, "avg_entropy_of_first_answer": 2.530126988887787, "lex_sim": 1.0}
{"question": "In which novel does Flora Poste move to live with her relatives, the Starkadders, in Sussex?", "answer": ["Cold Comfort Farm"], "scores": [[0.6455758213996887, 0.9796969294548035, 0.9958558678627014, 0.9367867708206177]], "normalized_score": [0.8764336705207825], "tokens": [["\u2581Cold", "\u2581Com", "fort", "\u2581Farm"]], "entropy": [[2.5205395221710205, 0.20312805473804474, 0.04046596586704254, 0.409949392080307]], "avg_entropy": [0.7935207337141037], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "I saw something nasty in the woodshed", "score_of_biggest_cluster": 0.8764336705207825, "score_of_first_answer": 0.8764336705207825, "max_score_of_first_answer": 0.9958558678627014, "min_score_of_first_answer": 0.6455758213996887, "avg_score": 0.8764336705207825, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14246236215457822, "std_of_first_answer": 0.14246236215457822, "avg_entropy_of_first_answer": 0.7935207337141037, "lex_sim": 1.0}
{"question": "In which book of the bible is the story of Noah's Ark?", "answer": ["Genesis"], "scores": [[0.41222816705703735, 0.9226699471473694]], "normalized_score": [0.6167256832122803], "tokens": [["\u2581Gen", "esis"]], "entropy": [[3.789085865020752, 0.5597511529922485]], "avg_entropy": [2.1744185090065002], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "G\u00e9nesis", "score_of_biggest_cluster": 0.6167256832122803, "score_of_first_answer": 0.6167256832122803, "max_score_of_first_answer": 0.9226699471473694, "min_score_of_first_answer": 0.41222816705703735, "avg_score": 0.6167256832122803, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.255220890045166, "std_of_first_answer": 0.255220890045166, "avg_entropy_of_first_answer": 2.1744185090065002, "lex_sim": 1.0}
{"question": "Which US state is called 'Old Dominion'?", "answer": ["Virginia; the Commonwealth of Virginia"], "scores": [[0.3640457093715668, 0.0004462910583242774, 0.03414592891931534, 0.1560559719800949, 0.9305949211120605, 0.9553170204162598]], "normalized_score": [0.0957304984331131], "tokens": [["\u2581Virginia", ";", "\u2581the", "\u2581Commonwealth", "\u2581of", "\u2581Virginia"]], "entropy": [[4.066709518432617, 1.123887538909912, 5.227873802185059, 4.612755298614502, 0.4158748984336853, 0.3284631669521332]], "avg_entropy": [2.629260703921318], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Virginia", "score_of_biggest_cluster": 0.0957304984331131, "score_of_first_answer": 0.0957304984331131, "max_score_of_first_answer": 0.9553170204162598, "min_score_of_first_answer": 0.0004462910583242774, "avg_score": 0.0957304984331131, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39663730813022363, "std_of_first_answer": 0.39663730813022363, "avg_entropy_of_first_answer": 2.629260703921318, "lex_sim": 1.0}
{"question": "Which fruit can go in the middle of the word aped to make a new word?", "answer": ["Sapience"], "scores": [[0.005527488887310028, 0.005193901248276234, 0.5870836973190308]], "normalized_score": [0.025639358907938004], "tokens": [["\u2581S", "api", "ence"]], "entropy": [[5.128373146057129, 4.864043235778809, 1.0500696897506714]], "avg_entropy": [3.6808286905288696], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pear (fruit)", "score_of_biggest_cluster": 0.025639358907938004, "score_of_first_answer": 0.025639358907938004, "max_score_of_first_answer": 0.5870836973190308, "min_score_of_first_answer": 0.005193901248276234, "avg_score": 0.025639358907938004, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27422688692586417, "std_of_first_answer": 0.27422688692586417, "avg_entropy_of_first_answer": 3.6808286905288696, "lex_sim": 1.0}
{"question": "In property rental the term 'pax' means what?", "answer": ["number of people to be accommodated"], "scores": [[0.007316220086067915, 0.9822900891304016, 0.3757666051387787, 0.04524537920951843, 0.5172974467277527, 0.7827067375183105, 0.9971628785133362]], "normalized_score": [0.2425113469362259], "tokens": [["\u2581number", "\u2581of", "\u2581people", "\u2581to", "\u2581be", "\u2581accommod", "ated"]], "entropy": [[5.35883903503418, 0.16512593626976013, 2.6173534393310547, 3.2248940467834473, 1.9344967603683472, 1.1963574886322021, 0.024675458669662476]], "avg_entropy": [2.074534595012665], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Per Annum Exclusive", "score_of_biggest_cluster": 0.2425113469362259, "score_of_first_answer": 0.2425113469362259, "max_score_of_first_answer": 0.9971628785133362, "min_score_of_first_answer": 0.007316220086067915, "avg_score": 0.2425113469362259, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3815011974185834, "std_of_first_answer": 0.3815011974185834, "avg_entropy_of_first_answer": 2.074534595012665, "lex_sim": 1.0}
{"question": "What was the name of Butch Cassidy's gang", "answer": ["The Wild Bunch"], "scores": [[0.15834157168865204, 0.7477654814720154, 0.9638388156890869, 0.9998388290405273]], "normalized_score": [0.5811974406242371], "tokens": [["\u2581The", "\u2581Wild", "\u2581B", "unch"]], "entropy": [[4.428071975708008, 1.4514509439468384, 0.32422012090682983, 0.0024316757917404175]], "avg_entropy": [1.551543679088354], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Hole in the Wall gang", "score_of_biggest_cluster": 0.5811974406242371, "score_of_first_answer": 0.5811974406242371, "max_score_of_first_answer": 0.9998388290405273, "min_score_of_first_answer": 0.15834157168865204, "avg_score": 0.5811974406242371, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3368872816798959, "std_of_first_answer": 0.3368872816798959, "avg_entropy_of_first_answer": 1.551543679088354, "lex_sim": 1.0}
{"question": "Which student of Socrates, and teacher of Aristole, wrote Republic?", "answer": ["Plato"], "scores": [[0.36217638850212097, 0.927959144115448]], "normalized_score": [0.5797283053398132], "tokens": [["\u2581Pl", "ato"]], "entropy": [[4.752715110778809, 0.5213397741317749]], "avg_entropy": [2.6370274424552917], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Plato's dialogues", "score_of_biggest_cluster": 0.5797283053398132, "score_of_first_answer": 0.5797283053398132, "max_score_of_first_answer": 0.927959144115448, "min_score_of_first_answer": 0.36217638850212097, "avg_score": 0.5797283053398132, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2828913778066635, "std_of_first_answer": 0.2828913778066635, "avg_entropy_of_first_answer": 2.6370274424552917, "lex_sim": 1.0}
{"question": "\"In what sport are the \"\"stones\"\" aimed at the \"\"house\"\"?\"", "answer": ["Curling"], "scores": [[0.5087036490440369, 0.9907517433166504]], "normalized_score": [0.7099288702011108], "tokens": [["\u2581Cur", "ling"]], "entropy": [[3.4331934452056885, 0.09321852028369904]], "avg_entropy": [1.7632059827446938], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Curling sheet", "score_of_biggest_cluster": 0.7099288702011108, "score_of_first_answer": 0.7099288702011108, "max_score_of_first_answer": 0.9907517433166504, "min_score_of_first_answer": 0.5087036490440369, "avg_score": 0.7099288702011108, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24102404713630676, "std_of_first_answer": 0.24102404713630676, "avg_entropy_of_first_answer": 1.7632059827446938, "lex_sim": 1.0}
{"question": "May 4, 1904 saw the US begin construction the Canal, following the creation of the state of Panama from Columbian territory due to the iron backbone and bad-assed support of which US president?", "answer": ["Theodore Roosevelt (United States)"], "scores": [[0.3181285858154297, 0.9223718047142029, 0.7703224420547485, 0.989284336566925, 0.9921033978462219, 0.9962032437324524, 0.04007257893681526, 0.021181544288992882, 0.9654524922370911, 0.9896542429924011, 0.42890995740890503]], "normalized_score": [0.4226502776145935], "tokens": [["\u2581The", "odore", "\u2581Ro", "ose", "vel", "t", "\u2581(", "Un", "ited", "\u2581States", ")"]], "entropy": [[4.32033634185791, 0.7375950217247009, 1.6534008979797363, 0.08773833513259888, 0.07855937629938126, 0.03571623936295509, 0.8729945421218872, 4.419427871704102, 0.20795869827270508, 0.09220914542675018, 1.604947805404663]], "avg_entropy": [1.2828076613897628], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "T Ros", "score_of_biggest_cluster": 0.4226502776145935, "score_of_first_answer": 0.4226502776145935, "max_score_of_first_answer": 0.9962032437324524, "min_score_of_first_answer": 0.021181544288992882, "avg_score": 0.4226502776145935, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3783543211048815, "std_of_first_answer": 0.3783543211048815, "avg_entropy_of_first_answer": 1.2828076613897628, "lex_sim": 1.0}
{"question": "Limburger cheese is made from the milk of which animal?", "answer": ["Cows"], "scores": [[0.07065283507108688, 0.5450219511985779]], "normalized_score": [0.19623291492462158], "tokens": [["\u2581C", "ows"]], "entropy": [[5.094078540802002, 2.4650373458862305]], "avg_entropy": [3.779557943344116], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "\ud83d\udc04", "score_of_biggest_cluster": 0.19623291492462158, "score_of_first_answer": 0.19623291492462158, "max_score_of_first_answer": 0.5450219511985779, "min_score_of_first_answer": 0.07065283507108688, "avg_score": 0.19623291492462158, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2371845580637455, "std_of_first_answer": 0.2371845580637455, "avg_entropy_of_first_answer": 3.779557943344116, "lex_sim": 1.0}
{"question": "Which British athlete won the gold medal in the Women's Javelin at the 1984 Olympic Games?", "answer": ["British woman with a javelin in one hand"], "scores": [[0.007857396267354488, 0.00598842604085803, 0.0049560898914933205, 0.33288174867630005, 0.33500027656555176, 0.9921283721923828, 0.996965229511261, 0.019633043557405472, 0.007736722007393837, 0.9728758931159973]], "normalized_score": [0.07210934162139893], "tokens": [["\u2581British", "\u2581woman", "\u2581with", "\u2581a", "\u2581j", "avel", "in", "\u2581in", "\u2581one", "\u2581hand"]], "entropy": [[4.069174289703369, 3.9933290481567383, 3.2546420097351074, 3.865720272064209, 4.445751667022705, 0.07997245341539383, 0.03082164376974106, 1.1619837284088135, 2.1853291988372803, 0.22712044417858124]], "avg_entropy": [2.3313844755291937], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tessa Sanderson", "score_of_biggest_cluster": 0.07210934162139893, "score_of_first_answer": 0.07210934162139893, "max_score_of_first_answer": 0.996965229511261, "min_score_of_first_answer": 0.0049560898914933205, "avg_score": 0.07210934162139893, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42391439513083373, "std_of_first_answer": 0.42391439513083373, "avg_entropy_of_first_answer": 2.3313844755291937, "lex_sim": 1.0}
{"question": "Which children's TV show used cabbages as the booby prize?", "answer": ["Crafty Cabbages"], "scores": [[0.13906915485858917, 0.002692718058824539, 0.5025835633277893, 0.9685723185539246, 0.6432158946990967]], "normalized_score": [0.16361527144908905], "tokens": [["\u2581Cra", "fty", "\u2581C", "abb", "ages"]], "entropy": [[4.897678375244141, 0.39174792170524597, 3.115154266357422, 0.24257053434848785, 0.6989459991455078]], "avg_entropy": [1.8692194193601608], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cracker-jack", "score_of_biggest_cluster": 0.16361527144908905, "score_of_first_answer": 0.16361527144908905, "max_score_of_first_answer": 0.9685723185539246, "min_score_of_first_answer": 0.002692718058824539, "avg_score": 0.16361527144908905, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3480715251995583, "std_of_first_answer": 0.3480715251995583, "avg_entropy_of_first_answer": 1.8692194193601608, "lex_sim": 1.0}
{"question": "The board game Pachisi originated in which country?", "answer": ["India"], "scores": [[0.5369660258293152]], "normalized_score": [0.5369660258293152], "tokens": [["\u2581India"]], "entropy": [[3.456488609313965]], "avg_entropy": [3.456488609313965], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "\u0d2d\u0d3e\u0d30\u0d24 \u0d2e\u0d39\u0d3e\u0d30\u0d3e\u0d1c\u0d4d\u0d2f\u0d02", "score_of_biggest_cluster": 0.5369660258293152, "score_of_first_answer": 0.5369660258293152, "max_score_of_first_answer": 0.5369660258293152, "min_score_of_first_answer": 0.5369660258293152, "avg_score": 0.5369660258293152, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.456488609313965, "lex_sim": 1.0}
{"question": "Which \u201cThom\u201d is Radiohead\u2019s lead singer and principal songwriter?", "answer": ["Yorke, who in July 2013 was awarded the Richard Lifetime Contribution Award."], "scores": [[0.10873917490243912, 0.9545267224311829, 0.9769796133041382, 0.031321555376052856, 0.009648829698562622, 0.002988079795613885, 0.0058088721707463264, 0.9185100197792053, 0.8711193203926086, 0.9965812563896179, 0.5458395481109619, 0.1361532062292099, 0.1254711151123047, 0.10963219404220581, 0.37446001172065735, 0.002062358194962144, 0.003559160279110074, 0.4776262044906616, 0.9712696075439453, 0.060112327337265015, 0.9806528091430664, 0.5497419238090515, 0.027952663600444794]], "normalized_score": [0.11854979395866394], "tokens": [["\u2581Y", "or", "ke", ",", "\u2581who", "\u2581in", "\u2581July", "\u2581", "2", "0", "1", "3", "\u2581was", "\u2581awarded", "\u2581the", "\u2581Richard", "\u2581L", "if", "etime", "\u2581Cont", "ribution", "\u2581Award", "."]], "entropy": [[5.261584758758545, 0.424885630607605, 0.20491337776184082, 1.7382820844650269, 2.820295810699463, 4.311051845550537, 3.618690252304077, 0.5934697389602661, 0.4076407551765442, 0.03206810727715492, 0.7040711045265198, 2.2008421421051025, 4.17404842376709, 4.012620449066162, 1.2315144538879395, 3.6004199981689453, 2.86039662361145, 2.687605381011963, 0.21322782337665558, 1.6670969724655151, 0.10740700364112854, 1.6396191120147705, 2.128436803817749]], "avg_entropy": [2.027834289261828], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Youwouldn\u2019tlikemewhenI\u2019mangry", "score_of_biggest_cluster": 0.11854979395866394, "score_of_first_answer": 0.11854979395866394, "max_score_of_first_answer": 0.9965812563896179, "min_score_of_first_answer": 0.002062358194962144, "avg_score": 0.11854979395866394, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4003160730895515, "std_of_first_answer": 0.4003160730895515, "avg_entropy_of_first_answer": 2.027834289261828, "lex_sim": 1.0}
{"question": "Which is the only mythical creature found in the Chinese zodiac?", "answer": ["Sai"], "scores": [[0.028379295021295547, 0.0027539373841136694]], "normalized_score": [0.008840522728860378], "tokens": [["\u2581S", "ai"]], "entropy": [[5.14474630355835, 3.0283753871917725]], "avg_entropy": [4.086560845375061], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mythical dragon", "score_of_biggest_cluster": 0.008840522728860378, "score_of_first_answer": 0.008840522728860378, "max_score_of_first_answer": 0.028379295021295547, "min_score_of_first_answer": 0.0027539373841136694, "avg_score": 0.008840522728860378, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.012812678818590939, "std_of_first_answer": 0.012812678818590939, "avg_entropy_of_first_answer": 4.086560845375061, "lex_sim": 1.0}
{"question": "The Royal Shakespeare Company is based in which British town?", "answer": ["Macbeth-the most popular play performed by the RSC"], "scores": [[0.0015591856790706515, 0.5919951796531677, 0.9831047654151917, 0.0023838733322918415, 0.018760278820991516, 0.0020318110473454, 0.031964343041181564, 0.38052433729171753, 0.016596002504229546, 0.7563565969467163, 0.8440535068511963, 0.5363753437995911, 0.9641206860542297]], "normalized_score": [0.07999717444181442], "tokens": [["\u2581Mac", "b", "eth", "-", "the", "\u2581most", "\u2581popular", "\u2581play", "\u2581performed", "\u2581by", "\u2581the", "\u2581R", "SC"]], "entropy": [[4.80532693862915, 2.8698277473449707, 0.17667043209075928, 2.4512734413146973, 5.996988296508789, 4.773486137390137, 4.967841148376465, 3.057455062866211, 2.6566247940063477, 1.24150812625885, 0.9043773412704468, 1.5639238357543945, 0.25174063444137573]], "avg_entropy": [2.7474649181732764], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stratford-Upon-Avon", "score_of_biggest_cluster": 0.07999717444181442, "score_of_first_answer": 0.07999717444181442, "max_score_of_first_answer": 0.9831047654151917, "min_score_of_first_answer": 0.0015591856790706515, "avg_score": 0.07999717444181442, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38639384877835053, "std_of_first_answer": 0.38639384877835053, "avg_entropy_of_first_answer": 2.7474649181732764, "lex_sim": 1.0}
{"question": "What is the waxy secretion of the sperm whale used in perfume manufacturing", "answer": ["Aromatic ambergris"], "scores": [[0.049336645752191544, 0.04425156116485596, 0.7353076338768005, 0.2224939614534378, 0.9962515234947205, 0.9624035954475403, 0.9932727813720703]], "normalized_score": [0.3195403814315796], "tokens": [["\u2581A", "rom", "atic", "\u2581am", "ber", "gr", "is"]], "entropy": [[4.422190189361572, 5.37945556640625, 1.0170456171035767, 4.394574165344238, 0.03618254512548447, 0.3027845025062561, 0.06476777791976929]], "avg_entropy": [2.2310000519667352], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ambarum", "score_of_biggest_cluster": 0.3195403814315796, "score_of_first_answer": 0.3195403814315796, "max_score_of_first_answer": 0.9962515234947205, "min_score_of_first_answer": 0.04425156116485596, "avg_score": 0.3195403814315796, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41582865000864994, "std_of_first_answer": 0.41582865000864994, "avg_entropy_of_first_answer": 2.2310000519667352, "lex_sim": 1.0}
{"question": "According to L Frank Baum's 'The Wonderful Wizard of Oz', the Road to the City of Emeralds is paved with what?", "answer": ["Yellow Brick Road"], "scores": [[0.27940821647644043, 0.9379599094390869, 0.5382116436958313, 0.7342016100883484, 0.6396520733833313]], "normalized_score": [0.5810682773590088], "tokens": [["\u2581Y", "ellow", "\u2581Br", "ick", "\u2581Road"]], "entropy": [[5.010793685913086, 0.4889548718929291, 2.0148537158966064, 0.6451758742332458, 1.2501273155212402]], "avg_entropy": [1.8819810926914216], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Roads of yellow-brick", "score_of_biggest_cluster": 0.5810682773590088, "score_of_first_answer": 0.5810682773590088, "max_score_of_first_answer": 0.9379599094390869, "min_score_of_first_answer": 0.27940821647644043, "avg_score": 0.5810682773590088, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2177363306130449, "std_of_first_answer": 0.2177363306130449, "avg_entropy_of_first_answer": 1.8819810926914216, "lex_sim": 1.0}
{"question": "Who was the British Prime Minister at the time of Queen Victoria's ascension to the throne?", "answer": ["Lord Melbourne"], "scores": [[0.19279101490974426, 0.6654893755912781]], "normalized_score": [0.3581904470920563], "tokens": [["\u2581Lord", "\u2581Melbourne"]], "entropy": [[4.555732250213623, 1.849388837814331]], "avg_entropy": [3.202560544013977], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "William Melbourne", "score_of_biggest_cluster": 0.3581904470920563, "score_of_first_answer": 0.3581904470920563, "max_score_of_first_answer": 0.6654893755912781, "min_score_of_first_answer": 0.19279101490974426, "avg_score": 0.3581904470920563, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2363491803407669, "std_of_first_answer": 0.2363491803407669, "avg_entropy_of_first_answer": 3.202560544013977, "lex_sim": 1.0}
{"question": "Artiodactyla Suidae is the scientific name for which farm animal?", "answer": ["Pigs"], "scores": [[0.3341602385044098, 0.23028132319450378]], "normalized_score": [0.2774001955986023], "tokens": [["\u2581P", "igs"]], "entropy": [[4.400062561035156, 1.1610767841339111]], "avg_entropy": [2.7805696725845337], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Chazer", "score_of_biggest_cluster": 0.2774001955986023, "score_of_first_answer": 0.2774001955986023, "max_score_of_first_answer": 0.3341602385044098, "min_score_of_first_answer": 0.23028132319450378, "avg_score": 0.2774001955986023, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.051939457654953, "std_of_first_answer": 0.051939457654953, "avg_entropy_of_first_answer": 2.7805696725845337, "lex_sim": 1.0}
{"question": "\"What is the first line of \"\"Walk on the Wild Side\"\" by Lou Reed, released as a single in 1972?\"", "answer": ["He never noticed that she was so much older than he was, when it should have been obvious, the way she looked at him."], "scores": [[0.007271102629601955, 0.22831761837005615, 0.04506572708487511, 0.21069861948490143, 0.12388525903224945, 0.755933403968811, 0.20948860049247742, 0.02748863957822323, 0.39131131768226624, 0.6632031798362732, 0.3924182951450348, 0.15487612783908844, 0.04334156960248947, 0.014438953250646591, 0.007680015172809362, 0.015103541314601898, 0.4344273805618286, 0.9304172992706299, 0.3156221807003021, 0.06813086569309235, 0.010699604637920856, 0.32302165031433105, 0.7353537678718567, 0.13801412284374237, 0.495128333568573, 0.8981856107711792, 0.18481387197971344]], "normalized_score": [0.1334909051656723], "tokens": [["\u2581He", "\u2581never", "\u2581noticed", "\u2581that", "\u2581she", "\u2581was", "\u2581so", "\u2581much", "\u2581older", "\u2581than", "\u2581he", "\u2581was", ",", "\u2581when", "\u2581it", "\u2581should", "\u2581have", "\u2581been", "\u2581obvious", ",", "\u2581the", "\u2581way", "\u2581she", "\u2581looked", "\u2581at", "\u2581him", "."]], "entropy": [[4.611701011657715, 2.138429880142212, 4.329532623291016, 2.917693614959717, 2.589597702026367, 1.4304099082946777, 4.310198783874512, 5.131947994232178, 2.3380637168884277, 1.524557113647461, 1.2955591678619385, 2.0064749717712402, 2.033094644546509, 4.5202155113220215, 1.9743362665176392, 3.2308907508850098, 1.3008067607879639, 0.5576610565185547, 3.8618581295013428, 2.903209686279297, 4.342617034912109, 4.843517780303955, 1.2172173261642456, 4.503593444824219, 2.255247116088867, 0.7171230316162109, 2.8909215927124023]], "avg_entropy": [2.8065361711714], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Holly came from Miami F L A", "score_of_biggest_cluster": 0.1334909051656723, "score_of_first_answer": 0.1334909051656723, "max_score_of_first_answer": 0.9304172992706299, "min_score_of_first_answer": 0.007271102629601955, "avg_score": 0.1334909051656723, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28185160225129957, "std_of_first_answer": 0.28185160225129957, "avg_entropy_of_first_answer": 2.8065361711714, "lex_sim": 1.0}
{"question": "Which word can go before Work, Guard and Dog to make three other words?", "answer": ["Bite"], "scores": [[0.021124349907040596, 0.06592798978090286]], "normalized_score": [0.03731871023774147], "tokens": [["\u2581B", "ite"]], "entropy": [[6.473297595977783, 3.982003688812256]], "avg_entropy": [5.2276506423950195], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Open flame", "score_of_biggest_cluster": 0.03731871023774147, "score_of_first_answer": 0.03731871023774147, "max_score_of_first_answer": 0.06592798978090286, "min_score_of_first_answer": 0.021124349907040596, "avg_score": 0.03731871023774147, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.022401819936931133, "std_of_first_answer": 0.022401819936931133, "avg_entropy_of_first_answer": 5.2276506423950195, "lex_sim": 1.0}
{"question": "What colour is the bottom stripe on the U.S. flag?", "answer": ["white"], "scores": [[0.016070138663053513]], "normalized_score": [0.016070136800408363], "tokens": [["\u2581white"]], "entropy": [[5.557141304016113]], "avg_entropy": [5.557141304016113], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Red (colour)", "score_of_biggest_cluster": 0.016070136800408363, "score_of_first_answer": 0.016070136800408363, "max_score_of_first_answer": 0.016070138663053513, "min_score_of_first_answer": 0.016070138663053513, "avg_score": 0.016070136800408363, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.557141304016113, "lex_sim": 1.0}
{"question": "Smoky, Leaf-nosed and Vesper are all species of which creature?", "answer": ["Bat"], "scores": [[0.12607884407043457]], "normalized_score": [0.12607885897159576], "tokens": [["\u2581Bat"]], "entropy": [[4.68400764465332]], "avg_entropy": [4.68400764465332], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bat Conservation", "score_of_biggest_cluster": 0.12607885897159576, "score_of_first_answer": 0.12607885897159576, "max_score_of_first_answer": 0.12607884407043457, "min_score_of_first_answer": 0.12607884407043457, "avg_score": 0.12607885897159576, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.68400764465332, "lex_sim": 1.0}
{"question": "In 2006, who became the UK's first woman Secretary of State for Foreign and Commonwealth Affairs?", "answer": ["Maria Carmelite"], "scores": [[0.002068807603791356, 0.04869428649544716, 0.014755352400243282, 0.05060308426618576]], "normalized_score": [0.01656079664826393], "tokens": [["\u2581Maria", "\u2581Car", "mel", "ite"]], "entropy": [[4.885339736938477, 5.093605995178223, 2.8432228565216064, 3.3868746757507324]], "avg_entropy": [4.0522608160972595], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Margaret Becket", "score_of_biggest_cluster": 0.01656079664826393, "score_of_first_answer": 0.01656079664826393, "max_score_of_first_answer": 0.05060308426618576, "min_score_of_first_answer": 0.002068807603791356, "avg_score": 0.01656079664826393, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02111133337255007, "std_of_first_answer": 0.02111133337255007, "avg_entropy_of_first_answer": 4.0522608160972595, "lex_sim": 1.0}
{"question": "Who is the father of the twin boys born to Brooke Mueller on 14 March 2009?", "answer": ["Bob the builder"], "scores": [[0.015200932510197163, 0.011892450973391533, 0.18856124579906464]], "normalized_score": [0.03242386132478714], "tokens": [["\u2581Bob", "\u2581the", "\u2581builder"]], "entropy": [[4.573792457580566, 4.720844745635986, 3.4724650382995605]], "avg_entropy": [4.255700747172038], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bi-winning", "score_of_biggest_cluster": 0.03242386132478714, "score_of_first_answer": 0.03242386132478714, "max_score_of_first_answer": 0.18856124579906464, "min_score_of_first_answer": 0.011892450973391533, "avg_score": 0.03242386132478714, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08251370750692777, "std_of_first_answer": 0.08251370750692777, "avg_entropy_of_first_answer": 4.255700747172038, "lex_sim": 1.0}
{"question": "In which is Shakespeare's plays are the title character's last words the rest is silence?", "answer": ["Taming of the Shrew"], "scores": [[0.005690436810255051, 0.021045083180069923, 0.940116822719574, 0.9687147736549377, 0.8836342096328735, 0.9984639883041382]], "normalized_score": [0.21406538784503937], "tokens": [["\u2581T", "aming", "\u2581of", "\u2581the", "\u2581Sh", "rew"]], "entropy": [[4.52965784072876, 4.740518569946289, 0.4321504533290863, 0.22913888096809387, 0.6653590202331543, 0.017939526587724686]], "avg_entropy": [1.769127381965518], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Murder of Gonzago", "score_of_biggest_cluster": 0.21406538784503937, "score_of_first_answer": 0.21406538784503937, "max_score_of_first_answer": 0.9984639883041382, "min_score_of_first_answer": 0.005690436810255051, "avg_score": 0.21406538784503937, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4418424002636849, "std_of_first_answer": 0.4418424002636849, "avg_entropy_of_first_answer": 1.769127381965518, "lex_sim": 1.0}
{"question": "The region of Catalonia is part of which country?", "answer": ["2017 General Election"], "scores": [[0.043153274804353714, 0.06149408966302872, 0.4396091103553772, 0.5711133480072021, 0.14462615549564362, 0.005441009066998959, 0.5541677474975586, 0.9594405293464661]], "normalized_score": [0.15158376097679138], "tokens": [["\u2581", "2", "0", "1", "7", "\u2581General", "\u2581E", "lection"]], "entropy": [[4.659501552581787, 4.945758819580078, 2.851944923400879, 1.701204538345337, 2.3208413124084473, 4.831487655639648, 2.0021862983703613, 0.1792437881231308]], "avg_entropy": [2.9365211110562086], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Islands of Spain", "score_of_biggest_cluster": 0.15158376097679138, "score_of_first_answer": 0.15158376097679138, "max_score_of_first_answer": 0.9594405293464661, "min_score_of_first_answer": 0.005441009066998959, "avg_score": 0.15158376097679138, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31785573239620524, "std_of_first_answer": 0.31785573239620524, "avg_entropy_of_first_answer": 2.9365211110562086, "lex_sim": 1.0}
{"question": "In 2015 Japan lowered its voting age from what to what?", "answer": ["18 to 18"], "scores": [[0.6674094200134277, 0.5579996109008789, 0.8584533929824829, 0.5882026553153992, 0.9871335029602051, 0.9851346611976624, 0.9732711911201477]], "normalized_score": [0.7814671993255615], "tokens": [["\u2581", "1", "8", "\u2581to", "\u2581", "1", "8"]], "entropy": [[2.5439302921295166, 1.5804698467254639, 0.6937235593795776, 2.0031728744506836, 0.13305087387561798, 0.1048552468419075, 0.17463058233261108]], "avg_entropy": [1.0334047536764825], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "20 to 18", "score_of_biggest_cluster": 0.7814671993255615, "score_of_first_answer": 0.7814671993255615, "max_score_of_first_answer": 0.9871335029602051, "min_score_of_first_answer": 0.5579996109008789, "avg_score": 0.7814671993255615, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1787623271435757, "std_of_first_answer": 0.1787623271435757, "avg_entropy_of_first_answer": 1.0334047536764825, "lex_sim": 1.0}
{"question": "Goodge Street tube station is on which London Underground line?", "answer": ["Vaughan Williams"], "scores": [[0.0022168951109051704, 0.014298917725682259, 0.8220908045768738, 0.16747701168060303]], "normalized_score": [0.04570680484175682], "tokens": [["\u2581V", "aug", "han", "\u2581Williams"]], "entropy": [[4.573450088500977, 6.067877769470215, 0.758691668510437, 4.036698818206787]], "avg_entropy": [3.859179586172104], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Northern (disambiguation)", "score_of_biggest_cluster": 0.04570680484175682, "score_of_first_answer": 0.04570680484175682, "max_score_of_first_answer": 0.8220908045768738, "min_score_of_first_answer": 0.0022168951109051704, "avg_score": 0.04570680484175682, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3357976127448662, "std_of_first_answer": 0.3357976127448662, "avg_entropy_of_first_answer": 3.859179586172104, "lex_sim": 1.0}
{"question": "Who has presented BBC TV programmes called his 'Toy Stories' and `Man Lab'?", "answer": ["Tom Wopat"], "scores": [[0.008502491749823093, 0.10274721682071686, 0.021590029820799828, 0.9730754494667053]], "normalized_score": [0.0654529333114624], "tokens": [["\u2581Tom", "\u2581W", "op", "at"]], "entropy": [[5.7513957023620605, 5.158977508544922, 4.106290340423584, 0.23527362942695618]], "avg_entropy": [3.8129842951893806], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Captain Slow", "score_of_biggest_cluster": 0.0654529333114624, "score_of_first_answer": 0.0654529333114624, "max_score_of_first_answer": 0.9730754494667053, "min_score_of_first_answer": 0.008502491749823093, "avg_score": 0.0654529333114624, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.40379733220672687, "std_of_first_answer": 0.40379733220672687, "avg_entropy_of_first_answer": 3.8129842951893806, "lex_sim": 1.0}
{"question": "\"What are the words called in crosswords that appear under the headings \"\"Across\"\" and \"\"Down\"\"?\"", "answer": ["Clues"], "scores": [[0.2193106710910797, 0.40948864817619324]], "normalized_score": [0.29967519640922546], "tokens": [["\u2581Cl", "ues"]], "entropy": [[4.416253566741943, 0.8950374722480774]], "avg_entropy": [2.6556455194950104], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Clue (disambiguation)", "score_of_biggest_cluster": 0.29967519640922546, "score_of_first_answer": 0.29967519640922546, "max_score_of_first_answer": 0.40948864817619324, "min_score_of_first_answer": 0.2193106710910797, "avg_score": 0.29967519640922546, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09508898854255676, "std_of_first_answer": 0.09508898854255676, "avg_entropy_of_first_answer": 2.6556455194950104, "lex_sim": 1.0}
{"question": "The roe of lumpfish is used as a substitute for which delicacy?", "answer": ["Tuna"], "scores": [[0.0030567217618227005, 0.17386449873447418]], "normalized_score": [0.02305331639945507], "tokens": [["\u2581T", "una"]], "entropy": [[3.9397788047790527, 4.685061454772949]], "avg_entropy": [4.312420129776001], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ikra", "score_of_biggest_cluster": 0.02305331639945507, "score_of_first_answer": 0.02305331639945507, "max_score_of_first_answer": 0.17386449873447418, "min_score_of_first_answer": 0.0030567217618227005, "avg_score": 0.02305331639945507, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08540388848632574, "std_of_first_answer": 0.08540388848632574, "avg_entropy_of_first_answer": 4.312420129776001, "lex_sim": 1.0}
{"question": "What was the name of the 'Birdman of Alcatraz'?", "answer": ["Robins"], "scores": [[0.011527962982654572, 0.24269497394561768]], "normalized_score": [0.05289402976632118], "tokens": [["\u2581Rob", "ins"]], "entropy": [[4.794774532318115, 3.41625714302063]], "avg_entropy": [4.105515837669373], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Birdman of Alcatrez", "score_of_biggest_cluster": 0.05289402976632118, "score_of_first_answer": 0.05289402976632118, "max_score_of_first_answer": 0.24269497394561768, "min_score_of_first_answer": 0.011527962982654572, "avg_score": 0.05289402976632118, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11558350548148155, "std_of_first_answer": 0.11558350548148155, "avg_entropy_of_first_answer": 4.105515837669373, "lex_sim": 1.0}
{"question": "'Baby Come Back' was a number one hit in 1968 for which group, founded by Eddy Grant?", "answer": ["The Equals"], "scores": [[0.2233995795249939, 0.9209817051887512, 0.9748970866203308]], "normalized_score": [0.5853703022003174], "tokens": [["\u2581The", "\u2581Equ", "als"]], "entropy": [[5.618111610412598, 0.7079067230224609, 0.17149940133094788]], "avg_entropy": [2.1658392449220023], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Equally", "score_of_biggest_cluster": 0.5853703022003174, "score_of_first_answer": 0.5853703022003174, "max_score_of_first_answer": 0.9748970866203308, "min_score_of_first_answer": 0.2233995795249939, "avg_score": 0.5853703022003174, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3422598417659673, "std_of_first_answer": 0.3422598417659673, "avg_entropy_of_first_answer": 2.1658392449220023, "lex_sim": 1.0}
{"question": "In which decade was the BBC's current affairs programme Panorama first broadcast?", "answer": ["Panorama"], "scores": [[0.0384179949760437, 0.9268702864646912, 0.9515235424041748]], "normalized_score": [0.323587030172348], "tokens": [["\u2581Pan", "or", "ama"]], "entropy": [[3.783535957336426, 0.5707643032073975, 0.24063920974731445]], "avg_entropy": [1.531646490097046], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Fifties", "score_of_biggest_cluster": 0.323587030172348, "score_of_first_answer": 0.323587030172348, "max_score_of_first_answer": 0.9515235424041748, "min_score_of_first_answer": 0.0384179949760437, "avg_score": 0.323587030172348, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42475051475004755, "std_of_first_answer": 0.42475051475004755, "avg_entropy_of_first_answer": 1.531646490097046, "lex_sim": 1.0}
{"question": "The highest 'need' in Maslow's original 'Hierarchy of Needs' theory is?", "answer": ["Esteem and Self Actualization"], "scores": [[0.049541059881448746, 0.9898776412010193, 0.009701622650027275, 0.3911128640174866, 0.15984652936458588, 0.9864562749862671, 0.6326183080673218]], "normalized_score": [0.2109055519104004], "tokens": [["\u2581Este", "em", "\u2581and", "\u2581Self", "\u2581Act", "ual", "ization"]], "entropy": [[4.306902885437012, 0.0880732536315918, 1.1700375080108643, 2.166879892349243, 0.6560336351394653, 0.11417368054389954, 0.782701849937439]], "avg_entropy": [1.326400386435645], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Self-actualise", "score_of_biggest_cluster": 0.2109055519104004, "score_of_first_answer": 0.2109055519104004, "max_score_of_first_answer": 0.9898776412010193, "min_score_of_first_answer": 0.009701622650027275, "avg_score": 0.2109055519104004, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3879996715444517, "std_of_first_answer": 0.3879996715444517, "avg_entropy_of_first_answer": 1.326400386435645, "lex_sim": 1.0}
{"question": "\"Who, as part of his performance in the 2015 film \"\"Revenant\"\", was realistically eviscerated by a bear on screen?\"", "answer": ["Leonardo Di Caprio"], "scores": [[0.386008620262146, 0.9437917470932007, 0.761921226978302, 0.07815936207771301, 0.9403117895126343]], "normalized_score": [0.45912107825279236], "tokens": [["\u2581Leon", "ardo", "\u2581Di", "\u2581Cap", "rio"]], "entropy": [[4.134441375732422, 0.485745906829834, 1.359076976776123, 0.5699934959411621, 0.45463812351226807]], "avg_entropy": [1.4007791757583619], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Leonardo D%60caprio", "score_of_biggest_cluster": 0.45912107825279236, "score_of_first_answer": 0.45912107825279236, "max_score_of_first_answer": 0.9437917470932007, "min_score_of_first_answer": 0.07815936207771301, "avg_score": 0.45912107825279236, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33938321612455774, "std_of_first_answer": 0.33938321612455774, "avg_entropy_of_first_answer": 1.4007791757583619, "lex_sim": 1.0}
{"question": "With a motto of Always Ready, Always There, what US military branch had it's founding on Dec 14, 1636?", "answer": ["Air Defense Artillery"], "scores": [[0.03784272074699402, 0.007666377350687981, 0.6922899484634399, 0.5322626233100891, 0.992660641670227]], "normalized_score": [0.1603827178478241], "tokens": [["\u2581Air", "\u2581Def", "ense", "\u2581Art", "illery"]], "entropy": [[4.148055553436279, 1.737532615661621, 0.742893636226654, 1.9388337135314941, 0.06357625126838684]], "avg_entropy": [1.7261783540248872], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "National Guard", "score_of_biggest_cluster": 0.1603827178478241, "score_of_first_answer": 0.1603827178478241, "max_score_of_first_answer": 0.992660641670227, "min_score_of_first_answer": 0.007666377350687981, "avg_score": 0.1603827178478241, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38090744688528455, "std_of_first_answer": 0.38090744688528455, "avg_entropy_of_first_answer": 1.7261783540248872, "lex_sim": 1.0}
{"question": "What age came after the Bronze Age?", "answer": ["Iron Age"], "scores": [[0.3820609152317047, 0.8336499333381653]], "normalized_score": [0.5643625259399414], "tokens": [["\u2581Iron", "\u2581Age"]], "entropy": [[4.255589008331299, 0.8519288897514343]], "avg_entropy": [2.5537589490413666], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "First Iron Age", "score_of_biggest_cluster": 0.5643625259399414, "score_of_first_answer": 0.5643625259399414, "max_score_of_first_answer": 0.8336499333381653, "min_score_of_first_answer": 0.3820609152317047, "avg_score": 0.5643625259399414, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22579450905323029, "std_of_first_answer": 0.22579450905323029, "avg_entropy_of_first_answer": 2.5537589490413666, "lex_sim": 1.0}
{"question": "In which present day British county was the first UK Women\u2019s Institute founded in 1915?", "answer": ["Caldicott"], "scores": [[0.007525311782956123, 0.0029152550268918276, 0.6459164619445801, 0.36083245277404785]], "normalized_score": [0.047552209347486496], "tokens": [["\u2581C", "ald", "ic", "ott"]], "entropy": [[5.658908367156982, 4.870782375335693, 2.208834648132324, 1.6302158832550049]], "avg_entropy": [3.592185318470001], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ynys M\u00f4n (Anglesey)", "score_of_biggest_cluster": 0.047552209347486496, "score_of_first_answer": 0.047552209347486496, "max_score_of_first_answer": 0.6459164619445801, "min_score_of_first_answer": 0.0029152550268918276, "avg_score": 0.047552209347486496, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2687027416506016, "std_of_first_answer": 0.2687027416506016, "avg_entropy_of_first_answer": 3.592185318470001, "lex_sim": 1.0}
{"question": "Who is the father-in-law of Manchester City footballer Sergio K\u0171n Aguerro?", "answer": ["\ua790\ua790\ua790"], "scores": [[0.026853976771235466, 0.010204951278865337, 0.09171763807535172, 0.018513070419430733, 0.38364291191101074, 0.867272675037384, 0.31940585374832153, 0.6029838919639587, 0.9967870712280273, 0.8794311285018921]], "normalized_score": [0.17447315156459808], "tokens": [["\u2581", "<0xEA>", "<0x9E>", "<0x90>", "<0xEA>", "<0x9E>", "<0x90>", "<0xEA>", "<0x9E>", "<0x90>"]], "entropy": [[5.434531211853027, 5.321776866912842, 3.5584256649017334, 3.882261276245117, 1.77517569065094, 0.8060645461082458, 2.129241466522217, 1.5626132488250732, 0.028415802866220474, 0.6298176050186157]], "avg_entropy": [2.512832337990403], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Maradonna", "score_of_biggest_cluster": 0.17447315156459808, "score_of_first_answer": 0.17447315156459808, "max_score_of_first_answer": 0.9967870712280273, "min_score_of_first_answer": 0.010204951278865337, "avg_score": 0.17447315156459808, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.371509499712013, "std_of_first_answer": 0.371509499712013, "avg_entropy_of_first_answer": 2.512832337990403, "lex_sim": 0.0}
{"question": "\u2018Be the best\u2019 is the official motto of which of the British armed forces?", "answer": ["Air Force"], "scores": [[0.008897455409169197, 0.600864052772522]], "normalized_score": [0.0731174424290657], "tokens": [["\u2581Air", "\u2581Force"]], "entropy": [[3.919910430908203, 2.101881504058838]], "avg_entropy": [3.0108959674835205], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ground formation", "score_of_biggest_cluster": 0.0731174424290657, "score_of_first_answer": 0.0731174424290657, "max_score_of_first_answer": 0.600864052772522, "min_score_of_first_answer": 0.008897455409169197, "avg_score": 0.0731174424290657, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2959832986816764, "std_of_first_answer": 0.2959832986816764, "avg_entropy_of_first_answer": 3.0108959674835205, "lex_sim": 1.0}
{"question": "Which of the 'Classic' horse races, run at Epsom for three year old fillies on the Friday after the derby, is named after the estate then owned by the Earl of Derby?", "answer": ["Misses"], "scores": [[0.0019922740757465363, 0.03259456902742386]], "normalized_score": [0.008058370091021061], "tokens": [["\u2581Miss", "es"]], "entropy": [[3.5854597091674805, 6.0528340339660645]], "avg_entropy": [4.8191468715667725], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Khost Airfield", "score_of_biggest_cluster": 0.008058370091021061, "score_of_first_answer": 0.008058370091021061, "max_score_of_first_answer": 0.03259456902742386, "min_score_of_first_answer": 0.0019922740757465363, "avg_score": 0.008058370091021061, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.015301147475838661, "std_of_first_answer": 0.015301147475838661, "avg_entropy_of_first_answer": 4.8191468715667725, "lex_sim": 1.0}
{"question": "Which British political party leader is MP for Westmorland and Lonsdale?", "answer": ["Nick Clegg"], "scores": [[0.022669486701488495, 0.5371637344360352, 0.9544957876205444, 0.9539908766746521]], "normalized_score": [0.3245014548301697], "tokens": [["\u2581Nick", "\u2581C", "leg", "g"]], "entropy": [[4.274546146392822, 3.666060447692871, 0.4196781814098358, 0.3379476070404053]], "avg_entropy": [2.1745580956339836], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tim Farron", "score_of_biggest_cluster": 0.3245014548301697, "score_of_first_answer": 0.3245014548301697, "max_score_of_first_answer": 0.9544957876205444, "min_score_of_first_answer": 0.022669486701488495, "avg_score": 0.3245014548301697, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3831020818198335, "std_of_first_answer": 0.3831020818198335, "avg_entropy_of_first_answer": 2.1745580956339836, "lex_sim": 1.0}
{"question": "What system of psychology, popularised by F J Gall and J K Spurzheim in the 19th century, claims that mental faculties and dispositions can be determined by the shape of the skull?", "answer": ["Phrenology"], "scores": [[0.4177476167678833, 0.9941306710243225, 0.995642900466919]], "normalized_score": [0.7449955344200134], "tokens": [["\u2581P", "hren", "ology"]], "entropy": [[3.5098907947540283, 0.05860331282019615, 0.0438273586332798]], "avg_entropy": [1.2041071554025014], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bump of Locality", "score_of_biggest_cluster": 0.7449955344200134, "score_of_first_answer": 0.7449955344200134, "max_score_of_first_answer": 0.995642900466919, "min_score_of_first_answer": 0.4177476167678833, "avg_score": 0.7449955344200134, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2720667138351379, "std_of_first_answer": 0.2720667138351379, "avg_entropy_of_first_answer": 1.2041071554025014, "lex_sim": 1.0}
{"question": "For which movie did Katharine Hepburn win her second Oscar?", "answer": ["Barbara Streisand"], "scores": [[0.0022524420637637377, 0.07603481411933899, 0.9757928252220154, 0.9886488318443298]], "normalized_score": [0.11337477713823318], "tokens": [["\u2581Barbara", "\u2581Stre", "is", "and"]], "entropy": [[3.9664173126220703, 4.804215431213379, 0.21646279096603394, 0.09119860827922821]], "avg_entropy": [2.269573535770178], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Guess Who's Coming To Dinner", "score_of_biggest_cluster": 0.11337477713823318, "score_of_first_answer": 0.11337477713823318, "max_score_of_first_answer": 0.9886488318443298, "min_score_of_first_answer": 0.0022524420637637377, "avg_score": 0.11337477713823318, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4722814742447072, "std_of_first_answer": 0.4722814742447072, "avg_entropy_of_first_answer": 2.269573535770178, "lex_sim": 1.0}
{"question": "High Willhays is the highest point of what National Park?", "answer": ["Dartmoor"], "scores": [[0.3491496443748474, 0.9867534041404724, 0.9345996975898743, 0.9964871406555176]], "normalized_score": [0.7526262998580933], "tokens": [["\u2581D", "art", "mo", "or"]], "entropy": [[4.341616630554199, 0.1451088786125183, 0.5335770845413208, 0.03389070928096771]], "avg_entropy": [1.2635483257472515], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Dartmoor National Park", "score_of_biggest_cluster": 0.7526262998580933, "score_of_first_answer": 0.7526262998580933, "max_score_of_first_answer": 0.9964871406555176, "min_score_of_first_answer": 0.3491496443748474, "avg_score": 0.7526262998580933, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2709913471531862, "std_of_first_answer": 0.2709913471531862, "avg_entropy_of_first_answer": 1.2635483257472515, "lex_sim": 1.0}
{"question": "Who was the 16th president of the U.S.A.,assasinated in 1865", "answer": ["Abraham Lincoln"], "scores": [[0.23880703747272491, 0.8803828358650208]], "normalized_score": [0.45852112770080566], "tokens": [["\u2581Abraham", "\u2581Lincoln"]], "entropy": [[4.807159423828125, 0.8673206567764282]], "avg_entropy": [2.8372400403022766], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Abaraham lincoln", "score_of_biggest_cluster": 0.45852112770080566, "score_of_first_answer": 0.45852112770080566, "max_score_of_first_answer": 0.8803828358650208, "min_score_of_first_answer": 0.23880703747272491, "avg_score": 0.45852112770080566, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3207878991961479, "std_of_first_answer": 0.3207878991961479, "avg_entropy_of_first_answer": 2.8372400403022766, "lex_sim": 1.0}
{"question": "Which number Beethoven symphony is known as \u2018The Pastoral\u2019?", "answer": ["Symphony No 6"], "scores": [[0.12346521019935608, 0.9394813179969788, 0.6503803730010986, 0.05276501923799515, 0.9627758264541626]], "normalized_score": [0.32862019538879395], "tokens": [["\u2581Sym", "phony", "\u2581No", "\u2581", "6"]], "entropy": [[4.265588760375977, 0.29246950149536133, 1.5190143585205078, 0.25217923521995544, 0.24976390600204468]], "avg_entropy": [1.3158031523227691], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "The Sixth", "score_of_biggest_cluster": 0.32862019538879395, "score_of_first_answer": 0.32862019538879395, "max_score_of_first_answer": 0.9627758264541626, "min_score_of_first_answer": 0.05276501923799515, "avg_score": 0.32862019538879395, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3901899634374997, "std_of_first_answer": 0.3901899634374997, "avg_entropy_of_first_answer": 1.3158031523227691, "lex_sim": 1.0}
{"question": "In the 2014 zombie comedy Life After Beth, a character takes offense when asked about zombies just because she is from what country?", "answer": ["Palestine"], "scores": [[0.01000288501381874, 0.8480545878410339]], "normalized_score": [0.09210316836833954], "tokens": [["\u2581Palest", "ine"]], "entropy": [[6.003048896789551, 0.6163852214813232]], "avg_entropy": [3.309717059135437], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Haitian Ancestry", "score_of_biggest_cluster": 0.09210316836833954, "score_of_first_answer": 0.09210316836833954, "max_score_of_first_answer": 0.8480545878410339, "min_score_of_first_answer": 0.01000288501381874, "avg_score": 0.09210316836833954, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4190258514136076, "std_of_first_answer": 0.4190258514136076, "avg_entropy_of_first_answer": 3.309717059135437, "lex_sim": 1.0}
{"question": "\"What is the real name of \"\"Baby Spice\"\" of the Spice Girls, whose 2007 single \"\"All I Need to Know\"\" was the lowest-charting single of her solo career?\"", "answer": ["Singer Emma Bunton"], "scores": [[0.004496391396969557, 0.08076594024896622, 0.172224760055542, 0.8201488852500916, 0.9967843294143677]], "normalized_score": [0.13859140872955322], "tokens": [["\u2581S", "inger", "\u2581Emma", "\u2581Bun", "ton"]], "entropy": [[4.272259712219238, 5.491527080535889, 3.225038528442383, 1.0402811765670776, 0.031239869073033333]], "avg_entropy": [2.8120692733675243], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Emma Bunton", "score_of_biggest_cluster": 0.13859140872955322, "score_of_first_answer": 0.13859140872955322, "max_score_of_first_answer": 0.9967843294143677, "min_score_of_first_answer": 0.004496391396969557, "avg_score": 0.13859140872955322, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.41031304986959216, "std_of_first_answer": 0.41031304986959216, "avg_entropy_of_first_answer": 2.8120692733675243, "lex_sim": 1.0}
{"question": "Which game that contained many D&D; features and references, including a computer controlled dungeon master, created in 1975 by Will Crowther on a DEC PDP-10 computer, the first widely used adventure game, was significantly expanded in 1976 by Don Woods, that inspired a group of students at MIT to write a game called Zork in the summer of 1977 for the PDP-10 minicomputer which became quite popular on the ARPANET?", "answer": ["ADVENT (aka Adventure game)"], "scores": [[0.07430744171142578, 0.8534611463546753, 0.9854592084884644, 0.0874413251876831, 0.04524373263120651, 0.13428297638893127, 0.9987164735794067, 0.01007893681526184, 0.785152792930603]], "normalized_score": [0.18568359315395355], "tokens": [["\u2581AD", "V", "ENT", "\u2581(", "aka", "\u2581Advent", "ure", "\u2581game", ")"]], "entropy": [[3.6873512268066406, 0.705926239490509, 0.1358790397644043, 1.7912137508392334, 3.553407669067383, 1.0676703453063965, 0.01348804123699665, 1.4455156326293945, 1.1231365203857422]], "avg_entropy": [1.5026209406140778], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Adventurers", "score_of_biggest_cluster": 0.18568359315395355, "score_of_first_answer": 0.18568359315395355, "max_score_of_first_answer": 0.9987164735794067, "min_score_of_first_answer": 0.01007893681526184, "avg_score": 0.18568359315395355, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42056897754948547, "std_of_first_answer": 0.42056897754948547, "avg_entropy_of_first_answer": 1.5026209406140778, "lex_sim": 1.0}
{"question": "\"In the James Bond film \"\"The World Is Not Enough\"\" who played Bond girl Elektra King?\"", "answer": ["Sophie Marceau"], "scores": [[0.21077702939510345, 0.9706425070762634, 0.8675907850265503, 0.9867637753486633, 0.9743884801864624]], "normalized_score": [0.7021478414535522], "tokens": [["\u2581Soph", "ie", "\u2581Mar", "ce", "au"]], "entropy": [[4.505169868469238, 0.16985264420509338, 0.99775630235672, 0.1279047131538391, 0.18913081288337708]], "avg_entropy": [1.1979628682136536], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Sophie Marceau", "score_of_biggest_cluster": 0.7021478414535522, "score_of_first_answer": 0.7021478414535522, "max_score_of_first_answer": 0.9867637753486633, "min_score_of_first_answer": 0.21077702939510345, "avg_score": 0.7021478414535522, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.29871139340175035, "std_of_first_answer": 0.29871139340175035, "avg_entropy_of_first_answer": 1.1979628682136536, "lex_sim": 1.0}
{"question": "Annually in July or August, the Obon (or Bon) Buddhist festival to honour the dead is held in which country?", "answer": ["Japan"], "scores": [[0.3765051066875458]], "normalized_score": [0.3765051066875458], "tokens": [["\u2581Japan"]], "entropy": [[4.246677875518799]], "avg_entropy": [4.246677875518799], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "\u65e5\u672c\u570b", "score_of_biggest_cluster": 0.3765051066875458, "score_of_first_answer": 0.3765051066875458, "max_score_of_first_answer": 0.3765051066875458, "min_score_of_first_answer": 0.3765051066875458, "avg_score": 0.3765051066875458, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.246677875518799, "lex_sim": 1.0}
{"question": "Sharing his surname with a biblical character, which footballer scored the deciding goal in the 1967 FA Cup Final?", "answer": ["David Fairclough"], "scores": [[0.06348992884159088, 0.009956222027540207, 0.8935097455978394, 0.9910171031951904]], "normalized_score": [0.1538136750459671], "tokens": [["\u2581David", "\u2581Fair", "cl", "ough"]], "entropy": [[5.90394926071167, 5.758906364440918, 0.8607592582702637, 0.08081874251365662]], "avg_entropy": [3.151108406484127], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Frank Saul (disambiguation)", "score_of_biggest_cluster": 0.1538136750459671, "score_of_first_answer": 0.1538136750459671, "max_score_of_first_answer": 0.9910171031951904, "min_score_of_first_answer": 0.009956222027540207, "avg_score": 0.1538136750459671, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45447499790240503, "std_of_first_answer": 0.45447499790240503, "avg_entropy_of_first_answer": 3.151108406484127, "lex_sim": 1.0}
{"question": "The dance fitness programme, the Zumba, originated in which country?", "answer": ["RIO"], "scores": [[0.0033039883710443974, 0.03034871444106102]], "normalized_score": [0.01001357939094305], "tokens": [["\u2581R", "IO"]], "entropy": [[4.632936954498291, 6.156355857849121]], "avg_entropy": [5.394646406173706], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rep\u00fablica de Colombia", "score_of_biggest_cluster": 0.01001357939094305, "score_of_first_answer": 0.01001357939094305, "max_score_of_first_answer": 0.03034871444106102, "min_score_of_first_answer": 0.0033039883710443974, "avg_score": 0.01001357939094305, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.013522363035008311, "std_of_first_answer": 0.013522363035008311, "avg_entropy_of_first_answer": 5.394646406173706, "lex_sim": 1.0}
{"question": "Peter Simple, Jack Horner and Ben Nevis have all won which famous sporting event?", "answer": ["Great South West Walk"], "scores": [[0.004401884973049164, 0.00305933877825737, 0.28000307083129883, 0.4482468068599701]], "normalized_score": [0.036056749522686005], "tokens": [["\u2581Great", "\u2581South", "\u2581West", "\u2581Walk"]], "entropy": [[3.8081583976745605, 4.536408424377441, 2.3027265071868896, 3.3902971744537354]], "avg_entropy": [3.5093976259231567], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Grand National", "score_of_biggest_cluster": 0.036056749522686005, "score_of_first_answer": 0.036056749522686005, "max_score_of_first_answer": 0.4482468068599701, "min_score_of_first_answer": 0.00305933877825737, "avg_score": 0.036056749522686005, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18976165938074194, "std_of_first_answer": 0.18976165938074194, "avg_entropy_of_first_answer": 3.5093976259231567, "lex_sim": 1.0}
{"question": "What are Merapi in Indonesia, Rabaul in Papua New Guinea and Ruapehu in New Zealand?", "answer": ["Ranges"], "scores": [[0.006706412881612778, 0.006593829486519098]], "normalized_score": [0.006649883463978767], "tokens": [["\u2581R", "anges"]], "entropy": [[3.5750374794006348, 5.311838150024414]], "avg_entropy": [4.443437814712524], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Volcanic vents", "score_of_biggest_cluster": 0.006649883463978767, "score_of_first_answer": 0.006649883463978767, "max_score_of_first_answer": 0.006706412881612778, "min_score_of_first_answer": 0.006593829486519098, "avg_score": 0.006649883463978767, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 5.6291697546839714e-05, "std_of_first_answer": 5.6291697546839714e-05, "avg_entropy_of_first_answer": 4.443437814712524, "lex_sim": 1.0}
{"question": "\"Which make of car is advertised under the slogan \"\"Vorsprung Durch Technik\"\"?\"", "answer": ["Audi Car Company"], "scores": [[0.42593303322792053, 0.8688075542449951, 0.010420672595500946, 0.3659372627735138]], "normalized_score": [0.19381697475910187], "tokens": [["\u2581A", "udi", "\u2581Car", "\u2581Company"]], "entropy": [[4.178553104400635, 1.0238103866577148, 1.8809452056884766, 2.360438823699951]], "avg_entropy": [2.3609368801116943], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Audi Automobilwerke GmbH", "score_of_biggest_cluster": 0.19381697475910187, "score_of_first_answer": 0.19381697475910187, "max_score_of_first_answer": 0.8688075542449951, "min_score_of_first_answer": 0.010420672595500946, "avg_score": 0.19381697475910187, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.30500885952308865, "std_of_first_answer": 0.30500885952308865, "avg_entropy_of_first_answer": 2.3609368801116943, "lex_sim": 1.0}
{"question": "Giallo is Italian for which colour?", "answer": ["1425"], "scores": [[0.047717370092868805, 0.04080351069569588, 0.04858808219432831, 0.042479731142520905, 0.048021234571933746]], "normalized_score": [0.04540500044822693], "tokens": [["\u2581", "1", "4", "2", "5"]], "entropy": [[4.455349922180176, 5.170720100402832, 3.155827522277832, 3.723050117492676, 3.118455648422241]], "avg_entropy": [3.9246806621551515], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yellowest", "score_of_biggest_cluster": 0.04540500044822693, "score_of_first_answer": 0.04540500044822693, "max_score_of_first_answer": 0.04858808219432831, "min_score_of_first_answer": 0.04080351069569588, "avg_score": 0.04540500044822693, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0032244759086583817, "std_of_first_answer": 0.0032244759086583817, "avg_entropy_of_first_answer": 3.9246806621551515, "lex_sim": 1.0}
{"question": "In the nursery rhyme, who put in his thumb and pulled out a plum?", "answer": ["Winken, blinken and nod"], "scores": [[0.005427517928183079, 0.04817315936088562, 0.11026781052350998, 0.48621150851249695, 0.073039710521698, 0.9978501796722412, 0.9871368408203125, 0.49216338992118835, 0.8350553512573242]], "normalized_score": [0.19535931944847107], "tokens": [["\u2581W", "ink", "en", ",", "\u2581bl", "ink", "en", "\u2581and", "\u2581nod"]], "entropy": [[5.396075248718262, 3.9541962146759033, 2.7434866428375244, 1.999513030052185, 0.6847813725471497, 0.02331576496362686, 0.08804161101579666, 1.0775279998779297, 0.7119125127792358]], "avg_entropy": [1.8532055997186236], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jack (fish)", "score_of_biggest_cluster": 0.19535931944847107, "score_of_first_answer": 0.19535931944847107, "max_score_of_first_answer": 0.9978501796722412, "min_score_of_first_answer": 0.005427517928183079, "avg_score": 0.19535931944847107, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3882460629652684, "std_of_first_answer": 0.3882460629652684, "avg_entropy_of_first_answer": 1.8532055997186236, "lex_sim": 1.0}
{"question": "\"Whose autobiography \"\"Going Rogue\"\" was published in 2009?\"", "answer": ["Sarah Louise Palin (Governor)"], "scores": [[0.374073326587677, 0.08199641108512878, 0.4544960558414459, 0.9974356293678284, 0.062460023909807205, 0.007254572119563818, 0.8357332944869995, 0.9901300072669983, 0.3372824788093567]], "normalized_score": [0.22938787937164307], "tokens": [["\u2581Sarah", "\u2581Louise", "\u2581Pal", "in", "\u2581(", "G", "overn", "or", ")"]], "entropy": [[4.17400598526001, 1.2553811073303223, 1.568556547164917, 0.027069374918937683, 0.9224531650543213, 4.3613481521606445, 0.8667005300521851, 0.06833283603191376, 1.2286850214004517]], "avg_entropy": [1.6080591910415225], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Sarrah Palin", "score_of_biggest_cluster": 0.22938787937164307, "score_of_first_answer": 0.22938787937164307, "max_score_of_first_answer": 0.9974356293678284, "min_score_of_first_answer": 0.007254572119563818, "avg_score": 0.22938787937164307, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.37109848217960456, "std_of_first_answer": 0.37109848217960456, "avg_entropy_of_first_answer": 1.6080591910415225, "lex_sim": 1.0}
{"question": "What was Eddie Murphy's first movie?", "answer": ["48hours"], "scores": [[0.24236170947551727, 0.6844132542610168, 0.8412415385246277, 0.005268782377243042, 0.6894946098327637]], "normalized_score": [0.2192748636007309], "tokens": [["\u2581", "4", "8", "h", "ours"]], "entropy": [[5.242398262023926, 1.713273525238037, 0.8824058175086975, 1.144328236579895, 2.2133030891418457]], "avg_entropy": [2.23914178609848], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "48 time", "score_of_biggest_cluster": 0.2192748636007309, "score_of_first_answer": 0.2192748636007309, "max_score_of_first_answer": 0.8412415385246277, "min_score_of_first_answer": 0.005268782377243042, "avg_score": 0.2192748636007309, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31534794824169154, "std_of_first_answer": 0.31534794824169154, "avg_entropy_of_first_answer": 2.23914178609848, "lex_sim": 1.0}
{"question": "Which Rugby Union Premiership team play home fixtures at Kingston Park?", "answer": ["Kingston Clinic"], "scores": [[0.059773799031972885, 0.90789395570755, 0.0011643480975180864, 0.010383103042840958, 0.7604202032089233]], "normalized_score": [0.05490375682711601], "tokens": [["\u2581King", "ston", "\u2581C", "lin", "ic"]], "entropy": [[3.372926712036133, 0.7339882850646973, 3.0833382606506348, 4.626644611358643, 1.1606593132019043]], "avg_entropy": [2.5955114364624023], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Newcastle Falcons", "score_of_biggest_cluster": 0.05490375682711601, "score_of_first_answer": 0.05490375682711601, "max_score_of_first_answer": 0.90789395570755, "min_score_of_first_answer": 0.0011643480975180864, "avg_score": 0.05490375682711601, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40023146316437774, "std_of_first_answer": 0.40023146316437774, "avg_entropy_of_first_answer": 2.5955114364624023, "lex_sim": 1.0}
{"question": "What are the membranes enveloping the brain and spinal cord called ?", "answer": ["Meninges"], "scores": [[0.3895590007305145, 0.8595202565193176, 0.9617173075675964]], "normalized_score": [0.6854233741760254], "tokens": [["\u2581Men", "ing", "es"]], "entropy": [[3.8800835609436035, 1.0297832489013672, 0.3297518491744995]], "avg_entropy": [1.74653955300649], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cerebral meninges", "score_of_biggest_cluster": 0.6854233741760254, "score_of_first_answer": 0.6854233741760254, "max_score_of_first_answer": 0.9617173075675964, "min_score_of_first_answer": 0.3895590007305145, "avg_score": 0.6854233741760254, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24914809234407592, "std_of_first_answer": 0.24914809234407592, "avg_entropy_of_first_answer": 1.74653955300649, "lex_sim": 1.0}
{"question": "What did Ukrainian prime minister Yulia Timoshenko change about her appearance which shocked commentators in 2009?", "answer": ["Changed her hair colour"], "scores": [[0.005730767734348774, 0.45042669773101807, 0.46370580792427063, 0.5644290447235107, 0.3302566409111023]], "normalized_score": [0.1860835999250412], "tokens": [["\u2581Ch", "anged", "\u2581her", "\u2581hair", "\u2581colour"]], "entropy": [[4.657930850982666, 2.5456111431121826, 2.479909896850586, 2.313943862915039, 2.2155232429504395]], "avg_entropy": [2.8425837993621825], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hairdo", "score_of_biggest_cluster": 0.1860835999250412, "score_of_first_answer": 0.1860835999250412, "max_score_of_first_answer": 0.5644290447235107, "min_score_of_first_answer": 0.005730767734348774, "avg_score": 0.1860835999250412, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19342809438371508, "std_of_first_answer": 0.19342809438371508, "avg_entropy_of_first_answer": 2.8425837993621825, "lex_sim": 1.0}
{"question": "According to a traditional English proverb, what is better than none?", "answer": ["None"], "scores": [[0.043364692479372025]], "normalized_score": [0.043364692479372025], "tokens": [["\u2581None"]], "entropy": [[4.245267391204834]], "avg_entropy": [4.245267391204834], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "HALF A LOAF", "score_of_biggest_cluster": 0.043364692479372025, "score_of_first_answer": 0.043364692479372025, "max_score_of_first_answer": 0.043364692479372025, "min_score_of_first_answer": 0.043364692479372025, "avg_score": 0.043364692479372025, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.245267391204834, "lex_sim": 1.0}
{"question": "Which American naval destroyer was attacked in Aden in October 2000?", "answer": ["Cole"], "scores": [[0.06262480467557907]], "normalized_score": [0.06262480467557907], "tokens": [["\u2581Cole"]], "entropy": [[4.02719259262085]], "avg_entropy": [4.02719259262085], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "U.S.S. Cole", "score_of_biggest_cluster": 0.06262480467557907, "score_of_first_answer": 0.06262480467557907, "max_score_of_first_answer": 0.06262480467557907, "min_score_of_first_answer": 0.06262480467557907, "avg_score": 0.06262480467557907, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.02719259262085, "lex_sim": 1.0}
{"question": "What was the previous occupation of comedian Jim Bowen?", "answer": ["Teacher"], "scores": [[0.07689893990755081, 0.7916273474693298]], "normalized_score": [0.24672919511795044], "tokens": [["\u2581Te", "acher"]], "entropy": [[5.858644485473633, 0.9180060625076294]], "avg_entropy": [3.388325273990631], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Teachings", "score_of_biggest_cluster": 0.24672919511795044, "score_of_first_answer": 0.24672919511795044, "max_score_of_first_answer": 0.7916273474693298, "min_score_of_first_answer": 0.07689893990755081, "avg_score": 0.24672919511795044, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3573642037808895, "std_of_first_answer": 0.3573642037808895, "avg_entropy_of_first_answer": 3.388325273990631, "lex_sim": 1.0}
{"question": "What name is given to the elements with atomic numbers from 89 to 103 in the periodic table?", "answer": ["The Periodic Table Group Names"], "scores": [[0.03628925606608391, 0.010990000329911709, 0.9282320141792297, 0.8033518195152283, 0.006922347471117973, 0.005753499921411276]], "normalized_score": [0.04774419590830803], "tokens": [["\u2581The", "\u2581Period", "ic", "\u2581Table", "\u2581Group", "\u2581Names"]], "entropy": [[4.369878768920898, 5.075010299682617, 0.4885060489177704, 1.0375149250030518, 2.2518906593322754, 2.419760227203369]], "avg_entropy": [2.607093488176664], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Actinide Series", "score_of_biggest_cluster": 0.04774419590830803, "score_of_first_answer": 0.04774419590830803, "max_score_of_first_answer": 0.9282320141792297, "min_score_of_first_answer": 0.005753499921411276, "avg_score": 0.04774419590830803, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40281762536916543, "std_of_first_answer": 0.40281762536916543, "avg_entropy_of_first_answer": 2.607093488176664, "lex_sim": 1.0}
{"question": "Charles Lindbergh died in 1974. Where was he buried?", "answer": ["Atlantic Ocean"], "scores": [[0.006963345222175121, 0.5334029197692871]], "normalized_score": [0.060944803059101105], "tokens": [["\u2581Atlantic", "\u2581Ocean"]], "entropy": [[5.69789981842041, 2.173264980316162]], "avg_entropy": [3.935582399368286], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Haiwii", "score_of_biggest_cluster": 0.060944803059101105, "score_of_first_answer": 0.060944803059101105, "max_score_of_first_answer": 0.5334029197692871, "min_score_of_first_answer": 0.006963345222175121, "avg_score": 0.060944803059101105, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.263219787273556, "std_of_first_answer": 0.263219787273556, "avg_entropy_of_first_answer": 3.935582399368286, "lex_sim": 1.0}
{"question": "What was the code name for the British retreat from Dunkirk in World War 2?", "answer": ["Operation Dynamo"], "scores": [[0.5963902473449707, 0.8061520457267761, 0.9852231740951538]], "normalized_score": [0.7795202136039734], "tokens": [["\u2581Operation", "\u2581Dynam", "o"]], "entropy": [[3.2115421295166016, 1.601883888244629, 0.10447047650814056]], "avg_entropy": [1.6392988314231236], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Dynamo", "score_of_biggest_cluster": 0.7795202136039734, "score_of_first_answer": 0.7795202136039734, "max_score_of_first_answer": 0.9852231740951538, "min_score_of_first_answer": 0.5963902473449707, "avg_score": 0.7795202136039734, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1589051170424968, "std_of_first_answer": 0.1589051170424968, "avg_entropy_of_first_answer": 1.6392988314231236, "lex_sim": 1.0}
{"question": "Which fashionable London thoroughfare, about three quarters of a mile (1.2 km) long, runs from Hyde Park Corner to Marble Arch, along the length of the eastern side of Hyde Park?", "answer": ["Park lane"], "scores": [[0.07614081352949142, 0.05847886577248573, 0.9905923008918762]], "normalized_score": [0.16399745643138885], "tokens": [["\u2581Park", "\u2581l", "ane"]], "entropy": [[5.883510112762451, 2.1256797313690186, 0.09984956681728363]], "avg_entropy": [2.703013136982918], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "A4202 road", "score_of_biggest_cluster": 0.16399745643138885, "score_of_first_answer": 0.16399745643138885, "max_score_of_first_answer": 0.9905923008918762, "min_score_of_first_answer": 0.05847886577248573, "avg_score": 0.16399745643138885, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4352992486743291, "std_of_first_answer": 0.4352992486743291, "avg_entropy_of_first_answer": 2.703013136982918, "lex_sim": 1.0}
{"question": "Who directed the 2011 Palme d'Or winning film 'The Tree Of Life'?", "answer": ["Terrence Malick"], "scores": [[0.24167689681053162, 0.9537496566772461, 0.7648077607154846, 0.8329238891601562]], "normalized_score": [0.6190229654312134], "tokens": [["\u2581Terr", "ence", "\u2581Mal", "ick"]], "entropy": [[5.048821449279785, 0.3292272090911865, 1.7299028635025024, 0.7234700918197632]], "avg_entropy": [1.9578554034233093], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Voyage of Time (film)", "score_of_biggest_cluster": 0.6190229654312134, "score_of_first_answer": 0.6190229654312134, "max_score_of_first_answer": 0.9537496566772461, "min_score_of_first_answer": 0.24167689681053162, "avg_score": 0.6190229654312134, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27217001065242163, "std_of_first_answer": 0.27217001065242163, "avg_entropy_of_first_answer": 1.9578554034233093, "lex_sim": 1.0}
{"question": "What sign of the zodiac is represented by a lion?", "answer": ["Leo"], "scores": [[0.4894880950450897]], "normalized_score": [0.4894880950450897], "tokens": [["\u2581Leo"]], "entropy": [[3.2691848278045654]], "avg_entropy": [3.2691848278045654], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Leo", "score_of_biggest_cluster": 0.4894880950450897, "score_of_first_answer": 0.4894880950450897, "max_score_of_first_answer": 0.4894880950450897, "min_score_of_first_answer": 0.4894880950450897, "avg_score": 0.4894880950450897, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.2691848278045654, "lex_sim": 1.0}
{"question": "What stone was used to decipher Ancient Egyptian hieroglyphics?", "answer": ["Rosetta Stone"], "scores": [[0.6193293333053589, 0.89824378490448, 0.5951126217842102]], "normalized_score": [0.6917858719825745], "tokens": [["\u2581Ros", "etta", "\u2581Stone"]], "entropy": [[2.881627321243286, 0.6649050712585449, 1.4383702278137207]], "avg_entropy": [1.6616342067718506], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Stone of Rosetta", "score_of_biggest_cluster": 0.6917858719825745, "score_of_first_answer": 0.6917858719825745, "max_score_of_first_answer": 0.89824378490448, "min_score_of_first_answer": 0.5951126217842102, "avg_score": 0.6917858719825745, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.13754523396661777, "std_of_first_answer": 0.13754523396661777, "avg_entropy_of_first_answer": 1.6616342067718506, "lex_sim": 1.0}
{"question": "\"The novel, \"\"Birdsong\"\", by Sebastian Faulks, is mainly set during which war?\"", "answer": ["WWI"], "scores": [[0.11228671669960022, 0.4922225773334503]], "normalized_score": [0.23509584367275238], "tokens": [["\u2581W", "WI"]], "entropy": [[4.33273983001709, 1.3450827598571777]], "avg_entropy": [2.838911294937134], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "WORLD WAR I", "score_of_biggest_cluster": 0.23509584367275238, "score_of_first_answer": 0.23509584367275238, "max_score_of_first_answer": 0.4922225773334503, "min_score_of_first_answer": 0.11228671669960022, "avg_score": 0.23509584367275238, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.18996793031692505, "std_of_first_answer": 0.18996793031692505, "avg_entropy_of_first_answer": 2.838911294937134, "lex_sim": 1.0}
{"question": "In the TV series 'Friends', what is 'rachel's' surname?", "answer": ["Rachel J."], "scores": [[0.051884304732084274, 0.9259951710700989, 0.0019734357483685017, 0.14488913118839264, 0.5061287879943848]], "normalized_score": [0.09298928081989288], "tokens": [["\u2581Rach", "el", "\u2581J", ".", "\u2581Green"]], "entropy": [[5.362992286682129, 0.5644588470458984, 4.635929107666016, 4.991916179656982, 2.578904628753662]], "avg_entropy": [3.6268402099609376], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Green (crater)", "score_of_biggest_cluster": 0.09298928081989288, "score_of_first_answer": 0.09298928081989288, "max_score_of_first_answer": 0.9259951710700989, "min_score_of_first_answer": 0.0019734357483685017, "avg_score": 0.09298928081989288, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3479581036381118, "std_of_first_answer": 0.3479581036381118, "avg_entropy_of_first_answer": 3.6268402099609376, "lex_sim": 1.0}
{"question": "Which US State is nicknamed The Beaver State?", "answer": ["Oregon"], "scores": [[0.28541073203086853]], "normalized_score": [0.28541070222854614], "tokens": [["\u2581Oregon"]], "entropy": [[4.572112083435059]], "avg_entropy": [4.572112083435059], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Ore.", "score_of_biggest_cluster": 0.28541070222854614, "score_of_first_answer": 0.28541070222854614, "max_score_of_first_answer": 0.28541073203086853, "min_score_of_first_answer": 0.28541073203086853, "avg_score": 0.28541070222854614, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.572112083435059, "lex_sim": 1.0}
{"question": "Which artery is the main source of blood for the leg?", "answer": ["Left pulm-pulm-pulm-pump"], "scores": [[0.012481103651225567, 0.004522469360381365, 0.009635752998292446, 0.0023508628364652395, 0.009764266200363636, 0.31593629717826843, 0.5274770855903625, 0.37057724595069885, 0.5278319716453552, 0.7304369807243347, 0.6761505603790283, 0.6773656010627747, 0.7136988043785095, 0.006061979103833437]], "normalized_score": [0.08168694376945496], "tokens": [["\u2581Left", "\u2581pul", "m", "-", "p", "ul", "m", "-", "p", "ul", "m", "-", "p", "ump"]], "entropy": [[4.741293907165527, 4.904973983764648, 0.19405865669250488, 3.6441495418548584, 5.874253273010254, 3.9648704528808594, 1.961125373840332, 2.493527889251709, 3.61167049407959, 1.6732014417648315, 1.4103612899780273, 1.553794264793396, 2.3442771434783936, 0.8007217645645142]], "avg_entropy": [2.798019962651389], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Femoral Arteries", "score_of_biggest_cluster": 0.08168694376945496, "score_of_first_answer": 0.08168694376945496, "max_score_of_first_answer": 0.7304369807243347, "min_score_of_first_answer": 0.0023508628364652395, "avg_score": 0.08168694376945496, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29906680741248987, "std_of_first_answer": 0.29906680741248987, "avg_entropy_of_first_answer": 2.798019962651389, "lex_sim": 1.0}
{"question": "What is the second most common word in written English?", "answer": ["The / and"], "scores": [[0.1690233051776886, 0.0014009801670908928, 0.029185231775045395]], "normalized_score": [0.019047902897000313], "tokens": [["\u2581The", "\u2581/", "\u2581and"]], "entropy": [[5.473283290863037, 0.9053954482078552, 4.911894798278809]], "avg_entropy": [3.7635245124499], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "O F", "score_of_biggest_cluster": 0.019047902897000313, "score_of_first_answer": 0.019047902897000313, "max_score_of_first_answer": 0.1690233051776886, "min_score_of_first_answer": 0.0014009801670908928, "avg_score": 0.019047902897000313, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07335143356346259, "std_of_first_answer": 0.07335143356346259, "avg_entropy_of_first_answer": 3.7635245124499, "lex_sim": 1.0}
{"question": "The opera Gloriana is by which English composer?", "answer": ["Elgar"], "scores": [[0.02578802965581417, 0.6398599743843079]], "normalized_score": [0.12845517694950104], "tokens": [["\u2581El", "gar"]], "entropy": [[4.845423698425293, 2.565593719482422]], "avg_entropy": [3.7055087089538574], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Britten, Benjamin", "score_of_biggest_cluster": 0.12845517694950104, "score_of_first_answer": 0.12845517694950104, "max_score_of_first_answer": 0.6398599743843079, "min_score_of_first_answer": 0.02578802965581417, "avg_score": 0.12845517694950104, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30703597236424685, "std_of_first_answer": 0.30703597236424685, "avg_entropy_of_first_answer": 3.7055087089538574, "lex_sim": 1.0}
{"question": "Known as the 'Phoenix Park Murders', the assassination of Lord Frederick Cavendish and Thomas Henry Burke in 1882 occurred in which city?", "answer": ["Dublin"], "scores": [[0.26812028884887695]], "normalized_score": [0.26812028884887695], "tokens": [["\u2581Dublin"]], "entropy": [[4.811785697937012]], "avg_entropy": [4.811785697937012], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "UN/LOCODE:IEDUB", "score_of_biggest_cluster": 0.26812028884887695, "score_of_first_answer": 0.26812028884887695, "max_score_of_first_answer": 0.26812028884887695, "min_score_of_first_answer": 0.26812028884887695, "avg_score": 0.26812028884887695, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.811785697937012, "lex_sim": 1.0}
{"question": "What is the name for the dot on the lower case letters I and J?", "answer": ["tittle"], "scores": [[0.09800861775875092, 0.8772807717323303]], "normalized_score": [0.2932252883911133], "tokens": [["\u2581t", "ittle"]], "entropy": [[4.410065650939941, 0.8320382237434387]], "avg_entropy": [2.62105193734169], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Tittles", "score_of_biggest_cluster": 0.2932252883911133, "score_of_first_answer": 0.2932252883911133, "max_score_of_first_answer": 0.8772807717323303, "min_score_of_first_answer": 0.09800861775875092, "avg_score": 0.2932252883911133, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3896360769867897, "std_of_first_answer": 0.3896360769867897, "avg_entropy_of_first_answer": 2.62105193734169, "lex_sim": 1.0}
{"question": "\"After he had directed \"\"Australia\"\", it was reported that Baz Luhrmann's next project was a film based on which book by F Scott Fitzgerald?\"", "answer": ["the Great Gatsby"], "scores": [[0.003495116950944066, 0.34078553318977356, 0.9438526034355164, 0.9914816617965698, 0.9971752166748047]], "normalized_score": [0.25655511021614075], "tokens": [["\u2581the", "\u2581Great", "\u2581G", "ats", "by"]], "entropy": [[3.7773756980895996, 3.030458927154541, 0.4536404013633728, 0.07423358410596848, 0.027651319280266762]], "avg_entropy": [1.4726719859987498], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George Wilson (Gatsby)", "score_of_biggest_cluster": 0.25655511021614075, "score_of_first_answer": 0.25655511021614075, "max_score_of_first_answer": 0.9971752166748047, "min_score_of_first_answer": 0.003495116950944066, "avg_score": 0.25655511021614075, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40912793036368367, "std_of_first_answer": 0.40912793036368367, "avg_entropy_of_first_answer": 1.4726719859987498, "lex_sim": 1.0}
{"question": "Which female singer had a top ten hit in 1998 with My Favourite Mistake", "answer": ["Macy Gray"], "scores": [[0.011699303984642029, 0.0317239873111248, 0.5632479190826416]], "normalized_score": [0.059349335730075836], "tokens": [["\u2581M", "acy", "\u2581Gray"]], "entropy": [[5.235467910766602, 3.5402660369873047, 2.9121315479278564]], "avg_entropy": [3.8959551652272544], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sheryl Suzanne Crow", "score_of_biggest_cluster": 0.059349335730075836, "score_of_first_answer": 0.059349335730075836, "max_score_of_first_answer": 0.5632479190826416, "min_score_of_first_answer": 0.011699303984642029, "avg_score": 0.059349335730075836, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25541351065036544, "std_of_first_answer": 0.25541351065036544, "avg_entropy_of_first_answer": 3.8959551652272544, "lex_sim": 1.0}
{"question": "Kingsley Amis, John Gardner and Raymond Benson have written novels about which character, after the death of the original author?", "answer": ["James Bond"], "scores": [[0.4576376974582672, 0.9390425682067871]], "normalized_score": [0.6555465459823608], "tokens": [["\u2581James", "\u2581Bond"]], "entropy": [[3.6804451942443848, 0.5675169229507446]], "avg_entropy": [2.1239810585975647], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "James Bond formula", "score_of_biggest_cluster": 0.6555465459823608, "score_of_first_answer": 0.6555465459823608, "max_score_of_first_answer": 0.9390425682067871, "min_score_of_first_answer": 0.4576376974582672, "avg_score": 0.6555465459823608, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24070243537425995, "std_of_first_answer": 0.24070243537425995, "avg_entropy_of_first_answer": 2.1239810585975647, "lex_sim": 1.0}
{"question": "A schism normally refers to a split within what sort of institution?", "answer": ["Church (institution)"], "scores": [[0.12653400003910065, 0.043955594301223755, 0.11510229110717773, 0.9777036309242249, 0.9156361818313599]], "normalized_score": [0.22472277283668518], "tokens": [["\u2581Church", "\u2581(", "inst", "itution", ")"]], "entropy": [[5.101745128631592, 1.4628758430480957, 4.267090797424316, 0.15851768851280212, 0.481375515460968]], "avg_entropy": [2.2943209946155547], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tserkov", "score_of_biggest_cluster": 0.22472277283668518, "score_of_first_answer": 0.22472277283668518, "max_score_of_first_answer": 0.9777036309242249, "min_score_of_first_answer": 0.043955594301223755, "avg_score": 0.22472277283668518, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41855389692814393, "std_of_first_answer": 0.41855389692814393, "avg_entropy_of_first_answer": 2.2943209946155547, "lex_sim": 1.0}
{"question": "What was the title of the musical based on the TV programme 'X Factor?", "answer": ["I Dream"], "scores": [[0.3428497612476349, 0.36960718035697937]], "normalized_score": [0.3559771776199341], "tokens": [["\u2581I", "\u2581Dream"]], "entropy": [[4.445085525512695, 2.5730535984039307]], "avg_entropy": [3.509069561958313], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The X Factor Musical", "score_of_biggest_cluster": 0.3559771776199341, "score_of_first_answer": 0.3559771776199341, "max_score_of_first_answer": 0.36960718035697937, "min_score_of_first_answer": 0.3428497612476349, "avg_score": 0.3559771776199341, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.013378709554672241, "std_of_first_answer": 0.013378709554672241, "avg_entropy_of_first_answer": 3.509069561958313, "lex_sim": 1.0}
{"question": "Who played the role of painter Vincent Van Gogh in the film Lust For Life", "answer": ["The Young Vic"], "scores": [[0.022840449586510658, 0.005261608399450779, 0.007020967081189156]], "normalized_score": [0.00944945216178894], "tokens": [["\u2581The", "\u2581Young", "\u2581Vic"]], "entropy": [[5.789997577667236, 6.7465105056762695, 3.3990468978881836]], "avg_entropy": [5.3118516604105634], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Douglas (disambiguation)", "score_of_biggest_cluster": 0.00944945216178894, "score_of_first_answer": 0.00944945216178894, "max_score_of_first_answer": 0.022840449586510658, "min_score_of_first_answer": 0.005261608399450779, "avg_score": 0.00944945216178894, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.007904759657510531, "std_of_first_answer": 0.007904759657510531, "avg_entropy_of_first_answer": 5.3118516604105634, "lex_sim": 1.0}
{"question": "Which principality was established in 1278 by the Treaty of Joint Suzerainty?", "answer": ["Andorra"], "scores": [[0.28930747509002686, 0.9652287364006042, 0.9628394842147827]], "normalized_score": [0.6454281806945801], "tokens": [["\u2581And", "or", "ra"]], "entropy": [[4.409146785736084, 0.3061147928237915, 0.27657443284988403]], "avg_entropy": [1.6639453371365864], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Military of Andorra", "score_of_biggest_cluster": 0.6454281806945801, "score_of_first_answer": 0.6454281806945801, "max_score_of_first_answer": 0.9652287364006042, "min_score_of_first_answer": 0.28930747509002686, "avg_score": 0.6454281806945801, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3180706817557869, "std_of_first_answer": 0.3180706817557869, "avg_entropy_of_first_answer": 1.6639453371365864, "lex_sim": 1.0}
{"question": "January 23 saw the anniversary of the introduction of the Apple Macintosh. In what year did this event happen?", "answer": ["\u21e8 \u21bb"], "scores": [[0.4584540128707886, 0.00878982711583376, 0.011542201042175293, 0.04865617677569389, 0.06326238065958023, 0.24545878171920776, 0.05325991287827492, 0.12254074215888977]], "normalized_score": [0.06238260120153427], "tokens": [["\u2581", "<0xE2>", "<0x87>", "<0xA8>", "\u2581", "<0xE2>", "<0x86>", "<0xBB>"]], "entropy": [[3.842532157897949, 1.7853100299835205, 3.5959460735321045, 3.4684627056121826, 2.985258102416992, 2.9189889430999756, 1.6998697519302368, 3.5015647411346436]], "avg_entropy": [2.9747415632009506], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1984", "score_of_biggest_cluster": 0.06238260120153427, "score_of_first_answer": 0.06238260120153427, "max_score_of_first_answer": 0.4584540128707886, "min_score_of_first_answer": 0.00878982711583376, "avg_score": 0.06238260120153427, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14441869771058632, "std_of_first_answer": 0.14441869771058632, "avg_entropy_of_first_answer": 2.9747415632009506, "lex_sim": 0.0}
{"question": "The holiday resort of Paphos is on which Mediterranean island?", "answer": ["Cyprus"], "scores": [[0.46388351917266846, 0.9069131016731262, 0.990829586982727]], "normalized_score": [0.7470067739486694], "tokens": [["\u2581Cy", "pr", "us"]], "entropy": [[3.9611332416534424, 0.573146641254425, 0.08704358339309692]], "avg_entropy": [1.5404411554336548], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Culture of Cyprus", "score_of_biggest_cluster": 0.7470067739486694, "score_of_first_answer": 0.7470067739486694, "max_score_of_first_answer": 0.990829586982727, "min_score_of_first_answer": 0.46388351917266846, "avg_score": 0.7470067739486694, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2311779847884716, "std_of_first_answer": 0.2311779847884716, "avg_entropy_of_first_answer": 1.5404411554336548, "lex_sim": 1.0}
{"question": "The major UK/European mobile network corp EE was originally branded?", "answer": ["Orange France Telecom"], "scores": [[0.17158707976341248, 0.0027240789495408535, 0.0796818807721138, 0.9776580333709717]], "normalized_score": [0.07768063247203827], "tokens": [["\u2581Orange", "\u2581France", "\u2581Tele", "com"]], "entropy": [[3.8542561531066895, 2.583669900894165, 1.4949305057525635, 0.14539042115211487]], "avg_entropy": [2.019561745226383], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Everything Everywhere", "score_of_biggest_cluster": 0.07768063247203827, "score_of_first_answer": 0.07768063247203827, "max_score_of_first_answer": 0.9776580333709717, "min_score_of_first_answer": 0.0027240789495408535, "avg_score": 0.07768063247203827, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39127128937898653, "std_of_first_answer": 0.39127128937898653, "avg_entropy_of_first_answer": 2.019561745226383, "lex_sim": 1.0}
{"question": "What name is given to the nine square mile area of West Yorkshire, between Wakefield, Morley and Rothwell, famous for growing a certain vegetable?", "answer": ["Rhubarb triangle"], "scores": [[0.10153724998235703, 0.9501339197158813, 0.9764926433563232, 0.9678775072097778, 0.15115393698215485]], "normalized_score": [0.4244864284992218], "tokens": [["\u2581Rh", "ub", "ar", "b", "\u2581triangle"]], "entropy": [[5.5945210456848145, 0.4020576775074005, 0.18823513388633728, 0.2627773880958557, 2.1468117237091064]], "avg_entropy": [1.7188805937767029], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yorkshire Forced Rhubarb", "score_of_biggest_cluster": 0.4244864284992218, "score_of_first_answer": 0.4244864284992218, "max_score_of_first_answer": 0.9764926433563232, "min_score_of_first_answer": 0.10153724998235703, "avg_score": 0.4244864284992218, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4111615121186606, "std_of_first_answer": 0.4111615121186606, "avg_entropy_of_first_answer": 1.7188805937767029, "lex_sim": 1.0}
{"question": "In which street did the Great Fire of London originate", "answer": ["The Old Duck"], "scores": [[0.04266216233372688, 0.010235889814794064, 0.002373715164139867, 0.15353326499462128]], "normalized_score": [0.01997329853475094], "tokens": [["\u2581The", "\u2581Old", "\u2581D", "uck"]], "entropy": [[4.8623270988464355, 5.911062240600586, 4.110878944396973, 3.673391103744507]], "avg_entropy": [4.639414846897125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pudding Lane", "score_of_biggest_cluster": 0.01997329853475094, "score_of_first_answer": 0.01997329853475094, "max_score_of_first_answer": 0.15353326499462128, "min_score_of_first_answer": 0.002373715164139867, "avg_score": 0.01997329853475094, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.060421532047084954, "std_of_first_answer": 0.060421532047084954, "avg_entropy_of_first_answer": 4.639414846897125, "lex_sim": 1.0}
{"question": "In computing, what started life at Stanford University as 'backrub'?", "answer": ["Google"], "scores": [[0.5538685917854309]], "normalized_score": [0.5538685917854309], "tokens": [["\u2581Google"]], "entropy": [[3.145413398742676]], "avg_entropy": [3.145413398742676], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Google.co.cr", "score_of_biggest_cluster": 0.5538685917854309, "score_of_first_answer": 0.5538685917854309, "max_score_of_first_answer": 0.5538685917854309, "min_score_of_first_answer": 0.5538685917854309, "avg_score": 0.5538685917854309, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.145413398742676, "lex_sim": 1.0}
{"question": "Which author created the LAPD detective Harry Bosch?", "answer": ["Michael Connelly"], "scores": [[0.3355118930339813, 0.926490306854248, 0.8778650760650635, 0.9924045205116272]], "normalized_score": [0.7213836908340454], "tokens": [["\u2581Michael", "\u2581Con", "nel", "ly"]], "entropy": [[4.427705764770508, 0.7166757583618164, 0.559102475643158, 0.05898056551814079]], "avg_entropy": [1.4406161410734057], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Michael Connelly", "score_of_biggest_cluster": 0.7213836908340454, "score_of_first_answer": 0.7213836908340454, "max_score_of_first_answer": 0.9924045205116272, "min_score_of_first_answer": 0.3355118930339813, "avg_score": 0.7213836908340454, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2615744117055233, "std_of_first_answer": 0.2615744117055233, "avg_entropy_of_first_answer": 1.4406161410734057, "lex_sim": 1.0}
{"question": "Who wrote the 20th century verse play Murder in the Cathedral?", "answer": ["TS Eliot"], "scores": [[0.2956785261631012, 0.2188667356967926, 0.8272499442100525, 0.9855573773384094]], "normalized_score": [0.47926923632621765], "tokens": [["\u2581T", "S", "\u2581Eli", "ot"]], "entropy": [[5.038829326629639, 1.6057485342025757, 1.202294945716858, 0.11062875390052795]], "avg_entropy": [1.9893753901124], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "TS Elliot", "score_of_biggest_cluster": 0.47926923632621765, "score_of_first_answer": 0.47926923632621765, "max_score_of_first_answer": 0.9855573773384094, "min_score_of_first_answer": 0.2188667356967926, "avg_score": 0.47926923632621765, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3304738042477291, "std_of_first_answer": 0.3304738042477291, "avg_entropy_of_first_answer": 1.9893753901124, "lex_sim": 1.0}
{"question": "Which city lies at the confluence of the rivers Rhine and Moselle", "answer": ["Koblenz"], "scores": [[0.1651269793510437, 0.9515230059623718, 0.9784199595451355]], "normalized_score": [0.5356990694999695], "tokens": [["\u2581Kob", "len", "z"]], "entropy": [[5.063173770904541, 0.4020843803882599, 0.1883491426706314]], "avg_entropy": [1.8845357646544774], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bailiwick of Koblenz", "score_of_biggest_cluster": 0.5356990694999695, "score_of_first_answer": 0.5356990694999695, "max_score_of_first_answer": 0.9784199595451355, "min_score_of_first_answer": 0.1651269793510437, "avg_score": 0.5356990694999695, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3772101725409126, "std_of_first_answer": 0.3772101725409126, "avg_entropy_of_first_answer": 1.8845357646544774, "lex_sim": 1.0}
{"question": "Which river is most sacred to the Hindu religion?", "answer": ["Dandhini"], "scores": [[0.0052080596797168255, 0.006371945142745972, 0.00915348157286644, 0.06213231384754181]], "normalized_score": [0.011720957234501839], "tokens": [["\u2581D", "and", "h", "ini"]], "entropy": [[4.776638031005859, 5.591423988342285, 4.137218475341797, 3.968944549560547]], "avg_entropy": [4.618556261062622], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ga\u1e45g\u0101", "score_of_biggest_cluster": 0.011720957234501839, "score_of_first_answer": 0.011720957234501839, "max_score_of_first_answer": 0.06213231384754181, "min_score_of_first_answer": 0.0052080596797168255, "avg_score": 0.011720957234501839, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.023954388987255788, "std_of_first_answer": 0.023954388987255788, "avg_entropy_of_first_answer": 4.618556261062622, "lex_sim": 1.0}
{"question": "What was Kerry Packer's media company called?", "answer": ["\u340c"], "scores": [[0.04416964203119278, 0.13747534155845642, 0.025760827586054802, 0.023351512849330902]], "normalized_score": [0.04371756315231323], "tokens": [["\u2581", "<0xE3>", "<0x90>", "<0x8C>"]], "entropy": [[4.960040092468262, 4.188088417053223, 2.5966567993164062, 3.8856911659240723]], "avg_entropy": [3.9076191186904907], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "CONSOLIDATED PRESS", "score_of_biggest_cluster": 0.04371756315231323, "score_of_first_answer": 0.04371756315231323, "max_score_of_first_answer": 0.13747534155845642, "min_score_of_first_answer": 0.023351512849330902, "avg_score": 0.04371756315231323, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04676297829546931, "std_of_first_answer": 0.04676297829546931, "avg_entropy_of_first_answer": 3.9076191186904907, "lex_sim": 0.0}
{"question": "What was the last name of brothers Chico, Harpo, Groucho, Zeppo, and Gummo?", "answer": ["The 7 dwarves"], "scores": [[0.024486057460308075, 0.009823902510106564, 0.07711335271596909, 0.03770628571510315, 0.9937660694122314, 0.2822851836681366]], "normalized_score": [0.07622891664505005], "tokens": [["\u2581The", "\u2581", "7", "\u2581d", "war", "ves"]], "entropy": [[5.739717483520508, 3.0391428470611572, 2.319171190261841, 4.422929763793945, 0.06633064150810242, 0.6965370178222656]], "avg_entropy": [2.713971490661303], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Karl Marx", "score_of_biggest_cluster": 0.07622891664505005, "score_of_first_answer": 0.07622891664505005, "max_score_of_first_answer": 0.9937660694122314, "min_score_of_first_answer": 0.009823902510106564, "avg_score": 0.07622891664505005, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3504275543172233, "std_of_first_answer": 0.3504275543172233, "avg_entropy_of_first_answer": 2.713971490661303, "lex_sim": 1.0}
{"question": "Which monastic order, founded in 529 AD, was based at a monastery at Monte Cassino, Italy, until it was sacked in 580, when the monks fled to Rome and then spread throughout Europe?", "answer": ["Order of Saint Benedict"], "scores": [[0.05193772539496422, 0.9558088779449463, 0.6158878207206726, 0.9778249859809875, 0.9996159076690674]], "normalized_score": [0.4955528974533081], "tokens": [["\u2581Order", "\u2581of", "\u2581Saint", "\u2581Bened", "ict"]], "entropy": [[4.737950325012207, 0.3779047131538391, 0.9942136406898499, 0.20583480596542358, 0.004697086289525032]], "avg_entropy": [1.264120114222169], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "St. Benedict's monks", "score_of_biggest_cluster": 0.4955528974533081, "score_of_first_answer": 0.4955528974533081, "max_score_of_first_answer": 0.9996159076690674, "min_score_of_first_answer": 0.05193772539496422, "avg_score": 0.4955528974533081, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3626047223902239, "std_of_first_answer": 0.3626047223902239, "avg_entropy_of_first_answer": 1.264120114222169, "lex_sim": 1.0}
{"question": "Technically lugumes/beans, cereals and nuts are defined as what category of foodstuff?", "answer": ["Legumes"], "scores": [[0.14081676304340363, 0.7558168172836304]], "normalized_score": [0.3262386918067932], "tokens": [["\u2581Leg", "umes"]], "entropy": [[4.884222030639648, 0.8693505525588989]], "avg_entropy": [2.8767862915992737], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Edible seed", "score_of_biggest_cluster": 0.3262386918067932, "score_of_first_answer": 0.3262386918067932, "max_score_of_first_answer": 0.7558168172836304, "min_score_of_first_answer": 0.14081676304340363, "avg_score": 0.3262386918067932, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3075000271201134, "std_of_first_answer": 0.3075000271201134, "avg_entropy_of_first_answer": 2.8767862915992737, "lex_sim": 1.0}
{"question": "\"Which Italian nobel prize winner (1934) wrote novels such as \"\"Mal Gioconda\"\" and switched to writing plays in 1910?\"", "answer": ["Pirandello"], "scores": [[0.018357589840888977, 0.9912815093994141, 0.9417611956596375]], "normalized_score": [0.257820725440979], "tokens": [["\u2581Pir", "and", "ello"]], "entropy": [[3.1104941368103027, 0.0901370421051979, 0.4313245415687561]], "avg_entropy": [1.2106519068280857], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Pirandello", "score_of_biggest_cluster": 0.257820725440979, "score_of_first_answer": 0.257820725440979, "max_score_of_first_answer": 0.9912815093994141, "min_score_of_first_answer": 0.018357589840888977, "avg_score": 0.257820725440979, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4474256529129325, "std_of_first_answer": 0.4474256529129325, "avg_entropy_of_first_answer": 1.2106519068280857, "lex_sim": 1.0}
{"question": "What is the square root of 169?", "answer": ["13"], "scores": [[0.525204062461853, 0.7475250363349915, 0.9280904531478882]], "normalized_score": [0.714246392250061], "tokens": [["\u2581", "1", "3"]], "entropy": [[3.7624378204345703, 1.34796142578125, 0.54046231508255]], "avg_entropy": [1.88362052043279], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "13", "score_of_biggest_cluster": 0.714246392250061, "score_of_first_answer": 0.714246392250061, "max_score_of_first_answer": 0.9280904531478882, "min_score_of_first_answer": 0.525204062461853, "avg_score": 0.714246392250061, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.16477187225330542, "std_of_first_answer": 0.16477187225330542, "avg_entropy_of_first_answer": 1.88362052043279, "lex_sim": 1.0}
{"question": "Which man, according to the Holy Bible, fathered Ishmael by sleeping with his wife's servant, Hagar?", "answer": ["Abraham"], "scores": [[0.40141916275024414]], "normalized_score": [0.40141916275024414], "tokens": [["\u2581Abraham"]], "entropy": [[3.761435031890869]], "avg_entropy": [3.761435031890869], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Abraham/Proposed Chronology Edits", "score_of_biggest_cluster": 0.40141916275024414, "score_of_first_answer": 0.40141916275024414, "max_score_of_first_answer": 0.40141916275024414, "min_score_of_first_answer": 0.40141916275024414, "avg_score": 0.40141916275024414, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.761435031890869, "lex_sim": 1.0}
{"question": "What does the average person have 206 of?", "answer": ["Bones"], "scores": [[0.6754603981971741, 0.9745098352432251]], "normalized_score": [0.8113216161727905], "tokens": [["\u2581B", "ones"]], "entropy": [[2.5764474868774414, 0.16181589663028717]], "avg_entropy": [1.3691316917538643], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bone lining cell", "score_of_biggest_cluster": 0.8113216161727905, "score_of_first_answer": 0.8113216161727905, "max_score_of_first_answer": 0.9745098352432251, "min_score_of_first_answer": 0.6754603981971741, "avg_score": 0.8113216161727905, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1495247185230255, "std_of_first_answer": 0.1495247185230255, "avg_entropy_of_first_answer": 1.3691316917538643, "lex_sim": 1.0}
{"question": "Which mammal has species called 'leopard', 'Grey' and 'Baikal'?", "answer": ["Sea Slots"], "scores": [[0.0948680117726326, 0.040318023413419724, 0.000860807893332094]], "normalized_score": [0.01487676240503788], "tokens": [["\u2581Sea", "\u2581Sl", "ots"]], "entropy": [[5.026296615600586, 3.10849666595459, 1.1253314018249512]], "avg_entropy": [3.0867082277933755], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Basic Underwater Demolition", "score_of_biggest_cluster": 0.01487676240503788, "score_of_first_answer": 0.01487676240503788, "max_score_of_first_answer": 0.0948680117726326, "min_score_of_first_answer": 0.000860807893332094, "avg_score": 0.01487676240503788, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.03854280083778909, "std_of_first_answer": 0.03854280083778909, "avg_entropy_of_first_answer": 3.0867082277933755, "lex_sim": 1.0}
{"question": "Which root vegetable pie was named after a Minister of Food during World War II?", "answer": ["Duff Coopers"], "scores": [[0.004350949078798294, 0.05944676324725151, 0.040280427783727646, 0.33710983395576477, 0.11403490602970123]], "normalized_score": [0.052544005215168], "tokens": [["\u2581D", "uff", "\u2581Co", "op", "ers"]], "entropy": [[5.7598185539245605, 4.9510178565979, 4.328309059143066, 2.6801886558532715, 2.54318904876709]], "avg_entropy": [4.052504634857177], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "WOOLTON (Pie)", "score_of_biggest_cluster": 0.052544005215168, "score_of_first_answer": 0.052544005215168, "max_score_of_first_answer": 0.33710983395576477, "min_score_of_first_answer": 0.004350949078798294, "avg_score": 0.052544005215168, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11846338835905026, "std_of_first_answer": 0.11846338835905026, "avg_entropy_of_first_answer": 4.052504634857177, "lex_sim": 1.0}
{"question": "Which is the film, about canine reincarnation and the relationships between father and son and master and dog, based on an Alan Sharp adaptation of a short novel by Irish author Lord Dunsany?", "answer": ["Hound Tueday"], "scores": [[0.10612468421459198, 0.467812716960907, 0.029148340225219727, 0.0006773326895199716, 0.9860427975654602]], "normalized_score": [0.06266716122627258], "tokens": [["\u2581H", "ound", "\u2581T", "ued", "ay"]], "entropy": [[4.819559574127197, 2.3811373710632324, 1.9297361373901367, 2.2657413482666016, 0.12047159671783447]], "avg_entropy": [2.3033292055130006], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Dean Spanley", "score_of_biggest_cluster": 0.06266716122627258, "score_of_first_answer": 0.06266716122627258, "max_score_of_first_answer": 0.9860427975654602, "min_score_of_first_answer": 0.0006773326895199716, "avg_score": 0.06266716122627258, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3735626126870787, "std_of_first_answer": 0.3735626126870787, "avg_entropy_of_first_answer": 2.3033292055130006, "lex_sim": 1.0}
{"question": "What corporation pioneered the use of coupons in consumer marketing, beginning in 1887, so that by 1913 over eight million Americans had sampled its product for free?", "answer": ["Aunt Jemima Mills"], "scores": [[0.019750352948904037, 0.36897069215774536, 0.8293860554695129, 0.9872209429740906, 0.9751087427139282, 0.11525017768144608, 0.9916098713874817]], "normalized_score": [0.3516499102115631], "tokens": [["\u2581A", "unt", "\u2581J", "em", "ima", "\u2581M", "ills"]], "entropy": [[4.960063934326172, 3.5504117012023926, 1.2148784399032593, 0.1263740211725235, 0.1554678976535797, 2.226445198059082, 0.060102835297584534]], "avg_entropy": [1.7562491468020849], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Coca-Cola (soft drink)", "score_of_biggest_cluster": 0.3516499102115631, "score_of_first_answer": 0.3516499102115631, "max_score_of_first_answer": 0.9916098713874817, "min_score_of_first_answer": 0.019750352948904037, "avg_score": 0.3516499102115631, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4001036988426751, "std_of_first_answer": 0.4001036988426751, "avg_entropy_of_first_answer": 1.7562491468020849, "lex_sim": 1.0}
{"question": "Which novel features the scholar 'Humbert Humbert'?", "answer": ["Lolita"], "scores": [[0.5819681286811829, 0.9675411581993103, 0.9840995073318481]], "normalized_score": [0.8213644027709961], "tokens": [["\u2581L", "ol", "ita"]], "entropy": [[3.218925952911377, 0.3039582371711731, 0.15901191532611847]], "avg_entropy": [1.2272987018028896], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Hunted Enchanters", "score_of_biggest_cluster": 0.8213644027709961, "score_of_first_answer": 0.8213644027709961, "max_score_of_first_answer": 0.9840995073318481, "min_score_of_first_answer": 0.5819681286811829, "avg_score": 0.8213644027709961, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18578673129073636, "std_of_first_answer": 0.18578673129073636, "avg_entropy_of_first_answer": 1.2272987018028896, "lex_sim": 1.0}
{"question": "For which art form is Henri Cartier-Bresson best known?", "answer": ["Photojournalism"], "scores": [[0.06065139174461365, 0.9360331892967224, 0.7535665035247803, 0.9939927458763123, 0.8783425688743591]], "normalized_score": [0.5181555151939392], "tokens": [["\u2581Ph", "oto", "j", "ournal", "ism"]], "entropy": [[4.516812801361084, 0.49523887038230896, 1.4809774160385132, 0.0499383807182312, 0.4179953336715698]], "avg_entropy": [1.3921925604343415], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pet photography", "score_of_biggest_cluster": 0.5181555151939392, "score_of_first_answer": 0.5181555151939392, "max_score_of_first_answer": 0.9939927458763123, "min_score_of_first_answer": 0.06065139174461365, "avg_score": 0.5181555151939392, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3413444011753117, "std_of_first_answer": 0.3413444011753117, "avg_entropy_of_first_answer": 1.3921925604343415, "lex_sim": 1.0}
{"question": "In 1976 who was the first gymnast to score a perfect ten in Olympic competition?", "answer": ["Nadia Com\u0103neci"], "scores": [[0.35043272376060486, 0.9891895055770874, 0.7023506760597229, 0.13836818933486938, 0.9945101737976074, 0.9662882089614868]], "normalized_score": [0.5645447969436646], "tokens": [["\u2581Nad", "ia", "\u2581Com", "\u0103", "ne", "ci"]], "entropy": [[4.501513957977295, 0.10649896413087845, 2.0033459663391113, 1.2528626918792725, 0.043220244348049164, 0.21990293264389038]], "avg_entropy": [1.3545574595530827], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Nadia Elena Com\u0103neci", "score_of_biggest_cluster": 0.5645447969436646, "score_of_first_answer": 0.5645447969436646, "max_score_of_first_answer": 0.9945101737976074, "min_score_of_first_answer": 0.13836818933486938, "avg_score": 0.5645447969436646, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.33623699306511706, "std_of_first_answer": 0.33623699306511706, "avg_entropy_of_first_answer": 1.3545574595530827, "lex_sim": 1.0}
{"question": "In which year was the 50p coin introduced into circulation in Britain?", "answer": ["1969-2019"], "scores": [[0.5348381996154785, 0.6081266403198242, 0.9357828497886658, 0.8010947108268738, 0.8353303670883179, 0.0369681715965271, 0.056467585265636444, 0.6464587450027466, 0.5488690733909607, 0.22899885475635529]], "normalized_score": [0.3579648435115814], "tokens": [["\u2581", "1", "9", "6", "9", "-", "2", "0", "1", "9"]], "entropy": [[3.499246597290039, 1.790869116783142, 0.5147780179977417, 0.8661812543869019, 0.618782639503479, 3.2371251583099365, 2.093020439147949, 1.2535115480422974, 1.2702531814575195, 2.123955011367798]], "avg_entropy": [1.7267722964286805], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and sixty-nine", "score_of_biggest_cluster": 0.3579648435115814, "score_of_first_answer": 0.3579648435115814, "max_score_of_first_answer": 0.9357828497886658, "min_score_of_first_answer": 0.0369681715965271, "avg_score": 0.3579648435115814, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3014779942380942, "std_of_first_answer": 0.3014779942380942, "avg_entropy_of_first_answer": 1.7267722964286805, "lex_sim": 1.0}
{"question": "In poker, what hand do you have if all your cards are the same suit?", "answer": ["C.P.A."], "scores": [[0.004841007757931948, 0.0044261314906179905, 0.013202743604779243, 0.7934267520904541, 0.05879341438412666, 0.7879335880279541, 0.0009808321483433247]], "normalized_score": [0.02690245769917965], "tokens": [["\u2581C", ".", "P", ".", "A", ".", "\u2581hand"]], "entropy": [[4.894234657287598, 5.279508590698242, 5.2105183601379395, 1.4136083126068115, 4.814759731292725, 1.2811448574066162, 4.248800277709961]], "avg_entropy": [3.8775106838771274], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Flush (disambiguation)", "score_of_biggest_cluster": 0.02690245769917965, "score_of_first_answer": 0.02690245769917965, "max_score_of_first_answer": 0.7934267520904541, "min_score_of_first_answer": 0.0009808321483433247, "avg_score": 0.02690245769917965, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35023905485858436, "std_of_first_answer": 0.35023905485858436, "avg_entropy_of_first_answer": 3.8775106838771274, "lex_sim": 1.0}
{"question": "Who was the mother of Queen Mary 1st of England?", "answer": ["Saint Anna"], "scores": [[0.005372262094169855, 0.008291912265121937]], "normalized_score": [0.0066743032075464725], "tokens": [["\u2581Saint", "\u2581Anna"]], "entropy": [[4.484565734863281, 3.7898783683776855]], "avg_entropy": [4.137222051620483], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Catherine of Aragon in Popular Culture", "score_of_biggest_cluster": 0.0066743032075464725, "score_of_first_answer": 0.0066743032075464725, "max_score_of_first_answer": 0.008291912265121937, "min_score_of_first_answer": 0.005372262094169855, "avg_score": 0.0066743032075464725, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0014598250854760408, "std_of_first_answer": 0.0014598250854760408, "avg_entropy_of_first_answer": 4.137222051620483, "lex_sim": 1.0}
{"question": "Lucy Honeychurch is the heroine of which novel by E. M. Forster?", "answer": ["A Room with a View (novel)"], "scores": [[0.4211204946041107, 0.8092336058616638, 0.823410153388977, 0.9782071113586426, 0.989254355430603, 0.39221006631851196, 0.945315957069397, 0.998794674873352, 0.9800666570663452]], "normalized_score": [0.7730093598365784], "tokens": [["\u2581A", "\u2581Room", "\u2581with", "\u2581a", "\u2581View", "\u2581(", "nov", "el", ")"]], "entropy": [[3.9651741981506348, 1.320683240890503, 0.5768570899963379, 0.1452784687280655, 0.09024704992771149, 0.8845370411872864, 0.4086303114891052, 0.010859795846045017, 0.126307412981987]], "avg_entropy": [0.8365082899108529], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "A Room With a View", "score_of_biggest_cluster": 0.7730093598365784, "score_of_first_answer": 0.7730093598365784, "max_score_of_first_answer": 0.998794674873352, "min_score_of_first_answer": 0.39221006631851196, "avg_score": 0.7730093598365784, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.22828016172334215, "std_of_first_answer": 0.22828016172334215, "avg_entropy_of_first_answer": 0.8365082899108529, "lex_sim": 1.0}
{"question": "\"The first line of which book is, \"\"Christmas won't be Christmas, without any presents, grumbled Jo, lying on the rug\"\"?\"", "answer": ["\"A Christmas Carol\""], "scores": [[0.06300096958875656, 0.1675712615251541, 0.4717111885547638, 0.8598610758781433, 0.6978161334991455]], "normalized_score": [0.3126644492149353], "tokens": [["\u2581\"", "A", "\u2581Christmas", "\u2581Carol", "\""]], "entropy": [[3.2255611419677734, 2.6950488090515137, 2.661196708679199, 0.6799508333206177, 1.4819480180740356]], "avg_entropy": [2.148741102218628], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Good Wives", "score_of_biggest_cluster": 0.3126644492149353, "score_of_first_answer": 0.3126644492149353, "max_score_of_first_answer": 0.8598610758781433, "min_score_of_first_answer": 0.06300096958875656, "avg_score": 0.3126644492149353, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30311209552917984, "std_of_first_answer": 0.30311209552917984, "avg_entropy_of_first_answer": 2.148741102218628, "lex_sim": 1.0}
{"question": "What does a kleptomaniac do compulsively?", "answer": ["I can't find a clear definition: I tried looking it up on the web, using terms such as 'compulsive', 'kleptomaniac' and'steal'."], "scores": [[0.0040390826761722565, 0.01893511228263378, 0.5730159282684326, 0.9986786246299744, 0.1286679059267044, 0.0880114734172821, 0.008089328184723854, 0.3803102374076843, 0.0017858388600870967, 0.042220596224069595, 0.002240024274215102, 0.05658159777522087, 0.08571943640708923, 0.9889948964118958, 0.16312219202518463, 0.37118303775787354, 0.1765754669904709, 0.22433435916900635, 0.01064296904951334, 0.004469322971999645, 0.28023988008499146, 0.9932697415351868, 0.2974916696548462, 0.191172793507576, 0.8828354477882385, 0.8535828590393066, 0.24513176083564758, 0.8206378221511841, 0.08140401542186737, 0.988690972328186, 0.7929940223693848, 0.9644926190376282, 0.5809071063995361, 0.28228795528411865, 0.791470468044281, 0.00040320356492884457, 0.11765765398740768, 0.631869912147522, 0.38691383600234985, 0.04883744567632675, 0.026862602680921555, 0.04307727515697479, 0.3144310712814331, 0.9996837377548218, 0.01612434722483158, 0.00311091891489923, 0.28183913230895996, 0.1490219533443451, 0.039870910346508026, 0.5103095173835754]], "normalized_score": [0.11683002859354019], "tokens": [["\u2581I", "\u2581can", "'", "t", "\u2581find", "\u2581a", "\u2581clear", "\u2581definition", ":", "\u2581I", "\u2581tried", "\u2581looking", "\u2581it", "\u2581up", "\u2581on", "\u2581the", "\u2581web", ",", "\u2581using", "\u2581terms", "\u2581such", "\u2581as", "\u2581'", "comp", "uls", "ive", "',", "\u2581'", "k", "le", "pt", "oman", "iac", "'", "\u2581and", "'", "ste", "al", "'.", "\u2581The", "\u2581top", "-", "r", "ated", "\u2581response", "\u2581found", "\u2581was", "\u2581'", "Ste", "al"]], "entropy": [[2.876669406890869, 4.397767543792725, 2.49898624420166, 0.015804626047611237, 3.8919506072998047, 2.4784836769104004, 4.534524440765381, 2.561213970184326, 2.2198383808135986, 5.269484519958496, 3.1790614128112793, 4.697628021240234, 2.328503131866455, 0.10204977542161942, 2.6033935546875, 3.0588786602020264, 2.665574073791504, 2.31439471244812, 1.7046626806259155, 4.029532432556152, 1.820948839187622, 0.07087008655071259, 3.28912615776062, 4.264303207397461, 0.47283539175987244, 0.5517899394035339, 2.749532699584961, 0.8661712408065796, 3.899441719055176, 0.11379954218864441, 0.6708617806434631, 0.26061880588531494, 0.8055819272994995, 1.0294026136398315, 1.1060192584991455, 0.7347725629806519, 4.423626899719238, 0.8836591243743896, 1.5847731828689575, 2.9212775230407715, 4.105071544647217, 3.4220948219299316, 1.878132700920105, 0.004036424681544304, 3.7500669956207275, 3.1481404304504395, 3.272183895111084, 3.5072343349456787, 4.580294609069824, 1.125225305557251]], "avg_entropy": [2.3748059087619184], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Steal (disambiguation)", "score_of_biggest_cluster": 0.11683002859354019, "score_of_first_answer": 0.11683002859354019, "max_score_of_first_answer": 0.9996837377548218, "min_score_of_first_answer": 0.00040320356492884457, "avg_score": 0.11683002859354019, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34750033260265295, "std_of_first_answer": 0.34750033260265295, "avg_entropy_of_first_answer": 2.3748059087619184, "lex_sim": 1.0}
{"question": "In 1825 which canal was opened, cutting travel time from New York City to the Great Lakes?", "answer": ["Erie Canal"], "scores": [[0.5190495252609253, 0.9915075302124023, 0.6774213314056396]], "normalized_score": [0.703808605670929], "tokens": [["\u2581Er", "ie", "\u2581Canal"]], "entropy": [[3.2428009510040283, 0.0949845239520073, 1.3150426149368286]], "avg_entropy": [1.5509426966309547], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Lock 8, Erie Canal", "score_of_biggest_cluster": 0.703808605670929, "score_of_first_answer": 0.703808605670929, "max_score_of_first_answer": 0.9915075302124023, "min_score_of_first_answer": 0.5190495252609253, "avg_score": 0.703808605670929, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.196341068241848, "std_of_first_answer": 0.196341068241848, "avg_entropy_of_first_answer": 1.5509426966309547, "lex_sim": 1.0}
{"question": "Which actor has been portrayed most on screen by other actors?", "answer": ["Charlie Chaplin"], "scores": [[0.020377643406391144, 0.9167534708976746, 0.9836754202842712]], "normalized_score": [0.263887882232666], "tokens": [["\u2581Charlie", "\u2581Chap", "lin"]], "entropy": [[5.456965923309326, 0.5311433672904968, 0.11099037528038025]], "avg_entropy": [2.0330332219600677], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Roy Export Company Establishment", "score_of_biggest_cluster": 0.263887882232666, "score_of_first_answer": 0.263887882232666, "max_score_of_first_answer": 0.9836754202842712, "min_score_of_first_answer": 0.020377643406391144, "avg_score": 0.263887882232666, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4391798895363255, "std_of_first_answer": 0.4391798895363255, "avg_entropy_of_first_answer": 2.0330332219600677, "lex_sim": 1.0}
{"question": "Lee Kuan Yew and Goh Chok Tong were the first two Prime Ministers of which country?", "answer": ["Singapore"], "scores": [[0.4360843300819397]], "normalized_score": [0.4360843300819397], "tokens": [["\u2581Singapore"]], "entropy": [[3.981189727783203]], "avg_entropy": [3.981189727783203], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Singapore City (disambiguation)", "score_of_biggest_cluster": 0.4360843300819397, "score_of_first_answer": 0.4360843300819397, "max_score_of_first_answer": 0.4360843300819397, "min_score_of_first_answer": 0.4360843300819397, "avg_score": 0.4360843300819397, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.981189727783203, "lex_sim": 1.0}
{"question": "'Feel Like Making Love' and 'The First Time Ever I Saw Your Face' were hit singles for which female artist?", "answer": ["Rozla T. Garman"], "scores": [[0.35575321316719055, 0.009446840733289719, 0.004250180441886187, 0.003697835374623537, 0.021228794008493423, 0.021979624405503273, 0.01305500976741314]], "normalized_score": [0.016419464722275734], "tokens": [["\u2581Ro", "z", "la", "\u2581T", ".", "\u2581Gar", "man"]], "entropy": [[4.8297810554504395, 0.7139453887939453, 4.734142303466797, 4.777544975280762, 5.992374897003174, 5.187863826751709, 3.873382091522217]], "avg_entropy": [4.301290648324149], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Roberta Flack", "score_of_biggest_cluster": 0.016419464722275734, "score_of_first_answer": 0.016419464722275734, "max_score_of_first_answer": 0.35575321316719055, "min_score_of_first_answer": 0.003697835374623537, "avg_score": 0.016419464722275734, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.120382423289312, "std_of_first_answer": 0.120382423289312, "avg_entropy_of_first_answer": 4.301290648324149, "lex_sim": 1.0}
{"question": "The term \u2018Heavy metal\u2019 is said to have first appeared in which 1968 song?", "answer": ["\u201cHeavy metal thunder!\u201d"], "scores": [[0.010559046640992165, 0.15266185998916626, 0.9644937515258789, 0.23382893204689026, 0.15040616691112518, 0.9988511800765991, 0.013323713093996048]], "normalized_score": [0.13278071582317352], "tokens": [["\u2581\u201c", "He", "avy", "\u2581metal", "\u2581th", "under", "!\u201d"]], "entropy": [[5.3057403564453125, 2.832207441329956, 0.27150803804397583, 1.0016738176345825, 3.7397549152374268, 0.010370185598731041, 1.523014783859253]], "avg_entropy": [2.0977527911641767], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Born to Be Wild", "score_of_biggest_cluster": 0.13278071582317352, "score_of_first_answer": 0.13278071582317352, "max_score_of_first_answer": 0.9988511800765991, "min_score_of_first_answer": 0.010559046640992165, "avg_score": 0.13278071582317352, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39976284771197673, "std_of_first_answer": 0.39976284771197673, "avg_entropy_of_first_answer": 2.0977527911641767, "lex_sim": 1.0}
{"question": "The island called Martha's Vineyard off the south of Cape Cod in New England is in which state of the USA?", "answer": ["Masschusetts"], "scores": [[0.04209510609507561, 0.010584295727312565, 0.8521738052368164]], "normalized_score": [0.07241145521402359], "tokens": [["\u2581Mass", "ch", "usetts"]], "entropy": [[4.102538108825684, 1.869926929473877, 1.0619752407073975]], "avg_entropy": [2.344813426335653], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Masachusetts", "score_of_biggest_cluster": 0.07241145521402359, "score_of_first_answer": 0.07241145521402359, "max_score_of_first_answer": 0.8521738052368164, "min_score_of_first_answer": 0.010584295727312565, "avg_score": 0.07241145521402359, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.38951441743244813, "std_of_first_answer": 0.38951441743244813, "avg_entropy_of_first_answer": 2.344813426335653, "lex_sim": 1.0}
{"question": "\"\"\"Alba\"\" is the Gaelic name for what?\"", "answer": ["Scotland"], "scores": [[0.3953360617160797]], "normalized_score": [0.3953360617160797], "tokens": [["\u2581Scotland"]], "entropy": [[4.073184967041016]], "avg_entropy": [4.073184967041016], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "The Scottish Nation", "score_of_biggest_cluster": 0.3953360617160797, "score_of_first_answer": 0.3953360617160797, "max_score_of_first_answer": 0.3953360617160797, "min_score_of_first_answer": 0.3953360617160797, "avg_score": 0.3953360617160797, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.073184967041016, "lex_sim": 1.0}
{"question": "Gary, Jason, Mark and Howard are all members of which British band?", "answer": ["Take That"], "scores": [[0.881974458694458, 0.9418081641197205]], "normalized_score": [0.9114004373550415], "tokens": [["\u2581Take", "\u2581That"]], "entropy": [[0.9899100065231323, 0.4280546307563782]], "avg_entropy": [0.7089823186397552], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Take that", "score_of_biggest_cluster": 0.9114004373550415, "score_of_first_answer": 0.9114004373550415, "max_score_of_first_answer": 0.9418081641197205, "min_score_of_first_answer": 0.881974458694458, "avg_score": 0.9114004373550415, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.029916852712631226, "std_of_first_answer": 0.029916852712631226, "avg_entropy_of_first_answer": 0.7089823186397552, "lex_sim": 1.0}
{"question": "\"Which Chinese games' name literally means in English \"\"little sparrow\"\"?\"", "answer": ["sparrow"], "scores": [[0.005357302725315094, 0.9764394760131836]], "normalized_score": [0.07232622057199478], "tokens": [["\u2581sp", "arrow"]], "entropy": [[4.853697776794434, 0.21772074699401855]], "avg_entropy": [2.535709261894226], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\ud83c\udc01", "score_of_biggest_cluster": 0.07232622057199478, "score_of_first_answer": 0.07232622057199478, "max_score_of_first_answer": 0.9764394760131836, "min_score_of_first_answer": 0.005357302725315094, "avg_score": 0.07232622057199478, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.48554108664393425, "std_of_first_answer": 0.48554108664393425, "avg_entropy_of_first_answer": 2.535709261894226, "lex_sim": 1.0}
{"question": "\"In the book of Genesis who asked the question,\"\"Am I my brotherskeeper\"\"?\"", "answer": ["Cain"], "scores": [[0.39644432067871094, 0.9754487872123718]], "normalized_score": [0.6218610405921936], "tokens": [["\u2581C", "ain"]], "entropy": [[4.368968486785889, 0.22478100657463074]], "avg_entropy": [2.2968747466802597], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Conflict Archive on the INternet", "score_of_biggest_cluster": 0.6218610405921936, "score_of_first_answer": 0.6218610405921936, "max_score_of_first_answer": 0.9754487872123718, "min_score_of_first_answer": 0.39644432067871094, "avg_score": 0.6218610405921936, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28950223326683044, "std_of_first_answer": 0.28950223326683044, "avg_entropy_of_first_answer": 2.2968747466802597, "lex_sim": 1.0}
{"question": "What is a female gerbil called?", "answer": ["Gerbina g. gerbus 2017"], "scores": [[0.3009609580039978, 0.4197365641593933, 0.023423034697771072, 0.00039645188371650875, 0.030705485492944717, 0.10664378106594086, 0.00029722892213612795, 0.0036671527195721865, 0.10644856840372086, 0.42958369851112366, 0.5959768891334534, 0.10305897146463394]], "normalized_score": [0.03883287310600281], "tokens": [["\u2581Ger", "b", "ina", "\u2581g", ".", "\u2581ger", "bus", "\u2581", "2", "0", "1", "7"]], "entropy": [[4.454678535461426, 2.4768941402435303, 3.063811779022217, 0.6715965270996094, 5.034854888916016, 4.059470176696777, 0.9987195730209351, 2.833199977874756, 4.2025651931762695, 2.6082420349121094, 1.4859615564346313, 2.347669839859009]], "avg_entropy": [2.8531386852264404], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "DOE", "score_of_biggest_cluster": 0.03883287310600281, "score_of_first_answer": 0.03883287310600281, "max_score_of_first_answer": 0.5959768891334534, "min_score_of_first_answer": 0.00029722892213612795, "avg_score": 0.03883287310600281, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19716034529022675, "std_of_first_answer": 0.19716034529022675, "avg_entropy_of_first_answer": 2.8531386852264404, "lex_sim": 1.0}
{"question": "How many \u2018Triangles\u2019 are there on the logo of car manufacturer BMW?", "answer": ["18"], "scores": [[0.35631561279296875, 0.176522359251976, 0.03988596424460411]], "normalized_score": [0.1358787566423416], "tokens": [["\u2581", "1", "8"]], "entropy": [[4.222779273986816, 2.811439275741577, 3.026460886001587]], "avg_entropy": [3.3535598119099936], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Four", "score_of_biggest_cluster": 0.1358787566423416, "score_of_first_answer": 0.1358787566423416, "max_score_of_first_answer": 0.35631561279296875, "min_score_of_first_answer": 0.03988596424460411, "avg_score": 0.1358787566423416, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1295817377993553, "std_of_first_answer": 0.1295817377993553, "avg_entropy_of_first_answer": 3.3535598119099936, "lex_sim": 1.0}
{"question": "Glen Moy and Autumn Bliss are varieties of which fruit?", "answer": ["Wine"], "scores": [[0.003695065388455987, 0.09669540822505951]], "normalized_score": [0.01890227198600769], "tokens": [["\u2581W", "ine"]], "entropy": [[4.859891414642334, 5.233453273773193]], "avg_entropy": [5.046672344207764], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hindberries", "score_of_biggest_cluster": 0.01890227198600769, "score_of_first_answer": 0.01890227198600769, "max_score_of_first_answer": 0.09669540822505951, "min_score_of_first_answer": 0.003695065388455987, "avg_score": 0.01890227198600769, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04650017141830176, "std_of_first_answer": 0.04650017141830176, "avg_entropy_of_first_answer": 5.046672344207764, "lex_sim": 1.0}
{"question": "What does the French word noir mean in English?", "answer": ["French word"], "scores": [[0.0028118069749325514, 0.24190934002399445]], "normalized_score": [0.02608068659901619], "tokens": [["\u2581French", "\u2581word"]], "entropy": [[4.378017902374268, 3.789444923400879]], "avg_entropy": [4.083731412887573], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Blackishly", "score_of_biggest_cluster": 0.02608068659901619, "score_of_first_answer": 0.02608068659901619, "max_score_of_first_answer": 0.24190934002399445, "min_score_of_first_answer": 0.0028118069749325514, "avg_score": 0.02608068659901619, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11954876652453095, "std_of_first_answer": 0.11954876652453095, "avg_entropy_of_first_answer": 4.083731412887573, "lex_sim": 1.0}
{"question": "Ridden by jockey Brian Fletcher, which horse won the 1968 Grand National at odds of 100 / 7 ?", "answer": ["Aintree Hurdle"], "scores": [[0.007009122986346483, 0.22225934267044067, 0.997509241104126, 0.011491077952086926, 0.9797831177711487, 0.8763458728790283]], "normalized_score": [0.15761633217334747], "tokens": [["\u2581A", "int", "ree", "\u2581H", "urd", "le"]], "entropy": [[3.1515135765075684, 5.449182033538818, 0.030149582773447037, 3.0670218467712402, 0.1393640786409378, 0.5046759247779846]], "avg_entropy": [2.0569845071683326], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Red Alligator", "score_of_biggest_cluster": 0.15761633217334747, "score_of_first_answer": 0.15761633217334747, "max_score_of_first_answer": 0.997509241104126, "min_score_of_first_answer": 0.007009122986346483, "avg_score": 0.15761633217334747, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4428466982532179, "std_of_first_answer": 0.4428466982532179, "avg_entropy_of_first_answer": 2.0569845071683326, "lex_sim": 1.0}
{"question": "In darts, what is the highest possible checkout with three darts, finishing with a double?", "answer": ["170 and 81"], "scores": [[0.5308152437210083, 0.8901025652885437, 0.5890772342681885, 0.9157221913337708, 0.001010098960250616, 0.44913530349731445, 0.014722545631229877, 0.15449894964694977]], "normalized_score": [0.15048599243164062], "tokens": [["\u2581", "1", "7", "0", "\u2581and", "\u2581", "8", "1"]], "entropy": [[3.5769906044006348, 0.6676088571548462, 1.3502624034881592, 0.4498744010925293, 1.6118624210357666, 3.518664836883545, 1.0461769104003906, 2.5045671463012695]], "avg_entropy": [1.8407509475946426], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one hundred and seventy", "score_of_biggest_cluster": 0.15048599243164062, "score_of_first_answer": 0.15048599243164062, "max_score_of_first_answer": 0.9157221913337708, "min_score_of_first_answer": 0.001010098960250616, "avg_score": 0.15048599243164062, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3380946707462077, "std_of_first_answer": 0.3380946707462077, "avg_entropy_of_first_answer": 1.8407509475946426, "lex_sim": 1.0}
{"question": "In 1487, Portuguese navigator Bartholomew Diaz was the first European to sail past which landmark?", "answer": ["Caboto"], "scores": [[0.017537293955683708, 0.0011967879254370928]], "normalized_score": [0.00458131218329072], "tokens": [["\u2581Cab", "oto"]], "entropy": [[4.904042720794678, 0.789058268070221]], "avg_entropy": [2.8465504944324493], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cape Point Nature Reserve", "score_of_biggest_cluster": 0.00458131218329072, "score_of_first_answer": 0.00458131218329072, "max_score_of_first_answer": 0.017537293955683708, "min_score_of_first_answer": 0.0011967879254370928, "avg_score": 0.00458131218329072, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.008170253015123308, "std_of_first_answer": 0.008170253015123308, "avg_entropy_of_first_answer": 2.8465504944324493, "lex_sim": 1.0}
{"question": "What was the first name of the fictional character Gulliver, which was created by Jonathan Swift?", "answer": ["Gleat"], "scores": [[0.019109651446342468, 0.00644359365105629, 0.005665474105626345]], "normalized_score": [0.008868955075740814], "tokens": [["\u2581G", "le", "at"]], "entropy": [[5.785757541656494, 5.823314666748047, 4.908053874969482]], "avg_entropy": [5.505708694458008], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lemuel", "score_of_biggest_cluster": 0.008868955075740814, "score_of_first_answer": 0.008868955075740814, "max_score_of_first_answer": 0.019109651446342468, "min_score_of_first_answer": 0.005665474105626345, "avg_score": 0.008868955075740814, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.006162434533969461, "std_of_first_answer": 0.006162434533969461, "avg_entropy_of_first_answer": 5.505708694458008, "lex_sim": 1.0}
{"question": "Elliot Carver is the villain in which James Bond film?", "answer": ["Tomorrow Never dies"], "scores": [[0.5770707726478577, 0.9867947697639465, 0.9430685639381409, 0.00384303811006248]], "normalized_score": [0.21314172446727753], "tokens": [["\u2581Tom", "orrow", "\u2581Never", "\u2581dies"]], "entropy": [[2.8934760093688965, 0.14787557721138, 0.4221109449863434, 0.35762959718704224]], "avg_entropy": [0.9552730321884155], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "List of James Bond allies in Tomorrow Never Dies", "score_of_biggest_cluster": 0.21314172446727753, "score_of_first_answer": 0.21314172446727753, "max_score_of_first_answer": 0.9867947697639465, "min_score_of_first_answer": 0.00384303811006248, "avg_score": 0.21314172446727753, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3937534800552406, "std_of_first_answer": 0.3937534800552406, "avg_entropy_of_first_answer": 0.9552730321884155, "lex_sim": 1.0}
{"question": "Between 1959 and 1967 which city was the capital of Pakistan (Islamabad was being built)?", "answer": ["Karachi"], "scores": [[0.40772199630737305, 0.9323588609695435]], "normalized_score": [0.6165575385093689], "tokens": [["\u2581Kar", "achi"]], "entropy": [[4.215424537658691, 0.5033838748931885]], "avg_entropy": [2.35940420627594], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "R\u0101walpindi, Pakistan", "score_of_biggest_cluster": 0.6165575385093689, "score_of_first_answer": 0.6165575385093689, "max_score_of_first_answer": 0.9323588609695435, "min_score_of_first_answer": 0.40772199630737305, "avg_score": 0.6165575385093689, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2623184323310852, "std_of_first_answer": 0.2623184323310852, "avg_entropy_of_first_answer": 2.35940420627594, "lex_sim": 1.0}
{"question": "In children's cartoons, which character lives in a pineapple under the sea?", "answer": ["SpongeBob"], "scores": [[0.3578599691390991, 0.9577044248580933, 0.6794774532318115, 0.600153386592865]], "normalized_score": [0.6114282011985779], "tokens": [["\u2581S", "pon", "ge", "Bob"]], "entropy": [[4.524190902709961, 0.36092036962509155, 0.7273614406585693, 1.9792563915252686]], "avg_entropy": [1.8979322761297226], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Money And Music", "score_of_biggest_cluster": 0.6114282011985779, "score_of_first_answer": 0.6114282011985779, "max_score_of_first_answer": 0.9577044248580933, "min_score_of_first_answer": 0.3578599691390991, "avg_score": 0.6114282011985779, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2141119125488733, "std_of_first_answer": 0.2141119125488733, "avg_entropy_of_first_answer": 1.8979322761297226, "lex_sim": 1.0}
{"question": "Josiah Bartlet was a central character in which U.S. TV drama series?", "answer": ["West Wing"], "scores": [[0.13512299954891205, 0.9326856136322021]], "normalized_score": [0.3550032079219818], "tokens": [["\u2581West", "\u2581Wing"]], "entropy": [[3.4220852851867676, 0.49723726511001587]], "avg_entropy": [1.9596612751483917], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "The West Wing (television series)", "score_of_biggest_cluster": 0.3550032079219818, "score_of_first_answer": 0.3550032079219818, "max_score_of_first_answer": 0.9326856136322021, "min_score_of_first_answer": 0.13512299954891205, "avg_score": 0.3550032079219818, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.39878130704164505, "std_of_first_answer": 0.39878130704164505, "avg_entropy_of_first_answer": 1.9596612751483917, "lex_sim": 1.0}
{"question": "What fictional doctor talks to animals?", "answer": ["A doctor who can talk to animals"], "scores": [[0.0107186958193779, 0.04670988768339157, 0.4595320224761963, 0.10698021203279495, 0.7075308561325073, 0.9237194061279297, 0.8597350716590881]], "normalized_score": [0.20222344994544983], "tokens": [["\u2581A", "\u2581doctor", "\u2581who", "\u2581can", "\u2581talk", "\u2581to", "\u2581animals"]], "entropy": [[4.088134765625, 6.39563512802124, 2.9412622451782227, 3.070702314376831, 1.4935011863708496, 0.41153356432914734, 0.6505036354064941]], "avg_entropy": [2.721610405615398], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dr. Doolittle", "score_of_biggest_cluster": 0.20222344994544983, "score_of_first_answer": 0.20222344994544983, "max_score_of_first_answer": 0.9237194061279297, "min_score_of_first_answer": 0.0107186958193779, "avg_score": 0.20222344994544983, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36484903347133646, "std_of_first_answer": 0.36484903347133646, "avg_entropy_of_first_answer": 2.721610405615398, "lex_sim": 1.0}
{"question": "What's the name of the intelligent talking horses in Swift's Gulliver's Travels ?", "answer": ["Horses"], "scores": [[0.1625623255968094, 0.6412186622619629, 0.7710389494895935]], "normalized_score": [0.4315529763698578], "tokens": [["\u2581H", "ors", "es"]], "entropy": [[3.653313159942627, 2.8140316009521484, 1.7232704162597656]], "avg_entropy": [2.7302050590515137], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Houyhnhm", "score_of_biggest_cluster": 0.4315529763698578, "score_of_first_answer": 0.4315529763698578, "max_score_of_first_answer": 0.7710389494895935, "min_score_of_first_answer": 0.1625623255968094, "avg_score": 0.4315529763698578, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.26166326907126053, "std_of_first_answer": 0.26166326907126053, "avg_entropy_of_first_answer": 2.7302050590515137, "lex_sim": 1.0}
{"question": "When the egg is released from the ovary, what is left behind?", "answer": ["Follicle"], "scores": [[0.06390958279371262, 0.979498028755188, 0.7896233797073364]], "normalized_score": [0.3669975697994232], "tokens": [["\u2581Fol", "lic", "le"]], "entropy": [[4.479519844055176, 0.17662540078163147, 0.7784626483917236]], "avg_entropy": [1.8115359644095104], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Corpora lutea", "score_of_biggest_cluster": 0.3669975697994232, "score_of_first_answer": 0.3669975697994232, "max_score_of_first_answer": 0.979498028755188, "min_score_of_first_answer": 0.06390958279371262, "avg_score": 0.3669975697994232, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3945482789441311, "std_of_first_answer": 0.3945482789441311, "avg_entropy_of_first_answer": 1.8115359644095104, "lex_sim": 1.0}
{"question": "Which country won the 2007 FIFA Women's World Cup?", "answer": ["World Football Women World Cup"], "scores": [[0.006247637793421745, 0.037466589361429214, 0.028325924649834633, 0.005503155291080475, 0.7484086155891418]], "normalized_score": [0.030708514153957367], "tokens": [["\u2581World", "\u2581Football", "\u2581Women", "\u2581World", "\u2581Cup"]], "entropy": [[4.679532051086426, 2.7522456645965576, 3.6978914737701416, 1.7561454772949219, 1.642329216003418]], "avg_entropy": [2.905628776550293], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "United States of Germany", "score_of_biggest_cluster": 0.030708514153957367, "score_of_first_answer": 0.030708514153957367, "max_score_of_first_answer": 0.7484086155891418, "min_score_of_first_answer": 0.005503155291080475, "avg_score": 0.030708514153957367, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29187379505275163, "std_of_first_answer": 0.29187379505275163, "avg_entropy_of_first_answer": 2.905628776550293, "lex_sim": 1.0}
{"question": "Which member of the cabinet is MP for Maidenhead?", "answer": ["Theresa May"], "scores": [[0.29051822423934937, 0.9864299297332764, 0.71645188331604]], "normalized_score": [0.5899413824081421], "tokens": [["\u2581Ther", "esa", "\u2581May"]], "entropy": [[4.655946731567383, 0.1179007887840271, 2.109300136566162]], "avg_entropy": [2.294382552305857], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Theresa Mary May", "score_of_biggest_cluster": 0.5899413824081421, "score_of_first_answer": 0.5899413824081421, "max_score_of_first_answer": 0.9864299297332764, "min_score_of_first_answer": 0.29051822423934937, "avg_score": 0.5899413824081421, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2864729441025786, "std_of_first_answer": 0.2864729441025786, "avg_entropy_of_first_answer": 2.294382552305857, "lex_sim": 1.0}
{"question": "Prior to carving Mount Rushmore, Gutzon Borglum started work on what Georgia monument, the largest bas relief structure in the world, featuring 3 Civil War leaders, Jefferson Davis, Robert E. Lee, and Stonewall Jackson?", "answer": ["Atlanta MonuMENTA"], "scores": [[0.002090351888909936, 0.002694496186450124, 0.048161692917346954, 0.08896947652101517, 0.009525756351649761]], "normalized_score": [0.011811568401753902], "tokens": [["\u2581Atlanta", "\u2581Mon", "u", "MENT", "A"]], "entropy": [[3.0606679916381836, 4.300779342651367, 3.1309521198272705, 3.673340082168579, 2.0275745391845703]], "avg_entropy": [3.2386628150939942], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Stone Mountain Park", "score_of_biggest_cluster": 0.011811568401753902, "score_of_first_answer": 0.011811568401753902, "max_score_of_first_answer": 0.08896947652101517, "min_score_of_first_answer": 0.002090351888909936, "avg_score": 0.011811568401753902, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03391331348476426, "std_of_first_answer": 0.03391331348476426, "avg_entropy_of_first_answer": 3.2386628150939942, "lex_sim": 1.0}
{"question": "The actor John Wayne was known by what nickname?", "answer": ["The Duke"], "scores": [[0.17709967494010925, 0.8481852412223816]], "normalized_score": [0.3875736594200134], "tokens": [["\u2581The", "\u2581Duke"]], "entropy": [[5.00360631942749, 1.292459487915039]], "avg_entropy": [3.1480329036712646], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "The Duke", "score_of_biggest_cluster": 0.3875736594200134, "score_of_first_answer": 0.3875736594200134, "max_score_of_first_answer": 0.8481852412223816, "min_score_of_first_answer": 0.17709967494010925, "avg_score": 0.3875736594200134, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.33554278314113617, "std_of_first_answer": 0.33554278314113617, "avg_entropy_of_first_answer": 3.1480329036712646, "lex_sim": 1.0}
{"question": "Between 1838 and 1839, against which country did France fight the so-called Pastry War?", "answer": ["San Salvadorian Army"], "scores": [[0.0023375703021883965, 0.15716949105262756, 0.00316635612398386, 0.00692336680367589]], "normalized_score": [0.009473327547311783], "tokens": [["\u2581San", "\u2581Salvador", "ian", "\u2581Army"]], "entropy": [[5.281035423278809, 4.565831184387207, 1.6013000011444092, 3.9773659706115723]], "avg_entropy": [3.8563831448554993], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mexican Republic", "score_of_biggest_cluster": 0.009473327547311783, "score_of_first_answer": 0.009473327547311783, "max_score_of_first_answer": 0.15716949105262756, "min_score_of_first_answer": 0.0023375703021883965, "avg_score": 0.009473327547311783, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06628518813038595, "std_of_first_answer": 0.06628518813038595, "avg_entropy_of_first_answer": 3.8563831448554993, "lex_sim": 1.0}
{"question": "Who is also known as Jove?", "answer": ["Hera"], "scores": [[0.006302004214376211, 0.12513455748558044]], "normalized_score": [0.028081996366381645], "tokens": [["\u2581H", "era"]], "entropy": [[4.35667610168457, 4.768283367156982]], "avg_entropy": [4.562479734420776], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Physical characteristics of Jupiter", "score_of_biggest_cluster": 0.028081996366381645, "score_of_first_answer": 0.028081996366381645, "max_score_of_first_answer": 0.12513455748558044, "min_score_of_first_answer": 0.006302004214376211, "avg_score": 0.028081996366381645, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05941627663560212, "std_of_first_answer": 0.05941627663560212, "avg_entropy_of_first_answer": 4.562479734420776, "lex_sim": 1.0}
{"question": "What is the capital of the US state of Georgia?", "answer": ["Atlanta"], "scores": [[0.2854464054107666]], "normalized_score": [0.2854464054107666], "tokens": [["\u2581Atlanta"]], "entropy": [[5.025028705596924]], "avg_entropy": [5.025028705596924], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marthasville, GA", "score_of_biggest_cluster": 0.2854464054107666, "score_of_first_answer": 0.2854464054107666, "max_score_of_first_answer": 0.2854464054107666, "min_score_of_first_answer": 0.2854464054107666, "avg_score": 0.2854464054107666, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.025028705596924, "lex_sim": 1.0}
{"question": "The Carnegie family fortune was built around Andrew Carnegie\u2019s success in manufacture in which industry?", "answer": ["steelworks plant"], "scores": [[0.05357016623020172, 0.02326958253979683, 0.0004909191047772765]], "normalized_score": [0.008489989675581455], "tokens": [["\u2581steel", "works", "\u2581plant"]], "entropy": [[4.511098384857178, 1.9442026615142822, 0.6887588500976562]], "avg_entropy": [2.3813532988230386], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Steel", "score_of_biggest_cluster": 0.008489989675581455, "score_of_first_answer": 0.008489989675581455, "max_score_of_first_answer": 0.05357016623020172, "min_score_of_first_answer": 0.0004909191047772765, "avg_score": 0.008489989675581455, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.02174191894645254, "std_of_first_answer": 0.02174191894645254, "avg_entropy_of_first_answer": 2.3813532988230386, "lex_sim": 1.0}
{"question": "What name is given to a line on a map joining places of equal rainfall?", "answer": ["Isodaphyll"], "scores": [[0.23614917695522308, 0.09414803981781006, 0.020394647493958473, 0.08442413806915283, 0.1230408251285553]], "normalized_score": [0.08602128177881241], "tokens": [["\u2581Is", "od", "ap", "hy", "ll"]], "entropy": [[4.4358978271484375, 3.6666340827941895, 4.431163787841797, 3.9600343704223633, 2.045022964477539]], "avg_entropy": [3.707750606536865], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Isopleths", "score_of_biggest_cluster": 0.08602128177881241, "score_of_first_answer": 0.08602128177881241, "max_score_of_first_answer": 0.23614917695522308, "min_score_of_first_answer": 0.020394647493958473, "avg_score": 0.08602128177881241, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07071760707562358, "std_of_first_answer": 0.07071760707562358, "avg_entropy_of_first_answer": 3.707750606536865, "lex_sim": 1.0}
{"question": "In 1987 a French court found which Gestapo chief guilty of war crimes and sentenced him to life imprisonment?", "answer": ["He has his own way"], "scores": [[0.005584989674389362, 0.004204927012324333, 0.01134374551475048, 0.2205866128206253, 0.07482202351093292]], "normalized_score": [0.02131223864853382], "tokens": [["\u2581He", "\u2581has", "\u2581his", "\u2581own", "\u2581way"]], "entropy": [[4.177091598510742, 3.7750306129455566, 4.900984287261963, 4.759676933288574, 6.484277725219727]], "avg_entropy": [4.8194122314453125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nickolas Barbie", "score_of_biggest_cluster": 0.02131223864853382, "score_of_first_answer": 0.02131223864853382, "max_score_of_first_answer": 0.2205866128206253, "min_score_of_first_answer": 0.004204927012324333, "avg_score": 0.02131223864853382, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08293917975116304, "std_of_first_answer": 0.08293917975116304, "avg_entropy_of_first_answer": 4.8194122314453125, "lex_sim": 1.0}
{"question": "Which British butterfly has the scientific name Vanessa atalanta?", "answer": ["\u6602\u6602\u7384\u68cb"], "scores": [[0.031075356528162956, 0.03466757759451866, 0.03464701026678085, 0.05825283005833626, 0.17522995173931122, 0.28102946281433105, 0.8727932572364807, 0.04177263751626015, 0.05662441998720169, 0.07654166221618652, 0.046639010310173035, 0.06771206855773926, 0.0446232371032238]], "normalized_score": [0.07502081245183945], "tokens": [["\u2581", "<0xE6>", "<0x98>", "<0x82>", "<0xE6>", "<0x98>", "<0x82>", "<0xE7>", "<0x8E>", "<0x84>", "<0xE6>", "<0xA3>", "<0x8B>"]], "entropy": [[4.78413200378418, 5.364241600036621, 3.964931011199951, 2.758776903152466, 4.389432907104492, 3.2994179725646973, 0.7869255542755127, 3.0922679901123047, 3.6695995330810547, 2.701087236404419, 2.8953990936279297, 3.280015468597412, 3.014768123626709]], "avg_entropy": [3.3846919536590576], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Papilio atalanta", "score_of_biggest_cluster": 0.07502081245183945, "score_of_first_answer": 0.07502081245183945, "max_score_of_first_answer": 0.8727932572364807, "min_score_of_first_answer": 0.031075356528162956, "avg_score": 0.07502081245183945, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22231702522550056, "std_of_first_answer": 0.22231702522550056, "avg_entropy_of_first_answer": 3.3846919536590576, "lex_sim": 0.0}
{"question": "Colin Meads, nicknamed 'Pinetree', a former rugby union footballer who played 55 test matches for his national team from 1957 until 1971 and was named his country's Player of the Century, represented which country?", "answer": ["Aotearoa New Zealand"], "scores": [[0.009306369349360466, 0.3396172821521759, 0.972886323928833, 0.9823556542396545, 0.22244249284267426, 0.9922565221786499]], "normalized_score": [0.2955677807331085], "tokens": [["\u2581A", "ot", "ear", "oa", "\u2581New", "\u2581Zealand"]], "entropy": [[3.4271554946899414, 3.4593417644500732, 0.21444912254810333, 0.1296987533569336, 2.136718273162842, 0.07176633179187775]], "avg_entropy": [1.5731882899999619], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "N Z", "score_of_biggest_cluster": 0.2955677807331085, "score_of_first_answer": 0.2955677807331085, "max_score_of_first_answer": 0.9922565221786499, "min_score_of_first_answer": 0.009306369349360466, "avg_score": 0.2955677807331085, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.407691851505306, "std_of_first_answer": 0.407691851505306, "avg_entropy_of_first_answer": 1.5731882899999619, "lex_sim": 1.0}
{"question": "In our solar system, the sun is composed of mostly which gas?", "answer": ["Helium-4"], "scores": [[0.20054610073566437, 0.9556101560592651, 0.03286152705550194, 0.4636726975440979]], "normalized_score": [0.2324601709842682], "tokens": [["\u2581Hel", "ium", "-", "4"]], "entropy": [[4.010472297668457, 0.3373414874076843, 1.2761350870132446, 1.926784873008728]], "avg_entropy": [1.8876834362745285], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Atomic number 1", "score_of_biggest_cluster": 0.2324601709842682, "score_of_first_answer": 0.2324601709842682, "max_score_of_first_answer": 0.9556101560592651, "min_score_of_first_answer": 0.03286152705550194, "avg_score": 0.2324601709842682, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34879625229821204, "std_of_first_answer": 0.34879625229821204, "avg_entropy_of_first_answer": 1.8876834362745285, "lex_sim": 1.0}
{"question": "Stylised as a lower case x, the album \u2018Multiply\u2019, which topped the UK charts for 8 weeks in 2014, is by which Brit award-winning singer-songwriter?", "answer": ["ed shearmon"], "scores": [[0.00798846036195755, 0.416612446308136, 0.0031612380407750607, 0.06603177636861801]], "normalized_score": [0.02887030690908432], "tokens": [["\u2581ed", "\u2581she", "arm", "on"]], "entropy": [[5.339905261993408, 2.8539693355560303, 0.9228505492210388, 4.102826118469238]], "avg_entropy": [3.304887816309929], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Edward Christopher Sheeran", "score_of_biggest_cluster": 0.02887030690908432, "score_of_first_answer": 0.02887030690908432, "max_score_of_first_answer": 0.416612446308136, "min_score_of_first_answer": 0.0031612380407750607, "avg_score": 0.02887030690908432, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1710568806789032, "std_of_first_answer": 0.1710568806789032, "avg_entropy_of_first_answer": 3.304887816309929, "lex_sim": 1.0}
{"question": "How much, in pounds sterling, does the Best in Show Winner receive at the annual Crufts Dog Show?", "answer": ["\u00db"], "scores": [[0.3176168203353882, 0.00035685134935192764]], "normalized_score": [0.010646218433976173], "tokens": [["\u2581", "\u00db"]], "entropy": [[3.605026960372925, 2.7922677993774414]], "avg_entropy": [3.198647379875183], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "100 pounds", "score_of_biggest_cluster": 0.010646218433976173, "score_of_first_answer": 0.010646218433976173, "max_score_of_first_answer": 0.3176168203353882, "min_score_of_first_answer": 0.00035685134935192764, "avg_score": 0.010646218433976173, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.15862998449301813, "std_of_first_answer": 0.15862998449301813, "avg_entropy_of_first_answer": 3.198647379875183, "lex_sim": 0.0}
{"question": "Chatham Dockyard, which closed in 1984 and is now a visitor centre, is located in which English county?", "answer": ["Kent"], "scores": [[0.3609843850135803]], "normalized_score": [0.3609843850135803], "tokens": [["\u2581Kent"]], "entropy": [[4.594094753265381]], "avg_entropy": [4.594094753265381], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Garden of england", "score_of_biggest_cluster": 0.3609843850135803, "score_of_first_answer": 0.3609843850135803, "max_score_of_first_answer": 0.3609843850135803, "min_score_of_first_answer": 0.3609843850135803, "avg_score": 0.3609843850135803, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.594094753265381, "lex_sim": 1.0}
{"question": "What are the topmost 7 vertebrae of the backbone called?", "answer": ["The cervical vertebrae"], "scores": [[0.05214698240160942, 0.259832501411438, 0.9862527251243591, 0.9410673975944519, 0.734066367149353, 0.9863554835319519, 0.9373549818992615]], "normalized_score": [0.5063581466674805], "tokens": [["\u2581The", "\u2581cer", "v", "ical", "\u2581verte", "bra", "e"]], "entropy": [[4.72952938079834, 4.099764823913574, 0.11193488538265228, 0.39488643407821655, 1.1570686101913452, 0.09001182019710541, 0.3199383020401001]], "avg_entropy": [1.5575906080859048], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Cervical", "score_of_biggest_cluster": 0.5063581466674805, "score_of_first_answer": 0.5063581466674805, "max_score_of_first_answer": 0.9863554835319519, "min_score_of_first_answer": 0.05214698240160942, "avg_score": 0.5063581466674805, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.35717367099640857, "std_of_first_answer": 0.35717367099640857, "avg_entropy_of_first_answer": 1.5575906080859048, "lex_sim": 1.0}
{"question": "The Syrah grape variety makes wine that is?", "answer": ["Shiraz wine"], "scores": [[0.19105729460716248, 0.9876682162284851, 0.9781562089920044, 0.05193733796477318]], "normalized_score": [0.31290724873542786], "tokens": [["\u2581Sh", "ira", "z", "\u2581wine"]], "entropy": [[4.991969108581543, 0.12191058695316315, 0.17648878693580627, 1.2965136766433716]], "avg_entropy": [1.646720539778471], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Red (colour)", "score_of_biggest_cluster": 0.31290724873542786, "score_of_first_answer": 0.31290724873542786, "max_score_of_first_answer": 0.9876682162284851, "min_score_of_first_answer": 0.05193733796477318, "avg_score": 0.31290724873542786, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43351990840049326, "std_of_first_answer": 0.43351990840049326, "avg_entropy_of_first_answer": 1.646720539778471, "lex_sim": 1.0}
{"question": "What kind of creature is Firenze?", "answer": ["A centaur"], "scores": [[0.09965896606445312, 0.48445752263069153, 0.960245668888092]], "normalized_score": [0.3592401146888733], "tokens": [["\u2581A", "\u2581cent", "aur"]], "entropy": [[4.283653736114502, 2.434051275253296, 0.19886714220046997]], "avg_entropy": [2.3055240511894226], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Centaurs", "score_of_biggest_cluster": 0.3592401146888733, "score_of_first_answer": 0.3592401146888733, "max_score_of_first_answer": 0.960245668888092, "min_score_of_first_answer": 0.09965896606445312, "avg_score": 0.3592401146888733, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35198702033067075, "std_of_first_answer": 0.35198702033067075, "avg_entropy_of_first_answer": 2.3055240511894226, "lex_sim": 1.0}
{"question": "Beethoven wrote only one opera, can you name it?", "answer": ["Fidelio"], "scores": [[0.4110439419746399, 0.8988026976585388, 0.961772084236145]], "normalized_score": [0.708285391330719], "tokens": [["\u2581F", "idel", "io"]], "entropy": [[4.157763957977295, 0.8434184789657593, 0.3012954592704773]], "avg_entropy": [1.7674926320711772], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Leonore", "score_of_biggest_cluster": 0.708285391330719, "score_of_first_answer": 0.708285391330719, "max_score_of_first_answer": 0.961772084236145, "min_score_of_first_answer": 0.4110439419746399, "avg_score": 0.708285391330719, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.24611994232060336, "std_of_first_answer": 0.24611994232060336, "avg_entropy_of_first_answer": 1.7674926320711772, "lex_sim": 1.0}
{"question": "Which element, atomic number 15, has the symbol 'P'?", "answer": ["Phosphorous"], "scores": [[0.37836432456970215, 0.8972575664520264, 0.16103827953338623]], "normalized_score": [0.3795352876186371], "tokens": [["\u2581Ph", "osph", "orous"]], "entropy": [[4.549479961395264, 0.7400280237197876, 0.9062793254852295]], "avg_entropy": [2.0652624368667603], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Phospho", "score_of_biggest_cluster": 0.3795352876186371, "score_of_first_answer": 0.3795352876186371, "max_score_of_first_answer": 0.8972575664520264, "min_score_of_first_answer": 0.16103827953338623, "avg_score": 0.3795352876186371, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30885085302946724, "std_of_first_answer": 0.30885085302946724, "avg_entropy_of_first_answer": 2.0652624368667603, "lex_sim": 1.0}
{"question": "With which area of manufacturing are 'Linn', 'Martin Logan' and 'Krell' associated?", "answer": ["Audio"], "scores": [[0.22692963480949402]], "normalized_score": [0.22692963480949402], "tokens": [["\u2581Audio"]], "entropy": [[4.221970558166504]], "avg_entropy": [4.221970558166504], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "EN 61305", "score_of_biggest_cluster": 0.22692963480949402, "score_of_first_answer": 0.22692963480949402, "max_score_of_first_answer": 0.22692963480949402, "min_score_of_first_answer": 0.22692963480949402, "avg_score": 0.22692963480949402, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.221970558166504, "lex_sim": 1.0}
{"question": "The line 'The mirror crack'd from side to side' comes from which poem?", "answer": ["Ms Rivers"], "scores": [[0.0022607522550970316, 0.10783419013023376, 0.0023295371793210506, 0.007674905937165022]], "normalized_score": [0.008125275373458862], "tokens": [["\u2581M", "s", "\u2581R", "ivers"]], "entropy": [[2.7491884231567383, 5.281115531921387, 4.532715797424316, 5.758732318878174]], "avg_entropy": [4.580438017845154], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Lady of Shallott", "score_of_biggest_cluster": 0.008125275373458862, "score_of_first_answer": 0.008125275373458862, "max_score_of_first_answer": 0.10783419013023376, "min_score_of_first_answer": 0.0022607522550970316, "avg_score": 0.008125275373458862, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04497690764975399, "std_of_first_answer": 0.04497690764975399, "avg_entropy_of_first_answer": 4.580438017845154, "lex_sim": 1.0}
{"question": "Which war, engineered by Bismarck, included the siege and surrender of Paris and ended in the union of German states with William I as emperor?", "answer": ["Franco-Prussian War"], "scores": [[0.7844521403312683, 0.9118268489837646, 0.9131543040275574, 0.9993674159049988, 0.9884734749794006, 0.8755598068237305]], "normalized_score": [0.9092147946357727], "tokens": [["\u2581Franco", "-", "Pr", "uss", "ian", "\u2581War"]], "entropy": [[1.4728695154190063, 0.416981041431427, 0.38022246956825256, 0.005803860258311033, 0.07549789547920227, 0.548394501209259]], "avg_entropy": [0.4832948805609097], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Invasion of France (1870)", "score_of_biggest_cluster": 0.9092147946357727, "score_of_first_answer": 0.9092147946357727, "max_score_of_first_answer": 0.9993674159049988, "min_score_of_first_answer": 0.7844521403312683, "avg_score": 0.9092147946357727, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.071970883461868, "std_of_first_answer": 0.071970883461868, "avg_entropy_of_first_answer": 0.4832948805609097, "lex_sim": 1.0}
{"question": "Name the space craft that exploded 72 seconds after lift off killing all 7 crew members in 1986.", "answer": ["Launching"], "scores": [[0.00252211419865489, 0.9485628604888916, 0.028674542903900146]], "normalized_score": [0.04093634709715843], "tokens": [["\u2581La", "unch", "ing"]], "entropy": [[4.367718696594238, 0.49486786127090454, 3.6832613945007324]], "avg_entropy": [2.8486159841219583], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "OV-099", "score_of_biggest_cluster": 0.04093634709715843, "score_of_first_answer": 0.04093634709715843, "max_score_of_first_answer": 0.9485628604888916, "min_score_of_first_answer": 0.00252211419865489, "avg_score": 0.04093634709715843, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4399332727178214, "std_of_first_answer": 0.4399332727178214, "avg_entropy_of_first_answer": 2.8486159841219583, "lex_sim": 1.0}
{"question": "In physics, 'the rate of change of position' is a basic definition of what term?", "answer": ["Velocity of motion"], "scores": [[0.33340129256248474, 0.9934036135673523, 0.005753678269684315, 0.04881969839334488]], "normalized_score": [0.09821061044931412], "tokens": [["\u2581Vel", "ocity", "\u2581of", "\u2581motion"]], "entropy": [[4.237468242645264, 0.0641518160700798, 0.5749074220657349, 3.3524415493011475]], "avg_entropy": [2.0572422575205564], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "First temporal derivative of displacement", "score_of_biggest_cluster": 0.09821061044931412, "score_of_first_answer": 0.09821061044931412, "max_score_of_first_answer": 0.9934036135673523, "min_score_of_first_answer": 0.005753678269684315, "avg_score": 0.09821061044931412, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.39476959706039777, "std_of_first_answer": 0.39476959706039777, "avg_entropy_of_first_answer": 2.0572422575205564, "lex_sim": 1.0}
{"question": "Which disc jockey used to advertise Brentford Nylon", "answer": ["Alan Freeman"], "scores": [[0.005529291462153196, 0.7726714611053467, 0.9277128577232361]], "normalized_score": [0.1582556664943695], "tokens": [["\u2581Alan", "\u2581Fre", "eman"]], "entropy": [[5.196159362792969, 1.3202255964279175, 0.3699050545692444]], "avg_entropy": [2.29543000459671], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "%22Fluff%22 Freeman", "score_of_biggest_cluster": 0.1582556664943695, "score_of_first_answer": 0.1582556664943695, "max_score_of_first_answer": 0.9277128577232361, "min_score_of_first_answer": 0.005529291462153196, "avg_score": 0.1582556664943695, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4031773076991893, "std_of_first_answer": 0.4031773076991893, "avg_entropy_of_first_answer": 2.29543000459671, "lex_sim": 1.0}
{"question": "Which orchestral conductor was married to one of the subjects of the film Hillary and Jackie?", "answer": ["Oph\u00e9lie Gaillard"], "scores": [[0.018426984548568726, 0.01897306926548481, 0.1369829922914505, 0.9778239727020264, 0.5275440812110901, 0.9811274409294128, 0.994510293006897]], "normalized_score": [0.218928262591362], "tokens": [["\u2581O", "ph", "\u00e9", "lie", "\u2581Ga", "ill", "ard"]], "entropy": [[6.022265911102295, 4.917786598205566, 3.1083884239196777, 0.22196123003959656, 2.5535600185394287, 0.14217080175876617, 0.05249789357185364]], "avg_entropy": [2.431232982448169], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Daniel Barenboim and Wagner", "score_of_biggest_cluster": 0.218928262591362, "score_of_first_answer": 0.218928262591362, "max_score_of_first_answer": 0.994510293006897, "min_score_of_first_answer": 0.018426984548568726, "avg_score": 0.218928262591362, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43040345035570554, "std_of_first_answer": 0.43040345035570554, "avg_entropy_of_first_answer": 2.431232982448169, "lex_sim": 1.0}
{"question": "Which Jersey-born actor played Superman in Man of Steel?", "answer": ["Henry Cavill"], "scores": [[0.36018505692481995, 0.4215396046638489, 0.8786356449127197]], "normalized_score": [0.5109648108482361], "tokens": [["\u2581Henry", "\u2581Cav", "ill"]], "entropy": [[4.636088848114014, 4.217824935913086, 0.6370123624801636]], "avg_entropy": [3.1636420488357544], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Henry Cavill", "score_of_biggest_cluster": 0.5109648108482361, "score_of_first_answer": 0.5109648108482361, "max_score_of_first_answer": 0.8786356449127197, "min_score_of_first_answer": 0.36018505692481995, "avg_score": 0.5109648108482361, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23129879250404353, "std_of_first_answer": 0.23129879250404353, "avg_entropy_of_first_answer": 3.1636420488357544, "lex_sim": 1.0}
{"question": "The seven sisters known as the 'Pleiades' were the daughters of which legendary figure?", "answer": ["Zeus"], "scores": [[0.013716303743422031, 0.9888112545013428]], "normalized_score": [0.11645957082509995], "tokens": [["\u2581Ze", "us"]], "entropy": [[4.92491340637207, 0.10888096690177917]], "avg_entropy": [2.5168971866369247], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Atlases", "score_of_biggest_cluster": 0.11645957082509995, "score_of_first_answer": 0.11645957082509995, "max_score_of_first_answer": 0.9888112545013428, "min_score_of_first_answer": 0.013716303743422031, "avg_score": 0.11645957082509995, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.48754747537896037, "std_of_first_answer": 0.48754747537896037, "avg_entropy_of_first_answer": 2.5168971866369247, "lex_sim": 1.0}
{"question": "With which orchestra was Arthur Fiedler (who died in 1979) most associated?", "answer": ["Boston Pops Orchestra"], "scores": [[0.30619576573371887, 0.7217761278152466, 0.9962181448936462, 0.6400559544563293]], "normalized_score": [0.6126938462257385], "tokens": [["\u2581Boston", "\u2581P", "ops", "\u2581Orchestra"]], "entropy": [[4.854167938232422, 1.499250888824463, 0.04022900015115738, 1.041549563407898]], "avg_entropy": [1.858799347653985], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "The Boston Pops Orchestra", "score_of_biggest_cluster": 0.6126938462257385, "score_of_first_answer": 0.6126938462257385, "max_score_of_first_answer": 0.9962181448936462, "min_score_of_first_answer": 0.30619576573371887, "avg_score": 0.6126938462257385, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.24611337073493908, "std_of_first_answer": 0.24611337073493908, "avg_entropy_of_first_answer": 1.858799347653985, "lex_sim": 1.0}
{"question": "Sound engineer Peter Lodge first announced which phrase on the London Undergound rail network in 1969?", "answer": ["Please mind the gap"], "scores": [[0.02210213616490364, 0.4320826530456543, 0.9939643740653992, 0.9571061730384827]], "normalized_score": [0.3087329864501953], "tokens": [["\u2581Please", "\u2581mind", "\u2581the", "\u2581gap"]], "entropy": [[2.4103078842163086, 2.1783714294433594, 0.05729684978723526, 0.3413168787956238]], "avg_entropy": [1.2468232605606318], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Mind The Gap", "score_of_biggest_cluster": 0.3087329864501953, "score_of_first_answer": 0.3087329864501953, "max_score_of_first_answer": 0.9939643740653992, "min_score_of_first_answer": 0.02210213616490364, "avg_score": 0.3087329864501953, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4015246003199373, "std_of_first_answer": 0.4015246003199373, "avg_entropy_of_first_answer": 1.2468232605606318, "lex_sim": 1.0}
{"question": "Which was the first group to have a number one hit in the USA for Motown Records?", "answer": ["1er Amour"], "scores": [[0.022855719551444054, 0.17189355194568634, 0.0005843212711624801, 0.0027142860926687717, 0.11420194804668427]], "normalized_score": [0.014806324616074562], "tokens": [["\u2581", "1", "er", "\u2581Am", "our"]], "entropy": [[4.2569427490234375, 3.8821725845336914, 2.718202590942383, 5.052889823913574, 3.8070664405822754]], "avg_entropy": [3.9434548377990724], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Juanita Cowart", "score_of_biggest_cluster": 0.014806324616074562, "score_of_first_answer": 0.014806324616074562, "max_score_of_first_answer": 0.17189355194568634, "min_score_of_first_answer": 0.0005843212711624801, "avg_score": 0.014806324616074562, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0687307499652754, "std_of_first_answer": 0.0687307499652754, "avg_entropy_of_first_answer": 3.9434548377990724, "lex_sim": 1.0}
{"question": "Nigel Hawthorne was Oscar nominated for The Madness of which King?", "answer": ["George III"], "scores": [[0.2510199546813965, 0.6986839771270752]], "normalized_score": [0.41878825426101685], "tokens": [["\u2581George", "\u2581III"]], "entropy": [[3.259049415588379, 1.5019395351409912]], "avg_entropy": [2.380494475364685], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gheevarghese", "score_of_biggest_cluster": 0.41878825426101685, "score_of_first_answer": 0.41878825426101685, "max_score_of_first_answer": 0.6986839771270752, "min_score_of_first_answer": 0.2510199546813965, "avg_score": 0.41878825426101685, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22383201122283936, "std_of_first_answer": 0.22383201122283936, "avg_entropy_of_first_answer": 2.380494475364685, "lex_sim": 1.0}
{"question": "Which vegetable do the French call Chou-Fleur?", "answer": ["Brassica oleracea Gaiola Group"], "scores": [[0.017412377521395683, 0.6052458882331848, 0.8851907253265381, 0.7196058034896851, 0.981427788734436, 0.991951584815979, 0.9853007197380066, 0.013180292211472988, 0.2625976800918579, 0.005517531186342239, 0.18558770418167114]], "normalized_score": [0.2019781917333603], "tokens": [["\u2581Br", "ass", "ica", "\u2581o", "ler", "ace", "a", "\u2581G", "ai", "ola", "\u2581Group"]], "entropy": [[3.3579161167144775, 0.9636585712432861, 0.632520318031311, 1.3450216054916382, 0.13601379096508026, 0.06454320251941681, 0.12918061017990112, 2.4362576007843018, 1.8070319890975952, 2.872795343399048, 1.9213764667510986]], "avg_entropy": [1.4242105104706504], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Purple cauliflower", "score_of_biggest_cluster": 0.2019781917333603, "score_of_first_answer": 0.2019781917333603, "max_score_of_first_answer": 0.991951584815979, "min_score_of_first_answer": 0.005517531186342239, "avg_score": 0.2019781917333603, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40277048270263466, "std_of_first_answer": 0.40277048270263466, "avg_entropy_of_first_answer": 1.4242105104706504, "lex_sim": 1.0}
{"question": "Which US author wrote \u2018The Naked and the Dead\u2019, based on his military service in World War II?", "answer": ["Norman Mailer"], "scores": [[0.14465220272541046, 0.7774484157562256, 0.976049542427063]], "normalized_score": [0.47880223393440247], "tokens": [["\u2581Norman", "\u2581Mail", "er"]], "entropy": [[5.5094475746154785, 1.9374535083770752, 0.21406497061252594]], "avg_entropy": [2.553655351201693], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Maileresque", "score_of_biggest_cluster": 0.47880223393440247, "score_of_first_answer": 0.47880223393440247, "max_score_of_first_answer": 0.976049542427063, "min_score_of_first_answer": 0.14465220272541046, "avg_score": 0.47880223393440247, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3545098323670228, "std_of_first_answer": 0.3545098323670228, "avg_entropy_of_first_answer": 2.553655351201693, "lex_sim": 1.0}
{"question": "The laburnum tree has what colour flowers?", "answer": ["Yellow"], "scores": [[0.21609099209308624, 0.89620041847229]], "normalized_score": [0.4400691092014313], "tokens": [["\u2581Y", "ellow"]], "entropy": [[4.7772746086120605, 0.7850207686424255]], "avg_entropy": [2.781147688627243], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yellowest", "score_of_biggest_cluster": 0.4400691092014313, "score_of_first_answer": 0.4400691092014313, "max_score_of_first_answer": 0.89620041847229, "min_score_of_first_answer": 0.21609099209308624, "avg_score": 0.4400691092014313, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3400547131896019, "std_of_first_answer": 0.3400547131896019, "avg_entropy_of_first_answer": 2.781147688627243, "lex_sim": 1.0}
{"question": "What was the first song played on MTV on 1 August 1981?", "answer": ["Video Killed the Radio Star"], "scores": [[0.22907623648643494, 0.9626022577285767, 0.9999363422393799, 0.8646796941757202, 0.9879825115203857, 0.9789491295814514]], "normalized_score": [0.7544445395469666], "tokens": [["\u2581Video", "\u2581Kil", "led", "\u2581the", "\u2581Radio", "\u2581Star"]], "entropy": [[4.807387351989746, 0.252644419670105, 0.0009905511979013681, 0.447540283203125, 0.10668110102415085, 0.1693410873413086]], "avg_entropy": [0.9640974657377228], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "\"\"\"Video Killed The Radio Star\"\" by the Buggles\"", "score_of_biggest_cluster": 0.7544445395469666, "score_of_first_answer": 0.7544445395469666, "max_score_of_first_answer": 0.9999363422393799, "min_score_of_first_answer": 0.22907623648643494, "avg_score": 0.7544445395469666, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.27556205292520014, "std_of_first_answer": 0.27556205292520014, "avg_entropy_of_first_answer": 0.9640974657377228, "lex_sim": 1.0}
{"question": "Which British city has the most canals ?", "answer": ["Narrowboats"], "scores": [[0.005721338093280792, 0.2005433291196823, 0.49766242504119873, 0.27439627051353455]], "normalized_score": [0.11188056319952011], "tokens": [["\u2581N", "arrow", "bo", "ats"]], "entropy": [[4.513398170471191, 5.07197904586792, 2.8894925117492676, 0.8438700437545776]], "avg_entropy": [3.329684942960739], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "County Borough of Birmingham", "score_of_biggest_cluster": 0.11188056319952011, "score_of_first_answer": 0.11188056319952011, "max_score_of_first_answer": 0.49766242504119873, "min_score_of_first_answer": 0.005721338093280792, "avg_score": 0.11188056319952011, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1760201782462262, "std_of_first_answer": 0.1760201782462262, "avg_entropy_of_first_answer": 3.329684942960739, "lex_sim": 1.0}
{"question": "How is the prince described in the title of the 1911 ballet music composed by Bela Bartok?", "answer": ["Bluebeard's Castle"], "scores": [[0.10990853607654572, 0.3535715937614441, 0.9844735860824585, 0.32339078187942505, 0.9988217949867249, 0.6778004765510559]], "normalized_score": [0.4506490230560303], "tokens": [["\u2581Blue", "be", "ard", "'", "s", "\u2581Castle"]], "entropy": [[5.084920883178711, 2.9055116176605225, 0.1136048212647438, 1.6182937622070312, 0.014591297134757042, 1.9307156801223755]], "avg_entropy": [1.94460634359469], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Diffuse-porous wood", "score_of_biggest_cluster": 0.4506490230560303, "score_of_first_answer": 0.4506490230560303, "max_score_of_first_answer": 0.9988217949867249, "min_score_of_first_answer": 0.10990853607654572, "avg_score": 0.4506490230560303, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3382574434082937, "std_of_first_answer": 0.3382574434082937, "avg_entropy_of_first_answer": 1.94460634359469, "lex_sim": 1.0}
{"question": "Which volcano in Tanzania is the highest mountain in Africa?", "answer": ["Kibo"], "scores": [[0.05459478124976158, 0.4112148582935333, 0.8916491866111755]], "normalized_score": [0.27152177691459656], "tokens": [["\u2581K", "ib", "o"]], "entropy": [[4.746044158935547, 3.388460636138916, 0.9279014468193054]], "avg_entropy": [3.020802080631256], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mawensi", "score_of_biggest_cluster": 0.27152177691459656, "score_of_first_answer": 0.27152177691459656, "max_score_of_first_answer": 0.8916491866111755, "min_score_of_first_answer": 0.05459478124976158, "avg_score": 0.27152177691459656, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3429698886542666, "std_of_first_answer": 0.3429698886542666, "avg_entropy_of_first_answer": 3.020802080631256, "lex_sim": 1.0}
{"question": "The North Sea and the Baltic Sea are linked through which canal?", "answer": ["The North Sea and the Baltic Sea"], "scores": [[0.10450015962123871, 0.019363315775990486, 0.6230849623680115, 0.29417097568511963, 0.9459773302078247, 0.9710091352462769, 0.9996073842048645, 0.969918966293335]], "normalized_score": [0.3671673536300659], "tokens": [["\u2581The", "\u2581North", "\u2581Sea", "\u2581and", "\u2581the", "\u2581Balt", "ic", "\u2581Sea"]], "entropy": [[4.992815971374512, 4.967345237731934, 2.3384060859680176, 2.494825839996338, 0.32156628370285034, 0.2719613015651703, 0.0049284473061561584, 0.22636517882347107]], "avg_entropy": [1.952276793308556], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kiel Canal", "score_of_biggest_cluster": 0.3671673536300659, "score_of_first_answer": 0.3671673536300659, "max_score_of_first_answer": 0.9996073842048645, "min_score_of_first_answer": 0.019363315775990486, "avg_score": 0.3671673536300659, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3918802918021771, "std_of_first_answer": 0.3918802918021771, "avg_entropy_of_first_answer": 1.952276793308556, "lex_sim": 1.0}
{"question": "Poopdeck Pappy is which fictional character\u2019s father?", "answer": ["Popeye the Sailor"], "scores": [[0.7222745418548584, 0.9951157569885254, 0.24075737595558167, 0.9338524341583252, 0.9996281862258911, 0.9030237197875977]], "normalized_score": [0.7255408763885498], "tokens": [["\u2581Pope", "ye", "\u2581the", "\u2581S", "ail", "or"]], "entropy": [[2.0095303058624268, 0.048959098756313324, 1.6057488918304443, 0.37059321999549866, 0.005010136403143406, 0.3330938518047333]], "avg_entropy": [0.72882258410876], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Popeye Cigarettes", "score_of_biggest_cluster": 0.7255408763885498, "score_of_first_answer": 0.7255408763885498, "max_score_of_first_answer": 0.9996281862258911, "min_score_of_first_answer": 0.24075737595558167, "avg_score": 0.7255408763885498, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2662206869303506, "std_of_first_answer": 0.2662206869303506, "avg_entropy_of_first_answer": 0.72882258410876, "lex_sim": 1.0}
{"question": "Which country declared a state of emergency on 8 November 2005 as a result of prolonged rioting, allegedly provoked by the death of 2 teenagers?", "answer": ["Ongerous"], "scores": [[0.006465760059654713, 0.15037015080451965, 0.01601794920861721, 0.06797286868095398]], "normalized_score": [0.03207606077194214], "tokens": [["\u2581O", "ng", "er", "ous"]], "entropy": [[4.839513778686523, 4.758424758911133, 2.9197895526885986, 3.7604055404663086]], "avg_entropy": [4.069533407688141], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "La Republique francaise", "score_of_biggest_cluster": 0.03207606077194214, "score_of_first_answer": 0.03207606077194214, "max_score_of_first_answer": 0.15037015080451965, "min_score_of_first_answer": 0.006465760059654713, "avg_score": 0.03207606077194214, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05707559802387867, "std_of_first_answer": 0.05707559802387867, "avg_entropy_of_first_answer": 4.069533407688141, "lex_sim": 1.0}
{"question": "What is the national anthem of the USA called?", "answer": ["The Defender of the People!"], "scores": [[0.25139978528022766, 0.0020652159582823515, 0.18921975791454315, 0.6214702725410461, 0.37777575850486755, 0.009127974510192871, 0.00256336759775877]], "normalized_score": [0.047426316887140274], "tokens": [["\u2581The", "\u2581Def", "ender", "\u2581of", "\u2581the", "\u2581People", "!"]], "entropy": [[4.427427291870117, 1.798992395401001, 2.279191017150879, 1.4828498363494873, 3.364464521408081, 5.165392875671387, 1.411673903465271]], "avg_entropy": [2.8471416916166032], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Star Spangled Banner (Beyonc\u00e9 Knowles song)", "score_of_biggest_cluster": 0.047426316887140274, "score_of_first_answer": 0.047426316887140274, "max_score_of_first_answer": 0.6214702725410461, "min_score_of_first_answer": 0.0020652159582823515, "avg_score": 0.047426316887140274, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21586382390262632, "std_of_first_answer": 0.21586382390262632, "avg_entropy_of_first_answer": 2.8471416916166032, "lex_sim": 1.0}
{"question": "Dilma Rousseff was elected the first female president of which country in October 2010?", "answer": ["Argentina"], "scores": [[0.010501181706786156]], "normalized_score": [0.010501178912818432], "tokens": [["\u2581Argentina"]], "entropy": [[4.8502044677734375]], "avg_entropy": [4.8502044677734375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brazil", "score_of_biggest_cluster": 0.010501178912818432, "score_of_first_answer": 0.010501178912818432, "max_score_of_first_answer": 0.010501181706786156, "min_score_of_first_answer": 0.010501181706786156, "avg_score": 0.010501178912818432, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.8502044677734375, "lex_sim": 1.0}
{"question": "In January 2010, who became the first mother to win the Australian Open Tennis Championships since Evonne Goolagong in the 1970's?", "answer": ["Caroline, or change"], "scores": [[0.007750913966447115, 0.002456762595102191, 0.007894428446888924, 0.2030346840620041]], "normalized_score": [0.013217572122812271], "tokens": [["\u2581Caroline", ",", "\u2581or", "\u2581change"]], "entropy": [[5.495281219482422, 2.358590841293335, 5.831020355224609, 3.542684555053711]], "avg_entropy": [4.306894242763519], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Clijsters, Kim", "score_of_biggest_cluster": 0.013217572122812271, "score_of_first_answer": 0.013217572122812271, "max_score_of_first_answer": 0.2030346840620041, "min_score_of_first_answer": 0.002456762595102191, "avg_score": 0.013217572122812271, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08533192165351794, "std_of_first_answer": 0.08533192165351794, "avg_entropy_of_first_answer": 4.306894242763519, "lex_sim": 1.0}
{"question": "Which county won the 2013 Cricket County Championship?", "answer": ["Notts"], "scores": [[0.130464106798172, 0.0772971361875534]], "normalized_score": [0.10042162239551544], "tokens": [["\u2581Not", "ts"]], "entropy": [[4.81688928604126, 0.8248893618583679]], "avg_entropy": [2.820889323949814], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Durham (disambiguation)", "score_of_biggest_cluster": 0.10042162239551544, "score_of_first_answer": 0.10042162239551544, "max_score_of_first_answer": 0.130464106798172, "min_score_of_first_answer": 0.0772971361875534, "avg_score": 0.10042162239551544, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.026583485305309296, "std_of_first_answer": 0.026583485305309296, "avg_entropy_of_first_answer": 2.820889323949814, "lex_sim": 1.0}
{"question": "In which children's TV series does Shaggy appear?", "answer": ["Scooby-Doo!"], "scores": [[0.47003546357154846, 0.9831288456916809, 0.9567927718162537, 0.5824800729751587, 0.957748293876648, 0.9992615580558777, 0.15364567935466766, 0.048714540898799896, 0.7265905141830444, 0.9974019527435303, 0.5181597471237183]], "normalized_score": [0.5161540508270264], "tokens": [["\u2581S", "co", "oby", "-", "D", "oo", "!", "\u2581Where", "\u2581Are", "\u2581You", "?"]], "entropy": [[3.87355637550354, 0.1551097333431244, 0.27899616956710815, 1.5127637386322021, 0.23624373972415924, 0.0085335373878479, 1.9052443504333496, 1.8569416999816895, 0.6712054014205933, 0.02568209543824196, 0.975529670715332]], "avg_entropy": [1.0454369556497445], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pterodactyl Ghost", "score_of_biggest_cluster": 0.5161540508270264, "score_of_first_answer": 0.5161540508270264, "max_score_of_first_answer": 0.9992615580558777, "min_score_of_first_answer": 0.048714540898799896, "avg_score": 0.5161540508270264, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33100024476219275, "std_of_first_answer": 0.33100024476219275, "avg_entropy_of_first_answer": 1.0454369556497445, "lex_sim": 1.0}
{"question": "Belonging to Indonesia, Irian Jaya is the western portion of which large island?", "answer": ["New Guinea"], "scores": [[0.42124947905540466, 0.9116981625556946]], "normalized_score": [0.6197195649147034], "tokens": [["\u2581New", "\u2581Guinea"]], "entropy": [[3.824733018875122, 0.6348648071289062]], "avg_entropy": [2.229798913002014], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "The Island Of New Guinea", "score_of_biggest_cluster": 0.6197195649147034, "score_of_first_answer": 0.6197195649147034, "max_score_of_first_answer": 0.9116981625556946, "min_score_of_first_answer": 0.42124947905540466, "avg_score": 0.6197195649147034, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.24522434175014496, "std_of_first_answer": 0.24522434175014496, "avg_entropy_of_first_answer": 2.229798913002014, "lex_sim": 1.0}
{"question": "Armistice Day (or Remembrance Day) takes place during which month of the year?", "answer": ["Racer"], "scores": [[0.0030920894350856543, 0.0024514049291610718, 0.05245795473456383]], "normalized_score": [0.007353477645665407], "tokens": [["\u2581R", "ac", "er"]], "entropy": [[4.309144020080566, 5.132155418395996, 3.372394561767578]], "avg_entropy": [4.271231333414714], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The month of November", "score_of_biggest_cluster": 0.007353477645665407, "score_of_first_answer": 0.007353477645665407, "max_score_of_first_answer": 0.05245795473456383, "min_score_of_first_answer": 0.0024514049291610718, "avg_score": 0.007353477645665407, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.023423763235091412, "std_of_first_answer": 0.023423763235091412, "avg_entropy_of_first_answer": 4.271231333414714, "lex_sim": 1.0}
{"question": "In 1902 which food company started production in a disused malthouse in Burton-on-Trent?", "answer": ["Marmite"], "scores": [[0.21765348315238953, 0.9668527841567993, 0.987105131149292]], "normalized_score": [0.5922383069992065], "tokens": [["\u2581Mar", "m", "ite"]], "entropy": [[5.156278610229492, 0.2817014753818512, 0.13644692301750183]], "avg_entropy": [1.858142336209615], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "MARMITE", "score_of_biggest_cluster": 0.5922383069992065, "score_of_first_answer": 0.5922383069992065, "max_score_of_first_answer": 0.987105131149292, "min_score_of_first_answer": 0.21765348315238953, "avg_score": 0.5922383069992065, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.35804493647047986, "std_of_first_answer": 0.35804493647047986, "avg_entropy_of_first_answer": 1.858142336209615, "lex_sim": 1.0}
{"question": "Thomas the Tank Engine pulled two coaches. One was Clarabel. What was the other?", "answer": ["Henry"], "scores": [[0.013301476836204529]], "normalized_score": [0.013301479630172253], "tokens": [["\u2581Henry"]], "entropy": [[4.39499568939209]], "avg_entropy": [4.39499568939209], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Annie (film)", "score_of_biggest_cluster": 0.013301479630172253, "score_of_first_answer": 0.013301479630172253, "max_score_of_first_answer": 0.013301476836204529, "min_score_of_first_answer": 0.013301476836204529, "avg_score": 0.013301479630172253, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.39499568939209, "lex_sim": 1.0}
{"question": "What is the title of the 18th Century novel by Lawrence Sterne, which was published in nine volumes?", "answer": ["A Series of Unfortunate Events (2004 film)"], "scores": [[0.11321359872817993, 0.00416093273088336, 0.9741265773773193, 0.4640698730945587, 0.9721729159355164, 0.9987657070159912, 0.9495054483413696, 0.22960041463375092, 0.011477361433207989, 0.9924098253250122, 0.9663546681404114, 0.9362284541130066, 0.771324634552002, 0.9843059778213501]], "normalized_score": [0.3460181951522827], "tokens": [["\u2581A", "\u2581Series", "\u2581of", "\u2581Un", "fortun", "ate", "\u2581Events", "\u2581(", "2", "0", "0", "4", "\u2581film", ")"]], "entropy": [[4.824897766113281, 3.9608616828918457, 0.1584669053554535, 2.6614787578582764, 0.21521466970443726, 0.013639085926115513, 0.47027790546417236, 1.3721781969070435, 1.33811354637146, 0.06573653966188431, 0.16148854792118073, 0.3493269085884094, 1.061722755432129, 0.13422438502311707]], "avg_entropy": [1.1991162609442003], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Shandean", "score_of_biggest_cluster": 0.3460181951522827, "score_of_first_answer": 0.3460181951522827, "max_score_of_first_answer": 0.9987657070159912, "min_score_of_first_answer": 0.00416093273088336, "avg_score": 0.3460181951522827, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3932530442845058, "std_of_first_answer": 0.3932530442845058, "avg_entropy_of_first_answer": 1.1991162609442003, "lex_sim": 1.0}
{"question": "How did US president William McKinley die in 1901?", "answer": ["\u32d3"], "scores": [[0.03196025639772415, 0.24390263855457306, 0.4147077798843384, 0.01722388155758381]], "normalized_score": [0.08638234436511993], "tokens": [["\u2581", "<0xE3>", "<0x8B>", "<0x93>"]], "entropy": [[4.538172721862793, 3.8767647743225098, 2.456110954284668, 3.5132765769958496]], "avg_entropy": [3.596081256866455], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Assination", "score_of_biggest_cluster": 0.08638234436511993, "score_of_first_answer": 0.08638234436511993, "max_score_of_first_answer": 0.4147077798843384, "min_score_of_first_answer": 0.01722388155758381, "avg_score": 0.08638234436511993, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1639709397729328, "std_of_first_answer": 0.1639709397729328, "avg_entropy_of_first_answer": 3.596081256866455, "lex_sim": 0.0}
{"question": "In the cartoon series, what kind of dog is Scooby Doo?", "answer": ["A beagle"], "scores": [[0.041915230453014374, 0.015923645347356796, 0.9423838257789612]], "normalized_score": [0.08568023890256882], "tokens": [["\u2581A", "\u2581be", "agle"]], "entropy": [[4.291043758392334, 4.187198638916016, 0.37262359261512756]], "avg_entropy": [2.9502886633078256], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Great daine", "score_of_biggest_cluster": 0.08568023890256882, "score_of_first_answer": 0.08568023890256882, "max_score_of_first_answer": 0.9423838257789612, "min_score_of_first_answer": 0.015923645347356796, "avg_score": 0.08568023890256882, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4307419593097408, "std_of_first_answer": 0.4307419593097408, "avg_entropy_of_first_answer": 2.9502886633078256, "lex_sim": 1.0}
{"question": "\u2018The Road Back\u2019 is the sequel to which novel by Erich Maria Remarque published in 1929?", "answer": ["Arch of Triumph"], "scores": [[0.0076501211151480675, 0.8667000532150269, 0.9538770914077759, 0.9917902946472168]], "normalized_score": [0.2814248204231262], "tokens": [["\u2581Arch", "\u2581of", "\u2581Tri", "umph"]], "entropy": [[4.812501907348633, 0.946233868598938, 0.33961135149002075, 0.05961251258850098]], "avg_entropy": [1.5394899100065231], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "All quiet", "score_of_biggest_cluster": 0.2814248204231262, "score_of_first_answer": 0.2814248204231262, "max_score_of_first_answer": 0.9917902946472168, "min_score_of_first_answer": 0.0076501211151480675, "avg_score": 0.2814248204231262, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4051642384406601, "std_of_first_answer": 0.4051642384406601, "avg_entropy_of_first_answer": 1.5394899100065231, "lex_sim": 1.0}
{"question": "Released in 1961, what was the last No.1 hit for Frankie Vaughan?", "answer": ["Tower of Strength"], "scores": [[0.22889989614486694, 0.845548689365387, 0.7716187834739685, 0.9146161675453186]], "normalized_score": [0.6079340577125549], "tokens": [["\u2581Tower", "\u2581of", "\u2581Str", "ength"]], "entropy": [[4.477026462554932, 0.8857013583183289, 0.8866773247718811, 0.38321733474731445]], "avg_entropy": [1.658155620098114], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tower of Strength (disambiguation)", "score_of_biggest_cluster": 0.6079340577125549, "score_of_first_answer": 0.6079340577125549, "max_score_of_first_answer": 0.9146161675453186, "min_score_of_first_answer": 0.22889989614486694, "avg_score": 0.6079340577125549, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27107315999166837, "std_of_first_answer": 0.27107315999166837, "avg_entropy_of_first_answer": 1.658155620098114, "lex_sim": 1.0}
{"question": "American jazz musician Art Tatum excelled on which instrument?", "answer": ["Mussorgsky"], "scores": [[0.003104014554992318, 0.005874614231288433, 0.2575601041316986, 0.9246516227722168]], "normalized_score": [0.045649901032447815], "tokens": [["\u2581M", "uss", "org", "sky"]], "entropy": [[4.9021501541137695, 5.9559173583984375, 2.9806032180786133, 0.4947066605091095]], "avg_entropy": [3.5833443477749825], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pianos", "score_of_biggest_cluster": 0.045649901032447815, "score_of_first_answer": 0.045649901032447815, "max_score_of_first_answer": 0.9246516227722168, "min_score_of_first_answer": 0.003104014554992318, "avg_score": 0.045649901032447815, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37637360456799285, "std_of_first_answer": 0.37637360456799285, "avg_entropy_of_first_answer": 3.5833443477749825, "lex_sim": 1.0}
{"question": "Whose hit 'I Will Survive' became an anthem for Women's' Lib?", "answer": ["Gloria Gaynor"], "scores": [[0.5732850432395935, 0.990546703338623, 0.6720080971717834, 0.9693068265914917]], "normalized_score": [0.779866635799408], "tokens": [["\u2581Gl", "oria", "\u2581Gay", "nor"]], "entropy": [[3.2851037979125977, 0.08982321619987488, 1.9715310335159302, 0.22751852869987488]], "avg_entropy": [1.3934941440820694], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "I Will Survive (album)", "score_of_biggest_cluster": 0.779866635799408, "score_of_first_answer": 0.779866635799408, "max_score_of_first_answer": 0.990546703338623, "min_score_of_first_answer": 0.5732850432395935, "avg_score": 0.779866635799408, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18217287424192596, "std_of_first_answer": 0.18217287424192596, "avg_entropy_of_first_answer": 1.3934941440820694, "lex_sim": 1.0}
{"question": "In what country is 'Mt. Everest'?", "answer": ["Nepal"], "scores": [[0.3869442343711853, 0.9709116816520691]], "normalized_score": [0.6129344701766968], "tokens": [["\u2581Nep", "al"]], "entropy": [[3.9799418449401855, 0.21710613369941711]], "avg_entropy": [2.0985239893198013], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nepol", "score_of_biggest_cluster": 0.6129344701766968, "score_of_first_answer": 0.6129344701766968, "max_score_of_first_answer": 0.9709116816520691, "min_score_of_first_answer": 0.3869442343711853, "avg_score": 0.6129344701766968, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2919837236404419, "std_of_first_answer": 0.2919837236404419, "avg_entropy_of_first_answer": 2.0985239893198013, "lex_sim": 1.0}
{"question": "What is the medical name for the breastbone?", "answer": ["Sternum"], "scores": [[0.25706449151039124, 0.9667666554450989]], "normalized_score": [0.4985192120075226], "tokens": [["\u2581Stern", "um"]], "entropy": [[5.048976421356201, 0.3230658173561096]], "avg_entropy": [2.6860211193561554], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "STERNUM", "score_of_biggest_cluster": 0.4985192120075226, "score_of_first_answer": 0.4985192120075226, "max_score_of_first_answer": 0.9667666554450989, "min_score_of_first_answer": 0.25706449151039124, "avg_score": 0.4985192120075226, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3548510819673538, "std_of_first_answer": 0.3548510819673538, "avg_entropy_of_first_answer": 2.6860211193561554, "lex_sim": 1.0}
{"question": "The International Bureau of Weights and Measures is based in which European country?", "answer": ["French Republic"], "scores": [[0.013488485477864742, 0.1338159441947937]], "normalized_score": [0.04248499125242233], "tokens": [["\u2581French", "\u2581Republic"]], "entropy": [[5.299043655395508, 4.024292945861816]], "avg_entropy": [4.661668300628662], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "La Republique francaise", "score_of_biggest_cluster": 0.04248499125242233, "score_of_first_answer": 0.04248499125242233, "max_score_of_first_answer": 0.1338159441947937, "min_score_of_first_answer": 0.013488485477864742, "avg_score": 0.04248499125242233, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06016372935846448, "std_of_first_answer": 0.06016372935846448, "avg_entropy_of_first_answer": 4.661668300628662, "lex_sim": 1.0}
{"question": "Proposed in 1776, 'E pluribus unum' ('One from many'), is on the national seal and banknotes of which nation?", "answer": ["United States of America"], "scores": [[0.14571978151798248, 0.908004641532898, 0.5354867577552795, 0.9764811992645264]], "normalized_score": [0.5128670334815979], "tokens": [["\u2581United", "\u2581States", "\u2581of", "\u2581America"]], "entropy": [[4.219398498535156, 0.5696375370025635, 1.4465711116790771, 0.2231045365333557]], "avg_entropy": [1.6146779209375381], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "The United States of America", "score_of_biggest_cluster": 0.5128670334815979, "score_of_first_answer": 0.5128670334815979, "max_score_of_first_answer": 0.9764811992645264, "min_score_of_first_answer": 0.14571978151798248, "avg_score": 0.5128670334815979, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.33176570676122036, "std_of_first_answer": 0.33176570676122036, "avg_entropy_of_first_answer": 1.6146779209375381, "lex_sim": 1.0}
{"question": "The 1951 \u2018Treaty of Peace With Japan\u2019 which served to officially end World War II, is commonly known as what?", "answer": ["Treaty of San Francisco"], "scores": [[0.1155502051115036, 0.9796133041381836, 0.9131224155426025, 0.7365819811820984, 0.9745711088180542]], "normalized_score": [0.5943986177444458], "tokens": [["\u2581Tre", "aty", "\u2581of", "\u2581San", "\u2581Francisco"]], "entropy": [[3.754021167755127, 0.1314661204814911, 0.6572102308273315, 1.4803736209869385, 0.2082277089357376]], "avg_entropy": [1.2462597697973252], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "San Francisco Peace Conference", "score_of_biggest_cluster": 0.5943986177444458, "score_of_first_answer": 0.5943986177444458, "max_score_of_first_answer": 0.9796133041381836, "min_score_of_first_answer": 0.1155502051115036, "avg_score": 0.5943986177444458, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3262769270129775, "std_of_first_answer": 0.3262769270129775, "avg_entropy_of_first_answer": 1.2462597697973252, "lex_sim": 1.0}
{"question": "What city had the largest library in the ancient world?", "answer": ["Alexandra in the ancient world"], "scores": [[0.015669256448745728, 0.49935996532440186, 0.011527417227625847, 0.03206007555127144, 0.07170818001031876, 0.8711665272712708]], "normalized_score": [0.07518619298934937], "tokens": [["\u2581Alex", "andra", "\u2581in", "\u2581the", "\u2581ancient", "\u2581world"]], "entropy": [[4.325799942016602, 3.0212161540985107, 1.5405640602111816, 1.2179008722305298, 4.8703742027282715, 0.8773847818374634]], "avg_entropy": [2.6422066688537598], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "\u1f08\u03bb\u03b5\u03be\u03ac\u03bd\u03b4\u03c1\u03b5\u03b9\u03b1", "score_of_biggest_cluster": 0.07518619298934937, "score_of_first_answer": 0.07518619298934937, "max_score_of_first_answer": 0.8711665272712708, "min_score_of_first_answer": 0.011527417227625847, "avg_score": 0.07518619298934937, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.32636731097466887, "std_of_first_answer": 0.32636731097466887, "avg_entropy_of_first_answer": 2.6422066688537598, "lex_sim": 1.0}
{"question": "What is the old name (Latin, meaning 'strong water') for a solution of nitric acid in water, used in alchemy and other ancient processes?", "answer": ["Aqua fortis"], "scores": [[0.7626999616622925, 0.984457790851593, 0.5447086095809937, 0.9868049621582031]], "normalized_score": [0.7970519065856934], "tokens": [["\u2581Aqu", "a", "\u2581fort", "is"]], "entropy": [[1.318234920501709, 0.13823968172073364, 1.5852863788604736, 0.11096007376909256]], "avg_entropy": [0.7881802637130022], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Aqua Fortis", "score_of_biggest_cluster": 0.7970519065856934, "score_of_first_answer": 0.7970519065856934, "max_score_of_first_answer": 0.9868049621582031, "min_score_of_first_answer": 0.5447086095809937, "avg_score": 0.7970519065856934, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.18298802082370605, "std_of_first_answer": 0.18298802082370605, "avg_entropy_of_first_answer": 0.7881802637130022, "lex_sim": 1.0}
{"question": "What is the largest country with a single time zone?", "answer": ["Chinese Democratic Republic"], "scores": [[0.003337665693834424, 0.002718773204833269, 0.8511490225791931]], "normalized_score": [0.01976698637008667], "tokens": [["\u2581Chinese", "\u2581Democratic", "\u2581Republic"]], "entropy": [[5.497868537902832, 4.462239742279053, 1.0863516330718994]], "avg_entropy": [3.682153304417928], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chinese People's Republic", "score_of_biggest_cluster": 0.01976698637008667, "score_of_first_answer": 0.01976698637008667, "max_score_of_first_answer": 0.8511490225791931, "min_score_of_first_answer": 0.002718773204833269, "avg_score": 0.01976698637008667, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3998080606081351, "std_of_first_answer": 0.3998080606081351, "avg_entropy_of_first_answer": 3.682153304417928, "lex_sim": 1.0}
{"question": "Late British singer-songwriter John Simon Ritchie was better known by what name?", "answer": ["Sid Vicious"], "scores": [[0.5342727303504944, 0.7046540975570679, 0.9775945544242859]], "normalized_score": [0.7166370749473572], "tokens": [["\u2581Sid", "\u2581V", "icious"]], "entropy": [[3.3566112518310547, 2.081364154815674, 0.2315354198217392]], "avg_entropy": [1.8898369421561558], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Simon Ritchie-Beverly", "score_of_biggest_cluster": 0.7166370749473572, "score_of_first_answer": 0.7166370749473572, "max_score_of_first_answer": 0.9775945544242859, "min_score_of_first_answer": 0.5342727303504944, "avg_score": 0.7166370749473572, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1825926075496975, "std_of_first_answer": 0.1825926075496975, "avg_entropy_of_first_answer": 1.8898369421561558, "lex_sim": 1.0}
{"question": "Otology is a branch of medicine concerned with which part of the body?", "answer": ["\u1ea8n l\u01b0u"], "scores": [[0.035628799349069595, 0.020357536152005196, 0.015438579954206944, 0.01545892283320427, 0.07640015333890915, 0.0028539448976516724, 0.07990618050098419, 0.7429205179214478]], "normalized_score": [0.034978169947862625], "tokens": [["\u2581", "<0xE1>", "<0xBA>", "<0xA8>", "n", "\u2581l", "\u01b0", "u"]], "entropy": [[4.008899688720703, 4.756924629211426, 3.7465412616729736, 3.8163743019104004, 3.2986433506011963, 4.800930023193359, 4.245488166809082, 1.3934526443481445]], "avg_entropy": [3.7584067583084106], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\ud83d\udc42", "score_of_biggest_cluster": 0.034978169947862625, "score_of_first_answer": 0.034978169947862625, "max_score_of_first_answer": 0.7429205179214478, "min_score_of_first_answer": 0.0028539448976516724, "avg_score": 0.034978169947862625, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.235601838273526, "std_of_first_answer": 0.235601838273526, "avg_entropy_of_first_answer": 3.7584067583084106, "lex_sim": 1.0}
{"question": "Which type of animal represents the Republican Party?", "answer": ["Eagle"], "scores": [[0.038486331701278687, 0.837579071521759]], "normalized_score": [0.17954204976558685], "tokens": [["\u2581E", "agle"]], "entropy": [[4.225156784057617, 1.2550039291381836]], "avg_entropy": [2.7400803565979004], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Elephantinely", "score_of_biggest_cluster": 0.17954204976558685, "score_of_first_answer": 0.17954204976558685, "max_score_of_first_answer": 0.837579071521759, "min_score_of_first_answer": 0.038486331701278687, "avg_score": 0.17954204976558685, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3995463699102402, "std_of_first_answer": 0.3995463699102402, "avg_entropy_of_first_answer": 2.7400803565979004, "lex_sim": 1.0}
{"question": "In the tv sitcom \u2018Allo Allo!\u2019, what was Rene\u2019s surname?", "answer": ["Rene\u2019s"], "scores": [[0.06620744615793228, 0.8656278252601624, 0.03845937177538872, 0.854562520980835]], "normalized_score": [0.20832721889019012], "tokens": [["\u2581R", "ene", "\u2019", "s"]], "entropy": [[5.374628067016602, 1.1351261138916016, 4.369008541107178, 0.9029237031936646]], "avg_entropy": [2.9454216063022614], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Artois", "score_of_biggest_cluster": 0.20832721889019012, "score_of_first_answer": 0.20832721889019012, "max_score_of_first_answer": 0.8656278252601624, "min_score_of_first_answer": 0.03845937177538872, "avg_score": 0.20832721889019012, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4040189555962342, "std_of_first_answer": 0.4040189555962342, "avg_entropy_of_first_answer": 2.9454216063022614, "lex_sim": 1.0}
{"question": "What colour ink was traditionally used by Roman emperors to sign important documents?", "answer": ["Indigo"], "scores": [[0.003638935973867774, 0.7205415368080139]], "normalized_score": [0.05120551213622093], "tokens": [["\u2581Ind", "igo"]], "entropy": [[4.641812324523926, 1.6720342636108398]], "avg_entropy": [3.156923294067383], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Purplingly", "score_of_biggest_cluster": 0.05120551213622093, "score_of_first_answer": 0.05120551213622093, "max_score_of_first_answer": 0.7205415368080139, "min_score_of_first_answer": 0.003638935973867774, "avg_score": 0.05120551213622093, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35845130041707307, "std_of_first_answer": 0.35845130041707307, "avg_entropy_of_first_answer": 3.156923294067383, "lex_sim": 1.0}
{"question": "Which musical featured the song The Street Where You Live?", "answer": ["My Fair Lady"], "scores": [[0.6201146841049194, 0.9461575150489807, 0.9828536510467529]], "normalized_score": [0.8323540687561035], "tokens": [["\u2581My", "\u2581Fair", "\u2581Lady"]], "entropy": [[3.088245391845703, 0.39519524574279785, 0.15200597047805786]], "avg_entropy": [1.2118155360221863], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "My Fair Lady (2010 film)", "score_of_biggest_cluster": 0.8323540687561035, "score_of_first_answer": 0.8323540687561035, "max_score_of_first_answer": 0.9828536510467529, "min_score_of_first_answer": 0.6201146841049194, "avg_score": 0.8323540687561035, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.16303717781373714, "std_of_first_answer": 0.16303717781373714, "avg_entropy_of_first_answer": 1.2118155360221863, "lex_sim": 1.0}
{"question": "Which part of the body is affected by Emphysema?", "answer": ["Emphysema"], "scores": [[0.01658828929066658, 0.8070384860038757, 0.7554988861083984]], "normalized_score": [0.21626019477844238], "tokens": [["\u2581Em", "phys", "ema"]], "entropy": [[4.109652519226074, 1.277245044708252, 1.1676487922668457]], "avg_entropy": [2.1848487854003906], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pulmonary surgical procedures", "score_of_biggest_cluster": 0.21626019477844238, "score_of_first_answer": 0.21626019477844238, "max_score_of_first_answer": 0.8070384860038757, "min_score_of_first_answer": 0.01658828929066658, "avg_score": 0.21626019477844238, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36108735691748145, "std_of_first_answer": 0.36108735691748145, "avg_entropy_of_first_answer": 2.1848487854003906, "lex_sim": 1.0}
{"question": "The Cascades were one hit wonders in the U.K. what was the name of their one and only chart entry", "answer": ["\u34fb"], "scores": [[0.015268741175532341, 0.15257792174816132, 0.005579949356615543, 0.012864035554230213]], "normalized_score": [0.020222075283527374], "tokens": [["\u2581", "<0xE3>", "<0x93>", "<0xBB>"]], "entropy": [[4.4803667068481445, 4.078689098358154, 2.498058795928955, 4.004415512084961]], "avg_entropy": [3.7653825283050537], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rhythm of the falling rain", "score_of_biggest_cluster": 0.020222075283527374, "score_of_first_answer": 0.020222075283527374, "max_score_of_first_answer": 0.15257792174816132, "min_score_of_first_answer": 0.005579949356615543, "avg_score": 0.020222075283527374, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.061306044848740714, "std_of_first_answer": 0.061306044848740714, "avg_entropy_of_first_answer": 3.7653825283050537, "lex_sim": 0.0}
{"question": "Swede Gunder Hagg held the athletics world record for which event until the 6th May 1954?", "answer": ["10000m Run Time"], "scores": [[0.2582375705242157, 0.4932113587856293, 0.5273233652114868, 0.7184142470359802, 0.09850065410137177, 0.8702044486999512, 0.21647578477859497, 0.010002782568335533, 0.0041951145976781845]], "normalized_score": [0.14961738884449005], "tokens": [["\u2581", "1", "0", "0", "0", "0", "m", "\u2581Run", "\u2581Time"]], "entropy": [[4.329275608062744, 2.069044589996338, 1.8790497779846191, 1.1477516889572144, 2.4905738830566406, 0.776149332523346, 2.3601832389831543, 2.373323917388916, 1.4666728973388672]], "avg_entropy": [2.099113881587982], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mile (Scottish)", "score_of_biggest_cluster": 0.14961738884449005, "score_of_first_answer": 0.14961738884449005, "max_score_of_first_answer": 0.8702044486999512, "min_score_of_first_answer": 0.0041951145976781845, "avg_score": 0.14961738884449005, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2949075140558553, "std_of_first_answer": 0.2949075140558553, "avg_entropy_of_first_answer": 2.099113881587982, "lex_sim": 1.0}
{"question": "Which chemical element is named after a university in Oakland, California?", "answer": ["CALIFORNIA I"], "scores": [[0.009674540720880032, 0.1543547809123993, 0.2931499481201172, 0.9216868877410889, 0.6387207508087158, 0.8763203620910645, 0.003513924777507782]], "normalized_score": [0.13443514704704285], "tokens": [["\u2581C", "AL", "IF", "OR", "N", "IA", "\u2581I"]], "entropy": [[4.997679233551025, 5.448400497436523, 3.1619625091552734, 0.5128297805786133, 0.7904877066612244, 0.8289008736610413, 2.6332831382751465]], "avg_entropy": [2.6247919627598355], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Berkellium", "score_of_biggest_cluster": 0.13443514704704285, "score_of_first_answer": 0.13443514704704285, "max_score_of_first_answer": 0.9216868877410889, "min_score_of_first_answer": 0.003513924777507782, "avg_score": 0.13443514704704285, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36566646218734133, "std_of_first_answer": 0.36566646218734133, "avg_entropy_of_first_answer": 2.6247919627598355, "lex_sim": 1.0}
{"question": "\"Who plays advertising executive Roger Thornhill in the 1959 film North by Northwest\"\"?\"", "answer": ["Bette"], "scores": [[0.002419437048956752, 0.004308511037379503]], "normalized_score": [0.0032286474015563726], "tokens": [["\u2581B", "ette"]], "entropy": [[3.8568291664123535, 6.3161420822143555]], "avg_entropy": [5.0864856243133545], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Archibald alec leach", "score_of_biggest_cluster": 0.0032286474015563726, "score_of_first_answer": 0.0032286474015563726, "max_score_of_first_answer": 0.004308511037379503, "min_score_of_first_answer": 0.002419437048956752, "avg_score": 0.0032286474015563726, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0009445369942113757, "std_of_first_answer": 0.0009445369942113757, "avg_entropy_of_first_answer": 5.0864856243133545, "lex_sim": 1.0}
{"question": "Name the new French president of 2012?", "answer": ["Fran\u00e7ois Hollande"], "scores": [[0.17842963337898254, 0.7466239333152771, 0.990526556968689, 0.9922614693641663]], "normalized_score": [0.6015410423278809], "tokens": [["\u2581Fran\u00e7ois", "\u2581H", "oll", "ande"]], "entropy": [[4.866649627685547, 1.9153237342834473, 0.11380712687969208, 0.07814758270978928]], "avg_entropy": [1.7434820178896189], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Francois Hollande", "score_of_biggest_cluster": 0.6015410423278809, "score_of_first_answer": 0.6015410423278809, "max_score_of_first_answer": 0.9922614693641663, "min_score_of_first_answer": 0.17842963337898254, "avg_score": 0.6015410423278809, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3320859952547291, "std_of_first_answer": 0.3320859952547291, "avg_entropy_of_first_answer": 1.7434820178896189, "lex_sim": 1.0}
{"question": "What river flows parallel to the Euphrates?", "answer": ["Tigris"], "scores": [[0.20001521706581116, 0.9771130084991455]], "normalized_score": [0.44208309054374695], "tokens": [["\u2581Tig", "ris"]], "entropy": [[5.427769660949707, 0.18136858940124512]], "avg_entropy": [2.804569125175476], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Tigris", "score_of_biggest_cluster": 0.44208309054374695, "score_of_first_answer": 0.44208309054374695, "max_score_of_first_answer": 0.9771130084991455, "min_score_of_first_answer": 0.20001521706581116, "avg_score": 0.44208309054374695, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3885488957166672, "std_of_first_answer": 0.3885488957166672, "avg_entropy_of_first_answer": 2.804569125175476, "lex_sim": 1.0}
{"question": "In which African country did the Mau Mau rebellion break out in October 1952 ?", "answer": ["Kenya"], "scores": [[0.3960616886615753, 0.8547818660736084]], "normalized_score": [0.5818473696708679], "tokens": [["\u2581Ken", "ya"]], "entropy": [[4.327938079833984, 0.7754266262054443]], "avg_entropy": [2.5516823530197144], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kenyan", "score_of_biggest_cluster": 0.5818473696708679, "score_of_first_answer": 0.5818473696708679, "max_score_of_first_answer": 0.8547818660736084, "min_score_of_first_answer": 0.3960616886615753, "avg_score": 0.5818473696708679, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22936008870601654, "std_of_first_answer": 0.22936008870601654, "avg_entropy_of_first_answer": 2.5516823530197144, "lex_sim": 1.0}
{"question": "Which real life astronaut was played by Tom Hanks in the film 'Apollo 13'?", "answer": ["James Lowell"], "scores": [[0.07010304927825928, 0.011268557980656624, 0.8952651023864746]], "normalized_score": [0.08909479528665543], "tokens": [["\u2581James", "\u2581Low", "ell"]], "entropy": [[5.137115955352783, 1.8637824058532715, 0.8213441371917725]], "avg_entropy": [2.607414166132609], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "James Arthur Lovell", "score_of_biggest_cluster": 0.08909479528665543, "score_of_first_answer": 0.08909479528665543, "max_score_of_first_answer": 0.8952651023864746, "min_score_of_first_answer": 0.011268557980656624, "avg_score": 0.08909479528665543, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40356794853702427, "std_of_first_answer": 0.40356794853702427, "avg_entropy_of_first_answer": 2.607414166132609, "lex_sim": 1.0}
{"question": "How many Von Trapp children were there in the Sound of Music?", "answer": ["Six"], "scores": [[0.05237800255417824]], "normalized_score": [0.05237800255417824], "tokens": [["\u2581Six"]], "entropy": [[4.690624237060547]], "avg_entropy": [4.690624237060547], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "seven", "score_of_biggest_cluster": 0.05237800255417824, "score_of_first_answer": 0.05237800255417824, "max_score_of_first_answer": 0.05237800255417824, "min_score_of_first_answer": 0.05237800255417824, "avg_score": 0.05237800255417824, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.690624237060547, "lex_sim": 1.0}
{"question": "Which famous city plaza's name loosely translates as the 'Gate of Heavenly Peace'?", "answer": ["Forbidden City Square"], "scores": [[0.013657626695930958, 0.9903913736343384, 0.9931365251541138, 0.8422757387161255, 0.014737656340003014]], "normalized_score": [0.17555563151836395], "tokens": [["\u2581For", "b", "idden", "\u2581City", "\u2581Square"]], "entropy": [[4.227280616760254, 0.10305383801460266, 0.05358975753188133, 0.9229059219360352, 1.5148078203201294]], "avg_entropy": [1.3643275909125805], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Square of Heavenly Peace", "score_of_biggest_cluster": 0.17555563151836395, "score_of_first_answer": 0.17555563151836395, "max_score_of_first_answer": 0.9931365251541138, "min_score_of_first_answer": 0.013657626695930958, "avg_score": 0.17555563151836395, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4577634944537914, "std_of_first_answer": 0.4577634944537914, "avg_entropy_of_first_answer": 1.3643275909125805, "lex_sim": 1.0}
{"question": "John Lennon had two number one hits in 1981. 'Imagine' was one, what was the other?", "answer": ["Happy Christmas (War is Over)"], "scores": [[0.0843864232301712, 0.3804938495159149, 0.5374462604522705, 0.8870974183082581, 0.9950739741325378, 0.512251079082489, 0.7737174034118652, 0.9838516116142273]], "normalized_score": [0.5268926024436951], "tokens": [["\u2581Happy", "\u2581Christmas", "\u2581(", "W", "ar", "\u2581is", "\u2581Over", ")"]], "entropy": [[5.395732879638672, 1.5955491065979004, 1.6440857648849487, 0.6806051731109619, 0.052704498171806335, 0.7569095492362976, 0.581873893737793, 0.1400497555732727]], "avg_entropy": [1.3559388276189566], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A women", "score_of_biggest_cluster": 0.5268926024436951, "score_of_first_answer": 0.5268926024436951, "max_score_of_first_answer": 0.9950739741325378, "min_score_of_first_answer": 0.0843864232301712, "avg_score": 0.5268926024436951, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30124505500852433, "std_of_first_answer": 0.30124505500852433, "avg_entropy_of_first_answer": 1.3559388276189566, "lex_sim": 1.0}
{"question": "What was the name of Shari Lewis' sassy sock puppet?", "answer": ["Lambchop's Play-Along"], "scores": [[0.16533063352108002, 0.5768197178840637, 0.8167797327041626, 0.025569260120391846, 0.9809178113937378, 0.8188393712043762, 0.7037208676338196, 0.7725026607513428, 0.9998776912689209]], "normalized_score": [0.45700573921203613], "tokens": [["\u2581Lamb", "ch", "op", "'", "s", "\u2581Play", "-", "Al", "ong"]], "entropy": [[6.017458915710449, 1.8422291278839111, 0.861703634262085, 1.4817019701004028, 0.13571985065937042, 1.4523924589157104, 0.9976149797439575, 1.019059658050537, 0.0013310411013662815]], "avg_entropy": [1.5343568484919767], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lamb Chop", "score_of_biggest_cluster": 0.45700573921203613, "score_of_first_answer": 0.45700573921203613, "max_score_of_first_answer": 0.9998776912689209, "min_score_of_first_answer": 0.025569260120391846, "avg_score": 0.45700573921203613, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32254157696541086, "std_of_first_answer": 0.32254157696541086, "avg_entropy_of_first_answer": 1.5343568484919767, "lex_sim": 1.0}
{"question": "Which highway was Revisited in a classic 60s album by Bob Dylan?", "answer": ["Highway 61 Blues"], "scores": [[0.17448149621486664, 0.803581178188324, 0.9631227254867554, 0.9513237476348877, 0.023609763011336327]], "normalized_score": [0.313600093126297], "tokens": [["\u2581Highway", "\u2581", "6", "1", "\u2581Blues"]], "entropy": [[5.260135650634766, 1.4042308330535889, 0.23823848366737366, 0.3021116852760315, 1.756906509399414]], "avg_entropy": [1.7923246324062347], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "61", "score_of_biggest_cluster": 0.313600093126297, "score_of_first_answer": 0.313600093126297, "max_score_of_first_answer": 0.9631227254867554, "min_score_of_first_answer": 0.023609763011336327, "avg_score": 0.313600093126297, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4021483011166792, "std_of_first_answer": 0.4021483011166792, "avg_entropy_of_first_answer": 1.7923246324062347, "lex_sim": 1.0}
{"question": "The human brain is made up of between 10 billion and 100 billion nerve cells known as what?", "answer": ["Nerve Neuron"], "scores": [[0.04117114469408989, 0.3154858648777008, 0.0018768482841551304, 0.9063759446144104, 0.23796239495277405]], "normalized_score": [0.08793540298938751], "tokens": [["\u2581N", "erve", "\u2581Ne", "ur", "on"]], "entropy": [[4.156165599822998, 3.5639538764953613, 3.0616493225097656, 0.6221194863319397, 0.9116113185882568]], "avg_entropy": [2.463099920749664], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brain cells", "score_of_biggest_cluster": 0.08793540298938751, "score_of_first_answer": 0.08793540298938751, "max_score_of_first_answer": 0.9063759446144104, "min_score_of_first_answer": 0.0018768482841551304, "avg_score": 0.08793540298938751, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3248533215721746, "std_of_first_answer": 0.3248533215721746, "avg_entropy_of_first_answer": 2.463099920749664, "lex_sim": 1.0}
{"question": "Which football team plays its home games at Portman Road?", "answer": ["Ipswich Town"], "scores": [[0.4380834102630615, 0.9599802494049072, 0.9466259479522705, 0.7955731749534607]], "normalized_score": [0.7501868009567261], "tokens": [["\u2581I", "ps", "wich", "\u2581Town"]], "entropy": [[4.196183204650879, 0.4209194481372833, 0.39837944507598877, 1.0255308151245117]], "avg_entropy": [1.5102532282471657], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Ipswich Town F.C.", "score_of_biggest_cluster": 0.7501868009567261, "score_of_first_answer": 0.7501868009567261, "max_score_of_first_answer": 0.9599802494049072, "min_score_of_first_answer": 0.4380834102630615, "avg_score": 0.7501868009567261, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2104779856261143, "std_of_first_answer": 0.2104779856261143, "avg_entropy_of_first_answer": 1.5102532282471657, "lex_sim": 1.0}
{"question": "In the classic fairy tale, what did Jack (of Jack and the Beanstalk fame) trade for the magic beans?", "answer": ["Black hen"], "scores": [[0.003591255284845829, 0.05237243324518204]], "normalized_score": [0.013714326545596123], "tokens": [["\u2581Black", "\u2581hen"]], "entropy": [[5.5879106521606445, 5.300154685974121]], "avg_entropy": [5.444032669067383], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "a cow", "score_of_biggest_cluster": 0.013714326545596123, "score_of_first_answer": 0.013714326545596123, "max_score_of_first_answer": 0.05237243324518204, "min_score_of_first_answer": 0.003591255284845829, "avg_score": 0.013714326545596123, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.024390588980168104, "std_of_first_answer": 0.024390588980168104, "avg_entropy_of_first_answer": 5.444032669067383, "lex_sim": 1.0}
{"question": "What was the Paramount Film Company originally called?", "answer": ["Famous Players-Lasky"], "scores": [[0.16541123390197754, 0.9956041574478149, 0.8934782147407532, 0.9984683394432068, 0.04810134693980217, 0.9779878854751587, 0.9815853834152222, 0.9950986504554749]], "normalized_score": [0.5353889465332031], "tokens": [["\u2581Fam", "ous", "\u2581Play", "ers", "-", "L", "ask", "y"]], "entropy": [[5.4044694900512695, 0.04536586254835129, 0.8056233525276184, 0.0195232555270195, 1.9881975650787354, 0.2117653489112854, 0.2014417052268982, 0.04995176941156387]], "avg_entropy": [1.0907922936603427], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Raleigh Studios", "score_of_biggest_cluster": 0.5353889465332031, "score_of_first_answer": 0.5353889465332031, "max_score_of_first_answer": 0.9984683394432068, "min_score_of_first_answer": 0.04810134693980217, "avg_score": 0.5353889465332031, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37787977139607065, "std_of_first_answer": 0.37787977139607065, "avg_entropy_of_first_answer": 1.0907922936603427, "lex_sim": 1.0}
{"question": "\"Sportscaster Jack Brickhouse who once said \"\"Any team can have a bad century\"\" provided play-by-play coverage of what American major league team's games from 1948 to 1981?\"", "answer": ["Chicago Cubs"], "scores": [[0.477443665266037, 0.6900230646133423, 0.9970974922180176]], "normalized_score": [0.6899873614311218], "tokens": [["\u2581Chicago", "\u2581C", "ubs"]], "entropy": [[3.4926393032073975, 1.2761390209197998, 0.03310944512486458]], "avg_entropy": [1.6006292564173539], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Chicaco Cubs", "score_of_biggest_cluster": 0.6899873614311218, "score_of_first_answer": 0.6899873614311218, "max_score_of_first_answer": 0.9970974922180176, "min_score_of_first_answer": 0.477443665266037, "avg_score": 0.6899873614311218, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.21331375039844191, "std_of_first_answer": 0.21331375039844191, "avg_entropy_of_first_answer": 1.6006292564173539, "lex_sim": 1.0}
{"question": "The lyrics of which song first put to music by Pete Seeger in 1959 and later made famous by The Byrds were taken almost entirely from the Book of Ecclesiastes in the Bible?", "answer": ["Turn!"], "scores": [[0.5687964558601379, 0.6503695249557495, 0.9672999382019043, 0.9933075904846191, 0.992944061756134, 0.9725779294967651]], "normalized_score": [0.8367618322372437], "tokens": [["\u2581Turn", "!", "\u2581Turn", "!", "\u2581Turn", "!"]], "entropy": [[2.4956140518188477, 0.9713617563247681, 0.17732691764831543, 0.06346195936203003, 0.06294433772563934, 0.17803645133972168]], "avg_entropy": [0.6581242457032204], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Turn Turn Turn", "score_of_biggest_cluster": 0.8367618322372437, "score_of_first_answer": 0.8367618322372437, "max_score_of_first_answer": 0.9933075904846191, "min_score_of_first_answer": 0.5687964558601379, "avg_score": 0.8367618322372437, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.17717247120727994, "std_of_first_answer": 0.17717247120727994, "avg_entropy_of_first_answer": 0.6581242457032204, "lex_sim": 1.0}
{"question": "On a clothing label a cross through what shape means 'Do Not Dry Clean'?", "answer": ["Pentagonal star"], "scores": [[0.0028019545134156942, 0.07133092731237411, 0.89632248878479, 0.08278600871562958]], "normalized_score": [0.062056899070739746], "tokens": [["\u2581Pent", "ag", "onal", "\u2581star"]], "entropy": [[5.338325023651123, 1.8615994453430176, 0.5656962990760803, 4.703230857849121]], "avg_entropy": [3.1172129064798355], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Centre for Innovation Research and Competence in the Learning Economy (CIRCLE)", "score_of_biggest_cluster": 0.062056899070739746, "score_of_first_answer": 0.062056899070739746, "max_score_of_first_answer": 0.89632248878479, "min_score_of_first_answer": 0.0028019545134156942, "avg_score": 0.062056899070739746, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3667472319253135, "std_of_first_answer": 0.3667472319253135, "avg_entropy_of_first_answer": 3.1172129064798355, "lex_sim": 1.0}
{"question": "Approximately what percentage of an incandescent light bulb's energy is wasted as heat?", "answer": ["99% (0.99) (80.6666666666667%)"], "scores": [[0.5711597800254822, 0.5495969653129578, 0.06411432474851608, 0.38505393266677856, 0.02079284004867077, 0.011427005752921104, 0.7511141896247864, 0.3718389868736267, 0.8500105738639832, 0.67612224817276, 0.00940598826855421, 0.00789002887904644, 0.25287026166915894, 0.08925896883010864, 0.09261292219161987, 0.05002432316541672, 0.04718366637825966, 0.4734068810939789, 0.8164072632789612, 0.819785475730896, 0.8920633792877197, 0.8481017351150513, 0.8775947690010071, 0.8520078063011169, 0.8729280233383179, 0.8455729484558105, 0.22618836164474487, 0.49391162395477295]], "normalized_score": [0.23143452405929565], "tokens": [["\u2581", "9", "9", "%", "\u2581(", "0", ".", "9", "9", ")", "\u2581(", "8", "0", ".", "6", "6", "6", "6", "6", "6", "6", "6", "6", "6", "6", "6", "7", "%)"]], "entropy": [[3.241255760192871, 2.062629461288452, 1.7524256706237793, 1.9026652574539185, 1.0172510147094727, 5.368895053863525, 1.4729700088500977, 1.7685307264328003, 0.8877090215682983, 2.0768678188323975, 0.4730723202228546, 5.187532424926758, 2.910153865814209, 2.3489792346954346, 2.3388607501983643, 2.5293102264404297, 2.33084774017334, 2.0428850650787354, 0.8773553967475891, 0.7824779748916626, 0.5683531165122986, 0.5903446078300476, 0.5118414163589478, 0.5311956405639648, 0.4920729398727417, 0.5528626441955566, 0.7494641542434692, 1.7364921569824219]], "avg_entropy": [1.7537607667701585], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "90 %", "score_of_biggest_cluster": 0.23143452405929565, "score_of_first_answer": 0.23143452405929565, "max_score_of_first_answer": 0.8920633792877197, "min_score_of_first_answer": 0.00789002887904644, "avg_score": 0.23143452405929565, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33944871548895705, "std_of_first_answer": 0.33944871548895705, "avg_entropy_of_first_answer": 1.7537607667701585, "lex_sim": 1.0}
{"question": "Who was elected Governor of the US State of Texas in 1994?", "answer": ["George Walker Bush"], "scores": [[0.27511265873908997, 0.2519601583480835, 0.9634519219398499]], "normalized_score": [0.4057179093360901], "tokens": [["\u2581George", "\u2581Walker", "\u2581Bush"]], "entropy": [[4.987936973571777, 1.9246258735656738, 0.27908656001091003]], "avg_entropy": [2.3972164690494537], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "George Walker Bush", "score_of_biggest_cluster": 0.4057179093360901, "score_of_first_answer": 0.4057179093360901, "max_score_of_first_answer": 0.9634519219398499, "min_score_of_first_answer": 0.2519601583480835, "avg_score": 0.4057179093360901, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3300786958686338, "std_of_first_answer": 0.3300786958686338, "avg_entropy_of_first_answer": 2.3972164690494537, "lex_sim": 1.0}
{"question": "What was the name of the German encryption machine whose code was deciphered at Bletchley Park during World War II ?", "answer": ["Enigma"], "scores": [[0.2854532301425934, 0.9807531237602234]], "normalized_score": [0.5291116833686829], "tokens": [["\u2581En", "igma"]], "entropy": [[4.951165199279785, 0.1706864833831787]], "avg_entropy": [2.560925841331482], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Steckerbrett", "score_of_biggest_cluster": 0.5291116833686829, "score_of_first_answer": 0.5291116833686829, "max_score_of_first_answer": 0.9807531237602234, "min_score_of_first_answer": 0.2854532301425934, "avg_score": 0.5291116833686829, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.347649946808815, "std_of_first_answer": 0.347649946808815, "avg_entropy_of_first_answer": 2.560925841331482, "lex_sim": 1.0}
{"question": "What can be a legislative body, a collective name for type of bird, and a funk band?", "answer": ["Congress (government), Congresses (buntings), Congress (band)"], "scores": [[0.09286554902791977, 0.048623546957969666, 0.024438465014100075, 0.7040479779243469, 0.994645357131958, 0.043434109538793564, 0.5011729598045349, 0.0035358902532607317, 0.7830548882484436, 0.012142050080001354, 0.03231934458017349, 0.9878559708595276, 0.660607099533081, 0.6264528632164001, 0.8855695724487305, 0.6147778630256653, 0.979887843132019]], "normalized_score": [0.18647503852844238], "tokens": [["\u2581Congress", "\u2581(", "g", "overn", "ment", "),", "\u2581Congress", "es", "\u2581(", "b", "unt", "ings", "),", "\u2581Congress", "\u2581(", "band", ")"]], "entropy": [[4.215682029724121, 0.7833431959152222, 4.56484842300415, 1.1643645763397217, 0.04601224511861801, 1.1476775407791138, 2.5012083053588867, 0.5334168076515198, 0.9454785585403442, 3.1767239570617676, 4.253335475921631, 0.11457895487546921, 1.1635948419570923, 1.5744423866271973, 0.7361435890197754, 1.791794776916504, 0.16006453335285187]], "avg_entropy": [1.698394717539058], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Parliamnet", "score_of_biggest_cluster": 0.18647503852844238, "score_of_first_answer": 0.18647503852844238, "max_score_of_first_answer": 0.994645357131958, "min_score_of_first_answer": 0.0035358902532607317, "avg_score": 0.18647503852844238, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38582923263185515, "std_of_first_answer": 0.38582923263185515, "avg_entropy_of_first_answer": 1.698394717539058, "lex_sim": 1.0}
{"question": "Whose 2014 best-selling novel \u2018Be Careful What You Wish For\u2019 is the fourth in the Clifton Chronicles series?", "answer": ["Jeffrey Archer"], "scores": [[0.6129738092422485, 0.9280862808227539, 0.9519758820533752, 0.9960765242576599]], "normalized_score": [0.8570126891136169], "tokens": [["\u2581Jeff", "rey", "\u2581Arch", "er"]], "entropy": [[3.0879406929016113, 0.6097031831741333, 0.44660884141921997, 0.03883644938468933]], "avg_entropy": [1.0457722917199135], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Baron Archer of Weston-super-Mare", "score_of_biggest_cluster": 0.8570126891136169, "score_of_first_answer": 0.8570126891136169, "max_score_of_first_answer": 0.9960765242576599, "min_score_of_first_answer": 0.6129738092422485, "avg_score": 0.8570126891136169, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.15168310268865667, "std_of_first_answer": 0.15168310268865667, "avg_entropy_of_first_answer": 1.0457722917199135, "lex_sim": 1.0}
{"question": "What was the name of the Prime Minister in the T.V. series Yes Prime Minister", "answer": ["Hiroshimo"], "scores": [[0.016677942126989365, 0.0006344889407046139, 0.24444197118282318, 0.010288942605257034]], "normalized_score": [0.012772568501532078], "tokens": [["\u2581H", "iro", "sh", "imo"]], "entropy": [[5.055627822875977, 1.2571759223937988, 2.8484864234924316, 1.5888392925262451]], "avg_entropy": [2.687532365322113], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jim Hacker", "score_of_biggest_cluster": 0.012772568501532078, "score_of_first_answer": 0.012772568501532078, "max_score_of_first_answer": 0.24444197118282318, "min_score_of_first_answer": 0.0006344889407046139, "avg_score": 0.012772568501532078, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.10202254723271481, "std_of_first_answer": 0.10202254723271481, "avg_entropy_of_first_answer": 2.687532365322113, "lex_sim": 1.0}
{"question": "What is the name of the strong fibrous tissue that joins one bone to another at a joint?", "answer": ["Cartilage"], "scores": [[0.03602985665202141, 0.9320173263549805, 0.8837645053863525]], "normalized_score": [0.30960479378700256], "tokens": [["\u2581Cart", "il", "age"]], "entropy": [[4.886663436889648, 0.6006803512573242, 0.6651179790496826]], "avg_entropy": [2.0508205890655518], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ligamentum", "score_of_biggest_cluster": 0.30960479378700256, "score_of_first_answer": 0.30960479378700256, "max_score_of_first_answer": 0.9320173263549805, "min_score_of_first_answer": 0.03602985665202141, "avg_score": 0.30960479378700256, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.411471062201201, "std_of_first_answer": 0.411471062201201, "avg_entropy_of_first_answer": 2.0508205890655518, "lex_sim": 1.0}
{"question": "Produced between 1970 and 1974, what is a Bond Bug?", "answer": ["Ford K-Victorian-Wagon-R"], "scores": [[0.009980564936995506, 0.003828727640211582, 0.07849913090467453, 0.0012679031351581216, 0.0028803544119000435, 0.7283663153648376, 0.06983630359172821, 0.02178964950144291, 0.010145541280508041, 0.03713739290833473, 0.2340572476387024, 0.027246281504631042]], "normalized_score": [0.023555029183626175], "tokens": [["\u2581Ford", "\u2581K", "-", "V", "ict", "or", "ian", "-", "W", "agon", "-", "R"]], "entropy": [[5.083211898803711, 4.381189346313477, 4.938092231750488, 2.634181022644043, 3.2987377643585205, 0.992516040802002, 3.205667495727539, 4.8321943283081055, 5.967253684997559, 4.546369552612305, 2.1329145431518555, 5.641916275024414]], "avg_entropy": [3.9711870153745017], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Automotive vehicle", "score_of_biggest_cluster": 0.023555029183626175, "score_of_first_answer": 0.023555029183626175, "max_score_of_first_answer": 0.7283663153648376, "min_score_of_first_answer": 0.0012679031351581216, "avg_score": 0.023555029183626175, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19876779486800128, "std_of_first_answer": 0.19876779486800128, "avg_entropy_of_first_answer": 3.9711870153745017, "lex_sim": 1.0}
{"question": "What London medicinal water site, later a music hall, was the birthplace of the Royal Ballet and English National Opera?", "answer": ["Covent Garden"], "scores": [[0.026886191219091415, 0.9879591464996338, 0.850821852684021]], "normalized_score": [0.28272807598114014], "tokens": [["\u2581Cov", "ent", "\u2581Garden"]], "entropy": [[5.2337751388549805, 0.12785696983337402, 0.9723767042160034]], "avg_entropy": [2.111336270968119], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sadler's Wells", "score_of_biggest_cluster": 0.28272807598114014, "score_of_first_answer": 0.28272807598114014, "max_score_of_first_answer": 0.9879591464996338, "min_score_of_first_answer": 0.026886191219091415, "avg_score": 0.28272807598114014, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42443921655657024, "std_of_first_answer": 0.42443921655657024, "avg_entropy_of_first_answer": 2.111336270968119, "lex_sim": 1.0}
{"question": "The Spanish La Liga football club Villarreal share their nickname with which Beatles song?", "answer": ["Yesterday's Sun"], "scores": [[0.26743629574775696, 0.04744713380932808, 0.023624194785952568, 0.9693543314933777, 0.030715668573975563]], "normalized_score": [0.09775206446647644], "tokens": [["\u2581Y", "esterday", "'", "s", "\u2581Sun"]], "entropy": [[4.86060094833374, 0.4805465042591095, 1.3195006847381592, 0.20827056467533112, 5.376211166381836]], "avg_entropy": [2.449025973677635], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yellow Submarine", "score_of_biggest_cluster": 0.09775206446647644, "score_of_first_answer": 0.09775206446647644, "max_score_of_first_answer": 0.9693543314933777, "min_score_of_first_answer": 0.023624194785952568, "avg_score": 0.09775206446647644, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36237129200193907, "std_of_first_answer": 0.36237129200193907, "avg_entropy_of_first_answer": 2.449025973677635, "lex_sim": 1.0}
{"question": "What gemstone is The Moonstone in the classic novel of that name by Wilkie Collins?", "answer": ["India"], "scores": [[0.0047476100735366344]], "normalized_score": [0.004747610539197922], "tokens": [["\u2581India"]], "entropy": [[5.281547546386719]], "avg_entropy": [5.281547546386719], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Diamond", "score_of_biggest_cluster": 0.004747610539197922, "score_of_first_answer": 0.004747610539197922, "max_score_of_first_answer": 0.0047476100735366344, "min_score_of_first_answer": 0.0047476100735366344, "avg_score": 0.004747610539197922, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.281547546386719, "lex_sim": 1.0}
{"question": "In cockney rhyming slang what is a dog and bone", "answer": ["Telephone"], "scores": [[0.10171224176883698, 0.9555212259292603]], "normalized_score": [0.31175023317337036], "tokens": [["\u2581Tele", "phone"]], "entropy": [[5.2060747146606445, 0.2956494092941284]], "avg_entropy": [2.7508620619773865], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "\u2121", "score_of_biggest_cluster": 0.31175023317337036, "score_of_first_answer": 0.31175023317337036, "max_score_of_first_answer": 0.9555212259292603, "min_score_of_first_answer": 0.10171224176883698, "avg_score": 0.31175023317337036, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.42690449208021164, "std_of_first_answer": 0.42690449208021164, "avg_entropy_of_first_answer": 2.7508620619773865, "lex_sim": 1.0}
{"question": "\u2018Swifter, Higher, Stronger\u2019 is the motto for the modern \u2018what\u2019?", "answer": ["2012 Summer Olympics"], "scores": [[0.05136335641145706, 0.3144358694553375, 0.8817978501319885, 0.7880825996398926, 0.6658439040184021, 0.68819659948349, 0.8262127041816711]], "normalized_score": [0.45833829045295715], "tokens": [["\u2581", "2", "0", "1", "2", "\u2581Summer", "\u2581Olympics"]], "entropy": [[4.200959205627441, 3.490078926086426, 0.7133749723434448, 0.8665186166763306, 1.2236000299453735, 1.323868751525879, 0.6390856504440308]], "avg_entropy": [1.7796408789498466], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "History of the Modern Olympics", "score_of_biggest_cluster": 0.45833829045295715, "score_of_first_answer": 0.45833829045295715, "max_score_of_first_answer": 0.8817978501319885, "min_score_of_first_answer": 0.05136335641145706, "avg_score": 0.45833829045295715, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28299018511631596, "std_of_first_answer": 0.28299018511631596, "avg_entropy_of_first_answer": 1.7796408789498466, "lex_sim": 1.0}
{"question": "The term photovoltaic refers to technology in what renewable energy?", "answer": ["Solar"], "scores": [[0.5286988019943237, 0.9900493621826172]], "normalized_score": [0.7234901189804077], "tokens": [["\u2581Sol", "ar"]], "entropy": [[3.2358341217041016, 0.10158782452344894]], "avg_entropy": [1.6687109731137753], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Solar power plant", "score_of_biggest_cluster": 0.7234901189804077, "score_of_first_answer": 0.7234901189804077, "max_score_of_first_answer": 0.9900493621826172, "min_score_of_first_answer": 0.5286988019943237, "avg_score": 0.7234901189804077, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23067528009414673, "std_of_first_answer": 0.23067528009414673, "avg_entropy_of_first_answer": 1.6687109731137753, "lex_sim": 1.0}
{"question": "Great short-stories like The Gift of the Magi, The Ransom of Red Chief, and The Cop and the Anthem are from what collection of O. Henry that was a reaction to a statement that only four hundred people mattered in New York?", "answer": ["The Four Million"], "scores": [[0.4121999740600586, 0.856593668460846, 0.961236298084259, 0.9926435351371765]], "normalized_score": [0.7618624567985535], "tokens": [["\u2581The", "\u2581Four", "\u2581Mill", "ion"]], "entropy": [[3.415186882019043, 1.0306016206741333, 0.25518959760665894, 0.05352190136909485]], "avg_entropy": [1.1886250004172325], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "The Four Million", "score_of_biggest_cluster": 0.7618624567985535, "score_of_first_answer": 0.7618624567985535, "max_score_of_first_answer": 0.9926435351371765, "min_score_of_first_answer": 0.4121999740600586, "avg_score": 0.7618624567985535, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23268640766779886, "std_of_first_answer": 0.23268640766779886, "avg_entropy_of_first_answer": 1.1886250004172325, "lex_sim": 1.0}
{"question": "Whose portrait (apart from the Queen) appeared on the now defunct one pound note?", "answer": ["Sir Isaac Newton (English physicist/mathematician/astronomer)"], "scores": [[0.10492247343063354, 0.09604787826538086, 0.971630871295929, 0.04824979603290558, 0.06913603842258453, 0.5710059404373169, 0.9966374635696411, 0.997536301612854, 0.009502451866865158, 0.9445765018463135, 0.9932371973991394, 0.9995095729827881, 0.07188476622104645, 0.3652637302875519, 0.9993440508842468, 0.9982182383537292, 0.6345388293266296]], "normalized_score": [0.3136138617992401], "tokens": [["\u2581Sir", "\u2581Isaac", "\u2581Newton", "\u2581(", "English", "\u2581phys", "ic", "ist", "/", "math", "ematic", "ian", "/", "astr", "onom", "er", ")"]], "entropy": [[4.46142578125, 2.582324266433716, 0.23353183269500732, 0.7824488878250122, 4.497627258300781, 1.6544015407562256, 0.02701478637754917, 0.019541116431355476, 1.4360219240188599, 0.37546032667160034, 0.05641957372426987, 0.005842733662575483, 1.2500486373901367, 2.1808042526245117, 0.006701192818582058, 0.016744764521718025, 1.551209807395935]], "avg_entropy": [1.2433863931116373], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Sir isaac newton", "score_of_biggest_cluster": 0.3136138617992401, "score_of_first_answer": 0.3136138617992401, "max_score_of_first_answer": 0.9995095729827881, "min_score_of_first_answer": 0.009502451866865158, "avg_score": 0.3136138617992401, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4178502423617075, "std_of_first_answer": 0.4178502423617075, "avg_entropy_of_first_answer": 1.2433863931116373, "lex_sim": 1.0}
{"question": "Which film star (of Brokeback Mountain fame) died on 22 Jan 2008?", "answer": ["Heathcliffe"], "scores": [[0.42814311385154724, 0.9457218647003174, 0.022443342953920364, 0.12096528708934784]], "normalized_score": [0.18208540976047516], "tokens": [["\u2581He", "ath", "cl", "iffe"]], "entropy": [[4.413472652435303, 0.5094811916351318, 2.340735912322998, 0.6022036075592041]], "avg_entropy": [1.9664733409881592], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Heath Leger", "score_of_biggest_cluster": 0.18208540976047516, "score_of_first_answer": 0.18208540976047516, "max_score_of_first_answer": 0.9457218647003174, "min_score_of_first_answer": 0.022443342953920364, "avg_score": 0.18208540976047516, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35961885198063376, "std_of_first_answer": 0.35961885198063376, "avg_entropy_of_first_answer": 1.9664733409881592, "lex_sim": 1.0}
{"question": "Who has been Secretary of State for Work and Pensions since May 2010?", "answer": ["Mark Harriet Harrid"], "scores": [[0.004059413447976112, 0.040583863854408264, 0.007917850278317928, 0.37247854471206665, 0.013795607723295689, 0.014181280508637428]], "normalized_score": [0.021363070234656334], "tokens": [["\u2581Mark", "\u2581Har", "ri", "et", "\u2581Har", "rid"]], "entropy": [[5.464727401733398, 5.3305583000183105, 1.5825579166412354, 2.3894877433776855, 2.608966827392578, 1.8900039196014404]], "avg_entropy": [3.211050351460775], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ian Duncan Smith", "score_of_biggest_cluster": 0.021363070234656334, "score_of_first_answer": 0.021363070234656334, "max_score_of_first_answer": 0.37247854471206665, "min_score_of_first_answer": 0.004059413447976112, "avg_score": 0.021363070234656334, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13332536286038804, "std_of_first_answer": 0.13332536286038804, "avg_entropy_of_first_answer": 3.211050351460775, "lex_sim": 1.0}
{"question": "June 2, 2004 saw the beginning of the streak in which what Edmonds, WA native won 74 consecutive games on Jeopardy?", "answer": ["Ken Jeopardy!"], "scores": [[0.41143351793289185, 0.02876022830605507, 0.8661003708839417, 0.9860107898712158, 0.9898479580879211, 0.4088829755783081]], "normalized_score": [0.3999001383781433], "tokens": [["\u2581Ken", "\u2581Je", "op", "ard", "y", "!"]], "entropy": [[4.063069820404053, 2.1150171756744385, 0.8789856433868408, 0.1459323614835739, 0.09912149608135223, 2.4429779052734375]], "avg_entropy": [1.624184067050616], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Ken William Jennings III", "score_of_biggest_cluster": 0.3999001383781433, "score_of_first_answer": 0.3999001383781433, "max_score_of_first_answer": 0.9898479580879211, "min_score_of_first_answer": 0.02876022830605507, "avg_score": 0.3999001383781433, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3579599199182673, "std_of_first_answer": 0.3579599199182673, "avg_entropy_of_first_answer": 1.624184067050616, "lex_sim": 1.0}
{"question": "Who painted The Card Players, at Feb 2012 the most expensive artwork in the world?", "answer": ["Paul C\u00e9zanne"], "scores": [[0.281582236289978, 0.7355484366416931, 0.5540333390235901, 0.9861319065093994, 0.991131603717804]], "normalized_score": [0.6456003189086914], "tokens": [["\u2581Paul", "\u2581C", "\u00e9", "z", "anne"]], "entropy": [[4.394387722015381, 2.0282037258148193, 1.4052449464797974, 0.12525883316993713, 0.07599306106567383]], "avg_entropy": [1.6058176577091217], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Paul C\u00e9zanne", "score_of_biggest_cluster": 0.6456003189086914, "score_of_first_answer": 0.6456003189086914, "max_score_of_first_answer": 0.991131603717804, "min_score_of_first_answer": 0.281582236289978, "avg_score": 0.6456003189086914, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2697418825757038, "std_of_first_answer": 0.2697418825757038, "avg_entropy_of_first_answer": 1.6058176577091217, "lex_sim": 1.0}
{"question": "According to the World Meteorological Organisation and the UK Met Office, what is the highest number on the Beaufort scale, representing hurricane force?", "answer": ["12"], "scores": [[0.3227102756500244, 0.7553507089614868, 0.7129785418510437]], "normalized_score": [0.558057963848114], "tokens": [["\u2581", "1", "2"]], "entropy": [[4.517771244049072, 1.268929123878479, 1.25222647190094]], "avg_entropy": [2.346308946609497], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "twelve", "score_of_biggest_cluster": 0.558057963848114, "score_of_first_answer": 0.558057963848114, "max_score_of_first_answer": 0.7553507089614868, "min_score_of_first_answer": 0.3227102756500244, "avg_score": 0.558057963848114, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1947312862739361, "std_of_first_answer": 0.1947312862739361, "avg_entropy_of_first_answer": 2.346308946609497, "lex_sim": 1.0}
{"question": "What part does Shirley Henderson play in the Harry Potter films?", "answer": ["Slytherin Prefect"], "scores": [[0.0033066566102206707, 0.18480335175991058, 0.8422625064849854, 0.9387579560279846, 0.0015024231979623437, 0.9982427358627319]], "normalized_score": [0.09477373957633972], "tokens": [["\u2581S", "ly", "ther", "in", "\u2581Pref", "ect"]], "entropy": [[4.765805244445801, 4.805315971374512, 1.4346280097961426, 0.4670395255088806, 3.7560815811157227, 0.017405148595571518]], "avg_entropy": [2.5410459134727716], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Harry Potter's future", "score_of_biggest_cluster": 0.09477373957633972, "score_of_first_answer": 0.09477373957633972, "max_score_of_first_answer": 0.9982427358627319, "min_score_of_first_answer": 0.0015024231979623437, "avg_score": 0.09477373957633972, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43823311438630613, "std_of_first_answer": 0.43823311438630613, "avg_entropy_of_first_answer": 2.5410459134727716, "lex_sim": 1.0}
{"question": "Sweeney Todd was the demon barber of what street?", "answer": ["Fleet Street"], "scores": [[0.7061095833778381, 0.9930804371833801, 0.6465461850166321]], "normalized_score": [0.768219530582428], "tokens": [["\u2581Fle", "et", "\u2581Street"]], "entropy": [[2.334911346435547, 0.06849662214517593, 1.5660609006881714]], "avg_entropy": [1.323156289756298], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Fleet", "score_of_biggest_cluster": 0.768219530582428, "score_of_first_answer": 0.768219530582428, "max_score_of_first_answer": 0.9930804371833801, "min_score_of_first_answer": 0.6465461850166321, "avg_score": 0.768219530582428, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.15128562290617514, "std_of_first_answer": 0.15128562290617514, "avg_entropy_of_first_answer": 1.323156289756298, "lex_sim": 1.0}
{"question": "Who was said to have presented the baby Jesus with Frankincense?", "answer": ["The Magis (Wiser)"], "scores": [[0.05487290024757385, 0.11817061901092529, 0.0037317187525331974, 0.06117205694317818, 0.042781174182891846, 0.0004977288190275431, 0.6057577729225159]], "normalized_score": [0.029423626139760017], "tokens": [["\u2581The", "\u2581Mag", "is", "\u2581(", "W", "iser", ")"]], "entropy": [[5.4328107833862305, 4.196938514709473, 0.2711789011955261, 1.271043300628662, 4.256779670715332, 0.5628060102462769, 2.4093809127807617]], "avg_entropy": [2.6287054419517517], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Balthazar", "score_of_biggest_cluster": 0.029423626139760017, "score_of_first_answer": 0.029423626139760017, "max_score_of_first_answer": 0.6057577729225159, "min_score_of_first_answer": 0.0004977288190275431, "avg_score": 0.029423626139760017, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.19896615774011614, "std_of_first_answer": 0.19896615774011614, "avg_entropy_of_first_answer": 2.6287054419517517, "lex_sim": 1.0}
{"question": "What were Scotsmen prohibited from wearing 1746-1782 under the Highland Garb Act, after the rising in Scotland in 1745?", "answer": ["Kilts"], "scores": [[0.2180556356906891, 0.981468915939331]], "normalized_score": [0.4626173973083496], "tokens": [["\u2581Kil", "ts"]], "entropy": [[4.310065746307373, 0.171711727976799]], "avg_entropy": [2.240888737142086], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Plaid cloth", "score_of_biggest_cluster": 0.4626173973083496, "score_of_first_answer": 0.4626173973083496, "max_score_of_first_answer": 0.981468915939331, "min_score_of_first_answer": 0.2180556356906891, "avg_score": 0.4626173973083496, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.381706640124321, "std_of_first_answer": 0.381706640124321, "avg_entropy_of_first_answer": 2.240888737142086, "lex_sim": 1.0}
{"question": "The 2009 film \u2018The Hangover\u2019 is set in which city?", "answer": ["Bangkok"], "scores": [[0.008892020210623741, 0.8790573477745056, 0.9891901016235352]], "normalized_score": [0.19774211943149567], "tokens": [["\u2581Bang", "k", "ok"]], "entropy": [[5.257813453674316, 0.8702417016029358, 0.10952629148960114]], "avg_entropy": [2.079193815588951], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Las Vegas City Council", "score_of_biggest_cluster": 0.19774211943149567, "score_of_first_answer": 0.19774211943149567, "max_score_of_first_answer": 0.9891901016235352, "min_score_of_first_answer": 0.008892020210623741, "avg_score": 0.19774211943149567, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4384697188564595, "std_of_first_answer": 0.4384697188564595, "avg_entropy_of_first_answer": 2.079193815588951, "lex_sim": 1.0}
{"question": "The carotid arteries supply blood from the heart to what part of the human body?", "answer": ["Neck Region"], "scores": [[0.11677460372447968, 0.9632696509361267, 0.0009995636064559221]], "normalized_score": [0.04826536029577255], "tokens": [["\u2581Ne", "ck", "\u2581Region"]], "entropy": [[4.732741355895996, 0.2760571539402008, 1.3385169506072998]], "avg_entropy": [2.1157718201478324], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Head and neck anatomy", "score_of_biggest_cluster": 0.04826536029577255, "score_of_first_answer": 0.04826536029577255, "max_score_of_first_answer": 0.9632696509361267, "min_score_of_first_answer": 0.0009995636064559221, "avg_score": 0.04826536029577255, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42894203776237266, "std_of_first_answer": 0.42894203776237266, "avg_entropy_of_first_answer": 2.1157718201478324, "lex_sim": 1.0}
{"question": "'Ben-Hur: A Tale of the Christ', the 1880 novel by Lew Wallace, is about a young aristocratic Jew. What is his first name?", "answer": ["Sweet"], "scores": [[0.007685200776904821, 0.019886255264282227]], "normalized_score": [0.012362437322735786], "tokens": [["\u2581S", "weet"]], "entropy": [[5.568148612976074, 6.096627235412598]], "avg_entropy": [5.832387924194336], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ioudas", "score_of_biggest_cluster": 0.012362437322735786, "score_of_first_answer": 0.012362437322735786, "max_score_of_first_answer": 0.019886255264282227, "min_score_of_first_answer": 0.007685200776904821, "avg_score": 0.012362437322735786, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.006100527243688703, "std_of_first_answer": 0.006100527243688703, "avg_entropy_of_first_answer": 5.832387924194336, "lex_sim": 1.0}
{"question": "The New Zealand government proposes to create a 620,000 sq km Ocean Sanctuary in 2016 round what island or islands?", "answer": ["Pole, Earth, Antarctica, Ocean"], "scores": [[0.006451199296861887, 0.003318226896226406, 0.006306839641183615, 0.003170380834490061, 0.17511697113513947, 0.08092235028743744, 0.5205383896827698, 0.9506696462631226, 0.3129543960094452, 0.02014515921473503]], "normalized_score": [0.042432647198438644], "tokens": [["\u2581P", "ole", ",", "\u2581Earth", ",", "\u2581Ant", "arct", "ica", ",", "\u2581Ocean"]], "entropy": [[4.564730644226074, 3.725371837615967, 3.9089202880859375, 5.620967864990234, 2.3834171295166016, 5.972681045532227, 1.3894892930984497, 0.3753659725189209, 1.3180798292160034, 5.555886745452881]], "avg_entropy": [3.4814910650253297], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Kermadecs", "score_of_biggest_cluster": 0.042432647198438644, "score_of_first_answer": 0.042432647198438644, "max_score_of_first_answer": 0.9506696462631226, "min_score_of_first_answer": 0.003170380834490061, "avg_score": 0.042432647198438644, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29629802095313373, "std_of_first_answer": 0.29629802095313373, "avg_entropy_of_first_answer": 3.4814910650253297, "lex_sim": 1.0}
{"question": "What is a patellar reflex more commonly called", "answer": ["Knee jerk reflex"], "scores": [[0.6256412863731384, 0.9923932552337646, 0.9885126352310181, 0.37567204236984253, 0.9979714751243591, 0.1811707764863968, 0.9964332580566406]], "normalized_score": [0.6347992420196533], "tokens": [["\u2581K", "ne", "e", "\u2581j", "erk", "\u2581ref", "lex"]], "entropy": [[2.45124888420105, 0.07618461549282074, 0.07965964078903198, 1.4467607736587524, 0.01736900769174099, 1.4083011150360107, 0.031555451452732086]], "avg_entropy": [0.7872970697603056], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Knee jerk reaction", "score_of_biggest_cluster": 0.6347992420196533, "score_of_first_answer": 0.6347992420196533, "max_score_of_first_answer": 0.9979714751243591, "min_score_of_first_answer": 0.1811707764863968, "avg_score": 0.6347992420196533, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3197768465142673, "std_of_first_answer": 0.3197768465142673, "avg_entropy_of_first_answer": 0.7872970697603056, "lex_sim": 1.0}
{"question": "In Jeff Wayne's 'Musical Version of Spartacus' released in 1992, which later Oscar-winning British actress sings the principal female role of Palene?", "answer": ["Sarah Polley"], "scores": [[0.005052280146628618, 0.013345090672373772, 0.8490198254585266]], "normalized_score": [0.03853974863886833], "tokens": [["\u2581Sarah", "\u2581Pol", "ley"]], "entropy": [[5.3859639167785645, 4.185498237609863, 0.8582508563995361]], "avg_entropy": [3.476571003595988], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Catherine \u03b6-Jones", "score_of_biggest_cluster": 0.03853974863886833, "score_of_first_answer": 0.03853974863886833, "max_score_of_first_answer": 0.8490198254585266, "min_score_of_first_answer": 0.005052280146628618, "avg_score": 0.03853974863886833, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39590995758100145, "std_of_first_answer": 0.39590995758100145, "avg_entropy_of_first_answer": 3.476571003595988, "lex_sim": 1.0}
{"question": "Which song was a UK number one for Terry Jacks in 1974 and for Westlife in 1999?", "answer": ["Seasons in the Sun"], "scores": [[0.5410335659980774, 0.9976612329483032, 0.8889249563217163, 0.9812613725662231, 0.8493079543113708]], "normalized_score": [0.8325003981590271], "tokens": [["\u2581Se", "asons", "\u2581in", "\u2581the", "\u2581Sun"]], "entropy": [[3.287017345428467, 0.021137680858373642, 0.5904140472412109, 0.14715170860290527, 0.5565275549888611]], "avg_entropy": [0.9204496674239635], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Seasons in the Sun", "score_of_biggest_cluster": 0.8325003981590271, "score_of_first_answer": 0.8325003981590271, "max_score_of_first_answer": 0.9976612329483032, "min_score_of_first_answer": 0.5410335659980774, "avg_score": 0.8325003981590271, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1649217171401336, "std_of_first_answer": 0.1649217171401336, "avg_entropy_of_first_answer": 0.9204496674239635, "lex_sim": 1.0}
{"question": "\u201cPoor old Johnny Ray\u201d is the opening line of which number one hit?", "answer": ["Diana"], "scores": [[0.01461587194353342, 0.37893685698509216]], "normalized_score": [0.07442104816436768], "tokens": [["\u2581D", "iana"]], "entropy": [[4.8064656257629395, 2.588857412338257]], "avg_entropy": [3.697661519050598], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "COME ON EILEEN", "score_of_biggest_cluster": 0.07442104816436768, "score_of_first_answer": 0.07442104816436768, "max_score_of_first_answer": 0.37893685698509216, "min_score_of_first_answer": 0.01461587194353342, "avg_score": 0.07442104816436768, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.18216049252077937, "std_of_first_answer": 0.18216049252077937, "avg_entropy_of_first_answer": 3.697661519050598, "lex_sim": 1.0}
{"question": "What was Reg Varney's character called in 'On the Buses'?", "answer": ["A bus driver"], "scores": [[0.015316843055188656, 0.019715052098035812, 0.7609384655952454]], "normalized_score": [0.061249930411577225], "tokens": [["\u2581A", "\u2581bus", "\u2581driver"]], "entropy": [[6.0463104248046875, 7.328244686126709, 1.1056917905807495]], "avg_entropy": [4.826748967170715], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "List of On the Buses characters", "score_of_biggest_cluster": 0.061249930411577225, "score_of_first_answer": 0.061249930411577225, "max_score_of_first_answer": 0.7609384655952454, "min_score_of_first_answer": 0.015316843055188656, "avg_score": 0.061249930411577225, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35045733564372816, "std_of_first_answer": 0.35045733564372816, "avg_entropy_of_first_answer": 4.826748967170715, "lex_sim": 1.0}
{"question": "American Jazz musician John Coltrane is famous for playing which instrument?", "answer": ["Duck Call Reviews"], "scores": [[0.003170368727296591, 0.01115997601300478, 0.03129487484693527, 0.00020771341223735362, 0.014857277274131775]], "normalized_score": [0.005090140271931887], "tokens": [["\u2581D", "uck", "\u2581Call", "\u2581Re", "views"]], "entropy": [[4.688407897949219, 5.842080116271973, 4.082080841064453, 1.370632529258728, 1.2099778652191162]], "avg_entropy": [3.4386358499526977], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Saxamophone", "score_of_biggest_cluster": 0.005090140271931887, "score_of_first_answer": 0.005090140271931887, "max_score_of_first_answer": 0.03129487484693527, "min_score_of_first_answer": 0.00020771341223735362, "avg_score": 0.005090140271931887, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.010936974009426587, "std_of_first_answer": 0.010936974009426587, "avg_entropy_of_first_answer": 3.4386358499526977, "lex_sim": 1.0}
{"question": "Which TV detective was assisted by DS George Toolan?", "answer": ["George's DS"], "scores": [[0.08235643804073334, 0.0072771646082401276, 0.9469510912895203, 0.030471937730908394, 0.3609890341758728]], "normalized_score": [0.091007299721241], "tokens": [["\u2581George", "'", "s", "\u2581D", "S"]], "entropy": [[5.269904136657715, 5.142967700958252, 0.46439120173454285, 6.457796096801758, 2.8291492462158203]], "avg_entropy": [4.032841676473618], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Advection frost", "score_of_biggest_cluster": 0.091007299721241, "score_of_first_answer": 0.091007299721241, "max_score_of_first_answer": 0.9469510912895203, "min_score_of_first_answer": 0.0072771646082401276, "avg_score": 0.091007299721241, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35409903878500343, "std_of_first_answer": 0.35409903878500343, "avg_entropy_of_first_answer": 4.032841676473618, "lex_sim": 1.0}
{"question": "The 2009 season of the Indian Premier League, a Twenty20 cricket competition initiated by the Board of Control for Cricket in India, was held where?", "answer": ["Bangladesh in 2009"], "scores": [[0.01809745654463768, 0.3552163243293762, 0.9464824795722961, 0.0048080808483064175, 0.4488285481929779, 0.9462628364562988, 0.9967107772827148, 0.6029067039489746, 0.7696840763092041]], "normalized_score": [0.26164546608924866], "tokens": [["\u2581Bang", "l", "adesh", "\u2581in", "\u2581", "2", "0", "0", "9"]], "entropy": [[4.554345607757568, 1.7120146751403809, 0.3158983588218689, 1.5725735425949097, 3.0540878772735596, 0.23709215223789215, 0.03264504298567772, 0.7914801836013794, 0.9719802737236023]], "avg_entropy": [1.471346412681871], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "South africa", "score_of_biggest_cluster": 0.26164546608924866, "score_of_first_answer": 0.26164546608924866, "max_score_of_first_answer": 0.9967107772827148, "min_score_of_first_answer": 0.0048080808483064175, "avg_score": 0.26164546608924866, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36377916331415444, "std_of_first_answer": 0.36377916331415444, "avg_entropy_of_first_answer": 1.471346412681871, "lex_sim": 1.0}
{"question": "In what modern-day country would you find Machu Picchu?", "answer": ["Peru"], "scores": [[0.35368630290031433]], "normalized_score": [0.35368630290031433], "tokens": [["\u2581Peru"]], "entropy": [[4.33211088180542]], "avg_entropy": [4.33211088180542], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lower Peru", "score_of_biggest_cluster": 0.35368630290031433, "score_of_first_answer": 0.35368630290031433, "max_score_of_first_answer": 0.35368630290031433, "min_score_of_first_answer": 0.35368630290031433, "avg_score": 0.35368630290031433, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.33211088180542, "lex_sim": 1.0}
{"question": "What name is given to the person in a restaurant responsible for the wine and assisting customers with their choice of wine?", "answer": ["Wine Sommelier"], "scores": [[0.1287216693162918, 0.9608385562896729, 0.020257754251360893, 0.9529113173484802, 0.9757798910140991, 0.970237672328949]], "normalized_score": [0.36226746439933777], "tokens": [["\u2581W", "ine", "\u2581S", "omm", "el", "ier"]], "entropy": [[4.320586204528809, 0.3471396863460541, 3.6893930435180664, 0.34598833322525024, 0.18370266258716583, 0.1694698929786682]], "avg_entropy": [1.509379970530669], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wine Butler", "score_of_biggest_cluster": 0.36226746439933777, "score_of_first_answer": 0.36226746439933777, "max_score_of_first_answer": 0.9757798910140991, "min_score_of_first_answer": 0.020257754251360893, "avg_score": 0.36226746439933777, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4209901013476556, "std_of_first_answer": 0.4209901013476556, "avg_entropy_of_first_answer": 1.509379970530669, "lex_sim": 1.0}
{"question": "As whom is Paul David Hewson better known?", "answer": ["Bono (Paul David Hewson)"], "scores": [[0.3690186142921448, 0.8905821442604065, 0.08288493007421494, 0.043918151408433914, 0.7643759846687317, 0.9761043787002563, 0.9998247027397156, 0.9987375140190125, 0.8738459944725037]], "normalized_score": [0.45144638419151306], "tokens": [["\u2581B", "ono", "\u2581(", "Paul", "\u2581David", "\u2581H", "ew", "son", ")"]], "entropy": [[3.972668170928955, 0.9407190084457397, 1.2087171077728271, 3.018263339996338, 0.9872903227806091, 0.16926263272762299, 0.0026792571879923344, 0.013769959099590778, 0.8893653154373169]], "avg_entropy": [1.244748346041888], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paul David Hewson", "score_of_biggest_cluster": 0.45144638419151306, "score_of_first_answer": 0.45144638419151306, "max_score_of_first_answer": 0.9998247027397156, "min_score_of_first_answer": 0.043918151408433914, "avg_score": 0.45144638419151306, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3707105316936463, "std_of_first_answer": 0.3707105316936463, "avg_entropy_of_first_answer": 1.244748346041888, "lex_sim": 1.0}
{"question": "What is the most famous work of the architect James Hoban?", "answer": ["\u4e00"], "scores": [[0.032316528260707855, 0.0021817360538989305]], "normalized_score": [0.008396794088184834], "tokens": [["\u2581", "\u4e00"]], "entropy": [[4.3904876708984375, 4.25884485244751]], "avg_entropy": [4.324666261672974], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "White House", "score_of_biggest_cluster": 0.008396794088184834, "score_of_first_answer": 0.008396794088184834, "max_score_of_first_answer": 0.032316528260707855, "min_score_of_first_answer": 0.0021817360538989305, "avg_score": 0.008396794088184834, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.015067396103404462, "std_of_first_answer": 0.015067396103404462, "avg_entropy_of_first_answer": 4.324666261672974, "lex_sim": 0.0}
{"question": "In the Muslim religion what name is given to an invisible spirit that appears in either human or 'animal form?", "answer": ["A djinn or a genie"], "scores": [[0.03282695263624191, 0.05396996811032295, 0.7397310733795166, 0.9916290044784546, 0.05363936722278595, 0.054743144661188126, 0.45168226957321167, 0.9776167869567871]], "normalized_score": [0.1898137480020523], "tokens": [["\u2581A", "\u2581d", "j", "inn", "\u2581or", "\u2581a", "\u2581gen", "ie"]], "entropy": [[4.510222434997559, 5.97918176651001, 1.2059502601623535, 0.0848589614033699, 1.3076587915420532, 1.811035394668579, 2.133301258087158, 0.14606985449790955]], "avg_entropy": [2.147284840233624], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Jinni", "score_of_biggest_cluster": 0.1898137480020523, "score_of_first_answer": 0.1898137480020523, "max_score_of_first_answer": 0.9916290044784546, "min_score_of_first_answer": 0.03282695263624191, "avg_score": 0.1898137480020523, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4019389563614146, "std_of_first_answer": 0.4019389563614146, "avg_entropy_of_first_answer": 2.147284840233624, "lex_sim": 1.0}
{"question": "What was the first name of the Dutch explorer, Tasman, after whom Tasmania is named?", "answer": ["Abel"], "scores": [[0.1922689527273178, 0.8899608254432678]], "normalized_score": [0.41365665197372437], "tokens": [["\u2581Ab", "el"]], "entropy": [[5.639878273010254, 0.9955919981002808]], "avg_entropy": [3.3177351355552673], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "ABEL", "score_of_biggest_cluster": 0.41365665197372437, "score_of_first_answer": 0.41365665197372437, "max_score_of_first_answer": 0.8899608254432678, "min_score_of_first_answer": 0.1922689527273178, "avg_score": 0.41365665197372437, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.348845936357975, "std_of_first_answer": 0.348845936357975, "avg_entropy_of_first_answer": 3.3177351355552673, "lex_sim": 1.0}
{"question": "Christopher Cockerill invented what in 1955?", "answer": ["Hovercraft"], "scores": [[0.3189792037010193, 0.6818674802780151, 0.9821789860725403]], "normalized_score": [0.5977932214736938], "tokens": [["\u2581H", "over", "craft"]], "entropy": [[3.892016887664795, 1.0357745885849, 0.16806580126285553]], "avg_entropy": [1.6986190925041835], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hoverbarge", "score_of_biggest_cluster": 0.5977932214736938, "score_of_first_answer": 0.5977932214736938, "max_score_of_first_answer": 0.9821789860725403, "min_score_of_first_answer": 0.3189792037010193, "avg_score": 0.5977932214736938, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2711516289883311, "std_of_first_answer": 0.2711516289883311, "avg_entropy_of_first_answer": 1.6986190925041835, "lex_sim": 1.0}
{"question": "Which famous British actor is/was married to Anouk Aimee?", "answer": ["Peter Ustinov"], "scores": [[0.09465714544057846, 0.2946716547012329, 0.9770240187644958, 0.9909982681274414, 0.9833574891090393]], "normalized_score": [0.4839898645877838], "tokens": [["\u2581Peter", "\u2581U", "st", "in", "ov"]], "entropy": [[5.3190083503723145, 2.276275873184204, 0.23610110580921173, 0.08878611773252487, 0.13674934208393097]], "avg_entropy": [1.6113841578364372], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Albert Finney", "score_of_biggest_cluster": 0.4839898645877838, "score_of_first_answer": 0.4839898645877838, "max_score_of_first_answer": 0.9909982681274414, "min_score_of_first_answer": 0.09465714544057846, "avg_score": 0.4839898645877838, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39175759167773005, "std_of_first_answer": 0.39175759167773005, "avg_entropy_of_first_answer": 1.6113841578364372, "lex_sim": 1.0}
{"question": "Which motor manufacturer makes the models Note and Juke?", "answer": ["Nissan"], "scores": [[0.5254838466644287, 0.802025556564331, 0.9602804780006409]], "normalized_score": [0.739687979221344], "tokens": [["\u2581N", "iss", "an"]], "entropy": [[3.6516618728637695, 1.5865685939788818, 0.34490275382995605]], "avg_entropy": [1.8610444068908691], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Nissan Motor", "score_of_biggest_cluster": 0.739687979221344, "score_of_first_answer": 0.739687979221344, "max_score_of_first_answer": 0.9602804780006409, "min_score_of_first_answer": 0.5254838466644287, "avg_score": 0.739687979221344, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.17968121403520684, "std_of_first_answer": 0.17968121403520684, "avg_entropy_of_first_answer": 1.8610444068908691, "lex_sim": 1.0}
{"question": "Whose backing band is known as The Miami Sound Machine?", "answer": ["Gloria Estefan"], "scores": [[0.6406539678573608, 0.9795258045196533, 0.8802024126052856, 0.9396084547042847, 0.9789028167724609]], "normalized_score": [0.873336672782898], "tokens": [["\u2581Gl", "oria", "\u2581Est", "ef", "an"]], "entropy": [[2.9229660034179688, 0.1589953601360321, 0.9487249851226807, 0.3867347538471222, 0.18115709722042084]], "avg_entropy": [0.9197156399488449], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Gloria Maria Estefan", "score_of_biggest_cluster": 0.873336672782898, "score_of_first_answer": 0.873336672782898, "max_score_of_first_answer": 0.9795258045196533, "min_score_of_first_answer": 0.6406539678573608, "avg_score": 0.873336672782898, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.12685069360231188, "std_of_first_answer": 0.12685069360231188, "avg_entropy_of_first_answer": 0.9197156399488449, "lex_sim": 1.0}
{"question": "What is the Japanese word for the sash worn as a belt in traditional dress (kimonos and martial arts uniforms, etc)?", "answer": ["obi"], "scores": [[0.05537443235516548, 0.9760092496871948]], "normalized_score": [0.23247788846492767], "tokens": [["\u2581ob", "i"]], "entropy": [[4.375851631164551, 0.20152349770069122]], "avg_entropy": [2.288687564432621], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Obi", "score_of_biggest_cluster": 0.23247788846492767, "score_of_first_answer": 0.23247788846492767, "max_score_of_first_answer": 0.9760092496871948, "min_score_of_first_answer": 0.05537443235516548, "avg_score": 0.23247788846492767, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.46031740866601467, "std_of_first_answer": 0.46031740866601467, "avg_entropy_of_first_answer": 2.288687564432621, "lex_sim": 1.0}
{"question": "Who designed St Paul\u2019s Cathedral in London?", "answer": ["Sir Christopher Wren"], "scores": [[0.18497490882873535, 0.8909324407577515, 0.9465358853340149, 0.9930205941200256]], "normalized_score": [0.6273548007011414], "tokens": [["\u2581Sir", "\u2581Christopher", "\u2581W", "ren"]], "entropy": [[4.600019454956055, 0.8842726945877075, 0.44620752334594727, 0.06791969388723373]], "avg_entropy": [1.4996048416942358], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Cristopher wren", "score_of_biggest_cluster": 0.6273548007011414, "score_of_first_answer": 0.6273548007011414, "max_score_of_first_answer": 0.9930205941200256, "min_score_of_first_answer": 0.18497490882873535, "avg_score": 0.6273548007011414, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3304318717222053, "std_of_first_answer": 0.3304318717222053, "avg_entropy_of_first_answer": 1.4996048416942358, "lex_sim": 1.0}
{"question": "\"Who wrote the sons \"\"I've Got You Under My Skin\"\" and \"\"I Get A Kick Out Of You\"\"?\"", "answer": ["Cole"], "scores": [[0.2491443008184433]], "normalized_score": [0.2491442859172821], "tokens": [["\u2581Cole"]], "entropy": [[5.231488227844238]], "avg_entropy": [5.231488227844238], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Cole Porter", "score_of_biggest_cluster": 0.2491442859172821, "score_of_first_answer": 0.2491442859172821, "max_score_of_first_answer": 0.2491443008184433, "min_score_of_first_answer": 0.2491443008184433, "avg_score": 0.2491442859172821, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.231488227844238, "lex_sim": 1.0}
{"question": "Kagoshima international airport is in which country?", "answer": ["The People's Republic of China"], "scores": [[0.016630038619041443, 0.016238607466220856, 0.9436432123184204, 0.9991981387138367, 0.9648905396461487, 0.9869929552078247, 0.9156396389007568]], "normalized_score": [0.300648957490921], "tokens": [["\u2581The", "\u2581People", "'", "s", "\u2581Republic", "\u2581of", "\u2581China"]], "entropy": [[3.83845853805542, 5.957858562469482, 0.3381847143173218, 0.010521729476749897, 0.3204815089702606, 0.09795214235782623, 0.5901675224304199]], "avg_entropy": [1.5933749597253544], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u65e5\u672c\u570b", "score_of_biggest_cluster": 0.300648957490921, "score_of_first_answer": 0.300648957490921, "max_score_of_first_answer": 0.9991981387138367, "min_score_of_first_answer": 0.016238607466220856, "avg_score": 0.300648957490921, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4279482180684537, "std_of_first_answer": 0.4279482180684537, "avg_entropy_of_first_answer": 1.5933749597253544, "lex_sim": 1.0}
{"question": "Which country singer/songwriter was known as the 'Man in Black' ?", "answer": ["Johnny Ray and Jack Jones"], "scores": [[0.393306702375412, 0.004484595265239477, 0.0016118466155603528, 0.009829460643231869, 0.0062160552479326725]], "normalized_score": [0.011167721822857857], "tokens": [["\u2581Johnny", "\u2581Ray", "\u2581and", "\u2581Jack", "\u2581Jones"]], "entropy": [[4.275595664978027, 1.8986761569976807, 1.6650125980377197, 4.761829376220703, 3.7959017753601074]], "avg_entropy": [3.2794031143188476], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Vivian Cash", "score_of_biggest_cluster": 0.011167721822857857, "score_of_first_answer": 0.011167721822857857, "max_score_of_first_answer": 0.393306702375412, "min_score_of_first_answer": 0.0016118466155603528, "avg_score": 0.011167721822857857, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.15513130668782374, "std_of_first_answer": 0.15513130668782374, "avg_entropy_of_first_answer": 3.2794031143188476, "lex_sim": 1.0}
{"question": "What is the Christian service or ceremony, named from Greek meaning grateful, commemorating the Last Supper?", "answer": ["The Holy Communion"], "scores": [[0.041604042053222656, 0.04476441070437431, 0.18232432007789612, 0.9860446453094482]], "normalized_score": [0.13527028262615204], "tokens": [["\u2581The", "\u2581Holy", "\u2581Comm", "union"]], "entropy": [[4.2049713134765625, 3.683593273162842, 2.0882511138916016, 0.1224706843495369]], "avg_entropy": [2.5248215962201357], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Communion (church)", "score_of_biggest_cluster": 0.13527028262615204, "score_of_first_answer": 0.13527028262615204, "max_score_of_first_answer": 0.9860446453094482, "min_score_of_first_answer": 0.041604042053222656, "avg_score": 0.13527028262615204, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.39232303395700613, "std_of_first_answer": 0.39232303395700613, "avg_entropy_of_first_answer": 2.5248215962201357, "lex_sim": 1.0}
{"question": "Wicket-keeper Matt Prior, currently part of the England team plays for which county?", "answer": ["Yorkshire County Cricket Club"], "scores": [[0.02283530868589878, 0.8886544108390808, 0.15523454546928406, 0.9597524404525757, 0.998006284236908, 0.9809826016426086]], "normalized_score": [0.3789195716381073], "tokens": [["\u2581York", "shire", "\u2581County", "\u2581Cr", "icket", "\u2581Club"]], "entropy": [[5.326992988586426, 0.7534974217414856, 2.219855308532715, 0.2888164818286896, 0.01992594450712204, 0.14362269639968872]], "avg_entropy": [1.458785140266021], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sussex County Council", "score_of_biggest_cluster": 0.3789195716381073, "score_of_first_answer": 0.3789195716381073, "max_score_of_first_answer": 0.998006284236908, "min_score_of_first_answer": 0.02283530868589878, "avg_score": 0.3789195716381073, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4122771987874635, "std_of_first_answer": 0.4122771987874635, "avg_entropy_of_first_answer": 1.458785140266021, "lex_sim": 1.0}
{"question": "\"What entertainment venue in New York City's Rockefeller Center, nicknamed \"\"the Showplace of the Nation\"\", opened to the public in December 1932?\"", "answer": ["Ice Rink"], "scores": [[0.015306614339351654, 0.24304534494876862, 0.9774169921875]], "normalized_score": [0.1537737250328064], "tokens": [["\u2581Ice", "\u2581R", "ink"]], "entropy": [[3.183927297592163, 2.667572021484375, 0.1608618050813675]], "avg_entropy": [2.0041203747193017], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "RCMH", "score_of_biggest_cluster": 0.1537737250328064, "score_of_first_answer": 0.1537737250328064, "max_score_of_first_answer": 0.9774169921875, "min_score_of_first_answer": 0.015306614339351654, "avg_score": 0.1537737250328064, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4105312310288846, "std_of_first_answer": 0.4105312310288846, "avg_entropy_of_first_answer": 2.0041203747193017, "lex_sim": 1.0}
{"question": "Once the longest street in the world Yonge (pronounced Young) Street has its southern end in which city? The ice hockey Hall of Fame stands on a corner with Front Street.", "answer": ["Toronto"], "scores": [[0.40636515617370605]], "normalized_score": [0.40636515617370605], "tokens": [["\u2581Toronto"]], "entropy": [[4.188647270202637]], "avg_entropy": [4.188647270202637], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Torontonians", "score_of_biggest_cluster": 0.40636515617370605, "score_of_first_answer": 0.40636515617370605, "max_score_of_first_answer": 0.40636515617370605, "min_score_of_first_answer": 0.40636515617370605, "avg_score": 0.40636515617370605, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 4.188647270202637, "lex_sim": 1.0}
{"question": "What ice cream company produces flavors such as Everything But The.., Neapolitan Dynamite, and Stephen Colbert's AmeriCone Dream?", "answer": ["Talenti"], "scores": [[0.0019563690293580294, 0.7133763432502747]], "normalized_score": [0.03735809400677681], "tokens": [["\u2581Tal", "enti"]], "entropy": [[2.2498950958251953, 1.6097939014434814]], "avg_entropy": [1.9298444986343384], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Free Cone Day", "score_of_biggest_cluster": 0.03735809400677681, "score_of_first_answer": 0.03735809400677681, "max_score_of_first_answer": 0.7133763432502747, "min_score_of_first_answer": 0.0019563690293580294, "avg_score": 0.03735809400677681, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3557099871104583, "std_of_first_answer": 0.3557099871104583, "avg_entropy_of_first_answer": 1.9298444986343384, "lex_sim": 1.0}
{"question": "In April 1966, Bobbi Gibb became the first woman to run the entire distance of which marathon?", "answer": ["Boston Marathon"], "scores": [[0.5119622349739075, 0.6314394474029541, 0.9934006929397583]], "normalized_score": [0.6848015189170837], "tokens": [["\u2581Boston", "\u2581Mar", "athon"]], "entropy": [[3.369844436645508, 1.1834921836853027, 0.05730391666293144]], "avg_entropy": [1.536880178997914], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Boston marathon", "score_of_biggest_cluster": 0.6848015189170837, "score_of_first_answer": 0.6848015189170837, "max_score_of_first_answer": 0.9934006929397583, "min_score_of_first_answer": 0.5119622349739075, "avg_score": 0.6848015189170837, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.20468776665635732, "std_of_first_answer": 0.20468776665635732, "avg_entropy_of_first_answer": 1.536880178997914, "lex_sim": 1.0}
{"question": "What was set at 3 groats per household in 1380 and caused social unrest?", "answer": ["Slang for 3 grams"], "scores": [[0.0012930074008181691, 0.005428043659776449, 0.055868420749902725, 0.03574443235993385, 0.408012330532074, 0.008480917662382126, 0.6326168775558472]], "normalized_score": [0.03148656710982323], "tokens": [["\u2581S", "lang", "\u2581for", "\u2581", "3", "\u2581gr", "ams"]], "entropy": [[3.4353585243225098, 5.460488319396973, 3.5135140419006348, 5.396945953369141, 2.004843235015869, 4.551478385925293, 2.0728259086608887]], "avg_entropy": [3.77649348122733], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Poll tax", "score_of_biggest_cluster": 0.03148656710982323, "score_of_first_answer": 0.03148656710982323, "max_score_of_first_answer": 0.6326168775558472, "min_score_of_first_answer": 0.0012930074008181691, "avg_score": 0.03148656710982323, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23393848276707266, "std_of_first_answer": 0.23393848276707266, "avg_entropy_of_first_answer": 3.77649348122733, "lex_sim": 1.0}
{"question": "Psychologists says men who wear what during sex are insecure?", "answer": ["Fingernails"], "scores": [[0.013966029509902, 0.0027910510543733835, 0.5868913531303406, 0.3722642958164215]], "normalized_score": [0.05402098596096039], "tokens": [["\u2581F", "ing", "ern", "ails"]], "entropy": [[5.937849044799805, 4.430574417114258, 2.1278116703033447, 0.7115951776504517]], "avg_entropy": [3.3019575774669647], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tube sock", "score_of_biggest_cluster": 0.05402098596096039, "score_of_first_answer": 0.05402098596096039, "max_score_of_first_answer": 0.5868913531303406, "min_score_of_first_answer": 0.0027910510543733835, "avg_score": 0.05402098596096039, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2475497891130676, "std_of_first_answer": 0.2475497891130676, "avg_entropy_of_first_answer": 3.3019575774669647, "lex_sim": 1.0}
{"question": "If you had a seal point and/or a blue point, what breed of cat do you have?", "answer": ["Point Siamese mix"], "scores": [[0.020600490272045135, 0.058544740080833435, 0.9993908405303955, 0.996360719203949, 0.0002321297797607258]], "normalized_score": [0.04887082427740097], "tokens": [["\u2581Point", "\u2581S", "iam", "ese", "\u2581mix"]], "entropy": [[4.606767654418945, 2.963655948638916, 0.007855471223592758, 0.03519828990101814, 1.9704885482788086]], "avg_entropy": [1.9167931824922562], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Siamese", "score_of_biggest_cluster": 0.04887082427740097, "score_of_first_answer": 0.04887082427740097, "max_score_of_first_answer": 0.9993908405303955, "min_score_of_first_answer": 0.0002321297797607258, "avg_score": 0.04887082427740097, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4762639286718686, "std_of_first_answer": 0.4762639286718686, "avg_entropy_of_first_answer": 1.9167931824922562, "lex_sim": 1.0}
{"question": "Sofia Helin returned on BBC4 in the third series of which Scandinavian drama?", "answer": ["The Bridge"], "scores": [[0.1887175440788269, 0.5385099053382874]], "normalized_score": [0.3187887370586395], "tokens": [["\u2581The", "\u2581Bridge"]], "entropy": [[5.294729232788086, 2.775144577026367]], "avg_entropy": [4.034936904907227], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Bridge (documentary film)", "score_of_biggest_cluster": 0.3187887370586395, "score_of_first_answer": 0.3187887370586395, "max_score_of_first_answer": 0.5385099053382874, "min_score_of_first_answer": 0.1887175440788269, "avg_score": 0.3187887370586395, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17489618062973022, "std_of_first_answer": 0.17489618062973022, "avg_entropy_of_first_answer": 4.034936904907227, "lex_sim": 1.0}
{"question": "In which country is the city of Kandahar?", "answer": ["Afghanistan (country)"], "scores": [[0.5145335793495178, 0.9280354380607605, 0.028988322243094444, 0.24017542600631714, 0.9722586870193481]], "normalized_score": [0.31761598587036133], "tokens": [["\u2581Afghan", "istan", "\u2581(", "country", ")"]], "entropy": [[3.5170400142669678, 0.5429834127426147, 0.7034134864807129, 4.561511039733887, 0.22294199466705322]], "avg_entropy": [1.909577989578247], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Afghanistan", "score_of_biggest_cluster": 0.31761598587036133, "score_of_first_answer": 0.31761598587036133, "max_score_of_first_answer": 0.9722586870193481, "min_score_of_first_answer": 0.028988322243094444, "avg_score": 0.31761598587036133, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3712260948619856, "std_of_first_answer": 0.3712260948619856, "avg_entropy_of_first_answer": 1.909577989578247, "lex_sim": 1.0}
{"question": "The works of artists De Chirico, Andr Breton, Mir, Magritte, Dal and Ernst are examples of what art movement?", "answer": ["Surrealism"], "scores": [[0.5233796238899231, 0.9781264662742615, 0.8793008923530579]], "normalized_score": [0.7663899064064026], "tokens": [["\u2581Sur", "real", "ism"]], "entropy": [[3.72831654548645, 0.17836539447307587, 0.5564618110656738]], "avg_entropy": [1.4877145836750667], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Surrealist writing", "score_of_biggest_cluster": 0.7663899064064026, "score_of_first_answer": 0.7663899064064026, "max_score_of_first_answer": 0.9781264662742615, "min_score_of_first_answer": 0.5233796238899231, "avg_score": 0.7663899064064026, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1952892823092588, "std_of_first_answer": 0.1952892823092588, "avg_entropy_of_first_answer": 1.4877145836750667, "lex_sim": 1.0}
{"question": "What country dominated the East Indies spice trade in the 17th and 18th centuries?", "answer": ["VOC"], "scores": [[0.011547346599400043, 0.901024580001831]], "normalized_score": [0.10200218111276627], "tokens": [["\u2581V", "OC"]], "entropy": [[5.138967037200928, 0.8781498670578003]], "avg_entropy": [3.008558452129364], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "NETHERLANDS", "score_of_biggest_cluster": 0.10200218111276627, "score_of_first_answer": 0.10200218111276627, "max_score_of_first_answer": 0.901024580001831, "min_score_of_first_answer": 0.011547346599400043, "avg_score": 0.10200218111276627, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4447386167012155, "std_of_first_answer": 0.4447386167012155, "avg_entropy_of_first_answer": 3.008558452129364, "lex_sim": 1.0}
{"question": "Who was the Whig Prime Minister of Great Britain from 1835 to 1841?", "answer": ["Pevensey"], "scores": [[0.012479633092880249, 0.0002156193513656035, 0.7077662944793701]], "normalized_score": [0.01239538099616766], "tokens": [["\u2581Pe", "ven", "sey"]], "entropy": [[5.360466480255127, 0.3630545139312744, 1.9137769937515259]], "avg_entropy": [2.545765995979309], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "VISCOUNT MELBOURNE", "score_of_biggest_cluster": 0.01239538099616766, "score_of_first_answer": 0.01239538099616766, "max_score_of_first_answer": 0.7077662944793701, "min_score_of_first_answer": 0.0002156193513656035, "avg_score": 0.01239538099616766, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3306898354465705, "std_of_first_answer": 0.3306898354465705, "avg_entropy_of_first_answer": 2.545765995979309, "lex_sim": 1.0}
{"question": "In which county did Eccles cakes originate", "answer": ["England\u00e8tres"], "scores": [[0.008666280657052994, 2.6637185790434614e-09]], "normalized_score": [4.804634045285638e-06], "tokens": [["\u2581England", "\u00e8tres"]], "entropy": [[5.682842254638672, 1.4577276706695557]], "avg_entropy": [3.5702849626541138], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lancastershire", "score_of_biggest_cluster": 4.804634045285638e-06, "score_of_first_answer": 4.804634045285638e-06, "max_score_of_first_answer": 0.008666280657052994, "min_score_of_first_answer": 2.6637185790434614e-09, "avg_score": 4.804634045285638e-06, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.004333138996667207, "std_of_first_answer": 0.004333138996667207, "avg_entropy_of_first_answer": 3.5702849626541138, "lex_sim": 1.0}
{"question": "The longest running television series ever, November 6, 1947 saw the debut of what now weekly news program, now hosted by David Gregory, following the unfortunante sudden death of host Tim Russert?", "answer": ["Meet the Press"], "scores": [[0.45469045639038086, 0.9928539395332336, 0.8924223184585571, 0.9299010634422302]], "normalized_score": [0.7823518514633179], "tokens": [["\u2581Me", "et", "\u2581the", "\u2581Press"]], "entropy": [[3.863042116165161, 0.06663225591182709, 0.4299832582473755, 0.5797483921051025]], "avg_entropy": [1.2348515056073666], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Meet the Press with David Gregory", "score_of_biggest_cluster": 0.7823518514633179, "score_of_first_answer": 0.7823518514633179, "max_score_of_first_answer": 0.9928539395332336, "min_score_of_first_answer": 0.45469045639038086, "avg_score": 0.7823518514633179, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.212501245607831, "std_of_first_answer": 0.212501245607831, "avg_entropy_of_first_answer": 1.2348515056073666, "lex_sim": 1.0}
{"question": "If you dress in clothes of the opposite sex, you are a what (no rude answers please!)?", "answer": ["Tomboy"], "scores": [[0.008246525190770626, 0.6938373446464539]], "normalized_score": [0.07564224302768707], "tokens": [["\u2581Tom", "boy"]], "entropy": [[4.394540786743164, 1.9555693864822388]], "avg_entropy": [3.1750550866127014], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Transvestites", "score_of_biggest_cluster": 0.07564224302768707, "score_of_first_answer": 0.07564224302768707, "max_score_of_first_answer": 0.6938373446464539, "min_score_of_first_answer": 0.008246525190770626, "avg_score": 0.07564224302768707, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3427954097278416, "std_of_first_answer": 0.3427954097278416, "avg_entropy_of_first_answer": 3.1750550866127014, "lex_sim": 1.0}
{"question": "Which car company invented the three point seatbelt system, but refused to patent it, allowing all other car companies to use it free of charge in an effort to increase road safety?", "answer": ["Volvo"], "scores": [[0.4641364514827728, 0.9590097665786743]], "normalized_score": [0.6671667098999023], "tokens": [["\u2581Vol", "vo"]], "entropy": [[3.8242692947387695, 0.38917088508605957]], "avg_entropy": [2.1067200899124146], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Glossary of Volvo features", "score_of_biggest_cluster": 0.6671667098999023, "score_of_first_answer": 0.6671667098999023, "max_score_of_first_answer": 0.9590097665786743, "min_score_of_first_answer": 0.4641364514827728, "avg_score": 0.6671667098999023, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24743665754795074, "std_of_first_answer": 0.24743665754795074, "avg_entropy_of_first_answer": 2.1067200899124146, "lex_sim": 1.0}
{"question": "In Greek mythology, which virgin goddess of the hearth, home and cooking gave up her seat as one of the Twelve Olympians to tend to the sacred flame on Mount Olympus, for Dionysus?", "answer": ["Hestia"], "scores": [[0.4406866133213043, 0.9355299472808838, 0.9796573519706726]], "normalized_score": [0.7391863465309143], "tokens": [["\u2581H", "est", "ia"]], "entropy": [[3.851382255554199, 0.47476041316986084, 0.18378984928131104]], "avg_entropy": [1.5033108393351238], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "HESTIA", "score_of_biggest_cluster": 0.7391863465309143, "score_of_first_answer": 0.7391863465309143, "max_score_of_first_answer": 0.9796573519706726, "min_score_of_first_answer": 0.4406866133213043, "avg_score": 0.7391863465309143, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2443373374226556, "std_of_first_answer": 0.2443373374226556, "avg_entropy_of_first_answer": 1.5033108393351238, "lex_sim": 1.0}
{"question": "In Milton's 'Paradise Lost' what is the name of the Capital of Hell?", "answer": ["Pandemonium"], "scores": [[0.3193739056587219, 0.9784286618232727, 0.9535709023475647, 0.9742982983589172]], "normalized_score": [0.7340376973152161], "tokens": [["\u2581Pan", "dem", "on", "ium"]], "entropy": [[4.6619110107421875, 0.22152888774871826, 0.3498690128326416, 0.22497862577438354]], "avg_entropy": [1.3645718842744827], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pand\u00e6monium (disambiguation)", "score_of_biggest_cluster": 0.7340376973152161, "score_of_first_answer": 0.7340376973152161, "max_score_of_first_answer": 0.9784286618232727, "min_score_of_first_answer": 0.3193739056587219, "avg_score": 0.7340376973152161, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28135270962795683, "std_of_first_answer": 0.28135270962795683, "avg_entropy_of_first_answer": 1.3645718842744827, "lex_sim": 1.0}
{"question": "Of what does the usual base of a Florentine biscuit consist?", "answer": ["Almond"], "scores": [[0.22433626651763916, 0.9805610775947571]], "normalized_score": [0.4690153896808624], "tokens": [["\u2581Al", "mond"]], "entropy": [[5.297579288482666, 0.17266950011253357]], "avg_entropy": [2.7351243942976], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Operation: ZERO", "score_of_biggest_cluster": 0.4690153896808624, "score_of_first_answer": 0.4690153896808624, "max_score_of_first_answer": 0.9805610775947571, "min_score_of_first_answer": 0.22433626651763916, "avg_score": 0.4690153896808624, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37811240553855896, "std_of_first_answer": 0.37811240553855896, "avg_entropy_of_first_answer": 2.7351243942976, "lex_sim": 1.0}
{"question": "Roman Emperor Vespasian came to power in 69AD - the Year of the Four Emperors. Name any of the other three Emperors that year.", "answer": ["Emperor Nero"], "scores": [[0.023798035457730293, 0.08911727368831635, 0.9354698657989502]], "normalized_score": [0.12565436959266663], "tokens": [["\u2581Emperor", "\u2581N", "ero"]], "entropy": [[5.073319435119629, 2.9887263774871826, 0.3804647922515869]], "avg_entropy": [2.8141702016194663], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Galba, Otho, Vitellius", "score_of_biggest_cluster": 0.12565436959266663, "score_of_first_answer": 0.12565436959266663, "max_score_of_first_answer": 0.9354698657989502, "min_score_of_first_answer": 0.023798035457730293, "avg_score": 0.12565436959266663, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4152274931077773, "std_of_first_answer": 0.4152274931077773, "avg_entropy_of_first_answer": 2.8141702016194663, "lex_sim": 1.0}
{"question": "What is the short metonymic name of the chief usher of the Lord Chamberlain's department of the UK House of Lords?", "answer": ["Black Rod"], "scores": [[0.33802908658981323, 0.9379794597625732]], "normalized_score": [0.5630846619606018], "tokens": [["\u2581Black", "\u2581Rod"]], "entropy": [[4.570069313049316, 0.39936724305152893]], "avg_entropy": [2.4847182780504227], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gentleman Usher of Black Rod", "score_of_biggest_cluster": 0.5630846619606018, "score_of_first_answer": 0.5630846619606018, "max_score_of_first_answer": 0.9379794597625732, "min_score_of_first_answer": 0.33802908658981323, "avg_score": 0.5630846619606018, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29997518658638, "std_of_first_answer": 0.29997518658638, "avg_entropy_of_first_answer": 2.4847182780504227, "lex_sim": 1.0}
{"question": "Generally the standard steps per minute for a military 'quick march' pace is?", "answer": ["180"], "scores": [[0.623638391494751, 0.8207153677940369, 0.05994800478219986, 0.7756978869438171]], "normalized_score": [0.3927789330482483], "tokens": [["\u2581", "1", "8", "0"]], "entropy": [[3.009000301361084, 0.9732515215873718, 1.808882713317871, 0.9580636024475098]], "avg_entropy": [1.6872995346784592], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "120", "score_of_biggest_cluster": 0.3927789330482483, "score_of_first_answer": 0.3927789330482483, "max_score_of_first_answer": 0.8207153677940369, "min_score_of_first_answer": 0.05994800478219986, "avg_score": 0.3927789330482483, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3033974007836187, "std_of_first_answer": 0.3033974007836187, "avg_entropy_of_first_answer": 1.6872995346784592, "lex_sim": 1.0}
{"question": "Astigmatism is a condition affecting which part of the body?", "answer": ["Cornea"], "scores": [[0.017041919752955437, 0.9798392057418823]], "normalized_score": [0.1292220652103424], "tokens": [["\u2581Cor", "nea"]], "entropy": [[3.979902744293213, 0.12608474493026733]], "avg_entropy": [2.05299374461174], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Eye (anatomy)", "score_of_biggest_cluster": 0.1292220652103424, "score_of_first_answer": 0.1292220652103424, "max_score_of_first_answer": 0.9798392057418823, "min_score_of_first_answer": 0.017041919752955437, "avg_score": 0.1292220652103424, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.48139864299446344, "std_of_first_answer": 0.48139864299446344, "avg_entropy_of_first_answer": 2.05299374461174, "lex_sim": 1.0}
{"question": "Which American president was assassinated by Charles Guiteau in 1881?", "answer": ["Garfield\u02c7e\u017e\u02dd-\u02dd\u02dd\u00b4!"], "scores": [[0.06696584820747375, 0.8967922329902649, 6.636290184047766e-09, 0.0019220435060560703, 0.013021481223404408, 0.030544284731149673, 0.36771461367607117, 0.0012797403614968061, 0.12826795876026154, 0.8413661122322083, 0.21441325545310974, 0.9494112730026245, 0.01258432399481535, 0.0009544581989757717]], "normalized_score": [0.015280305407941341], "tokens": [["\u2581Gar", "field", "\u02c7", "e", "\u017e", "<0xCB>", "<0x9D>", "-", "<0xCB>", "<0x9D>", "<0xCB>", "<0x9D>", "\u00b4", "!"]], "entropy": [[3.856595754623413, 0.8314347863197327, 2.23237943649292, 2.528728485107422, 5.465248107910156, 4.982231140136719, 2.3482680320739746, 4.708276748657227, 5.746991157531738, 0.9440129995346069, 4.664308547973633, 0.3878558576107025, 3.6102254390716553, 3.0018248558044434]], "avg_entropy": [3.2363129534891675], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Presidency of James Garfield", "score_of_biggest_cluster": 0.015280305407941341, "score_of_first_answer": 0.015280305407941341, "max_score_of_first_answer": 0.9494112730026245, "min_score_of_first_answer": 6.636290184047766e-09, "avg_score": 0.015280305407941341, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3514502599421835, "std_of_first_answer": 0.3514502599421835, "avg_entropy_of_first_answer": 3.2363129534891675, "lex_sim": 1.0}
{"question": "Which of the continents is the smallest in area?", "answer": ["Oceania"], "scores": [[0.08895179629325867, 0.9288737773895264, 0.9165050983428955]], "normalized_score": [0.4230731129646301], "tokens": [["\u2581O", "ce", "ania"]], "entropy": [[4.861822605133057, 0.6379757523536682, 0.4090474843978882]], "avg_entropy": [1.9696152806282043], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Australia (Commonwealth realm)", "score_of_biggest_cluster": 0.4230731129646301, "score_of_first_answer": 0.4230731129646301, "max_score_of_first_answer": 0.9288737773895264, "min_score_of_first_answer": 0.08895179629325867, "avg_score": 0.4230731129646301, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.393060129169302, "std_of_first_answer": 0.393060129169302, "avg_entropy_of_first_answer": 1.9696152806282043, "lex_sim": 1.0}
{"question": "Snail porridge is a dish associated with which famous chef?", "answer": ["Wii Hacking"], "scores": [[0.006928907707333565, 0.004506512079387903, 0.008909464813768864, 0.15537476539611816, 0.2257290929555893]], "normalized_score": [0.02499568462371826], "tokens": [["\u2581W", "ii", "\u2581H", "ack", "ing"]], "entropy": [[5.447328567504883, 4.835892200469971, 5.388931751251221, 4.7267961502075195, 3.2974560260772705]], "avg_entropy": [4.739280939102173], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Big Fat Duck Cookbook", "score_of_biggest_cluster": 0.02499568462371826, "score_of_first_answer": 0.02499568462371826, "max_score_of_first_answer": 0.2257290929555893, "min_score_of_first_answer": 0.004506512079387903, "avg_score": 0.02499568462371826, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09274741986845574, "std_of_first_answer": 0.09274741986845574, "avg_entropy_of_first_answer": 4.739280939102173, "lex_sim": 1.0}
{"question": "Hedonophobia is the irrational fear of what?", "answer": ["Pleasure"], "scores": [[0.35744231939315796, 0.962878406047821]], "normalized_score": [0.586663007736206], "tokens": [["\u2581Ple", "asure"]], "entropy": [[4.254517555236816, 0.2125956416130066]], "avg_entropy": [2.2335565984249115], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pleasant", "score_of_biggest_cluster": 0.586663007736206, "score_of_first_answer": 0.586663007736206, "max_score_of_first_answer": 0.962878406047821, "min_score_of_first_answer": 0.35744231939315796, "avg_score": 0.586663007736206, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30271804332733154, "std_of_first_answer": 0.30271804332733154, "avg_entropy_of_first_answer": 2.2335565984249115, "lex_sim": 1.0}
{"question": "In American football and other sports, who or what is a M.V.P.?", "answer": ["A Most Valuable Player"], "scores": [[0.0200338214635849, 0.5072640180587769, 0.9498774409294128, 0.9527941346168518, 0.9989452958106995, 0.8545970916748047]], "normalized_score": [0.4458217918872833], "tokens": [["\u2581A", "\u2581Most", "\u2581Val", "u", "able", "\u2581Player"]], "entropy": [[3.5721852779388428, 3.7298636436462402, 0.4053966999053955, 0.26253920793533325, 0.011579509824514389, 0.7376543283462524]], "avg_entropy": [1.4532031112660964], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "MVP", "score_of_biggest_cluster": 0.4458217918872833, "score_of_first_answer": 0.4458217918872833, "max_score_of_first_answer": 0.9989452958106995, "min_score_of_first_answer": 0.0200338214635849, "avg_score": 0.4458217918872833, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.35070190407185, "std_of_first_answer": 0.35070190407185, "avg_entropy_of_first_answer": 1.4532031112660964, "lex_sim": 1.0}
{"question": "Whole Again and Eternal Flame were Number Ones for which girl group in 2001?", "answer": ["Atomic kittens"], "scores": [[0.4977438151836395, 0.9994422793388367, 0.021711859852075577, 0.1781875640153885, 0.9501037001609802]], "normalized_score": [0.2834143340587616], "tokens": [["\u2581At", "omic", "\u2581k", "itt", "ens"]], "entropy": [[3.453606605529785, 0.006419779732823372, 0.48932743072509766, 0.7215774059295654, 0.34767526388168335]], "avg_entropy": [1.003721297159791], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Atomic Kitten", "score_of_biggest_cluster": 0.2834143340587616, "score_of_first_answer": 0.2834143340587616, "max_score_of_first_answer": 0.9994422793388367, "min_score_of_first_answer": 0.021711859852075577, "avg_score": 0.2834143340587616, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.39497617077709585, "std_of_first_answer": 0.39497617077709585, "avg_entropy_of_first_answer": 1.003721297159791, "lex_sim": 1.0}
{"question": "Staten Island, New York, is at the mouth of which river?", "answer": ["Ohio, the longest river in North America"], "scores": [[0.0018787673907354474, 0.003982722759246826, 0.035636357963085175, 0.027617383748292923, 0.5499200820922852, 0.8042625784873962, 0.1761639565229416, 0.9901566505432129]], "normalized_score": [0.06987246125936508], "tokens": [["\u2581Ohio", ",", "\u2581the", "\u2581longest", "\u2581river", "\u2581in", "\u2581North", "\u2581America"]], "entropy": [[4.192829608917236, 1.4912998676300049, 5.452274799346924, 5.557844161987305, 1.7906904220581055, 0.9785550236701965, 1.535287857055664, 0.08563543856143951]], "avg_entropy": [2.6355521474033594], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hudson (disambiguation)", "score_of_biggest_cluster": 0.06987246125936508, "score_of_first_answer": 0.06987246125936508, "max_score_of_first_answer": 0.9901566505432129, "min_score_of_first_answer": 0.0018787673907354474, "avg_score": 0.06987246125936508, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3749134715427803, "std_of_first_answer": 0.3749134715427803, "avg_entropy_of_first_answer": 2.6355521474033594, "lex_sim": 1.0}
{"question": "Who was the founder of Sikhism?", "answer": ["Guru."], "scores": [[0.375567764043808, 0.8519169092178345, 0.0008843158138915896]], "normalized_score": [0.06564944982528687], "tokens": [["\u2581G", "uru", "."]], "entropy": [[4.21953010559082, 0.886985182762146, 1.9606484174728394]], "avg_entropy": [2.3557212352752686], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Guru Nanak Dev ji", "score_of_biggest_cluster": 0.06564944982528687, "score_of_first_answer": 0.06564944982528687, "max_score_of_first_answer": 0.8519169092178345, "min_score_of_first_answer": 0.0008843158138915896, "avg_score": 0.06564944982528687, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.34825799337102675, "std_of_first_answer": 0.34825799337102675, "avg_entropy_of_first_answer": 2.3557212352752686, "lex_sim": 1.0}
{"question": "With atomic number 50 which element in the periodic table has the shortest name?", "answer": ["O"], "scores": [[0.004118266049772501]], "normalized_score": [0.004118266515433788], "tokens": [["\u2581O"]], "entropy": [[6.126204013824463]], "avg_entropy": [6.126204013824463], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "TINs", "score_of_biggest_cluster": 0.004118266515433788, "score_of_first_answer": 0.004118266515433788, "max_score_of_first_answer": 0.004118266049772501, "min_score_of_first_answer": 0.004118266049772501, "avg_score": 0.004118266515433788, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 6.126204013824463, "lex_sim": 1.0}
{"question": "Which League 2 football team play home games at the New York Stadium?", "answer": ["Hound dog"], "scores": [[0.003381395945325494, 0.006189359351992607, 0.012060485780239105]], "normalized_score": [0.006319784093648195], "tokens": [["\u2581H", "ound", "\u2581dog"]], "entropy": [[4.476487159729004, 5.395062446594238, 5.043663024902344]], "avg_entropy": [4.971737543741862], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rotherham FC", "score_of_biggest_cluster": 0.006319784093648195, "score_of_first_answer": 0.006319784093648195, "max_score_of_first_answer": 0.012060485780239105, "min_score_of_first_answer": 0.003381395945325494, "avg_score": 0.006319784093648195, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0036160350353535145, "std_of_first_answer": 0.0036160350353535145, "avg_entropy_of_first_answer": 4.971737543741862, "lex_sim": 1.0}
{"question": "In Greek mythology, who did flute playing shepherd Marsyas challenge to a music contest, the winner of which flayed Marsyas to death for his presumption?", "answer": ["Apollo"], "scores": [[0.3902648985385895, 0.9908201098442078]], "normalized_score": [0.621837854385376], "tokens": [["\u2581Apol", "lo"]], "entropy": [[4.080824851989746, 0.09216276556253433]], "avg_entropy": [2.08649380877614], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Apollo Culicarius", "score_of_biggest_cluster": 0.621837854385376, "score_of_first_answer": 0.621837854385376, "max_score_of_first_answer": 0.9908201098442078, "min_score_of_first_answer": 0.3902648985385895, "avg_score": 0.621837854385376, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.30027760565280914, "std_of_first_answer": 0.30027760565280914, "avg_entropy_of_first_answer": 2.08649380877614, "lex_sim": 1.0}
{"question": "How many US Presidents have been impeached?", "answer": ["2 and Closing to 3 after the new elections"], "scores": [[0.23400002717971802, 0.4908810257911682, 0.002485807752236724, 0.014261010102927685, 0.00011135356908198446, 0.0026935588102787733, 0.5079305768013, 0.580101490020752, 0.001425882917828858, 0.2361757457256317, 0.00313879968598485, 0.045062657445669174]], "normalized_score": [0.022533949464559555], "tokens": [["\u2581", "2", "\u2581and", "\u2581Cl", "osing", "\u2581to", "\u2581", "3", "\u2581after", "\u2581the", "\u2581new", "\u2581elections"]], "entropy": [[4.498101234436035, 2.087416410446167, 2.8187551498413086, 3.45947003364563, 0.2583805024623871, 3.293801784515381, 3.76751446723938, 1.4747520685195923, 1.7132787704467773, 3.889002561569214, 4.43170166015625, 4.664025783538818]], "avg_entropy": [3.0296833689014115], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2", "score_of_biggest_cluster": 0.022533949464559555, "score_of_first_answer": 0.022533949464559555, "max_score_of_first_answer": 0.580101490020752, "min_score_of_first_answer": 0.00011135356908198446, "avg_score": 0.022533949464559555, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21877464640847266, "std_of_first_answer": 0.21877464640847266, "avg_entropy_of_first_answer": 3.0296833689014115, "lex_sim": 1.0}
{"question": "Which art movement was pioneered by Pablo Picasso and George Braque?", "answer": ["Cubism"], "scores": [[0.34047460556030273, 0.8547704815864563]], "normalized_score": [0.5394697785377502], "tokens": [["\u2581Cub", "ism"]], "entropy": [[4.695739269256592, 0.559981644153595]], "avg_entropy": [2.6278604567050934], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Analytical Cubism", "score_of_biggest_cluster": 0.5394697785377502, "score_of_first_answer": 0.5394697785377502, "max_score_of_first_answer": 0.8547704815864563, "min_score_of_first_answer": 0.34047460556030273, "avg_score": 0.5394697785377502, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2571479380130768, "std_of_first_answer": 0.2571479380130768, "avg_entropy_of_first_answer": 2.6278604567050934, "lex_sim": 1.0}
{"question": "What common chemical compound type, including many natural fats and essential oils, results from replacing the hydrogen of an acid by an alkyl or other organic group?", "answer": ["Methane"], "scores": [[0.012162400409579277, 0.8428528308868408, 0.33892592787742615]], "normalized_score": [0.15145792067050934], "tokens": [["\u2581M", "eth", "ane"]], "entropy": [[4.384708404541016, 1.2266969680786133, 1.3761824369430542]], "avg_entropy": [2.3291959365208945], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Diester", "score_of_biggest_cluster": 0.15145792067050934, "score_of_first_answer": 0.15145792067050934, "max_score_of_first_answer": 0.8428528308868408, "min_score_of_first_answer": 0.012162400409579277, "avg_score": 0.15145792067050934, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3416891566169443, "std_of_first_answer": 0.3416891566169443, "avg_entropy_of_first_answer": 2.3291959365208945, "lex_sim": 1.0}
{"question": "What was Roy Castle\u2019s only Carry On film?", "answer": ["Carry On Regardless"], "scores": [[0.4056323766708374, 0.9937776327133179, 0.5210539102554321, 0.04050876945257187, 0.9809398055076599, 0.9912978410720825]], "normalized_score": [0.4497280716896057], "tokens": [["\u2581Car", "ry", "\u2581On", "\u2581Reg", "ard", "less"]], "entropy": [[4.523215293884277, 0.06781382858753204, 1.0005886554718018, 4.2764739990234375, 0.1614362597465515, 0.0770673006772995]], "avg_entropy": [1.6844325562318165], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "3rd Foot and Mouth Regiment", "score_of_biggest_cluster": 0.4497280716896057, "score_of_first_answer": 0.4497280716896057, "max_score_of_first_answer": 0.9937776327133179, "min_score_of_first_answer": 0.04050876945257187, "avg_score": 0.4497280716896057, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36327831396514576, "std_of_first_answer": 0.36327831396514576, "avg_entropy_of_first_answer": 1.6844325562318165, "lex_sim": 1.0}
{"question": "How many basic positions are there in classical ballet?", "answer": ["Fi"], "scores": [[0.013716460205614567, 0.07139512896537781]], "normalized_score": [0.031293585896492004], "tokens": [["\u2581F", "i"]], "entropy": [[4.2264580726623535, 5.131551742553711]], "avg_entropy": [4.679004907608032], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "five", "score_of_biggest_cluster": 0.031293585896492004, "score_of_first_answer": 0.031293585896492004, "max_score_of_first_answer": 0.07139512896537781, "min_score_of_first_answer": 0.013716460205614567, "avg_score": 0.031293585896492004, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.02883933437988162, "std_of_first_answer": 0.02883933437988162, "avg_entropy_of_first_answer": 4.679004907608032, "lex_sim": 1.0}
{"question": "Which town is the capital of Papua New Guinea?", "answer": ["Port Moresby"], "scores": [[0.32234036922454834, 0.8260764479637146, 0.9249588251113892, 0.9609060287475586]], "normalized_score": [0.6974844932556152], "tokens": [["\u2581Port", "\u2581M", "ores", "by"]], "entropy": [[4.96764612197876, 1.365722894668579, 0.5345209836959839, 0.3011069595813751]], "avg_entropy": [1.7922492399811745], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "PG-NCD", "score_of_biggest_cluster": 0.6974844932556152, "score_of_first_answer": 0.6974844932556152, "max_score_of_first_answer": 0.9609060287475586, "min_score_of_first_answer": 0.32234036922454834, "avg_score": 0.6974844932556152, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.25665079433626725, "std_of_first_answer": 0.25665079433626725, "avg_entropy_of_first_answer": 1.7922492399811745, "lex_sim": 1.0}
{"question": "Ivan Galamian has had world-wide influence as a teacher of what?", "answer": ["Violinists \u2013 Wikipedia, the free encyclopedia"], "scores": [[0.4501166045665741, 0.9555715322494507, 0.05025750771164894, 0.0002801605733111501, 0.00876060500741005, 0.26396486163139343, 0.9130576848983765, 0.99456387758255, 0.991738498210907, 0.9989218711853027, 0.9917833805084229]], "normalized_score": [0.19121545553207397], "tokens": [["\u2581Vi", "olin", "ists", "\u2581\u2013", "\u2581Wikipedia", ",", "\u2581the", "\u2581free", "\u2581en", "cyc", "lopedia"]], "entropy": [[4.013171195983887, 0.22394317388534546, 1.7072651386260986, 0.70624178647995, 6.2570271492004395, 1.5891799926757812, 0.6758918762207031, 0.048505429178476334, 0.075833261013031, 0.010943074710667133, 0.05542431026697159]], "avg_entropy": [1.396675126203759], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Violinist (disambiguation)", "score_of_biggest_cluster": 0.19121545553207397, "score_of_first_answer": 0.19121545553207397, "max_score_of_first_answer": 0.9989218711853027, "min_score_of_first_answer": 0.0002801605733111501, "avg_score": 0.19121545553207397, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4256551417216085, "std_of_first_answer": 0.4256551417216085, "avg_entropy_of_first_answer": 1.396675126203759, "lex_sim": 1.0}
{"question": "In 2009, who became the first woman to referee the World Snooker Championship final?", "answer": ["Eirian Rees"], "scores": [[0.04605593532323837, 0.35214757919311523, 0.9776095151901245, 0.005598297342658043, 0.9798160791397095]], "normalized_score": [0.15412573516368866], "tokens": [["\u2581E", "i", "rian", "\u2581Re", "es"]], "entropy": [[5.073131561279297, 3.4946348667144775, 0.2302483171224594, 1.0958242416381836, 0.16643798351287842]], "avg_entropy": [2.0120553940534593], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Michaela Tabb", "score_of_biggest_cluster": 0.15412573516368866, "score_of_first_answer": 0.15412573516368866, "max_score_of_first_answer": 0.9798160791397095, "min_score_of_first_answer": 0.005598297342658043, "avg_score": 0.15412573516368866, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43054415523305606, "std_of_first_answer": 0.43054415523305606, "avg_entropy_of_first_answer": 2.0120553940534593, "lex_sim": 1.0}
{"question": "What merchant bank collapsed in 1995 due to unauthorised debts accumulated by trader Nick Leeson?", "answer": ["Barings Investment Bank"], "scores": [[0.6138677000999451, 0.8213974237442017, 0.0057936981320381165, 0.9970531463623047, 0.9801282286643982, 0.9009499549865723]], "normalized_score": [0.3701527416706085], "tokens": [["\u2581Bar", "ings", "\u2581In", "vest", "ment", "\u2581Bank"]], "entropy": [[2.989912509918213, 0.6934453845024109, 1.2452325820922852, 0.02912658452987671, 0.12498205155134201, 0.5154595971107483]], "avg_entropy": [0.9330264516174793], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Barings Bank collapse", "score_of_biggest_cluster": 0.3701527416706085, "score_of_first_answer": 0.3701527416706085, "max_score_of_first_answer": 0.9970531463623047, "min_score_of_first_answer": 0.0057936981320381165, "avg_score": 0.3701527416706085, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3437062850773299, "std_of_first_answer": 0.3437062850773299, "avg_entropy_of_first_answer": 0.9330264516174793, "lex_sim": 1.0}
{"question": "8\u00c2\u00a0 Fort Ville-Marie was the original name of which Canadian city?", "answer": ["Montreal"], "scores": [[0.5170801877975464]], "normalized_score": [0.5170801877975464], "tokens": [["\u2581Montreal"]], "entropy": [[3.209505081176758]], "avg_entropy": [3.209505081176758], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Motnreal", "score_of_biggest_cluster": 0.5170801877975464, "score_of_first_answer": 0.5170801877975464, "max_score_of_first_answer": 0.5170801877975464, "min_score_of_first_answer": 0.5170801877975464, "avg_score": 0.5170801877975464, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.209505081176758, "lex_sim": 1.0}
{"question": "In what South American country was unrepentant asshat Che Guevara and his band of merry men captured and ultimately executed?", "answer": ["Bolivia"], "scores": [[0.31043314933776855, 0.8939120173454285]], "normalized_score": [0.5267826318740845], "tokens": [["\u2581Boliv", "ia"]], "entropy": [[5.054802417755127, 0.5650437474250793]], "avg_entropy": [2.809923082590103], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Bolivia, Plurinational State of", "score_of_biggest_cluster": 0.5267826318740845, "score_of_first_answer": 0.5267826318740845, "max_score_of_first_answer": 0.8939120173454285, "min_score_of_first_answer": 0.31043314933776855, "avg_score": 0.5267826318740845, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.29173943400382996, "std_of_first_answer": 0.29173943400382996, "avg_entropy_of_first_answer": 2.809923082590103, "lex_sim": 1.0}
{"question": "In which country is the Eurovision Song Contest to be held later this month?", "answer": ["EU"], "scores": [[0.002508932026103139]], "normalized_score": [0.0025089324917644262], "tokens": [["\u2581EU"]], "entropy": [[5.119695663452148]], "avg_entropy": [5.119695663452148], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u0421\u044a\u0440\u0431\u0438\u044f", "score_of_biggest_cluster": 0.0025089324917644262, "score_of_first_answer": 0.0025089324917644262, "max_score_of_first_answer": 0.002508932026103139, "min_score_of_first_answer": 0.002508932026103139, "avg_score": 0.0025089324917644262, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 5.119695663452148, "lex_sim": 1.0}
{"question": "The bumblebee or hog-nosed is the smallest mammal in the world - what is it?", "answer": ["Shrew (family Soricidae)"], "scores": [[0.04761730134487152, 0.9998262524604797, 0.060534458607435226, 0.0046544247306883335, 0.4375345706939697, 0.9983532428741455, 0.9916397929191589, 0.9210337996482849]], "normalized_score": [0.21931107342243195], "tokens": [["\u2581Sh", "rew", "\u2581(", "family", "\u2581Sor", "ic", "idae", ")"]], "entropy": [[5.374541282653809, 0.0024308068677783012, 0.9693641662597656, 3.7187297344207764, 2.0225629806518555, 0.018197184428572655, 0.06846173107624054, 0.515883207321167]], "avg_entropy": [1.5862713867099956], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bat Conservation", "score_of_biggest_cluster": 0.21931107342243195, "score_of_first_answer": 0.21931107342243195, "max_score_of_first_answer": 0.9998262524604797, "min_score_of_first_answer": 0.0046544247306883335, "avg_score": 0.21931107342243195, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4384095047065227, "std_of_first_answer": 0.4384095047065227, "avg_entropy_of_first_answer": 1.5862713867099956, "lex_sim": 1.0}
{"question": "What was the only western that Brigitte Bardot appeared in (it was based on a book by Louis L'Amour)", "answer": ["Shalako"], "scores": [[0.2728201448917389, 0.9998430013656616, 0.9769229888916016]], "normalized_score": [0.6435112953186035], "tokens": [["\u2581Sh", "al", "ako"]], "entropy": [[4.483976364135742, 0.002060670405626297, 0.13424530625343323]], "avg_entropy": [1.5400941135982673], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Shalako", "score_of_biggest_cluster": 0.6435112953186035, "score_of_first_answer": 0.6435112953186035, "max_score_of_first_answer": 0.9998430013656616, "min_score_of_first_answer": 0.2728201448917389, "avg_score": 0.6435112953186035, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3374493173032735, "std_of_first_answer": 0.3374493173032735, "avg_entropy_of_first_answer": 1.5400941135982673, "lex_sim": 1.0}
{"question": "According to Greek myth, who built the labyrinth on Crete for King Minos?", "answer": ["Daedalus"], "scores": [[0.45234906673431396, 0.9611122608184814, 0.9045798778533936]], "normalized_score": [0.7326528429985046], "tokens": [["\u2581Da", "ed", "alus"]], "entropy": [[3.8981645107269287, 0.2861288785934448, 0.5556087493896484]], "avg_entropy": [1.5799673795700073], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Daedalus", "score_of_biggest_cluster": 0.7326528429985046, "score_of_first_answer": 0.7326528429985046, "max_score_of_first_answer": 0.9611122608184814, "min_score_of_first_answer": 0.45234906673431396, "avg_score": 0.7326528429985046, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.22768121098097183, "std_of_first_answer": 0.22768121098097183, "avg_entropy_of_first_answer": 1.5799673795700073, "lex_sim": 1.0}
{"question": "Flemish is almost identical in form to which other language ?", "answer": ["Afrikaans"], "scores": [[0.10612212866544724, 0.8977075815200806, 0.9694098830223083]], "normalized_score": [0.4520114064216614], "tokens": [["\u2581A", "frika", "ans"]], "entropy": [[4.547149658203125, 0.9522027373313904, 0.19397899508476257]], "avg_entropy": [1.897777130206426], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Autochtones", "score_of_biggest_cluster": 0.4520114064216614, "score_of_first_answer": 0.4520114064216614, "max_score_of_first_answer": 0.9694098830223083, "min_score_of_first_answer": 0.10612212866544724, "avg_score": 0.4520114064216614, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3911542031865159, "std_of_first_answer": 0.3911542031865159, "avg_entropy_of_first_answer": 1.897777130206426, "lex_sim": 1.0}
{"question": "Who is currently the Prime Minister of New Zealand?", "answer": ["Jacinder Wells"], "scores": [[0.061553459614515305, 0.008274159394204617, 0.02091171406209469, 0.5309438109397888]], "normalized_score": [0.048764489591121674], "tokens": [["\u2581Jac", "inder", "\u2581Well", "s"]], "entropy": [[4.8409600257873535, 0.531808614730835, 4.09489107131958, 1.8361742496490479]], "avg_entropy": [2.825958490371704], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Key", "score_of_biggest_cluster": 0.048764489591121674, "score_of_first_answer": 0.048764489591121674, "max_score_of_first_answer": 0.5309438109397888, "min_score_of_first_answer": 0.008274159394204617, "avg_score": 0.048764489591121674, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21770015926599037, "std_of_first_answer": 0.21770015926599037, "avg_entropy_of_first_answer": 2.825958490371704, "lex_sim": 1.0}
{"question": "Capnolagia is the arousal from watching others do what?", "answer": ["Eat"], "scores": [[0.2193388044834137, 0.5818290710449219]], "normalized_score": [0.35723617672920227], "tokens": [["\u2581E", "at"]], "entropy": [[5.211795806884766, 1.1406283378601074]], "avg_entropy": [3.1762120723724365], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wood smoke", "score_of_biggest_cluster": 0.35723617672920227, "score_of_first_answer": 0.35723617672920227, "max_score_of_first_answer": 0.5818290710449219, "min_score_of_first_answer": 0.2193388044834137, "avg_score": 0.35723617672920227, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1812451332807541, "std_of_first_answer": 0.1812451332807541, "avg_entropy_of_first_answer": 3.1762120723724365, "lex_sim": 1.0}
{"question": "\"What does \"\"casa grande\"\" mean in Spanish?\"", "answer": ["It means house"], "scores": [[0.031241092830896378, 0.5114564895629883, 0.029404284432530403]], "normalized_score": [0.07774069905281067], "tokens": [["\u2581It", "\u2581means", "\u2581house"]], "entropy": [[4.278852462768555, 1.7232718467712402, 3.332122802734375]], "avg_entropy": [3.11141570409139], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Big House (disambiguation)", "score_of_biggest_cluster": 0.07774069905281067, "score_of_first_answer": 0.07774069905281067, "max_score_of_first_answer": 0.5114564895629883, "min_score_of_first_answer": 0.029404284432530403, "avg_score": 0.07774069905281067, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22680988847608077, "std_of_first_answer": 0.22680988847608077, "avg_entropy_of_first_answer": 3.11141570409139, "lex_sim": 1.0}
{"question": "What type of fish is used in the production of Worcestershire sauce?", "answer": ["Anchovy"], "scores": [[0.10993701219558716, 0.7527317404747009, 0.5921818017959595]], "normalized_score": [0.36594271659851074], "tokens": [["\u2581An", "ch", "ovy"]], "entropy": [[5.542729377746582, 1.6181538105010986, 1.009377121925354]], "avg_entropy": [2.7234201033910117], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Anchovy", "score_of_biggest_cluster": 0.36594271659851074, "score_of_first_answer": 0.36594271659851074, "max_score_of_first_answer": 0.7527317404747009, "min_score_of_first_answer": 0.10993701219558716, "avg_score": 0.36594271659851074, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.27315469418747457, "std_of_first_answer": 0.27315469418747457, "avg_entropy_of_first_answer": 2.7234201033910117, "lex_sim": 1.0}
{"question": "Who was the first Pope to be appointed in the 21st century?", "answer": ["Pope John Paul II"], "scores": [[0.23936766386032104, 0.2509165406227112, 0.9218613505363464, 0.786234974861145]], "normalized_score": [0.4567759335041046], "tokens": [["\u2581Pope", "\u2581John", "\u2581Paul", "\u2581II"]], "entropy": [[4.4801836013793945, 2.3455216884613037, 0.5995076894760132, 1.006419062614441]], "avg_entropy": [2.107908010482788], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pope Benedict XVI", "score_of_biggest_cluster": 0.4567759335041046, "score_of_first_answer": 0.4567759335041046, "max_score_of_first_answer": 0.9218613505363464, "min_score_of_first_answer": 0.23936766386032104, "avg_score": 0.4567759335041046, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30823308373217145, "std_of_first_answer": 0.30823308373217145, "avg_entropy_of_first_answer": 2.107908010482788, "lex_sim": 1.0}
{"question": "The Free Officers Movement was composed of young junior army officers committed to unseating the monarchy of which African country?", "answer": ["Egypt"], "scores": [[0.4276110827922821]], "normalized_score": [0.4276110827922821], "tokens": [["\u2581Egypt"]], "entropy": [[3.667005777359009]], "avg_entropy": [3.667005777359009], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "EGY", "score_of_biggest_cluster": 0.4276110827922821, "score_of_first_answer": 0.4276110827922821, "max_score_of_first_answer": 0.4276110827922821, "min_score_of_first_answer": 0.4276110827922821, "avg_score": 0.4276110827922821, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.667005777359009, "lex_sim": 1.0}
{"question": "Which river originates in the Taurus Mountains, and flows through Syria and Iraq?", "answer": ["Allegheny Mountain"], "scores": [[0.015271168202161789, 0.016079006716609, 0.26124393939971924, 0.8855628967285156, 0.018652014434337616]], "normalized_score": [0.06382998079061508], "tokens": [["\u2581Al", "leg", "hen", "y", "\u2581Mountain"]], "entropy": [[4.7667999267578125, 4.137669563293457, 3.033989906311035, 0.5147887468338013, 2.664517879486084]], "avg_entropy": [3.023553204536438], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Euphr\u00e1t\u0113s", "score_of_biggest_cluster": 0.06382998079061508, "score_of_first_answer": 0.06382998079061508, "max_score_of_first_answer": 0.8855628967285156, "min_score_of_first_answer": 0.015271168202161789, "avg_score": 0.06382998079061508, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33670144398807483, "std_of_first_answer": 0.33670144398807483, "avg_entropy_of_first_answer": 3.023553204536438, "lex_sim": 1.0}
{"question": "\"Who devised the science-fiction law that states \"\"Any sufficiently advanced technology is indistinguishable from magic\"\"?\"", "answer": ["Masker in the Future"], "scores": [[0.002373266965150833, 0.006212201900780201, 0.026767995208501816, 0.003098186803981662, 0.2491377294063568, 0.006975363474339247]], "normalized_score": [0.011338450945913792], "tokens": [["\u2581M", "ask", "er", "\u2581in", "\u2581the", "\u2581Future"]], "entropy": [[4.08164119720459, 5.710227966308594, 3.7075769901275635, 4.9038238525390625, 5.512852668762207, 6.190271377563477]], "avg_entropy": [5.017732342084249], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Arthur C. Clarke", "score_of_biggest_cluster": 0.011338450945913792, "score_of_first_answer": 0.011338450945913792, "max_score_of_first_answer": 0.2491377294063568, "min_score_of_first_answer": 0.002373266965150833, "avg_score": 0.011338450945913792, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08983987054990408, "std_of_first_answer": 0.08983987054990408, "avg_entropy_of_first_answer": 5.017732342084249, "lex_sim": 1.0}
{"question": "Which detective was created by G K Chesterton?", "answer": ["Mr. Trowel"], "scores": [[0.005664119962602854, 0.4279768764972687, 0.01138909999281168, 0.0032332490663975477, 0.22523851692676544]], "normalized_score": [0.02888449653983116], "tokens": [["\u2581Mr", ".", "\u2581T", "row", "el"]], "entropy": [[4.116752624511719, 4.14402961730957, 6.189934253692627, 5.214541435241699, 3.0957155227661133]], "avg_entropy": [4.552194690704345], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Father Brown", "score_of_biggest_cluster": 0.02888449653983116, "score_of_first_answer": 0.02888449653983116, "max_score_of_first_answer": 0.4279768764972687, "min_score_of_first_answer": 0.0032332490663975477, "avg_score": 0.02888449653983116, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1693209333385833, "std_of_first_answer": 0.1693209333385833, "avg_entropy_of_first_answer": 4.552194690704345, "lex_sim": 1.0}
{"question": "After Harold McMillan who was Britain's next Prime Minister?", "answer": ["Anthony Eden"], "scores": [[0.0772789865732193, 0.750109851360321, 0.9848476052284241]], "normalized_score": [0.3850511610507965], "tokens": [["\u2581Anthony", "\u2581E", "den"]], "entropy": [[4.7986345291137695, 1.8519287109375, 0.15543009340763092]], "avg_entropy": [2.2686644444863], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alec Douglas-Home", "score_of_biggest_cluster": 0.3850511610507965, "score_of_first_answer": 0.3850511610507965, "max_score_of_first_answer": 0.9848476052284241, "min_score_of_first_answer": 0.0772789865732193, "avg_score": 0.3850511610507965, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38463315614707155, "std_of_first_answer": 0.38463315614707155, "avg_entropy_of_first_answer": 2.2686644444863, "lex_sim": 1.0}
{"question": "Which city hosted the 1980 Summer Olympics, which were boycotted by over 60 nations?", "answer": ["Moscow Olympic"], "scores": [[0.3360312283039093, 0.02226404659450054]], "normalized_score": [0.08649517595767975], "tokens": [["\u2581Moscow", "\u2581Olympic"]], "entropy": [[4.360604286193848, 1.8083069324493408]], "avg_entropy": [3.0844556093215942], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Moskou", "score_of_biggest_cluster": 0.08649517595767975, "score_of_first_answer": 0.08649517595767975, "max_score_of_first_answer": 0.3360312283039093, "min_score_of_first_answer": 0.02226404659450054, "avg_score": 0.08649517595767975, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.15688359085470438, "std_of_first_answer": 0.15688359085470438, "avg_entropy_of_first_answer": 3.0844556093215942, "lex_sim": 1.0}
{"question": "In old measurement 1 rod = 1 pole = 1 perch = what fraction of 1 chain?", "answer": ["one fourth"], "scores": [[0.005860515404492617, 0.020163463428616524]], "normalized_score": [0.010870523750782013], "tokens": [["\u2581one", "\u2581fourth"]], "entropy": [[2.405914306640625, 3.6508607864379883]], "avg_entropy": [3.0283875465393066], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "A QUARTER", "score_of_biggest_cluster": 0.010870523750782013, "score_of_first_answer": 0.010870523750782013, "max_score_of_first_answer": 0.020163463428616524, "min_score_of_first_answer": 0.005860515404492617, "avg_score": 0.010870523750782013, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0071514740120619535, "std_of_first_answer": 0.0071514740120619535, "avg_entropy_of_first_answer": 3.0283875465393066, "lex_sim": 1.0}
{"question": "Who also writes using the pseudonym 'Barbara Vine'?", "answer": ["Ruth Rendell"], "scores": [[0.19462428987026215, 0.7812950611114502, 0.9897148013114929, 0.9221821427345276]], "normalized_score": [0.6103581190109253], "tokens": [["\u2581Ruth", "\u2581R", "end", "ell"]], "entropy": [[5.272219181060791, 1.4598277807235718, 0.10936769098043442, 0.4185616075992584]], "avg_entropy": [1.814994065091014], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Baroness Rendell", "score_of_biggest_cluster": 0.6103581190109253, "score_of_first_answer": 0.6103581190109253, "max_score_of_first_answer": 0.9897148013114929, "min_score_of_first_answer": 0.19462428987026215, "avg_score": 0.6103581190109253, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3136021127883291, "std_of_first_answer": 0.3136021127883291, "avg_entropy_of_first_answer": 1.814994065091014, "lex_sim": 1.0}
{"question": "Which commission was set up to investigate the assassination of JFK?", "answer": ["United States House Select Committee on Assassinations"], "scores": [[0.03397031128406525, 0.9815448522567749, 0.7455844283103943, 0.9340798854827881, 0.9878787398338318, 0.9539942741394043, 0.9951329827308655, 0.9997221827507019, 0.9950225353240967, 0.9996340274810791]], "normalized_score": [0.6816437244415283], "tokens": [["\u2581United", "\u2581States", "\u2581House", "\u2581Select", "\u2581Committee", "\u2581on", "\u2581Ass", "ass", "in", "ations"]], "entropy": [[3.634044647216797, 0.159146249294281, 1.2743959426879883, 0.30734682083129883, 0.11720095574855804, 0.27347132563591003, 0.04513119161128998, 0.003205754328519106, 0.03439153730869293, 0.004269489087164402]], "avg_entropy": [0.5852603913750499], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Warren Commission Report", "score_of_biggest_cluster": 0.6816437244415283, "score_of_first_answer": 0.6816437244415283, "max_score_of_first_answer": 0.9997221827507019, "min_score_of_first_answer": 0.03397031128406525, "avg_score": 0.6816437244415283, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2857107345156952, "std_of_first_answer": 0.2857107345156952, "avg_entropy_of_first_answer": 0.5852603913750499, "lex_sim": 1.0}
{"question": "What is harvested from Hevea brasiliensis, whose other plant names separately feature the words Para and India?", "answer": ["\u3413"], "scores": [[0.01624993234872818, 0.36674636602401733, 0.017931809648871422, 0.006324633024632931]], "normalized_score": [0.02867274172604084], "tokens": [["\u2581", "<0xE3>", "<0x90>", "<0x93>"]], "entropy": [[4.498599529266357, 4.20388126373291, 2.289966106414795, 3.9078421592712402]], "avg_entropy": [3.7250722646713257], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Latex", "score_of_biggest_cluster": 0.02867274172604084, "score_of_first_answer": 0.02867274172604084, "max_score_of_first_answer": 0.36674636602401733, "min_score_of_first_answer": 0.006324633024632931, "avg_score": 0.02867274172604084, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.15302353520040585, "std_of_first_answer": 0.15302353520040585, "avg_entropy_of_first_answer": 3.7250722646713257, "lex_sim": 0.0}
{"question": "What name has been used on Irish postage stamps since 1922, and on all Irish coinage (including Irish euro coins), on passports and other official state documents issued since 1937?", "answer": ["Dublinia"], "scores": [[0.002671491587534547, 0.026202863082289696]], "normalized_score": [0.008366642519831657], "tokens": [["\u2581Dublin", "ia"]], "entropy": [[4.834003448486328, 2.6095337867736816]], "avg_entropy": [3.721768617630005], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ire", "score_of_biggest_cluster": 0.008366642519831657, "score_of_first_answer": 0.008366642519831657, "max_score_of_first_answer": 0.026202863082289696, "min_score_of_first_answer": 0.002671491587534547, "avg_score": 0.008366642519831657, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.011765685747377574, "std_of_first_answer": 0.011765685747377574, "avg_entropy_of_first_answer": 3.721768617630005, "lex_sim": 1.0}
{"question": "What are double bitted, cruiser double bitted and cedar types of", "answer": ["Ax Handle"], "scores": [[0.0342700220644474, 0.0015802272828295827]], "normalized_score": [0.007358967326581478], "tokens": [["\u2581Ax", "\u2581Handle"]], "entropy": [[4.45647668838501, 2.0401644706726074]], "avg_entropy": [3.2483205795288086], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Axes", "score_of_biggest_cluster": 0.007358967326581478, "score_of_first_answer": 0.007358967326581478, "max_score_of_first_answer": 0.0342700220644474, "min_score_of_first_answer": 0.0015802272828295827, "avg_score": 0.007358967326581478, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.01634489739080891, "std_of_first_answer": 0.01634489739080891, "avg_entropy_of_first_answer": 3.2483205795288086, "lex_sim": 1.0}
{"question": "In the UK, the LTA is the governing body of which sport?", "answer": ["Lawn Tennis Association"], "scores": [[0.2588651776313782, 0.9963274598121643, 0.7291314005851746, 0.808197021484375]], "normalized_score": [0.6243810057640076], "tokens": [["\u2581La", "wn", "\u2581Tennis", "\u2581Association"]], "entropy": [[4.579802989959717, 0.04269932210445404, 0.8596346378326416, 0.8043555021286011]], "avg_entropy": [1.5716231130063534], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lawn tennis", "score_of_biggest_cluster": 0.6243810057640076, "score_of_first_answer": 0.6243810057640076, "max_score_of_first_answer": 0.9963274598121643, "min_score_of_first_answer": 0.2588651776313782, "avg_score": 0.6243810057640076, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27154702413457227, "std_of_first_answer": 0.27154702413457227, "avg_entropy_of_first_answer": 1.5716231130063534, "lex_sim": 1.0}
{"question": "What was the name of the IBM computer that defeated world chess champion Garry Kasparov in 1997?", "answer": ["Big Blue (Deep Blue Chess Computer)"], "scores": [[0.01127274427562952, 0.8230521082878113, 0.08979833126068115, 0.011292656883597374, 0.9940009713172913, 0.8358447551727295, 0.012456952594220638, 0.9961004257202148, 0.6935275197029114, 0.9550266265869141]], "normalized_score": [0.19088982045650482], "tokens": [["\u2581Big", "\u2581Blue", "\u2581(", "De", "ep", "\u2581Blue", "\u2581Ch", "ess", "\u2581Computer", ")"]], "entropy": [[3.940840482711792, 1.5075781345367432, 1.2732871770858765, 3.2412712574005127, 0.05275779217481613, 0.742693305015564, 1.3411362171173096, 0.032829321920871735, 1.2418994903564453, 0.37729108333587646]], "avg_entropy": [1.3751584261655807], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Deep Blue (disambiguation)", "score_of_biggest_cluster": 0.19088982045650482, "score_of_first_answer": 0.19088982045650482, "max_score_of_first_answer": 0.9961004257202148, "min_score_of_first_answer": 0.01127274427562952, "avg_score": 0.19088982045650482, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4263357849207628, "std_of_first_answer": 0.4263357849207628, "avg_entropy_of_first_answer": 1.3751584261655807, "lex_sim": 1.0}
{"question": "What proper noun links titles by Bjork, Bananarama, Shocking Blue and Mark Wynter?", "answer": ["Black Velvet"], "scores": [[0.005617231596261263, 0.020758118480443954, 0.991657018661499]], "normalized_score": [0.048718132078647614], "tokens": [["\u2581Black", "\u2581Vel", "vet"]], "entropy": [[5.543768882751465, 4.1496686935424805, 0.060237277299165726]], "avg_entropy": [3.251224951197704], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "VENUS", "score_of_biggest_cluster": 0.048718132078647614, "score_of_first_answer": 0.048718132078647614, "max_score_of_first_answer": 0.991657018661499, "min_score_of_first_answer": 0.005617231596261263, "avg_score": 0.048718132078647614, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46129628734976363, "std_of_first_answer": 0.46129628734976363, "avg_entropy_of_first_answer": 3.251224951197704, "lex_sim": 1.0}
{"question": "Proteus and Nereid are moons of which planet?", "answer": ["Neptune"], "scores": [[0.3998173177242279, 0.9896949529647827, 0.9563862085342407]], "normalized_score": [0.7233226895332336], "tokens": [["\u2581Ne", "pt", "une"]], "entropy": [[3.5648345947265625, 0.11236997693777084, 0.28711867332458496]], "avg_entropy": [1.3214410816629727], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "8th planet", "score_of_biggest_cluster": 0.7233226895332336, "score_of_first_answer": 0.7233226895332336, "max_score_of_first_answer": 0.9896949529647827, "min_score_of_first_answer": 0.3998173177242279, "avg_score": 0.7233226895332336, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.27056197220477074, "std_of_first_answer": 0.27056197220477074, "avg_entropy_of_first_answer": 1.3214410816629727, "lex_sim": 1.0}
{"question": "Which member of the Hollies went on to join the hugely successful American band commonly known by their initials CSNY?", "answer": ["7 Hollies!"], "scores": [[0.024975791573524475, 0.01882542110979557, 0.0015379750402644277, 0.7184305191040039, 0.0036365031264722347]], "normalized_score": [0.017999328672885895], "tokens": [["\u2581", "7", "\u2581Hol", "lies", "!"]], "entropy": [[5.385431289672852, 4.85336446762085, 4.395978927612305, 1.147484302520752, 2.600102186203003]], "avg_entropy": [3.676472234725952], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Graham William Nash", "score_of_biggest_cluster": 0.017999328672885895, "score_of_first_answer": 0.017999328672885895, "max_score_of_first_answer": 0.7184305191040039, "min_score_of_first_answer": 0.0015379750402644277, "avg_score": 0.017999328672885895, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28261412870970765, "std_of_first_answer": 0.28261412870970765, "avg_entropy_of_first_answer": 3.676472234725952, "lex_sim": 1.0}
{"question": "What Italian author envisioned 9 circles of hell?", "answer": ["Dante Alighieri"], "scores": [[0.4236481487751007, 0.9578148722648621, 0.45584627985954285, 0.9844912886619568, 0.9717119932174683]], "normalized_score": [0.707246720790863], "tokens": [["\u2581D", "ante", "\u2581Al", "igh", "ieri"]], "entropy": [[4.305550575256348, 0.3905388414859772, 1.7346371412277222, 0.1275414228439331, 0.192684143781662]], "avg_entropy": [1.3501904249191283], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Alighieri", "score_of_biggest_cluster": 0.707246720790863, "score_of_first_answer": 0.707246720790863, "max_score_of_first_answer": 0.9844912886619568, "min_score_of_first_answer": 0.4236481487751007, "avg_score": 0.707246720790863, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.26076144900181825, "std_of_first_answer": 0.26076144900181825, "avg_entropy_of_first_answer": 1.3501904249191283, "lex_sim": 1.0}
{"question": "In which sport is it forbidden for anyone to play left handed?", "answer": ["Squash"], "scores": [[0.04482223466038704, 0.4522344470024109, 0.975807785987854]], "normalized_score": [0.27044180035591125], "tokens": [["\u2581S", "qu", "ash"]], "entropy": [[4.618719100952148, 1.8715159893035889, 0.20466774702072144]], "avg_entropy": [2.231634279092153], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ice polo", "score_of_biggest_cluster": 0.27044180035591125, "score_of_first_answer": 0.27044180035591125, "max_score_of_first_answer": 0.975807785987854, "min_score_of_first_answer": 0.04482223466038704, "avg_score": 0.27044180035591125, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38105815374826035, "std_of_first_answer": 0.38105815374826035, "avg_entropy_of_first_answer": 2.231634279092153, "lex_sim": 1.0}
{"question": "The final carrier of the Olympic flame in the Olympic Torch Relay is often kept secret until the last moment, and is usually who?", "answer": ["The last torchbearer is typically a high-profile celebrity or well-known figure from the host country"], "scores": [[0.14418144524097443, 0.08801035583019257, 0.2896498441696167, 0.9965123534202576, 0.6115004420280457, 0.9995654225349426, 0.9933688044548035, 0.06106875464320183, 0.01023317500948906, 0.6353262066841125, 0.09976935386657715, 0.9659078121185303, 0.998670220375061, 0.04247007519006729, 0.9998201727867126, 0.39446985721588135, 0.3400404155254364, 0.9892220497131348, 0.9035819172859192, 0.3798925578594208, 0.9133630990982056, 0.9592260122299194, 0.8234653472900391, 0.20243965089321136]], "normalized_score": [0.361003577709198], "tokens": [["\u2581The", "\u2581last", "\u2581tor", "ch", "b", "ear", "er", "\u2581is", "\u2581typically", "\u2581a", "\u2581high", "-", "profile", "\u2581celebr", "ity", "\u2581or", "\u2581well", "-", "known", "\u2581figure", "\u2581from", "\u2581the", "\u2581host", "\u2581country"]], "entropy": [[5.912965774536133, 5.622669696807861, 3.818643093109131, 0.03472311049699783, 1.1740761995315552, 0.0059128389693796635, 0.044733669608831406, 2.216801166534424, 3.227266311645508, 1.7011902332305908, 3.225611686706543, 0.16735810041427612, 0.01234624907374382, 2.1595144271850586, 0.002054171171039343, 1.9348562955856323, 1.4094924926757812, 0.07053714990615845, 0.37858402729034424, 1.7216694355010986, 0.44264763593673706, 0.2550959587097168, 0.9847869873046875, 0.8210403323173523]], "avg_entropy": [1.5560240435103576], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "A sports celebrity of the host country", "score_of_biggest_cluster": 0.361003577709198, "score_of_first_answer": 0.361003577709198, "max_score_of_first_answer": 0.9998201727867126, "min_score_of_first_answer": 0.01023317500948906, "avg_score": 0.361003577709198, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3827210844327001, "std_of_first_answer": 0.3827210844327001, "avg_entropy_of_first_answer": 1.5560240435103576, "lex_sim": 1.0}
{"question": "Who was the illustrator for most of Roald Dahl\u2019s stories for children?", "answer": ["Quentin Blake"], "scores": [[0.345358669757843, 0.9775011539459229, 0.5362618565559387, 0.9925191402435303]], "normalized_score": [0.6510672569274902], "tokens": [["\u2581Qu", "entin", "\u2581Bla", "ke"]], "entropy": [[4.966933250427246, 0.16103506088256836, 3.451584815979004, 0.06918022036552429]], "avg_entropy": [2.1621833369135857], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Quentin Saxby Blake", "score_of_biggest_cluster": 0.6510672569274902, "score_of_first_answer": 0.6510672569274902, "max_score_of_first_answer": 0.9925191402435303, "min_score_of_first_answer": 0.345358669757843, "avg_score": 0.6510672569274902, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.28039628073451944, "std_of_first_answer": 0.28039628073451944, "avg_entropy_of_first_answer": 2.1621833369135857, "lex_sim": 1.0}
{"question": "English sportswoman Charlotte Edwards is associated with which sport?", "answer": ["England Cricket"], "scores": [[0.03956611454486847, 0.07523549348115921, 0.8859986662864685]], "normalized_score": [0.13816341757774353], "tokens": [["\u2581England", "\u2581Cr", "icket"]], "entropy": [[4.110298156738281, 2.7876319885253906, 0.36906930804252625]], "avg_entropy": [2.422333151102066], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Cricket games", "score_of_biggest_cluster": 0.13816341757774353, "score_of_first_answer": 0.13816341757774353, "max_score_of_first_answer": 0.8859986662864685, "min_score_of_first_answer": 0.03956611454486847, "avg_score": 0.13816341757774353, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3908761229765809, "std_of_first_answer": 0.3908761229765809, "avg_entropy_of_first_answer": 2.422333151102066, "lex_sim": 1.0}
{"question": "The Kina is the monetary unit of which Oceania country?", "answer": ["PNG"], "scores": [[0.09897653758525848, 0.5193460583686829]], "normalized_score": [0.22672247886657715], "tokens": [["\u2581P", "NG"]], "entropy": [[4.73875093460083, 3.603975296020508]], "avg_entropy": [4.171363115310669], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "ISO 3166-1:PG", "score_of_biggest_cluster": 0.22672247886657715, "score_of_first_answer": 0.22672247886657715, "max_score_of_first_answer": 0.5193460583686829, "min_score_of_first_answer": 0.09897653758525848, "avg_score": 0.22672247886657715, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2101847603917122, "std_of_first_answer": 0.2101847603917122, "avg_entropy_of_first_answer": 4.171363115310669, "lex_sim": 1.0}
{"question": "What well known alcoholic drink was originally known as 'Usquebaugh'?", "answer": ["Whiskey"], "scores": [[0.2674442529678345, 0.9606319665908813, 0.45493224263191223]], "normalized_score": [0.4889288544654846], "tokens": [["\u2581Wh", "is", "key"]], "entropy": [[4.480587482452393, 0.29404568672180176, 0.7402124404907227]], "avg_entropy": [1.8382818698883057], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Whiskey-A-Gogo", "score_of_biggest_cluster": 0.4889288544654846, "score_of_first_answer": 0.4889288544654846, "max_score_of_first_answer": 0.9606319665908813, "min_score_of_first_answer": 0.2674442529678345, "avg_score": 0.4889288544654846, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29276330283816543, "std_of_first_answer": 0.29276330283816543, "avg_entropy_of_first_answer": 1.8382818698883057, "lex_sim": 1.0}
{"question": "Michael Jackson was one of ten children. Who is the youngest, born in 1966?", "answer": ["Tito Jackson"], "scores": [[0.017274916172027588, 0.5581523776054382, 0.39765405654907227]], "normalized_score": [0.15651574730873108], "tokens": [["\u2581T", "ito", "\u2581Jackson"]], "entropy": [[5.462532997131348, 3.0336062908172607, 2.241563558578491]], "avg_entropy": [3.5792342821756997], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joint Academic Network", "score_of_biggest_cluster": 0.15651574730873108, "score_of_first_answer": 0.15651574730873108, "max_score_of_first_answer": 0.5581523776054382, "min_score_of_first_answer": 0.017274916172027588, "avg_score": 0.15651574730873108, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2268128033282421, "std_of_first_answer": 0.2268128033282421, "avg_entropy_of_first_answer": 3.5792342821756997, "lex_sim": 1.0}
{"question": "What is the name of the parson mentioned in the lyrics of the Christmas carol \"Winter Wonderland\"?", "answer": ["Jack Frosty"], "scores": [[0.008438394404947758, 0.37701934576034546, 0.9824320673942566, 0.01632591336965561]], "normalized_score": [0.08451831340789795], "tokens": [["\u2581Jack", "\u2581F", "rost", "y"]], "entropy": [[6.1551666259765625, 3.9002699851989746, 0.19464010000228882, 0.9356640577316284]], "avg_entropy": [2.7964351922273636], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Orangeish blacks", "score_of_biggest_cluster": 0.08451831340789795, "score_of_first_answer": 0.08451831340789795, "max_score_of_first_answer": 0.9824320673942566, "min_score_of_first_answer": 0.008438394404947758, "avg_score": 0.08451831340789795, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39643435949495753, "std_of_first_answer": 0.39643435949495753, "avg_entropy_of_first_answer": 2.7964351922273636, "lex_sim": 1.0}
{"question": "In June 2009 American neuroscientist Dr Gary Arendash claimed to have evidence that drinking a few cups of coffee a day could prevent or reverse the effects of what?", "answer": ["Alzheimer's disease"], "scores": [[0.6908293962478638, 0.9792773127555847, 0.8784016370773315, 0.8535622954368591, 0.9992902278900146, 0.607567548751831]], "normalized_score": [0.8217666745185852], "tokens": [["\u2581Al", "z", "heimer", "'", "s", "\u2581disease"]], "entropy": [[2.1557440757751465, 0.1517566740512848, 0.4227643311023712, 0.736382246017456, 0.008543232455849648, 1.1997640132904053]], "avg_entropy": [0.7791590954487523], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Alzheimer's diseases", "score_of_biggest_cluster": 0.8217666745185852, "score_of_first_answer": 0.8217666745185852, "max_score_of_first_answer": 0.9992902278900146, "min_score_of_first_answer": 0.607567548751831, "avg_score": 0.8217666745185852, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.14291371193528873, "std_of_first_answer": 0.14291371193528873, "avg_entropy_of_first_answer": 0.7791590954487523, "lex_sim": 1.0}
{"question": "Which international cricket side had its best year ever in 2014 with five Test wins?", "answer": ["Indian cricket team"], "scores": [[0.0034397330600768328, 0.5280956029891968, 0.9890390634536743, 0.9295923709869385]], "normalized_score": [0.20215561985969543], "tokens": [["\u2581Indian", "\u2581cr", "icket", "\u2581team"]], "entropy": [[4.660861015319824, 1.901752233505249, 0.06568814814090729, 0.4619911313056946]], "avg_entropy": [1.7725731320679188], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "N Z", "score_of_biggest_cluster": 0.20215561985969543, "score_of_first_answer": 0.20215561985969543, "max_score_of_first_answer": 0.9890390634536743, "min_score_of_first_answer": 0.0034397330600768328, "avg_score": 0.20215561985969543, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3938298306665193, "std_of_first_answer": 0.3938298306665193, "avg_entropy_of_first_answer": 1.7725731320679188, "lex_sim": 1.0}
{"question": "Which four word motto appeared on US banknotes for the first time on this day in 1982 ?", "answer": ["_ 53371 T"], "scores": [[0.00176104751881212, 0.008865071460604668, 0.03520260751247406, 0.03618636727333069, 0.057431962341070175, 0.04887208342552185, 0.03172898665070534, 0.0008941145497374237]], "normalized_score": [0.014123895205557346], "tokens": [["\u2581_", "\u2581", "5", "3", "3", "7", "1", "\u2581T"]], "entropy": [[3.762880325317383, 4.5163774490356445, 4.895651817321777, 4.275460243225098, 3.8814594745635986, 3.5194716453552246, 3.1009862422943115, 3.154977321624756]], "avg_entropy": [3.888408064842224], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "IN GOD WE TRUST", "score_of_biggest_cluster": 0.014123895205557346, "score_of_first_answer": 0.014123895205557346, "max_score_of_first_answer": 0.057431962341070175, "min_score_of_first_answer": 0.0008941145497374237, "avg_score": 0.014123895205557346, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.020071348317984853, "std_of_first_answer": 0.020071348317984853, "avg_entropy_of_first_answer": 3.888408064842224, "lex_sim": 1.0}
{"question": "Which late American musician, who pioneered the styles of rock and roll and rockabilly, was born Vincent Eugene Craddock in 1935?", "answer": ["Gene Vincent"], "scores": [[0.1373223066329956, 0.7631476521492004]], "normalized_score": [0.3237239420413971], "tokens": [["\u2581Gene", "\u2581Vincent"]], "entropy": [[4.843107223510742, 1.9709362983703613]], "avg_entropy": [3.4070217609405518], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Gene Vincent & the Blue Caps", "score_of_biggest_cluster": 0.3237239420413971, "score_of_first_answer": 0.3237239420413971, "max_score_of_first_answer": 0.7631476521492004, "min_score_of_first_answer": 0.1373223066329956, "avg_score": 0.3237239420413971, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3129126727581024, "std_of_first_answer": 0.3129126727581024, "avg_entropy_of_first_answer": 3.4070217609405518, "lex_sim": 1.0}
{"question": "Who played the role of George Carter in The Sweeney?", "answer": ["Dennis Waterman"], "scores": [[0.057362981140613556, 0.9567558765411377, 0.9896162748336792]], "normalized_score": [0.3787040114402771], "tokens": [["\u2581Dennis", "\u2581Wat", "erman"]], "entropy": [[5.801277160644531, 0.4202710688114166, 0.06554998457431793]], "avg_entropy": [2.0956994046767554], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Dennis Waterman", "score_of_biggest_cluster": 0.3787040114402771, "score_of_first_answer": 0.3787040114402771, "max_score_of_first_answer": 0.9896162748336792, "min_score_of_first_answer": 0.057362981140613556, "avg_score": 0.3787040114402771, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4319315261672403, "std_of_first_answer": 0.4319315261672403, "avg_entropy_of_first_answer": 2.0956994046767554, "lex_sim": 1.0}
{"question": "What was Louise Rednap's maiden name?", "answer": ["Looooseee Rednap"], "scores": [[0.018498495221138, 0.004871489014476538, 0.03278375416994095, 0.06510866433382034, 0.12552814185619354, 0.04653596132993698, 0.3694288432598114]], "normalized_score": [0.04568110406398773], "tokens": [["\u2581L", "oo", "o", "ose", "ee", "\u2581Red", "nap"]], "entropy": [[6.176702499389648, 5.311916351318359, 4.852998733520508, 3.884000301361084, 3.3645758628845215, 2.78549861907959, 3.367133140563965]], "avg_entropy": [4.248975072588239], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nurdin (disambiguation)", "score_of_biggest_cluster": 0.04568110406398773, "score_of_first_answer": 0.04568110406398773, "max_score_of_first_answer": 0.3694288432598114, "min_score_of_first_answer": 0.004871489014476538, "avg_score": 0.04568110406398773, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11791715169177311, "std_of_first_answer": 0.11791715169177311, "avg_entropy_of_first_answer": 4.248975072588239, "lex_sim": 1.0}
{"question": "Eventually a newspaper photographer for the Daily Bugle, what is the alter ego of Spiderman?", "answer": ["Peter Parker (Spider-Man)"], "scores": [[0.34808090329170227, 0.6921542286872864, 0.03753379359841347, 0.0903470441699028, 0.23061460256576538, 0.8551091551780701, 0.9399121403694153, 0.7259567379951477]], "normalized_score": [0.31999316811561584], "tokens": [["\u2581Peter", "\u2581Parker", "\u2581(", "Sp", "ider", "-", "Man", ")"]], "entropy": [[4.491271495819092, 2.2289044857025146, 0.7536954879760742, 4.640847206115723, 0.7167340517044067, 0.6792159080505371, 0.27343323826789856, 1.3834664821624756]], "avg_entropy": [1.8959460444748402], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wall Crawler", "score_of_biggest_cluster": 0.31999316811561584, "score_of_first_answer": 0.31999316811561584, "max_score_of_first_answer": 0.9399121403694153, "min_score_of_first_answer": 0.03753379359841347, "avg_score": 0.31999316811561584, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33244125429654264, "std_of_first_answer": 0.33244125429654264, "avg_entropy_of_first_answer": 1.8959460444748402, "lex_sim": 1.0}
{"question": "Redfoo and Sky Blu were members of which electronic rock band?", "answer": ["Dizze R"], "scores": [[0.006220415234565735, 0.010612723417580128, 0.32856014370918274, 0.001503931125625968]], "normalized_score": [0.013439169153571129], "tokens": [["\u2581D", "iz", "ze", "\u2581R"]], "entropy": [[3.858011245727539, 5.304901123046875, 2.432262659072876, 0.45682650804519653]], "avg_entropy": [3.0130003839731216], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "I AM NOT A WHORE", "score_of_biggest_cluster": 0.013439169153571129, "score_of_first_answer": 0.013439169153571129, "max_score_of_first_answer": 0.32856014370918274, "min_score_of_first_answer": 0.001503931125625968, "avg_score": 0.013439169153571129, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13966113822911197, "std_of_first_answer": 0.13966113822911197, "avg_entropy_of_first_answer": 3.0130003839731216, "lex_sim": 1.0}
{"question": "What are listed across the top of the Lincoln Memorial on the back of the $5 bill?", "answer": ["The Lincoln Memorial"], "scores": [[0.06424418091773987, 0.05492553859949112, 0.8652794361114502]], "normalized_score": [0.1450735181570053], "tokens": [["\u2581The", "\u2581Lincoln", "\u2581Memorial"]], "entropy": [[6.1016740798950195, 5.53178071975708, 1.1166728734970093]], "avg_entropy": [4.25004255771637], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "All 50 states", "score_of_biggest_cluster": 0.1450735181570053, "score_of_first_answer": 0.1450735181570053, "max_score_of_first_answer": 0.8652794361114502, "min_score_of_first_answer": 0.05492553859949112, "avg_score": 0.1450735181570053, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3798271180375986, "std_of_first_answer": 0.3798271180375986, "avg_entropy_of_first_answer": 4.25004255771637, "lex_sim": 1.0}
{"question": "Which American scene artist painted 'Cape Cod Evening'?", "answer": ["118 and 60"], "scores": [[0.0112291956320405, 0.0797961950302124, 0.03671163320541382, 0.03755154833197594, 0.0009739086963236332, 0.3846728205680847, 0.06716588139533997, 0.13112793862819672]], "normalized_score": [0.037694405764341354], "tokens": [["\u2581", "1", "1", "8", "\u2581and", "\u2581", "6", "0"]], "entropy": [[5.531763553619385, 5.01085901260376, 2.6269073486328125, 4.310926914215088, 4.0396599769592285, 4.704833507537842, 1.937761664390564, 2.743769645690918]], "avg_entropy": [3.8633102029561996], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Edward Hopper", "score_of_biggest_cluster": 0.037694405764341354, "score_of_first_answer": 0.037694405764341354, "max_score_of_first_answer": 0.3846728205680847, "min_score_of_first_answer": 0.0009739086963236332, "avg_score": 0.037694405764341354, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1165993541493663, "std_of_first_answer": 0.1165993541493663, "avg_entropy_of_first_answer": 3.8633102029561996, "lex_sim": 1.0}
{"question": "Which article has parts called - cantle, skirt, flap and pommel?", "answer": ["Briefcase"], "scores": [[0.0135786272585392, 0.01320522278547287, 0.44011151790618896]], "normalized_score": [0.04289316385984421], "tokens": [["\u2581Br", "ief", "case"]], "entropy": [[4.49462890625, 0.3314841687679291, 2.44282603263855]], "avg_entropy": [2.4229797025521598], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Saddle", "score_of_biggest_cluster": 0.04289316385984421, "score_of_first_answer": 0.04289316385984421, "max_score_of_first_answer": 0.44011151790618896, "min_score_of_first_answer": 0.01320522278547287, "avg_score": 0.04289316385984421, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20115760295760257, "std_of_first_answer": 0.20115760295760257, "avg_entropy_of_first_answer": 2.4229797025521598, "lex_sim": 1.0}
{"question": "Who wounded Achilles in the heel, leading to his death?", "answer": ["Paris, prince of Troy"], "scores": [[0.523283064365387, 0.056548163294792175, 0.14357063174247742, 0.9929185509681702, 0.9552644491195679, 0.9996925592422485]], "normalized_score": [0.3988908529281616], "tokens": [["\u2581Paris", ",", "\u2581prince", "\u2581of", "\u2581T", "roy"]], "entropy": [[3.490082263946533, 1.4787096977233887, 2.8435914516448975, 0.0655834972858429, 0.321455180644989, 0.004272084683179855]], "avg_entropy": [1.3672823626548052], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Places named Paris", "score_of_biggest_cluster": 0.3988908529281616, "score_of_first_answer": 0.3988908529281616, "max_score_of_first_answer": 0.9996925592422485, "min_score_of_first_answer": 0.056548163294792175, "avg_score": 0.3988908529281616, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3977139609874512, "std_of_first_answer": 0.3977139609874512, "avg_entropy_of_first_answer": 1.3672823626548052, "lex_sim": 1.0}
{"question": "The Red Sea is connected to the Indian Ocean by which gulf ?", "answer": ["The Gulf of Aden"], "scores": [[0.09652704745531082, 0.4214567244052887, 0.9717535376548767, 0.9606184363365173, 0.7422060966491699, 0.921480655670166]], "normalized_score": [0.5441933870315552], "tokens": [["\u2581The", "\u2581G", "ulf", "\u2581of", "\u2581A", "den"]], "entropy": [[4.892059326171875, 3.5734384059906006, 0.2625840902328491, 0.26660996675491333, 1.5148141384124756, 0.3693004846572876]], "avg_entropy": [1.8131344020366669], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Gulf of Aden", "score_of_biggest_cluster": 0.5441933870315552, "score_of_first_answer": 0.5441933870315552, "max_score_of_first_answer": 0.9717535376548767, "min_score_of_first_answer": 0.09652704745531082, "avg_score": 0.5441933870315552, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.32484393744333667, "std_of_first_answer": 0.32484393744333667, "avg_entropy_of_first_answer": 1.8131344020366669, "lex_sim": 1.0}
{"question": "\u201cInstrument for measuring the humidity of air or gas.\u201d A definition of what?", "answer": ["hygrometer or hygroscope"], "scores": [[0.05202704668045044, 0.9784638285636902, 0.9153396487236023, 0.9701491594314575, 0.002038126578554511, 0.3121631145477295, 0.9886990189552307, 0.17343489825725555, 0.9937431216239929]], "normalized_score": [0.25705817341804504], "tokens": [["\u2581h", "yg", "rom", "eter", "\u2581or", "\u2581h", "yg", "ros", "cope"]], "entropy": [[3.0413575172424316, 0.23278695344924927, 0.36949604749679565, 0.17535492777824402, 0.4522324800491333, 3.015075206756592, 0.11513925343751907, 1.2191200256347656, 0.06393510103225708]], "avg_entropy": [0.9649441680974431], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": true, "gold_answer": "Wet-and-dry-bulb thermometer", "score_of_biggest_cluster": 0.25705817341804504, "score_of_first_answer": 0.25705817341804504, "max_score_of_first_answer": 0.9937431216239929, "min_score_of_first_answer": 0.002038126578554511, "avg_score": 0.25705817341804504, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4227455180714466, "std_of_first_answer": 0.4227455180714466, "avg_entropy_of_first_answer": 0.9649441680974431, "lex_sim": 1.0}
{"question": "In which town in Greater Manchester is the TV series set?", "answer": ["Fools to the right of me"], "scores": [[0.010431552305817604, 0.030849440023303032, 0.0015176335582509637, 0.28041261434555054, 0.11307574808597565, 0.6557083129882812, 0.7949395775794983]], "normalized_score": [0.06979971379041672], "tokens": [["\u2581F", "ools", "\u2581to", "\u2581the", "\u2581right", "\u2581of", "\u2581me"]], "entropy": [[4.29716682434082, 4.808568477630615, 2.241018295288086, 5.135808944702148, 3.217373847961426, 1.4256408214569092, 0.8634172677993774]], "avg_entropy": [3.1412849255970547], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rochdale, England", "score_of_biggest_cluster": 0.06979971379041672, "score_of_first_answer": 0.06979971379041672, "max_score_of_first_answer": 0.7949395775794983, "min_score_of_first_answer": 0.0015176335582509637, "avg_score": 0.06979971379041672, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30371892892733776, "std_of_first_answer": 0.30371892892733776, "avg_entropy_of_first_answer": 3.1412849255970547, "lex_sim": 1.0}
{"question": "January 17, 1929 saw the pride of Chester, Il, Elzie Segar debut what iconic cartoon character in a Thimble Theater strip?", "answer": ["Thimble Theater (comic strip)"], "scores": [[0.03160691261291504, 0.9612082839012146, 0.9798402190208435, 0.4648282825946808, 0.23380984365940094, 0.5910590291023254, 0.9599288702011108, 0.898642361164093, 0.9664881825447083]], "normalized_score": [0.48884788155555725], "tokens": [["\u2581Th", "im", "ble", "\u2581Theater", "\u2581(", "com", "ic", "\u2581strip", ")"]], "entropy": [[4.476025104522705, 0.3393293023109436, 0.157362163066864, 1.4829578399658203, 2.425086736679077, 2.3746845722198486, 0.19562146067619324, 0.46639037132263184, 0.27591195702552795]], "avg_entropy": [1.3548188341988459], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Popeye Cigarettes", "score_of_biggest_cluster": 0.48884788155555725, "score_of_first_answer": 0.48884788155555725, "max_score_of_first_answer": 0.9798402190208435, "min_score_of_first_answer": 0.03160691261291504, "avg_score": 0.48884788155555725, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34169723556049936, "std_of_first_answer": 0.34169723556049936, "avg_entropy_of_first_answer": 1.3548188341988459, "lex_sim": 1.0}
{"question": "What bird organ contains stones for grinding up food?", "answer": ["Eminence"], "scores": [[0.003683760529384017, 0.0037849457003176212, 0.19134651124477386]], "normalized_score": [0.013869384303689003], "tokens": [["\u2581E", "min", "ence"]], "entropy": [[4.812621116638184, 5.153900146484375, 2.420598030090332]], "avg_entropy": [4.129039764404297], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gizzard", "score_of_biggest_cluster": 0.013869384303689003, "score_of_first_answer": 0.013869384303689003, "max_score_of_first_answer": 0.19134651124477386, "min_score_of_first_answer": 0.003683760529384017, "avg_score": 0.013869384303689003, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08844122914492912, "std_of_first_answer": 0.08844122914492912, "avg_entropy_of_first_answer": 4.129039764404297, "lex_sim": 1.0}
