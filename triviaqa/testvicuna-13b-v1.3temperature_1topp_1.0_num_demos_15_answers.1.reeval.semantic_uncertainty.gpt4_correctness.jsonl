{"question": "What is the second event of a men\u2019s decathlon?", "answer": ["110 metres hurdles"], "scores": [[0.83447265625, 0.9609375, 0.041351318359375, 0.998046875, 0.011871337890625, 0.95703125, 1.0, 1.0]], "normalized_score": [0.37316134572029114], "tokens": [["\u2581", "1", "1", "0", "\u2581metres", "\u2581hur", "d", "les"]], "entropy": [[0.7080078125, 0.2122802734375, 0.278564453125, 0.0222930908203125, 1.5859375, 0.31201171875, 0.0003826618194580078, 0.0013027191162109375]], "avg_entropy": [0.3900975286960602], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Longjumper", "score_of_biggest_cluster": 0.37316134572029114, "score_of_first_answer": 0.37316134572029114, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.011871337890625, "avg_score": 0.37316134572029114, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4066970848409845, "std_of_first_answer": 0.4066970848409845, "avg_entropy_of_first_answer": 0.3900975286960602, "lex_sim": 1.0}
{"question": "Which is the largest cat in the world?", "answer": ["Lion"], "scores": [[0.34130859375]], "normalized_score": [0.34130859375], "tokens": [["\u2581Lion"]], "entropy": [[1.5947265625]], "avg_entropy": [1.5947265625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Manchurian Tiger", "score_of_biggest_cluster": 0.34130859375, "score_of_first_answer": 0.34130859375, "max_score_of_first_answer": 0.34130859375, "min_score_of_first_answer": 0.34130859375, "avg_score": 0.34130859375, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.5947265625, "lex_sim": 1.0}
{"question": "\"Complete the name of the group for whom Young Girl was a Number One single in 1968 \u2013 \"\"Gary ???? and the Union Gap\"\".\"", "answer": ["Gary Puckett and the Union Gap"], "scores": [[0.6328125, 0.99462890625, 0.99609375, 0.9970703125, 0.87939453125, 0.95263671875, 1.0, 1.0, 1.0]], "normalized_score": [0.9306489825248718], "tokens": [["\u2581Gary", "\u2581P", "uck", "ett", "\u2581and", "\u2581the", "\u2581Union", "\u2581G", "ap"]], "entropy": [[2.21875, 0.050201416015625, 0.03326416015625, 0.0214385986328125, 0.43310546875, 0.202392578125, 0.0012426376342773438, 0.002475738525390625, 0.00015211105346679688]], "avg_entropy": [0.3292247454325358], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "PUCKETT", "score_of_biggest_cluster": 0.9306489825248718, "score_of_first_answer": 0.9306489825248718, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.6328125, "avg_score": 0.9306489825248718, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.11468734900836461, "std_of_first_answer": 0.11468734900836461, "avg_entropy_of_first_answer": 0.3292247454325358, "lex_sim": 1.0}
{"question": "The fictional country of the Republic of Elbonia, supposedly somewhere in the former Soviet bloc, is featured in which comic strip?", "answer": ["Tom Thug"], "scores": [[0.0177459716796875, 0.3671875, 0.94921875]], "normalized_score": [0.18356280028820038], "tokens": [["\u2581Tom", "\u2581Th", "ug"]], "entropy": [[3.3515625, 2.880859375, 0.33251953125]], "avg_entropy": [2.1883138020833335], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ashook", "score_of_biggest_cluster": 0.18356280028820038, "score_of_first_answer": 0.18356280028820038, "max_score_of_first_answer": 0.94921875, "min_score_of_first_answer": 0.0177459716796875, "avg_score": 0.18356280028820038, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38420354767914366, "std_of_first_answer": 0.38420354767914366, "avg_entropy_of_first_answer": 2.1883138020833335, "lex_sim": 1.0}
{"question": "What Michelle Pfeiffer movie got a boost from the Coolio song Gangsta's Paradise?", "answer": ["Dangerous Minds"], "scores": [[0.47509765625, 0.99169921875, 0.998046875, 0.9951171875, 0.9990234375]], "normalized_score": [0.8589200377464294], "tokens": [["\u2581D", "anger", "ous", "\u2581M", "inds"]], "entropy": [[2.14453125, 0.0660400390625, 0.015716552734375, 0.0521240234375, 0.01035308837890625]], "avg_entropy": [0.45775299072265624], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dangerous Minds (film)", "score_of_biggest_cluster": 0.8589200377464294, "score_of_first_answer": 0.8589200377464294, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.47509765625, "avg_score": 0.8589200377464294, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2083652573603707, "std_of_first_answer": 0.2083652573603707, "avg_entropy_of_first_answer": 0.45775299072265624, "lex_sim": 1.0}
{"question": "Lloret de Mar lies on which Spanish costa?", "answer": ["Costa Brava"], "scores": [[0.650390625, 0.9873046875, 0.99755859375]], "normalized_score": [0.8620278239250183], "tokens": [["\u2581Costa", "\u2581Bra", "va"]], "entropy": [[1.3857421875, 0.09967041015625, 0.0215301513671875]], "avg_entropy": [0.5023142496744791], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Costa Brava", "score_of_biggest_cluster": 0.8620278239250183, "score_of_first_answer": 0.8620278239250183, "max_score_of_first_answer": 0.99755859375, "min_score_of_first_answer": 0.650390625, "avg_score": 0.8620278239250183, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.16129401264276458, "std_of_first_answer": 0.16129401264276458, "avg_entropy_of_first_answer": 0.5023142496744791, "lex_sim": 1.0}
{"question": "The original Ferris Wheel was erected in which US state for the 1893 World\u2019s Columbian Exposition?", "answer": ["Illinois"], "scores": [[0.90283203125]], "normalized_score": [0.90283203125], "tokens": [["\u2581Illinois"]], "entropy": [[0.453857421875]], "avg_entropy": [0.453857421875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Transportation in Illinois", "score_of_biggest_cluster": 0.90283203125, "score_of_first_answer": 0.90283203125, "max_score_of_first_answer": 0.90283203125, "min_score_of_first_answer": 0.90283203125, "avg_score": 0.90283203125, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.453857421875, "lex_sim": 1.0}
{"question": "Which band was founded by Chris Difford and Glenn Tilbrook in 1974?", "answer": ["SQUEEZE"], "scores": [[0.98779296875, 0.0002868175506591797, 0.99609375, 0.8896484375]], "normalized_score": [0.12587735056877136], "tokens": [["\u2581S", "QUE", "E", "ZE"]], "entropy": [[0.12335205078125, 0.00458526611328125, 0.029083251953125, 0.354736328125]], "avg_entropy": [0.12793922424316406], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Squeeze", "score_of_biggest_cluster": 0.12587735056877136, "score_of_first_answer": 0.12587735056877136, "max_score_of_first_answer": 0.99609375, "min_score_of_first_answer": 0.0002868175506591797, "avg_score": 0.12587735056877136, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4167430108102056, "std_of_first_answer": 0.4167430108102056, "avg_entropy_of_first_answer": 0.12793922424316406, "lex_sim": 1.0}
{"question": "Sidney Poitier, Eartha Kitt, Val Doonican and Ferenc Puskas were all born in what year?", "answer": ["1927"], "scores": [[0.20703125, 0.97412109375, 0.99169921875, 0.98095703125, 0.97802734375]], "normalized_score": [0.7187964916229248], "tokens": [["\u2581", "1", "9", "2", "7"]], "entropy": [[2.384765625, 0.1715087890625, 0.071533203125, 0.12744140625, 0.146728515625]], "avg_entropy": [0.5803955078125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and twenty-seven", "score_of_biggest_cluster": 0.7187964916229248, "score_of_first_answer": 0.7187964916229248, "max_score_of_first_answer": 0.99169921875, "min_score_of_first_answer": 0.20703125, "avg_score": 0.7187964916229248, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.30972301291083054, "std_of_first_answer": 0.30972301291083054, "avg_entropy_of_first_answer": 0.5803955078125, "lex_sim": 1.0}
{"question": "Cambridge Favourite is a variety of which fruit?", "answer": ["Apples"], "scores": [[0.013824462890625, 0.96484375]], "normalized_score": [0.11549218744039536], "tokens": [["\u2581App", "les"]], "entropy": [[1.7919921875, 0.264404296875]], "avg_entropy": [1.0281982421875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fragaria ananassa", "score_of_biggest_cluster": 0.11549218744039536, "score_of_first_answer": 0.11549218744039536, "max_score_of_first_answer": 0.96484375, "min_score_of_first_answer": 0.013824462890625, "avg_score": 0.11549218744039536, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4755096435546875, "std_of_first_answer": 0.4755096435546875, "avg_entropy_of_first_answer": 1.0281982421875, "lex_sim": 1.0}
{"question": "In the UK television series \u2018Dad\u2019s Army\u2019, what was Lance-Corpopal Jack Jones occupation?", "answer": ["Janitor"], "scores": [[0.06719970703125, 0.99951171875]], "normalized_score": [0.25916576385498047], "tokens": [["\u2581Jan", "itor"]], "entropy": [[4.49609375, 0.004421234130859375]], "avg_entropy": [2.2502574920654297], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Butchers", "score_of_biggest_cluster": 0.25916576385498047, "score_of_first_answer": 0.25916576385498047, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.06719970703125, "avg_score": 0.25916576385498047, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.466156005859375, "std_of_first_answer": 0.466156005859375, "avg_entropy_of_first_answer": 2.2502574920654297, "lex_sim": 1.0}
{"question": "Badminton debuted at the 1992 Summer Olympics. How many different people have won gold medals for the men's singles event in its first 5 Olympiads to 2008?", "answer": ["5 different people"], "scores": [[0.0318603515625, 0.50537109375, 0.5908203125, 0.69189453125]], "normalized_score": [0.2848321795463562], "tokens": [["\u2581", "5", "\u2581different", "\u2581people"]], "entropy": [[3.458984375, 1.50390625, 1.6904296875, 1.4169921875]], "avg_entropy": [2.017578125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "five", "score_of_biggest_cluster": 0.2848321795463562, "score_of_first_answer": 0.2848321795463562, "max_score_of_first_answer": 0.69189453125, "min_score_of_first_answer": 0.0318603515625, "avg_score": 0.2848321795463562, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2530566066455888, "std_of_first_answer": 0.2530566066455888, "avg_entropy_of_first_answer": 2.017578125, "lex_sim": 1.0}
{"question": "The Futhark is what type of alphabet?", "answer": ["Phonemic"], "scores": [[0.025482177734375, 0.98779296875, 0.034698486328125, 0.98779296875]], "normalized_score": [0.17138376832008362], "tokens": [["\u2581Ph", "on", "em", "ic"]], "entropy": [[3.07421875, 0.10455322265625, 0.4736328125, 0.1005859375]], "avg_entropy": [0.9382476806640625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Runic script", "score_of_biggest_cluster": 0.17138376832008362, "score_of_first_answer": 0.17138376832008362, "max_score_of_first_answer": 0.98779296875, "min_score_of_first_answer": 0.025482177734375, "avg_score": 0.17138376832008362, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4788624047025653, "std_of_first_answer": 0.4788624047025653, "avg_entropy_of_first_answer": 0.9382476806640625, "lex_sim": 1.0}
{"question": "Portrayed by James Doohan, what was the actual name of the engineer on the starship Enterprise, who was commonly called Scotty?", "answer": ["Montgomery \"Scotty\" Scott Online ExclusivesNews"], "scores": [[0.986328125, 0.9912109375, 0.9990234375, 0.05401611328125, 0.84912109375, 0.9794921875, 1.0, 0.9990234375, 0.99462890625, 0.0, 0.00244140625, 0.1268310546875, 0.0007433891296386719, 1.6093254089355469e-06]], "normalized_score": [0.0], "tokens": [["\u2581Mont", "gom", "ery", "\u2581\"", "Sc", "ot", "ty", "\"", "\u2581Scott", "\u2581Online", "\u2581Ex", "clus", "ives", "News"]], "entropy": [[0.11285400390625, 0.06561279296875, 0.00974273681640625, 0.52197265625, 0.463623046875, 0.099853515625, 4.0531158447265625e-06, 0.01004791259765625, 0.041168212890625, 0.7021484375, 3.68359375, 0.89892578125, 0.01104736328125, 1.890625]], "avg_entropy": [0.607944233076913], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Montgomery Scott", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.46967189445928276, "std_of_first_answer": 0.46967189445928276, "avg_entropy_of_first_answer": 0.607944233076913, "lex_sim": 1.0}
{"question": "A spoof of fraternal organizations such as the Masons, Shriners, Elks Club, or Moose Lodge, of what lodge was Fred Flintstone elected Grand Imperial Poobah?", "answer": ["Sabertooth Lodge"], "scores": [[0.0031375885009765625, 0.90771484375, 0.1693115234375, 1.0, 0.30517578125, 0.99951171875]], "normalized_score": [0.22975324094295502], "tokens": [["\u2581Sab", "er", "to", "oth", "\u2581L", "odge"]], "entropy": [[2.77734375, 0.41455078125, 0.83349609375, 0.0020751953125, 1.5859375, 0.004428863525390625]], "avg_entropy": [0.9363053639729818], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Loyal Order of Water Buffalo", "score_of_biggest_cluster": 0.22975324094295502, "score_of_first_answer": 0.22975324094295502, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0031375885009765625, "avg_score": 0.22975324094295502, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4153797147447293, "std_of_first_answer": 0.4153797147447293, "avg_entropy_of_first_answer": 0.9363053639729818, "lex_sim": 1.0}
{"question": "In which modern-day country is the birthplace of Buddha (Siddhartha Gautama)?", "answer": ["Lumbini, Nepal #561 : On est l\u00e0 pour apprendre"], "scores": [[0.10546875, 0.99951171875, 0.9990234375, 0.99267578125, 0.90478515625, 0.8955078125, 1.0, 7.748603820800781e-07, 0.003692626953125, 0.0921630859375, 0.0416259765625, 9.799003601074219e-05, 0.001979827880859375, 0.00012022256851196289, 0.002803802490234375, 0.458984375, 0.2423095703125, 0.99853515625, 1.0]], "normalized_score": [0.041937582194805145], "tokens": [["\u2581L", "um", "bin", "i", ",", "\u2581Nep", "al", "\u2581#", "5", "6", "1", "\u2581:", "\u2581On", "\u2581est", "\u2581l\u00e0", "\u2581pour", "\u2581app", "r", "endre"]], "entropy": [[0.654296875, 0.003032684326171875, 0.009613037109375, 0.06451416015625, 0.470703125, 0.501953125, 0.002086639404296875, 0.75927734375, 5.578125, 3.5859375, 3.5859375, 2.970703125, 5.07421875, 3.201171875, 3.96484375, 2.455078125, 3.341796875, 0.0128326416015625, 9.268522262573242e-05]], "avg_entropy": [1.9071692008721202], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nepol", "score_of_biggest_cluster": 0.041937582194805145, "score_of_first_answer": 0.041937582194805145, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 7.748603820800781e-07, "avg_score": 0.041937582194805145, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45137992064895693, "std_of_first_answer": 0.45137992064895693, "avg_entropy_of_first_answer": 1.9071692008721202, "lex_sim": 1.0}
{"question": "Which guitar innovator and player has a range of Gibson Guitars named after him?", "answer": ["Les Paul #MotherhoodUnfiltered with Alliyah Yisrael"], "scores": [[0.96728515625, 0.9853515625, 5.364418029785156e-07, 0.0029811859130859375, 0.0172576904296875, 0.00014483928680419922, 0.0106048583984375, 0.57861328125, 1.0, 0.00022125244140625, 0.001277923583984375, 0.0171966552734375, 0.008758544921875, 0.876953125, 0.0011835098266601562, 0.2000732421875, 0.8603515625]], "normalized_score": [0.015530397184193134], "tokens": [["\u2581Les", "\u2581Paul", "\u2581#", "M", "other", "hood", "Un", "filter", "ed", "\u2581with", "\u2581Al", "li", "y", "ah", "\u2581Y", "is", "rael"]], "entropy": [[0.2880859375, 0.1131591796875, 0.67529296875, 4.9921875, 5.015625, 1.8369140625, 1.845703125, 1.8515625, 0.0002989768981933594, 0.93603515625, 5.5859375, 3.724609375, 2.3046875, 0.91162109375, 4.23046875, 3.966796875, 0.66259765625]], "avg_entropy": [2.2906813621520996], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Les Paul", "score_of_biggest_cluster": 0.015530397184193134, "score_of_first_answer": 0.015530397184193134, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.364418029785156e-07, "avg_score": 0.015530397184193134, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4196417165904288, "std_of_first_answer": 0.4196417165904288, "avg_entropy_of_first_answer": 2.2906813621520996, "lex_sim": 1.0}
{"question": "Who, in 1960, made international news for banging his shoe on a table in the UN?", "answer": ["U.S.S.R. Premier Nikita Khrushchev"], "scores": [[0.004364013671875, 0.401611328125, 0.8173828125, 0.998046875, 0.108642578125, 0.7587890625, 0.9873046875, 0.916015625, 0.256591796875, 0.9833984375, 1.0, 0.744140625, 1.0, 0.99658203125, 0.99951171875]], "normalized_score": [0.4860095679759979], "tokens": [["\u2581U", ".", "S", ".", "S", ".", "R", ".", "\u2581Premier", "\u2581Nik", "ita", "\u2581K", "hr", "ush", "chev"]], "entropy": [[1.0634765625, 1.34375, 0.5703125, 0.0169677734375, 2.341796875, 0.90625, 0.09906005859375, 0.4013671875, 2.716796875, 0.1346435546875, 0.0013380050659179688, 0.64990234375, 0.0003724098205566406, 0.0228729248046875, 0.002803802490234375]], "avg_entropy": [0.6847807248433431], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Khrushchevian", "score_of_biggest_cluster": 0.4860095679759979, "score_of_first_answer": 0.4860095679759979, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.004364013671875, "avg_score": 0.4860095679759979, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3444404313073616, "std_of_first_answer": 0.3444404313073616, "avg_entropy_of_first_answer": 0.6847807248433431, "lex_sim": 1.0}
{"question": "Edward Elgar was appointed Master of the King\u2019s Musick to which monarch?", "answer": ["George V"], "scores": [[0.141357421875, 0.9609375]], "normalized_score": [0.3685588836669922], "tokens": [["\u2581George", "\u2581V"]], "entropy": [[1.0419921875, 0.233154296875]], "avg_entropy": [0.6375732421875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George V of Great Britain and Ireland", "score_of_biggest_cluster": 0.3685588836669922, "score_of_first_answer": 0.3685588836669922, "max_score_of_first_answer": 0.9609375, "min_score_of_first_answer": 0.141357421875, "avg_score": 0.3685588836669922, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4097900390625, "std_of_first_answer": 0.4097900390625, "avg_entropy_of_first_answer": 0.6375732421875, "lex_sim": 1.0}
{"question": "Which poet was married to Ted Hughes until her death in 1963?", "answer": ["Edith Poet"], "scores": [[0.0003685951232910156, 0.9697265625, 0.0025615692138671875, 0.08740234375]], "normalized_score": [0.016819264739751816], "tokens": [["\u2581Ed", "ith", "\u2581Po", "et"]], "entropy": [[0.10400390625, 0.14013671875, 3.34375, 0.62939453125]], "avg_entropy": [1.0543212890625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Victoria Lucas", "score_of_biggest_cluster": 0.016819264739751816, "score_of_first_answer": 0.016819264739751816, "max_score_of_first_answer": 0.9697265625, "min_score_of_first_answer": 0.0003685951232910156, "avg_score": 0.016819264739751816, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4083761026696881, "std_of_first_answer": 0.4083761026696881, "avg_entropy_of_first_answer": 1.0543212890625, "lex_sim": 1.0}
{"question": "Which acid is found in rhubarb leaves ?", "answer": ["Oxalic acid"], "scores": [[0.9013671875, 0.9951171875, 0.9990234375, 0.5126953125]], "normalized_score": [0.8232898116111755], "tokens": [["\u2581Ox", "al", "ic", "\u2581acid"]], "entropy": [[0.37353515625, 0.04443359375, 0.00754547119140625, 1.130859375]], "avg_entropy": [0.38909339904785156], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "OXALIC", "score_of_biggest_cluster": 0.8232898116111755, "score_of_first_answer": 0.8232898116111755, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.5126953125, "avg_score": 0.8232898116111755, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.19978938934439058, "std_of_first_answer": 0.19978938934439058, "avg_entropy_of_first_answer": 0.38909339904785156, "lex_sim": 1.0}
{"question": "Which famous conductor conducted the very first performance of Hoist's 'The Planets' in 1918?", "answer": ["Walter Straramond #GivingTuesday is a global day of giving fueled by the power of social media and collaboration."], "scores": [[0.0013589859008789062, 0.0255279541015625, 0.38671875, 0.0011844635009765625, 4.76837158203125e-07, 0.0030460357666015625, 0.00013077259063720703, 0.039825439453125, 0.9765625, 0.9716796875, 0.007389068603515625, 0.54248046875, 0.7412109375, 0.87109375, 0.994140625, 0.98583984375, 0.2469482421875, 0.9326171875, 0.9990234375, 0.9951171875, 0.99951171875, 0.9970703125, 0.99853515625, 0.9990234375, 0.990234375, 0.95947265625, 0.966796875, 0.806640625, 1.0, 0.9990234375, 1.0, 0.9990234375, 0.998046875, 0.99609375, 0.99853515625, 0.99462890625, 0.99267578125, 0.326171875, 0.9951171875, 1.0, 0.8818359375, 0.9990234375, 0.99365234375, 0.740234375, 0.99755859375, 0.998046875, 0.8525390625, 0.97216796875]], "normalized_score": [0.28617414832115173], "tokens": [["\u2581Walter", "\u2581Str", "aram", "ond", "\u2581#", "G", "iving", "T", "ues", "day", "\u2581is", "\u2581a", "\u2581global", "\u2581day", "\u2581of", "\u2581giving", "\u2581fue", "led", "\u2581by", "\u2581the", "\u2581power", "\u2581of", "\u2581social", "\u2581media", "\u2581and", "\u2581collaboration", ".", "\u2581C", "ele", "br", "ated", "\u2581on", "\u2581the", "\u2581T", "ues", "day", "\u2581following", "\u2581Thanks", "g", "iving", "\u2581(", "in", "\u2581the", "\u2581U", ".", "S", ".)", "\u2581and"]], "entropy": [[2.7265625, 4.2421875, 2.388671875, 1.2021484375, 1.775390625, 5.2421875, 3.4609375, 3.556640625, 0.1949462890625, 0.2127685546875, 2.66796875, 1.873046875, 1.138671875, 0.74755859375, 0.052032470703125, 0.11285400390625, 1.802734375, 0.51025390625, 0.00946807861328125, 0.0411376953125, 0.003078460693359375, 0.023162841796875, 0.01381683349609375, 0.009674072265625, 0.070068359375, 0.255615234375, 0.2322998046875, 0.89697265625, 0.0017995834350585938, 0.00919342041015625, 0.00023376941680908203, 0.0081787109375, 0.018035888671875, 0.031768798828125, 0.0159759521484375, 0.0418701171875, 0.050506591796875, 1.837890625, 0.0413818359375, 0.0008292198181152344, 0.52783203125, 0.01027679443359375, 0.049957275390625, 0.791015625, 0.0199127197265625, 0.0174102783203125, 0.465087890625, 0.146728515625]], "avg_entropy": [0.8239321236809095], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sir Adrian Cedric Boult", "score_of_biggest_cluster": 0.28617414832115173, "score_of_first_answer": 0.28617414832115173, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 4.76837158203125e-07, "avg_score": 0.28617414832115173, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3756166899576538, "std_of_first_answer": 0.3756166899576538, "avg_entropy_of_first_answer": 0.8239321236809095, "lex_sim": 1.0}
{"question": "Which Olympic event covers the longest distance?", "answer": ["The marathon"], "scores": [[0.138671875, 0.572265625, 0.99951171875]], "normalized_score": [0.4296596944332123], "tokens": [["\u2581The", "\u2581mar", "athon"]], "entropy": [[3.08203125, 1.962890625, 0.0037250518798828125]], "avg_entropy": [1.682882308959961], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marathon (race)", "score_of_biggest_cluster": 0.4296596944332123, "score_of_first_answer": 0.4296596944332123, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.138671875, "avg_score": 0.4296596944332123, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.35143957932595227, "std_of_first_answer": 0.35143957932595227, "avg_entropy_of_first_answer": 1.682882308959961, "lex_sim": 1.0}
{"question": "Ford recently announced that they were shutting down production of another of their marques. Active for the last 81 years, what brand is being discontinued?", "answer": ["Mercury"], "scores": [[0.131103515625, 0.99951171875]], "normalized_score": [0.3619937598705292], "tokens": [["\u2581Mercur", "y"]], "entropy": [[2.75, 0.0066070556640625]], "avg_entropy": [1.3783035278320312], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mercury", "score_of_biggest_cluster": 0.3619937598705292, "score_of_first_answer": 0.3619937598705292, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.131103515625, "avg_score": 0.3619937598705292, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4342041015625, "std_of_first_answer": 0.4342041015625, "avg_entropy_of_first_answer": 1.3783035278320312, "lex_sim": 1.0}
{"question": "Which animal represents the deadly sin of envy?", "answer": ["Snake #27 - 12\" x 12\" #27 - 12\" x 12\""], "scores": [[0.35205078125, 0.99609375, 8.940696716308594e-07, 0.0225830078125, 0.0209503173828125, 0.00978851318359375, 0.0028362274169921875, 0.1256103515625, 0.0858154296875, 0.002529144287109375, 0.0248260498046875, 0.9501953125, 0.51416015625, 0.380859375, 0.94873046875, 0.170654296875, 0.1680908203125, 0.755859375, 0.437744140625, 0.344970703125, 0.82373046875, 0.93359375, 0.97509765625, 0.9912109375, 0.99365234375, 0.99365234375, 0.99609375, 0.96240234375]], "normalized_score": [0.1395723968744278], "tokens": [["\u2581S", "nake", "\u2581#", "2", "7", "\u2581-", "\u2581", "1", "2", "\"", "\u2581x", "\u2581", "1", "2", "\"", "\u2581#", "2", "7", "\u2581-", "\u2581", "1", "2", "\"", "\u2581x", "\u2581", "1", "2", "\""]], "entropy": [[1.9189453125, 0.039306640625, 0.80126953125, 5.13671875, 3.35546875, 3.466796875, 2.458984375, 4.1015625, 3.6328125, 4.25, 5.45703125, 0.460205078125, 1.654296875, 1.888671875, 0.350830078125, 4.00390625, 4.12890625, 1.306640625, 3.205078125, 3.216796875, 0.814453125, 0.430419921875, 0.2027587890625, 0.0693359375, 0.051422119140625, 0.05084228515625, 0.031890869140625, 0.2381591796875]], "avg_entropy": [2.0258396693638394], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Snkae", "score_of_biggest_cluster": 0.1395723968744278, "score_of_first_answer": 0.1395723968744278, "max_score_of_first_answer": 0.99609375, "min_score_of_first_answer": 8.940696716308594e-07, "avg_score": 0.1395723968744278, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4090679114383429, "std_of_first_answer": 0.4090679114383429, "avg_entropy_of_first_answer": 2.0258396693638394, "lex_sim": 1.0}
{"question": "August 2, 1990 saw Iraq invading what country, sparking a series of events that culminated in the 1st Gulf War?", "answer": ["Kuwait"], "scores": [[0.974609375, 0.998046875, 1.0]], "normalized_score": [0.9908179044723511], "tokens": [["\u2581K", "uw", "ait"]], "entropy": [[0.2403564453125, 0.02191162109375, 0.0021152496337890625]], "avg_entropy": [0.08812777201334636], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ISO 3166-1:KW", "score_of_biggest_cluster": 0.9908179044723511, "score_of_first_answer": 0.9908179044723511, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.974609375, "avg_score": 0.9908179044723511, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.011536487725696192, "std_of_first_answer": 0.011536487725696192, "avg_entropy_of_first_answer": 0.08812777201334636, "lex_sim": 1.0}
{"question": "Which fictional UK television doctor lives in Tannochbrae?", "answer": ["Dr James Kerr"], "scores": [[0.72607421875, 0.0894775390625, 0.01009368896484375, 0.521484375]], "normalized_score": [0.13598676025867462], "tokens": [["\u2581Dr", "\u2581James", "\u2581K", "err"]], "entropy": [[1.060546875, 1.23828125, 3.25390625, 2.111328125]], "avg_entropy": [1.916015625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dr Finlay", "score_of_biggest_cluster": 0.13598676025867462, "score_of_first_answer": 0.13598676025867462, "max_score_of_first_answer": 0.72607421875, "min_score_of_first_answer": 0.01009368896484375, "avg_score": 0.13598676025867462, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29729957445130123, "std_of_first_answer": 0.29729957445130123, "avg_entropy_of_first_answer": 1.916015625, "lex_sim": 1.0}
{"question": "In 2010 which country saw the Red Shirt Rebellion?", "answer": ["Thailand"], "scores": [[0.955078125, 0.9990234375]], "normalized_score": [0.9768036603927612], "tokens": [["\u2581Th", "ailand"]], "entropy": [[0.40478515625, 0.01141357421875]], "avg_entropy": [0.208099365234375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Muang Thai", "score_of_biggest_cluster": 0.9768036603927612, "score_of_first_answer": 0.9768036603927612, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.955078125, "avg_score": 0.9768036603927612, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.02197265625, "std_of_first_answer": 0.02197265625, "avg_entropy_of_first_answer": 0.208099365234375, "lex_sim": 1.0}
{"question": "How many valves does a trumpet have?", "answer": ["3"], "scores": [[0.00908660888671875, 0.97021484375]], "normalized_score": [0.09389336407184601], "tokens": [["\u2581", "3"]], "entropy": [[0.958984375, 0.19482421875]], "avg_entropy": [0.576904296875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "3", "score_of_biggest_cluster": 0.09389336407184601, "score_of_first_answer": 0.09389336407184601, "max_score_of_first_answer": 0.97021484375, "min_score_of_first_answer": 0.00908660888671875, "avg_score": 0.09389336407184601, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4805641174316406, "std_of_first_answer": 0.4805641174316406, "avg_entropy_of_first_answer": 0.576904296875, "lex_sim": 1.0}
{"question": "What does the musical term con fuoco mean", "answer": ["With fire #1193: Marine Links Trump Tower Moscow To Russian Mob Boss With Ties To FBI"], "scores": [[0.64599609375, 0.95068359375, 1.7881393432617188e-07, 0.004520416259765625, 0.02740478515625, 0.0239410400390625, 0.06256103515625, 0.0023326873779296875, 2.384185791015625e-07, 0.00011104345321655273, 0.320068359375, 0.00023555755615234375, 0.00679779052734375, 0.149658203125, 0.100830078125, 0.10528564453125, 0.68798828125, 0.1470947265625, 0.03411865234375, 0.99658203125, 0.07177734375, 0.251953125, 0.99072265625, 0.978515625, 0.01727294921875, 0.82666015625]], "normalized_score": [0.024128474295139313], "tokens": [["\u2581With", "\u2581fire", "\u2581#", "1", "1", "9", "3", ":", "\u2581Marine", "\u2581Link", "s", "\u2581Trump", "\u2581Tower", "\u2581Moscow", "\u2581To", "\u2581Russian", "\u2581M", "ob", "\u2581B", "oss", "\u2581With", "\u2581T", "ies", "\u2581To", "\u2581F", "BI"]], "entropy": [[1.21875, 0.263916015625, 1.4912109375, 3.333984375, 3.875, 2.34765625, 3.13671875, 2.505859375, 4.640625, 6.03125, 4.2265625, 4.16796875, 4.12890625, 4.04296875, 2.603515625, 4.46484375, 1.2822265625, 0.70166015625, 3.728515625, 0.03411865234375, 3.7265625, 3.9296875, 0.0833740234375, 0.1611328125, 3.146484375, 0.7236328125]], "avg_entropy": [2.6921973595252404], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "With fire", "score_of_biggest_cluster": 0.024128474295139313, "score_of_first_answer": 0.024128474295139313, "max_score_of_first_answer": 0.99658203125, "min_score_of_first_answer": 1.7881393432617188e-07, "avg_score": 0.024128474295139313, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3692619592156486, "std_of_first_answer": 0.3692619592156486, "avg_entropy_of_first_answer": 2.6921973595252404, "lex_sim": 1.0}
{"question": "What was the name of the country prior to being renamed as Namibia?", "answer": ["German Southwest Africa"], "scores": [[0.458251953125, 0.9677734375, 0.06353759765625, 0.99755859375]], "normalized_score": [0.40946027636528015], "tokens": [["\u2581German", "\u2581South", "west", "\u2581Africa"]], "entropy": [[1.13671875, 0.2469482421875, 0.72412109375, 0.0218963623046875]], "avg_entropy": [0.5324211120605469], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Southwest Africa", "score_of_biggest_cluster": 0.40946027636528015, "score_of_first_answer": 0.40946027636528015, "max_score_of_first_answer": 0.99755859375, "min_score_of_first_answer": 0.06353759765625, "avg_score": 0.40946027636528015, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.38707137978751466, "std_of_first_answer": 0.38707137978751466, "avg_entropy_of_first_answer": 0.5324211120605469, "lex_sim": 1.0}
{"question": "Who was the first cricketer to score 10,000 test runs?", "answer": ["Sachin Tendulkar"], "scores": [[0.7373046875, 1.0, 0.98486328125, 0.99853515625, 0.99853515625, 0.99951171875]], "normalized_score": [0.9475226402282715], "tokens": [["\u2581Sach", "in", "\u2581T", "end", "ul", "kar"]], "entropy": [[1.29296875, 0.0002143383026123047, 0.115234375, 0.01477813720703125, 0.01352691650390625, 0.00313568115234375]], "avg_entropy": [0.23997636636098227], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sunil Manohar Gavaskar", "score_of_biggest_cluster": 0.9475226402282715, "score_of_first_answer": 0.9475226402282715, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.7373046875, "avg_score": 0.9475226402282715, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09665995695132515, "std_of_first_answer": 0.09665995695132515, "avg_entropy_of_first_answer": 0.23997636636098227, "lex_sim": 1.0}
{"question": "Which contestant was first to be voted off the 2011 'Strictly Come Dancing' competition?", "answer": ["Matthew Cutler **RELEASE BLITZ** His To Possess (A Bad Boy Mafia Romance) by Opal Carew"], "scores": [[0.0070037841796875, 0.9892578125, 0.974609375, 0.970703125, 5.960464477539062e-07, 2.086162567138672e-06, 6.252527236938477e-05, 0.175048828125, 0.0009937286376953125, 0.990234375, 0.8818359375, 2.0563602447509766e-05, 0.0030307769775390625, 0.02972412109375, 0.99951171875, 0.1630859375, 0.0145721435546875, 0.01898193359375, 0.98974609375, 0.60791015625, 0.9990234375, 0.99658203125, 0.99609375, 1.0, 0.9248046875, 0.499755859375, 0.2318115234375, 0.99462890625, 0.99072265625, 0.99658203125]], "normalized_score": [0.06122775375843048], "tokens": [["\u2581Matthew", "\u2581C", "ut", "ler", "\u2581**", "RELEASE", "\u2581B", "L", "IT", "Z", "**", "\u2581His", "\u2581To", "\u2581Poss", "ess", "\u2581(", "A", "\u2581Bad", "\u2581Boy", "\u2581M", "af", "ia", "\u2581Rom", "ance", ")", "\u2581by", "\u2581Op", "al", "\u2581Care", "w"]], "entropy": [[3.798828125, 0.0885009765625, 0.209228515625, 0.189697265625, 1.0849609375, 5.8203125, 0.8984375, 2.287109375, 0.76904296875, 0.08013916015625, 0.86669921875, 2.4921875, 6.0, 2.7265625, 0.00405120849609375, 1.3798828125, 3.26171875, 5.37109375, 0.0701904296875, 1.134765625, 0.01116180419921875, 0.0268096923828125, 0.038055419921875, 0.0023345947265625, 0.339111328125, 2.080078125, 3.4921875, 0.052001953125, 0.08050537109375, 0.031524658203125]], "avg_entropy": [1.4895726521809896], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Edwina Curry", "score_of_biggest_cluster": 0.06122775375843048, "score_of_first_answer": 0.06122775375843048, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.960464477539062e-07, "avg_score": 0.06122775375843048, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45188471409037456, "std_of_first_answer": 0.45188471409037456, "avg_entropy_of_first_answer": 1.4895726521809896, "lex_sim": 1.0}
{"question": "The highest mountain in North America is part of which mountain range?", "answer": ["Denali CHAPTER TWELVE: THE EXAMINATION"], "scores": [[0.90673828125, 0.99853515625, 0.0, 0.00928497314453125, 0.289306640625, 0.180419921875, 1.0, 0.08404541015625, 0.031982421875, 0.005886077880859375, 0.0577392578125, 0.260009765625, 0.9541015625]], "normalized_score": [0.0], "tokens": [["\u2581Den", "ali", "\u2581CHAPTER", "\u2581T", "W", "EL", "VE", ":", "\u2581THE", "\u2581EX", "AM", "IN", "ATION"]], "entropy": [[0.468017578125, 0.0136260986328125, 1.6513671875, 2.833984375, 1.0302734375, 0.8251953125, 0.0013341903686523438, 2.69921875, 3.7265625, 4.4921875, 2.619140625, 2.826171875, 0.266357421875]], "avg_entropy": [1.8041105270385742], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alaska range", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40845086053470087, "std_of_first_answer": 0.40845086053470087, "avg_entropy_of_first_answer": 1.8041105270385742, "lex_sim": 1.0}
{"question": "The cocktail called a Tom Collins consists of lemon or lime juice, sugar, soda water and what else?", "answer": ["Gin"], "scores": [[0.525390625, 0.9970703125]], "normalized_score": [0.7237758040428162], "tokens": [["\u2581G", "in"]], "entropy": [[1.4208984375, 0.0271759033203125]], "avg_entropy": [0.7240371704101562], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mothers ruin", "score_of_biggest_cluster": 0.7237758040428162, "score_of_first_answer": 0.7237758040428162, "max_score_of_first_answer": 0.9970703125, "min_score_of_first_answer": 0.525390625, "avg_score": 0.7237758040428162, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.23583984375, "std_of_first_answer": 0.23583984375, "avg_entropy_of_first_answer": 0.7240371704101562, "lex_sim": 1.0}
{"question": "Which city is the home of the Palais du Luxembourg?", "answer": ["Paris"], "scores": [[0.89453125]], "normalized_score": [0.89453125], "tokens": [["\u2581Paris"]], "entropy": [[0.81787109375]], "avg_entropy": [0.81787109375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pa\u0159\u00ed\u017e", "score_of_biggest_cluster": 0.89453125, "score_of_first_answer": 0.89453125, "max_score_of_first_answer": 0.89453125, "min_score_of_first_answer": 0.89453125, "avg_score": 0.89453125, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.81787109375, "lex_sim": 1.0}
{"question": "Which boxer fought Mohammed Ali in the fight known as 'The Thriller In Manilla'?", "answer": ["Joe Frazier"], "scores": [[0.99267578125, 0.9501953125, 0.9951171875, 1.0]], "normalized_score": [0.9842912554740906], "tokens": [["\u2581Joe", "\u2581Fra", "z", "ier"]], "entropy": [[0.05670166015625, 0.2607421875, 0.03790283203125, 0.001007080078125]], "avg_entropy": [0.08908843994140625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joe Speedo Frazier", "score_of_biggest_cluster": 0.9842912554740906, "score_of_first_answer": 0.9842912554740906, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.9501953125, "avg_score": 0.9842912554740906, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01997892401756404, "std_of_first_answer": 0.01997892401756404, "avg_entropy_of_first_answer": 0.08908843994140625, "lex_sim": 1.0}
{"question": "Until 1796, there was a state in the United States called Franklin. Today it's known as what?", "answer": ["Tennessee"], "scores": [[0.77587890625]], "normalized_score": [0.77587890625], "tokens": [["\u2581Tennessee"]], "entropy": [[1.140625]], "avg_entropy": [1.140625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tennessee!", "score_of_biggest_cluster": 0.77587890625, "score_of_first_answer": 0.77587890625, "max_score_of_first_answer": 0.77587890625, "min_score_of_first_answer": 0.77587890625, "avg_score": 0.77587890625, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.140625, "lex_sim": 1.0}
{"question": "Who did US Forbes magazine rank in 2010 as the most powerful woman in the world?", "answer": ["Angela Merkel"], "scores": [[0.91064453125, 1.0, 0.9951171875, 1.0]], "normalized_score": [0.9756763577461243], "tokens": [["\u2581Ang", "ela", "\u2581Mer", "kel"]], "entropy": [[0.4765625, 0.0024776458740234375, 0.039642333984375, 0.001293182373046875]], "avg_entropy": [0.12999391555786133], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "First Lady Obama", "score_of_biggest_cluster": 0.9756763577461243, "score_of_first_answer": 0.9756763577461243, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.91064453125, "avg_score": 0.9756763577461243, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0380395460021668, "std_of_first_answer": 0.0380395460021668, "avg_entropy_of_first_answer": 0.12999391555786133, "lex_sim": 1.0}
{"question": "Who was on the thrones of England and Scotland at the time of the Act of Union in 1707?", "answer": ["Great Britain and Scotland"], "scores": [[0.0004820823669433594, 0.92333984375, 0.1328125, 0.064453125]], "normalized_score": [0.04418160393834114], "tokens": [["\u2581Great", "\u2581Britain", "\u2581and", "\u2581Scotland"]], "entropy": [[0.401123046875, 0.56591796875, 1.8955078125, 1.4404296875]], "avg_entropy": [1.07574462890625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Anne", "score_of_biggest_cluster": 0.04418160393834114, "score_of_first_answer": 0.04418160393834114, "max_score_of_first_answer": 0.92333984375, "min_score_of_first_answer": 0.0004820823669433594, "avg_score": 0.04418160393834114, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3742127538649813, "std_of_first_answer": 0.3742127538649813, "avg_entropy_of_first_answer": 1.07574462890625, "lex_sim": 1.0}
{"question": "Who retired from the \u2018Eggheads\u2019 team in May 2014?", "answer": ["Dara \u00d3 Briain"], "scores": [[0.005100250244140625, 0.01025390625, 0.591796875, 0.998046875, 1.0, 0.99951171875]], "normalized_score": [0.17711910605430603], "tokens": [["\u2581D", "ara", "\u2581\u00d3", "\u2581B", "ri", "ain"]], "entropy": [[3.46875, 1.1806640625, 1.0234375, 0.0198974609375, 0.00037479400634765625, 0.003875732421875]], "avg_entropy": [0.9494999249776205], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "DAPHNE Fowler", "score_of_biggest_cluster": 0.17711910605430603, "score_of_first_answer": 0.17711910605430603, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.005100250244140625, "avg_score": 0.17711910605430603, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44343746082300445, "std_of_first_answer": 0.44343746082300445, "avg_entropy_of_first_answer": 0.9494999249776205, "lex_sim": 1.0}
{"question": "The bumblebee or hog-nosed is the smallest mammal in the world - what is it?", "answer": ["Bumblebee bat"], "scores": [[0.191650390625, 0.9990234375, 0.99560546875, 0.9970703125, 0.8134765625]], "normalized_score": [0.6884124279022217], "tokens": [["\u2581B", "umble", "be", "e", "\u2581bat"]], "entropy": [[2.54296875, 0.01450347900390625, 0.038726806640625, 0.0216217041015625, 0.75732421875]], "avg_entropy": [0.6750289916992187], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bat Conservation", "score_of_biggest_cluster": 0.6884124279022217, "score_of_first_answer": 0.6884124279022217, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.191650390625, "avg_score": 0.6884124279022217, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.312082484761516, "std_of_first_answer": 0.312082484761516, "avg_entropy_of_first_answer": 0.6750289916992187, "lex_sim": 1.0}
{"question": "\"\"\"Aimee Anne\"\" are the first names of which currently successful British female singer?\"", "answer": ["Aa #2 in Best Things To Do in Singapore"], "scores": [[0.87939453125, 5.066394805908203e-06, 2.562999725341797e-06, 0.0175323486328125, 0.00015366077423095703, 1.1801719665527344e-05, 2.4497509002685547e-05, 0.033355712890625, 0.8818359375, 0.6025390625, 0.03594970703125]], "normalized_score": [0.002193876774981618], "tokens": [["\u2581A", "a", "\u2581#", "2", "\u2581in", "\u2581Best", "\u2581Things", "\u2581To", "\u2581Do", "\u2581in", "\u2581Singapore"]], "entropy": [[0.6982421875, 0.64501953125, 3.818359375, 5.34765625, 4.43359375, 6.09765625, 4.16015625, 3.080078125, 0.80859375, 2.45703125, 5.2578125]], "avg_entropy": [3.3458362926136362], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Duffy (disambiguation)", "score_of_biggest_cluster": 0.002193876774981618, "score_of_first_answer": 0.002193876774981618, "max_score_of_first_answer": 0.8818359375, "min_score_of_first_answer": 2.562999725341797e-06, "avg_score": 0.002193876774981618, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35299944091078433, "std_of_first_answer": 0.35299944091078433, "avg_entropy_of_first_answer": 3.3458362926136362, "lex_sim": 1.0}
{"question": "Which country is known as The Hashemite Kingdom?", "answer": ["Jordan"], "scores": [[0.947265625]], "normalized_score": [0.947265625], "tokens": [["\u2581Jordan"]], "entropy": [[0.39111328125]], "avg_entropy": [0.39111328125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jordan (country)", "score_of_biggest_cluster": 0.947265625, "score_of_first_answer": 0.947265625, "max_score_of_first_answer": 0.947265625, "min_score_of_first_answer": 0.947265625, "avg_score": 0.947265625, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.39111328125, "lex_sim": 1.0}
{"question": "Which family of insects has species named 'drivers', 'weavers', 'and even individuals called 'soldiers'?", "answer": ["Spiders' #BikramChoudhury"], "scores": [[0.423828125, 0.452880859375, 6.002187728881836e-05, 2.5033950805664062e-06, 0.0036907196044921875, 1.728534698486328e-05, 0.388427734375, 0.0044097900390625, 0.98876953125, 0.9990234375, 0.83740234375]], "normalized_score": [0.013450529426336288], "tokens": [["\u2581Sp", "iders", "'", "\u2581#", "B", "ik", "ram", "Ch", "oud", "h", "ury"]], "entropy": [[2.251953125, 0.7001953125, 0.900390625, 1.3330078125, 4.67578125, 3.39453125, 1.818359375, 2.82421875, 0.09649658203125, 0.009674072265625, 0.55419921875]], "avg_entropy": [1.687164306640625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "History of ants", "score_of_biggest_cluster": 0.013450529426336288, "score_of_first_answer": 0.013450529426336288, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 2.5033950805664062e-06, "avg_score": 0.013450529426336288, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39144524563153504, "std_of_first_answer": 0.39144524563153504, "avg_entropy_of_first_answer": 1.687164306640625, "lex_sim": 1.0}
{"question": "In which 70\u2019s sitcom did we hear about the Tooting Popular Front", "answer": ["Porridge (TV series) -- Tooting Popular Front was a recurring joke/plot point in the series, with lead character Fletcher referring to his time in prison and his association with the organization."], "scores": [[0.00693511962890625, 0.9873046875, 0.072509765625, 0.1744384765625, 0.9404296875, 0.994140625, 2.568960189819336e-05, 0.03314208984375, 0.99365234375, 0.97216796875, 0.9931640625, 0.6328125, 0.424072265625, 0.0208740234375, 0.99951171875, 1.0, 0.7548828125, 1.0, 0.0124053955078125, 0.8212890625, 0.615234375, 0.88427734375, 0.96484375, 0.318115234375, 0.1395263671875, 0.1199951171875, 0.12481689453125, 0.99951171875, 0.86181640625, 0.998046875, 0.93603515625, 0.317626953125, 0.99755859375, 0.65087890625, 0.9072265625, 0.66796875, 0.91162109375, 0.240234375, 0.5888671875, 0.2066650390625, 0.99951171875, 0.9228515625, 0.01483154296875, 0.9375, 1.7285346984863281e-06, 8.392333984375e-05, 3.039836883544922e-06, 0.1842041015625]], "normalized_score": [0.15730248391628265], "tokens": [["\u2581Por", "ridge", "\u2581(", "TV", "\u2581series", ")", "\u2581--", "\u2581To", "oting", "\u2581Popular", "\u2581Front", "\u2581was", "\u2581a", "\u2581rec", "urr", "ing", "\u2581jo", "ke", "/", "plot", "\u2581point", "\u2581in", "\u2581the", "\u2581series", ",", "\u2581with", "\u2581lead", "\u2581character", "\u2581F", "let", "cher", "\u2581referring", "\u2581to", "\u2581his", "\u2581time", "\u2581in", "\u2581prison", "\u2581and", "\u2581his", "\u2581association", "\u2581with", "\u2581the", "\u2581organization", ".", "\u2581#", "Book", "Feature", ":"]], "entropy": [[2.15234375, 0.09368896484375, 1.107421875, 2.98828125, 0.271240234375, 0.04986572265625, 0.6796875, 4.140625, 0.06085205078125, 0.220458984375, 0.0560302734375, 1.373046875, 1.0693359375, 1.671875, 0.00498199462890625, 0.0001971721649169922, 1.19921875, 0.00030541419982910156, 1.03125, 0.88330078125, 1.255859375, 0.5625, 0.2181396484375, 1.06640625, 0.923828125, 3.236328125, 1.796875, 0.003719329833984375, 0.86669921875, 0.0200958251953125, 0.23828125, 2.75390625, 0.023223876953125, 1.103515625, 0.658203125, 0.990234375, 0.434814453125, 2.0390625, 1.595703125, 1.5556640625, 0.00370025634765625, 0.342529296875, 1.64453125, 0.35791015625, 0.5849609375, 4.76953125, 4.109375, 3.142578125]], "avg_entropy": [1.1531704862912495], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tooting Popular Front", "score_of_biggest_cluster": 0.15730248391628265, "score_of_first_answer": 0.15730248391628265, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.7285346984863281e-06, "avg_score": 0.15730248391628265, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4015026102936136, "std_of_first_answer": 0.4015026102936136, "avg_entropy_of_first_answer": 1.1531704862912495, "lex_sim": 1.0}
{"question": "The character Sally Bowles appears in which film?", "answer": ["Cabaret (film) #JCPenneyStylinSavings #BackToSchool"], "scores": [[0.6904296875, 0.98291015625, 0.291259765625, 0.25048828125, 0.99560546875, 1.430511474609375e-06, 0.00047969818115234375, 8.821487426757812e-06, 0.129638671875, 0.9921875, 0.0002892017364501953, 0.334228515625, 0.060577392578125, 0.27587890625, 0.96142578125, 1.0, 0.1005859375, 0.004436492919921875, 0.466796875, 0.9189453125, 1.0]], "normalized_score": [0.058161213994026184], "tokens": [["\u2581Cab", "aret", "\u2581(", "film", ")", "\u2581#", "J", "CP", "en", "ney", "St", "y", "lin", "S", "av", "ings", "\u2581#", "Back", "To", "Sch", "ool"]], "entropy": [[1.638671875, 0.09356689453125, 1.2607421875, 0.7548828125, 0.04364013671875, 0.82666015625, 4.30859375, 2.630859375, 4.33984375, 0.061614990234375, 3.265625, 1.4501953125, 1.0263671875, 3.3203125, 0.2337646484375, 0.0010013580322265625, 1.224609375, 4.578125, 1.2255859375, 0.425048828125, 0.0006933212280273438]], "avg_entropy": [1.557638304574149], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Caberet", "score_of_biggest_cluster": 0.058161213994026184, "score_of_first_answer": 0.058161213994026184, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.430511474609375e-06, "avg_score": 0.058161213994026184, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4090127875674065, "std_of_first_answer": 0.4090127875674065, "avg_entropy_of_first_answer": 1.557638304574149, "lex_sim": 1.0}
{"question": "I n dressmaking what are featherweight, skirt weight and open ended", "answer": ["1. Featherweight: This term is used to describe a weight of fabric that is lighter than standard fabric weights. The exact definition of featherweight can vary, but it is typically around 4 oz (113 g"], "scores": [[0.0762939453125, 0.96923828125, 0.97021484375, 0.6953125, 0.99951171875, 0.99462890625, 0.65869140625, 0.0267486572265625, 0.08245849609375, 0.416748046875, 0.560546875, 0.98828125, 0.97314453125, 0.86669921875, 0.002872467041015625, 0.93115234375, 0.94482421875, 0.9140625, 0.99755859375, 0.0230712890625, 1.0, 0.82666015625, 0.1903076171875, 0.028228759765625, 0.78466796875, 0.467529296875, 0.0217437744140625, 0.489990234375, 0.354736328125, 0.6943359375, 0.89599609375, 1.0, 1.0, 0.65478515625, 0.99951171875, 0.457763671875, 0.99951171875, 0.9697265625, 0.9150390625, 0.1610107421875, 0.51220703125, 0.998046875, 0.250244140625, 0.045196533203125, 0.99951171875, 0.6982421875, 0.994140625, 0.98291015625, 0.916015625, 0.99462890625]], "normalized_score": [0.42938971519470215], "tokens": [["\u2581", "1", ".", "\u2581Fe", "ather", "weight", ":", "\u2581This", "\u2581term", "\u2581is", "\u2581used", "\u2581to", "\u2581describe", "\u2581a", "\u2581weight", "\u2581of", "\u2581fabric", "\u2581that", "\u2581is", "\u2581l", "ighter", "\u2581than", "\u2581standard", "\u2581fabric", "\u2581weights", ".", "\u2581The", "\u2581exact", "\u2581definition", "\u2581of", "\u2581fe", "ather", "weight", "\u2581can", "\u2581vary", ",", "\u2581but", "\u2581it", "\u2581is", "\u2581typically", "\u2581around", "\u2581", "4", "\u2581o", "z", "\u2581(", "1", "1", "3", "\u2581g"]], "entropy": [[3.150390625, 0.2607421875, 0.2193603515625, 0.8427734375, 0.0030651092529296875, 0.042083740234375, 1.19921875, 1.80859375, 1.1025390625, 1.0732421875, 1.35546875, 0.07464599609375, 0.1502685546875, 0.6015625, 1.1572265625, 0.3291015625, 0.3642578125, 0.4365234375, 0.0211944580078125, 1.341796875, 0.0005345344543457031, 0.6181640625, 2.44140625, 1.1337890625, 0.806640625, 0.880859375, 1.154296875, 1.396484375, 1.30078125, 0.85107421875, 0.416748046875, 2.5570392608642578e-05, 0.0011758804321289062, 0.9580078125, 0.0030269622802734375, 0.85693359375, 0.004550933837890625, 0.1612548828125, 0.371337890625, 0.69140625, 1.427734375, 0.0189971923828125, 1.87890625, 1.62890625, 0.004314422607421875, 0.96533203125, 0.0540771484375, 0.1048583984375, 0.379150390625, 0.03643798828125]], "avg_entropy": [0.721625372171402], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Zips", "score_of_biggest_cluster": 0.42938971519470215, "score_of_first_answer": 0.42938971519470215, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.002872467041015625, "avg_score": 0.42938971519470215, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36021129808593366, "std_of_first_answer": 0.36021129808593366, "avg_entropy_of_first_answer": 0.721625372171402, "lex_sim": 1.0}
{"question": "On today's date in 1908, the first of over 15 million of what automobile was first produced?", "answer": ["Model T Ford **COVER REVEAL** Tempting B Fortune by KD Grace"], "scores": [[0.289306640625, 0.970703125, 0.8564453125, 2.086162567138672e-06, 0.0008707046508789062, 0.00027751922607421875, 0.00015544891357421875, 0.75439453125, 0.50537109375, 0.83447265625, 0.0005483627319335938, 0.00036144256591796875, 0.50390625, 0.01287841796875, 0.00015604496002197266, 0.11602783203125, 0.2166748046875, 0.2135009765625, 0.01061248779296875, 0.2117919921875]], "normalized_score": [0.01811743713915348], "tokens": [["\u2581Model", "\u2581T", "\u2581Ford", "\u2581**", "CO", "VER", "\u2581RE", "VE", "AL", "**", "\u2581T", "empt", "ing", "\u2581B", "\u2581Fort", "une", "\u2581by", "\u2581K", "D", "\u2581Grace"]], "entropy": [[2.353515625, 0.1812744140625, 0.59130859375, 0.83447265625, 4.7578125, 0.87109375, 1.1357421875, 0.7353515625, 0.73193359375, 0.9794921875, 2.15625, 4.2421875, 1.46484375, 4.34765625, 1.728515625, 2.48828125, 3.779296875, 3.484375, 3.171875, 4.46875]], "avg_entropy": [2.225201416015625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Model-T", "score_of_biggest_cluster": 0.01811743713915348, "score_of_first_answer": 0.01811743713915348, "max_score_of_first_answer": 0.970703125, "min_score_of_first_answer": 2.086162567138672e-06, "avg_score": 0.01811743713915348, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.32967525714376184, "std_of_first_answer": 0.32967525714376184, "avg_entropy_of_first_answer": 2.225201416015625, "lex_sim": 1.0}
{"question": "In 1947, Chuck Yeager was the first to do what in an aeroplane?", "answer": ["Break the sound barrier"], "scores": [[0.53076171875, 0.93359375, 0.98828125, 0.9912109375, 1.0]], "normalized_score": [0.8654078245162964], "tokens": [["\u2581Break", "\u2581the", "\u2581sound", "\u2581bar", "rier"]], "entropy": [[2.048828125, 0.338623046875, 0.0784912109375, 0.0634765625, 0.0002536773681640625]], "avg_entropy": [0.5059345245361329], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Break the sound barrier", "score_of_biggest_cluster": 0.8654078245162964, "score_of_first_answer": 0.8654078245162964, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.53076171875, "avg_score": 0.8654078245162964, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.18052581281300709, "std_of_first_answer": 0.18052581281300709, "avg_entropy_of_first_answer": 0.5059345245361329, "lex_sim": 1.0}
{"question": "\"In New Zealand, the word \"\"moko\"\" refers to a type of what?\"", "answer": ["Facial tattoo"], "scores": [[0.006237030029296875, 1.0, 0.845703125, 0.994140625, 0.73046875]], "normalized_score": [0.32858601212501526], "tokens": [["\u2581Fa", "cial", "\u2581t", "atto", "o"]], "entropy": [[2.173828125, 0.0017833709716796875, 0.497314453125, 0.04644775390625, 0.5908203125]], "avg_entropy": [0.6620388031005859], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tattoo", "score_of_biggest_cluster": 0.32858601212501526, "score_of_first_answer": 0.32858601212501526, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.006237030029296875, "avg_score": 0.32858601212501526, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36845937225961517, "std_of_first_answer": 0.36845937225961517, "avg_entropy_of_first_answer": 0.6620388031005859, "lex_sim": 1.0}
{"question": "According to Greek myth, who built the labyrinth on Crete for King Minos?", "answer": ["Dedalus"], "scores": [[0.2159423828125, 0.71826171875, 0.8779296875]], "normalized_score": [0.5144700407981873], "tokens": [["\u2581D", "ed", "alus"]], "entropy": [[0.8681640625, 1.083984375, 0.53515625]], "avg_entropy": [0.8291015625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Daedalus", "score_of_biggest_cluster": 0.5144700407981873, "score_of_first_answer": 0.5144700407981873, "max_score_of_first_answer": 0.8779296875, "min_score_of_first_answer": 0.2159423828125, "avg_score": 0.5144700407981873, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2820649584500514, "std_of_first_answer": 0.2820649584500514, "avg_entropy_of_first_answer": 0.8291015625, "lex_sim": 1.0}
{"question": "In the George Orwell novel \u2018Animal Farm\u2019 who is the owner of Foxwood Farm?", "answer": ["Mr. Frederick"], "scores": [[0.79345703125, 0.650390625, 0.84814453125]], "normalized_score": [0.7592576742172241], "tokens": [["\u2581Mr", ".", "\u2581Frederick"]], "entropy": [[1.3046875, 0.82421875, 0.5380859375]], "avg_entropy": [0.8889973958333334], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mr Pilkington", "score_of_biggest_cluster": 0.7592576742172241, "score_of_first_answer": 0.7592576742172241, "max_score_of_first_answer": 0.84814453125, "min_score_of_first_answer": 0.650390625, "avg_score": 0.7592576742172241, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08337687308618874, "std_of_first_answer": 0.08337687308618874, "avg_entropy_of_first_answer": 0.8889973958333334, "lex_sim": 1.0}
{"question": "According to famous American architect Louis Henri Sullivan, known as the father of skyscrapers, \"form follows what\"?", "answer": ["Function"], "scores": [[0.2509765625]], "normalized_score": [0.2509765625], "tokens": [["\u2581Function"]], "entropy": [[0.75048828125]], "avg_entropy": [0.75048828125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Function", "score_of_biggest_cluster": 0.2509765625, "score_of_first_answer": 0.2509765625, "max_score_of_first_answer": 0.2509765625, "min_score_of_first_answer": 0.2509765625, "avg_score": 0.2509765625, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.75048828125, "lex_sim": 1.0}
{"question": "On what date does official grouse shooting start", "answer": ["12 August"], "scores": [[0.0316162109375, 0.99462890625, 0.99462890625, 0.63232421875]], "normalized_score": [0.37501007318496704], "tokens": [["\u2581", "1", "2", "\u2581August"]], "entropy": [[1.2490234375, 0.04779052734375, 0.042816162109375, 1.095703125]], "avg_entropy": [0.6088333129882812], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Aug 12", "score_of_biggest_cluster": 0.37501007318496704, "score_of_first_answer": 0.37501007318496704, "max_score_of_first_answer": 0.99462890625, "min_score_of_first_answer": 0.0316162109375, "avg_score": 0.37501007318496704, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.39355482357013327, "std_of_first_answer": 0.39355482357013327, "avg_entropy_of_first_answer": 0.6088333129882812, "lex_sim": 1.0}
{"question": "How are the towns of Hanley, Burslem, Stoke, Longton,Tunstall and Fenton collectively known?", "answer": ["The Six Towns"], "scores": [[0.486328125, 0.09259033203125, 0.9951171875, 0.99853515625]], "normalized_score": [0.4599209129810333], "tokens": [["\u2581The", "\u2581Six", "\u2581Town", "s"]], "entropy": [[1.2353515625, 0.74951171875, 0.03558349609375, 0.013916015625]], "avg_entropy": [0.5085906982421875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Staffordshire potteries", "score_of_biggest_cluster": 0.4599209129810333, "score_of_first_answer": 0.4599209129810333, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.09259033203125, "avg_score": 0.4599209129810333, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3800949087460671, "std_of_first_answer": 0.3800949087460671, "avg_entropy_of_first_answer": 0.5085906982421875, "lex_sim": 1.0}
{"question": "The majority of the land area of Africa lies in which hemisphere?", "answer": ["Northern Hemisphere"], "scores": [[0.08599853515625, 0.146484375, 0.9853515625, 0.998046875]], "normalized_score": [0.33362314105033875], "tokens": [["\u2581Northern", "\u2581Hem", "is", "phere"]], "entropy": [[2.94921875, 1.3134765625, 0.1143798828125, 0.013519287109375]], "avg_entropy": [1.0976486206054688], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Northern (disambiguation)", "score_of_biggest_cluster": 0.33362314105033875, "score_of_first_answer": 0.33362314105033875, "max_score_of_first_answer": 0.998046875, "min_score_of_first_answer": 0.08599853515625, "avg_score": 0.33362314105033875, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4382739297128848, "std_of_first_answer": 0.4382739297128848, "avg_entropy_of_first_answer": 1.0976486206054688, "lex_sim": 1.0}
{"question": "What epitaph is on Frank Sinatra's tombstone?", "answer": ["The best is yet to come"], "scores": [[0.0204315185546875, 0.27734375, 0.99951171875, 0.99951171875, 1.0, 0.99951171875]], "normalized_score": [0.42213091254234314], "tokens": [["\u2581The", "\u2581best", "\u2581is", "\u2581yet", "\u2581to", "\u2581come"]], "entropy": [[0.5576171875, 3.13671875, 0.00710296630859375, 0.0029144287109375, 0.0019683837890625, 0.00391387939453125]], "avg_entropy": [0.6183725992838541], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Best Is Yet To Come", "score_of_biggest_cluster": 0.42213091254234314, "score_of_first_answer": 0.42213091254234314, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0204315185546875, "avg_score": 0.42213091254234314, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.40784545397927074, "std_of_first_answer": 0.40784545397927074, "avg_entropy_of_first_answer": 0.6183725992838541, "lex_sim": 1.0}
{"question": "What US corporation sold Hummer to the Chinese in 2009?", "answer": ["General Motors Corporation"], "scores": [[0.93359375, 0.9775390625, 1.0, 0.0179290771484375]], "normalized_score": [0.35765349864959717], "tokens": [["\u2581General", "\u2581Mot", "ors", "\u2581Corporation"]], "entropy": [[0.49169921875, 0.169677734375, 0.0009417533874511719, 1.0849609375]], "avg_entropy": [0.4368199110031128], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "General motors", "score_of_biggest_cluster": 0.35765349864959717, "score_of_first_answer": 0.35765349864959717, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0179290771484375, "avg_score": 0.35765349864959717, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4131133234997744, "std_of_first_answer": 0.4131133234997744, "avg_entropy_of_first_answer": 0.4368199110031128, "lex_sim": 1.0}
{"question": "Which Scottish newspaper features the Broons and Oor Wullie?", "answer": ["Broons and Oor Wullie #PrivacyMatters: How to Manage Your Personal Information Online"], "scores": [[0.00038933753967285156, 0.95849609375, 0.51318359375, 0.9794921875, 1.0, 0.99267578125, 0.99853515625, 0.99853515625, 2.384185791015625e-07, 0.00025844573974609375, 0.0028400421142578125, 0.8671875, 0.0007429122924804688, 0.99951171875, 0.002765655517578125, 0.0088348388671875, 0.450927734375, 0.002101898193359375, 0.9658203125, 0.59765625, 0.0021877288818359375, 0.7255859375, 0.3857421875]], "normalized_score": [0.045954931527376175], "tokens": [["\u2581Bro", "ons", "\u2581and", "\u2581O", "or", "\u2581W", "ull", "ie", "\u2581#", "P", "riv", "acy", "Mat", "ters", ":", "\u2581How", "\u2581to", "\u2581Man", "age", "\u2581Your", "\u2581Personal", "\u2581Information", "\u2581Online"]], "entropy": [[0.619140625, 0.2197265625, 1.43359375, 0.1556396484375, 0.0019016265869140625, 0.05596923828125, 0.01238250732421875, 0.01323699951171875, 1.82421875, 4.609375, 4.9375, 0.68115234375, 2.32421875, 0.00740814208984375, 1.982421875, 5.68359375, 3.2734375, 2.2734375, 0.217041015625, 1.587890625, 1.2890625, 0.76416015625, 2.42578125]], "avg_entropy": [1.5822734832763672], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sunday Post", "score_of_biggest_cluster": 0.045954931527376175, "score_of_first_answer": 0.045954931527376175, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.384185791015625e-07, "avg_score": 0.045954931527376175, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43425166282087063, "std_of_first_answer": 0.43425166282087063, "avg_entropy_of_first_answer": 1.5822734832763672, "lex_sim": 1.0}
{"question": "For which 1993 film did Steven Spielberg win his first 'Oscar' for Best Director?", "answer": ["Schindler's List"], "scores": [[0.8671875, 0.9990234375, 0.99658203125, 0.96875, 1.0, 0.984375]], "normalized_score": [0.9681195616722107], "tokens": [["\u2581Sch", "ind", "ler", "'", "s", "\u2581List"]], "entropy": [[0.93359375, 0.0130462646484375, 0.0258636474609375, 0.209716796875, 0.0011730194091796875, 0.10821533203125]], "avg_entropy": [0.21526813507080078], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Schindler's List (film)", "score_of_biggest_cluster": 0.9681195616722107, "score_of_first_answer": 0.9681195616722107, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.8671875, "avg_score": 0.9681195616722107, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.04694819916275582, "std_of_first_answer": 0.04694819916275582, "avg_entropy_of_first_answer": 0.21526813507080078, "lex_sim": 1.0}
{"question": "With which French King were Mme de Pompadour and Mme Du Barry associated?", "answer": ["Louis XV"], "scores": [[0.67431640625, 0.9990234375]], "normalized_score": [0.8207666277885437], "tokens": [["\u2581Louis", "\u2581XV"]], "entropy": [[1.0244140625, 0.0081024169921875]], "avg_entropy": [0.5162582397460938], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Louis XV the Beloved", "score_of_biggest_cluster": 0.8207666277885437, "score_of_first_answer": 0.8207666277885437, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.67431640625, "avg_score": 0.8207666277885437, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.162353515625, "std_of_first_answer": 0.162353515625, "avg_entropy_of_first_answer": 0.5162582397460938, "lex_sim": 1.0}
{"question": "In 1968 what became the longest course used for the British Open golf championship", "answer": ["Royal Liverpool Golf Club (Hoylake)"], "scores": [[0.173095703125, 0.1112060546875, 0.9736328125, 1.0, 0.99609375, 0.70751953125, 0.98828125, 0.9912109375, 1.0, 1.0, 0.9833984375]], "normalized_score": [0.6725013852119446], "tokens": [["\u2581Royal", "\u2581Liverpool", "\u2581G", "olf", "\u2581Club", "\u2581(", "H", "oy", "la", "ke", ")"]], "entropy": [[2.478515625, 1.7578125, 0.155517578125, 0.0009765625, 0.031707763671875, 1.08203125, 0.08953857421875, 0.06304931640625, 5.364418029785156e-05, 0.0005784034729003906, 0.11492919921875]], "avg_entropy": [0.5249736742539839], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Royal Burgh of Carnoustie", "score_of_biggest_cluster": 0.6725013852119446, "score_of_first_answer": 0.6725013852119446, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.1112060546875, "avg_score": 0.6725013852119446, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3259724700158406, "std_of_first_answer": 0.3259724700158406, "avg_entropy_of_first_answer": 0.5249736742539839, "lex_sim": 1.0}
{"question": "Under which British monarch was Warren Hastings appointed first Governor of the Presidency of Fort William (Bengal) and the head of the Supreme Council of Bengal, effectively the first Governor-General of India?", "answer": ["George III"], "scores": [[0.5546875, 0.94775390625]], "normalized_score": [0.7250567078590393], "tokens": [["\u2581George", "\u2581III"]], "entropy": [[1.7109375, 0.361572265625]], "avg_entropy": [1.0362548828125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George III, King of Great Britain and Ireland", "score_of_biggest_cluster": 0.7250567078590393, "score_of_first_answer": 0.7250567078590393, "max_score_of_first_answer": 0.94775390625, "min_score_of_first_answer": 0.5546875, "avg_score": 0.7250567078590393, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.196533203125, "std_of_first_answer": 0.196533203125, "avg_entropy_of_first_answer": 1.0362548828125, "lex_sim": 1.0}
{"question": "What is the official color of the UPS trucks?", "answer": ["Brown \\\u02c8pr\u00e4p\u0259r\\, /\u02c8pre-p\u0259r/"], "scores": [[0.399658203125, 9.512901306152344e-05, 0.00010216236114501953, 3.9517879486083984e-05, 0.09271240234375, 0.0034027099609375, 0.16943359375, 0.9287109375, 0.0127105712890625, 0.038299560546875, 0.7099609375, 0.002422332763671875, 0.0016498565673828125, 0.2421875, 0.98486328125, 0.9970703125, 0.93115234375]], "normalized_score": [0.027125701308250427], "tokens": [["\u2581Brown", "\u2581\\", "\u02c8", "pr", "\u00e4", "p", "\u0259", "r", "\\,", "\u2581/", "\u02c8", "pre", "-", "p", "\u0259", "r", "/"]], "entropy": [[2.20703125, 1.0625, 3.271484375, 1.37890625, 1.3056640625, 2.537109375, 2.326171875, 0.36865234375, 2.7109375, 4.33984375, 1.6337890625, 2.365234375, 0.319580078125, 1.0283203125, 0.1300048828125, 0.0269622802734375, 0.416748046875]], "avg_entropy": [1.6134670481962317], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Orangeish blacks", "score_of_biggest_cluster": 0.027125701308250427, "score_of_first_answer": 0.027125701308250427, "max_score_of_first_answer": 0.9970703125, "min_score_of_first_answer": 3.9517879486083984e-05, "avg_score": 0.027125701308250427, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3960275998695726, "std_of_first_answer": 0.3960275998695726, "avg_entropy_of_first_answer": 1.6134670481962317, "lex_sim": 1.0}
{"question": "\"What group of people in the United States speak a dialect called \"\"Pennsylvania German\"\"?\"", "answer": ["Anabaptist German"], "scores": [[0.0014162063598632812, 0.94482421875, 0.998046875, 0.41455078125, 0.015594482421875]], "normalized_score": [0.09710361808538437], "tokens": [["\u2581An", "ab", "apt", "ist", "\u2581German"]], "entropy": [[1.298828125, 0.376953125, 0.01399993896484375, 0.6884765625, 1.5888671875]], "avg_entropy": [0.7934249877929688], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Amish", "score_of_biggest_cluster": 0.09710361808538437, "score_of_first_answer": 0.09710361808538437, "max_score_of_first_answer": 0.998046875, "min_score_of_first_answer": 0.0014162063598632812, "avg_score": 0.09710361808538437, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.43204210739274823, "std_of_first_answer": 0.43204210739274823, "avg_entropy_of_first_answer": 0.7934249877929688, "lex_sim": 1.0}
{"question": "Who won the young PFA player of the year 2011?", "answer": ["The Young PFA player of the year is selected by a panel of former players, managers and football writers, the winner of 2011 was not recorded in any databases as I am not able to confirm the information."], "scores": [[0.0936279296875, 0.472900390625, 0.85009765625, 1.0, 0.032928466796875, 0.9990234375, 1.0, 0.9921875, 0.0137786865234375, 0.0012979507446289062, 0.86181640625, 0.19091796875, 0.73828125, 0.986328125, 0.55859375, 0.92626953125, 0.28759765625, 0.04547119140625, 1.0, 0.6748046875, 0.2919921875, 0.409912109375, 0.3564453125, 0.05572509765625, 0.2381591796875, 0.76904296875, 0.048095703125, 1.0, 1.0, 0.99951171875, 0.9990234375, 0.55908203125, 0.0089263916015625, 0.050384521484375, 0.481201171875, 0.034210205078125, 0.0051422119140625, 0.02130126953125, 0.07696533203125, 0.560546875, 0.452880859375, 0.421630859375, 0.998046875, 0.0008182525634765625, 0.37841796875, 0.46875, 0.32958984375, 1.7881393432617188e-07]], "normalized_score": [0.16081690788269043], "tokens": [["\u2581The", "\u2581Young", "\u2581P", "FA", "\u2581player", "\u2581of", "\u2581the", "\u2581year", "\u2581is", "\u2581selected", "\u2581by", "\u2581a", "\u2581panel", "\u2581of", "\u2581former", "\u2581players", ",", "\u2581man", "agers", "\u2581and", "\u2581football", "\u2581writers", ",", "\u2581the", "\u2581winner", "\u2581of", "\u2581", "2", "0", "1", "1", "\u2581was", "\u2581not", "\u2581recorded", "\u2581in", "\u2581any", "\u2581databases", "\u2581as", "\u2581I", "\u2581am", "\u2581not", "\u2581able", "\u2581to", "\u2581confirm", "\u2581the", "\u2581information", ".", "\u2581#"]], "entropy": [[4.1015625, 1.8046875, 0.46484375, 0.002262115478515625, 0.28759765625, 0.0082855224609375, 0.0022125244140625, 0.046966552734375, 1.3017578125, 1.0390625, 0.74853515625, 1.615234375, 0.806640625, 0.1058349609375, 1.8896484375, 0.359375, 0.94189453125, 2.439453125, 0.0002415180206298828, 0.64453125, 1.5751953125, 1.216796875, 1.30859375, 2.14453125, 1.4072265625, 0.79541015625, 0.50732421875, 0.001056671142578125, 0.0022563934326171875, 0.006275177001953125, 0.008270263671875, 1.5419921875, 2.486328125, 2.53125, 1.81640625, 1.0693359375, 3.05859375, 2.55078125, 2.5078125, 1.67578125, 1.92578125, 1.7685546875, 0.0179290771484375, 1.8076171875, 2.587890625, 2.337890625, 2.548828125, 1.1328125]], "avg_entropy": [1.2697739452123642], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "JACK WILSHERE", "score_of_biggest_cluster": 0.16081690788269043, "score_of_first_answer": 0.16081690788269043, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.7881393432617188e-07, "avg_score": 0.16081690788269043, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3797306711639696, "std_of_first_answer": 0.3797306711639696, "avg_entropy_of_first_answer": 1.2697739452123642, "lex_sim": 1.0}
{"question": "Where does Rupert Bear live?", "answer": ["The Rupert Bear series takes place in the fictional town of \"Frewyn Forest.\""], "scores": [[0.1663818359375, 0.1666259765625, 0.99755859375, 0.90234375, 0.050628662109375, 0.17626953125, 0.99853515625, 0.97900390625, 0.921875, 0.87646484375, 0.99853515625, 0.748046875, 0.998046875, 0.86083984375, 0.9140625, 0.0693359375, 0.1817626953125, 0.0927734375, 0.07159423828125, 1.7285346984863281e-06, 0.00394439697265625, 0.05657958984375, 1.2993812561035156e-05, 0.01152801513671875, 0.00791168212890625, 0.006092071533203125, 0.443359375]], "normalized_score": [0.0845206081867218], "tokens": [["\u2581The", "\u2581Ru", "pert", "\u2581Bear", "\u2581series", "\u2581takes", "\u2581place", "\u2581in", "\u2581the", "\u2581fict", "ional", "\u2581town", "\u2581of", "\u2581\"", "F", "rew", "yn", "\u2581Forest", ".\"", "\u2581#", "3", "4", "\u2581J", "esse", "\u2581K", "not", "ts"]], "entropy": [[3.251953125, 4.60546875, 0.0240936279296875, 0.6923828125, 2.744140625, 1.755859375, 0.01349639892578125, 0.1217041015625, 0.369384765625, 0.56103515625, 0.011932373046875, 0.89697265625, 0.019683837890625, 0.787109375, 0.43212890625, 3.51953125, 2.90234375, 2.673828125, 1.5009765625, 1.58984375, 4.1015625, 3.21875, 2.94921875, 5.3046875, 5.03515625, 3.20703125, 1.1494140625]], "avg_entropy": [1.979247764304832], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nutwood", "score_of_biggest_cluster": 0.0845206081867218, "score_of_first_answer": 0.0845206081867218, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 1.7285346984863281e-06, "avg_score": 0.0845206081867218, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42088042554205535, "std_of_first_answer": 0.42088042554205535, "avg_entropy_of_first_answer": 1.979247764304832, "lex_sim": 1.0}
{"question": "Which character created by Keith Waterhouse was played on film by Tom Courtneyand on TV by Jeff Rawle?", "answer": ["Mr. J.L.B."], "scores": [[0.00443267822265625, 0.798828125, 0.1978759765625, 0.85302734375, 0.92919921875, 0.99853515625, 0.9912109375, 0.96435546875, 0.99755859375, 0.9951171875, 0.970703125]], "normalized_score": [0.5020535588264465], "tokens": [["\u2581Mr", ".", "\u2581J", ".", "L", ".", "B", ".", "\u2581Mate", "k", "oni"]], "entropy": [[1.48046875, 1.40625, 4.1953125, 0.6376953125, 0.2978515625, 0.01235198974609375, 0.06817626953125, 0.20166015625, 0.0232696533203125, 0.040435791015625, 0.251220703125]], "avg_entropy": [0.7831538807262074], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "'BILLY LIAR'", "score_of_biggest_cluster": 0.5020535588264465, "score_of_first_answer": 0.5020535588264465, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.00443267822265625, "avg_score": 0.5020535588264465, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3334833814539105, "std_of_first_answer": 0.3334833814539105, "avg_entropy_of_first_answer": 0.7831538807262074, "lex_sim": 1.0}
{"question": "Which river runs through The Grand Canyon", "answer": ["Colorado"], "scores": [[0.476318359375]], "normalized_score": [0.476318359375], "tokens": [["\u2581Colorado"]], "entropy": [[1.1826171875]], "avg_entropy": [1.1826171875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Colorado (State)", "score_of_biggest_cluster": 0.476318359375, "score_of_first_answer": 0.476318359375, "max_score_of_first_answer": 0.476318359375, "min_score_of_first_answer": 0.476318359375, "avg_score": 0.476318359375, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.1826171875, "lex_sim": 1.0}
{"question": "Which English football ground features the 'Holte End'?", "answer": ["Villa Park Audience Award Winner: Best Feature Film, Miami Film Festival"], "scores": [[0.144775390625, 0.9990234375, 0.0002269744873046875, 3.159046173095703e-06, 0.0545654296875, 9.5367431640625e-07, 0.00225067138671875, 0.96142578125, 0.153564453125, 0.0008492469787597656, 0.156005859375, 0.99853515625, 0.09381103515625, 0.15087890625, 0.0007700920104980469, 0.83056640625, 0.99365234375]], "normalized_score": [0.0179746113717556], "tokens": [["\u2581Villa", "\u2581Park", "\u2581A", "udi", "ence", "\u2581Award", "\u2581W", "inner", ":", "\u2581Best", "\u2581Fe", "ature", "\u2581Film", ",", "\u2581Miami", "\u2581Film", "\u2581Festival"]], "entropy": [[0.59326171875, 0.01015472412109375, 1.0009765625, 0.037017822265625, 2.314453125, 0.79736328125, 2.7421875, 0.324462890625, 2.71875, 1.9091796875, 3.9375, 0.0132598876953125, 2.609375, 2.9609375, 4.0, 0.86083984375, 0.05718994140625]], "avg_entropy": [1.5815829108743107], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Villa Park", "score_of_biggest_cluster": 0.0179746113717556, "score_of_first_answer": 0.0179746113717556, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 9.5367431640625e-07, "avg_score": 0.0179746113717556, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41260261372298407, "std_of_first_answer": 0.41260261372298407, "avg_entropy_of_first_answer": 1.5815829108743107, "lex_sim": 1.0}
{"question": "The Bells of Shandon are housed in the Church of St Anne in which Irish city?", "answer": ["Cork \u00cdrta: Kov\u00e1csn\u00e9 Heged\u0171s Zsuzsa, szerkesztette: Papp-S\u00e1ndor Istv\u00e1n"], "scores": [[0.875, 0.99951171875, 0.0, 0.046478271484375, 0.00012683868408203125, 0.0731201171875, 0.00034499168395996094, 0.00328826904296875, 0.98583984375, 0.04046630859375, 0.0190277099609375, 0.08740234375, 0.771484375, 0.97265625, 0.9970703125, 0.029541015625, 0.35107421875, 0.63671875, 0.99169921875, 0.035919189453125, 0.2181396484375, 0.9794921875, 0.88720703125, 0.3486328125, 0.97314453125, 0.022918701171875, 0.451904296875, 0.055938720703125, 0.05548095703125, 0.0640869140625, 0.98974609375, 0.08111572265625, 0.99951171875]], "normalized_score": [0.0], "tokens": [["\u2581C", "ork", "\u2581\u00cd", "r", "ta", ":", "\u2581K", "ov\u00e1", "cs", "n\u00e9", "\u2581H", "eg", "ed", "\u0171", "s", "\u2581Z", "su", "z", "sa", ",", "\u2581sz", "erk", "eszt", "ette", ":", "\u2581P", "app", "-", "S", "\u00e1nd", "or", "\u2581Ist", "v\u00e1n"]], "entropy": [[0.81982421875, 0.005031585693359375, 0.9384765625, 0.86962890625, 2.38671875, 1.8525390625, 1.9072265625, 5.453125, 0.10479736328125, 3.80859375, 3.767578125, 2.896484375, 0.59423828125, 0.167236328125, 0.028350830078125, 3.64453125, 1.2646484375, 0.66064453125, 0.050811767578125, 3.072265625, 4.265625, 0.16796875, 0.485107421875, 1.005859375, 0.239013671875, 3.587890625, 2.173828125, 3.55078125, 2.29296875, 1.4912109375, 0.07220458984375, 2.8984375, 0.004428863525390625]], "avg_entropy": [1.7129720052083333], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cork (disambiguation)", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4224551047824608, "std_of_first_answer": 0.4224551047824608, "avg_entropy_of_first_answer": 1.7129720052083333, "lex_sim": 1.0}
{"question": "What type of creature is a Lion's Mane, found in the Arctic'?", "answer": ["Jellyfish ### Introduction"], "scores": [[0.7294921875, 0.99951171875, 0.98876953125, 0.0, 4.673004150390625e-05]], "normalized_score": [0.0], "tokens": [["\u2581J", "elly", "fish", "\u2581###", "\u2581Introduction"]], "entropy": [[1.3212890625, 0.0027599334716796875, 0.08740234375, 0.771484375, 1.892578125]], "avg_entropy": [0.8151027679443359], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jelly-fish", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.45421105733680467, "std_of_first_answer": 0.45421105733680467, "avg_entropy_of_first_answer": 0.8151027679443359, "lex_sim": 1.0}
{"question": "Who was the architect of Marble Arch?", "answer": ["George dance the younger Online Exclusives News Mining"], "scores": [[0.032135009765625, 0.0223236083984375, 0.6572265625, 0.87548828125, 0.0, 0.0001914501190185547, 0.0025615692138671875, 0.0032711029052734375, 0.0007848739624023438, 0.0004839897155761719, 0.0019521713256835938]], "normalized_score": [0.0], "tokens": [["\u2581George", "\u2581dance", "\u2581the", "\u2581younger", "\u2581Online", "\u2581Ex", "clus", "ives", "\u2581News", "\u2581M", "ining"]], "entropy": [[0.2646484375, 3.646484375, 1.14453125, 0.57275390625, 0.728515625, 2.09375, 0.49853515625, 0.0281829833984375, 2.439453125, 3.26953125, 4.8046875]], "avg_entropy": [1.771915782581676], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John nash", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.87548828125, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29669281091705796, "std_of_first_answer": 0.29669281091705796, "avg_entropy_of_first_answer": 1.771915782581676, "lex_sim": 1.0}
{"question": "In cooking, 190 degrees Celsius is the equivalent to which Gas Mark?", "answer": ["6 Gas Mark"], "scores": [[0.302001953125, 0.326416015625, 0.354248046875, 0.87744140625]], "normalized_score": [0.4183855652809143], "tokens": [["\u2581", "6", "\u2581Gas", "\u2581Mark"]], "entropy": [[1.4599609375, 1.9375, 1.5078125, 0.66845703125]], "avg_entropy": [1.3934326171875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "five", "score_of_biggest_cluster": 0.4183855652809143, "score_of_first_answer": 0.4183855652809143, "max_score_of_first_answer": 0.87744140625, "min_score_of_first_answer": 0.302001953125, "avg_score": 0.4183855652809143, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23882409312919514, "std_of_first_answer": 0.23882409312919514, "avg_entropy_of_first_answer": 1.3934326171875, "lex_sim": 1.0}
{"question": "Sir Basil Spence is associated with what activity?", "answer": ["Architectural design of religious buildings"], "scores": [[0.25830078125, 0.9951171875, 0.473388671875, 0.89794921875, 0.073974609375, 0.0030918121337890625, 0.84326171875]], "normalized_score": [0.2147638201713562], "tokens": [["\u2581Arch", "itect", "ural", "\u2581design", "\u2581of", "\u2581religious", "\u2581buildings"]], "entropy": [[1.8115234375, 0.046356201171875, 1.474609375, 0.583984375, 1.14453125, 2.5703125, 0.61279296875]], "avg_entropy": [1.1777300153459822], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Architecturally", "score_of_biggest_cluster": 0.2147638201713562, "score_of_first_answer": 0.2147638201713562, "max_score_of_first_answer": 0.9951171875, "min_score_of_first_answer": 0.0030918121337890625, "avg_score": 0.2147638201713562, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37958340070922925, "std_of_first_answer": 0.37958340070922925, "avg_entropy_of_first_answer": 1.1777300153459822, "lex_sim": 1.0}
{"question": "\"Who wrote the opera \"\"Dido and Aeneas\"\"?\"", "answer": ["Henry Purcell"], "scores": [[0.9970703125, 0.99609375, 0.99951171875]], "normalized_score": [0.9975575804710388], "tokens": [["\u2581Henry", "\u2581Pur", "cell"]], "entropy": [[0.03057861328125, 0.03753662109375, 0.0037517547607421875]], "avg_entropy": [0.023955663045247395], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "PURCELL", "score_of_biggest_cluster": 0.9975575804710388, "score_of_first_answer": 0.9975575804710388, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.99609375, "avg_score": 0.9975575804710388, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0014374610785038813, "std_of_first_answer": 0.0014374610785038813, "avg_entropy_of_first_answer": 0.023955663045247395, "lex_sim": 1.0}
{"question": "\"Who was King of England when Daniel Defoe published \"\"Robinson Crusoe\"\" and Jonathan Swift published \"\"Gulliver's Travels\"\"?\"", "answer": ["King William III #OTD in 1934 \u2013 Birth of singer and actress, Val Doonican, in Carlow."], "scores": [[0.86669921875, 0.85400390625, 0.88330078125, 9.5367431640625e-07, 3.415346145629883e-05, 0.247314453125, 0.035125732421875, 0.986328125, 0.99755859375, 0.0206756591796875, 0.04888916015625, 0.07611083984375, 0.0012884140014648438, 0.001789093017578125, 0.401611328125, 0.5185546875, 0.001758575439453125, 0.113037109375, 0.1224365234375, 0.0848388671875, 1.2695789337158203e-05, 0.04461669921875, 1.0, 1.0, 0.23046875, 0.390380859375, 0.00164794921875, 0.65771484375, 0.30615234375]], "normalized_score": [0.04299996793270111], "tokens": [["\u2581King", "\u2581William", "\u2581III", "\u2581#", "OT", "D", "\u2581in", "\u2581", "1", "9", "3", "4", "\u2581\u2013", "\u2581B", "irth", "\u2581of", "\u2581singer", "\u2581and", "\u2581actress", ",", "\u2581Val", "\u2581Do", "on", "ican", ",", "\u2581in", "\u2581Car", "low", "."]], "entropy": [[0.86572265625, 0.76513671875, 0.63134765625, 1.2626953125, 2.845703125, 1.1181640625, 1.6181640625, 0.1300048828125, 0.0222625732421875, 0.3740234375, 2.140625, 2.22265625, 1.369140625, 3.400390625, 2.96875, 0.98681640625, 5.50390625, 3.25, 2.5625, 3.083984375, 2.72265625, 1.765625, 0.0016717910766601562, 0.0004200935363769531, 0.990234375, 1.443359375, 2.763671875, 1.2041015625, 0.87060546875]], "avg_entropy": [1.6856668899799216], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George the First", "score_of_biggest_cluster": 0.04299996793270111, "score_of_first_answer": 0.04299996793270111, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 9.5367431640625e-07, "avg_score": 0.04299996793270111, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3771142716352443, "std_of_first_answer": 0.3771142716352443, "avg_entropy_of_first_answer": 1.6856668899799216, "lex_sim": 1.0}
{"question": "In which month of 1929 did the St Valentines Day massacre take place?", "answer": ["February 14"], "scores": [[0.85888671875, 0.10174560546875, 0.9990234375, 0.90771484375]], "normalized_score": [0.5305718183517456], "tokens": [["\u2581February", "\u2581", "1", "4"]], "entropy": [[0.8916015625, 1.12109375, 0.01018524169921875, 0.314697265625]], "avg_entropy": [0.5843944549560547], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u32c1", "score_of_biggest_cluster": 0.5305718183517456, "score_of_first_answer": 0.5305718183517456, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.10174560546875, "avg_score": 0.5305718183517456, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3586708389454009, "std_of_first_answer": 0.3586708389454009, "avg_entropy_of_first_answer": 0.5843944549560547, "lex_sim": 1.0}
{"question": "Which golf shot is the opposite of a slice?", "answer": ["Hook Chapter 10A: Financial and legal"], "scores": [[0.75390625, 0.0, 0.1898193359375, 0.334228515625, 0.06732177734375, 0.0010776519775390625, 0.11865234375, 0.00015628337860107422, 0.10296630859375, 0.99853515625, 0.0138702392578125, 0.00893402099609375]], "normalized_score": [0.0], "tokens": [["\u2581Hook", "\u2581Chapter", "\u2581", "1", "0", "A", ":", "\u2581Fin", "an", "cial", "\u2581and", "\u2581legal"]], "entropy": [[0.98046875, 1.0107421875, 2.708984375, 2.126953125, 3.44921875, 3.33984375, 4.125, 5.640625, 1.3056640625, 0.0147705078125, 4.171875, 4.453125]], "avg_entropy": [2.7772725423177085], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hook", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31409851961845636, "std_of_first_answer": 0.31409851961845636, "avg_entropy_of_first_answer": 2.7772725423177085, "lex_sim": 1.0}
{"question": "Who bought Donington Park\u2019s Dunlop Bridge at a memorabilia auction?", "answer": ["Richard Branson, Founder of Virgin Group"], "scores": [[0.0894775390625, 0.8818359375, 1.0, 0.07403564453125, 0.053741455078125, 0.9990234375, 0.99267578125, 0.470458984375, 0.9677734375]], "normalized_score": [0.3735874891281128], "tokens": [["\u2581Richard", "\u2581Br", "anson", ",", "\u2581Found", "er", "\u2581of", "\u2581Virgin", "\u2581Group"]], "entropy": [[3.869140625, 0.76806640625, 0.0005512237548828125, 0.97509765625, 2.65234375, 0.007549285888671875, 0.051239013671875, 0.72509765625, 0.2012939453125]], "avg_entropy": [1.0278199513753254], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Evans, Chris", "score_of_biggest_cluster": 0.3735874891281128, "score_of_first_answer": 0.3735874891281128, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.053741455078125, "avg_score": 0.3735874891281128, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4134817061504778, "std_of_first_answer": 0.4134817061504778, "avg_entropy_of_first_answer": 1.0278199513753254, "lex_sim": 1.0}
{"question": "On July 21, 1969, who became the first man to set foot on the moon?", "answer": ["Apollo 11 astronauts Neil A. Armstrong and Edwin E. Aldrin Jr."], "scores": [[0.01363372802734375, 0.9970703125, 0.8955078125, 0.99951171875, 0.9990234375, 0.406005859375, 0.99951171875, 0.32568359375, 0.71484375, 0.10357666015625, 0.99853515625, 0.99951171875, 1.0, 0.96142578125, 0.97021484375, 1.0, 0.9296875, 0.99951171875, 0.80615234375, 1.0, 0.6640625, 0.9169921875]], "normalized_score": [0.6378236413002014], "tokens": [["\u2581Apol", "lo", "\u2581", "1", "1", "\u2581astr", "onaut", "s", "\u2581Neil", "\u2581A", ".", "\u2581Arm", "strong", "\u2581and", "\u2581Ed", "win", "\u2581E", ".", "\u2581Ald", "rin", "\u2581Jr", "."]], "entropy": [[0.87255859375, 0.03363037109375, 0.55126953125, 0.004207611083984375, 0.00823974609375, 2.12109375, 0.0025539398193359375, 1.30859375, 1.2109375, 0.5244140625, 0.01165771484375, 0.00643157958984375, 0.002197265625, 0.2159423828125, 0.1397705078125, 0.0002903938293457031, 0.28271484375, 0.004543304443359375, 0.58642578125, 0.0001424551010131836, 0.763671875, 0.384521484375]], "avg_entropy": [0.4107185656374151], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "That's one small step for a man one giant leap for mankind", "score_of_biggest_cluster": 0.6378236413002014, "score_of_first_answer": 0.6378236413002014, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.01363372802734375, "avg_score": 0.6378236413002014, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30155932541246083, "std_of_first_answer": 0.30155932541246083, "avg_entropy_of_first_answer": 0.4107185656374151, "lex_sim": 1.0}
{"question": "If the states of the US are listed alphabetically which is the last that does NOT contain the letter O?", "answer": ["HI"], "scores": [[0.0024318695068359375, 0.342529296875]], "normalized_score": [0.028861504048109055], "tokens": [["\u2581H", "I"]], "entropy": [[4.30859375, 3.974609375]], "avg_entropy": [4.1416015625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Demographics of West Virginia", "score_of_biggest_cluster": 0.028861504048109055, "score_of_first_answer": 0.028861504048109055, "max_score_of_first_answer": 0.342529296875, "min_score_of_first_answer": 0.0024318695068359375, "avg_score": 0.028861504048109055, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17004871368408203, "std_of_first_answer": 0.17004871368408203, "avg_entropy_of_first_answer": 4.1416015625, "lex_sim": 1.0}
{"question": "What was the name of the Roman road that stretched from Dover, Kent, almost to Shrewsbury, Shropshire, a distance of 322 km (200 miles)?", "answer": ["Stane Street"], "scores": [[0.356689453125, 0.994140625, 0.9228515625]], "normalized_score": [0.6891122460365295], "tokens": [["\u2581St", "ane", "\u2581Street"]], "entropy": [[1.568359375, 0.05865478515625, 0.337158203125]], "avg_entropy": [0.65472412109375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stone Street (Kent)", "score_of_biggest_cluster": 0.6891122460365295, "score_of_first_answer": 0.6891122460365295, "max_score_of_first_answer": 0.994140625, "min_score_of_first_answer": 0.356689453125, "avg_score": 0.6891122460365295, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28518330577774187, "std_of_first_answer": 0.28518330577774187, "avg_entropy_of_first_answer": 0.65472412109375, "lex_sim": 1.0}
{"question": "What fictional spy show showcases the dysfunctional International Secret Intelligence Service (ISIS)?", "answer": ["The Man from U.N.C.L.E"], "scores": [[0.0169830322265625, 0.005584716796875, 0.7587890625, 0.87841796875, 1.0, 0.99951171875, 0.99951171875, 0.99951171875, 1.0, 0.99951171875, 0.99951171875, 1.0]], "normalized_score": [0.44666460156440735], "tokens": [["\u2581The", "\u2581Man", "\u2581from", "\u2581U", ".", "N", ".", "C", ".", "L", ".", "E"]], "entropy": [[1.0654296875, 5.50390625, 0.81396484375, 0.482421875, 0.0011262893676757812, 0.002788543701171875, 0.0030879974365234375, 0.0031299591064453125, 0.001667022705078125, 0.003887176513671875, 0.0029010772705078125, 0.0004584789276123047]], "avg_entropy": [0.6570641001065572], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Toxophilite", "score_of_biggest_cluster": 0.44666460156440735, "score_of_first_answer": 0.44666460156440735, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.005584716796875, "avg_score": 0.44666460156440735, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3618021600818219, "std_of_first_answer": 0.3618021600818219, "avg_entropy_of_first_answer": 0.6570641001065572, "lex_sim": 1.0}
{"question": "Flemington, Greyville, and Longchamp are associated with what, respectively in Australia, South Africa, and France?", "answer": ["Horse race tracks The Gift of Gratitude in Midst of Busy Lives"], "scores": [[0.60986328125, 0.9560546875, 0.0058746337890625, 0.330810546875, 2.384185791015625e-06, 0.0263671875, 2.9385089874267578e-05, 0.060699462890625, 0.002567291259765625, 0.8388671875, 0.99267578125, 0.0101470947265625, 3.701448440551758e-05, 0.07452392578125, 0.97314453125, 0.0007753372192382812, 0.556640625, 0.071533203125, 0.62060546875]], "normalized_score": [0.023790137842297554], "tokens": [["\u2581Hor", "se", "\u2581race", "\u2581tracks", "\u2581The", "\u2581G", "ift", "\u2581of", "\u2581Gr", "at", "itude", "\u2581in", "\u2581Mid", "st", "\u2581of", "\u2581Bus", "y", "\u2581L", "ives"]], "entropy": [[1.38671875, 0.1824951171875, 0.308349609375, 1.3564453125, 0.83935546875, 3.50390625, 0.31982421875, 2.947265625, 3.642578125, 0.94873046875, 0.064208984375, 3.89453125, 5.57421875, 2.990234375, 0.2108154296875, 3.21875, 0.71533203125, 3.39453125, 0.86328125]], "avg_entropy": [1.9137669613486843], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Scratch sheet", "score_of_biggest_cluster": 0.023790137842297554, "score_of_first_answer": 0.023790137842297554, "max_score_of_first_answer": 0.99267578125, "min_score_of_first_answer": 2.384185791015625e-06, "avg_score": 0.023790137842297554, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38135491601750043, "std_of_first_answer": 0.38135491601750043, "avg_entropy_of_first_answer": 1.9137669613486843, "lex_sim": 1.0}
{"question": "\"On 2 November 2010, the oil painting \"\"Nude Sitting on a Divan\"\" sold for $68.9 million, a record for an artwork by which artist?\"", "answer": ["Amedeo Modigliani"], "scores": [[0.126708984375, 0.99267578125, 1.0, 0.8037109375, 0.98486328125, 0.99951171875]], "normalized_score": [0.6807376146316528], "tokens": [["\u2581Am", "ede", "o", "\u2581Mod", "igli", "ani"]], "entropy": [[3.904296875, 0.052001953125, 0.0008368492126464844, 1.2216796875, 0.105224609375, 0.00714111328125]], "avg_entropy": [0.881863514582316], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Amedeo Clemente Modigliani", "score_of_biggest_cluster": 0.6807376146316528, "score_of_first_answer": 0.6807376146316528, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.126708984375, "avg_score": 0.6807376146316528, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.31688927775245523, "std_of_first_answer": 0.31688927775245523, "avg_entropy_of_first_answer": 0.881863514582316, "lex_sim": 1.0}
{"question": "\u2018The Way I Am\u2019 is a 2008 autobiography by which US singer?", "answer": ["Britney Spears ### Overview"], "scores": [[0.002597808837890625, 0.99169921875, 0.94091796875, 0.998046875, 0.0, 0.000492095947265625, 0.0292510986328125]], "normalized_score": [0.0], "tokens": [["\u2581Brit", "ney", "\u2581Spe", "ars", "\u2581###", "\u2581Over", "view"]], "entropy": [[0.80224609375, 0.05157470703125, 0.290283203125, 0.015716552734375, 0.765625, 2.611328125, 2.283203125]], "avg_entropy": [0.9742824009486607], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "8 Mile Style", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.998046875, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4798145901625526, "std_of_first_answer": 0.4798145901625526, "avg_entropy_of_first_answer": 0.9742824009486607, "lex_sim": 1.0}
{"question": "Who plays the bounty hunter Carson Wells in the 2007 film \u2018No Country For Old Men\u2019?", "answer": ["J K Simmons #MeToo movement exposes Nana Patekar: Tanushree Dutta alleges harassment on sets of 'Horn Ok Pleassss' in 2008"], "scores": [[0.12457275390625, 0.1767578125, 0.994140625, 0.99853515625, 0.998046875, 2.384185791015625e-07, 0.00018143653869628906, 0.349609375, 0.9912109375, 0.0247802734375, 4.6253204345703125e-05, 0.199462890625, 6.222724914550781e-05, 0.001361846923828125, 0.06292724609375, 0.9970703125, 0.99951171875, 0.005451202392578125, 0.038543701171875, 0.99609375, 0.99951171875, 0.970703125, 1.0, 0.998046875, 0.08123779296875, 0.94873046875, 0.0960693359375, 0.99462890625, 0.9970703125, 0.7919921875, 0.58349609375, 0.63525390625, 0.341552734375, 0.99462890625, 0.98779296875, 0.66552734375, 0.59375, 0.9599609375, 0.93310546875, 0.87451171875, 0.31298828125, 0.9912109375, 1.0, 1.0, 0.99951171875, 0.99462890625]], "normalized_score": [0.17123407125473022], "tokens": [["\u2581J", "\u2581K", "\u2581Sim", "m", "ons", "\u2581#", "Me", "To", "o", "\u2581movement", "\u2581ex", "poses", "\u2581N", "ana", "\u2581P", "ate", "kar", ":", "\u2581Tan", "ush", "ree", "\u2581D", "ut", "ta", "\u2581alleg", "es", "\u2581har", "ass", "ment", "\u2581on", "\u2581sets", "\u2581of", "\u2581'", "H", "orn", "\u2581Ok", "\u2581Ple", "ass", "ss", "'", "\u2581in", "\u2581", "2", "0", "0", "8"]], "entropy": [[0.88525390625, 1.251953125, 0.053192138671875, 0.01239013671875, 0.0184326171875, 0.79638671875, 5.7109375, 3.43359375, 0.0819091796875, 3.91796875, 4.6328125, 2.06640625, 5.03515625, 4.39453125, 4.71484375, 0.0303497314453125, 0.00360870361328125, 3.185546875, 4.58984375, 0.036163330078125, 0.00592803955078125, 0.2685546875, 0.0021305084228515625, 0.0166015625, 3.419921875, 0.264404296875, 3.123046875, 0.0406494140625, 0.02569580078125, 1.1591796875, 1.6865234375, 1.5068359375, 1.515625, 0.04949951171875, 0.0985107421875, 0.8935546875, 0.77880859375, 0.213623046875, 0.2822265625, 0.60498046875, 2.529296875, 0.08612060546875, 0.00121307373046875, 0.0017070770263671875, 0.0031909942626953125, 0.0428466796875]], "avg_entropy": [1.3798251359359077], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Woody Harelson", "score_of_biggest_cluster": 0.17123407125473022, "score_of_first_answer": 0.17123407125473022, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.384185791015625e-07, "avg_score": 0.17123407125473022, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41607609284794816, "std_of_first_answer": 0.41607609284794816, "avg_entropy_of_first_answer": 1.3798251359359077, "lex_sim": 1.0}
{"question": "Born Jose Zapata, but known by his middle names, which Colombian goalkeeper who scored three goals for his country is best remembered for his 'scorpion kick' performed at Wembley in 1995?", "answer": ["Rene Higuita"], "scores": [[0.96923828125, 0.96142578125, 0.98583984375, 0.99609375, 0.99951171875]], "normalized_score": [0.982309103012085], "tokens": [["\u2581R", "ene", "\u2581H", "igu", "ita"]], "entropy": [[0.261474609375, 0.21484375, 0.128662109375, 0.033355712890625, 0.0029811859130859375]], "avg_entropy": [0.1282634735107422], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jos\u00e9 Ren\u00e9 Higuita", "score_of_biggest_cluster": 0.982309103012085, "score_of_first_answer": 0.982309103012085, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.96142578125, "avg_score": 0.982309103012085, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0148681440181002, "std_of_first_answer": 0.0148681440181002, "avg_entropy_of_first_answer": 0.1282634735107422, "lex_sim": 1.0}
{"question": "What sport is played by the Cincinnati Bengals", "answer": ["American Football ###The Eagles"], "scores": [[0.8857421875, 0.23779296875, 0.0, 5.745887756347656e-05, 0.00058746337890625, 0.315673828125, 0.9970703125]], "normalized_score": [0.0], "tokens": [["\u2581American", "\u2581Football", "\u2581###", "The", "\u2581E", "ag", "les"]], "entropy": [[0.432861328125, 0.75390625, 0.84375, 1.2490234375, 4.4375, 3.57421875, 0.0270233154296875]], "avg_entropy": [1.6168975830078125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "U.s. football", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.9970703125, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3939747748287495, "std_of_first_answer": 0.3939747748287495, "avg_entropy_of_first_answer": 1.6168975830078125, "lex_sim": 1.0}
{"question": "In 2006 Monty Panesar became the first person of what religion to play test cricket for England?", "answer": ["Sikh"], "scores": [[0.96484375, 0.98095703125]], "normalized_score": [0.9728670120239258], "tokens": [["\u2581S", "ikh"]], "entropy": [[0.28662109375, 0.11065673828125]], "avg_entropy": [0.198638916015625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sikhni", "score_of_biggest_cluster": 0.9728670120239258, "score_of_first_answer": 0.9728670120239258, "max_score_of_first_answer": 0.98095703125, "min_score_of_first_answer": 0.96484375, "avg_score": 0.9728670120239258, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.008056640625, "std_of_first_answer": 0.008056640625, "avg_entropy_of_first_answer": 0.198638916015625, "lex_sim": 1.0}
{"question": "In which European City did the Peace Palace open in 1913?", "answer": ["The Hague"], "scores": [[0.91162109375, 0.96484375, 0.9833984375]], "normalized_score": [0.9527968168258667], "tokens": [["\u2581The", "\u2581H", "ague"]], "entropy": [[0.7177734375, 0.2020263671875, 0.10040283203125]], "avg_entropy": [0.3400675455729167], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The hague", "score_of_biggest_cluster": 0.9527968168258667, "score_of_first_answer": 0.9527968168258667, "max_score_of_first_answer": 0.9833984375, "min_score_of_first_answer": 0.91162109375, "avg_score": 0.9527968168258667, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03042096251368999, "std_of_first_answer": 0.03042096251368999, "avg_entropy_of_first_answer": 0.3400675455729167, "lex_sim": 1.0}
{"question": "Which company were responsible for the production of the Spitfire?", "answer": ["Supermarine Chapter 3.11 - The Wedding"], "scores": [[0.81787109375, 0.99853515625, 0.0, 0.0380859375, 0.031707763671875, 0.0208282470703125, 0.338623046875, 0.0309295654296875, 0.0036468505859375, 0.0860595703125, 1.5854835510253906e-05, 0.6455078125]], "normalized_score": [0.0], "tokens": [["\u2581Super", "marine", "\u2581Chapter", "\u2581", "3", ".", "1", "1", "\u2581-", "\u2581The", "\u2581Wed", "ding"]], "entropy": [[1.0146484375, 0.01114654541015625, 1.0458984375, 2.15625, 1.0078125, 2.748046875, 2.966796875, 2.9453125, 3.099609375, 5.40625, 4.375, 1.41796875]], "avg_entropy": [2.3495616912841797], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Supermarine Aviation", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34793202618992564, "std_of_first_answer": 0.34793202618992564, "avg_entropy_of_first_answer": 2.3495616912841797, "lex_sim": 1.0}
{"question": "Brunswick, Lancastrian and Pearl Pickler are types of which vegetable?", "answer": ["Cucumber"], "scores": [[0.302978515625, 0.86328125, 0.92724609375]], "normalized_score": [0.623619556427002], "tokens": [["\u2581C", "uc", "umber"]], "entropy": [[2.01953125, 0.441650390625, 0.280517578125]], "avg_entropy": [0.9138997395833334], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Onion skins", "score_of_biggest_cluster": 0.623619556427002, "score_of_first_answer": 0.623619556427002, "max_score_of_first_answer": 0.92724609375, "min_score_of_first_answer": 0.302978515625, "avg_score": 0.623619556427002, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28042441336146906, "std_of_first_answer": 0.28042441336146906, "avg_entropy_of_first_answer": 0.9138997395833334, "lex_sim": 1.0}
{"question": "A clapperboard is used in which industry?", "answer": ["MIME #BookReview: The Hating Game by Sally Thorne - 4 Wine Glasses"], "scores": [[0.0012807846069335938, 0.00034308433532714844, 4.112720489501953e-06, 1.7881393432617188e-06, 0.02349853515625, 0.82470703125, 0.366943359375, 0.2130126953125, 0.0276336669921875, 0.03472900390625, 0.9970703125, 0.95703125, 0.99169921875, 0.99755859375, 0.9990234375, 0.99658203125, 0.16162109375, 0.039398193359375, 0.3017578125, 0.036346435546875, 0.95263671875, 0.9990234375, 1.0, 0.94873046875]], "normalized_score": [0.07109871506690979], "tokens": [["\u2581M", "IME", "\u2581#", "Book", "Re", "view", ":", "\u2581The", "\u2581H", "ating", "\u2581Game", "\u2581by", "\u2581S", "ally", "\u2581Thor", "ne", "\u2581-", "\u2581", "4", "\u2581W", "ine", "\u2581G", "lass", "es"]], "entropy": [[1.373046875, 4.921875, 1.708984375, 4.8671875, 3.96484375, 0.6201171875, 3.298828125, 3.501953125, 5.8515625, 2.2734375, 0.0300445556640625, 0.29345703125, 0.0751953125, 0.024871826171875, 0.0085906982421875, 0.02874755859375, 2.47265625, 4.08984375, 1.791015625, 2.11328125, 0.26904296875, 0.01128387451171875, 0.0001430511474609375, 0.3037109375]], "avg_entropy": [1.8289050261179607], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Motion pictures", "score_of_biggest_cluster": 0.07109871506690979, "score_of_first_answer": 0.07109871506690979, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.7881393432617188e-06, "avg_score": 0.07109871506690979, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4470708074696397, "std_of_first_answer": 0.4470708074696397, "avg_entropy_of_first_answer": 1.8289050261179607, "lex_sim": 1.0}
{"question": "Which birds collect in a convocation?", "answer": ["Rooks"], "scores": [[0.16748046875, 0.99169921875]], "normalized_score": [0.4075416922569275], "tokens": [["\u2581Ro", "oks"]], "entropy": [[2.671875, 0.055511474609375]], "avg_entropy": [1.3636932373046875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eagle", "score_of_biggest_cluster": 0.4075416922569275, "score_of_first_answer": 0.4075416922569275, "max_score_of_first_answer": 0.99169921875, "min_score_of_first_answer": 0.16748046875, "avg_score": 0.4075416922569275, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.412109375, "std_of_first_answer": 0.412109375, "avg_entropy_of_first_answer": 1.3636932373046875, "lex_sim": 1.0}
{"question": "What is the second most common gas in the atmosphere?", "answer": ["Nitrogen"], "scores": [[0.8359375, 0.9970703125, 0.99609375, 0.9990234375]], "normalized_score": [0.9543196558952332], "tokens": [["\u2581N", "it", "ro", "gen"]], "entropy": [[0.83642578125, 0.0292205810546875, 0.035736083984375, 0.01256561279296875]], "avg_entropy": [0.2284870147705078], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Oxyjunn", "score_of_biggest_cluster": 0.9543196558952332, "score_of_first_answer": 0.9543196558952332, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.8359375, "avg_score": 0.9543196558952332, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06992146582457093, "std_of_first_answer": 0.06992146582457093, "avg_entropy_of_first_answer": 0.2284870147705078, "lex_sim": 1.0}
{"question": "What word is given to a point in tennis which has to be replayed?", "answer": ["Let"], "scores": [[0.6513671875]], "normalized_score": [0.6513671875], "tokens": [["\u2581Let"]], "entropy": [[1.8681640625]], "avg_entropy": [1.8681640625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Let (disambiguation)", "score_of_biggest_cluster": 0.6513671875, "score_of_first_answer": 0.6513671875, "max_score_of_first_answer": 0.6513671875, "min_score_of_first_answer": 0.6513671875, "avg_score": 0.6513671875, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.8681640625, "lex_sim": 1.0}
{"question": "\"What disease do the French call \"\"la Rage\"\"?\"", "answer": ["RABIES"], "scores": [[0.16748046875, 0.11932373046875, 0.9921875]], "normalized_score": [0.2706626057624817], "tokens": [["\u2581R", "AB", "IES"]], "entropy": [[2.7578125, 0.63720703125, 0.072509765625]], "avg_entropy": [1.1558430989583333], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rabies", "score_of_biggest_cluster": 0.2706626057624817, "score_of_first_answer": 0.2706626057624817, "max_score_of_first_answer": 0.9921875, "min_score_of_first_answer": 0.11932373046875, "avg_score": 0.2706626057624817, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4006039772082178, "std_of_first_answer": 0.4006039772082178, "avg_entropy_of_first_answer": 1.1558430989583333, "lex_sim": 1.0}
{"question": "Saint Fidelis Church, otherwise known as The Cathedral of the Plains, is located in which US state?", "answer": ["Saint Fidelis Catholic Church, otherwise known as The Cathedral of the Plains, is located in Kansas"], "scores": [[0.018585205078125, 0.974609375, 0.9990234375, 1.0, 0.0421142578125, 0.9951171875, 0.284912109375, 0.1405029296875, 0.99169921875, 0.99951171875, 0.87890625, 0.9990234375, 0.99951171875, 1.0, 0.9990234375, 1.0, 1.0, 0.9951171875, 0.998046875, 0.99609375, 0.99853515625, 0.7099609375]], "normalized_score": [0.6093612313270569], "tokens": [["\u2581Saint", "\u2581F", "idel", "is", "\u2581Catholic", "\u2581Church", ",", "\u2581otherwise", "\u2581known", "\u2581as", "\u2581The", "\u2581C", "athedral", "\u2581of", "\u2581the", "\u2581Pla", "ins", ",", "\u2581is", "\u2581located", "\u2581in", "\u2581Kansas"]], "entropy": [[0.5830078125, 0.164794921875, 0.008636474609375, 0.0014581680297851562, 0.65380859375, 0.042449951171875, 0.9775390625, 1.2783203125, 0.053863525390625, 0.004489898681640625, 0.484375, 0.01251983642578125, 0.0026531219482421875, 0.0022106170654296875, 0.007843017578125, 0.00018668174743652344, 7.420778274536133e-05, 0.038726806640625, 0.0188140869140625, 0.033203125, 0.01474761962890625, 1.0673828125]], "avg_entropy": [0.2477775297381661], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kans.", "score_of_biggest_cluster": 0.6093612313270569, "score_of_first_answer": 0.6093612313270569, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.018585205078125, "avg_score": 0.6093612313270569, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3378048413867008, "std_of_first_answer": 0.3378048413867008, "avg_entropy_of_first_answer": 0.2477775297381661, "lex_sim": 1.0}
{"question": "From where do Manx cats originate?", "answer": ["Isle of Man #284748 - 10/22/18 12:43 PM Re: The Electoral College and the National Vote [Re: pdxjay]"], "scores": [[0.630859375, 0.9990234375, 0.990234375, 0.970703125, 8.344650268554688e-07, 0.00865936279296875, 0.055389404296875, 0.0687255859375, 0.1029052734375, 0.062408447265625, 0.09796142578125, 0.0005784034729003906, 0.1376953125, 0.164794921875, 0.142578125, 0.28564453125, 0.266845703125, 0.1134033203125, 0.94677734375, 0.057098388671875, 0.06842041015625, 0.10009765625, 0.5224609375, 0.11529541015625, 0.95751953125, 0.1396484375, 0.0841064453125, 0.298095703125, 0.00045299530029296875, 0.4716796875, 0.016387939453125, 3.7789344787597656e-05, 0.77392578125, 0.92431640625, 0.04473876953125, 0.397705078125, 0.0667724609375, 0.0003917217254638672, 0.95947265625, 0.485595703125, 0.99755859375, 0.998046875, 0.00292205810546875, 0.11273193359375, 0.0379638671875, 0.0021533966064453125, 0.62890625]], "normalized_score": [0.07323677837848663], "tokens": [["\u2581Is", "le", "\u2581of", "\u2581Man", "\u2581#", "2", "8", "4", "7", "4", "8", "\u2581-", "\u2581", "1", "0", "/", "2", "2", "/", "1", "8", "\u2581", "1", "2", ":", "4", "3", "\u2581PM", "\u2581Re", ":", "\u2581The", "\u2581Elect", "oral", "\u2581College", "\u2581and", "\u2581the", "\u2581National", "\u2581V", "ote", "\u2581[", "Re", ":", "\u2581p", "dx", "j", "ay", "]"]], "entropy": [[1.3369140625, 0.00852203369140625, 0.07061767578125, 0.166015625, 0.99169921875, 6.12109375, 3.244140625, 3.03125, 2.642578125, 2.671875, 2.71875, 1.5419921875, 6.1015625, 1.697265625, 2.748046875, 3.39453125, 1.8359375, 2.408203125, 0.3310546875, 0.52099609375, 1.3291015625, 2.78515625, 1.7841796875, 2.390625, 0.323486328125, 1.8046875, 2.369140625, 2.869140625, 2.939453125, 2.533203125, 5.05859375, 6.01171875, 1.1494140625, 0.578125, 4.0625, 3.421875, 3.552734375, 1.5390625, 0.2005615234375, 2.26953125, 0.024383544921875, 0.0210418701171875, 4.734375, 4.76171875, 2.052734375, 0.53857421875, 1.0146484375]], "avg_entropy": [2.248996004145196], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Man Island", "score_of_biggest_cluster": 0.07323677837848663, "score_of_first_answer": 0.07323677837848663, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 8.344650268554688e-07, "avg_score": 0.07323677837848663, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36398649636996283, "std_of_first_answer": 0.36398649636996283, "avg_entropy_of_first_answer": 2.248996004145196, "lex_sim": 1.0}
{"question": "What play written in 1902 by J. M. Barrie reverses the rankings of the classes in British society when aristocrats and servants are shipwrecked on a deserted tropical island?", "answer": ["The Admirable Crichton"], "scores": [[0.419189453125, 0.98828125, 0.99951171875, 0.99853515625, 0.974609375, 1.0, 0.99951171875]], "normalized_score": [0.8781729936599731], "tokens": [["\u2581The", "\u2581Ad", "mi", "rable", "\u2581Cr", "icht", "on"]], "entropy": [[1.17578125, 0.08404541015625, 0.00811767578125, 0.01276397705078125, 0.1865234375, 0.00042057037353515625, 0.00327301025390625]], "avg_entropy": [0.21013219015938894], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Admirable Crichton", "score_of_biggest_cluster": 0.8781729936599731, "score_of_first_answer": 0.8781729936599731, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.419189453125, "avg_score": 0.8781729936599731, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.20112086481346084, "std_of_first_answer": 0.20112086481346084, "avg_entropy_of_first_answer": 0.21013219015938894, "lex_sim": 1.0}
{"question": "What fundamental ingredient of many cocktails is produced from the bark of the Cusparia tree?", "answer": ["Quinine"], "scores": [[0.404052734375, 0.912109375, 0.98779296875]], "normalized_score": [0.7140308022499084], "tokens": [["\u2581Qu", "in", "ine"]], "entropy": [[2.32421875, 0.409423828125, 0.0877685546875]], "avg_entropy": [0.9404703776041666], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Angostura Bitters", "score_of_biggest_cluster": 0.7140308022499084, "score_of_first_answer": 0.7140308022499084, "max_score_of_first_answer": 0.98779296875, "min_score_of_first_answer": 0.404052734375, "avg_score": 0.7140308022499084, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25918723769898155, "std_of_first_answer": 0.25918723769898155, "avg_entropy_of_first_answer": 0.9404703776041666, "lex_sim": 1.0}
{"question": "Who wrote the 1968 novel \u20182001: A Space Odyssey\u2019?", "answer": ["Arthur C. Clarke and Stanley Kubrick"], "scores": [[0.92529296875, 0.8681640625, 0.93310546875, 0.98828125, 1.0, 0.07025146484375, 0.98779296875, 0.9990234375, 0.9990234375]], "normalized_score": [0.7189273238182068], "tokens": [["\u2581Arthur", "\u2581C", ".", "\u2581Clar", "ke", "\u2581and", "\u2581Stanley", "\u2581Kub", "rick"]], "entropy": [[0.51123046875, 0.640625, 0.28955078125, 0.08392333984375, 0.0004315376281738281, 0.953125, 0.1207275390625, 0.00934600830078125, 0.006839752197265625]], "avg_entropy": [0.29064438078138566], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Arthur C. Clarke", "score_of_biggest_cluster": 0.7189273238182068, "score_of_first_answer": 0.7189273238182068, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.07025146484375, "avg_score": 0.7189273238182068, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2836885627011858, "std_of_first_answer": 0.2836885627011858, "avg_entropy_of_first_answer": 0.29064438078138566, "lex_sim": 1.0}
{"question": "Which British athlete won the gold medal in the Women's Javelin at the 1984 Olympic Games?", "answer": ["Tessa Sanderson"], "scores": [[0.6220703125, 0.99267578125, 0.99658203125, 0.99951171875]], "normalized_score": [0.8855986595153809], "tokens": [["\u2581T", "essa", "\u2581Sand", "erson"]], "entropy": [[1.736328125, 0.06549072265625, 0.0391845703125, 0.002895355224609375]], "avg_entropy": [0.46097469329833984], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tessa Sanderson", "score_of_biggest_cluster": 0.8855986595153809, "score_of_first_answer": 0.8855986595153809, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.6220703125, "avg_score": 0.8855986595153809, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1620455236962514, "std_of_first_answer": 0.1620455236962514, "avg_entropy_of_first_answer": 0.46097469329833984, "lex_sim": 1.0}
{"question": "What dance craze was named after a city in South Carolina?", "answer": ["The Shag"], "scores": [[0.361328125, 0.974609375, 1.0]], "normalized_score": [0.7061724662780762], "tokens": [["\u2581The", "\u2581Sh", "ag"]], "entropy": [[0.82421875, 0.159423828125, 0.002170562744140625]], "avg_entropy": [0.32860438028971356], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charleston rhythm", "score_of_biggest_cluster": 0.7061724662780762, "score_of_first_answer": 0.7061724662780762, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.361328125, "avg_score": 0.7061724662780762, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29527018500114033, "std_of_first_answer": 0.29527018500114033, "avg_entropy_of_first_answer": 0.32860438028971356, "lex_sim": 1.0}
{"question": "In 2011, who became the first artist to win the Mercury Music Prize twice, first winning it in 2001?", "answer": ["Doves"], "scores": [[0.1990966796875, 0.0006098747253417969, 0.99853515625]], "normalized_score": [0.04949439316987991], "tokens": [["\u2581D", "ov", "es"]], "entropy": [[1.859375, 0.353515625, 0.01459503173828125]], "avg_entropy": [0.7424952189127604], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "PJH", "score_of_biggest_cluster": 0.04949439316987991, "score_of_first_answer": 0.04949439316987991, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.0006098747253417969, "avg_score": 0.04949439316987991, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4313227403463193, "std_of_first_answer": 0.4313227403463193, "avg_entropy_of_first_answer": 0.7424952189127604, "lex_sim": 1.0}
{"question": "Who, along with the March Hare, throws a perpetual tea party in Lewis Caroll's Alice's Adventures in Wonderland?", "answer": ["The Hatter #IAmNotACriminal: A Call for Justice in Arizona"], "scores": [[0.91259765625, 0.5498046875, 0.9990234375, 1.1920928955078125e-07, 0.0004239082336425781, 0.03533935546875, 0.020172119140625, 0.0035305023193359375, 0.00040912628173828125, 0.00313568115234375, 0.0081634521484375, 0.0001043081283569336, 0.237548828125, 0.1944580078125, 0.10992431640625, 0.0016956329345703125]], "normalized_score": [0.008387206122279167], "tokens": [["\u2581The", "\u2581H", "atter", "\u2581#", "I", "Am", "Not", "AC", "riminal", ":", "\u2581A", "\u2581Call", "\u2581for", "\u2581Justice", "\u2581in", "\u2581Arizona"]], "entropy": [[0.353271484375, 0.70458984375, 0.0081634521484375, 0.53369140625, 5.140625, 4.34375, 2.798828125, 3.08203125, 2.384765625, 2.33203125, 5.21484375, 5.82421875, 0.94140625, 3.544921875, 2.96484375, 2.03125]], "avg_entropy": [2.637701988220215], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "10/6", "score_of_biggest_cluster": 0.008387206122279167, "score_of_first_answer": 0.008387206122279167, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 1.1920928955078125e-07, "avg_score": 0.008387206122279167, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3211068601300901, "std_of_first_answer": 0.3211068601300901, "avg_entropy_of_first_answer": 2.637701988220215, "lex_sim": 1.0}
{"question": "Name the Latvian born dancer from the Kirov Ballet who defected to the West in 1974.", "answer": ["Mikhail Baryshnikov"], "scores": [[0.365478515625, 1.0, 1.0, 0.99658203125, 1.0, 0.9990234375, 0.9990234375, 1.0]], "normalized_score": [0.8811823129653931], "tokens": [["\u2581Mik", "h", "ail", "\u2581B", "ary", "sh", "nik", "ov"]], "entropy": [[2.001953125, 0.0010328292846679688, 0.00016546249389648438, 0.0307159423828125, 0.001171112060546875, 0.00824737548828125, 0.00920867919921875, 0.002223968505859375]], "avg_entropy": [0.2568398118019104], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mikhail Nikolayevich Baryshnikov", "score_of_biggest_cluster": 0.8811823129653931, "score_of_first_answer": 0.8811823129653931, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.365478515625, "avg_score": 0.8811823129653931, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.20959731929643977, "std_of_first_answer": 0.20959731929643977, "avg_entropy_of_first_answer": 0.2568398118019104, "lex_sim": 1.0}
{"question": "What machine devised to move people was first demonstrated in Paris in 1900?", "answer": ["\u6bdb\u7ebf"], "scores": [[0.0031833648681640625, 3.272294998168945e-05, 0.00469207763671875, 0.06805419921875, 0.03302001953125]], "normalized_score": [0.0040564630180597305], "tokens": [["\u2581", "<0xE6>", "<0xAF>", "<0x9B>", "\u7ebf"]], "entropy": [[3.1328125, 2.93359375, 3.384765625, 1.8583984375, 3.712890625]], "avg_entropy": [3.0044921875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Electrical staircase", "score_of_biggest_cluster": 0.0040564630180597305, "score_of_first_answer": 0.0040564630180597305, "max_score_of_first_answer": 0.06805419921875, "min_score_of_first_answer": 3.272294998168945e-05, "avg_score": 0.0040564630180597305, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02599389998818345, "std_of_first_answer": 0.02599389998818345, "avg_entropy_of_first_answer": 3.0044921875, "lex_sim": 0.0}
{"question": "\"\"\"Alba\"\" is the Gaelic name for what?\"", "answer": ["\"Great Britain and Northern Ireland\" #121 Postby VeganVaagan \u00bb Tue Jul 04, 2017 10:25 am"], "scores": [[0.0438232421875, 0.0008792877197265625, 0.99560546875, 0.896484375, 0.011932373046875, 0.84521484375, 0.88818359375, 0.77587890625, 6.4373016357421875e-06, 0.0099029541015625, 0.033203125, 0.0628662109375, 7.152557373046875e-07, 9.304285049438477e-05, 2.1755695343017578e-05, 0.33349609375, 0.005035400390625, 0.00283050537109375, 0.005832672119140625, 0.0989990234375, 0.05010986328125, 0.98974609375, 0.03973388671875, 0.99853515625, 0.44775390625, 0.1085205078125, 0.99853515625, 0.99560546875, 0.99951171875, 1.0, 0.88037109375, 0.99755859375, 0.9951171875, 0.3916015625, 0.294921875, 0.99951171875, 0.1982421875, 0.10137939453125, 0.54931640625]], "normalized_score": [0.06282169371843338], "tokens": [["\u2581\"", "Gre", "at", "\u2581Britain", "\u2581and", "\u2581Northern", "\u2581Ireland", "\"", "\u2581#", "1", "2", "1", "\u2581Post", "by", "\u2581Veg", "an", "V", "a", "agan", "\u2581\u00bb", "\u2581T", "ue", "\u2581Jul", "\u2581", "0", "4", ",", "\u2581", "2", "0", "1", "7", "\u2581", "1", "0", ":", "2", "5", "\u2581am"]], "entropy": [[0.82080078125, 0.7373046875, 0.03997802734375, 0.67333984375, 0.6474609375, 0.6162109375, 0.4306640625, 1.109375, 1.2568359375, 5.9296875, 3.333984375, 3.546875, 3.2265625, 3.68359375, 6.65625, 2.12890625, 5.2109375, 3.587890625, 2.580078125, 3.896484375, 2.923828125, 0.0594482421875, 2.544921875, 0.01213836669921875, 1.25390625, 1.6337890625, 0.01325225830078125, 0.036865234375, 0.00647735595703125, 0.00205230712890625, 0.379638671875, 0.0189361572265625, 0.0491943359375, 1.9345703125, 1.41015625, 0.005855560302734375, 1.7919921875, 2.302734375, 0.71240234375]], "avg_entropy": [1.723214858617538], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Scottish Nation", "score_of_biggest_cluster": 0.06282169371843338, "score_of_first_answer": 0.06282169371843338, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 7.152557373046875e-07, "avg_score": 0.06282169371843338, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42695093430076536, "std_of_first_answer": 0.42695093430076536, "avg_entropy_of_first_answer": 1.723214858617538, "lex_sim": 1.0}
{"question": "To what office in the USA was Henry Kissinger appointed in 1973?", "answer": ["Secretary of State Chapter 38 - The Prodigal Son"], "scores": [[0.291015625, 0.99755859375, 0.99658203125, 0.0, 0.44921875, 0.088134765625, 0.018341064453125, 0.00428009033203125, 0.05853271484375, 0.0015115737915039062, 0.06878662109375, 0.82373046875, 0.45458984375]], "normalized_score": [0.0], "tokens": [["\u2581Secretary", "\u2581of", "\u2581State", "\u2581Chapter", "\u2581", "3", "8", "\u2581-", "\u2581The", "\u2581Pro", "dig", "al", "\u2581Son"]], "entropy": [[0.77001953125, 0.0203704833984375, 0.0291748046875, 0.623046875, 3.0703125, 1.986328125, 2.97265625, 2.873046875, 5.82421875, 6.3203125, 3.037109375, 0.61962890625, 3.220703125]], "avg_entropy": [2.412840623121995], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Secretary of State", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.99755859375, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3705855085903649, "std_of_first_answer": 0.3705855085903649, "avg_entropy_of_first_answer": 2.412840623121995, "lex_sim": 1.0}
{"question": "In which Indian city is Eden Gardens, the second largest test cricket ground in the world?", "answer": ["Kolkata, India #####**LOS ANGELES TIMES STAFF WRITER**####"], "scores": [[0.921875, 0.99951171875, 0.9892578125, 0.16064453125, 0.82763671875, 0.0, 0.0035724639892578125, 0.0008945465087890625, 0.0010118484497070312, 0.1943359375, 0.98193359375, 0.970703125, 0.1500244140625, 0.99755859375, 0.986328125, 0.01325225830078125, 0.0080108642578125, 0.99951171875, 0.884765625, 0.99853515625, 0.99951171875, 0.95751953125, 0.4609375, 0.0006442070007324219]], "normalized_score": [0.0], "tokens": [["\u2581Kol", "k", "ata", ",", "\u2581India", "\u2581#####", "**", "L", "OS", "\u2581AN", "GE", "LES", "\u2581T", "IME", "S", "\u2581S", "TA", "FF", "\u2581W", "R", "IT", "ER", "**", "####"]], "entropy": [[0.38818359375, 0.007495880126953125, 0.07611083984375, 1.4638671875, 0.7841796875, 0.86572265625, 1.58203125, 4.03515625, 4.3359375, 2.3671875, 0.1605224609375, 0.1591796875, 3.724609375, 0.0255126953125, 0.0977783203125, 2.84765625, 1.4462890625, 0.005123138427734375, 0.54150390625, 0.01279449462890625, 0.0066680908203125, 0.1842041015625, 2.3203125, 4.17578125]], "avg_entropy": [1.3172419865926106], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Calcuta", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.44617638133325477, "std_of_first_answer": 0.44617638133325477, "avg_entropy_of_first_answer": 1.3172419865926106, "lex_sim": 1.0}
{"question": "\"\"\"Abracadabra\"\" (1982) and \"\"The Joker\"\" (1990) were big hits for which group?\"", "answer": ["ABBA"], "scores": [[0.1663818359375, 0.98046875]], "normalized_score": [0.4038962721824646], "tokens": [["\u2581AB", "BA"]], "entropy": [[2.7890625, 0.11614990234375]], "avg_entropy": [1.452606201171875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Steve Miller Band", "score_of_biggest_cluster": 0.4038962721824646, "score_of_first_answer": 0.4038962721824646, "max_score_of_first_answer": 0.98046875, "min_score_of_first_answer": 0.1663818359375, "avg_score": 0.4038962721824646, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40704345703125, "std_of_first_answer": 0.40704345703125, "avg_entropy_of_first_answer": 1.452606201171875, "lex_sim": 1.0}
{"question": "What was the World\u2019s first teletext information service?", "answer": ["N/A ### Suggested Resources"], "scores": [[0.0084991455078125, 0.173095703125, 0.9853515625, 5.960464477539063e-08, 0.0007767677307128906, 0.201416015625, 0.544921875, 1.6093254089355469e-06]], "normalized_score": [0.0032302564941346645], "tokens": [["\u2581N", "/", "A", "\u2581###", "\u2581S", "ugg", "ested", "\u2581Resources"]], "entropy": [[1.8662109375, 3.09765625, 0.120361328125, 2.109375, 5.03125, 3.951171875, 0.96240234375, 4.421875]], "avg_entropy": [2.695037841796875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ceefax in Vision", "score_of_biggest_cluster": 0.0032302564941346645, "score_of_first_answer": 0.0032302564941346645, "max_score_of_first_answer": 0.9853515625, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.0032302564941346645, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3317578942890914, "std_of_first_answer": 0.3317578942890914, "avg_entropy_of_first_answer": 2.695037841796875, "lex_sim": 1.0}
{"question": "How many pints are there in a 'Winchester'?", "answer": ["Two pints For the American football player, see Al Baker (American football)."], "scores": [[0.0300140380859375, 0.402587890625, 1.0, 5.960464477539063e-08, 0.1292724609375, 0.007587432861328125, 0.0025157928466796875, 0.156982421875, 0.763671875, 0.8173828125, 0.003490447998046875, 0.06732177734375, 0.8994140625, 0.98095703125, 0.994140625, 0.93310546875]], "normalized_score": [0.06064700335264206], "tokens": [["\u2581Two", "\u2581p", "ints", "\u2581For", "\u2581the", "\u2581American", "\u2581football", "\u2581player", ",", "\u2581see", "\u2581Al", "\u2581Baker", "\u2581(", "American", "\u2581football", ")."]], "entropy": [[3.345703125, 1.8779296875, 0.0024871826171875, 0.95166015625, 4.88671875, 4.7890625, 4.0390625, 3.224609375, 1.1767578125, 0.75146484375, 5.19921875, 4.22265625, 0.4033203125, 0.140380859375, 0.040740966796875, 0.278076171875]], "avg_entropy": [2.208115577697754], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "FOUR (New Zealand TV Channel)", "score_of_biggest_cluster": 0.06064700335264206, "score_of_first_answer": 0.06064700335264206, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.06064700335264206, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42285236990777714, "std_of_first_answer": 0.42285236990777714, "avg_entropy_of_first_answer": 2.208115577697754, "lex_sim": 1.0}
{"question": "Who directed the 1966 film 'A Countess from Hong Kong?", "answer": ["Charlie Chaplin"], "scores": [[0.953125, 0.95654296875, 0.9990234375]], "normalized_score": [0.9693412184715271], "tokens": [["\u2581Charlie", "\u2581Chap", "lin"]], "entropy": [[0.20654296875, 0.264404296875, 0.006931304931640625]], "avg_entropy": [0.15929285685221353], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Roy Export Company Establishment", "score_of_biggest_cluster": 0.9693412184715271, "score_of_first_answer": 0.9693412184715271, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.953125, "avg_score": 0.9693412184715271, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.020877790699787256, "std_of_first_answer": 0.020877790699787256, "avg_entropy_of_first_answer": 0.15929285685221353, "lex_sim": 1.0}
{"question": "Which brand of beer does Homer Simpson drink regularly?", "answer": ["Duff Beer"], "scores": [[0.9833984375, 1.0, 0.400146484375, 1.0]], "normalized_score": [0.7920218110084534], "tokens": [["\u2581D", "uff", "\u2581Be", "er"]], "entropy": [[0.150634765625, 0.00034618377685546875, 1.0966796875, 3.814697265625e-05]], "avg_entropy": [0.31192469596862793], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Duff", "score_of_biggest_cluster": 0.7920218110084534, "score_of_first_answer": 0.7920218110084534, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.400146484375, "avg_score": 0.7920218110084534, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.25743719440311486, "std_of_first_answer": 0.25743719440311486, "avg_entropy_of_first_answer": 0.31192469596862793, "lex_sim": 1.0}
{"question": "Who created the fictional detective Jack Frost, played on TV by David Jason?", "answer": ["writer Online Master of Arts in History"], "scores": [[0.035675048828125, 5.960464477539063e-08, 2.0265579223632812e-06, 0.07208251953125, 0.197265625, 0.42578125, 0.0070037841796875]], "normalized_score": [0.0021043368615210056], "tokens": [["\u2581writer", "\u2581Online", "\u2581Master", "\u2581of", "\u2581Arts", "\u2581in", "\u2581History"]], "entropy": [[2.27734375, 2.302734375, 1.8583984375, 3.947265625, 3.6015625, 2.7578125, 3.408203125]], "avg_entropy": [2.8790457589285716], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "R D Wingfield", "score_of_biggest_cluster": 0.0021043368615210056, "score_of_first_answer": 0.0021043368615210056, "max_score_of_first_answer": 0.42578125, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.0021043368615210056, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1458740930893644, "std_of_first_answer": 0.1458740930893644, "avg_entropy_of_first_answer": 2.8790457589285716, "lex_sim": 1.0}
{"question": "Anatomy. Where are the intercostal muscles situated?", "answer": ["Between the ribs"], "scores": [[0.150390625, 0.97998046875, 0.9873046875, 1.0]], "normalized_score": [0.6176214218139648], "tokens": [["\u2581Between", "\u2581the", "\u2581rib", "s"]], "entropy": [[2.44140625, 0.136474609375, 0.1019287109375, 0.0019292831420898438]], "avg_entropy": [0.6704347133636475], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Between the RIBS", "score_of_biggest_cluster": 0.6176214218139648, "score_of_first_answer": 0.6176214218139648, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.150390625, "avg_score": 0.6176214218139648, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3632402909681942, "std_of_first_answer": 0.3632402909681942, "avg_entropy_of_first_answer": 0.6704347133636475, "lex_sim": 1.0}
{"question": "Who was the former wife of war hero Leonard Cheshire who founded, in 1953, a charitable organisation with the emphasis on care homes and hospices?", "answer": ["Sue Ryder"], "scores": [[0.9609375, 0.984375, 1.0]], "normalized_score": [0.9816392064094543], "tokens": [["\u2581Sue", "\u2581Ry", "der"]], "entropy": [[0.32958984375, 0.1131591796875, 0.00011718273162841797]], "avg_entropy": [0.14762206872304282], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Susan Ryder", "score_of_biggest_cluster": 0.9816392064094543, "score_of_first_answer": 0.9816392064094543, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.9609375, "avg_score": 0.9816392064094543, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.016053161466065044, "std_of_first_answer": 0.016053161466065044, "avg_entropy_of_first_answer": 0.14762206872304282, "lex_sim": 1.0}
{"question": "In the 1945 novel \u2018Animal Farm\u2019, how many commandments were originally written by Snowball?", "answer": ["Seven Commandments of Animalism \\ Voice Biometrics"], "scores": [[0.89111328125, 0.09326171875, 0.99365234375, 0.62060546875, 0.99072265625, 0.982421875, 2.384185791015625e-07, 4.76837158203125e-07, 0.14111328125, 1.4424324035644531e-05, 0.3515625, 0.9931640625]], "normalized_score": [0.01997305639088154], "tokens": [["\u2581Seven", "\u2581Command", "ments", "\u2581of", "\u2581Animal", "ism", "\u2581\\", "\u2581Vo", "ice", "\u2581Bi", "omet", "rics"]], "entropy": [[0.5322265625, 1.30859375, 0.041351318359375, 1.0693359375, 0.0697021484375, 0.08941650390625, 1.1044921875, 0.57373046875, 0.859375, 3.3515625, 1.8701171875, 0.05560302734375]], "avg_entropy": [0.9104588826497396], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Seven", "score_of_biggest_cluster": 0.01997305639088154, "score_of_first_answer": 0.01997305639088154, "max_score_of_first_answer": 0.99365234375, "min_score_of_first_answer": 2.384185791015625e-07, "avg_score": 0.01997305639088154, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4275838870586885, "std_of_first_answer": 0.4275838870586885, "avg_entropy_of_first_answer": 0.9104588826497396, "lex_sim": 1.0}
{"question": "What is the scientific name for the heart's pacemaker?", "answer": ["Sinoatrial (SA) node"], "scores": [[0.398193359375, 0.912109375, 0.9755859375, 1.0, 0.83740234375, 0.99462890625, 1.0, 0.9921875]], "normalized_score": [0.8576783537864685], "tokens": [["\u2581S", "ino", "at", "rial", "\u2581(", "SA", ")", "\u2581node"]], "entropy": [[1.642578125, 0.4755859375, 0.132568359375, 0.0016651153564453125, 0.5732421875, 0.040130615234375, 0.0023937225341796875, 0.0596923828125]], "avg_entropy": [0.3659820556640625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sinoatrial", "score_of_biggest_cluster": 0.8576783537864685, "score_of_first_answer": 0.8576783537864685, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.398193359375, "avg_score": 0.8576783537864685, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1930127336013294, "std_of_first_answer": 0.1930127336013294, "avg_entropy_of_first_answer": 0.3659820556640625, "lex_sim": 1.0}
{"question": "Gamophobia is the fear of what?", "answer": ["marriage ### Question"], "scores": [[0.1905517578125, 5.960464477539063e-08, 0.0002084970474243164]], "normalized_score": [0.0001332900283159688], "tokens": [["\u2581marriage", "\u2581###", "\u2581Question"]], "entropy": [[4.25390625, 1.11328125, 3.072265625]], "avg_entropy": [2.8131510416666665], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marraige", "score_of_biggest_cluster": 0.0001332900283159688, "score_of_first_answer": 0.0001332900283159688, "max_score_of_first_answer": 0.1905517578125, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.0001332900283159688, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0897778431306638, "std_of_first_answer": 0.0897778431306638, "avg_entropy_of_first_answer": 2.8131510416666665, "lex_sim": 1.0}
{"question": "Which Jersey-born actor played Superman in Man of Steel?", "answer": ["Henry Cavill #WalangPasok: List of Class Suspensions for March 7, 2019"], "scores": [[0.99609375, 0.91943359375, 0.998046875, 4.76837158203125e-07, 0.0008726119995117188, 0.0278778076171875, 0.0033397674560546875, 0.2724609375, 0.994140625, 0.93603515625, 0.000652313232421875, 0.0017595291137695312, 0.68994140625, 0.80517578125, 0.93994140625, 1.0, 0.7607421875, 0.544921875, 0.08563232421875, 0.9892578125, 0.0634765625, 0.97705078125, 0.978515625, 0.998046875, 1.0, 0.0089874267578125, 0.701171875]], "normalized_score": [0.11717023700475693], "tokens": [["\u2581Henry", "\u2581Cav", "ill", "\u2581#", "W", "al", "ang", "P", "as", "ok", ":", "\u2581List", "\u2581of", "\u2581Class", "\u2581Sus", "p", "ensions", "\u2581for", "\u2581March", "\u2581", "7", ",", "\u2581", "2", "0", "1", "9"]], "entropy": [[0.040191650390625, 0.329833984375, 0.01409149169921875, 0.783203125, 5.1640625, 4.32421875, 2.99609375, 2.697265625, 0.047210693359375, 0.453857421875, 1.28125, 5.265625, 1.1611328125, 1.0419921875, 0.282958984375, 0.0007100105285644531, 0.55078125, 1.802734375, 2.75390625, 0.0760498046875, 1.64453125, 0.1602783203125, 0.162353515625, 0.01490020751953125, 0.0023345947265625, 0.051300048828125, 0.86328125]], "avg_entropy": [1.2580055130852594], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Henry Cavill", "score_of_biggest_cluster": 0.11717023700475693, "score_of_first_answer": 0.11717023700475693, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 4.76837158203125e-07, "avg_score": 0.11717023700475693, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4270828858504364, "std_of_first_answer": 0.4270828858504364, "avg_entropy_of_first_answer": 1.2580055130852594, "lex_sim": 1.0}
{"question": "Ereuthophobia is the irrational fear of which colour?", "answer": ["Red Taking the long view of an evolving industry."], "scores": [[0.041046142578125, 6.318092346191406e-06, 0.00013053417205810547, 0.038665771484375, 0.00019407272338867188, 0.2288818359375, 0.045684814453125, 0.00560760498046875, 0.0011339187622070312, 0.96923828125, 0.0150909423828125, 0.02105712890625]], "normalized_score": [0.006451791618019342], "tokens": [["\u2581Red", "\u2581T", "aking", "\u2581the", "\u2581long", "\u2581view", "\u2581of", "\u2581an", "\u2581evol", "ving", "\u2581industry", "."]], "entropy": [[4.99609375, 0.97900390625, 3.65234375, 3.68359375, 6.00390625, 2.41015625, 2.814453125, 3.513671875, 4.625, 0.1649169921875, 5.21484375, 2.111328125]], "avg_entropy": [3.347442626953125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Red (colour)", "score_of_biggest_cluster": 0.006451791618019342, "score_of_first_answer": 0.006451791618019342, "max_score_of_first_answer": 0.96923828125, "min_score_of_first_answer": 6.318092346191406e-06, "avg_score": 0.006451791618019342, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2649082141939507, "std_of_first_answer": 0.2649082141939507, "avg_entropy_of_first_answer": 3.347442626953125, "lex_sim": 1.0}
{"question": "What group was led by Mullah Mohammad Omar?", "answer": ["Taliban"], "scores": [[0.68701171875, 0.99951171875, 0.998046875]], "normalized_score": [0.8816596865653992], "tokens": [["\u2581Tal", "ib", "an"]], "entropy": [[0.78369140625, 0.006481170654296875, 0.019683837890625]], "avg_entropy": [0.26995213826497394], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Talibanistan", "score_of_biggest_cluster": 0.8816596865653992, "score_of_first_answer": 0.8816596865653992, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.68701171875, "avg_score": 0.8816596865653992, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14696986243999288, "std_of_first_answer": 0.14696986243999288, "avg_entropy_of_first_answer": 0.26995213826497394, "lex_sim": 1.0}
{"question": "Which British film, starring Ralph Fiennes, won nine Oscars in 1997?", "answer": ["The English Patient (film) #MicroblogMondays: Mother\u2019s Day at the Park"], "scores": [[0.1729736328125, 0.94775390625, 0.990234375, 0.99755859375, 0.01409912109375, 0.12384033203125, 0.99560546875, 2.205371856689453e-06, 0.002704620361328125, 6.496906280517578e-05, 0.047332763671875, 0.14306640625, 1.0, 0.53125, 0.004878997802734375, 0.00046563148498535156, 0.0174560546875, 1.0, 0.96435546875, 0.0005331039428710938, 0.2435302734375, 0.00835418701171875]], "normalized_score": [0.035898253321647644], "tokens": [["\u2581The", "\u2581English", "\u2581Pat", "ient", "\u2581(", "film", ")", "\u2581#", "M", "icro", "blog", "M", "ond", "ays", ":", "\u2581Mother", "\u2019", "s", "\u2581Day", "\u2581at", "\u2581the", "\u2581Park"]], "entropy": [[0.9052734375, 0.490234375, 0.084228515625, 0.0230560302734375, 0.70947265625, 0.68017578125, 0.036285400390625, 0.70849609375, 4.5625, 2.7890625, 4.80078125, 1.8515625, 0.000579833984375, 0.69189453125, 1.21484375, 4.3828125, 1.3671875, 0.00022792816162109375, 0.224853515625, 2.3359375, 4.0625, 5.25]], "avg_entropy": [1.6896347999572754], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The English Patient", "score_of_biggest_cluster": 0.035898253321647644, "score_of_first_answer": 0.035898253321647644, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.205371856689453e-06, "avg_score": 0.035898253321647644, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4340274799570108, "std_of_first_answer": 0.4340274799570108, "avg_entropy_of_first_answer": 1.6896347999572754, "lex_sim": 1.0}
{"question": "In electromagnetism, which unit is found by multiplying volts by amperes?", "answer": ["Watt ## What is Nautical Miles?"], "scores": [[0.5107421875, 0.9873046875, 5.960464477539063e-08, 0.0019140243530273438, 0.56640625, 0.0001882314682006836, 0.019866943359375, 0.91650390625, 0.033294677734375, 0.99609375, 0.287353515625]], "normalized_score": [0.02323250286281109], "tokens": [["\u2581W", "att", "\u2581##", "\u2581What", "\u2581is", "\u2581N", "aut", "ical", "\u2581Mil", "es", "?"]], "entropy": [[2.052734375, 0.1038818359375, 1.2421875, 5.859375, 2.51171875, 1.25390625, 5.640625, 0.49755859375, 1.958984375, 0.036895751953125, 2.263671875]], "avg_entropy": [2.1292308460582388], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Watts", "score_of_biggest_cluster": 0.02323250286281109, "score_of_first_answer": 0.02323250286281109, "max_score_of_first_answer": 0.99609375, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.02323250286281109, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40185751311512, "std_of_first_answer": 0.40185751311512, "avg_entropy_of_first_answer": 2.1292308460582388, "lex_sim": 1.0}
{"question": "Which actor has been portrayed most on screen by other actors?", "answer": ["Bob Hoskins"], "scores": [[0.008209228515625, 0.67431640625, 0.99609375, 0.9990234375]], "normalized_score": [0.27243348956108093], "tokens": [["\u2581Bob", "\u2581H", "os", "kins"]], "entropy": [[4.64453125, 1.3955078125, 0.031463623046875, 0.0078582763671875]], "avg_entropy": [1.5198402404785156], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Roy Export Company Establishment", "score_of_biggest_cluster": 0.27243348956108093, "score_of_first_answer": 0.27243348956108093, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.008209228515625, "avg_score": 0.27243348956108093, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40391144565562276, "std_of_first_answer": 0.40391144565562276, "avg_entropy_of_first_answer": 1.5198402404785156, "lex_sim": 1.0}
{"question": "\"What city is known to its inhabitants as \"\"auld reekie\"\"?\"", "answer": ["Edinburgh Chapter 19: In Which We Begin Our Adventure"], "scores": [[0.9658203125, 0.0, 0.3427734375, 0.3115234375, 0.011871337890625, 0.13427734375, 0.01110076904296875, 0.0189361572265625, 0.0299072265625, 0.015655517578125, 0.1573486328125, 0.0187225341796875, 0.96875]], "normalized_score": [0.0], "tokens": [["\u2581Edinburgh", "\u2581Chapter", "\u2581", "1", "9", ":", "\u2581In", "\u2581Which", "\u2581We", "\u2581Begin", "\u2581Our", "\u2581Advent", "ure"]], "entropy": [[0.2174072265625, 1.1298828125, 3.76171875, 2.017578125, 3.322265625, 2.82421875, 4.08203125, 2.791015625, 4.1953125, 4.375, 3.677734375, 4.66015625, 0.14990234375]], "avg_entropy": [2.8618633563701925], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dun Eideann", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.96875, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.33334562710581794, "std_of_first_answer": 0.33334562710581794, "avg_entropy_of_first_answer": 2.8618633563701925, "lex_sim": 1.0}
{"question": "Which American golfer, winner of three majors, was tragically killed in a plane crash on October 25th 1999, at the time of his death he was the reigning US Open Champion?", "answer": ["Payne Stewart"], "scores": [[0.9599609375, 0.99951171875, 0.994140625]], "normalized_score": [0.9843807220458984], "tokens": [["\u2581Pay", "ne", "\u2581Stewart"]], "entropy": [[0.318603515625, 0.0046234130859375, 0.041656494140625]], "avg_entropy": [0.1216278076171875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paine Stewart", "score_of_biggest_cluster": 0.9843807220458984, "score_of_first_answer": 0.9843807220458984, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.9599609375, "avg_score": 0.9843807220458984, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.017516227343537118, "std_of_first_answer": 0.017516227343537118, "avg_entropy_of_first_answer": 0.1216278076171875, "lex_sim": 1.0}
{"question": "The Japanese musical instrument the shamisen (or samisen or sangen) has traditionally how many strings (the answer, in Japanese and the earlier Chinese, is in the name)?", "answer": ["3 ###### QUESTION 2 ######"], "scores": [[0.052734375, 0.95751953125, 0.0, 0.224853515625, 1.2934207916259766e-05, 0.005298614501953125, 0.99267578125, 0.974609375, 0.01898193359375, 0.151123046875, 0.00021076202392578125, 0.98291015625]], "normalized_score": [0.0], "tokens": [["\u2581", "3", "\u2581#####", "#", "\u2581Q", "UE", "ST", "ION", "\u2581", "2", "\u2581#####", "#"]], "entropy": [[1.6748046875, 0.257568359375, 2.611328125, 2.484375, 2.349609375, 1.6357421875, 0.07391357421875, 0.1610107421875, 3.64453125, 2.125, 2.759765625, 0.1358642578125]], "avg_entropy": [1.6594594319661458], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "3", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.99267578125, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.43874651672910076, "std_of_first_answer": 0.43874651672910076, "avg_entropy_of_first_answer": 1.6594594319661458, "lex_sim": 1.0}
{"question": "'Wilson, the Wonder Athlete' who was still breaking world records at the age of 128, first appeared in 1933 in which boy's comic?", "answer": ["The Adventures of Dan Leno (comic)"], "scores": [[0.63232421875, 0.047149658203125, 0.75439453125, 0.98046875, 0.053863525390625, 0.383056640625, 0.994140625, 0.0139007568359375, 0.11138916015625, 0.95947265625, 0.68115234375]], "normalized_score": [0.26527246832847595], "tokens": [["\u2581The", "\u2581Advent", "ures", "\u2581of", "\u2581Dan", "\u2581L", "eno", "\u2581(", "com", "ic", ")"]], "entropy": [[1.9853515625, 2.849609375, 0.6845703125, 0.11395263671875, 2.77734375, 0.80078125, 0.05908203125, 0.77685546875, 3.83984375, 0.1749267578125, 1.021484375]], "avg_entropy": [1.3712546608664773], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tottenham Hotspurs F.C.", "score_of_biggest_cluster": 0.26527246832847595, "score_of_first_answer": 0.26527246832847595, "max_score_of_first_answer": 0.994140625, "min_score_of_first_answer": 0.0139007568359375, "avg_score": 0.26527246832847595, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38199882523183815, "std_of_first_answer": 0.38199882523183815, "avg_entropy_of_first_answer": 1.3712546608664773, "lex_sim": 1.0}
{"question": "Where did the Shinning Path terrorists operate?", "answer": ["Peru"], "scores": [[0.8720703125]], "normalized_score": [0.8720703125], "tokens": [["\u2581Peru"]], "entropy": [[0.72412109375]], "avg_entropy": [0.72412109375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lower Peru", "score_of_biggest_cluster": 0.8720703125, "score_of_first_answer": 0.8720703125, "max_score_of_first_answer": 0.8720703125, "min_score_of_first_answer": 0.8720703125, "avg_score": 0.8720703125, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.72412109375, "lex_sim": 1.0}
{"question": "What is the name of the commercial airport which opened in 2005 on the site of the former RAF Finningley?", "answer": ["Doncaster Sheffield"], "scores": [[0.9716796875, 0.99365234375, 0.99951171875, 0.94873046875, 1.0, 0.98193359375]], "normalized_score": [0.9824151992797852], "tokens": [["\u2581Don", "c", "aster", "\u2581She", "ff", "ield"]], "entropy": [[0.22607421875, 0.058258056640625, 0.00630950927734375, 0.30322265625, 0.0009050369262695312, 0.10009765625]], "avg_entropy": [0.11581118901570638], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Robin Hood Doncaster Sheffield Airport", "score_of_biggest_cluster": 0.9824151992797852, "score_of_first_answer": 0.9824151992797852, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.94873046875, "avg_score": 0.9824151992797852, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.018155630836865097, "std_of_first_answer": 0.018155630836865097, "avg_entropy_of_first_answer": 0.11581118901570638, "lex_sim": 1.0}
{"question": "What type of animal is a reedbuck?", "answer": ["Antelope"], "scores": [[0.50927734375, 0.96728515625, 0.9716796875]], "normalized_score": [0.7822471857070923], "tokens": [["\u2581Ant", "el", "ope"]], "entropy": [[1.8671875, 0.2166748046875, 0.140869140625]], "avg_entropy": [0.7415771484375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Botswanan Antelopes", "score_of_biggest_cluster": 0.7822471857070923, "score_of_first_answer": 0.7822471857070923, "max_score_of_first_answer": 0.9716796875, "min_score_of_first_answer": 0.50927734375, "avg_score": 0.7822471857070923, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.21695017239537223, "std_of_first_answer": 0.21695017239537223, "avg_entropy_of_first_answer": 0.7415771484375, "lex_sim": 1.0}
{"question": "What is the capital of Uzbekistan?", "answer": ["Tashkent"], "scores": [[0.96923828125, 0.99560546875, 0.99951171875, 0.9990234375]], "normalized_score": [0.990764319896698], "tokens": [["\u2581T", "ash", "k", "ent"]], "entropy": [[0.313720703125, 0.037689208984375, 0.006500244140625, 0.008056640625]], "avg_entropy": [0.09149169921875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Toshkent", "score_of_biggest_cluster": 0.990764319896698, "score_of_first_answer": 0.990764319896698, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.96923828125, "avg_score": 0.990764319896698, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0125649434081024, "std_of_first_answer": 0.0125649434081024, "avg_entropy_of_first_answer": 0.09149169921875, "lex_sim": 1.0}
{"question": "In 1657, who turned down the opportunity to become King of England?", "answer": ["George Monck #026 \u2013 The Best Of The Rest"], "scores": [[0.011444091796875, 0.9443359375, 0.99951171875, 4.76837158203125e-07, 0.009765625, 0.0811767578125, 0.3447265625, 1.4007091522216797e-05, 0.01174163818359375, 0.0003902912139892578, 0.03106689453125, 0.0548095703125, 0.0131072998046875]], "normalized_score": [0.008841138333082199], "tokens": [["\u2581George", "\u2581Mon", "ck", "\u2581#", "0", "2", "6", "\u2581\u2013", "\u2581The", "\u2581Best", "\u2581Of", "\u2581The", "\u2581Rest"]], "entropy": [[2.390625, 0.415771484375, 0.004795074462890625, 0.92822265625, 4.828125, 2.791015625, 2.369140625, 2.93359375, 4.7890625, 5.5390625, 3.298828125, 4.79296875, 5.09375]], "avg_entropy": [3.090381622314453], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cromwell", "score_of_biggest_cluster": 0.008841138333082199, "score_of_first_answer": 0.008841138333082199, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 4.76837158203125e-07, "avg_score": 0.008841138333082199, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3440876179602792, "std_of_first_answer": 0.3440876179602792, "avg_entropy_of_first_answer": 3.090381622314453, "lex_sim": 1.0}
{"question": "Poopdeck Pappy is which fictional character\u2019s father?", "answer": ["Mr. Howell"], "scores": [[0.234130859375, 0.9599609375, 0.2386474609375, 0.98828125]], "normalized_score": [0.4798299968242645], "tokens": [["\u2581Mr", ".", "\u2581How", "ell"]], "entropy": [[3.337890625, 0.292724609375, 2.58203125, 0.07330322265625]], "avg_entropy": [1.5714874267578125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Popeye Cigarettes", "score_of_biggest_cluster": 0.4798299968242645, "score_of_first_answer": 0.4798299968242645, "max_score_of_first_answer": 0.98828125, "min_score_of_first_answer": 0.234130859375, "avg_score": 0.4798299968242645, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36900529323400605, "std_of_first_answer": 0.36900529323400605, "avg_entropy_of_first_answer": 1.5714874267578125, "lex_sim": 1.0}
{"question": "What was 'Bifrost', the bridge that led to 'Asgard' in Scandinavian mythology?", "answer": ["Bifrost, the rainbow #Egyptian authorities #Egyptian-American #US citizen"], "scores": [[0.203369140625, 0.99853515625, 0.96337890625, 0.1458740234375, 0.57275390625, 0.69580078125, 1.0, 0.0, 0.000579833984375, 0.0007834434509277344, 0.61865234375, 3.4749507904052734e-05, 0.03485107421875, 0.0122528076171875, 0.75830078125, 0.280029296875, 0.00904083251953125, 0.0843505859375, 0.197509765625, 0.0088348388671875, 0.052703857421875, 1.0]], "normalized_score": [0.0], "tokens": [["\u2581B", "if", "rost", ",", "\u2581the", "\u2581rain", "bow", "\u2581#", "E", "gypt", "ian", "\u2581authorities", "\u2581#", "E", "gypt", "ian", "-", "American", "\u2581#", "US", "\u2581citiz", "en"]], "entropy": [[2.51953125, 0.01554107666015625, 0.1683349609375, 1.580078125, 1.5126953125, 1.0009765625, 0.00019478797912597656, 1.1357421875, 4.375, 4.26953125, 1.775390625, 3.158203125, 3.87109375, 6.12109375, 1.6962890625, 2.712890625, 4.55859375, 4.2109375, 4.0078125, 5.3828125, 3.919921875, 0.0026569366455078125]], "avg_entropy": [2.636150977828286], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Colors of rainbow", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38346914102501156, "std_of_first_answer": 0.38346914102501156, "avg_entropy_of_first_answer": 2.636150977828286, "lex_sim": 1.0}
{"question": "John Sholto Douglas sponsored the rules for which sport in the 19th century?", "answer": ["Lawn Bowls Rules"], "scores": [[0.448974609375, 0.9990234375, 0.2449951171875, 0.429443359375, 0.0017156600952148438, 0.97802734375]], "normalized_score": [0.20722420513629913], "tokens": [["\u2581La", "wn", "\u2581Bow", "ls", "\u2581R", "ules"]], "entropy": [[2.435546875, 0.010467529296875, 1.7197265625, 0.6982421875, 0.69140625, 0.1685791015625]], "avg_entropy": [0.9539947509765625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Prize fight", "score_of_biggest_cluster": 0.20722420513629913, "score_of_first_answer": 0.20722420513629913, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.0017156600952148438, "avg_score": 0.20722420513629913, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3643924228367439, "std_of_first_answer": 0.3643924228367439, "avg_entropy_of_first_answer": 0.9539947509765625, "lex_sim": 1.0}
{"question": "\u201cPoor old Johnny Ray\u201d is the opening line of which number one hit?", "answer": ["Um, um, um, um, um #MHYALit: Mental Health in The Girl with All the Gifts by M.R."], "scores": [[0.0030307769775390625, 0.3310546875, 0.010467529296875, 0.85693359375, 0.93359375, 0.8056640625, 0.91748046875, 0.86083984375, 0.90869140625, 6.9141387939453125e-06, 2.193450927734375e-05, 0.00040841102600097656, 0.02349853515625, 0.00279998779296875, 0.00011861324310302734, 0.0008950233459472656, 0.005126953125, 0.98046875, 0.89208984375, 0.6728515625, 2.2411346435546875e-05, 0.00040912628173828125, 0.2193603515625, 0.0689697265625, 0.9931640625, 0.99951171875, 0.99951171875, 0.1619873046875, 0.9130859375, 0.98583984375, 0.76904296875, 0.99365234375, 0.990234375, 0.99658203125]], "normalized_score": [0.05429640784859657], "tokens": [["\u2581Um", ",", "\u2581um", ",", "\u2581um", ",", "\u2581um", ",", "\u2581um", "\u2581#", "M", "H", "Y", "AL", "it", ":", "\u2581M", "ental", "\u2581Health", "\u2581in", "\u2581The", "\u2581Girl", "\u2581with", "\u2581All", "\u2581the", "\u2581G", "ifts", "\u2581by", "\u2581M", ".", "R", ".", "\u2581Care", "y"]], "entropy": [[1.580078125, 2.828125, 4.1796875, 0.79638671875, 0.442626953125, 0.81396484375, 0.59130859375, 0.7099609375, 0.7021484375, 1.9140625, 0.471435546875, 2.623046875, 3.49609375, 3.66796875, 2.267578125, 3.40234375, 4.99609375, 0.1875, 0.51123046875, 1.6650390625, 0.337646484375, 2.373046875, 1.5146484375, 0.48681640625, 0.045562744140625, 0.004638671875, 0.002735137939453125, 1.9951171875, 0.5625, 0.1009521484375, 0.75634765625, 0.050933837890625, 0.07177734375, 0.0286865234375]], "avg_entropy": [1.3581790924072266], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "COME ON EILEEN", "score_of_biggest_cluster": 0.05429640784859657, "score_of_first_answer": 0.05429640784859657, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 6.9141387939453125e-06, "avg_score": 0.05429640784859657, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.440767258243101, "std_of_first_answer": 0.440767258243101, "avg_entropy_of_first_answer": 1.3581790924072266, "lex_sim": 1.0}
{"question": "How many scottish football league teams names end in United?", "answer": ["One"], "scores": [[0.03387451171875]], "normalized_score": [0.0338745079934597], "tokens": [["\u2581One"]], "entropy": [[3.83203125]], "avg_entropy": [3.83203125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "3", "score_of_biggest_cluster": 0.0338745079934597, "score_of_first_answer": 0.0338745079934597, "max_score_of_first_answer": 0.03387451171875, "min_score_of_first_answer": 0.03387451171875, "avg_score": 0.0338745079934597, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.83203125, "lex_sim": 1.0}
{"question": "In which US town or city were the 1932 Winter Olympics held?", "answer": ["Lake Placid"], "scores": [[0.9716796875, 0.9951171875, 1.0, 0.99951171875]], "normalized_score": [0.9915081858634949], "tokens": [["\u2581Lake", "\u2581P", "lac", "id"]], "entropy": [[0.2095947265625, 0.050323486328125, 2.6702880859375e-05, 0.00540924072265625]], "avg_entropy": [0.06633853912353516], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lake Placid (disambiguation)", "score_of_biggest_cluster": 0.9915081858634949, "score_of_first_answer": 0.9915081858634949, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.9716796875, "avg_score": 0.9915081858634949, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01164412580243016, "std_of_first_answer": 0.01164412580243016, "avg_entropy_of_first_answer": 0.06633853912353516, "lex_sim": 1.0}
{"question": "Which country is separated into two parts by the town of Limbang of Malaysia?", "answer": ["Brunei"], "scores": [[0.8671875, 1.0, 0.9990234375]], "normalized_score": [0.9532999396324158], "tokens": [["\u2581Br", "une", "i"]], "entropy": [[0.8203125, 0.0009546279907226562, 0.007354736328125]], "avg_entropy": [0.27620728810628253], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Abode of Peace", "score_of_biggest_cluster": 0.9532999396324158, "score_of_first_answer": 0.9532999396324158, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.8671875, "avg_score": 0.9532999396324158, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06237950896378849, "std_of_first_answer": 0.06237950896378849, "avg_entropy_of_first_answer": 0.27620728810628253, "lex_sim": 1.0}
{"question": "Which jazz saxophonist was nicknamed Bird", "answer": ["John William Coltrane #BlogTour #BookReview The Missing Girls by Carol Wyer @carolewyer @Bookouture"], "scores": [[0.0015020370483398438, 0.0943603515625, 0.9638671875, 1.0, 1.0, 3.0994415283203125e-06, 0.0230255126953125, 8.177757263183594e-05, 0.0077667236328125, 0.0191192626953125, 0.315673828125, 0.143310546875, 0.41552734375, 0.97509765625, 0.03125, 0.00185394287109375, 0.99755859375, 0.005809783935546875, 0.96240234375, 0.2470703125, 0.8310546875, 0.90869140625, 0.5146484375, 0.83447265625, 0.9501953125, 0.99853515625, 0.99951171875, 0.818359375, 0.42529296875, 0.9111328125, 0.9990234375]], "normalized_score": [0.11799203604459763], "tokens": [["\u2581John", "\u2581William", "\u2581Col", "tr", "ane", "\u2581#", "B", "log", "T", "our", "\u2581#", "Book", "Re", "view", "\u2581The", "\u2581Miss", "ing", "\u2581Girls", "\u2581by", "\u2581Carol", "\u2581W", "yer", "\u2581@", "car", "ole", "w", "yer", "\u2581@", "Book", "out", "ure"]], "entropy": [[0.345947265625, 3.203125, 0.2215576171875, 0.002124786376953125, 0.00162506103515625, 1.1103515625, 4.91015625, 3.912109375, 5.17578125, 2.10546875, 3.5703125, 4.8671875, 2.46875, 0.1541748046875, 2.390625, 6.1484375, 0.0262451171875, 2.9296875, 0.259521484375, 2.0078125, 0.98291015625, 0.67822265625, 1.4189453125, 0.78955078125, 0.202392578125, 0.01373291015625, 0.0030727386474609375, 0.724609375, 1.0703125, 0.340576171875, 0.01174163818359375]], "avg_entropy": [1.6789376658778037], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charlie Parker", "score_of_biggest_cluster": 0.11799203604459763, "score_of_first_answer": 0.11799203604459763, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 3.0994415283203125e-06, "avg_score": 0.11799203604459763, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42309169981091993, "std_of_first_answer": 0.42309169981091993, "avg_entropy_of_first_answer": 1.6789376658778037, "lex_sim": 1.0}
{"question": "The song 'Bewitched, Bothered And Bewildered' came from which musical?", "answer": ["Pal Joey #NewRelease by C.H."], "scores": [[0.76708984375, 0.99658203125, 1.0, 4.76837158203125e-07, 5.441904067993164e-05, 1.615285873413086e-05, 0.0016126632690429688, 0.00040459632873535156, 0.265869140625, 0.00905609130859375, 0.62744140625, 0.0002307891845703125, 0.99951171875, 0.01398468017578125, 0.0816650390625, 0.011444091796875, 0.08721923828125, 0.29150390625, 0.99951171875]], "normalized_score": [0.015069345943629742], "tokens": [["\u2581Pal", "\u2581Jo", "ey", "\u2581#", "New", "Release", "\u2581by", "\u2581C", ".", "H", ".", "\u2581Arm", "strong", "\u2581-", "\u2581The", "\u2581Edge", "\u2581of", "\u2581Mad", "ness"]], "entropy": [[1.30859375, 0.03155517578125, 0.001277923583984375, 0.74365234375, 3.826171875, 2.947265625, 2.671875, 2.50390625, 4.64453125, 5.421875, 1.7822265625, 5.08984375, 0.0058746337890625, 2.240234375, 5.69140625, 4.90625, 3.064453125, 3.5546875, 0.00313568115234375]], "avg_entropy": [2.654674530029297], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pal Joey (disambiguation)", "score_of_biggest_cluster": 0.015069345943629742, "score_of_first_answer": 0.015069345943629742, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 4.76837158203125e-07, "avg_score": 0.015069345943629742, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40723409777665254, "std_of_first_answer": 0.40723409777665254, "avg_entropy_of_first_answer": 2.654674530029297, "lex_sim": 1.0}
{"question": "What are the French speaking citizens of Belgium called?", "answer": ["Fran\u00e7ais de Belgique"], "scores": [[0.0019016265869140625, 0.281982421875, 0.453369140625, 0.970703125]], "normalized_score": [0.12394281476736069], "tokens": [["\u2581Fran", "\u00e7ais", "\u2581de", "\u2581Belgique"]], "entropy": [[1.8232421875, 1.9052734375, 1.9912109375, 0.188232421875]], "avg_entropy": [1.47698974609375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wallonian", "score_of_biggest_cluster": 0.12394281476736069, "score_of_first_answer": 0.12394281476736069, "max_score_of_first_answer": 0.970703125, "min_score_of_first_answer": 0.0019016265869140625, "avg_score": 0.12394281476736069, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35286227634823897, "std_of_first_answer": 0.35286227634823897, "avg_entropy_of_first_answer": 1.47698974609375, "lex_sim": 1.0}
{"question": "Which river flows into the North Sea at Hamburg?", "answer": ["Elbe"], "scores": [[0.7060546875, 0.984375]], "normalized_score": [0.8336801528930664], "tokens": [["\u2581El", "be"]], "entropy": [[0.8837890625, 0.08428955078125]], "avg_entropy": [0.484039306640625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Elbe", "score_of_biggest_cluster": 0.8336801528930664, "score_of_first_answer": 0.8336801528930664, "max_score_of_first_answer": 0.984375, "min_score_of_first_answer": 0.7060546875, "avg_score": 0.8336801528930664, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.13916015625, "std_of_first_answer": 0.13916015625, "avg_entropy_of_first_answer": 0.484039306640625, "lex_sim": 1.0}
{"question": "At which school was Billy Bunter a pupil", "answer": ["Bunter is best known as a character in the books by Francis #WheresRey?"], "scores": [[0.006683349609375, 0.69140625, 0.158203125, 0.0011806488037109375, 0.9794921875, 0.921875, 0.490966796875, 0.685546875, 0.93701171875, 0.728515625, 0.01129150390625, 0.11138916015625, 0.015869140625, 6.496906280517578e-06, 0.00074005126953125, 0.0001519918441772461, 0.0006799697875976562, 0.001514434814453125, 0.0003571510314941406, 0.1011962890625, 0.0018463134765625, 0.00033974647521972656, 0.00017058849334716797, 7.569789886474609e-06, 0.3154296875, 0.322021484375, 0.03912353515625, 0.52197265625, 0.958984375, 0.07586669921875, 0.5205078125, 0.0595703125, 0.9853515625, 0.99951171875, 0.99951171875, 0.4208984375, 0.99169921875]], "normalized_score": [0.03298994526267052], "tokens": [["\u2581B", "unter", "\u2581is", "\u2581best", "\u2581known", "\u2581as", "\u2581a", "\u2581character", "\u2581in", "\u2581the", "\u2581books", "\u2581by", "\u2581Francis", "\u2581#", "W", "her", "es", "R", "ey", "?", "\u2581And", "\u2581other", "\u2581questions", "\u2581raised", "\u2581by", "\u2581the", "\u2581new", "\u2581Star", "\u2581Wars", ":", "\u2581The", "\u2581Force", "\u2581Aw", "ak", "ens", "\u2581tra", "iler"]], "entropy": [[1.7666015625, 1.9873046875, 1.2177734375, 0.06103515625, 0.12548828125, 0.3134765625, 0.712890625, 0.6650390625, 0.25732421875, 0.64501953125, 2.029296875, 1.033203125, 3.16015625, 3.24609375, 2.83984375, 3.99609375, 1.01953125, 4.4375, 4.375, 3.462890625, 4.91796875, 4.87109375, 3.99609375, 4.21484375, 2.826171875, 4.109375, 4.5859375, 2.255859375, 0.302734375, 1.9306640625, 1.1181640625, 1.005859375, 0.1304931640625, 0.0059814453125, 0.0055084228515625, 2.37109375, 0.056365966796875]], "avg_entropy": [2.0555613749736064], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "GREYFRIARS", "score_of_biggest_cluster": 0.03298994526267052, "score_of_first_answer": 0.03298994526267052, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 6.496906280517578e-06, "avg_score": 0.03298994526267052, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3909163758594748, "std_of_first_answer": 0.3909163758594748, "avg_entropy_of_first_answer": 2.0555613749736064, "lex_sim": 1.0}
{"question": "What is the name for a female bear?", "answer": ["Sow #WeRise #WomensMarch2018"], "scores": [[0.76416015625, 0.986328125, 1.7881393432617188e-07, 0.0004940032958984375, 0.0019054412841796875, 0.2142333984375, 0.158935546875, 0.032379150390625, 0.207763671875, 0.9970703125, 0.470703125, 0.97900390625, 0.065185546875, 0.9990234375, 0.04876708984375, 0.1651611328125]], "normalized_score": [0.05499596148729324], "tokens": [["\u2581S", "ow", "\u2581#", "We", "R", "ise", "\u2581#", "W", "om", "ens", "M", "arch", "2", "0", "1", "8"]], "entropy": [[0.947265625, 0.07440185546875, 1.009765625, 5.6484375, 4.375, 3.349609375, 2.3671875, 4.7109375, 2.548828125, 0.0250396728515625, 1.783203125, 0.11151123046875, 1.5439453125, 0.0083770751953125, 0.265869140625, 0.76171875]], "avg_entropy": [1.845693588256836], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "SOW (disambiguation)", "score_of_biggest_cluster": 0.05499596148729324, "score_of_first_answer": 0.05499596148729324, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 1.7881393432617188e-07, "avg_score": 0.05499596148729324, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3999544169887157, "std_of_first_answer": 0.3999544169887157, "avg_entropy_of_first_answer": 1.845693588256836, "lex_sim": 1.0}
{"question": "The male of which species of white whale develops a long twisted tusk that projects through its upper lip ?", "answer": ["Narwhal #14 in Best Things To Do in Houston"], "scores": [[0.37939453125, 0.98876953125, 0.9912109375, 1.5497207641601562e-06, 0.0272674560546875, 0.0198516845703125, 0.0004241466522216797, 0.0002536773681640625, 0.00664520263671875, 0.0035572052001953125, 0.348876953125, 0.44775390625, 0.00035762786865234375]], "normalized_score": [0.011213441379368305], "tokens": [["\u2581Nar", "wh", "al", "\u2581#", "1", "4", "\u2581in", "\u2581Best", "\u2581Things", "\u2581To", "\u2581Do", "\u2581in", "\u2581Houston"]], "entropy": [[2.6328125, 0.0885009765625, 0.061309814453125, 0.85791015625, 5.03125, 3.548828125, 3.076171875, 5.328125, 4.72265625, 1.90625, 2.6328125, 2.779296875, 4.5]], "avg_entropy": [2.858917236328125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Narwahl", "score_of_biggest_cluster": 0.011213441379368305, "score_of_first_answer": 0.011213441379368305, "max_score_of_first_answer": 0.9912109375, "min_score_of_first_answer": 1.5497207641601562e-06, "avg_score": 0.011213441379368305, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3543920153441109, "std_of_first_answer": 0.3543920153441109, "avg_entropy_of_first_answer": 2.858917236328125, "lex_sim": 1.0}
{"question": "What type of art does the English artist Banksy specialize in?", "answer": ["Street art #Toronto Real Estate Update: November/December 2017 #LuxuryHomes #Condos #Houses #Realty Note: \"Waste Lands\" refers to the"], "scores": [[0.708984375, 0.859375, 2.980232238769531e-07, 0.0013189315795898438, 0.0018167495727539062, 0.8349609375, 0.00012010335922241211, 0.92626953125, 0.99951171875, 4.5299530029296875e-05, 0.1915283203125, 0.00023949146270751953, 0.0017147064208984375, 0.88671875, 0.99853515625, 0.84765625, 0.998046875, 0.9990234375, 0.136474609375, 0.054901123046875, 0.115234375, 0.034454345703125, 0.9423828125, 0.99609375, 0.19287109375, 1.0, 0.9091796875, 0.16943359375, 0.41259765625, 0.87939453125, 0.78466796875, 0.9052734375, 0.62451171875, 0.004695892333984375, 0.1998291015625, 8.821487426757812e-06, 0.94482421875, 0.0009236335754394531, 0.0036029815673828125, 0.000850677490234375, 0.06768798828125, 0.0022296905517578125, 0.7373046875, 0.08929443359375, 0.99462890625, 0.2071533203125]], "normalized_score": [0.05712563544511795], "tokens": [["\u2581Street", "\u2581art", "\u2581#", "T", "or", "onto", "\u2581Real", "\u2581Est", "ate", "\u2581Update", ":", "\u2581November", "/", "Dec", "ember", "\u2581", "2", "0", "1", "7", "\u2581#", "L", "ux", "ury", "Hom", "es", "\u2581#", "Cond", "os", "\u2581#", "H", "ouses", "\u2581#", "Re", "alty", "\u2581Note", ":", "\u2581\"", "W", "aste", "\u2581Land", "s", "\"", "\u2581refers", "\u2581to", "\u2581the"]], "entropy": [[1.2099609375, 0.59912109375, 0.84423828125, 5.56640625, 3.619140625, 1.125, 3.7890625, 0.44384765625, 0.0054931640625, 2.453125, 3.44921875, 4.2890625, 0.8916015625, 0.5419921875, 0.01284027099609375, 1.0009765625, 0.01434326171875, 0.01099395751953125, 0.406982421875, 1.0908203125, 3.0859375, 4.4140625, 0.35302734375, 0.0394287109375, 2.244140625, 0.0018100738525390625, 0.53369140625, 2.1484375, 0.7529296875, 0.5576171875, 1.1201171875, 0.34814453125, 1.4990234375, 2.9609375, 1.4365234375, 2.013671875, 0.383056640625, 2.7421875, 4.7265625, 2.630859375, 3.6796875, 0.341552734375, 1.3955078125, 2.119140625, 0.04888916015625, 3.52734375]], "avg_entropy": [1.6623590303503948], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Graffitti", "score_of_biggest_cluster": 0.05712563544511795, "score_of_first_answer": 0.05712563544511795, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.980232238769531e-07, "avg_score": 0.05712563544511795, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42071476201840924, "std_of_first_answer": 0.42071476201840924, "avg_entropy_of_first_answer": 1.6623590303503948, "lex_sim": 1.0}
{"question": "In 1969, a man was stabbed to death by a member of the Hell's Angels at a Rolling Stones concert. Where was the concert held?", "answer": ["Altamont Free Concert"], "scores": [[0.90625, 0.998046875, 0.9951171875, 0.035308837890625, 0.92578125, 0.99755859375]], "normalized_score": [0.5553932189941406], "tokens": [["\u2581Alt", "am", "ont", "\u2581Free", "\u2581Con", "cert"]], "entropy": [[0.5390625, 0.015869140625, 0.03533935546875, 1.107421875, 0.415771484375, 0.019775390625]], "avg_entropy": [0.3555399576822917], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Altamont", "score_of_biggest_cluster": 0.5553932189941406, "score_of_first_answer": 0.5553932189941406, "max_score_of_first_answer": 0.998046875, "min_score_of_first_answer": 0.035308837890625, "avg_score": 0.5553932189941406, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3482392276159241, "std_of_first_answer": 0.3482392276159241, "avg_entropy_of_first_answer": 0.3555399576822917, "lex_sim": 1.0}
{"question": "The fictional characters William Dobbin and Rawdon Crawley appear in which novel?", "answer": ["The Pickwick Papers"], "scores": [[0.2471923828125, 0.478271484375, 1.0, 0.9873046875, 1.0]], "normalized_score": [0.6507769227027893], "tokens": [["\u2581The", "\u2581Pick", "wick", "\u2581Pap", "ers"]], "entropy": [[1.064453125, 3.41015625, 0.0018787384033203125, 0.09466552734375, 0.00017952919006347656]], "avg_entropy": [0.9142666339874268], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Vanity fair (disambiguation)", "score_of_biggest_cluster": 0.6507769227027893, "score_of_first_answer": 0.6507769227027893, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.2471923828125, "avg_score": 0.6507769227027893, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31864970256097797, "std_of_first_answer": 0.31864970256097797, "avg_entropy_of_first_answer": 0.9142666339874268, "lex_sim": 1.0}
{"question": "English economist and physician Nicholas Barbon helped to pioneer which type of insurance in 1666?", "answer": ["Insurance"], "scores": [[0.0156402587890625, 0.98388671875, 0.99365234375]], "normalized_score": [0.24820347130298615], "tokens": [["\u2581In", "sur", "ance"]], "entropy": [[1.48046875, 0.1373291015625, 0.058868408203125]], "avg_entropy": [0.5588887532552084], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Buildings insurance", "score_of_biggest_cluster": 0.24820347130298615, "score_of_first_answer": 0.24820347130298615, "max_score_of_first_answer": 0.99365234375, "min_score_of_first_answer": 0.0156402587890625, "avg_score": 0.24820347130298615, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4587548622797932, "std_of_first_answer": 0.4587548622797932, "avg_entropy_of_first_answer": 0.5588887532552084, "lex_sim": 1.0}
{"question": "Snail porridge is a dish associated with which famous chef?", "answer": ["Heston Blumenthal"], "scores": [[0.49609375, 0.99609375, 0.99658203125, 0.97705078125, 0.998046875]], "normalized_score": [0.8635530471801758], "tokens": [["\u2581H", "eston", "\u2581Bl", "ument", "hal"]], "entropy": [[2.076171875, 0.033935546875, 0.031646728515625, 0.1290283203125, 0.0162200927734375]], "avg_entropy": [0.4574005126953125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Big Fat Duck Cookbook", "score_of_biggest_cluster": 0.8635530471801758, "score_of_first_answer": 0.8635530471801758, "max_score_of_first_answer": 0.998046875, "min_score_of_first_answer": 0.49609375, "avg_score": 0.8635530471801758, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1984899256381391, "std_of_first_answer": 0.1984899256381391, "avg_entropy_of_first_answer": 0.4574005126953125, "lex_sim": 1.0}
{"question": "\"El Gouna, known as the \"\"Venice of the Red Sea\"\", is a holiday resort in which country?\"", "answer": ["Egypt"], "scores": [[0.943359375]], "normalized_score": [0.943359375], "tokens": [["\u2581Egypt"]], "entropy": [[0.431640625]], "avg_entropy": [0.431640625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "EGY", "score_of_biggest_cluster": 0.943359375, "score_of_first_answer": 0.943359375, "max_score_of_first_answer": 0.943359375, "min_score_of_first_answer": 0.943359375, "avg_score": 0.943359375, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.431640625, "lex_sim": 1.0}
{"question": "Which island of just over 30 square miles lies between North Uist and South Uist?", "answer": ["Benbecula"], "scores": [[0.11993408203125, 1.0, 1.0]], "normalized_score": [0.49315211176872253], "tokens": [["\u2581Ben", "bec", "ula"]], "entropy": [[1.583984375, 0.0024318695068359375, 0.002162933349609375]], "avg_entropy": [0.5295263926188151], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Benbecula", "score_of_biggest_cluster": 0.49315211176872253, "score_of_first_answer": 0.49315211176872253, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.11993408203125, "avg_score": 0.49315211176872253, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.414867052324578, "std_of_first_answer": 0.414867052324578, "avg_entropy_of_first_answer": 0.5295263926188151, "lex_sim": 1.0}
{"question": "How many North American railway metro systems are busier than Mexico City's metro?", "answer": ["4"], "scores": [[0.04095458984375, 0.057647705078125]], "normalized_score": [0.04858947917819023], "tokens": [["\u2581", "4"]], "entropy": [[4.03125, 1.9150390625]], "avg_entropy": [2.97314453125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1", "score_of_biggest_cluster": 0.04858947917819023, "score_of_first_answer": 0.04858947917819023, "max_score_of_first_answer": 0.057647705078125, "min_score_of_first_answer": 0.04095458984375, "avg_score": 0.04858947917819023, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0083465576171875, "std_of_first_answer": 0.0083465576171875, "avg_entropy_of_first_answer": 2.97314453125, "lex_sim": 1.0}
{"question": "\"How many adjectives are there in the sentence \"\"Jack ran down the path.\"\"?\"", "answer": ["0 #RealtyNewsRoundup: Govt Considering Realty Tax To Bolster State Finances"], "scores": [[0.0382080078125, 0.1566162109375, 2.086162567138672e-06, 7.200241088867188e-05, 9.387731552124023e-05, 0.00048661231994628906, 0.0010013580322265625, 0.5126953125, 0.89111328125, 0.12371826171875, 0.018768310546875, 0.45703125, 0.99169921875, 0.0008764266967773438, 0.9990234375, 0.032012939453125, 0.24462890625, 0.030242919921875, 0.418212890625, 0.0008339881896972656, 0.9921875, 0.03509521484375, 0.09881591796875, 0.99560546875]], "normalized_score": [0.025336839258670807], "tokens": [["\u2581", "0", "\u2581#", "Re", "alty", "News", "R", "ound", "up", ":", "\u2581G", "ov", "t", "\u2581Consider", "ing", "\u2581Real", "ty", "\u2581Tax", "\u2581To", "\u2581Bol", "ster", "\u2581State", "\u2581Fin", "ances"]], "entropy": [[2.658203125, 1.546875, 1.236328125, 4.34765625, 2.822265625, 4.01171875, 1.5341796875, 1.6455078125, 0.409423828125, 1.5107421875, 5.109375, 1.443359375, 0.0654296875, 4.37890625, 0.00986480712890625, 4.328125, 0.5703125, 3.251953125, 2.083984375, 1.83203125, 0.052215576171875, 2.2265625, 2.4765625, 0.035369873046875]], "avg_entropy": [2.0661230087280273], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "None", "score_of_biggest_cluster": 0.025336839258670807, "score_of_first_answer": 0.025336839258670807, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 2.086162567138672e-06, "avg_score": 0.025336839258670807, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3790242277121894, "std_of_first_answer": 0.3790242277121894, "avg_entropy_of_first_answer": 2.0661230087280273, "lex_sim": 1.0}
{"question": "Which British race track has corners named Copse, Maggots and Becketts?", "answer": ["Silverstone Circuit #065 The Power of Your Questions, WIth Guest Host David Lurwick"], "scores": [[0.98388671875, 0.99853515625, 0.4931640625, 1.0, 5.960464477539062e-07, 0.00562286376953125, 0.029632568359375, 0.0302734375, 0.00034117698669433594, 5.6803226470947266e-05, 0.309326171875, 0.0003800392150878906, 0.025054931640625, 0.29443359375, 0.024871826171875, 0.0004057884216308594, 0.02001953125, 0.01318359375, 0.0001901388168334961, 0.01220703125, 0.277587890625, 0.020233154296875, 0.002849578857421875, 0.0291900634765625, 0.0001404285430908203]], "normalized_score": [0.009997195564210415], "tokens": [["\u2581Silver", "stone", "\u2581Circ", "uit", "\u2581#", "0", "6", "5", "\u2581The", "\u2581Power", "\u2581of", "\u2581Your", "\u2581Question", "s", ",", "\u2581W", "I", "th", "\u2581Gu", "est", "\u2581Host", "\u2581David", "\u2581L", "ur", "wick"]], "entropy": [[0.140869140625, 0.01364898681640625, 1.2158203125, 0.0004642009735107422, 0.90771484375, 4.703125, 3.599609375, 4.18359375, 3.46484375, 3.76171875, 3.501953125, 4.76171875, 3.94921875, 2.56640625, 3.708984375, 4.80078125, 3.671875, 3.048828125, 5.14453125, 1.869140625, 4.0703125, 5.35546875, 2.400390625, 3.833984375, 0.630859375]], "avg_entropy": [3.0122344493865967], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Silverstone", "score_of_biggest_cluster": 0.009997195564210415, "score_of_first_answer": 0.009997195564210415, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.960464477539062e-07, "avg_score": 0.009997195564210415, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.32476668630176986, "std_of_first_answer": 0.32476668630176986, "avg_entropy_of_first_answer": 3.0122344493865967, "lex_sim": 1.0}
{"question": "Taphephobia is the abnormal fear of being what?", "answer": ["Buried alive #TheReel"], "scores": [[0.1409912109375, 0.99853515625, 0.92919921875, 1.7881393432617188e-07, 0.0006284713745117188, 0.0025691986083984375, 0.003170013427734375]], "normalized_score": [0.005314453039318323], "tokens": [["\u2581Bur", "ied", "\u2581alive", "\u2581#", "The", "Re", "el"]], "entropy": [[2.955078125, 0.01265716552734375, 0.35205078125, 0.8837890625, 6.0078125, 5.52734375, 3.33984375]], "avg_entropy": [2.725510733468192], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Texan funeral", "score_of_biggest_cluster": 0.005314453039318323, "score_of_first_answer": 0.005314453039318323, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 1.7881393432617188e-07, "avg_score": 0.005314453039318323, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4251444333274445, "std_of_first_answer": 0.4251444333274445, "avg_entropy_of_first_answer": 2.725510733468192, "lex_sim": 1.0}
{"question": "On TV who played the private eye Jim Rockford?", "answer": ["LARRY"], "scores": [[3.7729740142822266e-05, 0.00594329833984375, 0.9853515625]], "normalized_score": [0.0060455272905528545], "tokens": [["\u2581L", "AR", "RY"]], "entropy": [[0.2266845703125, 5.34765625, 0.1265869140625]], "avg_entropy": [1.9003092447916667], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "James Scott Bumgarner", "score_of_biggest_cluster": 0.0060455272905528545, "score_of_first_answer": 0.0060455272905528545, "max_score_of_first_answer": 0.9853515625, "min_score_of_first_answer": 3.7729740142822266e-05, "avg_score": 0.0060455272905528545, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4630957151693152, "std_of_first_answer": 0.4630957151693152, "avg_entropy_of_first_answer": 1.9003092447916667, "lex_sim": 1.0}
{"question": "Whose picture is on the US 20 dollar note?", "answer": ["ISO 4217:USD"], "scores": [[5.960464477539063e-08, 0.48046875, 0.53662109375, 0.99755859375, 0.99951171875, 0.99755859375, 0.0287017822265625, 0.033477783203125, 0.96044921875]], "normalized_score": [0.062284454703330994], "tokens": [["\u2581ISO", "\u2581", "4", "2", "1", "7", ":", "US", "D"]], "entropy": [[0.8154296875, 3.32421875, 1.5966796875, 0.02362060546875, 0.006015777587890625, 0.0228118896484375, 1.841796875, 2.828125, 0.2685546875]], "avg_entropy": [1.191916995578342], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Seventh President of the United States", "score_of_biggest_cluster": 0.062284454703330994, "score_of_first_answer": 0.062284454703330994, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.062284454703330994, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42379460314527245, "std_of_first_answer": 0.42379460314527245, "avg_entropy_of_first_answer": 1.191916995578342, "lex_sim": 1.0}
{"question": "Which organization launched the Firefox smartphone operating system in 2013, targeting the developing world?", "answer": ["Mozilla #Khashoggi\u2019s last words: \u201cI am suffocating \u2026 Take me to a hospital.\u201d"], "scores": [[0.87060546875, 0.9990234375, 1.7881393432617188e-07, 0.00011682510375976562, 1.0251998901367188e-05, 0.9599609375, 0.9990234375, 0.0008440017700195312, 0.96826171875, 0.01019287109375, 0.767578125, 0.05670166015625, 0.315185546875, 0.9443359375, 0.25390625, 0.9228515625, 1.0, 0.9990234375, 0.52783203125, 0.73388671875, 0.52587890625, 0.970703125, 0.58642578125, 0.982421875, 0.3291015625]], "normalized_score": [0.10212528705596924], "tokens": [["\u2581Moz", "illa", "\u2581#", "K", "hash", "og", "gi", "\u2019", "s", "\u2581last", "\u2581words", ":", "\u2581\u201c", "I", "\u2581am", "\u2581suff", "oc", "ating", "\u2581\u2026", "\u2581Take", "\u2581me", "\u2581to", "\u2581a", "\u2581hospital", ".\u201d"]], "entropy": [[0.71142578125, 0.00775146484375, 0.98095703125, 4.3515625, 3.76953125, 0.33056640625, 0.00769805908203125, 2.75, 0.2469482421875, 3.81640625, 1.392578125, 3.28125, 3.193359375, 0.476318359375, 1.08984375, 0.60888671875, 0.002696990966796875, 0.00733184814453125, 1.4873046875, 1.2353515625, 1.6337890625, 0.14111328125, 1.123046875, 0.10687255859375, 1.18359375]], "avg_entropy": [1.3574473571777343], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mozillian", "score_of_biggest_cluster": 0.10212528705596924, "score_of_first_answer": 0.10212528705596924, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.7881393432617188e-07, "avg_score": 0.10212528705596924, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3945911381680786, "std_of_first_answer": 0.3945911381680786, "avg_entropy_of_first_answer": 1.3574473571777343, "lex_sim": 1.0}
{"question": "When Sir Alex Ferguson retired in May 2013 after 26 years as manager for Manchester United Football Club who succeeded him ?", "answer": ["David Moyes #ThrowbackThursday \u2013 \u201cThe Devil\u2019s Rejects\u201d (2005)"], "scores": [[0.958984375, 0.98486328125, 0.998046875, 5.960464477539062e-07, 3.409385681152344e-05, 0.9912109375, 0.112060546875, 0.9990234375, 0.99658203125, 4.571676254272461e-05, 0.0003833770751953125, 0.0445556640625, 1.2993812561035156e-05, 0.65576171875, 0.1544189453125, 0.98974609375, 0.00016808509826660156, 0.9873046875, 0.97705078125, 0.8505859375, 0.419921875, 0.98095703125, 0.99951171875, 0.99951171875, 0.9990234375, 0.87890625]], "normalized_score": [0.06439892202615738], "tokens": [["\u2581David", "\u2581Mo", "yes", "\u2581#", "Throw", "back", "Th", "urs", "day", "\u2581\u2013", "\u2581\u201c", "The", "\u2581Dev", "il", "\u2019", "s", "\u2581Re", "ject", "s", "\u201d", "\u2581(", "2", "0", "0", "5", ")"]], "entropy": [[0.358642578125, 0.122314453125, 0.0186309814453125, 0.90576171875, 5.91015625, 0.056884765625, 2.51171875, 0.012786865234375, 0.0296783447265625, 2.01953125, 4.890625, 4.4765625, 3.783203125, 0.7548828125, 2.716796875, 0.093505859375, 1.69921875, 0.111083984375, 0.13720703125, 0.95068359375, 3.357421875, 0.1876220703125, 0.00612640380859375, 0.002696990966796875, 0.007465362548828125, 0.63720703125]], "avg_entropy": [1.3753236623910756], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "List of David Moyes thropies", "score_of_biggest_cluster": 0.06439892202615738, "score_of_first_answer": 0.06439892202615738, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 5.960464477539062e-07, "avg_score": 0.06439892202615738, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.440611106123802, "std_of_first_answer": 0.440611106123802, "avg_entropy_of_first_answer": 1.3753236623910756, "lex_sim": 1.0}
{"question": "In which town or city were most of the Bowie knives made", "answer": ["SWEETGUM"], "scores": [[0.00344085693359375, 0.0006937980651855469, 0.86474609375, 0.02581787109375, 0.08428955078125]], "normalized_score": [0.021404076367616653], "tokens": [["\u2581S", "WE", "ET", "G", "UM"]], "entropy": [[4.65625, 5.08984375, 0.8798828125, 3.666015625, 2.71484375]], "avg_entropy": [3.4013671875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "County Borough of Sheffield", "score_of_biggest_cluster": 0.021404076367616653, "score_of_first_answer": 0.021404076367616653, "max_score_of_first_answer": 0.86474609375, "min_score_of_first_answer": 0.0006937980651855469, "avg_score": 0.021404076367616653, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3358231232963377, "std_of_first_answer": 0.3358231232963377, "avg_entropy_of_first_answer": 3.4013671875, "lex_sim": 1.0}
{"question": "Who was said to have presented the baby Jesus with Frankincense?", "answer": ["Melchior"], "scores": [[0.2340087890625, 0.99853515625, 1.0]], "normalized_score": [0.6159306764602661], "tokens": [["\u2581Mel", "ch", "ior"]], "entropy": [[2.302734375, 0.01395416259765625, 0.001659393310546875]], "avg_entropy": [0.7727826436360677], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Balthazar", "score_of_biggest_cluster": 0.6159306764602661, "score_of_first_answer": 0.6159306764602661, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.2340087890625, "avg_score": 0.6159306764602661, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3607469484163881, "std_of_first_answer": 0.3607469484163881, "avg_entropy_of_first_answer": 0.7727826436360677, "lex_sim": 1.0}
{"question": "In WTF taekwondo competitions points are scored for a kick to the head or body, a turning kick to the head, and what else?", "answer": ["Punch"], "scores": [[0.2115478515625, 0.94140625]], "normalized_score": [0.4462650418281555], "tokens": [["\u2581P", "unch"]], "entropy": [[3.21484375, 0.426513671875]], "avg_entropy": [1.8206787109375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Punch to the body", "score_of_biggest_cluster": 0.4462650418281555, "score_of_first_answer": 0.4462650418281555, "max_score_of_first_answer": 0.94140625, "min_score_of_first_answer": 0.2115478515625, "avg_score": 0.4462650418281555, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.36492919921875, "std_of_first_answer": 0.36492919921875, "avg_entropy_of_first_answer": 1.8206787109375, "lex_sim": 1.0}
{"question": "A ewer is a large?", "answer": ["\u2267\uf8ff"], "scores": [[0.0004611015319824219, 0.0030651092529296875, 0.0005002021789550781, 0.031707763671875, 0.05224609375, 0.07379150390625, 0.8515625]], "normalized_score": [0.013299424201250076], "tokens": [["\u2581", "<0xE2>", "<0x89>", "<0xA7>", "<0xEF>", "<0xA3>", "<0xBF>"]], "entropy": [[3.142578125, 2.90234375, 2.099609375, 2.50390625, 1.0166015625, 1.876953125, 0.85498046875]], "avg_entropy": [2.0567103794642856], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Toby jugs", "score_of_biggest_cluster": 0.013299424201250076, "score_of_first_answer": 0.013299424201250076, "max_score_of_first_answer": 0.8515625, "min_score_of_first_answer": 0.0004611015319824219, "avg_score": 0.013299424201250076, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2897428504809174, "std_of_first_answer": 0.2897428504809174, "avg_entropy_of_first_answer": 2.0567103794642856, "lex_sim": 0.0}
{"question": "Name the year: Kate Middleton is born; Mail on Sunday first published; Laker Airways collapsed", "answer": ["1982 Tags: oral health oral hygiene children's dental health"], "scores": [[0.49853515625, 0.9990234375, 0.99462890625, 0.9990234375, 0.9990234375, 0.0, 0.89892578125, 9.953975677490234e-06, 0.00981903076171875, 0.0012273788452148438, 0.003925323486328125, 0.8349609375, 0.63232421875, 1.0, 0.99951171875, 0.00016880035400390625, 0.425048828125, 1.0, 0.465576171875, 0.837890625, 0.7080078125]], "normalized_score": [0.0], "tokens": [["\u2581", "1", "9", "8", "2", "\u2581Tags", ":", "\u2581or", "al", "\u2581health", "\u2581or", "al", "\u2581h", "yg", "iene", "\u2581children", "'", "s", "\u2581dent", "al", "\u2581health"]], "entropy": [[1.619140625, 0.01129150390625, 0.048004150390625, 0.00891876220703125, 0.00988006591796875, 1.13671875, 0.80419921875, 2.26171875, 4.66796875, 3.4921875, 2.189453125, 1.5205078125, 1.783203125, 0.0006489753723144531, 0.0036067962646484375, 2.84375, 2.794921875, 0.0021648406982421875, 1.63671875, 0.5498046875, 0.89501953125]], "avg_entropy": [1.3466584568931943], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and eighty-two", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.40962826117872264, "std_of_first_answer": 0.40962826117872264, "avg_entropy_of_first_answer": 1.3466584568931943, "lex_sim": 1.0}
{"question": "Who in 1928 piloted the first flight between Australia and New Zealand?", "answer": ["J. C. \"Bert\" Hinkler Tags: Barack Obama | Obama | Obamacare | Health | Care | Bill | Reid"], "scores": [[0.014678955078125, 0.615234375, 0.04779052734375, 0.9921875, 0.28369140625, 0.16845703125, 0.17919921875, 0.99853515625, 0.99951171875, 1.0, 1.0, 0.0, 0.91845703125, 2.562999725341797e-06, 0.0123748779296875, 0.626953125, 0.99853515625, 0.004077911376953125, 0.0012378692626953125, 0.9716796875, 0.71826171875, 0.031982421875, 0.02783203125, 0.72216796875, 0.998046875, 0.84326171875, 0.12066650390625, 0.0175323486328125, 0.423583984375, 0.90869140625, 0.0052642822265625, 0.90625, 0.0614013671875, 0.00391387939453125]], "normalized_score": [0.0], "tokens": [["\u2581J", ".", "\u2581C", ".", "\u2581\"", "B", "ert", "\"", "\u2581H", "ink", "ler", "\u2581Tags", ":", "\u2581Bar", "ack", "\u2581Ob", "ama", "\u2581|", "\u2581Ob", "ama", "\u2581|", "\u2581Ob", "am", "ac", "are", "\u2581|", "\u2581Health", "\u2581|", "\u2581Care", "\u2581|", "\u2581Bill", "\u2581|", "\u2581Re", "id"]], "entropy": [[3.14453125, 1.951171875, 3.154296875, 0.08355712890625, 2.44921875, 2.708984375, 2.919921875, 0.0121307373046875, 0.007610321044921875, 0.0020809173583984375, 0.0013294219970703125, 0.71728515625, 0.71337890625, 4.44140625, 2.224609375, 1.646484375, 0.0121612548828125, 2.55078125, 6.05859375, 0.1708984375, 1.912109375, 5.078125, 0.183349609375, 0.7333984375, 0.0209503173828125, 0.86181640625, 2.939453125, 1.31640625, 2.556640625, 0.479736328125, 3.427734375, 0.61376953125, 4.35546875, 1.24609375]], "avg_entropy": [1.785161298864028], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charles Edward Kingsford Smith", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4233871134410806, "std_of_first_answer": 0.4233871134410806, "avg_entropy_of_first_answer": 1.785161298864028, "lex_sim": 1.0}
{"question": "Biologically-speaking, what name is given to an organ that serves no evident purpose?", "answer": ["Vestigial organ"], "scores": [[0.90087890625, 1.0, 0.99755859375, 0.8740234375]], "normalized_score": [0.94141685962677], "tokens": [["\u2581V", "estig", "ial", "\u2581organ"]], "entropy": [[0.61083984375, 0.0022430419921875, 0.0182342529296875, 0.58642578125]], "avg_entropy": [0.30443572998046875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Vestigal Structures", "score_of_biggest_cluster": 0.94141685962677, "score_of_first_answer": 0.94141685962677, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.8740234375, "avg_score": 0.94141685962677, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05647463977119621, "std_of_first_answer": 0.05647463977119621, "avg_entropy_of_first_answer": 0.30443572998046875, "lex_sim": 1.0}
{"question": "What is Britain\u2019s bestselling breakfast cereal?", "answer": ["Squares"], "scores": [[0.002838134765625, 0.02685546875, 0.406005859375]], "normalized_score": [0.03139539808034897], "tokens": [["\u2581S", "qu", "ares"]], "entropy": [[3.30859375, 3.4453125, 1.947265625]], "avg_entropy": [2.900390625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Weetaflakes", "score_of_biggest_cluster": 0.03139539808034897, "score_of_first_answer": 0.03139539808034897, "max_score_of_first_answer": 0.406005859375, "min_score_of_first_answer": 0.002838134765625, "avg_score": 0.03139539808034897, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1846546521992723, "std_of_first_answer": 0.1846546521992723, "avg_entropy_of_first_answer": 2.900390625, "lex_sim": 1.0}
{"question": "Who did Simple Simon meet on his way to the fair?", "answer": ["The piper #91 \u2013 4th of July Parade in DUMBO"], "scores": [[0.736328125, 0.91552734375, 0.32080078125, 0.0, 0.01430511474609375, 0.01006317138671875, 7.152557373046875e-06, 0.0648193359375, 0.033233642578125, 0.01369476318359375, 0.072021484375, 0.0992431640625, 0.025054931640625, 0.9990234375, 0.0053253173828125, 0.0048828125, 0.00859832763671875, 0.99951171875]], "normalized_score": [0.0], "tokens": [["\u2581The", "\u2581pi", "per", "\u2581#", "9", "1", "\u2581\u2013", "\u2581", "4", "th", "\u2581of", "\u2581July", "\u2581Par", "ade", "\u2581in", "\u2581D", "UM", "BO"]], "entropy": [[1.2705078125, 0.360595703125, 0.8388671875, 0.712890625, 5.4921875, 2.400390625, 1.2373046875, 4.8671875, 2.92578125, 3.796875, 4.9453125, 2.75, 3.865234375, 0.01035308837890625, 3.625, 5.9140625, 3.091796875, 0.006076812744140625]], "avg_entropy": [2.6728013356526694], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pieman (disambiguation)", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36998287447280176, "std_of_first_answer": 0.36998287447280176, "avg_entropy_of_first_answer": 2.6728013356526694, "lex_sim": 1.0}
{"question": "What commonly used English Latin expression means literally 'so' or 'thus' or 'in this manner', and generally refers to a quoted error?", "answer": ["E pur si muove #1338 Dashing through the snow in a one-horse open sleigh"], "scores": [[0.375, 0.2418212890625, 0.86181640625, 0.96630859375, 0.99365234375, 5.960464477539063e-08, 0.005954742431640625, 0.03997802734375, 0.050048828125, 0.021942138671875, 2.3066997528076172e-05, 0.0004382133483886719, 0.007659912109375, 0.052764892578125, 0.97607421875, 0.88623046875, 0.042877197265625, 0.9833984375, 0.9990234375, 0.7294921875, 0.99951171875, 0.99951171875, 0.99169921875, 0.998046875, 1.0]], "normalized_score": [0.0765516459941864], "tokens": [["\u2581E", "\u2581pur", "\u2581si", "\u2581mu", "ove", "\u2581#", "1", "3", "3", "8", "\u2581D", "ash", "ing", "\u2581through", "\u2581the", "\u2581snow", "\u2581in", "\u2581a", "\u2581one", "-", "hor", "se", "\u2581open", "\u2581sle", "igh"]], "entropy": [[2.33984375, 4.16015625, 0.576171875, 0.233154296875, 0.04461669921875, 1.0732421875, 4.63671875, 2.84765625, 2.197265625, 1.65625, 3.205078125, 4.89453125, 3.75, 4.80078125, 0.230224609375, 0.8388671875, 3.060546875, 0.1690673828125, 0.01308441162109375, 0.60302734375, 0.006603240966796875, 0.007549285888671875, 0.06219482421875, 0.019989013671875, 0.0017910003662109375]], "avg_entropy": [1.657136459350586], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sic, recte", "score_of_biggest_cluster": 0.0765516459941864, "score_of_first_answer": 0.0765516459941864, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.0765516459941864, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45126695321965127, "std_of_first_answer": 0.45126695321965127, "avg_entropy_of_first_answer": 1.657136459350586, "lex_sim": 1.0}
{"question": "Near which Chinese city is the famous Terracotta Army to be found?", "answer": ["Xi'an"], "scores": [[0.89794921875, 0.93994140625, 0.986328125, 0.99609375]], "normalized_score": [0.9542639851570129], "tokens": [["\u2581X", "i", "'", "an"]], "entropy": [[0.82470703125, 0.25390625, 0.10308837890625, 0.03192138671875]], "avg_entropy": [0.30340576171875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Xian, China", "score_of_biggest_cluster": 0.9542639851570129, "score_of_first_answer": 0.9542639851570129, "max_score_of_first_answer": 0.99609375, "min_score_of_first_answer": 0.89794921875, "avg_score": 0.9542639851570129, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03921631094958613, "std_of_first_answer": 0.03921631094958613, "avg_entropy_of_first_answer": 0.30340576171875, "lex_sim": 1.0}
{"question": "Which film studio produced the 'Bugs Bunny' cartoons?", "answer": ["Warner Bros."], "scores": [[0.880859375, 0.98388671875, 1.0, 0.7783203125]], "normalized_score": [0.9062593579292297], "tokens": [["\u2581Warner", "\u2581B", "ros", "."]], "entropy": [[0.57958984375, 0.0975341796875, 0.0018463134765625, 0.685546875]], "avg_entropy": [0.3411293029785156], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Warner Brothers (and Sister)", "score_of_biggest_cluster": 0.9062593579292297, "score_of_first_answer": 0.9062593579292297, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.7783203125, "avg_score": 0.9062593579292297, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0890864925526134, "std_of_first_answer": 0.0890864925526134, "avg_entropy_of_first_answer": 0.3411293029785156, "lex_sim": 1.0}
{"question": "What is another name for the flower known as the snapdragon?", "answer": ["Frying pan"], "scores": [[0.002124786376953125, 0.035736083984375, 0.6640625, 0.497802734375]], "normalized_score": [0.07078187167644501], "tokens": [["\u2581F", "ry", "ing", "\u2581pan"]], "entropy": [[1.599609375, 3.876953125, 2.3984375, 1.7001953125]], "avg_entropy": [2.393798828125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Snap dragons", "score_of_biggest_cluster": 0.07078187167644501, "score_of_first_answer": 0.07078187167644501, "max_score_of_first_answer": 0.6640625, "min_score_of_first_answer": 0.002124786376953125, "avg_score": 0.07078187167644501, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28732928282578163, "std_of_first_answer": 0.28732928282578163, "avg_entropy_of_first_answer": 2.393798828125, "lex_sim": 1.0}
{"question": "Real Love in 1996 was whose last top 10 hit in the UK?", "answer": ["Bryan Adams"], "scores": [[0.0902099609375, 1.0, 0.89697265625]], "normalized_score": [0.43252503871917725], "tokens": [["\u2581Bry", "an", "\u2581Adams"]], "entropy": [[4.4765625, 0.0015287399291992188, 0.445068359375]], "avg_entropy": [1.6410531997680664], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fab 4", "score_of_biggest_cluster": 0.43252503871917725, "score_of_first_answer": 0.43252503871917725, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0902099609375, "avg_score": 0.43252503871917725, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40677574975318825, "std_of_first_answer": 0.40677574975318825, "avg_entropy_of_first_answer": 1.6410531997680664, "lex_sim": 1.0}
{"question": "Who was known as Dr Angelicus?", "answer": ["William Byrd (composer) #ThrowbackThursday Review: Delirious by Daniel Lumby"], "scores": [[0.61669921875, 0.97265625, 0.99951171875, 0.00818634033203125, 0.4326171875, 0.99951171875, 0.9921875, 1.0371208190917969e-05, 1.9431114196777344e-05, 0.99365234375, 0.14990234375, 1.0, 0.9990234375, 1.1920928955078125e-07, 0.187255859375, 0.00560760498046875, 0.0146026611328125, 0.40234375, 0.83056640625, 0.08306884765625, 0.0175323486328125, 0.004505157470703125, 7.456541061401367e-05, 0.84130859375]], "normalized_score": [0.03548388183116913], "tokens": [["\u2581William", "\u2581By", "rd", "\u2581(", "compos", "er", ")", "\u2581#", "Throw", "back", "Th", "urs", "day", "\u2581Review", ":", "\u2581D", "eli", "ri", "ous", "\u2581by", "\u2581Daniel", "\u2581L", "um", "by"]], "entropy": [[1.5498046875, 0.197265625, 0.00653839111328125, 0.7470703125, 1.9228515625, 0.00493621826171875, 0.061981201171875, 0.76708984375, 4.7265625, 0.046661376953125, 2.646484375, 0.002414703369140625, 0.0082550048828125, 1.599609375, 3.3671875, 5.3046875, 4.9765625, 2.193359375, 0.90185546875, 1.76953125, 3.7890625, 3.1796875, 0.456787109375, 0.89697265625]], "avg_entropy": [1.7134674390157063], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Thomas Aquinas", "score_of_biggest_cluster": 0.03548388183116913, "score_of_first_answer": 0.03548388183116913, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.1920928955078125e-07, "avg_score": 0.03548388183116913, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4314540533948738, "std_of_first_answer": 0.4314540533948738, "avg_entropy_of_first_answer": 1.7134674390157063, "lex_sim": 1.0}
{"question": "Which actress played Lillie Langtree in the 1970s TV drama series Lillie?", "answer": ["Louise J ##### The Tao of Pooh by Benjamin Hoff The Tao of Pooh is a book that teaches Taoist philosophy through the character of Winnie-the-Pooh."], "scores": [[0.043182373046875, 0.482177734375, 5.960464477539063e-08, 2.9802322387695312e-05, 0.0006246566772460938, 0.00020265579223632812, 0.06695556640625, 0.400146484375, 0.99560546875, 0.056121826171875, 0.96142578125, 0.99560546875, 0.00970458984375, 0.07696533203125, 0.9921875, 0.93505859375, 0.85986328125, 1.0, 0.24267578125, 0.9345703125, 0.7353515625, 0.42626953125, 0.033233642578125, 1.0, 0.2939453125, 0.99951171875, 0.433349609375, 0.426513671875, 0.9267578125, 0.990234375, 0.8603515625, 0.98583984375, 0.97900390625, 1.0, 0.99951171875, 0.77734375, 0.99560546875, 0.9970703125, 0.9931640625, 1.0, 0.90234375, 0.295166015625, 0.00710296630859375, 0.61767578125, 0.1500244140625, 0.99951171875, 0.921875, 0.81005859375]], "normalized_score": [0.19194984436035156], "tokens": [["\u2581Louise", "\u2581J", "\u2581#####", "\u2581The", "\u2581T", "ao", "\u2581of", "\u2581Po", "oh", "\u2581by", "\u2581Benjamin", "\u2581Hoff", "\u2581The", "\u2581T", "ao", "\u2581of", "\u2581Po", "oh", "\u2581is", "\u2581a", "\u2581book", "\u2581that", "\u2581teach", "es", "\u2581T", "ao", "ist", "\u2581philosophy", "\u2581through", "\u2581the", "\u2581character", "\u2581of", "\u2581W", "inn", "ie", "-", "the", "-", "Po", "oh", ".", "\u2581It", "\u2581has", "\u2581been", "\u2581pra", "ised", "\u2581for", "\u2581its"]], "entropy": [[4.515625, 2.654296875, 3.38671875, 1.9677734375, 4.38671875, 5.1953125, 1.583984375, 3.30078125, 0.03363037109375, 2.669921875, 0.290771484375, 0.045745849609375, 2.5390625, 5.18359375, 0.07684326171875, 0.3525390625, 1.0498046875, 0.0005192756652832031, 2.94921875, 0.458984375, 1.5458984375, 1.3251953125, 2.826171875, 0.0011348724365234375, 0.84521484375, 0.00487518310546875, 0.69970703125, 1.048828125, 0.348388671875, 0.07537841796875, 0.77197265625, 0.0987548828125, 0.1314697265625, 0.0022373199462890625, 0.0031490325927734375, 0.6005859375, 0.034393310546875, 0.0240631103515625, 0.047882080078125, 0.00203704833984375, 0.441162109375, 2.529296875, 2.068359375, 1.7734375, 2.587890625, 0.0042266845703125, 0.3388671875, 0.568359375]], "avg_entropy": [1.3206413288911183], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Francesca Annis", "score_of_biggest_cluster": 0.19194984436035156, "score_of_first_answer": 0.19194984436035156, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.19194984436035156, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39932267480100664, "std_of_first_answer": 0.39932267480100664, "avg_entropy_of_first_answer": 1.3206413288911183, "lex_sim": 1.0}
{"question": "A quadruped is an animal with how many feet?", "answer": ["Four feet/legs Tags: / Biology / Chemistry / Physics"], "scores": [[0.626953125, 0.24267578125, 0.0002856254577636719, 0.72607421875, 0.9990234375, 0.0, 0.94482421875, 8.928775787353516e-05, 1.7881393432617188e-07, 0.9853515625, 0.285400390625, 0.000514984130859375, 0.95849609375, 0.9091796875, 0.37451171875]], "normalized_score": [0.0], "tokens": [["\u2581Four", "\u2581feet", "/", "leg", "s", "\u2581Tags", ":", "\u2581/", "\u2581Bi", "ology", "\u2581/", "\u2581Chem", "istry", "\u2581/", "\u2581Physics"]], "entropy": [[0.8291015625, 1.4111328125, 1.1064453125, 1.05859375, 0.00946807861328125, 0.97119140625, 0.298095703125, 2.654296875, 0.329833984375, 0.11083984375, 2.4765625, 2.490234375, 0.2001953125, 0.513671875, 2.830078125]], "avg_entropy": [1.1526494344075522], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Four", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.40752593692979405, "std_of_first_answer": 0.40752593692979405, "avg_entropy_of_first_answer": 1.1526494344075522, "lex_sim": 1.0}
{"question": "In 'Moh's Scale' of hardness, Talc is at number 1, but what is number 2?", "answer": ["Gypsum"], "scores": [[0.59912109375, 0.9970703125]], "normalized_score": [0.7728944420814514], "tokens": [["\u2581Gy", "psum"]], "entropy": [[1.8916015625, 0.018951416015625]], "avg_entropy": [0.9552764892578125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "CaSO4\u00b72H2O", "score_of_biggest_cluster": 0.7728944420814514, "score_of_first_answer": 0.7728944420814514, "max_score_of_first_answer": 0.9970703125, "min_score_of_first_answer": 0.59912109375, "avg_score": 0.7728944420814514, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.198974609375, "std_of_first_answer": 0.198974609375, "avg_entropy_of_first_answer": 0.9552764892578125, "lex_sim": 1.0}
{"question": "In 1953, Edmund Hillary was half of the famous pair to first do what?", "answer": ["Reach the summit of Mount Everest"], "scores": [[0.28076171875, 0.9765625, 0.935546875, 0.93017578125, 0.99951171875, 0.99658203125, 0.96240234375, 0.99951171875, 0.99951171875]], "normalized_score": [0.8487247824668884], "tokens": [["\u2581Re", "ach", "\u2581the", "\u2581sum", "mit", "\u2581of", "\u2581Mount", "\u2581Ever", "est"]], "entropy": [[2.693359375, 0.1500244140625, 0.35595703125, 0.4013671875, 0.00337982177734375, 0.0263214111328125, 0.1953125, 0.004062652587890625, 0.003459930419921875]], "avg_entropy": [0.42591603597005206], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Climb Everest", "score_of_biggest_cluster": 0.8487247824668884, "score_of_first_answer": 0.8487247824668884, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.28076171875, "avg_score": 0.8487247824668884, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.21969169015540949, "std_of_first_answer": 0.21969169015540949, "avg_entropy_of_first_answer": 0.42591603597005206, "lex_sim": 1.0}
{"question": "In which town did the Co-operative Movement begin?", "answer": ["Rochdale, England"], "scores": [[0.93994140625, 1.0, 1.0, 0.93359375, 0.8623046875]], "normalized_score": [0.9457664489746094], "tokens": [["\u2581Ro", "ch", "dale", ",", "\u2581England"]], "entropy": [[0.36376953125, 0.0008115768432617188, 0.0016632080078125, 0.29541015625, 0.60986328125]], "avg_entropy": [0.2543035507202148], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rochdale, England", "score_of_biggest_cluster": 0.9457664489746094, "score_of_first_answer": 0.9457664489746094, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.8623046875, "avg_score": 0.9457664489746094, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.05103078713239257, "std_of_first_answer": 0.05103078713239257, "avg_entropy_of_first_answer": 0.2543035507202148, "lex_sim": 1.0}
{"question": "Which American singer-songwriter wrote 'Sweet Baby James', the title track of his second album in 1970?", "answer": ["James Taylor #ReleaseBlitz #CruelLegacy by KA Robb"], "scores": [[0.99560546875, 0.99560546875, 2.980232238769531e-07, 9.238719940185547e-06, 2.4318695068359375e-05, 0.9912109375, 0.1634521484375, 0.01000213623046875, 0.00937652587890625, 0.77294921875, 0.005985260009765625, 0.99951171875, 0.0008816719055175781, 0.52734375, 0.352783203125, 0.01136016845703125, 0.40380859375]], "normalized_score": [0.018508117645978928], "tokens": [["\u2581James", "\u2581Taylor", "\u2581#", "Release", "Bl", "itz", "\u2581#", "C", "ru", "el", "Leg", "acy", "\u2581by", "\u2581K", "A", "\u2581Rob", "b"]], "entropy": [[0.04534912109375, 0.045166015625, 0.626953125, 5.31640625, 0.84326171875, 0.07220458984375, 2.548828125, 5.0859375, 3.474609375, 0.9462890625, 3.392578125, 0.0039825439453125, 1.3994140625, 2.484375, 0.78662109375, 4.78515625, 2.517578125]], "avg_entropy": [2.0220417695886947], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Taylor, James", "score_of_biggest_cluster": 0.018508117645978928, "score_of_first_answer": 0.018508117645978928, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 2.980232238769531e-07, "avg_score": 0.018508117645978928, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4104324643263849, "std_of_first_answer": 0.4104324643263849, "avg_entropy_of_first_answer": 2.0220417695886947, "lex_sim": 1.0}
{"question": "In which city\u2019s harbour was the ship Queen Elizabeth ravaged by fire in 1972?", "answer": ["Cannes Bay"], "scores": [[0.257568359375, 0.013427734375, 0.0007290840148925781]], "normalized_score": [0.013611027039587498], "tokens": [["\u2581C", "annes", "\u2581Bay"]], "entropy": [[2.490234375, 1.4384765625, 1.4541015625]], "avg_entropy": [1.7942708333333333], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kong Hong", "score_of_biggest_cluster": 0.013611027039587498, "score_of_first_answer": 0.013611027039587498, "max_score_of_first_answer": 0.257568359375, "min_score_of_first_answer": 0.0007290840148925781, "avg_score": 0.013611027039587498, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11819584212646775, "std_of_first_answer": 0.11819584212646775, "avg_entropy_of_first_answer": 1.7942708333333333, "lex_sim": 1.0}
{"question": "Players form scrums in which sport?", "answer": ["Rugby #10 - \"Fashion for the Common Man\""], "scores": [[0.9111328125, 1.4901161193847656e-06, 0.0170440673828125, 0.12310791015625, 0.0003228187561035156, 0.002529144287109375, 0.0025157928466796875, 0.0018358230590820312, 0.0018072128295898438, 0.140625, 0.0014705657958984375, 0.064208984375, 0.78271484375]], "normalized_score": [0.007406346965581179], "tokens": [["\u2581Rugby", "\u2581#", "1", "0", "\u2581-", "\u2581\"", "F", "ashion", "\u2581for", "\u2581the", "\u2581Common", "\u2581Man", "\""]], "entropy": [[0.465087890625, 1.513671875, 4.86328125, 3.14453125, 3.208984375, 4.3984375, 5.078125, 4.62890625, 3.03515625, 4.390625, 5.8203125, 0.60205078125, 1.3095703125]], "avg_entropy": [3.266056941105769], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rugby (disambiguation)", "score_of_biggest_cluster": 0.007406346965581179, "score_of_first_answer": 0.007406346965581179, "max_score_of_first_answer": 0.9111328125, "min_score_of_first_answer": 1.4901161193847656e-06, "avg_score": 0.007406346965581179, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29860157516554886, "std_of_first_answer": 0.29860157516554886, "avg_entropy_of_first_answer": 3.266056941105769, "lex_sim": 1.0}
{"question": "First put on sale last week, Decision Points, currently ranked #1 in books on Amazon.com, is a memoir by what politician?", "answer": ["George W. Bush"], "scores": [[0.68310546875, 0.95654296875, 0.97265625, 0.99267578125]], "normalized_score": [0.8912299275398254], "tokens": [["\u2581George", "\u2581W", ".", "\u2581Bush"]], "entropy": [[1.259765625, 0.22119140625, 0.14111328125, 0.06414794921875]], "avg_entropy": [0.4215545654296875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George Walker Bush", "score_of_biggest_cluster": 0.8912299275398254, "score_of_first_answer": 0.8912299275398254, "max_score_of_first_answer": 0.99267578125, "min_score_of_first_answer": 0.68310546875, "avg_score": 0.8912299275398254, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.12659174027986445, "std_of_first_answer": 0.12659174027986445, "avg_entropy_of_first_answer": 0.4215545654296875, "lex_sim": 1.0}
{"question": "In 1932 Margaret Kelly founded which famous all-female dancing troupe?", "answer": ["Rockettes"], "scores": [[0.043609619140625, 0.9775390625]], "normalized_score": [0.20647059381008148], "tokens": [["\u2581Rock", "ettes"]], "entropy": [[0.55810546875, 0.1097412109375]], "avg_entropy": [0.33392333984375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Bluebell Girls", "score_of_biggest_cluster": 0.20647059381008148, "score_of_first_answer": 0.20647059381008148, "max_score_of_first_answer": 0.9775390625, "min_score_of_first_answer": 0.043609619140625, "avg_score": 0.20647059381008148, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4669647216796875, "std_of_first_answer": 0.4669647216796875, "avg_entropy_of_first_answer": 0.33392333984375, "lex_sim": 1.0}
{"question": "When was the guillotine last used in France?", "answer": ["in 1981, in French Polynesia"], "scores": [[0.06146240234375, 0.58740234375, 0.9990234375, 0.9921875, 0.865234375, 0.994140625, 0.07037353515625, 0.095703125, 0.2252197265625, 0.94580078125, 0.99951171875, 0.9990234375]], "normalized_score": [0.4335442781448364], "tokens": [["\u2581in", "\u2581", "1", "9", "8", "1", ",", "\u2581in", "\u2581French", "\u2581Pol", "yn", "esia"]], "entropy": [[2.546875, 1.5029296875, 0.01136016845703125, 0.054229736328125, 0.5595703125, 0.04638671875, 1.255859375, 3.330078125, 2.484375, 0.376220703125, 0.0033817291259765625, 0.01096343994140625]], "avg_entropy": [1.015185832977295], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1977", "score_of_biggest_cluster": 0.4335442781448364, "score_of_first_answer": 0.4335442781448364, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.06146240234375, "avg_score": 0.4335442781448364, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39885092265996747, "std_of_first_answer": 0.39885092265996747, "avg_entropy_of_first_answer": 1.015185832977295, "lex_sim": 1.0}
{"question": "George Bush, American president 2001 to 2009, had what middle name?", "answer": ["Herbert Walker"], "scores": [[0.00928497314453125, 0.94775390625]], "normalized_score": [0.09380762279033661], "tokens": [["\u2581Herbert", "\u2581Walker"]], "entropy": [[1.0673828125, 0.26220703125]], "avg_entropy": [0.664794921875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alick Walker", "score_of_biggest_cluster": 0.09380762279033661, "score_of_first_answer": 0.09380762279033661, "max_score_of_first_answer": 0.94775390625, "min_score_of_first_answer": 0.00928497314453125, "avg_score": 0.09380762279033661, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4692344665527344, "std_of_first_answer": 0.4692344665527344, "avg_entropy_of_first_answer": 0.664794921875, "lex_sim": 1.0}
{"question": "What type of creature is a tanager?", "answer": ["Tanager is a common name for a group of birds that are native to the Americas, but the specific bird known as a \"tanager\" can be one of several different species and genera."], "scores": [[0.06610107421875, 0.54541015625, 0.95556640625, 0.99560546875, 0.0009560585021972656, 0.98193359375, 0.9736328125, 0.8818359375, 0.71630859375, 1.0, 0.4853515625, 0.271484375, 0.51611328125, 0.498779296875, 1.0, 0.64794921875, 0.64306640625, 1.0, 0.1778564453125, 0.007053375244140625, 0.45556640625, 0.386962890625, 0.00594329833984375, 0.016021728515625, 0.99658203125, 0.90966796875, 0.16064453125, 0.99951171875, 1.0, 0.99951171875, 0.08758544921875, 0.0811767578125, 0.03509521484375, 0.99853515625, 0.8583984375, 0.440673828125, 0.892578125, 0.0030956268310546875, 0.775390625, 1.0, 0.15185546875, 0.0606689453125, 0.966796875, 0.98828125, 0.6005859375, 0.9990234375, 0.9990234375, 0.94140625, 0.468994140625, 0.97900390625]], "normalized_score": [0.32503506541252136], "tokens": [["\u2581Tan", "ager", "\u2581is", "\u2581a", "\u2581common", "\u2581name", "\u2581for", "\u2581a", "\u2581group", "\u2581of", "\u2581birds", "\u2581that", "\u2581are", "\u2581native", "\u2581to", "\u2581the", "\u2581Amer", "icas", ",", "\u2581but", "\u2581the", "\u2581specific", "\u2581bird", "\u2581known", "\u2581as", "\u2581a", "\u2581\"", "tan", "ager", "\"", "\u2581can", "\u2581be", "\u2581one", "\u2581of", "\u2581several", "\u2581different", "\u2581species", "\u2581and", "\u2581gener", "a", ".", "\u2581These", "\u2581birds", "\u2581are", "\u2581known", "\u2581for", "\u2581their", "\u2581bright", "ly", "\u2581colored"]], "entropy": [[1.642578125, 0.7080078125, 0.270263671875, 0.035980224609375, 0.583984375, 0.130859375, 0.1829833984375, 0.65087890625, 1.181640625, 0.0010433197021484375, 1.8701171875, 2.236328125, 1.5419921875, 2.19921875, 0.001102447509765625, 0.92626953125, 1.3779296875, 0.0002148151397705078, 0.73046875, 2.625, 2.30859375, 1.9892578125, 0.2198486328125, 1.6689453125, 0.0282440185546875, 0.382568359375, 0.5634765625, 0.004810333251953125, 0.0008072853088378906, 0.004222869873046875, 1.0634765625, 0.91650390625, 2.697265625, 0.01299285888671875, 0.5654296875, 1.0625, 0.448974609375, 1.603515625, 1.1455078125, 0.001422882080078125, 1.5712890625, 2.349609375, 0.1959228515625, 0.087158203125, 1.4443359375, 0.00942230224609375, 0.00902557373046875, 0.299560546875, 1.34765625, 0.11407470703125]], "avg_entropy": [0.8602656126022339], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Avians", "score_of_biggest_cluster": 0.32503506541252136, "score_of_first_answer": 0.32503506541252136, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0009560585021972656, "avg_score": 0.32503506541252136, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38067553327652504, "std_of_first_answer": 0.38067553327652504, "avg_entropy_of_first_answer": 0.8602656126022339, "lex_sim": 1.0}
{"question": "Which novelist, born in Bombay in 1865, was awarded the Nobel prize for Literature in 1907?", "answer": ["Rabindranath Tagore"], "scores": [[0.59033203125, 0.9970703125, 0.99853515625, 0.9970703125, 0.873046875, 0.99609375]], "normalized_score": [0.89373379945755], "tokens": [["\u2581Rab", "ind", "ran", "ath", "\u2581Tag", "ore"]], "entropy": [[1.1416015625, 0.0299530029296875, 0.01444244384765625, 0.026885986328125, 0.6162109375, 0.029754638671875]], "avg_entropy": [0.30980809529622394], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joseph Rudyard Kipling", "score_of_biggest_cluster": 0.89373379945755, "score_of_first_answer": 0.89373379945755, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.59033203125, "avg_score": 0.89373379945755, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1494188494943945, "std_of_first_answer": 0.1494188494943945, "avg_entropy_of_first_answer": 0.30980809529622394, "lex_sim": 1.0}
{"question": "Who did Michael Gorbachev succeed as General Secretary of the Communist Party of the Soviet Union?", "answer": ["Yuri Andropov"], "scores": [[0.76123046875, 0.9326171875, 0.9814453125, 0.9990234375, 0.99951171875]], "normalized_score": [0.9300147891044617], "tokens": [["\u2581Y", "uri", "\u2581And", "rop", "ov"]], "entropy": [[1.1845703125, 0.303955078125, 0.140380859375, 0.008392333984375, 0.002796173095703125]], "avg_entropy": [0.3280189514160156], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Constantin Chernenko", "score_of_biggest_cluster": 0.9300147891044617, "score_of_first_answer": 0.9300147891044617, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.76123046875, "avg_score": 0.9300147891044617, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0901323278149077, "std_of_first_answer": 0.0901323278149077, "avg_entropy_of_first_answer": 0.3280189514160156, "lex_sim": 1.0}
{"question": "What is the star sign of the date of the Battle of Waterloo?", "answer": ["Leo"], "scores": [[0.3818359375]], "normalized_score": [0.3818359375], "tokens": [["\u2581Leo"]], "entropy": [[3.41015625]], "avg_entropy": [3.41015625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gemini (single)", "score_of_biggest_cluster": 0.3818359375, "score_of_first_answer": 0.3818359375, "max_score_of_first_answer": 0.3818359375, "min_score_of_first_answer": 0.3818359375, "avg_score": 0.3818359375, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.41015625, "lex_sim": 1.0}
{"question": "What name is given to the lowest commissioned officer in a cavalry regiment, such as Prince Harry?", "answer": ["Cornet Tags: News, Volusia County"], "scores": [[0.229248046875, 0.99951171875, 1.1920928955078125e-07, 0.80029296875, 3.731250762939453e-05, 0.1600341796875, 0.00047516822814941406, 0.0011453628540039062, 0.94189453125, 0.254150390625]], "normalized_score": [0.01054533664137125], "tokens": [["\u2581Cor", "net", "\u2581Tags", ":", "\u2581News", ",", "\u2581Vol", "us", "ia", "\u2581County"]], "entropy": [[0.98828125, 0.0036640167236328125, 1.0, 1.2529296875, 5.44140625, 4.4765625, 6.82421875, 2.794921875, 0.3740234375, 3.50390625]], "avg_entropy": [2.6659914016723634], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Cornet", "score_of_biggest_cluster": 0.01054533664137125, "score_of_first_answer": 0.01054533664137125, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 1.1920928955078125e-07, "avg_score": 0.01054533664137125, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.390177250928031, "std_of_first_answer": 0.390177250928031, "avg_entropy_of_first_answer": 2.6659914016723634, "lex_sim": 1.0}
{"question": "June 5, 2004 saw the death of what man, the 40th President of the United States?", "answer": ["George HW Bush [2015.01.15] JIMMY WANG'S PRODIGY NEPHEW DIES AT 19"], "scores": [[0.140869140625, 0.247802734375, 0.0269012451171875, 0.9853515625, 1.6808509826660156e-05, 0.0031795501708984375, 0.449462890625, 0.2607421875, 0.0237274169921875, 0.01477813720703125, 0.591796875, 0.0667724609375, 0.9765625, 0.269775390625, 0.06561279296875, 0.34814453125, 0.00012922286987304688, 0.001995086669921875, 0.016876220703125, 0.98828125, 0.1217041015625, 0.0018138885498046875, 0.00576019287109375, 0.93701171875, 0.0018148422241210938, 0.0006170272827148438, 0.9951171875, 0.7685546875, 0.007022857666015625, 0.00098419189453125, 0.97802734375, 0.83984375, 0.00262451171875, 0.73046875, 0.1864013671875, 0.787109375, 0.25390625, 0.074951171875]], "normalized_score": [0.05075794458389282], "tokens": [["\u2581George", "\u2581H", "W", "\u2581Bush", "\u2581[", "2", "0", "1", "5", ".", "0", "1", ".", "1", "5", "]", "\u2581J", "I", "MM", "Y", "\u2581W", "ANG", "'", "S", "\u2581PRO", "DI", "G", "Y", "\u2581NE", "P", "HE", "W", "\u2581DI", "ES", "\u2581AT", "\u2581", "1", "9"]], "entropy": [[2.287109375, 0.80224609375, 0.302734375, 0.1068115234375, 0.7021484375, 1.900390625, 2.150390625, 1.4013671875, 1.9384765625, 2.7109375, 1.3076171875, 1.66015625, 0.1717529296875, 1.130859375, 2.28515625, 2.962890625, 1.5654296875, 3.8828125, 4.64453125, 0.10919189453125, 3.599609375, 2.14453125, 2.77734375, 0.27490234375, 4.921875, 2.033203125, 0.043853759765625, 0.6845703125, 4.25, 2.51953125, 0.1690673828125, 0.48583984375, 4.61328125, 1.087890625, 3.09375, 0.90869140625, 1.7119140625, 2.072265625]], "avg_entropy": [1.8793455425061678], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "President Regan", "score_of_biggest_cluster": 0.05075794458389282, "score_of_first_answer": 0.05075794458389282, "max_score_of_first_answer": 0.9951171875, "min_score_of_first_answer": 1.6808509826660156e-05, "avg_score": 0.05075794458389282, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3735243149635532, "std_of_first_answer": 0.3735243149635532, "avg_entropy_of_first_answer": 1.8793455425061678, "lex_sim": 1.0}
{"question": "What is the chief river of Burma ?", "answer": ["Ayeyarwady #ThrowbackThursday \u2013 Celebrating 28 Years of Fetal Anomaly Detection with 4D Ultrasound"], "scores": [[0.296142578125, 0.9990234375, 0.9638671875, 0.99951171875, 0.9775390625, 7.748603820800781e-07, 1.2516975402832031e-06, 0.76025390625, 0.0531005859375, 0.99951171875, 0.99951171875, 3.2067298889160156e-05, 0.006595611572265625, 0.59619140625, 0.99462890625, 0.822265625, 0.343017578125, 0.13037109375, 0.030120849609375, 0.368896484375, 0.75830078125, 0.00554656982421875, 0.00095367431640625, 0.0010347366333007812, 0.97314453125, 0.9833984375, 0.042877197265625, 0.9990234375, 0.99658203125, 0.253173828125, 0.02911376953125, 0.0782470703125, 0.95947265625, 0.6162109375, 0.984375, 0.998046875]], "normalized_score": [0.09085987508296967], "tokens": [["\u2581Ay", "ey", "ar", "w", "ady", "\u2581#", "Throw", "back", "Th", "urs", "day", "\u2581\u2013", "\u2581C", "ele", "br", "ating", "\u2581", "2", "8", "\u2581Years", "\u2581of", "\u2581F", "etal", "\u2581An", "om", "aly", "\u2581D", "ete", "ction", "\u2581with", "\u2581", "4", "D", "\u2581Ul", "tras", "ound"]], "entropy": [[1.8369140625, 0.009735107421875, 0.175537109375, 0.0029754638671875, 0.12152099609375, 1.4296875, 4.37109375, 0.92529296875, 2.66015625, 0.0051116943359375, 0.007236480712890625, 1.1025390625, 5.05859375, 1.4736328125, 0.043670654296875, 0.515625, 2.76953125, 1.87109375, 1.8056640625, 1.2158203125, 1.2763671875, 5.19140625, 5.109375, 3.244140625, 0.1607666015625, 0.09130859375, 1.3095703125, 0.007659912109375, 0.0223846435546875, 2.92578125, 4.140625, 1.6796875, 0.27392578125, 1.9375, 0.08648681640625, 0.014984130859375]], "avg_entropy": [1.5242611567179363], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "IRRAWADDY", "score_of_biggest_cluster": 0.09085987508296967, "score_of_first_answer": 0.09085987508296967, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 7.748603820800781e-07, "avg_score": 0.09085987508296967, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42851366206814606, "std_of_first_answer": 0.42851366206814606, "avg_entropy_of_first_answer": 1.5242611567179363, "lex_sim": 1.0}
{"question": "Damascus is the capital of which country?", "answer": ["Syria"], "scores": [[0.9404296875, 0.98876953125]], "normalized_score": [0.9642967581748962], "tokens": [["\u2581Sy", "ria"]], "entropy": [[0.498291015625, 0.073486328125]], "avg_entropy": [0.285888671875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u0627\u0644\u062c\u0645\u0647\u0648\u0631\u064a\u0629 \u0627\u0644\u0639\u0631\u0628\u064a\u0629 \u0627\u0644\u0633\u0648\u0631\u064a\u0629", "score_of_biggest_cluster": 0.9642967581748962, "score_of_first_answer": 0.9642967581748962, "max_score_of_first_answer": 0.98876953125, "min_score_of_first_answer": 0.9404296875, "avg_score": 0.9642967581748962, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.024169921875, "std_of_first_answer": 0.024169921875, "avg_entropy_of_first_answer": 0.285888671875, "lex_sim": 1.0}
{"question": "Petroliam Nasional Berhad, the huge Malaysian gas/oil corporation gave its abbreviated name to what world-leading Kuala Lumpa landmark?", "answer": ["Petronas Towers"], "scores": [[0.5908203125, 0.99951171875, 0.9990234375, 0.2266845703125, 1.0]], "normalized_score": [0.6687259674072266], "tokens": [["\u2581Pet", "ron", "as", "\u2581Tow", "ers"]], "entropy": [[1.5244140625, 0.0052490234375, 0.012542724609375, 0.7861328125, 7.277727127075195e-05]], "avg_entropy": [0.46568228006362916], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Petronas Tower 2", "score_of_biggest_cluster": 0.6687259674072266, "score_of_first_answer": 0.6687259674072266, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.2266845703125, "avg_score": 0.6687259674072266, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.31147832179066026, "std_of_first_answer": 0.31147832179066026, "avg_entropy_of_first_answer": 0.46568228006362916, "lex_sim": 1.0}
{"question": "The 2013 G8 Summit was held at Lough Erne Resort in Northern Ireland. In which county is this?", "answer": ["County Fermanagh #30days30poets: Making Room for Discomfort"], "scores": [[0.62109375, 0.998046875, 1.0, 1.0, 2.384185791015625e-07, 0.0023593902587890625, 0.048095703125, 0.00045418739318847656, 0.0243072509765625, 0.99951171875, 0.0001437664031982422, 0.328857421875, 0.0024318695068359375, 0.0010213851928710938, 0.035919189453125, 0.006137847900390625, 0.76171875, 0.034423828125, 0.8349609375, 0.99951171875]], "normalized_score": [0.02825060486793518], "tokens": [["\u2581County", "\u2581F", "erman", "agh", "\u2581#", "3", "0", "days", "3", "0", "po", "ets", ":", "\u2581M", "aking", "\u2581Room", "\u2581for", "\u2581Dis", "com", "fort"]], "entropy": [[0.9931640625, 0.0185699462890625, 0.0015163421630859375, 0.0012426376342773438, 1.1572265625, 2.90234375, 3.51953125, 3.376953125, 2.95703125, 0.003742218017578125, 1.896484375, 1.0908203125, 1.9951171875, 5.3125, 5.15625, 4.30859375, 1.0615234375, 4.12109375, 0.6103515625, 0.002552032470703125]], "avg_entropy": [2.0243303775787354], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "County Fermanagh", "score_of_biggest_cluster": 0.02825060486793518, "score_of_first_answer": 0.02825060486793518, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.384185791015625e-07, "avg_score": 0.02825060486793518, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4362984987843968, "std_of_first_answer": 0.4362984987843968, "avg_entropy_of_first_answer": 2.0243303775787354, "lex_sim": 1.0}
{"question": "Which book of the Old Testament explains how the festival of Purim came to be celebrated by the Jews?", "answer": ["The Book of Esther"], "scores": [[0.68505859375, 0.93798828125, 0.99609375, 0.99560546875, 1.0]], "normalized_score": [0.9138239026069641], "tokens": [["\u2581The", "\u2581Book", "\u2581of", "\u2581Est", "her"]], "entropy": [[1.0302734375, 0.302490234375, 0.0299072265625, 0.033599853515625, 0.0005707740783691406]], "avg_entropy": [0.27936830520629885], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Esther", "score_of_biggest_cluster": 0.9138239026069641, "score_of_first_answer": 0.9138239026069641, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.68505859375, "avg_score": 0.9138239026069641, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.12114784263110062, "std_of_first_answer": 0.12114784263110062, "avg_entropy_of_first_answer": 0.27936830520629885, "lex_sim": 1.0}
{"question": "If you divide a prime number by 4, the answer must result in ?", "answer": ["0.25"], "scores": [[0.115966796875, 0.301513671875, 0.165771484375, 0.59716796875, 0.974609375]], "normalized_score": [0.3203432261943817], "tokens": [["\u2581", "0", ".", "2", "5"]], "entropy": [[3.380859375, 1.373046875, 2.005859375, 1.783203125, 0.1956787109375]], "avg_entropy": [1.7477294921875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fraction (quantity)", "score_of_biggest_cluster": 0.3203432261943817, "score_of_first_answer": 0.3203432261943817, "max_score_of_first_answer": 0.974609375, "min_score_of_first_answer": 0.115966796875, "avg_score": 0.3203432261943817, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3192120737058245, "std_of_first_answer": 0.3192120737058245, "avg_entropy_of_first_answer": 1.7477294921875, "lex_sim": 1.0}
{"question": "Who played King Arthur in the 2004 film 'King Arthur'?", "answer": ["Clive Owen"], "scores": [[0.9765625, 1.0, 0.998046875, 0.99951171875]], "normalized_score": [0.9934813380241394], "tokens": [["\u2581C", "live", "\u2581O", "wen"]], "entropy": [[0.147705078125, 0.0005173683166503906, 0.01983642578125, 0.006572723388671875]], "avg_entropy": [0.043657898902893066], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Clive Owens", "score_of_biggest_cluster": 0.9934813380241394, "score_of_first_answer": 0.9934813380241394, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.9765625, "avg_score": 0.9934813380241394, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.009822678796039479, "std_of_first_answer": 0.009822678796039479, "avg_entropy_of_first_answer": 0.043657898902893066, "lex_sim": 1.0}
{"question": "July 2, 1961 saw what Nobel Prize winning author take his own life in Ketchum, Idaho, who once stated that every boy must Plant a tree, Fight a bull, Write a novel, and Father a son in order to be a man?", "answer": ["Ernest Hemingway"], "scores": [[0.837890625, 0.69970703125, 0.99560546875, 0.9990234375]], "normalized_score": [0.8738595247268677], "tokens": [["\u2581Ernest", "\u2581Hem", "ing", "way"]], "entropy": [[1.158203125, 0.64501953125, 0.0311279296875, 0.01042938232421875]], "avg_entropy": [0.4611949920654297], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Famous at Twenty-Five Thirty a Master", "score_of_biggest_cluster": 0.8738595247268677, "score_of_first_answer": 0.8738595247268677, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.69970703125, "avg_score": 0.8738595247268677, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12427045597723653, "std_of_first_answer": 0.12427045597723653, "avg_entropy_of_first_answer": 0.4611949920654297, "lex_sim": 1.0}
{"question": "How was Moose the dog better known when he starred in a popular US sitcom?", "answer": ["Moose the Dog ###"], "scores": [[0.8681640625, 0.9501953125, 0.53955078125, 0.505859375, 2.980232238769531e-07]], "normalized_score": [0.0367574580013752], "tokens": [["\u2581Mo", "ose", "\u2581the", "\u2581Dog", "\u2581###"]], "entropy": [[0.77197265625, 0.2734375, 1.96875, 1.4267578125, 2.443359375]], "avg_entropy": [1.37685546875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Quiksilver Big Wave Invitational in Memory of Eddie Aikau", "score_of_biggest_cluster": 0.0367574580013752, "score_of_first_answer": 0.0367574580013752, "max_score_of_first_answer": 0.9501953125, "min_score_of_first_answer": 2.980232238769531e-07, "avg_score": 0.0367574580013752, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33566447447572506, "std_of_first_answer": 0.33566447447572506, "avg_entropy_of_first_answer": 1.37685546875, "lex_sim": 1.0}
{"question": "In which film does psychopath Annie Wilkes wield a sledgehammer to horrific effect?", "answer": ["Misery (1990 film)"], "scores": [[0.8603515625, 0.99267578125, 0.0921630859375, 0.92431640625, 1.0, 1.0, 0.99951171875, 0.035400390625, 0.99267578125]], "normalized_score": [0.5151579976081848], "tokens": [["\u2581Mis", "ery", "\u2581(", "1", "9", "9", "0", "\u2581film", ")"]], "entropy": [[0.828125, 0.073486328125, 1.041015625, 0.47216796875, 0.0014333724975585938, 0.0020599365234375, 0.00368499755859375, 0.21728515625, 0.0589599609375]], "avg_entropy": [0.2998020384046767], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Misery (disambiguation)", "score_of_biggest_cluster": 0.5151579976081848, "score_of_first_answer": 0.5151579976081848, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.035400390625, "avg_score": 0.5151579976081848, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37838850641200944, "std_of_first_answer": 0.37838850641200944, "avg_entropy_of_first_answer": 0.2998020384046767, "lex_sim": 1.0}
{"question": "In jazz, which clarinettist/bandleader was known as the King of Swing?", "answer": ["Benny Goodman"], "scores": [[0.73583984375, 0.99853515625, 0.9912109375, 0.994140625]], "normalized_score": [0.9224442839622498], "tokens": [["\u2581Ben", "ny", "\u2581Good", "man"]], "entropy": [[1.7744140625, 0.0138702392578125, 0.0731201171875, 0.04083251953125]], "avg_entropy": [0.4755592346191406], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Benny Goodman Band", "score_of_biggest_cluster": 0.9224442839622498, "score_of_first_answer": 0.9224442839622498, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.73583984375, "avg_score": 0.9224442839622498, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11208926562146714, "std_of_first_answer": 0.11208926562146714, "avg_entropy_of_first_answer": 0.4755592346191406, "lex_sim": 1.0}
{"question": "A sequicentenary is an anniversary of how many years?", "answer": ["250, or 250 years."], "scores": [[0.7841796875, 0.7724609375, 0.9990234375, 0.9990234375, 0.006603240966796875, 0.2406005859375, 0.50927734375, 0.98486328125, 0.99609375, 0.90673828125, 0.85302734375, 0.457763671875, 1.5497207641601562e-06, 6.139278411865234e-05, 0.451904296875, 0.08636474609375, 0.96630859375, 0.98486328125, 0.2802734375, 0.0226898193359375, 0.257080078125, 0.0022373199462890625, 0.140869140625, 0.9990234375, 0.83203125, 0.92431640625, 1.0, 1.0, 0.60791015625, 0.7255859375, 0.0250244140625, 0.9990234375, 0.00421142578125, 0.06756591796875, 0.9873046875, 1.0, 0.03460693359375, 0.279052734375, 0.08038330078125, 0.77783203125, 0.8623046875, 0.9921875, 0.9970703125, 0.494384765625, 0.051605224609375, 0.99609375, 0.99951171875, 0.556640625]], "normalized_score": [0.2126648873090744], "tokens": [["\u2581", "2", "5", "0", ",", "\u2581or", "\u2581", "2", "5", "0", "\u2581years", ".", "\u2581#", "OT", "D", "\u2581in", "\u2581", "1", "7", "9", "8", "\u2581\u2013", "\u2581United", "\u2581Irish", "men", "\u2581Reb", "ell", "ion", ":", "\u2581The", "\u2581Battle", "\u2581of", "\u2581the", "\u2581Four", "\u2581Cour", "ts", ",", "\u2581the", "\u2581last", "\u2581major", "\u2581action", "\u2581of", "\u2581the", "\u2581Irish", "\u2581re", "bell", "ion", "\u2581of"]], "entropy": [[1.06640625, 0.55712890625, 0.01212310791015625, 0.0118865966796875, 1.4814453125, 3.494140625, 1.84375, 0.1033935546875, 0.037384033203125, 0.5478515625, 0.61376953125, 1.1416015625, 0.63623046875, 5.61328125, 2.458984375, 2.9921875, 0.25537109375, 0.09478759765625, 1.45703125, 1.3798828125, 2.162109375, 0.64453125, 2.376953125, 0.008148193359375, 0.484375, 0.456787109375, 3.600120544433594e-05, 0.0010232925415039062, 1.1767578125, 1.4306640625, 2.697265625, 0.00933074951171875, 3.25390625, 1.95703125, 0.08935546875, 5.733966827392578e-05, 0.88671875, 2.3125, 2.53515625, 0.69775390625, 0.5654296875, 0.054412841796875, 0.027374267578125, 1.341796875, 0.239990234375, 0.0260009765625, 0.004833221435546875, 0.91064453125]], "avg_entropy": [1.0864496007561684], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "150", "score_of_biggest_cluster": 0.2126648873090744, "score_of_first_answer": 0.2126648873090744, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.5497207641601562e-06, "avg_score": 0.2126648873090744, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39733259227029377, "std_of_first_answer": 0.39733259227029377, "avg_entropy_of_first_answer": 1.0864496007561684, "lex_sim": 1.0}
{"question": "Morays and congers are types of what?", "answer": ["Eel"], "scores": [[0.228271484375, 0.177001953125]], "normalized_score": [0.20100870728492737], "tokens": [["\u2581E", "el"]], "entropy": [[1.9736328125, 0.51708984375]], "avg_entropy": [1.245361328125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eels", "score_of_biggest_cluster": 0.20100870728492737, "score_of_first_answer": 0.20100870728492737, "max_score_of_first_answer": 0.228271484375, "min_score_of_first_answer": 0.177001953125, "avg_score": 0.20100870728492737, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.025634765625, "std_of_first_answer": 0.025634765625, "avg_entropy_of_first_answer": 1.245361328125, "lex_sim": 1.0}
{"question": "The Asian Tsunami or Boxing day tsunami that hit Thailand, Indonesia and many other countries happened in what year?", "answer": ["26th December, 2004"], "scores": [[0.66162109375, 0.98095703125, 0.011932373046875, 0.50439453125, 0.7392578125, 0.10711669921875, 0.7822265625, 0.99169921875, 0.99072265625, 0.99853515625, 0.99951171875]], "normalized_score": [0.46824708580970764], "tokens": [["\u2581", "2", "6", "th", "\u2581December", ",", "\u2581", "2", "0", "0", "4"]], "entropy": [[1.4765625, 0.11419677734375, 0.0880126953125, 1.373046875, 0.92578125, 0.654296875, 0.91064453125, 0.055816650390625, 0.07733154296875, 0.0114898681640625, 0.005359649658203125]], "avg_entropy": [0.5175035650079901], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "two thousand and four", "score_of_biggest_cluster": 0.46824708580970764, "score_of_first_answer": 0.46824708580970764, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.011932373046875, "avg_score": 0.46824708580970764, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.34376787857744934, "std_of_first_answer": 0.34376787857744934, "avg_entropy_of_first_answer": 0.5175035650079901, "lex_sim": 1.0}
{"question": "Which jockey won the champion national hunt jockey title 7 times between 1975 and 1985", "answer": ["A.P."], "scores": [[0.0165863037109375, 0.27880859375, 0.7080078125, 0.9013671875, 0.89599609375, 0.99609375, 1.0, 1.1324882507324219e-06, 0.00218963623046875, 0.00011909008026123047, 0.005859375, 0.052093505859375, 0.00014901161193847656, 0.003726959228515625, 0.06988525390625, 6.753206253051758e-05, 0.0199432373046875, 0.83984375, 0.57373046875, 0.98974609375, 0.98486328125, 0.423583984375, 0.1259765625, 0.669921875, 0.98974609375, 0.2481689453125, 0.02264404296875, 0.94189453125, 0.98876953125]], "normalized_score": [0.0575127899646759], "tokens": [["\u2581A", ".", "P", ".", "\u2581Mc", "Co", "y", "\u2581#", "B", "log", "T", "our", "\u2581The", "\u2581H", "idden", "\u2581by", "\u2581He", "ather", "\u2581Ch", "is", "vin", "\u2581#", "Book", "Re", "view", "\u2581#", "G", "ive", "away"]], "entropy": [[2.748046875, 1.8408203125, 0.65478515625, 0.3779296875, 0.451416015625, 0.02935791015625, 0.00019991397857666016, 0.8818359375, 4.73046875, 4.62109375, 4.56640625, 3.03125, 3.78515625, 4.40234375, 1.6982421875, 4.0234375, 4.328125, 0.477783203125, 2.2109375, 0.09649658203125, 0.10992431640625, 2.287109375, 4.28515625, 1.5673828125, 0.0775146484375, 2.7734375, 3.8046875, 0.408935546875, 0.08453369140625]], "avg_entropy": [2.081200488682451], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "John Francome", "score_of_biggest_cluster": 0.0575127899646759, "score_of_first_answer": 0.0575127899646759, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.1324882507324219e-06, "avg_score": 0.0575127899646759, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42034750135920784, "std_of_first_answer": 0.42034750135920784, "avg_entropy_of_first_answer": 2.081200488682451, "lex_sim": 1.0}
{"question": "Betty, Earl and Joyce are the puppet pensioners in ads for which company?", "answer": ["Saga holidays"], "scores": [[0.77880859375, 0.93408203125, 0.046478271484375, 1.0, 1.0]], "normalized_score": [0.5079408288002014], "tokens": [["\u2581S", "aga", "\u2581hol", "id", "ays"]], "entropy": [[1.4443359375, 0.288818359375, 1.333984375, 0.0004754066467285156, 0.00020074844360351562]], "avg_entropy": [0.6135629653930664], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wonga", "score_of_biggest_cluster": 0.5079408288002014, "score_of_first_answer": 0.5079408288002014, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.046478271484375, "avg_score": 0.5079408288002014, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3618401294915544, "std_of_first_answer": 0.3618401294915544, "avg_entropy_of_first_answer": 0.6135629653930664, "lex_sim": 1.0}
{"question": "Who wrote One-upmanship and The Theory and Practice of Gamesmanship?", "answer": ["Sean Hemingway"], "scores": [[0.00395965576171875, 0.80126953125, 0.07293701171875, 0.62841796875, 0.99951171875]], "normalized_score": [0.17079851031303406], "tokens": [["\u2581Se", "an", "\u2581Hem", "ing", "way"]], "entropy": [[2.71875, 1.224609375, 3.44921875, 1.1708984375, 0.0056915283203125]], "avg_entropy": [1.7138336181640625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Potter, Stephen Meredith", "score_of_biggest_cluster": 0.17079851031303406, "score_of_first_answer": 0.17079851031303406, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.00395965576171875, "avg_score": 0.17079851031303406, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39628218466946163, "std_of_first_answer": 0.39628218466946163, "avg_entropy_of_first_answer": 1.7138336181640625, "lex_sim": 1.0}
{"question": "Who is the lexicographer, adjudicator, and longest-serving member of the on-screen team on Countdown?", "answer": ["Susie Dent"], "scores": [[0.9736328125, 1.0, 0.99951171875, 0.99853515625]], "normalized_score": [0.9928568005561829], "tokens": [["\u2581Sus", "ie", "\u2581D", "ent"]], "entropy": [[0.19775390625, 0.0021686553955078125, 0.0051422119140625, 0.01439666748046875]], "avg_entropy": [0.054865360260009766], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "SUSIE DENT", "score_of_biggest_cluster": 0.9928568005561829, "score_of_first_answer": 0.9928568005561829, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.9736328125, "avg_score": 0.9928568005561829, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0111479004461239, "std_of_first_answer": 0.0111479004461239, "avg_entropy_of_first_answer": 0.054865360260009766, "lex_sim": 1.0}
{"question": "The day after Shrove Tuesday is known as \u2018What\u2019\u2026.Wednesday\u2019?", "answer": ["Ash Wednesday"], "scores": [[0.994140625, 0.9501953125, 0.99462890625, 0.9990234375]], "normalized_score": [0.9842929244041443], "tokens": [["\u2581Ash", "\u2581Wed", "nes", "day"]], "entropy": [[0.061767578125, 0.28759765625, 0.04290771484375, 0.01033782958984375]], "avg_entropy": [0.10065269470214844], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ash-Wednesday", "score_of_biggest_cluster": 0.9842929244041443, "score_of_first_answer": 0.9842929244041443, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.9501953125, "avg_score": 0.9842929244041443, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.01989521411777655, "std_of_first_answer": 0.01989521411777655, "avg_entropy_of_first_answer": 0.10065269470214844, "lex_sim": 1.0}
{"question": "Simon Legree is the villain in which famous novel?", "answer": ["Uncle Tom's Cabin ###### **Limbo**"], "scores": [[0.73583984375, 0.94873046875, 0.99072265625, 1.0, 0.98876953125, 1.0, 0.0, 0.2822265625, 0.0001596212387084961, 0.0007290840148925781, 0.0010814666748046875, 0.004573822021484375, 0.86962890625]], "normalized_score": [0.0], "tokens": [["\u2581Uncle", "\u2581Tom", "'", "s", "\u2581Cab", "in", "\u2581#####", "#", "\u2581**", "L", "im", "bo", "**"]], "entropy": [[1.21875, 0.266845703125, 0.07244873046875, 3.713369369506836e-05, 0.07916259765625, 0.002288818359375, 0.76318359375, 1.7265625, 0.19775390625, 4.9609375, 3.83984375, 0.2822265625, 0.9345703125]], "avg_entropy": [1.1034316237156208], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Uncle Tom\u2019s Cabin", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45202235332758534, "std_of_first_answer": 0.45202235332758534, "avg_entropy_of_first_answer": 1.1034316237156208, "lex_sim": 1.0}
{"question": "Who was Jackie Kennedy's second husband", "answer": ["Aristrocratic Charles"], "scores": [[0.004589080810546875, 0.03729248046875, 0.966796875, 0.79931640625, 0.0029964447021484375]], "normalized_score": [0.05243261530995369], "tokens": [["\u2581Ar", "istro", "cr", "atic", "\u2581Charles"]], "entropy": [[1.2607421875, 3.369140625, 0.1748046875, 0.9443359375, 3.4765625]], "avg_entropy": [1.8451171875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u0391\u03c1\u03b9\u03c3\u03c4\u03bf\u03c4\u03ad\u03bb\u03b7\u03c2 \u03a9\u03bd\u03ac\u03c3\u03b7\u03c2", "score_of_biggest_cluster": 0.05243261530995369, "score_of_first_answer": 0.05243261530995369, "max_score_of_first_answer": 0.966796875, "min_score_of_first_answer": 0.0029964447021484375, "avg_score": 0.05243261530995369, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.42873903339154695, "std_of_first_answer": 0.42873903339154695, "avg_entropy_of_first_answer": 1.8451171875, "lex_sim": 1.0}
{"question": "Who became Israel's head of state in 1993?", "answer": ["Ezer Weizman #56432 - 10/21/11 05:21 AM Re: Interesting Facts about History's Most Influential Documents [Re: ke"], "scores": [[0.67236328125, 0.99755859375, 0.99560546875, 0.99755859375, 0.95068359375, 5.960464477539063e-08, 0.0060882568359375, 0.06768798828125, 0.06817626953125, 0.094970703125, 0.05816650390625, 0.005260467529296875, 0.10638427734375, 0.2998046875, 0.09210205078125, 0.25439453125, 0.2327880859375, 0.06475830078125, 0.8837890625, 0.07281494140625, 0.02947998046875, 0.392578125, 0.22509765625, 0.0799560546875, 0.9833984375, 0.19677734375, 0.09417724609375, 0.33251953125, 0.00659942626953125, 0.9892578125, 0.0204620361328125, 0.9990234375, 0.28271484375, 0.99169921875, 0.0026702880859375, 0.04376220703125, 0.0004940032958984375, 0.9931640625, 0.88330078125, 0.673828125, 0.65380859375, 0.99951171875, 8.046627044677734e-05, 0.82421875, 0.77490234375, 1.0, 1.0, 0.00017714500427246094]], "normalized_score": [0.09684111177921295], "tokens": [["\u2581E", "zer", "\u2581We", "iz", "man", "\u2581#", "5", "6", "4", "3", "2", "\u2581-", "\u2581", "1", "0", "/", "2", "1", "/", "1", "1", "\u2581", "0", "5", ":", "2", "1", "\u2581AM", "\u2581Re", ":", "\u2581Interest", "ing", "\u2581Fact", "s", "\u2581about", "\u2581History", "'", "s", "\u2581Most", "\u2581In", "flu", "ential", "\u2581Document", "s", "\u2581[", "Re", ":", "\u2581ke"]], "entropy": [[1.365234375, 0.02276611328125, 0.03948974609375, 0.0188446044921875, 0.20751953125, 0.7734375, 4.53125, 3.533203125, 3.73046875, 3.072265625, 3.53515625, 3.599609375, 6.16015625, 2.33203125, 2.806640625, 3.552734375, 2.09375, 2.607421875, 0.712890625, 0.77490234375, 1.4150390625, 2.52734375, 1.8740234375, 2.37109375, 0.1241455078125, 1.810546875, 2.31640625, 1.5234375, 1.9443359375, 0.091064453125, 5.07421875, 0.01253509521484375, 2.873046875, 0.07373046875, 1.9609375, 4.21484375, 1.5244140625, 0.07330322265625, 0.70068359375, 1.3623046875, 0.66259765625, 0.0035152435302734375, 0.4970703125, 0.7412109375, 1.2275390625, 0.0025119781494140625, 0.00215911865234375, 5.67578125]], "avg_entropy": [1.8364502588907878], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u05e2\u05d6\u05e8 \u05d5\u05d9\u05e6\u05de\u05df", "score_of_biggest_cluster": 0.09684111177921295, "score_of_first_answer": 0.09684111177921295, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.09684111177921295, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4087447576202791, "std_of_first_answer": 0.4087447576202791, "avg_entropy_of_first_answer": 1.8364502588907878, "lex_sim": 1.0}
{"question": "From which 1960s sci-fi movie did 1980s pop sensation Duran Duran take their name from a character from the movie?", "answer": ["Barbarella"], "scores": [[0.6669921875, 0.99853515625, 0.99951171875]], "normalized_score": [0.8731536269187927], "tokens": [["\u2581Bar", "bare", "lla"]], "entropy": [[1.697265625, 0.01824951171875, 0.0028896331787109375]], "avg_entropy": [0.5728015899658203], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Barbarella (disambiguation)", "score_of_biggest_cluster": 0.8731536269187927, "score_of_first_answer": 0.8731536269187927, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.6669921875, "avg_score": 0.8731536269187927, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1565215400386369, "std_of_first_answer": 0.1565215400386369, "avg_entropy_of_first_answer": 0.5728015899658203, "lex_sim": 1.0}
{"question": "Which artist painted the 'Seagram Murals' which were originally commissioned by the Four Seasons Hotel in New York in 1958?", "answer": ["1922\u20131987"], "scores": [[0.0009870529174804688, 0.07269287109375, 0.52587890625, 0.01438140869140625, 0.07470703125, 0.011260986328125, 0.11798095703125, 0.98681640625, 0.281005859375, 0.67529296875]], "normalized_score": [0.07950076460838318], "tokens": [["\u2581", "1", "9", "2", "2", "\u2013", "1", "9", "8", "7"]], "entropy": [[2.029296875, 2.10546875, 2.28515625, 1.75390625, 1.7548828125, 3.662109375, 0.94775390625, 0.09222412109375, 1.4248046875, 1.3115234375]], "avg_entropy": [1.736712646484375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marcus Rothkovich", "score_of_biggest_cluster": 0.07950076460838318, "score_of_first_answer": 0.07950076460838318, "max_score_of_first_answer": 0.98681640625, "min_score_of_first_answer": 0.0009870529174804688, "avg_score": 0.07950076460838318, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32375068855940364, "std_of_first_answer": 0.32375068855940364, "avg_entropy_of_first_answer": 1.736712646484375, "lex_sim": 1.0}
{"question": "In the Bible's New Testament what was the last battle between good and evil before the day of judgement?", "answer": ["The Battle of Armageddon"], "scores": [[0.423828125, 0.8486328125, 0.9990234375, 0.99072265625, 0.99755859375, 1.0]], "normalized_score": [0.8415164947509766], "tokens": [["\u2581The", "\u2581Battle", "\u2581of", "\u2581Arm", "aged", "don"]], "entropy": [[1.736328125, 0.7705078125, 0.010772705078125, 0.0784912109375, 0.01800537109375, 0.0019073486328125]], "avg_entropy": [0.43600209554036456], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Amargeddon", "score_of_biggest_cluster": 0.8415164947509766, "score_of_first_answer": 0.8415164947509766, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.423828125, "avg_score": 0.8415164947509766, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.20962448565493857, "std_of_first_answer": 0.20962448565493857, "avg_entropy_of_first_answer": 0.43600209554036456, "lex_sim": 1.0}
{"question": "What was the name of the building destroyed in the Oklahoma bombing in March 95", "answer": ["Alfred P. Murrah Federal Building #JudicialReform: A New Dawn in Uganda's Judiciary"], "scores": [[0.95947265625, 0.9951171875, 0.9306640625, 0.99609375, 0.9990234375, 0.9814453125, 0.98974609375, 4.172325134277344e-07, 0.00015676021575927734, 0.0109405517578125, 0.25244140625, 0.0008883476257324219, 0.144287109375, 0.0005793571472167969, 0.0057220458984375, 0.025848388671875, 0.06536865234375, 0.99853515625, 0.0282135009765625, 0.0251922607421875, 0.390625, 0.98046875, 0.230712890625, 0.99755859375, 0.326171875, 0.91796875, 0.99853515625]], "normalized_score": [0.07956043630838394], "tokens": [["\u2581Alfred", "\u2581P", ".", "\u2581Mur", "rah", "\u2581Federal", "\u2581Building", "\u2581#", "J", "ud", "icial", "Re", "form", ":", "\u2581A", "\u2581New", "\u2581Da", "wn", "\u2581in", "\u2581U", "g", "anda", "'", "s", "\u2581Jud", "ici", "ary"]], "entropy": [[0.275390625, 0.048431396484375, 0.30419921875, 0.0360107421875, 0.0105743408203125, 0.13720703125, 0.081298828125, 0.96484375, 3.759765625, 5.046875, 1.4775390625, 4.3671875, 0.65087890625, 2.28515625, 5.39453125, 5.91015625, 3.84375, 0.01519012451171875, 3.173828125, 3.734375, 1.2412109375, 0.1103515625, 2.666015625, 0.0235443115234375, 1.9755859375, 0.302978515625, 0.0158843994140625]], "avg_entropy": [1.772324456108941], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Murrah Federal Building", "score_of_biggest_cluster": 0.07956043630838394, "score_of_first_answer": 0.07956043630838394, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 4.172325134277344e-07, "avg_score": 0.07956043630838394, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4473185077043929, "std_of_first_answer": 0.4473185077043929, "avg_entropy_of_first_answer": 1.772324456108941, "lex_sim": 1.0}
{"question": "Aesculus hippocastanum is the Latin name for which tree?", "answer": ["Horse Chestnut tree"], "scores": [[0.94580078125, 0.9990234375, 0.274169921875, 0.99951171875, 0.9990234375, 0.09600830078125]], "normalized_score": [0.5401461124420166], "tokens": [["\u2581Hor", "se", "\u2581Ch", "est", "nut", "\u2581tree"]], "entropy": [[0.27783203125, 0.00917816162109375, 0.90966796875, 0.0028171539306640625, 0.00962066650390625, 1.2392578125]], "avg_entropy": [0.40806229909261066], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Common horse chestnut", "score_of_biggest_cluster": 0.5401461124420166, "score_of_first_answer": 0.5401461124420166, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.09600830078125, "avg_score": 0.5401461124420166, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3814324077068993, "std_of_first_answer": 0.3814324077068993, "avg_entropy_of_first_answer": 0.40806229909261066, "lex_sim": 1.0}
{"question": "Raph Koster, Brad McQuaid, Mark Jacobs, Brian Green and J. Todd Coleman are influential in MMORPG design. What is MMORPG?", "answer": ["A genre of multiplayer online games in which players can interact with each other in a virtual world"], "scores": [[0.1597900390625, 0.052001953125, 0.99951171875, 0.278076171875, 0.9853515625, 0.890625, 0.48681640625, 0.7412109375, 0.99853515625, 0.94482421875, 0.955078125, 0.9716796875, 0.9970703125, 0.96044921875, 0.99951171875, 0.90771484375, 0.90625, 0.7265625, 0.99560546875]], "normalized_score": [0.6592487096786499], "tokens": [["\u2581A", "\u2581genre", "\u2581of", "\u2581multi", "player", "\u2581online", "\u2581games", "\u2581in", "\u2581which", "\u2581players", "\u2581can", "\u2581interact", "\u2581with", "\u2581each", "\u2581other", "\u2581in", "\u2581a", "\u2581virtual", "\u2581world"]], "entropy": [[1.6142578125, 1.0908203125, 0.006732940673828125, 1.8974609375, 0.07745361328125, 0.435791015625, 1.283203125, 1.0908203125, 0.012451171875, 0.2900390625, 0.2100830078125, 0.1417236328125, 0.024261474609375, 0.2135009765625, 0.00463104248046875, 0.345458984375, 0.39501953125, 0.6611328125, 0.03424072265625]], "avg_entropy": [0.5173201310007196], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Massively Multi Player On-line Roleplaying Game", "score_of_biggest_cluster": 0.6592487096786499, "score_of_first_answer": 0.6592487096786499, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.052001953125, "avg_score": 0.6592487096786499, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.29977326329678805, "std_of_first_answer": 0.29977326329678805, "avg_entropy_of_first_answer": 0.5173201310007196, "lex_sim": 1.0}
{"question": "Which English town's Anglo Saxon name was Gipeswic pronounced Gipperswick", "answer": ["Ipswich #HoustonFlood #HoustonStrong: A Tale Of Resilience, Friendship & Gratitude"], "scores": [[0.93115234375, 0.99853515625, 0.99365234375, 1.1920928955078125e-07, 0.004886627197265625, 4.792213439941406e-05, 0.51904296875, 0.0029506683349609375, 0.2548828125, 0.93505859375, 0.73046875, 0.061248779296875, 0.87548828125, 0.9931640625, 0.03582763671875, 1.0, 0.00014698505401611328, 0.0276336669921875, 0.005100250244140625, 0.30859375, 0.02899169921875, 0.2191162109375, 0.98974609375, 0.99951171875, 0.076171875, 0.533203125, 0.9931640625, 0.07305908203125, 0.0252838134765625, 0.96435546875, 0.99462890625]], "normalized_score": [0.07754427194595337], "tokens": [["\u2581I", "ps", "wich", "\u2581#", "H", "ou", "ston", "F", "lo", "od", "\u2581#", "H", "ou", "ston", "Str", "ong", ":", "\u2581A", "\u2581T", "ale", "\u2581Of", "\u2581Res", "ili", "ence", ",", "\u2581Friend", "ship", "\u2581&", "\u2581Gr", "at", "itude"]], "entropy": [[0.35546875, 0.0160064697265625, 0.042999267578125, 1.13671875, 6.05859375, 2.728515625, 2.974609375, 3.88671875, 2.96484375, 0.322998046875, 1.19140625, 4.34375, 0.4521484375, 0.0599365234375, 1.3251953125, 0.00095367431640625, 0.9052734375, 5.375, 5.01953125, 1.107421875, 0.217529296875, 3.166015625, 0.07275390625, 0.004482269287109375, 1.7373046875, 1.7939453125, 0.049163818359375, 1.0966796875, 4.140625, 0.20654296875, 0.041534423828125]], "avg_entropy": [1.7030537512994581], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ipswich, UK", "score_of_biggest_cluster": 0.07754427194595337, "score_of_first_answer": 0.07754427194595337, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.1920928955078125e-07, "avg_score": 0.07754427194595337, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43344781916484126, "std_of_first_answer": 0.43344781916484126, "avg_entropy_of_first_answer": 1.7030537512994581, "lex_sim": 1.0}
{"question": "What is the name for a hare's nest?", "answer": ["form #ChicagoGirl: A Film About a Young Woman Fighting for Freedom in Sudan"], "scores": [[0.2244873046875, 1.1920928955078125e-07, 2.1636486053466797e-05, 0.0005035400390625, 0.0005197525024414062, 0.0288543701171875, 0.01971435546875, 0.09796142578125, 7.867813110351562e-06, 0.1156005859375, 0.010009765625, 0.479248046875, 0.307861328125, 0.0022716522216796875, 0.99951171875, 0.7041015625, 0.0272216796875, 0.9951171875, 0.1390380859375, 0.4404296875, 0.998046875]], "normalized_score": [0.01633845828473568], "tokens": [["\u2581form", "\u2581#", "Ch", "icago", "G", "irl", ":", "\u2581A", "\u2581Film", "\u2581About", "\u2581a", "\u2581Young", "\u2581Woman", "\u2581Fight", "ing", "\u2581for", "\u2581Fre", "edom", "\u2581in", "\u2581Sud", "an"]], "entropy": [[1.5859375, 1.4091796875, 5.08203125, 3.74609375, 3.794921875, 1.431640625, 3.080078125, 4.875, 5.01171875, 2.841796875, 2.013671875, 2.341796875, 1.5986328125, 0.66455078125, 0.00469207763671875, 0.95556640625, 1.1474609375, 0.039154052734375, 2.896484375, 1.802734375, 0.01702880859375]], "avg_entropy": [2.2066748482840404], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Forms", "score_of_biggest_cluster": 0.01633845828473568, "score_of_first_answer": 0.01633845828473568, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 1.1920928955078125e-07, "avg_score": 0.01633845828473568, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35227438624731705, "std_of_first_answer": 0.35227438624731705, "avg_entropy_of_first_answer": 2.2066748482840404, "lex_sim": 1.0}
{"question": "In what year did Edward Vlll abdicate?", "answer": ["1936"], "scores": [[0.65478515625, 0.99169921875, 0.97900390625, 0.970703125, 0.9970703125]], "normalized_score": [0.9074341058731079], "tokens": [["\u2581", "1", "9", "3", "6"]], "entropy": [[1.5009765625, 0.06268310546875, 0.141357421875, 0.1513671875, 0.02410888671875]], "avg_entropy": [0.3760986328125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and thirty-six", "score_of_biggest_cluster": 0.9074341058731079, "score_of_first_answer": 0.9074341058731079, "max_score_of_first_answer": 0.9970703125, "min_score_of_first_answer": 0.65478515625, "avg_score": 0.9074341058731079, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.13225937496769635, "std_of_first_answer": 0.13225937496769635, "avg_entropy_of_first_answer": 0.3760986328125, "lex_sim": 1.0}
{"question": "Whose armies were defeated at the Battle of Waterloo in 1815?", "answer": ["The First French Empire of Napoleon Bonaparte"], "scores": [[0.060272216796875, 0.0011892318725585938, 0.9794921875, 0.99951171875, 0.294189453125, 0.96240234375, 0.94921875, 1.0, 0.99951171875]], "normalized_score": [0.2985619604587555], "tokens": [["\u2581The", "\u2581First", "\u2581French", "\u2581Empire", "\u2581of", "\u2581Napoleon", "\u2581Bon", "ap", "arte"]], "entropy": [[0.830078125, 1.14453125, 0.1727294921875, 0.0027256011962890625, 1.921875, 0.2113037109375, 0.266357421875, 0.002227783203125, 0.00646209716796875]], "avg_entropy": [0.5064767201741537], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Napoleon Buonaparte", "score_of_biggest_cluster": 0.2985619604587555, "score_of_first_answer": 0.2985619604587555, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0011892318725585938, "avg_score": 0.2985619604587555, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4137147662353386, "std_of_first_answer": 0.4137147662353386, "avg_entropy_of_first_answer": 0.5064767201741537, "lex_sim": 1.0}
{"question": "What is the name of the charity for single parents founded in the 1970s which currently has J K Rowling as its President?", "answer": ["Gingerbread (charity)"], "scores": [[0.9765625, 0.9990234375, 0.99951171875, 0.99951171875, 0.1129150390625, 0.045806884765625, 0.9990234375, 0.94580078125]], "normalized_score": [0.5125529170036316], "tokens": [["\u2581G", "inger", "b", "read", "\u2581(", "char", "ity", ")"]], "entropy": [[0.171630859375, 0.0077362060546875, 0.003864288330078125, 0.005207061767578125, 0.80859375, 3.833984375, 0.00910186767578125, 0.29150390625]], "avg_entropy": [0.6414527893066406], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gingerbreads", "score_of_biggest_cluster": 0.5125529170036316, "score_of_first_answer": 0.5125529170036316, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.045806884765625, "avg_score": 0.5125529170036316, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39357459714779497, "std_of_first_answer": 0.39357459714779497, "avg_entropy_of_first_answer": 0.6414527893066406, "lex_sim": 1.0}
{"question": "Which word can go before Side, End and Ability to make three other words?", "answer": ["Pre- \\Assistant"], "scores": [[0.45654296875, 0.433349609375, 0.0006618499755859375, 7.152557373046875e-07, 0.325439453125]], "normalized_score": [0.007885008119046688], "tokens": [["\u2581Pre", "-", "\u2581\\", "Ass", "istant"]], "entropy": [[2.01953125, 1.7177734375, 0.9091796875, 1.931640625, 2.26171875]], "avg_entropy": [1.76796875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sea port", "score_of_biggest_cluster": 0.007885008119046688, "score_of_first_answer": 0.007885008119046688, "max_score_of_first_answer": 0.45654296875, "min_score_of_first_answer": 7.152557373046875e-07, "avg_score": 0.007885008119046688, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2031777892498952, "std_of_first_answer": 0.2031777892498952, "avg_entropy_of_first_answer": 1.76796875, "lex_sim": 1.0}
{"question": "Can you name the river that flows through Cardiff?", "answer": ["Taff Tags: Acer, Android, Dual-core, Honeycomb, nvidia, Tablet, Tegra 2, touchpad, Transformer"], "scores": [[0.6396484375, 0.97265625, 0.0, 0.626953125, 0.0152435302734375, 1.329183578491211e-05, 0.264404296875, 0.00012683868408203125, 0.88330078125, 0.004505157470703125, 0.00855255126953125, 0.218505859375, 0.68408203125, 0.8115234375, 0.01580810546875, 0.250732421875, 0.99609375, 0.86572265625, 0.0010051727294921875, 0.15869140625, 0.347900390625, 0.232177734375, 0.9541015625, 0.50439453125, 0.341796875, 0.9990234375, 1.0, 0.2156982421875, 0.97314453125, 0.5, 0.00933074951171875, 0.00928497314453125, 0.60546875, 0.1685791015625, 0.8623046875]], "normalized_score": [0.0], "tokens": [["\u2581T", "aff", "\u2581Tags", ":", "\u2581A", "cer", ",", "\u2581Android", ",", "\u2581D", "ual", "-", "core", ",", "\u2581H", "oney", "comb", ",", "\u2581n", "vidia", ",", "\u2581Table", "t", ",", "\u2581T", "eg", "ra", "\u2581", "2", ",", "\u2581touch", "pad", ",", "\u2581Trans", "former"]], "entropy": [[1.04296875, 0.1419677734375, 0.94970703125, 1.7919921875, 3.837890625, 2.79296875, 4.39453125, 6.25390625, 0.90478515625, 5.65625, 4.34375, 3.0546875, 1.8017578125, 0.8701171875, 3.939453125, 2.611328125, 0.032318115234375, 0.7177734375, 4.12890625, 2.63671875, 1.8525390625, 3.078125, 0.2044677734375, 1.9677734375, 3.615234375, 0.01197052001953125, 0.00180816650390625, 2.015625, 0.1337890625, 1.86328125, 4.03125, 1.5791015625, 1.5458984375, 3.94921875, 0.427001953125]], "avg_entropy": [2.233738926478795], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Taff Valley", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37378468800391573, "std_of_first_answer": 0.37378468800391573, "avg_entropy_of_first_answer": 2.233738926478795, "lex_sim": 1.0}
{"question": "SS Thistlegorm is the name of the sunken WWII munitions ship which is an attraction to divers in which area of water?", "answer": ["The Red Sea"], "scores": [[0.348388671875, 0.9892578125, 0.98828125]], "normalized_score": [0.6983685493469238], "tokens": [["\u2581The", "\u2581Red", "\u2581Sea"]], "entropy": [[1.4599609375, 0.09808349609375, 0.07867431640625]], "avg_entropy": [0.5455729166666666], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Red Sea (disambiguation)", "score_of_biggest_cluster": 0.6983685493469238, "score_of_first_answer": 0.6983685493469238, "max_score_of_first_answer": 0.9892578125, "min_score_of_first_answer": 0.348388671875, "avg_score": 0.6983685493469238, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30187869539844453, "std_of_first_answer": 0.30187869539844453, "avg_entropy_of_first_answer": 0.5455729166666666, "lex_sim": 1.0}
{"question": "The Corryvreckan whirlpool lies to the north of which Scottish island?", "answer": ["Jura"], "scores": [[0.8173828125, 0.9990234375]], "normalized_score": [0.90365070104599], "tokens": [["\u2581J", "ura"]], "entropy": [[0.89208984375, 0.009735107421875]], "avg_entropy": [0.4509124755859375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jura (disambiguation)", "score_of_biggest_cluster": 0.90365070104599, "score_of_first_answer": 0.90365070104599, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.8173828125, "avg_score": 0.90365070104599, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0908203125, "std_of_first_answer": 0.0908203125, "avg_entropy_of_first_answer": 0.4509124755859375, "lex_sim": 1.0}
{"question": "What would you find at 1600 Pennsylvania Avenue, Washington?", "answer": ["The White House"], "scores": [[0.93017578125, 0.99755859375, 0.9970703125]], "normalized_score": [0.9744130373001099], "tokens": [["\u2581The", "\u2581White", "\u2581House"]], "entropy": [[0.381591796875, 0.024810791015625, 0.023345947265625]], "avg_entropy": [0.14324951171875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "White House", "score_of_biggest_cluster": 0.9744130373001099, "score_of_first_answer": 0.9744130373001099, "max_score_of_first_answer": 0.99755859375, "min_score_of_first_answer": 0.93017578125, "avg_score": 0.9744130373001099, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03165010119370602, "std_of_first_answer": 0.03165010119370602, "avg_entropy_of_first_answer": 0.14324951171875, "lex_sim": 1.0}
{"question": "Which company merged with BOAC in 1974 to form British Airways?", "answer": ["BOAC"], "scores": [[0.0243988037109375, 0.99658203125]], "normalized_score": [0.15593399107456207], "tokens": [["\u2581BO", "AC"]], "entropy": [[0.447265625, 0.03021240234375]], "avg_entropy": [0.238739013671875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bea (disambiguation)", "score_of_biggest_cluster": 0.15593399107456207, "score_of_first_answer": 0.15593399107456207, "max_score_of_first_answer": 0.99658203125, "min_score_of_first_answer": 0.0243988037109375, "avg_score": 0.15593399107456207, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.48609161376953125, "std_of_first_answer": 0.48609161376953125, "avg_entropy_of_first_answer": 0.238739013671875, "lex_sim": 1.0}
{"question": "If I Were A Rich Man Was a big hit from which stage show?", "answer": ["Fiddler on the Roof"], "scores": [[0.60400390625, 0.998046875, 1.0, 0.984375, 0.99560546875, 0.9365234375, 0.99951171875]], "normalized_score": [0.9188598990440369], "tokens": [["\u2581F", "idd", "ler", "\u2581on", "\u2581the", "\u2581Ro", "of"]], "entropy": [[1.607421875, 0.0198974609375, 0.00020051002502441406, 0.0987548828125, 0.0299835205078125, 0.253662109375, 0.0057525634765625]], "avg_entropy": [0.28795327459062847], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fiddler on a Roof", "score_of_biggest_cluster": 0.9188598990440369, "score_of_first_answer": 0.9188598990440369, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.60400390625, "avg_score": 0.9188598990440369, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13518753508684572, "std_of_first_answer": 0.13518753508684572, "avg_entropy_of_first_answer": 0.28795327459062847, "lex_sim": 1.0}
{"question": "In which country was the actor Sid James born?", "answer": ["South Africa #ChurchToo: Sexual Abuse in Evangelical Congregations"], "scores": [[0.921875, 0.99365234375, 5.960464477539062e-07, 0.00024628639221191406, 0.00760650634765625, 4.57763671875e-05, 0.036468505859375, 0.00821685791015625, 0.004390716552734375, 0.8759765625, 0.052520751953125, 0.99267578125, 0.27978515625, 0.0031795501708984375, 0.99951171875, 0.1077880859375, 0.99365234375, 0.99755859375]], "normalized_score": [0.029387645423412323], "tokens": [["\u2581South", "\u2581Africa", "\u2581#", "Ch", "urch", "To", "o", ":", "\u2581Sex", "ual", "\u2581Ab", "use", "\u2581in", "\u2581Evangel", "ical", "\u2581Cong", "reg", "ations"]], "entropy": [[0.583984375, 0.045684814453125, 0.697265625, 5.9921875, 3.728515625, 0.71630859375, 4.03125, 2.375, 4.81640625, 0.72509765625, 2.15625, 0.054412841796875, 2.345703125, 1.650390625, 0.004627227783203125, 2.107421875, 0.0528564453125, 0.0205078125]], "avg_entropy": [1.783548355102539], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "South africa", "score_of_biggest_cluster": 0.029387645423412323, "score_of_first_answer": 0.029387645423412323, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 5.960464477539062e-07, "avg_score": 0.029387645423412323, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4549198335960106, "std_of_first_answer": 0.4549198335960106, "avg_entropy_of_first_answer": 1.783548355102539, "lex_sim": 1.0}
{"question": "in which US State is the Chambers Bay Course, the venue for the 2015 US Open Golf Championship?", "answer": ["State of Washington #GivingTuesday 2018: Our Focus on Comunity Impac..."], "scores": [[0.01018524169921875, 0.99169921875, 0.98876953125, 1.9669532775878906e-06, 0.006923675537109375, 8.976459503173828e-05, 0.80419921875, 0.998046875, 0.99609375, 0.00563812255859375, 0.404541015625, 0.95654296875, 0.252197265625, 0.1768798828125, 0.024017333984375, 0.0021514892578125, 0.0316162109375, 0.61083984375, 0.2342529296875, 0.0007734298706054688, 0.0011587142944335938, 0.012176513671875, 0.00749969482421875, 0.024261474609375]], "normalized_score": [0.030562061816453934], "tokens": [["\u2581State", "\u2581of", "\u2581Washington", "\u2581#", "G", "iving", "T", "ues", "day", "\u2581", "2", "0", "1", "8", ":", "\u2581Our", "\u2581F", "ocus", "\u2581on", "\u2581Com", "unity", "\u2581Imp", "ac", "..."]], "entropy": [[0.85595703125, 0.07012939453125, 0.0955810546875, 0.91162109375, 5.03515625, 0.69140625, 1.162109375, 0.0204315185546875, 0.031036376953125, 2.419921875, 2.33203125, 0.319580078125, 0.62158203125, 1.4912109375, 3.560546875, 5.1484375, 5.4453125, 0.87158203125, 3.19140625, 4.7890625, 1.1376953125, 4.26953125, 0.1832275390625, 2.703125]], "avg_entropy": [1.9732367197672527], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Washington (Amtrak station)", "score_of_biggest_cluster": 0.030562061816453934, "score_of_first_answer": 0.030562061816453934, "max_score_of_first_answer": 0.998046875, "min_score_of_first_answer": 1.9669532775878906e-06, "avg_score": 0.030562061816453934, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3995351620278114, "std_of_first_answer": 0.3995351620278114, "avg_entropy_of_first_answer": 1.9732367197672527, "lex_sim": 1.0}
{"question": "Which opera by Rossini , his last , is based on a play by Schiller ?", "answer": ["William Tell"], "scores": [[0.61279296875, 0.87841796875]], "normalized_score": [0.7336813807487488], "tokens": [["\u2581William", "\u2581Tell"]], "entropy": [[1.5244140625, 0.48486328125]], "avg_entropy": [1.004638671875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "There But for the Grace of Grace", "score_of_biggest_cluster": 0.7336813807487488, "score_of_first_answer": 0.7336813807487488, "max_score_of_first_answer": 0.87841796875, "min_score_of_first_answer": 0.61279296875, "avg_score": 0.7336813807487488, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1328125, "std_of_first_answer": 0.1328125, "avg_entropy_of_first_answer": 1.004638671875, "lex_sim": 1.0}
{"question": "Films. Which Dominic starred as Merry (Meriadoc) in \u2018The Lord of the Rings\u2019?", "answer": ["Dominic Monaghan"], "scores": [[0.89794921875, 0.99951171875, 0.99755859375, 0.99951171875, 1.0]], "normalized_score": [0.9780322909355164], "tokens": [["\u2581Domin", "ic", "\u2581Mon", "ag", "han"]], "entropy": [[0.6884765625, 0.0040283203125, 0.019561767578125, 0.005130767822265625, 0.00012302398681640625]], "avg_entropy": [0.1434640884399414], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Monaghan Urban Council", "score_of_biggest_cluster": 0.9780322909355164, "score_of_first_answer": 0.9780322909355164, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.89794921875, "avg_score": 0.9780322909355164, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04048720244733783, "std_of_first_answer": 0.04048720244733783, "avg_entropy_of_first_answer": 0.1434640884399414, "lex_sim": 1.0}
{"question": "Caroline of Brunswick was the queen of which British king?", "answer": ["King George IV ##### CLOSE ONE TACTICAL COMPLETE **COPY** \\*\\*\\*"], "scores": [[0.43115234375, 0.998046875, 0.9931640625, 0.0, 0.00021564960479736328, 0.001979827880859375, 0.55517578125, 0.0004382133483886719, 0.334716796875, 0.0249481201171875, 0.00015056133270263672, 0.8271484375, 0.0226287841796875, 0.0266876220703125, 0.3310546875, 0.361572265625, 0.0002570152282714844, 0.0029582977294921875, 0.1846923828125, 0.427490234375, 0.002826690673828125, 0.39404296875, 0.18505859375, 0.79443359375]], "normalized_score": [0.0], "tokens": [["\u2581King", "\u2581George", "\u2581IV", "\u2581#####", "\u2581C", "LO", "SE", "\u2581ON", "E", "\u2581T", "ACT", "IC", "AL", "\u2581CO", "MP", "LETE", "\u2581**", "CO", "PY", "**", "\u2581\\", "*\\", "*\\", "*"]], "entropy": [[1.451171875, 0.01702880859375, 0.058441162109375, 0.9072265625, 2.537109375, 3.61328125, 1.0068359375, 2.279296875, 3.064453125, 3.638671875, 2.419921875, 1.212890625, 3.59765625, 3.693359375, 1.4814453125, 0.81298828125, 2.57421875, 2.978515625, 2.28125, 2.83203125, 2.390625, 1.1669921875, 0.55810546875, 0.6796875]], "avg_entropy": [1.9688835144042969], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George, Prince Regent", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.998046875, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32712521298525715, "std_of_first_answer": 0.32712521298525715, "avg_entropy_of_first_answer": 1.9688835144042969, "lex_sim": 1.0}
{"question": "What river separates Texas and Mexico?", "answer": ["Rio Grande #225871 - 10/31/17 02:56 AM Re: Manson's Guitar [Re: Marty]"], "scores": [[0.8974609375, 0.95556640625, 4.172325134277344e-07, 0.00931549072265625, 0.1463623046875, 0.06390380859375, 0.08172607421875, 0.03057861328125, 0.06878662109375, 0.0005316734313964844, 0.1456298828125, 0.21533203125, 0.09967041015625, 0.140625, 0.0904541015625, 0.2822265625, 0.74365234375, 0.0238189697265625, 0.05511474609375, 0.146240234375, 0.19140625, 0.0867919921875, 0.98583984375, 0.1539306640625, 0.09698486328125, 0.0660400390625, 0.005855560302734375, 0.91845703125, 0.0031948089599609375, 0.002048492431640625, 0.18505859375, 1.0, 0.0005245208740234375, 0.033721923828125, 0.0023441314697265625, 0.99951171875, 0.9990234375, 0.0034236907958984375, 0.86962890625, 0.431640625]], "normalized_score": [0.0575680211186409], "tokens": [["\u2581Rio", "\u2581Grande", "\u2581#", "2", "2", "5", "8", "7", "1", "\u2581-", "\u2581", "1", "0", "/", "3", "1", "/", "1", "7", "\u2581", "0", "2", ":", "5", "6", "\u2581AM", "\u2581Re", ":", "\u2581M", "anson", "'", "s", "\u2581Gu", "itar", "\u2581[", "Re", ":", "\u2581Mart", "y", "]"]], "entropy": [[0.52392578125, 0.255126953125, 0.91748046875, 4.9765625, 3.046875, 3.326171875, 2.83203125, 3.05859375, 2.755859375, 2.6640625, 5.77734375, 1.90234375, 3.22265625, 3.59375, 1.96484375, 1.4111328125, 0.99755859375, 0.68701171875, 1.4404296875, 2.662109375, 2.169921875, 2.423828125, 0.11480712890625, 1.8369140625, 2.3203125, 2.419921875, 2.83203125, 0.4521484375, 4.53515625, 3.611328125, 4.359375, 0.0023441314697265625, 5.40625, 1.044921875, 3.25390625, 0.005390167236328125, 0.0103607177734375, 5.8359375, 0.6669921875, 2.779296875]], "avg_entropy": [2.3524253368377686], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rio Grande del Norte", "score_of_biggest_cluster": 0.0575680211186409, "score_of_first_answer": 0.0575680211186409, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 4.172325134277344e-07, "avg_score": 0.0575680211186409, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3618560427133338, "std_of_first_answer": 0.3618560427133338, "avg_entropy_of_first_answer": 2.3524253368377686, "lex_sim": 1.0}
{"question": "Who was the Republican Presidential Nominee in the 2008 American Election?", "answer": ["Sen. John McCain #1 Post by Hokahey \u00bb Thu Apr 23, 2015 8:59 pm"], "scores": [[0.0120849609375, 0.97265625, 0.99658203125, 0.90576171875, 0.9990234375, 3.5762786865234375e-07, 0.1588134765625, 5.304813385009766e-06, 0.0021076202392578125, 0.00460052490234375, 1.609325408935547e-05, 0.66943359375, 1.0, 0.3505859375, 0.154052734375, 0.998046875, 0.09539794921875, 1.0, 0.9990234375, 0.307861328125, 0.07867431640625, 0.9990234375, 0.9921875, 0.99951171875, 1.0, 0.01064300537109375, 0.99072265625, 0.99462890625, 0.0850830078125, 0.99951171875, 0.1519775390625, 0.0960693359375, 0.6435546875]], "normalized_score": [0.0968235656619072], "tokens": [["\u2581Sen", ".", "\u2581John", "\u2581McC", "ain", "\u2581#", "1", "\u2581Post", "\u2581by", "\u2581H", "oka", "he", "y", "\u2581\u00bb", "\u2581Th", "u", "\u2581A", "pr", "\u2581", "2", "3", ",", "\u2581", "2", "0", "1", "5", "\u2581", "8", ":", "5", "9", "\u2581pm"]], "entropy": [[0.93896484375, 0.171875, 0.0288848876953125, 0.42919921875, 0.0092010498046875, 1.3515625, 3.796875, 3.00390625, 2.248046875, 6.5078125, 5.4296875, 1.6796875, 0.00017952919006347656, 2.84375, 2.376953125, 0.017974853515625, 2.525390625, 0.00075531005859375, 0.00943756103515625, 1.1220703125, 1.943359375, 0.01097869873046875, 0.06622314453125, 0.0026073455810546875, 0.00021946430206298828, 0.0635986328125, 0.07000732421875, 0.051361083984375, 1.9189453125, 0.006420135498046875, 1.7939453125, 2.302734375, 0.6640625]], "avg_entropy": [1.3147477923017559], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jon McCain", "score_of_biggest_cluster": 0.0968235656619072, "score_of_first_answer": 0.0968235656619072, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 3.5762786865234375e-07, "avg_score": 0.0968235656619072, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4413585939065428, "std_of_first_answer": 0.4413585939065428, "avg_entropy_of_first_answer": 1.3147477923017559, "lex_sim": 1.0}
{"question": "\"In which \"\" Star Wars\"\" episode does Anakin Skywalker transform into the black-suited Darth Vader?\"", "answer": ["Revenge of the Sith"], "scores": [[0.6181640625, 0.99462890625, 0.9990234375, 0.94873046875, 0.9921875, 0.95263671875, 1.0]], "normalized_score": [0.9183339476585388], "tokens": [["\u2581Re", "ven", "ge", "\u2581of", "\u2581the", "\u2581S", "ith"]], "entropy": [[1.89453125, 0.048675537109375, 0.01319122314453125, 0.2880859375, 0.0556640625, 0.30126953125, 0.0017290115356445312]], "avg_entropy": [0.3718780790056501], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Star Wars: Episode III Revenge of the Sith", "score_of_biggest_cluster": 0.9183339476585388, "score_of_first_answer": 0.9183339476585388, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.6181640625, "avg_score": 0.9183339476585388, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.12862473548378817, "std_of_first_answer": 0.12862473548378817, "avg_entropy_of_first_answer": 0.3718780790056501, "lex_sim": 1.0}
{"question": "Charles Rennie Mackintosh was a leading exponent of which art movement around the 1900's?", "answer": ["Art Nouveau #56: David vs. Goliath: How Small Businesses Can Compete with Big Brands 0"], "scores": [[0.86865234375, 0.85498046875, 0.9814453125, 0.9990234375, 2.086162567138672e-06, 0.0034961700439453125, 0.052001953125, 0.002002716064453125, 4.5239925384521484e-05, 8.45193862915039e-05, 0.513671875, 0.69677734375, 0.9990234375, 0.9951171875, 0.0814208984375, 0.0172576904296875, 0.060272216796875, 0.061798095703125, 0.85986328125, 0.64453125, 0.62939453125, 1.0, 0.251953125, 0.728515625, 0.75341796875, 0.99951171875, 0.0084228515625, 0.020843505859375]], "normalized_score": [0.07513109594583511], "tokens": [["\u2581Art", "\u2581N", "ouve", "au", "\u2581#", "5", "6", ":", "\u2581David", "\u2581vs", ".", "\u2581G", "oli", "ath", ":", "\u2581How", "\u2581Small", "\u2581Business", "es", "\u2581Can", "\u2581Comp", "ete", "\u2581with", "\u2581Big", "\u2581Br", "ands", "\u2581", "0"]], "entropy": [[0.6875, 0.6025390625, 0.1033935546875, 0.0119781494140625, 0.900390625, 5.22265625, 3.74609375, 3.2421875, 4.71484375, 3.533203125, 1.7392578125, 1.98046875, 0.011993408203125, 0.03924560546875, 3.326171875, 5.1171875, 4.109375, 3.40625, 0.6298828125, 1.7353515625, 1.6611328125, 0.0021915435791015625, 1.4873046875, 1.09375, 1.046875, 0.00394439697265625, 3.337890625, 3.439453125]], "avg_entropy": [2.033304010118757], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Art-Nouveau", "score_of_biggest_cluster": 0.07513109594583511, "score_of_first_answer": 0.07513109594583511, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.086162567138672e-06, "avg_score": 0.07513109594583511, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.414055551223057, "std_of_first_answer": 0.414055551223057, "avg_entropy_of_first_answer": 2.033304010118757, "lex_sim": 1.0}
{"question": "Name the manufacturer of the British World War II bomber the Lancaster", "answer": ["Avro"], "scores": [[0.947265625, 1.0]], "normalized_score": [0.9732757210731506], "tokens": [["\u2581Av", "ro"]], "entropy": [[0.33349609375, 0.002429962158203125]], "avg_entropy": [0.16796302795410156], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "AVRO (television)", "score_of_biggest_cluster": 0.9732757210731506, "score_of_first_answer": 0.9732757210731506, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.947265625, "avg_score": 0.9732757210731506, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0263671875, "std_of_first_answer": 0.0263671875, "avg_entropy_of_first_answer": 0.16796302795410156, "lex_sim": 1.0}
{"question": "in which century was St Bartholomew's Hospital located in West Smithfieid, London founded?", "answer": ["St Bartholomew's Hospital was founded in the 12th century in West Smithfield, London."], "scores": [[0.006351470947265625, 0.85888671875, 1.0, 0.99951171875, 1.0, 1.0, 0.98388671875, 1.0, 0.97509765625, 0.79443359375, 0.92578125, 0.99658203125, 0.90185546875, 0.94140625, 0.9990234375, 0.97021484375, 0.99560546875, 0.98876953125, 0.450927734375, 0.88671875, 0.99951171875, 0.99462890625, 0.99609375, 0.9990234375, 0.91943359375]], "normalized_score": [0.7622860074043274], "tokens": [["\u2581St", "\u2581Bar", "th", "ol", "ome", "w", "'", "s", "\u2581Hospital", "\u2581was", "\u2581founded", "\u2581in", "\u2581the", "\u2581", "1", "2", "th", "\u2581century", "\u2581in", "\u2581West", "\u2581Smith", "field", ",", "\u2581London", "."]], "entropy": [[1.673828125, 0.486328125, 0.002140045166015625, 0.00443267822265625, 0.0009493827819824219, 0.0017023086547851562, 0.1234130859375, 0.0010557174682617188, 0.1746826171875, 0.79052734375, 0.37646484375, 0.03155517578125, 0.372314453125, 0.34619140625, 0.0093536376953125, 0.163330078125, 0.035430908203125, 0.07672119140625, 1.31640625, 0.42236328125, 0.0030918121337890625, 0.043426513671875, 0.0296630859375, 0.00803375244140625, 0.379150390625]], "avg_entropy": [0.2749022483825684], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "12th century", "score_of_biggest_cluster": 0.7622860074043274, "score_of_first_answer": 0.7622860074043274, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.006351470947265625, "avg_score": 0.7622860074043274, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2151580499311101, "std_of_first_answer": 0.2151580499311101, "avg_entropy_of_first_answer": 0.2749022483825684, "lex_sim": 1.0}
{"question": "\"Who composed \"\"Flight of the Valkyries\"\"?\"", "answer": ["Richard Wagner"], "scores": [[0.9609375, 0.98193359375]], "normalized_score": [0.9713788032531738], "tokens": [["\u2581Richard", "\u2581Wagner"]], "entropy": [[0.304443359375, 0.1492919921875]], "avg_entropy": [0.22686767578125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wagner's", "score_of_biggest_cluster": 0.9713788032531738, "score_of_first_answer": 0.9713788032531738, "max_score_of_first_answer": 0.98193359375, "min_score_of_first_answer": 0.9609375, "avg_score": 0.9713788032531738, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.010498046875, "std_of_first_answer": 0.010498046875, "avg_entropy_of_first_answer": 0.22686767578125, "lex_sim": 1.0}
{"question": "Which hymn traditionally closes The Last Night of the Proms?", "answer": ["Rule, Britannia!"], "scores": [[0.005390167236328125, 0.93408203125, 0.8896484375, 1.0, 0.9716796875]], "normalized_score": [0.3370899260044098], "tokens": [["\u2581Rule", ",", "\u2581Britann", "ia", "!"]], "entropy": [[0.85302734375, 0.3232421875, 0.459228515625, 3.4868717193603516e-05, 0.1708984375]], "avg_entropy": [0.3612862706184387], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Neighbourhoods of Jerusalem", "score_of_biggest_cluster": 0.3370899260044098, "score_of_first_answer": 0.3370899260044098, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.005390167236328125, "avg_score": 0.3370899260044098, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3791984790714476, "std_of_first_answer": 0.3791984790714476, "avg_entropy_of_first_answer": 0.3612862706184387, "lex_sim": 1.0}
{"question": "Which film starring Steve McQueen featured a car chase through the streets of San Francisco?", "answer": ["Bullitt (film)"], "scores": [[0.93359375, 0.95556640625, 0.094482421875, 0.007305145263671875, 0.99609375]], "normalized_score": [0.2277926802635193], "tokens": [["\u2581Bull", "itt", "\u2581(", "film", ")"]], "entropy": [[0.381103515625, 0.1851806640625, 0.87646484375, 0.09259033203125, 0.03460693359375]], "avg_entropy": [0.3139892578125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bullitt", "score_of_biggest_cluster": 0.2277926802635193, "score_of_first_answer": 0.2277926802635193, "max_score_of_first_answer": 0.99609375, "min_score_of_first_answer": 0.007305145263671875, "avg_score": 0.2277926802635193, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4475274619559252, "std_of_first_answer": 0.4475274619559252, "avg_entropy_of_first_answer": 0.3139892578125, "lex_sim": 1.0}
{"question": "Ray Harryhausen is famous in the world of cinema for what?", "answer": ["Ray Harryhausen is famous in the world of cinema for his work as a motion picture special effects creator."], "scores": [[0.035186767578125, 0.99755859375, 1.0, 0.69873046875, 0.58447265625, 0.5859375, 0.99951171875, 0.99951171875, 0.99951171875, 0.9990234375, 0.98486328125, 0.6103515625, 0.70263671875, 0.59912109375, 0.97998046875, 0.0002818107604980469, 0.75537109375, 0.56689453125, 0.9853515625, 0.75439453125, 0.99951171875, 0.6435546875, 2.384185791015625e-07, 1.7881393432617188e-06, 0.006908416748046875, 0.2061767578125, 0.0760498046875, 0.134765625, 0.0008325576782226562, 0.0029354095458984375, 0.0024509429931640625, 0.41357421875, 0.465576171875, 0.380615234375, 0.025054931640625, 0.036376953125, 0.9873046875]], "normalized_score": [0.10296450555324554], "tokens": [["\u2581Ray", "\u2581Harry", "hausen", "\u2581is", "\u2581famous", "\u2581in", "\u2581the", "\u2581world", "\u2581of", "\u2581cinema", "\u2581for", "\u2581his", "\u2581work", "\u2581as", "\u2581a", "\u2581motion", "\u2581picture", "\u2581special", "\u2581effects", "\u2581cre", "ator", ".", "\u2581#", "Book", "Re", "view", ":", "\u2581The", "\u2581F", "allen", "\u2581Arch", "itect", "\u2581by", "\u2581Charles", "\u2581B", "elf", "oure"]], "entropy": [[2.51171875, 0.0244293212890625, 0.002033233642578125, 0.7099609375, 1.1396484375, 0.734375, 0.005859375, 0.004352569580078125, 0.00342559814453125, 0.0099945068359375, 0.08203125, 1.2001953125, 1.005859375, 0.78173828125, 0.10296630859375, 0.826171875, 0.9482421875, 1.046875, 0.09344482421875, 0.890625, 0.005748748779296875, 0.9892578125, 0.8203125, 4.15625, 3.966796875, 0.623046875, 2.91796875, 2.740234375, 1.9951171875, 4.4453125, 2.216796875, 1.845703125, 1.927734375, 3.859375, 5.12890625, 1.3876953125, 0.10223388671875]], "avg_entropy": [1.3852010159879118], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brief history of special effects", "score_of_biggest_cluster": 0.10296450555324554, "score_of_first_answer": 0.10296450555324554, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.384185791015625e-07, "avg_score": 0.10296450555324554, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3944479873901153, "std_of_first_answer": 0.3944479873901153, "avg_entropy_of_first_answer": 1.3852010159879118, "lex_sim": 1.0}
{"question": "Phlebitis refers to inflammation of what part of the human body?", "answer": ["Venous"], "scores": [[0.00969696044921875, 0.96826171875]], "normalized_score": [0.09689785540103912], "tokens": [["\u2581Ven", "ous"]], "entropy": [[2.376953125, 0.21728515625]], "avg_entropy": [1.297119140625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Vein valves", "score_of_biggest_cluster": 0.09689785540103912, "score_of_first_answer": 0.09689785540103912, "max_score_of_first_answer": 0.96826171875, "min_score_of_first_answer": 0.00969696044921875, "avg_score": 0.09689785540103912, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4792823791503906, "std_of_first_answer": 0.4792823791503906, "avg_entropy_of_first_answer": 1.297119140625, "lex_sim": 1.0}
{"question": "Who was British Prime Minister at the end of the Second World War?", "answer": ["Winston Churchill"], "scores": [[0.89453125, 0.99658203125, 0.99951171875, 0.99169921875, 0.99560546875]], "normalized_score": [0.9747039675712585], "tokens": [["\u2581W", "inst", "on", "\u2581Church", "ill"]], "entropy": [[0.57666015625, 0.0286102294921875, 0.004398345947265625, 0.07342529296875, 0.03515625]], "avg_entropy": [0.14365005493164062], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Premiership of Clement Attlee", "score_of_biggest_cluster": 0.9747039675712585, "score_of_first_answer": 0.9747039675712585, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.89453125, "avg_score": 0.9747039675712585, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04060433669207176, "std_of_first_answer": 0.04060433669207176, "avg_entropy_of_first_answer": 0.14365005493164062, "lex_sim": 1.0}
{"question": "What was the name of the first wife of former British Prime Minister Margaret Thatcher\u2019s husband Denis?", "answer": ["Beatrice"], "scores": [[0.3515625, 0.97509765625]], "normalized_score": [0.5854979157447815], "tokens": [["\u2581Be", "atrice"]], "entropy": [[2.677734375, 0.2171630859375]], "avg_entropy": [1.44744873046875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Margrete", "score_of_biggest_cluster": 0.5854979157447815, "score_of_first_answer": 0.5854979157447815, "max_score_of_first_answer": 0.97509765625, "min_score_of_first_answer": 0.3515625, "avg_score": 0.5854979157447815, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.311767578125, "std_of_first_answer": 0.311767578125, "avg_entropy_of_first_answer": 1.44744873046875, "lex_sim": 1.0}
{"question": "What city has a neighborhood called Little Havana?", "answer": ["Miami, Florida, USA #15 | Ranking the Best Movies of 1994"], "scores": [[0.98193359375, 0.6044921875, 0.92138671875, 0.0892333984375, 0.68896484375, 5.960464477539063e-08, 0.0136871337890625, 0.0217437744140625, 0.0001437664031982422, 0.0011663436889648438, 0.234375, 0.0195159912109375, 0.012939453125, 0.008270263671875, 0.9931640625, 0.9111328125, 0.434326171875, 0.175537109375, 0.99169921875, 0.225830078125, 0.07440185546875]], "normalized_score": [0.04516242444515228], "tokens": [["\u2581Miami", ",", "\u2581Florida", ",", "\u2581USA", "\u2581#", "1", "5", "\u2581|", "\u2581Rank", "ing", "\u2581the", "\u2581Best", "\u2581Mov", "ies", "\u2581of", "\u2581", "1", "9", "9", "4"]], "entropy": [[0.1697998046875, 0.97802734375, 0.42578125, 1.029296875, 0.8369140625, 0.8681640625, 5.0234375, 3.478515625, 3.345703125, 5.140625, 2.185546875, 2.341796875, 3.68359375, 2.66796875, 0.044891357421875, 0.5830078125, 0.9384765625, 0.467529296875, 0.065185546875, 1.4677734375, 2.1484375]], "avg_entropy": [1.8043082101004464], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "UN/LOCODE:USMIA", "score_of_biggest_cluster": 0.04516242444515228, "score_of_first_answer": 0.04516242444515228, "max_score_of_first_answer": 0.9931640625, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.04516242444515228, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.38932152116814395, "std_of_first_answer": 0.38932152116814395, "avg_entropy_of_first_answer": 1.8043082101004464, "lex_sim": 1.0}
{"question": "What is the name of the fictional club in the James Bond novel \u2018Moonraker\u2019?", "answer": ["Blades"], "scores": [[0.984375, 0.99951171875]], "normalized_score": [0.9919145107269287], "tokens": [["\u2581Bl", "ades"]], "entropy": [[0.12042236328125, 0.0045166015625]], "avg_entropy": [0.062469482421875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Blades Club", "score_of_biggest_cluster": 0.9919145107269287, "score_of_first_answer": 0.9919145107269287, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.984375, "avg_score": 0.9919145107269287, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.007568359375, "std_of_first_answer": 0.007568359375, "avg_entropy_of_first_answer": 0.062469482421875, "lex_sim": 1.0}
{"question": "At the 2010 MTV Video Music Awards, Lady Gaga wore a controversial dress made of what?", "answer": ["Yak fur"], "scores": [[0.045654296875, 0.196533203125, 0.08905029296875]], "normalized_score": [0.09279351681470871], "tokens": [["\u2581Y", "ak", "\u2581fur"]], "entropy": [[2.869140625, 4.36328125, 2.4453125]], "avg_entropy": [3.2259114583333335], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Raw meat", "score_of_biggest_cluster": 0.09279351681470871, "score_of_first_answer": 0.09279351681470871, "max_score_of_first_answer": 0.196533203125, "min_score_of_first_answer": 0.045654296875, "avg_score": 0.09279351681470871, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06342119589942709, "std_of_first_answer": 0.06342119589942709, "avg_entropy_of_first_answer": 3.2259114583333335, "lex_sim": 1.0}
{"question": "A second referendum is due to be held for which commonwealth to decide whether, among other options, to become the 51st state of the USA?", "answer": ["Puerto Rico #Education #News #School"], "scores": [[0.60107421875, 0.99609375, 1.6093254089355469e-06, 0.001033782958984375, 0.0025997161865234375, 0.5009765625, 0.008026123046875, 0.60693359375, 0.0015869140625, 0.51123046875]], "normalized_score": [0.018663199618458748], "tokens": [["\u2581Puerto", "\u2581Rico", "\u2581#", "E", "ducation", "\u2581#", "News", "\u2581#", "Sch", "ool"]], "entropy": [[1.7490234375, 0.037017822265625, 0.94580078125, 4.171875, 3.96875, 2.630859375, 5.19921875, 1.3310546875, 2.94921875, 1.1865234375]], "avg_entropy": [2.4169342041015627], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Puertorican", "score_of_biggest_cluster": 0.018663199618458748, "score_of_first_answer": 0.018663199618458748, "max_score_of_first_answer": 0.99609375, "min_score_of_first_answer": 1.6093254089355469e-06, "avg_score": 0.018663199618458748, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3451472963440418, "std_of_first_answer": 0.3451472963440418, "avg_entropy_of_first_answer": 2.4169342041015627, "lex_sim": 1.0}
{"question": "Clifford\u2019s Tower is all that remains of which city\u2019s castle?", "answer": ["York"], "scores": [[0.939453125]], "normalized_score": [0.939453125], "tokens": [["\u2581York"]], "entropy": [[0.349853515625]], "avg_entropy": [0.349853515625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Park Grove (1895)", "score_of_biggest_cluster": 0.939453125, "score_of_first_answer": 0.939453125, "max_score_of_first_answer": 0.939453125, "min_score_of_first_answer": 0.939453125, "avg_score": 0.939453125, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.349853515625, "lex_sim": 1.0}
{"question": "Which city, since the 14th century BC, has been controlled by Israelites, Judaeans, Babylonians, Persians, Greeks, Hasmoneans, Romans, Byzantines, Arabs, Mamluks, Turks, and the British and for a short time after World War II was occupied by Israel and Jordan?", "answer": ["Jude #464112 - 03/06/08 03:16 PM Re: Gay marriages in California!"], "scores": [[0.006893157958984375, 0.040771484375, 1.1920928955078125e-07, 0.0032672882080078125, 0.025604248046875, 0.0518798828125, 0.047698974609375, 0.035400390625, 0.041778564453125, 0.002552032470703125, 0.1092529296875, 0.0810546875, 0.01032257080078125, 0.28564453125, 0.365234375, 0.0870361328125, 0.89111328125, 0.033294677734375, 0.1292724609375, 0.41845703125, 0.248291015625, 0.08709716796875, 0.9755859375, 0.1407470703125, 0.11297607421875, 0.41357421875, 0.0875244140625, 0.9970703125, 9.655952453613281e-06, 0.0016355514526367188, 0.71728515625, 0.9599609375, 0.13671875, 0.333251953125, 0.0053558349609375, 0.87451171875, 0.99951171875, 0.99951171875, 0.0022411346435546875, 0.9873046875, 0.9931640625]], "normalized_score": [0.058104779571294785], "tokens": [["\u2581J", "ude", "\u2581#", "4", "6", "4", "1", "1", "2", "\u2581-", "\u2581", "0", "3", "/", "0", "6", "/", "0", "8", "\u2581", "0", "3", ":", "1", "6", "\u2581PM", "\u2581Re", ":", "\u2581Gay", "\u2581mar", "ri", "ages", "\u2581in", "\u2581California", "!", "\u2581[", "Re", ":", "\u2581Reb", "oot", "]"]], "entropy": [[0.3994140625, 1.78125, 0.127197265625, 3.83984375, 4.37109375, 3.568359375, 2.71484375, 4.08984375, 3.009765625, 3.599609375, 5.59765625, 2.564453125, 1.2734375, 3.302734375, 1.3955078125, 2.142578125, 0.70361328125, 0.63623046875, 2.310546875, 2.623046875, 1.533203125, 2.287109375, 0.1983642578125, 1.8759765625, 2.30859375, 2.796875, 1.9599609375, 0.0308990478515625, 5.77734375, 5.0078125, 1.1435546875, 0.278076171875, 3.642578125, 3.63671875, 2.23046875, 0.86767578125, 0.004283905029296875, 0.0035305023193359375, 4.390625, 0.09796142578125, 0.07196044921875]], "avg_entropy": [2.199868225469822], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Neighbourhoods of Jerusalem", "score_of_biggest_cluster": 0.058104779571294785, "score_of_first_answer": 0.058104779571294785, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 1.1920928955078125e-07, "avg_score": 0.058104779571294785, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3759897814775648, "std_of_first_answer": 0.3759897814775648, "avg_entropy_of_first_answer": 2.199868225469822, "lex_sim": 1.0}
{"question": "The bestselling children\u2019s book \u201cGrandpa\u2019s great escape\u201d is by which writer?", "answer": ["Yasusada Utamaro"], "scores": [[0.01352691650390625, 0.246826171875, 0.0037994384765625, 0.63671875, 0.0006999969482421875, 0.014678955078125, 0.78271484375]], "normalized_score": [0.035048097372055054], "tokens": [["\u2581Y", "as", "us", "ada", "\u2581Ut", "amar", "o"]], "entropy": [[2.04296875, 3.474609375, 1.8095703125, 0.89208984375, 2.380859375, 1.564453125, 0.54638671875]], "avg_entropy": [1.8158482142857142], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "David Walliams", "score_of_biggest_cluster": 0.035048097372055054, "score_of_first_answer": 0.035048097372055054, "max_score_of_first_answer": 0.78271484375, "min_score_of_first_answer": 0.0006999969482421875, "avg_score": 0.035048097372055054, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30869198943999554, "std_of_first_answer": 0.30869198943999554, "avg_entropy_of_first_answer": 1.8158482142857142, "lex_sim": 1.0}
{"question": "What Cantonese mixed bite-size food means literally 'touch the heart'?", "answer": ["\"Chinese #WrestleMania Diaries: Preparation"], "scores": [[0.0101470947265625, 0.0064697265625, 0.135498046875, 1.3768672943115234e-05, 0.0002827644348144531, 1.6689300537109375e-06, 0.047698974609375, 0.70263671875, 0.99951171875, 0.00054931640625, 0.0850830078125, 0.478759765625, 0.0030422210693359375, 0.10931396484375, 0.1495361328125]], "normalized_score": [0.010525011457502842], "tokens": [["\u2581\"", "Ch", "inese", "\u2581#", "W", "rest", "le", "Man", "ia", "\u2581Di", "aries", ":", "\u2581Pre", "par", "ation"]], "entropy": [[1.7900390625, 3.685546875, 1.33203125, 4.40625, 4.80078125, 1.5625, 1.10546875, 1.7998046875, 0.00528717041015625, 4.109375, 2.18359375, 2.62890625, 5.125, 1.9072265625, 0.64111328125]], "avg_entropy": [2.4721949259440104], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dim-sum", "score_of_biggest_cluster": 0.010525011457502842, "score_of_first_answer": 0.010525011457502842, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 1.6689300537109375e-06, "avg_score": 0.010525011457502842, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2930234977071854, "std_of_first_answer": 0.2930234977071854, "avg_entropy_of_first_answer": 2.4721949259440104, "lex_sim": 1.0}
{"question": "Carpology is the study of what?", "answer": ["Carp"], "scores": [[0.274658203125, 0.66357421875]], "normalized_score": [0.42691460251808167], "tokens": [["\u2581Car", "p"]], "entropy": [[4.53515625, 1.0537109375]], "avg_entropy": [2.79443359375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fruits and seeds", "score_of_biggest_cluster": 0.42691460251808167, "score_of_first_answer": 0.42691460251808167, "max_score_of_first_answer": 0.66357421875, "min_score_of_first_answer": 0.274658203125, "avg_score": 0.42691460251808167, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1944580078125, "std_of_first_answer": 0.1944580078125, "avg_entropy_of_first_answer": 2.79443359375, "lex_sim": 1.0}
{"question": "The name of Niccolo Paganini is associated with which instrument?", "answer": ["\u53e4\u7434"], "scores": [[0.0003027915954589844, 2.3245811462402344e-06, 0.970703125, 0.482177734375, 1.0]], "normalized_score": [0.012692771852016449], "tokens": [["\u2581", "\u53e4", "<0xE7>", "<0x90>", "<0xB4>"]], "entropy": [[0.55322265625, 2.46875, 0.25390625, 1.013671875, 0.002643585205078125]], "avg_entropy": [0.8584388732910156], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Violinist (disambiguation)", "score_of_biggest_cluster": 0.012692771852016449, "score_of_first_answer": 0.012692771852016449, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.3245811462402344e-06, "avg_score": 0.012692771852016449, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4407120896847082, "std_of_first_answer": 0.4407120896847082, "avg_entropy_of_first_answer": 0.8584388732910156, "lex_sim": 0.0}
{"question": "What was Moses elder brother called", "answer": ["Aaron"], "scores": [[0.98193359375, 0.99951171875]], "normalized_score": [0.9906836748123169], "tokens": [["\u2581A", "aron"]], "entropy": [[0.1654052734375, 0.006214141845703125]], "avg_entropy": [0.08580970764160156], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u05d0\u05b7\u05d4\u05b2\u05e8\u05b9\u05df", "score_of_biggest_cluster": 0.9906836748123169, "score_of_first_answer": 0.9906836748123169, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.98193359375, "avg_score": 0.9906836748123169, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0087890625, "std_of_first_answer": 0.0087890625, "avg_entropy_of_first_answer": 0.08580970764160156, "lex_sim": 1.0}
{"question": "After what is Fleet Street in London named?", "answer": ["It was named after the River Fleet, which flows under it."], "scores": [[0.1063232421875, 0.01122283935546875, 0.9326171875, 0.966796875, 0.97265625, 0.97900390625, 1.0, 1.0, 0.78076171875, 0.75390625, 0.366943359375, 0.6572265625, 0.2041015625, 0.84521484375, 0.0, 0.97998046875, 1.1920928955078125e-07, 0.001766204833984375, 1.0, 0.92333984375, 0.007678985595703125, 0.99658203125, 0.452392578125, 0.040252685546875, 0.02789306640625, 0.64306640625, 0.65478515625, 0.0235137939453125, 0.97412109375, 0.98779296875, 0.98291015625, 0.8466796875, 0.002162933349609375, 1.0, 0.97021484375, 0.7685546875, 1.0]], "normalized_score": [0.0], "tokens": [["\u2581It", "\u2581was", "\u2581named", "\u2581after", "\u2581the", "\u2581River", "\u2581Fle", "et", ",", "\u2581which", "\u2581flows", "\u2581under", "\u2581it", ".", "\u2581Tags", ":", "\u2581Trump", "\u2581im", "pe", "achment", "\u2581hear", "ings", ",", "\u2581Trump", "\u2581and", "\u2581Ukraine", ",", "\u2581Rud", "y", "\u2581Giul", "iani", ",", "\u2581im", "pe", "achment", "\u2581inqu", "iry"]], "entropy": [[2.677734375, 0.34619140625, 0.41064453125, 0.1925048828125, 0.1517333984375, 0.11993408203125, 0.0008683204650878906, 0.00013136863708496094, 0.775390625, 0.615234375, 1.322265625, 0.96337890625, 1.1044921875, 0.66015625, 0.98876953125, 0.16943359375, 3.65625, 4.5390625, 0.0007305145263671875, 0.321044921875, 2.876953125, 0.03643798828125, 2.708984375, 6.3359375, 1.8447265625, 1.91015625, 1.154296875, 3.64453125, 0.13134765625, 0.09197998046875, 0.1148681640625, 0.78759765625, 3.32421875, 0.0013914108276367188, 0.1434326171875, 1.16796875, 2.3186206817626953e-05]], "avg_entropy": [1.2240757700559255], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fleet river", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.40578043804695896, "std_of_first_answer": 0.40578043804695896, "avg_entropy_of_first_answer": 1.2240757700559255, "lex_sim": 1.0}
{"question": "Which US statesman is on the $100 bill?", "answer": ["Alexander Hamilton CHAPTER XXVII."], "scores": [[0.87353515625, 0.99609375, 0.0, 0.01352691650390625, 0.195556640625, 0.377685546875, 0.212890625]], "normalized_score": [0.0], "tokens": [["\u2581Alexander", "\u2581Hamilton", "\u2581CHAPTER", "\u2581XX", "V", "II", "."]], "entropy": [[0.68505859375, 0.037139892578125, 0.912109375, 3.4375, 2.541015625, 2.25, 3.447265625]], "avg_entropy": [1.9014413016183036], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A Tradesman of Philidelphia", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.99609375, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3710581216235576, "std_of_first_answer": 0.3710581216235576, "avg_entropy_of_first_answer": 1.9014413016183036, "lex_sim": 1.0}
{"question": "\"Who painted \"\"Dedham Vale\"\" and \"\"The Valley Farm\"\"?\"", "answer": ["John Constable \\\u02cces-t\u012b-\u02c8t\u012bz-m\u0259, \u02c8e-\u02ccsti-t\u012bz-\\"], "scores": [[0.9580078125, 0.99853515625, 0.99951171875, 3.5762786865234375e-07, 2.5510787963867188e-05, 0.0, 0.34033203125, 0.20751953125, 0.015380859375, 0.2861328125, 0.35400390625, 0.021270751953125, 0.018341064453125, 0.0011739730834960938, 0.5029296875, 0.0008511543273925781, 0.96630859375, 0.08245849609375, 0.220947265625, 0.90673828125, 0.00197601318359375, 0.1475830078125, 0.1270751953125, 0.927734375, 0.311767578125, 0.63330078125, 0.163330078125, 0.43212890625, 0.92724609375, 0.490966796875]], "normalized_score": [0.0], "tokens": [["\u2581John", "\u2581Const", "able", "\u2581\\", "\u02cc", "es", "-", "t", "\u012b", "-", "\u02c8", "t", "\u012b", "z", "-", "m", "\u0259", ",", "\u2581", "\u02c8", "e", "-", "\u02cc", "st", "i", "-", "t", "\u012b", "z", "-\\"]], "entropy": [[0.34765625, 0.0152740478515625, 0.0047607421875, 0.90625, 1.8916015625, 0.51513671875, 4.2109375, 3.322265625, 3.498046875, 2.306640625, 2.064453125, 3.58203125, 4.4375, 1.29296875, 2.0234375, 1.5283203125, 0.27783203125, 1.9482421875, 2.384765625, 0.45654296875, 0.75439453125, 1.361328125, 2.400390625, 0.484130859375, 0.97900390625, 1.0166015625, 2.642578125, 2.189453125, 0.43310546875, 1.1171875]], "avg_entropy": [1.679761250813802], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "J Constable", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.36474879301212376, "std_of_first_answer": 0.36474879301212376, "avg_entropy_of_first_answer": 1.679761250813802, "lex_sim": 1.0}
{"question": "What is the last Grand Slam tennis tournament played in a calendar year?", "answer": ["Calendar Year Grand Slam"], "scores": [[0.0020732879638671875, 0.059844970703125, 0.4921875, 0.95458984375, 1.0]], "normalized_score": [0.14227445423603058], "tokens": [["\u2581Calendar", "\u2581Year", "\u2581Grand", "\u2581S", "lam"]], "entropy": [[1.8388671875, 1.3564453125, 2.033203125, 0.255126953125, 2.962350845336914e-05]], "avg_entropy": [1.0967344403266908], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "United States Open Championship", "score_of_biggest_cluster": 0.14227445423603058, "score_of_first_answer": 0.14227445423603058, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0020732879638671875, "avg_score": 0.14227445423603058, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4238785908420163, "std_of_first_answer": 0.4238785908420163, "avg_entropy_of_first_answer": 1.0967344403266908, "lex_sim": 1.0}
{"question": "In which month of the year does pheasant shooting legally begin?", "answer": ["December"], "scores": [[0.00039458274841308594]], "normalized_score": [0.0003945827193092555], "tokens": [["\u2581December"]], "entropy": [[0.15478515625]], "avg_entropy": [0.15478515625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ocotber", "score_of_biggest_cluster": 0.0003945827193092555, "score_of_first_answer": 0.0003945827193092555, "max_score_of_first_answer": 0.00039458274841308594, "min_score_of_first_answer": 0.00039458274841308594, "avg_score": 0.0003945827193092555, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.15478515625, "lex_sim": 1.0}
{"question": "In which London park would you find the Zoological Gardens?", "answer": ["None exist, there is a #BEYONC\u00c9: How Spike Jonze, Her and the Arcade Fire are revolutionizing the music video"], "scores": [[0.00131988525390625, 0.135009765625, 0.43505859375, 0.07415771484375, 0.67431640625, 0.65576171875, 7.152557373046875e-07, 0.0020313262939453125, 4.76837158203125e-07, 0.99560546875, 0.01091766357421875, 0.1927490234375, 0.008819580078125, 0.0007290840148925781, 0.001781463623046875, 0.0008511543273925781, 0.9990234375, 0.80712890625, 1.0, 0.68115234375, 0.01161956787109375, 0.0172576904296875, 0.96337890625, 0.0001996755599975586, 0.9189453125, 0.97119140625, 0.033294677734375, 0.85107421875, 0.8369140625, 0.466064453125, 0.342529296875, 0.382080078125]], "normalized_score": [0.03922221437096596], "tokens": [["\u2581None", "\u2581exist", ",", "\u2581there", "\u2581is", "\u2581a", "\u2581#", "B", "EY", "ON", "C", "\u00c9", ":", "\u2581How", "\u2581S", "pi", "ke", "\u2581Jon", "ze", ",", "\u2581Her", "\u2581and", "\u2581the", "\u2581Ar", "cade", "\u2581Fire", "\u2581are", "\u2581revolution", "izing", "\u2581the", "\u2581music", "\u2581video"]], "entropy": [[0.31005859375, 1.7314453125, 1.9892578125, 2.85546875, 1.1396484375, 1.19140625, 1.619140625, 2.076171875, 3.564453125, 0.04351806640625, 0.88525390625, 2.068359375, 4.8359375, 4.01171875, 3.884765625, 1.115234375, 0.00960540771484375, 0.5615234375, 0.0014123916625976562, 1.5048828125, 4.4765625, 1.5048828125, 0.3212890625, 4.5, 0.449462890625, 0.2452392578125, 3.814453125, 0.6728515625, 0.513671875, 2.55078125, 2.70703125, 1.662109375]], "avg_entropy": [1.8380499184131622], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Regent's Park", "score_of_biggest_cluster": 0.03922221437096596, "score_of_first_answer": 0.03922221437096596, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 4.76837158203125e-07, "avg_score": 0.03922221437096596, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39537289574677986, "std_of_first_answer": 0.39537289574677986, "avg_entropy_of_first_answer": 1.8380499184131622, "lex_sim": 1.0}
{"question": "Which is the only marsupial to be found in North America?", "answer": ["Opossum"], "scores": [[0.424560546875, 0.98095703125, 0.99951171875]], "normalized_score": [0.7466650605201721], "tokens": [["\u2581O", "pos", "sum"]], "entropy": [[1.9521484375, 0.10894775390625, 0.00484466552734375]], "avg_entropy": [0.6886469523111979], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "O'possum", "score_of_biggest_cluster": 0.7466650605201721, "score_of_first_answer": 0.7466650605201721, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.424560546875, "avg_score": 0.7466650605201721, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.26676876676534306, "std_of_first_answer": 0.26676876676534306, "avg_entropy_of_first_answer": 0.6886469523111979, "lex_sim": 1.0}
{"question": "The existance of which planet was mathematically", "answer": ["2007 OR10"], "scores": [[0.0006198883056640625, 0.0887451171875, 0.52294921875, 0.78515625, 0.05657958984375, 0.50244140625, 0.8681640625, 0.9990234375]], "normalized_score": [0.16528160870075226], "tokens": [["\u2581", "2", "0", "0", "7", "\u2581OR", "1", "0"]], "entropy": [[1.9873046875, 2.58984375, 2.560546875, 1.1435546875, 2.044921875, 2.287109375, 0.68505859375, 0.0095062255859375]], "avg_entropy": [1.6634807586669922], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "NEPTUNE", "score_of_biggest_cluster": 0.16528160870075226, "score_of_first_answer": 0.16528160870075226, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.0006198883056640625, "avg_score": 0.16528160870075226, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3670256255799828, "std_of_first_answer": 0.3670256255799828, "avg_entropy_of_first_answer": 1.6634807586669922, "lex_sim": 1.0}
{"question": "Riff, Bernado and Joyboy are all characters in which musical?", "answer": ["West Side Story"], "scores": [[0.9658203125, 0.98388671875, 0.99169921875]], "normalized_score": [0.9804086685180664], "tokens": [["\u2581West", "\u2581Side", "\u2581Story"]], "entropy": [[0.261474609375, 0.1046142578125, 0.060302734375]], "avg_entropy": [0.14213053385416666], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Sharks (West Side Story)", "score_of_biggest_cluster": 0.9804086685180664, "score_of_first_answer": 0.9804086685180664, "max_score_of_first_answer": 0.99169921875, "min_score_of_first_answer": 0.9658203125, "avg_score": 0.9804086685180664, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01083793737928808, "std_of_first_answer": 0.01083793737928808, "avg_entropy_of_first_answer": 0.14213053385416666, "lex_sim": 1.0}
{"question": "What is the traditional name for a 40th wedding anniversary?", "answer": ["Ruby"], "scores": [[0.62158203125]], "normalized_score": [0.62158203125], "tokens": [["\u2581Ruby"]], "entropy": [[1.4697265625]], "avg_entropy": [1.4697265625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "RUBY", "score_of_biggest_cluster": 0.62158203125, "score_of_first_answer": 0.62158203125, "max_score_of_first_answer": 0.62158203125, "min_score_of_first_answer": 0.62158203125, "avg_score": 0.62158203125, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.4697265625, "lex_sim": 1.0}
{"question": "On which Scottish island is Brodick Castle?", "answer": ["ISO 3166-2:GB\\*HN- ########################"], "scores": [[1.537799835205078e-05, 0.08319091796875, 0.03533935546875, 0.98193359375, 0.994140625, 0.9990234375, 0.9775390625, 0.10400390625, 0.83203125, 0.9384765625, 0.05975341796875, 0.2744140625, 0.00841522216796875, 0.01654052734375, 0.8037109375, 1.1920928955078125e-07, 0.05389404296875, 0.146240234375, 0.1590576171875]], "normalized_score": [0.05443735793232918], "tokens": [["\u2581ISO", "\u2581", "3", "1", "6", "6", "-", "2", ":", "GB", "\\", "*", "H", "N", "-", "\u2581###", "################", "####", "#"]], "entropy": [[0.91162109375, 4.29296875, 2.19140625, 0.1392822265625, 0.04779052734375, 0.01169586181640625, 0.1837158203125, 0.4560546875, 1.12890625, 0.341552734375, 2.560546875, 2.95703125, 4.4375, 4.3359375, 0.98095703125, 0.8037109375, 3.98828125, 1.5908203125, 1.580078125]], "avg_entropy": [1.7336767096268504], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Arran (disambiguation)", "score_of_biggest_cluster": 0.05443735793232918, "score_of_first_answer": 0.05443735793232918, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 1.1920928955078125e-07, "avg_score": 0.05443735793232918, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4191346001517622, "std_of_first_answer": 0.4191346001517622, "avg_entropy_of_first_answer": 1.7336767096268504, "lex_sim": 1.0}
{"question": "The term \u2018Heavy metal\u2019 is said to have first appeared in which 1968 song?", "answer": ["\u201cPaint It Black\u201d"], "scores": [[0.006267547607421875, 0.13037109375, 0.86669921875, 0.802734375, 0.98388671875]], "normalized_score": [0.22363147139549255], "tokens": [["\u2581\u201c", "Paint", "\u2581It", "\u2581Black", "\u201d"]], "entropy": [[1.3818359375, 2.5625, 0.394775390625, 0.50390625, 0.11077880859375]], "avg_entropy": [0.99075927734375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Born to Be Wild", "score_of_biggest_cluster": 0.22363147139549255, "score_of_first_answer": 0.22363147139549255, "max_score_of_first_answer": 0.98388671875, "min_score_of_first_answer": 0.006267547607421875, "avg_score": 0.22363147139549255, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4059174298592967, "std_of_first_answer": 0.4059174298592967, "avg_entropy_of_first_answer": 0.99075927734375, "lex_sim": 1.0}
{"question": "\"The \"\"Hall of Mirrors\"\" is in which building?\"", "answer": ["Ch\u00e2teau de Versailles, France  1."], "scores": [[0.06573486328125, 0.051300048828125, 0.978515625, 0.931640625, 0.99951171875, 1.0, 0.13916015625, 0.75244140625, 1.9073486328125e-06, 7.259845733642578e-05, 0.0699462890625, 0.00035858154296875, 0.036346435546875, 0.93017578125, 1.0]], "normalized_score": [0.05071653798222542], "tokens": [["\u2581Ch", "\u00e2teau", "\u2581de", "\u2581Vers", "a", "illes", ",", "\u2581France", "\u2581\u2581", "1", ".", "\u2581T", "aj", "\u2581Mah", "al"]], "entropy": [[1.8134765625, 1.39453125, 0.1434326171875, 0.380859375, 0.005138397216796875, 0.0007276535034179688, 1.267578125, 1.0595703125, 1.0185546875, 0.06903076171875, 2.9296875, 4.03125, 5.703125, 0.390625, 0.0027294158935546875]], "avg_entropy": [1.3473544438680014], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Palais de Versailles", "score_of_biggest_cluster": 0.05071653798222542, "score_of_first_answer": 0.05071653798222542, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.9073486328125e-06, "avg_score": 0.05071653798222542, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4519148205256655, "std_of_first_answer": 0.4519148205256655, "avg_entropy_of_first_answer": 1.3473544438680014, "lex_sim": 1.0}
{"question": "In which country is the Grand Coulee Dam", "answer": ["United States of America"], "scores": [[0.74658203125, 0.98876953125, 0.390625, 0.99853515625]], "normalized_score": [0.7325276136398315], "tokens": [["\u2581United", "\u2581States", "\u2581of", "\u2581America"]], "entropy": [[0.818359375, 0.08099365234375, 1.42578125, 0.01184844970703125]], "avg_entropy": [0.5842456817626953], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The United States of America", "score_of_biggest_cluster": 0.7325276136398315, "score_of_first_answer": 0.7325276136398315, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.390625, "avg_score": 0.7325276136398315, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.24701563414813082, "std_of_first_answer": 0.24701563414813082, "avg_entropy_of_first_answer": 0.5842456817626953, "lex_sim": 1.0}
{"question": "\"What single word sounds like \"\"Youth in Asia\"\"?\"", "answer": ["Boith"], "scores": [[0.01287841796875, 0.0055999755859375]], "normalized_score": [0.008492281660437584], "tokens": [["\u2581Bo", "ith"]], "entropy": [[4.61328125, 3.046875]], "avg_entropy": [3.830078125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Euthenasia", "score_of_biggest_cluster": 0.008492281660437584, "score_of_first_answer": 0.008492281660437584, "max_score_of_first_answer": 0.01287841796875, "min_score_of_first_answer": 0.0055999755859375, "avg_score": 0.008492281660437584, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.00363922119140625, "std_of_first_answer": 0.00363922119140625, "avg_entropy_of_first_answer": 3.830078125, "lex_sim": 1.0}
{"question": "The region of Catalonia is part of which country?", "answer": ["ES"], "scores": [[2.2113323211669922e-05]], "normalized_score": [2.211333230661694e-05], "tokens": [["\u2581ES"]], "entropy": [[0.35791015625]], "avg_entropy": [0.35791015625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Islands of Spain", "score_of_biggest_cluster": 2.211333230661694e-05, "score_of_first_answer": 2.211333230661694e-05, "max_score_of_first_answer": 2.2113323211669922e-05, "min_score_of_first_answer": 2.2113323211669922e-05, "avg_score": 2.211333230661694e-05, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.35791015625, "lex_sim": 1.0}
{"question": "What U.S. state borders four Great Lakes?", "answer": ["Michigan"], "scores": [[0.95654296875]], "normalized_score": [0.95654296875], "tokens": [["\u2581Michigan"]], "entropy": [[0.31298828125]], "avg_entropy": [0.31298828125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Demographics of Michigan", "score_of_biggest_cluster": 0.95654296875, "score_of_first_answer": 0.95654296875, "max_score_of_first_answer": 0.95654296875, "min_score_of_first_answer": 0.95654296875, "avg_score": 0.95654296875, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.31298828125, "lex_sim": 1.0}
{"question": "Who's missing: Miss Scarlett, Mrs. White, Mr. Green, Mrs. Peacock, Professor Plum?", "answer": ["Mr. Boddy"], "scores": [[0.06842041015625, 0.99365234375, 0.47802734375, 0.9970703125]], "normalized_score": [0.4242771565914154], "tokens": [["\u2581Mr", ".", "\u2581Bod", "dy"]], "entropy": [[1.8291015625, 0.058258056640625, 0.9892578125, 0.03118896484375]], "avg_entropy": [0.7269515991210938], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "List of Cluedo characters", "score_of_biggest_cluster": 0.4242771565914154, "score_of_first_answer": 0.4242771565914154, "max_score_of_first_answer": 0.9970703125, "min_score_of_first_answer": 0.06842041015625, "avg_score": 0.4242771565914154, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3890299666041928, "std_of_first_answer": 0.3890299666041928, "avg_entropy_of_first_answer": 0.7269515991210938, "lex_sim": 1.0}
{"question": "Ur was a major city of which ancient civilization?", "answer": ["Ur was a major city of the Sumerian civilization."], "scores": [[0.00943756103515625, 0.591796875, 0.98291015625, 0.9912109375, 0.99462890625, 0.9677734375, 0.8984375, 0.83154296875, 1.0, 0.9873046875, 0.94921875, 0.99951171875, 0.55224609375, 0.0, 0.003269195556640625, 0.0794677734375, 0.0001976490020751953, 0.1514892578125, 0.006679534912109375, 0.05029296875, 0.99462890625, 0.0001996755599975586, 0.10546875, 0.31787109375]], "normalized_score": [0.0], "tokens": [["\u2581Ur", "\u2581was", "\u2581a", "\u2581major", "\u2581city", "\u2581of", "\u2581the", "\u2581S", "umer", "ian", "\u2581civil", "ization", ".", "\u2581CHAPTER", "\u2581XX", "IV", ".--", "The", "\u2581T", "aking", "\u2581of", "\u2581New", "\u2581Orleans", "."]], "entropy": [[0.403076171875, 0.966796875, 0.10870361328125, 0.05108642578125, 0.036376953125, 0.1470947265625, 0.443359375, 0.55712890625, 2.759695053100586e-05, 0.07470703125, 0.20361328125, 0.004180908203125, 1.4013671875, 0.74853515625, 2.490234375, 2.958984375, 3.556640625, 4.3515625, 6.09375, 4.4140625, 0.047210693359375, 3.2734375, 1.0400390625, 2.943359375]], "avg_entropy": [1.5131389672557514], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "SUMER", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4359816919006285, "std_of_first_answer": 0.4359816919006285, "avg_entropy_of_first_answer": 1.5131389672557514, "lex_sim": 1.0}
{"question": "Who was the demon barber of Fleet Street?", "answer": ["Sweeney Todd #FollowFriday + #MusicMonday = Music Friday Follow \u2013 A Perfect Pairing of Twitter and Music"], "scores": [[0.9443359375, 1.0, 0.9990234375, 0.99658203125, 0.99951171875, 1.7881393432617188e-07, 0.003993988037109375, 2.4437904357910156e-06, 0.0008301734924316406, 0.67236328125, 1.0, 2.5093555450439453e-05, 0.06500244140625, 0.033905029296875, 0.96435546875, 0.9482421875, 1.0, 0.0162353515625, 0.0020904541015625, 0.005645751953125, 0.0163421630859375, 0.006561279296875, 0.036102294921875, 0.04864501953125, 0.9990234375, 0.058624267578125, 0.99462890625, 0.724609375, 0.10650634765625, 0.009765625, 0.006366729736328125, 0.755859375]], "normalized_score": [0.03798532858490944], "tokens": [["\u2581S", "ween", "ey", "\u2581Tod", "d", "\u2581#", "F", "ollow", "F", "rid", "ay", "\u2581+", "\u2581#", "Music", "M", "ond", "ay", "\u2581=", "\u2581Music", "\u2581Friday", "\u2581Follow", "\u2581\u2013", "\u2581A", "\u2581Per", "fect", "\u2581P", "air", "ing", "\u2581of", "\u2581Twitter", "\u2581and", "\u2581Music"]], "entropy": [[0.469482421875, 4.935264587402344e-05, 0.00861358642578125, 0.03228759765625, 0.0060577392578125, 1.0419921875, 4.9609375, 0.69921875, 1.6259765625, 1.798828125, 0.0010881423950195312, 2.6328125, 4.33203125, 1.7705078125, 0.299072265625, 0.336181640625, 0.0014448165893554688, 2.0703125, 3.134765625, 3.80859375, 2.65234375, 2.869140625, 4.4453125, 4.54296875, 0.01149749755859375, 2.759765625, 0.0404052734375, 1.2607421875, 2.533203125, 2.8828125, 1.0927734375, 1.0478515625]], "avg_entropy": [1.7240334674715996], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sweeny tod", "score_of_biggest_cluster": 0.03798532858490944, "score_of_first_answer": 0.03798532858490944, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.7881393432617188e-07, "avg_score": 0.03798532858490944, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.455781216313522, "std_of_first_answer": 0.455781216313522, "avg_entropy_of_first_answer": 1.7240334674715996, "lex_sim": 1.0}
{"question": "Cayenne, after which the spicy condiment is named, is a city in which country?", "answer": ["French Guiana"], "scores": [[0.7236328125, 0.97412109375, 0.99951171875]], "normalized_score": [0.889828622341156], "tokens": [["\u2581French", "\u2581Gu", "iana"]], "entropy": [[1.12890625, 0.1805419921875, 0.0036258697509765625]], "avg_entropy": [0.4376913706461589], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sport in French Guiana", "score_of_biggest_cluster": 0.889828622341156, "score_of_first_answer": 0.889828622341156, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.7236328125, "avg_score": 0.889828622341156, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12449820783858075, "std_of_first_answer": 0.12449820783858075, "avg_entropy_of_first_answer": 0.4376913706461589, "lex_sim": 1.0}
{"question": "\u2018Swifter, Higher, Stronger\u2019 is the motto for the modern \u2018what\u2019?", "answer": ["Olympic Games"], "scores": [[0.474365234375, 0.92041015625]], "normalized_score": [0.6607651710510254], "tokens": [["\u2581Olympic", "\u2581Games"]], "entropy": [[1.275390625, 0.402587890625]], "avg_entropy": [0.8389892578125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "History of the Modern Olympics", "score_of_biggest_cluster": 0.6607651710510254, "score_of_first_answer": 0.6607651710510254, "max_score_of_first_answer": 0.92041015625, "min_score_of_first_answer": 0.474365234375, "avg_score": 0.6607651710510254, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2230224609375, "std_of_first_answer": 0.2230224609375, "avg_entropy_of_first_answer": 0.8389892578125, "lex_sim": 1.0}
{"question": "Bjrn Ulvaeus and Benny Andersson wrote most of the songs for which pop group?", "answer": ["ABBA"], "scores": [[0.732421875, 1.0]], "normalized_score": [0.8558164834976196], "tokens": [["\u2581AB", "BA"]], "entropy": [[1.1220703125, 0.0003924369812011719]], "avg_entropy": [0.5612313747406006], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Infina AB", "score_of_biggest_cluster": 0.8558164834976196, "score_of_first_answer": 0.8558164834976196, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.732421875, "avg_score": 0.8558164834976196, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1337890625, "std_of_first_answer": 0.1337890625, "avg_entropy_of_first_answer": 0.5612313747406006, "lex_sim": 1.0}
{"question": "Which TV series was won by a man named Paddy Doherty on September 8th this year?", "answer": ["I am not aware of such information."], "scores": [[0.654296875, 0.27197265625, 0.1815185546875, 0.055267333984375, 0.99609375, 0.006328582763671875, 0.7568359375, 0.30908203125]], "normalized_score": [0.200699582695961], "tokens": [["\u2581I", "\u2581am", "\u2581not", "\u2581aware", "\u2581of", "\u2581such", "\u2581information", "."]], "entropy": [[1.8134765625, 1.3291015625, 1.412109375, 0.71533203125, 0.035858154296875, 1.3486328125, 0.904296875, 1.318359375]], "avg_entropy": [1.1096458435058594], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "CELEBRITY BIG BROTHER", "score_of_biggest_cluster": 0.200699582695961, "score_of_first_answer": 0.200699582695961, "max_score_of_first_answer": 0.99609375, "min_score_of_first_answer": 0.006328582763671875, "avg_score": 0.200699582695961, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.334180405956502, "std_of_first_answer": 0.334180405956502, "avg_entropy_of_first_answer": 1.1096458435058594, "lex_sim": 1.0}
{"question": "Which British university was the first to throw open all degrees to women (in 1878)?", "answer": ["Nottingham University #51 The Dreadful Lemon"], "scores": [[0.0006952285766601562, 0.857421875, 0.9990234375, 0.336181640625, 4.76837158203125e-07, 0.004428863525390625, 0.062744140625, 0.00018143653869628906, 0.00024020671844482422, 0.146240234375, 0.15087890625, 0.0108184814453125, 0.72607421875]], "normalized_score": [0.012593401595950127], "tokens": [["\u2581Not", "ting", "ham", "\u2581University", "\u2581#", "5", "1", "\u2581The", "\u2581D", "read", "ful", "\u2581Lem", "on"]], "entropy": [[2.4296875, 0.84130859375, 0.01015472412109375, 1.580078125, 0.82373046875, 5.828125, 3.298828125, 3.3515625, 1.5400390625, 4.3515625, 1.1728515625, 5.359375, 1.0849609375]], "avg_entropy": [2.4363280076246996], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "London (UK)", "score_of_biggest_cluster": 0.012593401595950127, "score_of_first_answer": 0.012593401595950127, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 4.76837158203125e-07, "avg_score": 0.012593401595950127, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3493429899958475, "std_of_first_answer": 0.3493429899958475, "avg_entropy_of_first_answer": 2.4363280076246996, "lex_sim": 1.0}
{"question": "Which was the earliest London underground line?", "answer": ["Metropolitan District Railway (1863) \\_|_"], "scores": [[0.296142578125, 0.01354217529296875, 0.7978515625, 0.44384765625, 0.2322998046875, 0.9921875, 0.9921875, 0.6767578125, 0.54736328125, 4.184246063232422e-05, 0.091796875, 0.0007581710815429688, 0.75244140625]], "normalized_score": [0.10774145275354385], "tokens": [["\u2581Metropolitan", "\u2581District", "\u2581Railway", "\u2581(", "1", "8", "6", "3", ")", "\u2581\\", "_", "|", "_"]], "entropy": [[1.671875, 1.6142578125, 0.91357421875, 1.6943359375, 2.83203125, 0.04974365234375, 0.053131103515625, 0.8330078125, 1.1474609375, 1.08203125, 2.078125, 6.20703125, 0.8759765625]], "avg_entropy": [1.6194293682391827], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Met line", "score_of_biggest_cluster": 0.10774145275354385, "score_of_first_answer": 0.10774145275354385, "max_score_of_first_answer": 0.9921875, "min_score_of_first_answer": 4.184246063232422e-05, "avg_score": 0.10774145275354385, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35685388513082117, "std_of_first_answer": 0.35685388513082117, "avg_entropy_of_first_answer": 1.6194293682391827, "lex_sim": 1.0}
{"question": "Peter Schmeiche\u2019s son is currently the regular goalkeeper at which English football league team?", "answer": ["Nottingham Forest"], "scores": [[0.086181640625, 0.1368408203125, 0.99951171875, 0.96826171875]], "normalized_score": [0.3268532156944275], "tokens": [["\u2581Not", "ting", "ham", "\u2581Forest"]], "entropy": [[3.58984375, 3.169921875, 0.005580902099609375, 0.195556640625]], "avg_entropy": [1.7402257919311523], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Leicester Fosse FC", "score_of_biggest_cluster": 0.3268532156944275, "score_of_first_answer": 0.3268532156944275, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.086181640625, "avg_score": 0.3268532156944275, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.436695102459157, "std_of_first_answer": 0.436695102459157, "avg_entropy_of_first_answer": 1.7402257919311523, "lex_sim": 1.0}
{"question": "In WWII, who was the head of the Nazi party's security service?", "answer": ["Gestapo #84 in Best Things To Do in Puerto Rico"], "scores": [[0.006122589111328125, 0.9990234375, 0.99951171875, 2.7418136596679688e-06, 0.006168365478515625, 0.11077880859375, 4.875659942626953e-05, 7.146596908569336e-05, 0.0009188652038574219, 0.0007457733154296875, 0.385009765625, 0.272705078125, 0.0005431175231933594, 0.3466796875]], "normalized_score": [0.006887590046972036], "tokens": [["\u2581Gest", "ap", "o", "\u2581#", "8", "4", "\u2581in", "\u2581Best", "\u2581Things", "\u2581To", "\u2581Do", "\u2581in", "\u2581Puerto", "\u2581Rico"]], "entropy": [[1.9638671875, 0.007717132568359375, 0.00441741943359375, 1.3818359375, 5.10546875, 1.37109375, 2.578125, 4.8515625, 5.484375, 0.44482421875, 1.91015625, 2.45703125, 5.47265625, 2.21484375]], "avg_entropy": [2.5177124568394254], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Reinhard Heidrich", "score_of_biggest_cluster": 0.006887590046972036, "score_of_first_answer": 0.006887590046972036, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 2.7418136596679688e-06, "avg_score": 0.006887590046972036, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3436335208650846, "std_of_first_answer": 0.3436335208650846, "avg_entropy_of_first_answer": 2.5177124568394254, "lex_sim": 1.0}
{"question": "What is a legislative or executive act by which a state restores those who may have been guilty of an offence against it to the positions of innocent people?", "answer": ["Act of Oblivion BRIAN GLEESON INTERVIEWS PAUL KELLY"], "scores": [[0.06683349609375, 0.84521484375, 0.161376953125, 0.99951171875, 0.998046875, 0.99365234375, 2.980232238769531e-07, 0.0002932548522949219, 0.0750732421875, 0.0094451904296875, 0.00373077392578125, 0.073974609375, 0.99951171875, 0.0019140243530273438, 0.06512451171875, 0.314208984375, 0.99462890625, 0.08856201171875, 0.005645751953125, 0.93505859375, 0.034332275390625, 0.384033203125, 0.890625, 0.98876953125]], "normalized_score": [0.0650259405374527], "tokens": [["\u2581Act", "\u2581of", "\u2581O", "bl", "iv", "ion", "\u2581B", "RI", "AN", "\u2581G", "LE", "ES", "ON", "\u2581IN", "TER", "V", "IE", "WS", "\u2581PA", "UL", "\u2581K", "E", "LL", "Y"]], "entropy": [[2.365234375, 0.708984375, 2.958984375, 0.004650115966796875, 0.016357421875, 0.041107177734375, 1.111328125, 1.8896484375, 0.64013671875, 4.109375, 3.841796875, 1.4921875, 0.00287628173828125, 3.22265625, 4.25, 1.66015625, 0.043670654296875, 0.3271484375, 4.36328125, 0.408935546875, 3.6640625, 2.654296875, 0.55029296875, 0.0831298828125]], "avg_entropy": [1.6837623914082844], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Amnesty", "score_of_biggest_cluster": 0.0650259405374527, "score_of_first_answer": 0.0650259405374527, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 2.980232238769531e-07, "avg_score": 0.0650259405374527, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4341382925798026, "std_of_first_answer": 0.4341382925798026, "avg_entropy_of_first_answer": 1.6837623914082844, "lex_sim": 1.0}
{"question": "'Ben-Hur: A Tale of the Christ', the 1880 novel by Lew Wallace, is about a young aristocratic Jew. What is his first name?", "answer": ["Judah Ben-Hur"], "scores": [[0.9794921875, 0.99951171875, 0.20458984375, 0.9990234375, 0.9990234375, 0.99951171875]], "normalized_score": [0.7646018862724304], "tokens": [["\u2581Jud", "ah", "\u2581Ben", "-", "H", "ur"]], "entropy": [[0.1412353515625, 0.005126953125, 1.2021484375, 0.007045745849609375, 0.00868988037109375, 0.002735137939453125]], "avg_entropy": [0.22783025105794272], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ioudas", "score_of_biggest_cluster": 0.7646018862724304, "score_of_first_answer": 0.7646018862724304, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.20458984375, "avg_score": 0.7646018862724304, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2947734602483243, "std_of_first_answer": 0.2947734602483243, "avg_entropy_of_first_answer": 0.22783025105794272, "lex_sim": 1.0}
{"question": "In which year did South Korea hold its first 'Grand Prix' motor race?", "answer": ["1962"], "scores": [[0.18017578125, 0.97900390625, 0.9833984375, 0.81591796875, 0.50146484375]], "normalized_score": [0.5891412496566772], "tokens": [["\u2581", "1", "9", "6", "2"]], "entropy": [[1.7373046875, 0.1270751953125, 0.10498046875, 0.6748046875, 0.98291015625]], "avg_entropy": [0.7254150390625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "two thousand and ten", "score_of_biggest_cluster": 0.5891412496566772, "score_of_first_answer": 0.5891412496566772, "max_score_of_first_answer": 0.9833984375, "min_score_of_first_answer": 0.18017578125, "avg_score": 0.5891412496566772, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31013146596900165, "std_of_first_answer": 0.31013146596900165, "avg_entropy_of_first_answer": 0.7254150390625, "lex_sim": 1.0}
{"question": "The London production of which play by acclaimed Australian playwright, David Williamson, won him the George Devine Award, the first time for a non-UK writer?", "answer": ["The Removalists #BlackLivesMatter: Panel Discussion on Systemic Racism"], "scores": [[0.3212890625, 0.71826171875, 0.9990234375, 0.9990234375, 7.748603820800781e-07, 7.748603820800781e-06, 0.0038433074951171875, 0.72509765625, 0.9921875, 0.9990234375, 0.01042938232421875, 0.0030193328857421875, 0.0703125, 0.7197265625, 0.998046875, 0.98291015625, 0.07891845703125, 0.01479339599609375, 0.984375, 0.85009765625, 0.99951171875, 0.9990234375]], "normalized_score": [0.08787761628627777], "tokens": [["\u2581The", "\u2581Rem", "oval", "ists", "\u2581#", "Black", "L", "ives", "M", "atter", ":", "\u2581P", "anel", "\u2581Disc", "uss", "ion", "\u2581on", "\u2581System", "ic", "\u2581R", "ac", "ism"]], "entropy": [[1.3583984375, 1.5390625, 0.0102691650390625, 0.00841522216796875, 1.0458984375, 4.9296875, 3.671875, 1.380859375, 0.07232666015625, 0.0105133056640625, 2.3828125, 5.546875, 4.734375, 1.7861328125, 0.0203094482421875, 0.09808349609375, 3.728515625, 3.4140625, 0.1077880859375, 0.76318359375, 0.00321197509765625, 0.00879669189453125]], "avg_entropy": [1.6646114696155896], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Removalists", "score_of_biggest_cluster": 0.08787761628627777, "score_of_first_answer": 0.08787761628627777, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 7.748603820800781e-07, "avg_score": 0.08787761628627777, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4388488428166494, "std_of_first_answer": 0.4388488428166494, "avg_entropy_of_first_answer": 1.6646114696155896, "lex_sim": 1.0}
{"question": "Who directed the 1949 film, 'The Third Man'?", "answer": ["Reed Hadley #NewRelease #KindleUnlimited - Dare to Desire by Carly Phillips"], "scores": [[0.007732391357421875, 0.84326171875, 0.003993988037109375, 0.9599609375, 4.947185516357422e-06, 0.00015592575073242188, 0.0001366138458251953, 0.321044921875, 0.0018939971923828125, 0.99951171875, 0.88525390625, 0.99853515625, 0.00218963623046875, 0.01971435546875, 0.00788116455078125, 0.77392578125, 0.1103515625, 0.61474609375, 0.07342529296875, 0.8857421875, 0.99951171875, 0.97412109375, 0.9931640625]], "normalized_score": [0.05062990263104439], "tokens": [["\u2581Re", "ed", "\u2581Had", "ley", "\u2581#", "New", "Release", "\u2581#", "Kind", "le", "Un", "limited", "\u2581-", "\u2581D", "are", "\u2581to", "\u2581Des", "ire", "\u2581by", "\u2581Car", "ly", "\u2581Phill", "ips"]], "entropy": [[0.50830078125, 1.3671875, 1.7275390625, 0.188720703125, 1.150390625, 5.3515625, 4.65625, 2.6171875, 5.40625, 0.006519317626953125, 0.75244140625, 0.0148468017578125, 1.7216796875, 5.3203125, 3.3203125, 1.0234375, 3.796875, 0.66943359375, 1.525390625, 0.79736328125, 0.0029735565185546875, 0.1734619140625, 0.045867919921875]], "avg_entropy": [1.8323610554570737], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Carol Reid", "score_of_biggest_cluster": 0.05062990263104439, "score_of_first_answer": 0.05062990263104439, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 4.947185516357422e-06, "avg_score": 0.05062990263104439, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4402300107292478, "std_of_first_answer": 0.4402300107292478, "avg_entropy_of_first_answer": 1.8323610554570737, "lex_sim": 1.0}
{"question": "Who wrote the 1975 novel \u2018The History Man\u2019?", "answer": ["EPHEST"], "scores": [[0.00021505355834960938, 0.0008387565612792969, 4.470348358154297e-06]], "normalized_score": [9.30767273530364e-05], "tokens": [["\u2581EP", "HE", "ST"]], "entropy": [[1.63671875, 5.1328125, 0.11810302734375]], "avg_entropy": [2.2958780924479165], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Malcolm Bradbury", "score_of_biggest_cluster": 9.30767273530364e-05, "score_of_first_answer": 9.30767273530364e-05, "max_score_of_first_answer": 0.0008387565612792969, "min_score_of_first_answer": 4.470348358154297e-06, "avg_score": 9.30767273530364e-05, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.00035424163273488856, "std_of_first_answer": 0.00035424163273488856, "avg_entropy_of_first_answer": 2.2958780924479165, "lex_sim": 1.0}
{"question": "What is the square root of 169?", "answer": ["13 THE NEW YORKER, FEBRUARY 22 & 29, 2016 49 dence than black Americans, and that the risk of being shot by a police o"], "scores": [[0.7119140625, 0.9970703125, 0.97021484375, 2.384185791015625e-07, 0.0002646446228027344, 0.057525634765625, 0.90625, 0.99755859375, 0.54541015625, 0.77392578125, 0.079345703125, 0.9912109375, 0.99951171875, 0.99951171875, 0.9990234375, 0.96875, 0.36279296875, 0.143798828125, 0.216064453125, 0.04779052734375, 0.99951171875, 0.99853515625, 0.99658203125, 0.94140625, 0.98974609375, 0.99951171875, 0.42724609375, 0.92919921875, 0.94287109375, 0.010498046875, 0.1741943359375, 7.581710815429688e-05, 0.01544952392578125, 0.003704071044921875, 0.0011138916015625, 0.10980224609375, 0.263427734375, 0.406494140625, 0.12548828125, 0.2196044921875, 0.01096343994140625, 0.66796875, 0.35009765625, 0.1915283203125, 0.708984375, 0.83056640625, 0.94677734375, 0.2332763671875]], "normalized_score": [0.15693122148513794], "tokens": [["\u2581", "1", "3", "\u2581THE", "\u2581NEW", "\u2581Y", "OR", "K", "ER", ",", "\u2581F", "E", "BR", "U", "ARY", "\u2581", "2", "2", "\u2581&", "\u2581", "2", "9", ",", "\u2581", "2", "0", "1", "6", "\u2581", "4", "9", "\u2581d", "ence", "\u2581than", "\u2581black", "\u2581Americans", ",", "\u2581and", "\u2581that", "\u2581the", "\u2581risk", "\u2581of", "\u2581being", "\u2581shot", "\u2581by", "\u2581a", "\u2581police", "\u2581o"]], "entropy": [[1.181640625, 0.022125244140625, 0.14794921875, 0.744140625, 3.63671875, 4.26171875, 0.384033203125, 0.0261993408203125, 2.017578125, 1.248046875, 2.07421875, 0.07403564453125, 0.004947662353515625, 0.005504608154296875, 0.007762908935546875, 0.225830078125, 1.4150390625, 2.185546875, 0.56982421875, 0.43896484375, 0.0032558441162109375, 0.015380859375, 0.03009033203125, 0.25341796875, 0.056915283203125, 0.0034580230712890625, 0.72314453125, 0.323486328125, 0.406982421875, 1.546875, 1.6484375, 0.89453125, 6.109375, 4.58203125, 5.47265625, 3.662109375, 2.5390625, 2.9921875, 4.50390625, 4.02734375, 4.90234375, 1.4580078125, 3.0703125, 1.60546875, 0.91650390625, 0.55908203125, 0.323974609375, 0.84326171875]], "avg_entropy": [1.5446964104970295], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "13", "score_of_biggest_cluster": 0.15693122148513794, "score_of_first_answer": 0.15693122148513794, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 2.384185791015625e-07, "avg_score": 0.15693122148513794, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40483092356800854, "std_of_first_answer": 0.40483092356800854, "avg_entropy_of_first_answer": 1.5446964104970295, "lex_sim": 1.0}
{"question": "Who was the mother of 'Samantha Stevens' in the television comedy series, 'Bewitched'?", "answer": ["Endora #ThrowbackThursday: A Look Back at the 2017 NHL Entry Draft"], "scores": [[0.55078125, 0.99755859375, 1.3709068298339844e-06, 3.546476364135742e-05, 0.990234375, 0.56689453125, 1.0, 0.9990234375, 0.0010080337524414062, 0.01250457763671875, 0.001811981201171875, 0.50341796875, 0.935546875, 0.529296875, 0.06292724609375, 0.033843994140625, 0.85400390625, 0.2900390625, 0.0362548828125, 0.0052947998046875, 0.102783203125, 0.130859375, 0.9970703125, 0.99951171875]], "normalized_score": [0.06854633241891861], "tokens": [["\u2581End", "ora", "\u2581#", "Throw", "back", "Th", "urs", "day", ":", "\u2581A", "\u2581Look", "\u2581Back", "\u2581at", "\u2581the", "\u2581", "2", "0", "1", "7", "\u2581N", "HL", "\u2581Entry", "\u2581D", "raft"]], "entropy": [[1.921875, 0.01708984375, 1.224609375, 5.1328125, 0.05706787109375, 1.51953125, 0.0018835067749023438, 0.009552001953125, 1.4609375, 3.990234375, 4.62890625, 1.107421875, 0.34814453125, 2.244140625, 4.125, 0.69970703125, 0.6923828125, 1.341796875, 1.736328125, 4.65625, 3.83984375, 2.77734375, 0.0277252197265625, 0.004650115966796875]], "avg_entropy": [1.8152180910110474], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Endora (disambiguation)", "score_of_biggest_cluster": 0.06854633241891861, "score_of_first_answer": 0.06854633241891861, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.3709068298339844e-06, "avg_score": 0.06854633241891861, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41662953248468493, "std_of_first_answer": 0.41662953248468493, "avg_entropy_of_first_answer": 1.8152180910110474, "lex_sim": 1.0}
{"question": "What language was first to be represented in dictionaries and related language theory?", "answer": ["Language of the ancient Phoenicians"], "scores": [[0.0013227462768554688, 0.2398681640625, 0.27001953125, 0.125732421875, 0.00897216796875, 0.99560546875, 0.97314453125]], "normalized_score": [0.0990653783082962], "tokens": [["\u2581Language", "\u2581of", "\u2581the", "\u2581ancient", "\u2581Pho", "en", "icians"]], "entropy": [[3.228515625, 3.12109375, 4.44921875, 4.84765625, 3.611328125, 0.032958984375, 0.13232421875]], "avg_entropy": [2.7747279575892856], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chine\u017fe", "score_of_biggest_cluster": 0.0990653783082962, "score_of_first_answer": 0.0990653783082962, "max_score_of_first_answer": 0.99560546875, "min_score_of_first_answer": 0.0013227462768554688, "avg_score": 0.0990653783082962, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39784054064930185, "std_of_first_answer": 0.39784054064930185, "avg_entropy_of_first_answer": 2.7747279575892856, "lex_sim": 1.0}
{"question": "Translating from the Greek as \"separating circle\", what is the name of the apparent line that separates the earth from the sky?", "answer": ["Horizon THE LUCKY CORNER"], "scores": [[0.79541015625, 0.96923828125, 5.960464477539063e-08, 0.002468109130859375, 0.00015878677368164062, 0.0977783203125, 0.9609375, 0.0043487548828125, 0.06414794921875, 0.07421875]], "normalized_score": [0.011335688643157482], "tokens": [["\u2581Hor", "izon", "\u2581THE", "\u2581L", "U", "CK", "Y", "\u2581C", "OR", "NER"]], "entropy": [[0.9140625, 0.187744140625, 0.97314453125, 2.078125, 1.02734375, 3.267578125, 0.276123046875, 2.388671875, 2.48828125, 1.71484375]], "avg_entropy": [1.531591796875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Horizon distance", "score_of_biggest_cluster": 0.011335688643157482, "score_of_first_answer": 0.011335688643157482, "max_score_of_first_answer": 0.96923828125, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.011335688643157482, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4041519670001189, "std_of_first_answer": 0.4041519670001189, "avg_entropy_of_first_answer": 1.531591796875, "lex_sim": 1.0}
{"question": "Which disastrous amphibious landing in 1942 used British troops, Canadian troops and US Rangers?", "answer": ["Dieppe Raid"], "scores": [[0.578125, 0.9990234375, 0.18896484375, 1.0]], "normalized_score": [0.5747706890106201], "tokens": [["\u2581Die", "ppe", "\u2581Ra", "id"]], "entropy": [[1.826171875, 0.00868988037109375, 1.4619140625, 0.0009975433349609375]], "avg_entropy": [0.8244433403015137], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dieppe, France", "score_of_biggest_cluster": 0.5747706890106201, "score_of_first_answer": 0.5747706890106201, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.18896484375, "avg_score": 0.5747706890106201, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33731972418458867, "std_of_first_answer": 0.33731972418458867, "avg_entropy_of_first_answer": 0.8244433403015137, "lex_sim": 1.0}
{"question": "Regarded as its Nobel Prize, in what field is the annual Pritzer Prize awarded?", "answer": ["Musical composition (Composition) #Review: The 100 by Kass Morgan"], "scores": [[0.004047393798828125, 0.6708984375, 0.0070648193359375, 0.08203125, 0.97705078125, 0.95263671875, 4.172325134277344e-07, 3.30805778503418e-05, 0.0017490386962890625, 0.0120849609375, 0.3154296875, 0.00096893310546875, 0.2393798828125, 0.091064453125, 0.460205078125, 0.0008397102355957031, 0.8251953125, 0.99951171875, 0.99951171875]], "normalized_score": [0.026231369003653526], "tokens": [["\u2581Musical", "\u2581composition", "\u2581(", "Com", "position", ")", "\u2581#", "Re", "view", ":", "\u2581The", "\u2581", "1", "0", "0", "\u2581by", "\u2581K", "ass", "\u2581Morgan"]], "entropy": [[2.1171875, 1.0830078125, 0.9541015625, 3.796875, 0.12445068359375, 0.31396484375, 0.74462890625, 5.90625, 2.859375, 4.0, 4.59375, 1.0087890625, 2.17578125, 1.58203125, 2.70703125, 3.99609375, 1.3017578125, 0.005321502685546875, 0.002925872802734375]], "avg_entropy": [2.0670170031095805], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Architecturally", "score_of_biggest_cluster": 0.026231369003653526, "score_of_first_answer": 0.026231369003653526, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 4.172325134277344e-07, "avg_score": 0.026231369003653526, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40097070342618035, "std_of_first_answer": 0.40097070342618035, "avg_entropy_of_first_answer": 2.0670170031095805, "lex_sim": 1.0}
{"question": "In 1932 which British physicist discovered the part of the nucleus of an atom which was later called the neutron?", "answer": ["James Chadwick"], "scores": [[0.83935546875, 0.99609375, 0.99755859375, 0.99951171875]], "normalized_score": [0.9555273056030273], "tokens": [["\u2581James", "\u2581Ch", "ad", "wick"]], "entropy": [[0.80859375, 0.035980224609375, 0.0242156982421875, 0.00395965576171875]], "avg_entropy": [0.2181873321533203], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "(James) CHADWICK", "score_of_biggest_cluster": 0.9555273056030273, "score_of_first_answer": 0.9555273056030273, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.83935546875, "avg_score": 0.9555273056030273, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06858515918088882, "std_of_first_answer": 0.06858515918088882, "avg_entropy_of_first_answer": 0.2181873321533203, "lex_sim": 1.0}
{"question": "The 'Gowk' is an old dialect word for which bird?", "answer": ["Cuckoo"], "scores": [[0.282470703125, 0.99853515625, 0.9912109375]], "normalized_score": [0.6538843512535095], "tokens": [["\u2581C", "uck", "oo"]], "entropy": [[1.544921875, 0.01239776611328125, 0.0709228515625]], "avg_entropy": [0.5427474975585938], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The cuckoo", "score_of_biggest_cluster": 0.6538843512535095, "score_of_first_answer": 0.6538843512535095, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.282470703125, "avg_score": 0.6538843512535095, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.33584299655765626, "std_of_first_answer": 0.33584299655765626, "avg_entropy_of_first_answer": 0.5427474975585938, "lex_sim": 1.0}
{"question": "What is the only sequel to win an Academy Award for Best Picture?", "answer": ["The Godfather Part II (1974) #1449 \u2013 1999 37c Love series: 1st Day Cover"], "scores": [[0.89013671875, 0.95849609375, 0.99951171875, 0.583984375, 0.9990234375, 0.0775146484375, 0.99658203125, 1.0, 1.0, 0.99951171875, 0.97802734375, 7.152557373046875e-07, 0.0572509765625, 0.03009033203125, 0.064697265625, 0.06396484375, 2.2530555725097656e-05, 0.345703125, 0.434326171875, 0.58056640625, 0.09869384765625, 0.0838623046875, 0.0213775634765625, 0.0194549560546875, 0.033477783203125, 0.0006613731384277344, 0.0007739067077636719, 0.367431640625, 0.00833892822265625, 0.04376220703125, 0.452880859375, 0.11492919921875, 0.33642578125, 0.00604248046875]], "normalized_score": [0.06889383494853973], "tokens": [["\u2581The", "\u2581God", "father", "\u2581Part", "\u2581II", "\u2581(", "1", "9", "7", "4", ")", "\u2581#", "1", "4", "4", "9", "\u2581\u2013", "\u2581", "1", "9", "9", "9", "\u2581", "3", "7", "c", "\u2581Love", "\u2581series", ":", "\u2581", "1", "st", "\u2581Day", "\u2581Cover"]], "entropy": [[0.71240234375, 0.28564453125, 0.00579833984375, 0.76708984375, 0.01025390625, 1.0224609375, 0.0352783203125, 0.0007205009460449219, 0.000667572021484375, 0.00435638427734375, 0.135498046875, 0.77392578125, 5.5390625, 3.294921875, 3.419921875, 2.884765625, 2.734375, 4.67578125, 2.015625, 1.970703125, 1.5771484375, 1.97265625, 3.267578125, 3.72265625, 3.31640625, 3.939453125, 5.1640625, 2.859375, 2.8046875, 4.6796875, 1.9306640625, 1.4873046875, 2.568359375, 1.046875]], "avg_entropy": [2.0772402006037094], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Godfather Part 2", "score_of_biggest_cluster": 0.06889383494853973, "score_of_first_answer": 0.06889383494853973, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 7.152557373046875e-07, "avg_score": 0.06889383494853973, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.40015177282853337, "std_of_first_answer": 0.40015177282853337, "avg_entropy_of_first_answer": 2.0772402006037094, "lex_sim": 1.0}
{"question": "\"Commenting on the death of screenwriter Ruth Prawer Jhabvala in April 2013, which film company issued the statement \"\"The passing of our two-time Academy Award winning screenwriter is a significant loss to the global film community.\"\"?\"", "answer": ["Merchant Ivory Productions"], "scores": [[0.91455078125, 0.9814453125, 0.9423828125, 1.0, 0.99951171875, 0.8076171875, 1.0]], "normalized_score": [0.946951687335968], "tokens": [["\u2581Mer", "chant", "\u2581I", "v", "ory", "\u2581Produ", "ctions"]], "entropy": [[0.63720703125, 0.1561279296875, 0.291748046875, 0.0007891654968261719, 0.004405975341796875, 0.65185546875, 0.0006303787231445312]], "avg_entropy": [0.24896628516060965], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Merchant Ivory Productions", "score_of_biggest_cluster": 0.946951687335968, "score_of_first_answer": 0.946951687335968, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.8076171875, "avg_score": 0.946951687335968, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06549249586390807, "std_of_first_answer": 0.06549249586390807, "avg_entropy_of_first_answer": 0.24896628516060965, "lex_sim": 1.0}
{"question": "What is the only letter of the alphabet which does not appear in any of the names of the 50 American states?", "answer": ["3"], "scores": [[0.0005068778991699219, 0.1866455078125]], "normalized_score": [0.009726587682962418], "tokens": [["\u2581", "3"]], "entropy": [[1.4013671875, 3.666015625]], "avg_entropy": [2.53369140625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u24e0", "score_of_biggest_cluster": 0.009726587682962418, "score_of_first_answer": 0.009726587682962418, "max_score_of_first_answer": 0.1866455078125, "min_score_of_first_answer": 0.0005068778991699219, "avg_score": 0.009726587682962418, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09306931495666504, "std_of_first_answer": 0.09306931495666504, "avg_entropy_of_first_answer": 2.53369140625, "lex_sim": 1.0}
{"question": "Also a plumber, what is the name of 'Super Mario's' older brother?", "answer": ["Joseph Mario"], "scores": [[0.010772705078125, 0.86767578125]], "normalized_score": [0.09668098390102386], "tokens": [["\u2581Joseph", "\u2581Mario"]], "entropy": [[1.21484375, 0.6484375]], "avg_entropy": [0.931640625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mr. L", "score_of_biggest_cluster": 0.09668098390102386, "score_of_first_answer": 0.09668098390102386, "max_score_of_first_answer": 0.86767578125, "min_score_of_first_answer": 0.010772705078125, "avg_score": 0.09668098390102386, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4284515380859375, "std_of_first_answer": 0.4284515380859375, "avg_entropy_of_first_answer": 0.931640625, "lex_sim": 1.0}
{"question": "Caroline of Brunswick was the queen consort of which British King?", "answer": ["George IV #MindfulMonday: The Art of Letting Go"], "scores": [[0.349609375, 0.98681640625, 1.1920928955078125e-07, 0.0019664764404296875, 0.0095672607421875, 0.006168365478515625, 0.04595947265625, 0.54931640625, 0.99169921875, 0.0016613006591796875, 0.046356201171875, 0.011566162109375, 0.97900390625, 0.043609619140625, 0.99951171875, 0.9892578125]], "normalized_score": [0.03473643958568573], "tokens": [["\u2581George", "\u2581IV", "\u2581#", "M", "ind", "ful", "M", "ond", "ay", ":", "\u2581The", "\u2581Art", "\u2581of", "\u2581Let", "ting", "\u2581Go"]], "entropy": [[1.2265625, 0.10723876953125, 1.109375, 5.18359375, 5.40234375, 3.48046875, 2.8828125, 1.55078125, 0.0484619140625, 1.3603515625, 4.88671875, 5.91015625, 0.1483154296875, 2.8984375, 0.0026493072509765625, 0.08074951171875]], "avg_entropy": [2.267438530921936], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George, Prince Regent", "score_of_biggest_cluster": 0.03473643958568573, "score_of_first_answer": 0.03473643958568573, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 1.1920928955078125e-07, "avg_score": 0.03473643958568573, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4376588294513178, "std_of_first_answer": 0.4376588294513178, "avg_entropy_of_first_answer": 2.267438530921936, "lex_sim": 1.0}
{"question": "The currency of Venezuela is named after which revolutionary?", "answer": ["Sim\u00f3n Bol\u00edvar #52 Ancestors in 52 Weeks \u2013 Week 8: Grandmother"], "scores": [[0.058837890625, 0.0928955078125, 0.87255859375, 0.99560546875, 0.99951171875, 3.5762786865234375e-07, 0.030914306640625, 0.00112152099609375, 8.064508438110352e-05, 0.0299224853515625, 0.7470703125, 0.196044921875, 0.112548828125, 0.99658203125, 0.8916015625, 0.99951171875, 0.958984375, 0.99951171875, 0.00809478759765625, 0.90185546875, 0.9833984375, 0.04327392578125, 0.356689453125, 0.0240478515625, 0.4091796875, 0.7578125]], "normalized_score": [0.09329701960086823], "tokens": [["\u2581Sim", "\u00f3n", "\u2581Bol", "\u00ed", "var", "\u2581#", "5", "2", "\u2581An", "c", "est", "ors", "\u2581in", "\u2581", "5", "2", "\u2581We", "eks", "\u2581\u2013", "\u2581Week", "\u2581", "8", ":", "\u2581Grand", "m", "other"]], "entropy": [[2.484375, 1.1435546875, 0.53173828125, 0.0297088623046875, 0.005825042724609375, 0.693359375, 4.0, 1.134765625, 2.716796875, 3.921875, 1.23046875, 1.42578125, 2.091796875, 0.03729248046875, 0.35400390625, 0.00359344482421875, 0.1842041015625, 0.0034656524658203125, 1.9453125, 0.66162109375, 0.099365234375, 1.654296875, 1.77734375, 3.482421875, 1.6796875, 0.55615234375]], "avg_entropy": [1.3018771685086763], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Santiago Bobadilla", "score_of_biggest_cluster": 0.09329701960086823, "score_of_first_answer": 0.09329701960086823, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 3.5762786865234375e-07, "avg_score": 0.09329701960086823, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.427601103343752, "std_of_first_answer": 0.427601103343752, "avg_entropy_of_first_answer": 1.3018771685086763, "lex_sim": 1.0}
{"question": "The Suez Canal connects the Mediterranean Sea to which other Sea?", "answer": ["The Red Sea."], "scores": [[0.7900390625, 0.9677734375, 0.81103515625, 0.056854248046875, 1.7881393432617188e-06, 1.0073184967041016e-05, 0.00528717041015625, 0.0565185546875, 0.0875244140625, 0.176513671875, 0.00098419189453125, 0.9931640625, 0.640625, 0.00159454345703125, 0.06201171875, 0.9794921875, 0.8623046875, 0.0049591064453125, 0.84716796875, 0.8466796875, 0.75, 0.98193359375]], "normalized_score": [0.056848544627428055], "tokens": [["\u2581The", "\u2581Red", "\u2581Sea", ".", "\u2581#", "Book", "Re", "view", ":", "\u2581The", "\u2581Sec", "rets", "\u2581of", "\u2581Villa", "\u2581Ros", "so", "\u2581by", "\u2581J", "ill", "\u2581Ste", "e", "ples"]], "entropy": [[1.04296875, 0.2344970703125, 0.67578125, 0.84033203125, 0.456298828125, 5.39453125, 3.873046875, 0.346435546875, 2.07421875, 2.771484375, 3.732421875, 0.05255126953125, 1.0283203125, 3.134765625, 3.947265625, 0.1810302734375, 0.63134765625, 1.0, 0.98828125, 0.689453125, 0.79638671875, 0.09234619140625]], "avg_entropy": [1.5447165749289773], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Arabicus Sinus", "score_of_biggest_cluster": 0.056848544627428055, "score_of_first_answer": 0.056848544627428055, "max_score_of_first_answer": 0.9931640625, "min_score_of_first_answer": 1.7881393432617188e-06, "avg_score": 0.056848544627428055, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4185160142352208, "std_of_first_answer": 0.4185160142352208, "avg_entropy_of_first_answer": 1.5447165749289773, "lex_sim": 1.0}
{"question": "Where would you find 'Timbuktu?", "answer": ["Mali #MoscowMitch #MoscowMitchTrump #MoscowMitchTrumpPoll #MitchMcConnell #Cocaine Mitch #TrumpRussia #KremlinAn"], "scores": [[0.85546875, 0.99755859375, 3.5762786865234375e-07, 0.005382537841796875, 2.205371856689453e-05, 0.99169921875, 0.0098724365234375, 0.0025539398193359375, 0.1719970703125, 0.09393310546875, 0.890625, 0.9990234375, 0.69140625, 0.96337890625, 0.24853515625, 0.1043701171875, 0.732421875, 0.2398681640625, 0.90234375, 0.99951171875, 0.87255859375, 0.9970703125, 0.08709716796875, 0.97607421875, 0.00719451904296875, 0.0440673828125, 0.4296875, 0.67236328125, 0.0144805908203125, 0.248046875, 0.97607421875, 0.98193359375, 0.77294921875, 0.00530242919921875, 0.1541748046875, 0.442138671875, 0.09088134765625, 0.8818359375, 0.05157470703125, 0.9658203125, 0.189697265625, 0.99853515625, 0.8251953125, 0.04730224609375, 0.99658203125, 1.0, 0.998046875, 0.56103515625]], "normalized_score": [0.15835809707641602], "tokens": [["\u2581M", "ali", "\u2581#", "M", "osc", "ow", "M", "itch", "\u2581#", "M", "osc", "ow", "M", "itch", "Tr", "ump", "\u2581#", "M", "osc", "ow", "M", "itch", "Tr", "ump", "P", "oll", "\u2581#", "M", "itch", "Mc", "Con", "nell", "\u2581#", "C", "oca", "ine", "\u2581Mitch", "\u2581#", "Tr", "ump", "Russ", "ia", "\u2581#", "K", "re", "ml", "in", "An"]], "entropy": [[0.802734375, 0.027984619140625, 1.0234375, 4.33984375, 1.578125, 0.07672119140625, 1.9306640625, 2.2578125, 2.083984375, 5.37109375, 0.765625, 0.0090484619140625, 1.423828125, 0.272216796875, 3.08984375, 0.396484375, 1.009765625, 3.78515625, 0.466796875, 0.004474639892578125, 0.70849609375, 0.0271453857421875, 3.34765625, 0.1483154296875, 2.451171875, 1.798828125, 1.037109375, 2.181640625, 0.1566162109375, 1.9453125, 0.1505126953125, 0.12432861328125, 1.2001953125, 2.2890625, 2.798828125, 0.7900390625, 1.4765625, 0.50634765625, 3.208984375, 0.18017578125, 2.533203125, 0.010467529296875, 0.783203125, 3.05859375, 0.032196044921875, 0.00033545494079589844, 0.016754150390625, 1.2939453125]], "avg_entropy": [1.3535764167706172], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "In the African country of Mali", "score_of_biggest_cluster": 0.15835809707641602, "score_of_first_answer": 0.15835809707641602, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 3.5762786865234375e-07, "avg_score": 0.15835809707641602, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4108098105705873, "std_of_first_answer": 0.4108098105705873, "avg_entropy_of_first_answer": 1.3535764167706172, "lex_sim": 1.0}
{"question": "Great short-stories like The Gift of the Magi, The Ransom of Red Chief, and The Cop and the Anthem are from what collection of O. Henry that was a reaction to a statement that only four hundred people mattered in New York?", "answer": ["The Four Hundred"], "scores": [[0.93701171875, 0.64892578125, 0.78759765625, 0.99853515625]], "normalized_score": [0.8315759897232056], "tokens": [["\u2581The", "\u2581Four", "\u2581H", "undred"]], "entropy": [[0.473388671875, 1.5166015625, 0.74267578125, 0.01227569580078125]], "avg_entropy": [0.6862354278564453], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Four Million", "score_of_biggest_cluster": 0.8315759897232056, "score_of_first_answer": 0.8315759897232056, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.64892578125, "avg_score": 0.8315759897232056, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13579728359994458, "std_of_first_answer": 0.13579728359994458, "avg_entropy_of_first_answer": 0.6862354278564453, "lex_sim": 1.0}
{"question": "Which British king married Mary of Teck?", "answer": ["George V"], "scores": [[0.29345703125, 0.94677734375]], "normalized_score": [0.527103841304779], "tokens": [["\u2581George", "\u2581V"]], "entropy": [[0.865234375, 0.319091796875]], "avg_entropy": [0.5921630859375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George V of Great Britain and Ireland", "score_of_biggest_cluster": 0.527103841304779, "score_of_first_answer": 0.527103841304779, "max_score_of_first_answer": 0.94677734375, "min_score_of_first_answer": 0.29345703125, "avg_score": 0.527103841304779, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.32666015625, "std_of_first_answer": 0.32666015625, "avg_entropy_of_first_answer": 0.5921630859375, "lex_sim": 1.0}
{"question": "Which line on the London Underground rail system has the most stations?", "answer": ["Jubilee line"], "scores": [[0.0030956268310546875, 0.9951171875, 0.98828125, 0.99951171875, 0.3525390625]], "normalized_score": [0.25474151968955994], "tokens": [["\u2581J", "ub", "ile", "e", "\u2581line"]], "entropy": [[1.658203125, 0.046234130859375, 0.0938720703125, 0.00756072998046875, 1.453125]], "avg_entropy": [0.6517990112304688], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "District line", "score_of_biggest_cluster": 0.25474151968955994, "score_of_first_answer": 0.25474151968955994, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.0030956268310546875, "avg_score": 0.25474151968955994, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4149936766218835, "std_of_first_answer": 0.4149936766218835, "avg_entropy_of_first_answer": 0.6517990112304688, "lex_sim": 1.0}
{"question": "In Egyptian mythology, of what is Nut the goddess?", "answer": ["Sky"], "scores": [[0.203857421875]], "normalized_score": [0.203857421875], "tokens": [["\u2581Sky"]], "entropy": [[2.224609375]], "avg_entropy": [2.224609375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Sky", "score_of_biggest_cluster": 0.203857421875, "score_of_first_answer": 0.203857421875, "max_score_of_first_answer": 0.203857421875, "min_score_of_first_answer": 0.203857421875, "avg_score": 0.203857421875, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.224609375, "lex_sim": 1.0}
{"question": "Who was the author of 'Daniel Deronda' published in 1876?", "answer": ["George Eliot"], "scores": [[0.95654296875, 0.9423828125, 0.99853515625]], "normalized_score": [0.9655284285545349], "tokens": [["\u2581George", "\u2581Eli", "ot"]], "entropy": [[0.416015625, 0.4375, 0.0148162841796875]], "avg_entropy": [0.2894439697265625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mary Ann Evans", "score_of_biggest_cluster": 0.9655284285545349, "score_of_first_answer": 0.9655284285545349, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.9423828125, "avg_score": 0.9655284285545349, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.023844262167186596, "std_of_first_answer": 0.023844262167186596, "avg_entropy_of_first_answer": 0.2894439697265625, "lex_sim": 1.0}
{"question": "Odontology is the scientific study of what?", "answer": ["TEETH #TheRedQuest: Why is this story not working?"], "scores": [[0.005344390869140625, 0.994140625, 0.99658203125, 6.318092346191406e-06, 0.007198333740234375, 1.2636184692382812e-05, 9.101629257202148e-05, 0.611328125, 0.012725830078125, 0.0007290840148925781, 0.103515625, 0.0321044921875, 0.0011310577392578125, 0.0231170654296875, 0.005397796630859375, 0.78173828125]], "normalized_score": [0.008366160094738007], "tokens": [["\u2581TE", "ET", "H", "\u2581#", "The", "Red", "Qu", "est", ":", "\u2581Why", "\u2581is", "\u2581this", "\u2581story", "\u2581not", "\u2581working", "?"]], "entropy": [[1.0693359375, 0.044769287109375, 0.0321044921875, 1.1416015625, 5.203125, 3.806640625, 1.6318359375, 1.7666015625, 2.517578125, 5.46484375, 2.58984375, 3.8203125, 3.48046875, 3.892578125, 4.9921875, 1.1435546875]], "avg_entropy": [2.6623363494873047], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tooth root", "score_of_biggest_cluster": 0.008366160094738007, "score_of_first_answer": 0.008366160094738007, "max_score_of_first_answer": 0.99658203125, "min_score_of_first_answer": 6.318092346191406e-06, "avg_score": 0.008366160094738007, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3691222893580278, "std_of_first_answer": 0.3691222893580278, "avg_entropy_of_first_answer": 2.6623363494873047, "lex_sim": 1.0}
{"question": "Chanterelle and Oyster are types of what?", "answer": ["Mushrooms"], "scores": [[0.806640625, 0.99951171875, 0.6845703125]], "normalized_score": [0.8202797770500183], "tokens": [["\u2581M", "ush", "rooms"]], "entropy": [[0.81591796875, 0.00606536865234375, 0.64013671875]], "avg_entropy": [0.48737335205078125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Psychoactive mushroom", "score_of_biggest_cluster": 0.8202797770500183, "score_of_first_answer": 0.8202797770500183, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.6845703125, "avg_score": 0.8202797770500183, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1296527453404741, "std_of_first_answer": 0.1296527453404741, "avg_entropy_of_first_answer": 0.48737335205078125, "lex_sim": 1.0}
{"question": "What is the basic monetary unit of Israel?", "answer": ["\u5784"], "scores": [[0.01486968994140625, 0.00039315223693847656, 0.0021381378173828125, 0.28759765625]], "normalized_score": [0.007743209600448608], "tokens": [["\u2581", "<0xE5>", "<0x9E>", "<0x84>"]], "entropy": [[1.8515625, 2.5234375, 2.65625, 1.943359375]], "avg_entropy": [2.24365234375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Shekel", "score_of_biggest_cluster": 0.007743209600448608, "score_of_first_answer": 0.007743209600448608, "max_score_of_first_answer": 0.28759765625, "min_score_of_first_answer": 0.00039315223693847656, "avg_score": 0.007743209600448608, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12214970705400387, "std_of_first_answer": 0.12214970705400387, "avg_entropy_of_first_answer": 2.24365234375, "lex_sim": 0.0}
{"question": "In the British military, a bombadier is equivalent to which other rank?", "answer": ["Corporal"], "scores": [[0.0784912109375, 0.99951171875]], "normalized_score": [0.2800944149494171], "tokens": [["\u2581Corpor", "al"]], "entropy": [[2.962890625, 0.00408935546875]], "avg_entropy": [1.483489990234375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Corporal", "score_of_biggest_cluster": 0.2800944149494171, "score_of_first_answer": 0.2800944149494171, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.0784912109375, "avg_score": 0.2800944149494171, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.46051025390625, "std_of_first_answer": 0.46051025390625, "avg_entropy_of_first_answer": 1.483489990234375, "lex_sim": 1.0}
{"question": "What is the waxy secretion of the sperm whale used in perfume manufacturing", "answer": ["Ambergris #176) The New Yorker and the Environmental Movement"], "scores": [[0.8271484375, 0.9853515625, 0.9990234375, 0.998046875, 1.0728836059570312e-06, 0.036651611328125, 0.03594970703125, 0.039642333984375, 0.00044155120849609375, 0.0015153884887695312, 0.0024890899658203125, 0.483642578125, 0.33935546875, 0.004245758056640625, 0.0838623046875, 5.0961971282958984e-05, 0.525390625, 0.0007243156433105469, 0.9990234375]], "normalized_score": [0.023170797154307365], "tokens": [["\u2581Am", "ber", "gr", "is", "\u2581#", "1", "7", "6", ")", "\u2581The", "\u2581New", "\u2581York", "er", "\u2581and", "\u2581the", "\u2581Environment", "al", "\u2581Mov", "ement"]], "entropy": [[0.91845703125, 0.09228515625, 0.01088714599609375, 0.017120361328125, 0.7822265625, 4.6015625, 3.369140625, 3.337890625, 2.9375, 2.62890625, 5.42578125, 3.12890625, 2.54296875, 3.478515625, 5.2890625, 5.50390625, 2.29296875, 0.6162109375, 0.00968170166015625]], "avg_entropy": [2.4728409616570723], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ambarum", "score_of_biggest_cluster": 0.023170797154307365, "score_of_first_answer": 0.023170797154307365, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 1.0728836059570312e-06, "avg_score": 0.023170797154307365, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4073678171515242, "std_of_first_answer": 0.4073678171515242, "avg_entropy_of_first_answer": 2.4728409616570723, "lex_sim": 1.0}
{"question": "Indonesia's internet country TLD (top level domain) is?", "answer": ["IDN ccTLD"], "scores": [[0.0244598388671875, 0.31689453125, 0.0027008056640625, 0.99072265625, 0.998046875]], "normalized_score": [0.1156625896692276], "tokens": [["\u2581ID", "N", "\u2581cc", "T", "LD"]], "entropy": [[0.6884765625, 1.2060546875, 1.4951171875, 0.0667724609375, 0.016204833984375]], "avg_entropy": [0.694525146484375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": ".go.id", "score_of_biggest_cluster": 0.1156625896692276, "score_of_first_answer": 0.1156625896692276, "max_score_of_first_answer": 0.998046875, "min_score_of_first_answer": 0.0027008056640625, "avg_score": 0.1156625896692276, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44502639147853573, "std_of_first_answer": 0.44502639147853573, "avg_entropy_of_first_answer": 0.694525146484375, "lex_sim": 1.0}
{"question": "In the culinary world, what word is used for the edible offal of a fowl, typically including the heart, gizzard and liver?", "answer": ["Calomel"], "scores": [[0.04278564453125, 0.078857421875, 0.955078125]], "normalized_score": [0.14770430326461792], "tokens": [["\u2581Cal", "om", "el"]], "entropy": [[3.189453125, 2.6640625, 0.3720703125]], "avg_entropy": [2.0751953125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Giblets", "score_of_biggest_cluster": 0.14770430326461792, "score_of_first_answer": 0.14770430326461792, "max_score_of_first_answer": 0.955078125, "min_score_of_first_answer": 0.04278564453125, "avg_score": 0.14770430326461792, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42181373767542774, "std_of_first_answer": 0.42181373767542774, "avg_entropy_of_first_answer": 2.0751953125, "lex_sim": 1.0}
{"question": "Who won the 2010 Boston Marathon with a time of 2:05:52, easily breaking the course record of 2:07:14?", "answer": ["Men's division"], "scores": [[0.038238525390625, 0.703125, 1.0, 0.0248870849609375]], "normalized_score": [0.16083364188671112], "tokens": [["\u2581Men", "'", "s", "\u2581division"]], "entropy": [[2.767578125, 1.2373046875, 0.0006804466247558594, 2.017578125]], "avg_entropy": [1.505785346031189], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Robert Kiprono Cheruiyot", "score_of_biggest_cluster": 0.16083364188671112, "score_of_first_answer": 0.16083364188671112, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0248870849609375, "avg_score": 0.16083364188671112, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42324815794197074, "std_of_first_answer": 0.42324815794197074, "avg_entropy_of_first_answer": 1.505785346031189, "lex_sim": 1.0}
{"question": "Which English civil engineer's most famous project was the construction of Tower Bridge over the River Thames in London?", "answer": ["Horace Jones"], "scores": [[0.88232421875, 0.99951171875, 0.97998046875]], "normalized_score": [0.952528178691864], "tokens": [["\u2581Hor", "ace", "\u2581Jones"]], "entropy": [[0.436279296875, 0.002864837646484375, 0.194580078125]], "avg_entropy": [0.21124140421549478], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sir John Wolfe-Barry", "score_of_biggest_cluster": 0.952528178691864, "score_of_first_answer": 0.952528178691864, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.88232421875, "avg_score": 0.952528178691864, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.051263072096431064, "std_of_first_answer": 0.051263072096431064, "avg_entropy_of_first_answer": 0.21124140421549478, "lex_sim": 1.0}
{"question": "Who wrote the play \u2018The Iceman Cometh\u2019?", "answer": ["ICEMAN COMETH"], "scores": [[0.00032591819763183594, 0.0205535888671875, 0.83984375, 0.06964111328125, 0.9970703125, 0.9951171875]], "normalized_score": [0.08542991429567337], "tokens": [["\u2581I", "CE", "MAN", "\u2581CO", "ME", "TH"]], "entropy": [[0.2548828125, 3.03125, 1.1142578125, 3.205078125, 0.0299530029296875, 0.04168701171875]], "avg_entropy": [1.2795181274414062], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "SS Glencairn", "score_of_biggest_cluster": 0.08542991429567337, "score_of_first_answer": 0.08542991429567337, "max_score_of_first_answer": 0.9970703125, "min_score_of_first_answer": 0.00032591819763183594, "avg_score": 0.08542991429567337, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4603378863691045, "std_of_first_answer": 0.4603378863691045, "avg_entropy_of_first_answer": 1.2795181274414062, "lex_sim": 1.0}
{"question": "In Germany, what is an autobahn?", "answer": ["network"], "scores": [[0.0007033348083496094]], "normalized_score": [0.0007033348665572703], "tokens": [["\u2581network"]], "entropy": [[1.3857421875]], "avg_entropy": [1.3857421875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Elevated freeway", "score_of_biggest_cluster": 0.0007033348665572703, "score_of_first_answer": 0.0007033348665572703, "max_score_of_first_answer": 0.0007033348083496094, "min_score_of_first_answer": 0.0007033348083496094, "avg_score": 0.0007033348665572703, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.3857421875, "lex_sim": 1.0}
{"question": "Which British motorcycle manufacture produced models called 'Bonneville' and 'Thunderbird'?", "answer": ["Triumph"], "scores": [[0.9755859375, 0.9990234375]], "normalized_score": [0.9872351288795471], "tokens": [["\u2581Tri", "umph"]], "entropy": [[0.2003173828125, 0.0109710693359375]], "avg_entropy": [0.10564422607421875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "St Joseph's College, Geelong", "score_of_biggest_cluster": 0.9872351288795471, "score_of_first_answer": 0.9872351288795471, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.9755859375, "avg_score": 0.9872351288795471, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01171875, "std_of_first_answer": 0.01171875, "avg_entropy_of_first_answer": 0.10564422607421875, "lex_sim": 1.0}
{"question": "Who sailed in the yacht 'Suhaili'?", "answer": ["Smeets, Robin Knox-"], "scores": [[0.0041046142578125, 0.0178985595703125, 0.1395263671875, 0.0994873046875, 0.435302734375, 0.259033203125, 0.96826171875, 0.99560546875]], "normalized_score": [0.135080486536026], "tokens": [["\u2581S", "me", "ets", ",", "\u2581Robin", "\u2581Kno", "x", "-"]], "entropy": [[1.0380859375, 3.5078125, 3.388671875, 3.24609375, 3.484375, 1.681640625, 0.1566162109375, 0.039642333984375]], "avg_entropy": [2.0678672790527344], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Robin Knox-Johnson", "score_of_biggest_cluster": 0.135080486536026, "score_of_first_answer": 0.135080486536026, "max_score_of_first_answer": 0.99560546875, "min_score_of_first_answer": 0.0041046142578125, "avg_score": 0.135080486536026, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37911402619060214, "std_of_first_answer": 0.37911402619060214, "avg_entropy_of_first_answer": 2.0678672790527344, "lex_sim": 1.0}
{"question": "Who commanded the Prussian army at the Battle of Waterloo in 1815?", "answer": ["Gebhard Leberecht von Bl\u00fccher"], "scores": [[0.8994140625, 0.998046875, 0.828125, 0.998046875, 1.0, 0.99169921875, 0.9990234375, 0.75537109375, 1.0]], "normalized_score": [0.9367164969444275], "tokens": [["\u2581Geb", "hard", "\u2581Leb", "ere", "cht", "\u2581von", "\u2581Bl", "\u00fc", "cher"]], "entropy": [[0.6708984375, 0.0167388916015625, 0.58349609375, 0.01485443115234375, 0.0008611679077148438, 0.06707763671875, 0.007633209228515625, 1.0419921875, 0.00011980533599853516]], "avg_entropy": [0.2670746511883206], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Von Bluecher", "score_of_biggest_cluster": 0.9367164969444275, "score_of_first_answer": 0.9367164969444275, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.75537109375, "avg_score": 0.9367164969444275, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08713719066904915, "std_of_first_answer": 0.08713719066904915, "avg_entropy_of_first_answer": 0.2670746511883206, "lex_sim": 1.0}
{"question": "What is the name of the fuel depot near Hemel Hempstead that caught fire in December 2005?", "answer": ["Buncefield"], "scores": [[0.95166015625, 0.99951171875, 0.99951171875]], "normalized_score": [0.9832996726036072], "tokens": [["\u2581Bun", "ce", "field"]], "entropy": [[0.38134765625, 0.004413604736328125, 0.0037136077880859375]], "avg_entropy": [0.12982495625813803], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Buncefield", "score_of_biggest_cluster": 0.9832996726036072, "score_of_first_answer": 0.9832996726036072, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.95166015625, "avg_score": 0.9832996726036072, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0225574428894146, "std_of_first_answer": 0.0225574428894146, "avg_entropy_of_first_answer": 0.12982495625813803, "lex_sim": 1.0}
{"question": "Which King of England sold Dunkirk to Louis XIV of France?", "answer": ["Edward I (king of England)"], "scores": [[0.0013580322265625, 0.339599609375, 0.037139892578125, 0.003253936767578125, 0.9384765625, 0.9814453125, 0.60107421875]], "normalized_score": [0.08453747630119324], "tokens": [["\u2581Edward", "\u2581I", "\u2581(", "king", "\u2581of", "\u2581England", ")"]], "entropy": [[1.3876953125, 1.576171875, 1.3369140625, 0.75830078125, 0.36181640625, 0.1180419921875, 1.322265625]], "avg_entropy": [0.9801722935267857], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "King Charles the Second", "score_of_biggest_cluster": 0.08453747630119324, "score_of_first_answer": 0.08453747630119324, "max_score_of_first_answer": 0.9814453125, "min_score_of_first_answer": 0.0013580322265625, "avg_score": 0.08453747630119324, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3997514569155277, "std_of_first_answer": 0.3997514569155277, "avg_entropy_of_first_answer": 0.9801722935267857, "lex_sim": 1.0}
{"question": "The ashes of author Alfred Wainwright were scattered on the summit of which Lake District fell?", "answer": ["Haystacks"], "scores": [[0.71875, 0.8515625, 0.998046875]], "normalized_score": [0.8484933376312256], "tokens": [["\u2581Hay", "stack", "s"]], "entropy": [[1.490234375, 0.654296875, 0.0185394287109375]], "avg_entropy": [0.7210235595703125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hay bale", "score_of_biggest_cluster": 0.8484933376312256, "score_of_first_answer": 0.8484933376312256, "max_score_of_first_answer": 0.998046875, "min_score_of_first_answer": 0.71875, "avg_score": 0.8484933376312256, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11406799953446689, "std_of_first_answer": 0.11406799953446689, "avg_entropy_of_first_answer": 0.7210235595703125, "lex_sim": 1.0}
{"question": "Who wrote 'The Owl and the Pussycat'?", "answer": ["Edward Lear #Toronto Argos #Toronto Raptors"], "scores": [[0.96630859375, 0.99853515625, 1.7881393432617188e-07, 0.0020885467529296875, 0.00109100341796875, 0.63818359375, 8.702278137207031e-06, 0.079345703125, 0.09527587890625, 0.07843017578125, 0.82470703125, 0.99609375, 0.09063720703125, 0.970703125, 0.9990234375]], "normalized_score": [0.03399218991398811], "tokens": [["\u2581Edward", "\u2581Lear", "\u2581#", "T", "or", "onto", "\u2581Arg", "os", "\u2581#", "T", "or", "onto", "\u2581R", "apt", "ors"]], "entropy": [[0.286865234375, 0.0175628662109375, 0.70263671875, 5.1796875, 4.19921875, 1.84375, 3.787109375, 1.28125, 4.515625, 5.08203125, 1.3515625, 0.039337158203125, 4.16796875, 0.218505859375, 0.010528564453125]], "avg_entropy": [2.1789093017578125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Edward Lear", "score_of_biggest_cluster": 0.03399218991398811, "score_of_first_answer": 0.03399218991398811, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 1.7881393432617188e-07, "avg_score": 0.03399218991398811, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4435921618872211, "std_of_first_answer": 0.4435921618872211, "avg_entropy_of_first_answer": 2.1789093017578125, "lex_sim": 1.0}
{"question": "Actor Omar Sharif is one of the world's leading players of which game?", "answer": ["Bridge THE HAEREN EFFECT ON THE SUPREME COURT OF THE UNITED STATES:"], "scores": [[0.91259765625, 1.7881393432617188e-07, 0.0016126632690429688, 0.0038127899169921875, 0.00186920166015625, 0.007167816162109375, 0.005008697509765625, 0.046844482421875, 0.94091796875, 0.0009374618530273438, 0.10272216796875, 0.04620361328125, 0.00437164306640625, 0.0740966796875, 0.98583984375, 0.81298828125, 0.904296875, 0.99365234375, 0.258544921875, 0.615234375, 0.87939453125, 0.97900390625, 0.9951171875, 0.9873046875, 0.9931640625, 0.9990234375, 0.103271484375]], "normalized_score": [0.06887736171483994], "tokens": [["\u2581Bridge", "\u2581THE", "\u2581H", "A", "ER", "EN", "\u2581E", "FF", "ECT", "\u2581ON", "\u2581THE", "\u2581S", "UP", "RE", "ME", "\u2581CO", "UR", "T", "\u2581OF", "\u2581THE", "\u2581UN", "IT", "ED", "\u2581ST", "AT", "ES", ":"]], "entropy": [[0.5244140625, 0.6767578125, 2.49609375, 2.646484375, 3.3828125, 3.2109375, 3.7578125, 5.16015625, 0.4150390625, 3.10546875, 4.4453125, 4.47265625, 2.8671875, 1.611328125, 0.1046142578125, 1.3193359375, 0.350830078125, 0.060211181640625, 3.3046875, 1.7490234375, 0.86279296875, 0.170654296875, 0.048065185546875, 0.1171875, 0.055908203125, 0.00853729248046875, 3.701171875]], "avg_entropy": [1.875017801920573], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bridges", "score_of_biggest_cluster": 0.06887736171483994, "score_of_first_answer": 0.06887736171483994, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 1.7881393432617188e-07, "avg_score": 0.06887736171483994, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.44648386601367623, "std_of_first_answer": 0.44648386601367623, "avg_entropy_of_first_answer": 1.875017801920573, "lex_sim": 1.0}
{"question": "'Paralympics' is a combination of Olympics and what other word?", "answer": ["Para #The Good Doctor #Inspiration"], "scores": [[0.1663818359375, 2.384185791015625e-07, 0.000339508056640625, 1.5497207641601562e-06, 0.0016145706176757812, 0.0794677734375, 0.0031948089599609375, 0.08441162109375, 0.10888671875]], "normalized_score": [0.001624144148081541], "tokens": [["\u2581Para", "\u2581#", "The", "\u2581Good", "\u2581Doctor", "\u2581#", "In", "sp", "iration"]], "entropy": [[2.046875, 1.318359375, 5.37109375, 2.16015625, 4.5390625, 3.171875, 4.359375, 4.24609375, 1.2939453125]], "avg_entropy": [3.1674262152777777], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paralel", "score_of_biggest_cluster": 0.001624144148081541, "score_of_first_answer": 0.001624144148081541, "max_score_of_first_answer": 0.1663818359375, "min_score_of_first_answer": 2.384185791015625e-07, "avg_score": 0.001624144148081541, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.058745341480068845, "std_of_first_answer": 0.058745341480068845, "avg_entropy_of_first_answer": 3.1674262152777777, "lex_sim": 1.0}
{"question": "Australian Shane Warne is associated with which sport?", "answer": ["Cricket"], "scores": [[0.96240234375, 0.99365234375]], "normalized_score": [0.9779025316238403], "tokens": [["\u2581Cr", "icket"]], "entropy": [[0.282958984375, 0.0478515625]], "avg_entropy": [0.1654052734375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cricket games", "score_of_biggest_cluster": 0.9779025316238403, "score_of_first_answer": 0.9779025316238403, "max_score_of_first_answer": 0.99365234375, "min_score_of_first_answer": 0.96240234375, "avg_score": 0.9779025316238403, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.015625, "std_of_first_answer": 0.015625, "avg_entropy_of_first_answer": 0.1654052734375, "lex_sim": 1.0}
{"question": "April 18, 1775 saw Paul Revere (along with William Dawes) make his famous midnight ride. What was the stated profession of Mr. Revere?", "answer": ["silversmith"], "scores": [[0.30517578125, 1.0, 0.99951171875, 0.99609375]], "normalized_score": [0.7424368858337402], "tokens": [["\u2581sil", "vers", "m", "ith"]], "entropy": [[3.423828125, 0.0008182525634765625, 0.00421905517578125, 0.0325927734375]], "avg_entropy": [0.8653645515441895], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Silversmithery", "score_of_biggest_cluster": 0.7424368858337402, "score_of_first_answer": 0.7424368858337402, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.30517578125, "avg_score": 0.7424368858337402, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30023718835334956, "std_of_first_answer": 0.30023718835334956, "avg_entropy_of_first_answer": 0.8653645515441895, "lex_sim": 1.0}
{"question": "In literature, whose girlfriend is Becky Thatcher?", "answer": ["Tom Sawyer"], "scores": [[0.98974609375, 0.99365234375, 0.99951171875, 0.9990234375]], "normalized_score": [0.9954752326011658], "tokens": [["\u2581Tom", "\u2581S", "aw", "yer"]], "entropy": [[0.0845947265625, 0.058349609375, 0.003814697265625, 0.00769805908203125]], "avg_entropy": [0.03861427307128906], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Thomas Sawyer", "score_of_biggest_cluster": 0.9954752326011658, "score_of_first_answer": 0.9954752326011658, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.98974609375, "avg_score": 0.9954752326011658, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0040320177160433944, "std_of_first_answer": 0.0040320177160433944, "avg_entropy_of_first_answer": 0.03861427307128906, "lex_sim": 1.0}
{"question": "Which Canadian darts player is nicknamed ' Darth Maple ' ?", "answer": ["Jim Widmayer"], "scores": [[0.00543975830078125, 0.022216796875, 0.9912109375, 0.99658203125, 0.9169921875]], "normalized_score": [0.1613842397928238], "tokens": [["\u2581Jim", "\u2581W", "id", "m", "ayer"]], "entropy": [[0.470703125, 0.54931640625, 0.059814453125, 0.031158447265625, 0.5234375]], "avg_entropy": [0.326885986328125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "JOHN PART", "score_of_biggest_cluster": 0.1613842397928238, "score_of_first_answer": 0.1613842397928238, "max_score_of_first_answer": 0.99658203125, "min_score_of_first_answer": 0.00543975830078125, "avg_score": 0.1613842397928238, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4684506032481638, "std_of_first_answer": 0.4684506032481638, "avg_entropy_of_first_answer": 0.326885986328125, "lex_sim": 1.0}
{"question": "Known as The Mother of Presidents or The Heart of It All, what state was the 17th state to join the union on March 1, 1803?", "answer": ["Pennsylvania"], "scores": [[0.004581451416015625]], "normalized_score": [0.004581451881676912], "tokens": [["\u2581Pennsylvania"]], "entropy": [[1.208984375]], "avg_entropy": [1.208984375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "State of Ohio", "score_of_biggest_cluster": 0.004581451881676912, "score_of_first_answer": 0.004581451881676912, "max_score_of_first_answer": 0.004581451416015625, "min_score_of_first_answer": 0.004581451416015625, "avg_score": 0.004581451881676912, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.208984375, "lex_sim": 1.0}
{"question": "The territory of Idaho, formed on this day in 1863, later was divided up into what 3 states?", "answer": ["Idaho, Montana, and Wyoming"], "scores": [[0.55322265625, 0.99951171875, 0.875, 0.9423828125, 1.0, 0.841796875, 0.53857421875, 0.9990234375, 0.99951171875]], "normalized_score": [0.839189350605011], "tokens": [["\u2581Id", "aho", ",", "\u2581Mont", "ana", ",", "\u2581and", "\u2581Wy", "oming"]], "entropy": [[2.103515625, 0.0051727294921875, 0.64306640625, 0.3232421875, 0.0008325576782226562, 0.466552734375, 0.7890625, 0.00901031494140625, 0.005878448486328125]], "avg_entropy": [0.4829259448581272], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Idaho, Montana and Wyoming", "score_of_biggest_cluster": 0.839189350605011, "score_of_first_answer": 0.839189350605011, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.53857421875, "avg_score": 0.839189350605011, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.17720536160094458, "std_of_first_answer": 0.17720536160094458, "avg_entropy_of_first_answer": 0.4829259448581272, "lex_sim": 1.0}
{"question": "Who coined the phrase 'Somebody up there likes me'?", "answer": ["Rocky Graziano"], "scores": [[0.058929443359375, 0.88232421875, 0.72802734375, 1.0, 1.0]], "normalized_score": [0.5195431709289551], "tokens": [["\u2581Rock", "y", "\u2581Gra", "z", "iano"]], "entropy": [[4.51171875, 0.65869140625, 0.88037109375, 0.002063751220703125, 0.0014801025390625]], "avg_entropy": [1.2108650207519531], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rocky Graziano", "score_of_biggest_cluster": 0.5195431709289551, "score_of_first_answer": 0.5195431709289551, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.058929443359375, "avg_score": 0.5195431709289551, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.35192837326451726, "std_of_first_answer": 0.35192837326451726, "avg_entropy_of_first_answer": 1.2108650207519531, "lex_sim": 1.0}
{"question": "Who was the leader of the Medellin cartel in the 1980's?", "answer": ["Pablo Escobar #10 - Can AI Solve the World\u2019s Problems?"], "scores": [[0.94287109375, 0.7412109375, 0.9970703125, 9.5367431640625e-07, 0.1904296875, 0.053466796875, 0.0011587142944335938, 5.650520324707031e-05, 5.78761100769043e-05, 0.97119140625, 8.910894393920898e-05, 0.99853515625, 0.1986083984375, 0.2135009765625, 0.04388427734375, 0.99951171875, 0.325439453125, 0.998046875, 0.90869140625]], "normalized_score": [0.038319725543260574], "tokens": [["\u2581Pablo", "\u2581Esc", "obar", "\u2581#", "1", "0", "\u2581-", "\u2581Can", "\u2581A", "I", "\u2581Sol", "ve", "\u2581the", "\u2581World", "\u2019", "s", "\u2581Problem", "s", "?"]], "entropy": [[0.51318359375, 0.62939453125, 0.0230560302734375, 0.7109375, 4.51953125, 2.927734375, 3.208984375, 5.04296875, 1.9951171875, 0.2147216796875, 4.47265625, 0.01465606689453125, 3.908203125, 3.828125, 0.4150390625, 0.004810333251953125, 1.8994140625, 0.0171356201171875, 0.63525390625]], "avg_entropy": [1.8411011946828741], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pablo Emilio Escobar Gaviria", "score_of_biggest_cluster": 0.038319725543260574, "score_of_first_answer": 0.038319725543260574, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 9.5367431640625e-07, "avg_score": 0.038319725543260574, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.43211010639732017, "std_of_first_answer": 0.43211010639732017, "avg_entropy_of_first_answer": 1.8411011946828741, "lex_sim": 1.0}
{"question": "How many imperial gallons are in a firkin?", "answer": ["9 gallons Tags: economy, elections, EPS, EU budget, European Parliament, eurozone, Faymann, finances, Fico, French budget, growth, IMF, Ireland, jobs, Labour Day, labour"], "scores": [[0.056427001953125, 0.39794921875, 0.007843017578125, 0.97119140625, 0.0, 0.990234375, 1.9550323486328125e-05, 0.65087890625, 1.7881393432617188e-05, 0.908203125, 0.0009560585021972656, 0.0023670196533203125, 0.58349609375, 0.01275634765625, 0.007595062255859375, 0.91845703125, 0.15478515625, 0.146240234375, 0.919921875, 0.0225677490234375, 0.8203125, 0.8935546875, 0.03204345703125, 0.0005726814270019531, 0.91748046875, 0.9638671875, 0.07598876953125, 0.07427978515625, 0.9853515625, 0.040313720703125, 0.039398193359375, 0.986328125, 0.0166778564453125, 0.050506591796875, 0.962890625, 0.01506805419921875, 0.90966796875, 0.016876220703125, 0.998046875, 0.97314453125, 0.1644287109375, 0.9541015625, 0.037078857421875, 0.98046875, 0.0194854736328125, 0.0112152099609375, 0.98388671875, 0.04278564453125]], "normalized_score": [0.0], "tokens": [["\u2581", "9", "\u2581gall", "ons", "\u2581Tags", ":", "\u2581economy", ",", "\u2581elections", ",", "\u2581E", "PS", ",", "\u2581EU", "\u2581budget", ",", "\u2581European", "\u2581Parliament", ",", "\u2581euro", "zone", ",", "\u2581F", "ay", "mann", ",", "\u2581fin", "ances", ",", "\u2581F", "ico", ",", "\u2581French", "\u2581budget", ",", "\u2581growth", ",", "\u2581IM", "F", ",", "\u2581Ireland", ",", "\u2581jobs", ",", "\u2581Labour", "\u2581Day", ",", "\u2581labour"]], "entropy": [[2.53515625, 1.779296875, 2.345703125, 0.15087890625, 1.1455078125, 0.09063720703125, 5.046875, 1.787109375, 6.2421875, 0.51708984375, 6.17578125, 4.61328125, 1.7705078125, 6.56640625, 1.2236328125, 0.515625, 4.76953125, 2.634765625, 0.4287109375, 4.8828125, 0.87451171875, 0.564453125, 4.3203125, 2.10546875, 0.49072265625, 0.2705078125, 5.15234375, 0.322265625, 0.110107421875, 4.31640625, 3.37109375, 0.1051025390625, 4.71875, 3.275390625, 0.29443359375, 3.341796875, 0.587890625, 4.21875, 0.017913818359375, 0.2318115234375, 3.15625, 0.30859375, 3.501953125, 0.1593017578125, 3.94140625, 1.1044921875, 0.1480712890625, 4.8515625]], "avg_entropy": [2.314232508341471], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "9", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.998046875, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.435014282663086, "std_of_first_answer": 0.435014282663086, "avg_entropy_of_first_answer": 2.314232508341471, "lex_sim": 1.0}
{"question": "\u2018Divine Wind\u2019 is the English translation of which Japanese word?", "answer": ["Kamikaze"], "scores": [[0.90673828125, 1.0, 1.0]], "normalized_score": [0.9678929448127747], "tokens": [["\u2581Kam", "ik", "aze"]], "entropy": [[0.51611328125, 0.0012388229370117188, 0.0010004043579101562]], "avg_entropy": [0.17278416951497397], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tokubetsu K\u014dgekitai", "score_of_biggest_cluster": 0.9678929448127747, "score_of_first_answer": 0.9678929448127747, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.90673828125, "avg_score": 0.9678929448127747, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04396399583549172, "std_of_first_answer": 0.04396399583549172, "avg_entropy_of_first_answer": 0.17278416951497397, "lex_sim": 1.0}
{"question": "Ommatophobia is the irrational fear of which part of the body?", "answer": ["Eyes"], "scores": [[0.6318359375, 0.96240234375]], "normalized_score": [0.7797951102256775], "tokens": [["\u2581E", "yes"]], "entropy": [[1.498046875, 0.210205078125]], "avg_entropy": [0.8541259765625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eye (anatomy)", "score_of_biggest_cluster": 0.7797951102256775, "score_of_first_answer": 0.7797951102256775, "max_score_of_first_answer": 0.96240234375, "min_score_of_first_answer": 0.6318359375, "avg_score": 0.7797951102256775, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.165283203125, "std_of_first_answer": 0.165283203125, "avg_entropy_of_first_answer": 0.8541259765625, "lex_sim": 1.0}
{"question": "\"What hit musical show includes \"\"The Lambeth Walk\"\"?\"", "answer": ["What hit musical show includes \"The Lambeth Walk\"?..."], "scores": [[0.000308990478515625, 0.209716796875, 0.98291015625, 0.9638671875, 0.96728515625, 0.07415771484375, 0.892578125, 0.9951171875, 1.0, 0.99755859375, 0.947265625, 7.832050323486328e-05, 0.01151275634765625, 0.00771331787109375, 0.06610107421875, 0.92626953125, 0.75, 0.42138671875]], "normalized_score": [0.14119474589824677], "tokens": [["\u2581What", "\u2581hit", "\u2581musical", "\u2581show", "\u2581includes", "\u2581\"", "The", "\u2581Lamb", "eth", "\u2581Walk", "\"?", "...", "\u2581[", "p", "aus", "es", "]", "..."]], "entropy": [[2.052734375, 2.943359375, 0.1400146484375, 0.262451171875, 0.246337890625, 0.343505859375, 0.7353515625, 0.056182861328125, 0.0018701553344726562, 0.02496337890625, 0.30419921875, 1.3251953125, 4.453125, 5.09375, 0.55322265625, 0.2802734375, 1.0849609375, 1.779296875]], "avg_entropy": [1.2044885953267415], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Me & My Girl", "score_of_biggest_cluster": 0.14119474589824677, "score_of_first_answer": 0.14119474589824677, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 7.832050323486328e-05, "avg_score": 0.14119474589824677, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4324725193367419, "std_of_first_answer": 0.4324725193367419, "avg_entropy_of_first_answer": 1.2044885953267415, "lex_sim": 1.0}
{"question": "Which US state is known as The First State as it was first to ratify the Constitution in 1787?", "answer": ["Delaware ##### THE ADVENTURE OF THE SPIRAL STAIRCASE #####"], "scores": [[0.9814453125, 0.99853515625, 0.0, 0.0003941059112548828, 0.0001533031463623047, 0.61376953125, 0.9501953125, 0.953125, 0.52734375, 0.265380859375, 0.239013671875, 0.0068206787109375, 0.36962890625, 0.51123046875, 0.64111328125, 1.0, 0.9931640625, 0.998046875, 0.173828125]], "normalized_score": [0.0], "tokens": [["\u2581Del", "aware", "\u2581#####", "\u2581THE", "\u2581AD", "V", "ENT", "URE", "\u2581OF", "\u2581THE", "\u2581S", "PI", "R", "AL", "\u2581S", "TA", "IR", "CASE", "\u2581#####"]], "entropy": [[0.1767578125, 0.01212310791015625, 0.6669921875, 1.677734375, 2.5859375, 1.1982421875, 0.2034912109375, 0.28466796875, 1.2490234375, 1.46875, 2.84375, 1.056640625, 1.4345703125, 0.69873046875, 1.90234375, 0.0006213188171386719, 0.048614501953125, 0.018463134765625, 2.150390625]], "avg_entropy": [1.035676027599134], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Religion in Delaware", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3889942270838078, "std_of_first_answer": 0.3889942270838078, "avg_entropy_of_first_answer": 1.035676027599134, "lex_sim": 1.0}
{"question": "What name connects 'Cold Comfort Farm, stamp collecting and ancient Rome?", "answer": ["St. Crispin's Day"], "scores": [[0.6025390625, 0.478759765625, 0.03521728515625, 0.994140625, 0.382080078125, 0.9990234375, 0.9423828125]], "normalized_score": [0.4481942653656006], "tokens": [["\u2581St", ".", "\u2581Cris", "pin", "'", "s", "\u2581Day"]], "entropy": [[2.025390625, 1.9130859375, 4.0546875, 0.03680419921875, 1.216796875, 0.00733184814453125, 0.3134765625]], "avg_entropy": [1.3667962210518974], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "GIBBONS", "score_of_biggest_cluster": 0.4481942653656006, "score_of_first_answer": 0.4481942653656006, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.03521728515625, "avg_score": 0.4481942653656006, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33914491660258866, "std_of_first_answer": 0.33914491660258866, "avg_entropy_of_first_answer": 1.3667962210518974, "lex_sim": 1.0}
{"question": "Who played their final live gig at Candlestick Park, San Francisco in 1966?", "answer": ["The Beatles"], "scores": [[0.91943359375, 0.974609375, 0.99658203125]], "normalized_score": [0.9629890322685242], "tokens": [["\u2581The", "\u2581Beat", "les"]], "entropy": [[0.74462890625, 0.1842041015625, 0.031951904296875]], "avg_entropy": [0.3202616373697917], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fab 4", "score_of_biggest_cluster": 0.9629890322685242, "score_of_first_answer": 0.9629890322685242, "max_score_of_first_answer": 0.99658203125, "min_score_of_first_answer": 0.91943359375, "avg_score": 0.9629890322685242, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.032453463893054146, "std_of_first_answer": 0.032453463893054146, "avg_entropy_of_first_answer": 0.3202616373697917, "lex_sim": 1.0}
{"question": "Ladon the dragon was killed by Heracles while guarding which of the twelve labours?", "answer": ["The Golden Apples of the Hesperides"], "scores": [[0.185302734375, 0.299072265625, 0.91748046875, 1.0, 0.9736328125, 0.9921875, 0.99609375, 0.9990234375, 1.0]], "normalized_score": [0.7150667905807495], "tokens": [["\u2581The", "\u2581Golden", "\u2581App", "les", "\u2581of", "\u2581the", "\u2581H", "esper", "ides"]], "entropy": [[1.724609375, 1.822265625, 0.41943359375, 0.00020766258239746094, 0.1641845703125, 0.06671142578125, 0.0302734375, 0.00829315185546875, 0.0006837844848632812]], "avg_entropy": [0.47074029180738663], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Apples of the Hesperides", "score_of_biggest_cluster": 0.7150667905807495, "score_of_first_answer": 0.7150667905807495, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.185302734375, "avg_score": 0.7150667905807495, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.30997954115739224, "std_of_first_answer": 0.30997954115739224, "avg_entropy_of_first_answer": 0.47074029180738663, "lex_sim": 1.0}
{"question": "Harold Holt became Prime Minister of which country in January 1966?", "answer": ["AUSTRALIA"], "scores": [[0.0011348724365234375, 0.6767578125, 0.89794921875, 0.990234375, 0.98974609375]], "normalized_score": [0.23226231336593628], "tokens": [["\u2581A", "U", "STR", "AL", "IA"]], "entropy": [[0.5, 1.3994140625, 0.46826171875, 0.07208251953125, 0.0665283203125]], "avg_entropy": [0.50125732421875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Australia (Commonwealth realm)", "score_of_biggest_cluster": 0.23226231336593628, "score_of_first_answer": 0.23226231336593628, "max_score_of_first_answer": 0.990234375, "min_score_of_first_answer": 0.0011348724365234375, "avg_score": 0.23226231336593628, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3730153141172547, "std_of_first_answer": 0.3730153141172547, "avg_entropy_of_first_answer": 0.50125732421875, "lex_sim": 1.0}
{"question": "In the Bible who was David's father?", "answer": ["Jesse"], "scores": [[0.86474609375, 1.0]], "normalized_score": [0.9299172759056091], "tokens": [["\u2581J", "esse"]], "entropy": [[0.63623046875, 0.0009417533874511719]], "avg_entropy": [0.3185861110687256], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jesse (given name)", "score_of_biggest_cluster": 0.9299172759056091, "score_of_first_answer": 0.9299172759056091, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.86474609375, "avg_score": 0.9299172759056091, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.067626953125, "std_of_first_answer": 0.067626953125, "avg_entropy_of_first_answer": 0.3185861110687256, "lex_sim": 1.0}
{"question": "On which Italian track was the Formula One San Marino Grand Prix held?", "answer": ["Autodromo Enzo e Dino Ferrari circuit in Imola #BEYSQUARE #BEYONDCHARLATAN"], "scores": [[0.5048828125, 0.87744140625, 0.9990234375, 0.99951171875, 0.990234375, 1.0, 0.9990234375, 0.99609375, 1.0, 0.99609375, 1.0, 0.00286865234375, 0.222412109375, 0.939453125, 0.9990234375, 1.3113021850585938e-06, 0.002079010009765625, 1.8477439880371094e-06, 0.0017061233520507812, 0.002483367919921875, 0.7744140625, 0.8388671875, 0.004322052001953125, 0.0276336669921875, 0.7177734375, 0.057891845703125, 0.68603515625, 0.0010471343994140625, 0.00022268295288085938, 0.90966796875, 0.9658203125]], "normalized_score": [0.07106034457683563], "tokens": [["\u2581Aut", "od", "rom", "o", "\u2581En", "zo", "\u2581e", "\u2581D", "ino", "\u2581Ferr", "ari", "\u2581circuit", "\u2581in", "\u2581Im", "ola", "\u2581#", "B", "EY", "S", "QU", "A", "RE", "\u2581#", "B", "EY", "ON", "D", "CHAR", "L", "AT", "AN"]], "entropy": [[1.1279296875, 0.478759765625, 0.00850677490234375, 0.0032215118408203125, 0.08465576171875, 0.0003857612609863281, 0.007274627685546875, 0.031982421875, 0.00054168701171875, 0.031402587890625, 1.6629695892333984e-05, 1.2998046875, 1.86328125, 0.402099609375, 0.0090179443359375, 1.291015625, 4.125, 3.744140625, 1.1689453125, 3.978515625, 1.3017578125, 0.70654296875, 1.568359375, 5.3203125, 1.328125, 3.271484375, 1.2939453125, 3.80859375, 1.0068359375, 0.382568359375, 0.2139892578125]], "avg_entropy": [1.2857745981985522], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Imolensis", "score_of_biggest_cluster": 0.07106034457683563, "score_of_first_answer": 0.07106034457683563, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.3113021850585938e-06, "avg_score": 0.07106034457683563, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44255736492329034, "std_of_first_answer": 0.44255736492329034, "avg_entropy_of_first_answer": 1.2857745981985522, "lex_sim": 1.0}
{"question": "How many squares are found along one side of a Scrabble board?", "answer": ["108 #FridayReads: The Lonely City by Olivia Laing"], "scores": [[0.119873046875, 0.869140625, 0.02252197265625, 0.1058349609375, 4.172325134277344e-06, 0.0012292861938476562, 0.00273895263671875, 0.99755859375, 0.000270843505859375, 0.5927734375, 0.01033782958984375, 0.0938720703125, 0.002910614013671875, 0.27490234375, 0.90576171875, 0.57177734375, 0.200439453125, 0.99951171875, 0.998046875, 0.98583984375, 1.0]], "normalized_score": [0.062041349709033966], "tokens": [["\u2581", "1", "0", "8", "\u2581#", "F", "rid", "ay", "Read", "s", ":", "\u2581The", "\u2581L", "on", "ely", "\u2581City", "\u2581by", "\u2581Oliv", "ia", "\u2581La", "ing"]], "entropy": [[2.38671875, 0.60400390625, 0.630859375, 2.197265625, 0.9267578125, 5.56640625, 3.984375, 0.0260467529296875, 0.98193359375, 1.0087890625, 1.5546875, 4.19921875, 5.88671875, 3.048828125, 0.400146484375, 2.19140625, 0.9658203125, 0.003635406494140625, 0.0146026611328125, 0.08087158203125, 0.00232696533203125]], "avg_entropy": [1.7457818530854725], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "fifteen", "score_of_biggest_cluster": 0.062041349709033966, "score_of_first_answer": 0.062041349709033966, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 4.172325134277344e-06, "avg_score": 0.062041349709033966, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42069760824749697, "std_of_first_answer": 0.42069760824749697, "avg_entropy_of_first_answer": 1.7457818530854725, "lex_sim": 1.0}
{"question": "Which of the King George VI\u2019s brothers died in 1942?", "answer": ["King George V ##Budget"], "scores": [[0.1302490234375, 0.57958984375, 0.91259765625, 0.0, 0.0011892318725585938, 0.00019943714141845703, 0.243408203125]], "normalized_score": [0.0], "tokens": [["\u2581King", "\u2581George", "\u2581V", "\u2581##", "B", "ud", "get"]], "entropy": [[1.9599609375, 0.7578125, 0.387451171875, 1.0283203125, 5.37109375, 3.43359375, 3.982421875]], "avg_entropy": [2.417236328125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dukes of Kent", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.91259765625, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3266480424804763, "std_of_first_answer": 0.3266480424804763, "avg_entropy_of_first_answer": 2.417236328125, "lex_sim": 1.0}
{"question": "Which Tim Burton film is the highest-grossing film of 2010 and, already, the fifth highest-grossing film of all time worldwide?", "answer": ["Alice in Wonderland (2010 film) #559: The Day The Music Died"], "scores": [[0.73681640625, 0.9755859375, 0.919921875, 1.0, 0.322265625, 0.96533203125, 0.99951171875, 0.99951171875, 1.0, 0.69189453125, 0.99072265625, 5.346536636352539e-05, 0.0134429931640625, 0.0172576904296875, 0.03814697265625, 0.0004551410675048828, 0.025848388671875, 0.0001119375228881836, 0.0265655517578125, 0.01029205322265625, 0.99267578125, 1.0]], "normalized_score": [0.09368783980607986], "tokens": [["\u2581Alice", "\u2581in", "\u2581Wonder", "land", "\u2581(", "2", "0", "1", "0", "\u2581film", ")", "\u2581#", "5", "5", "9", ":", "\u2581The", "\u2581Day", "\u2581The", "\u2581Music", "\u2581D", "ied"]], "entropy": [[1.419921875, 0.1485595703125, 0.55078125, 0.0019283294677734375, 1.2392578125, 0.2098388671875, 0.0029296875, 0.0027923583984375, 0.0008563995361328125, 0.74169921875, 0.073974609375, 0.7080078125, 4.84765625, 3.1015625, 3.443359375, 2.70703125, 4.87109375, 3.515625, 2.53125, 1.126953125, 0.07196044921875, 0.00027942657470703125]], "avg_entropy": [1.4235144961964001], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alice's Adventures In Wonderland", "score_of_biggest_cluster": 0.09368783980607986, "score_of_first_answer": 0.09368783980607986, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.346536636352539e-05, "avg_score": 0.09368783980607986, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45593867630359214, "std_of_first_answer": 0.45593867630359214, "avg_entropy_of_first_answer": 1.4235144961964001, "lex_sim": 1.0}
{"question": "Who played the female lead in the 1957 film 'An Affair To Remember'?", "answer": ["Navya Naveli"], "scores": [[0.004947662353515625, 0.185546875, 0.61572265625, 0.99072265625]], "normalized_score": [0.1538323611021042], "tokens": [["\u2581Nav", "ya", "\u2581Nav", "eli"]], "entropy": [[2.9140625, 4.609375, 1.7529296875, 0.0714111328125]], "avg_entropy": [2.336944580078125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Deborah Kerr-Trimmer", "score_of_biggest_cluster": 0.1538323611021042, "score_of_first_answer": 0.1538323611021042, "max_score_of_first_answer": 0.99072265625, "min_score_of_first_answer": 0.004947662353515625, "avg_score": 0.1538323611021042, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38335678134531137, "std_of_first_answer": 0.38335678134531137, "avg_entropy_of_first_answer": 2.336944580078125, "lex_sim": 1.0}
{"question": "What is the distinctive colour of the soles of shoes designed by Christian Louboutin?", "answer": ["Red Soles"], "scores": [[0.53857421875, 0.00469970703125, 0.96240234375]], "normalized_score": [0.13455228507518768], "tokens": [["\u2581Red", "\u2581Sol", "es"]], "entropy": [[1.9384765625, 1.2333984375, 0.23095703125]], "avg_entropy": [1.13427734375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Red (colour)", "score_of_biggest_cluster": 0.13455228507518768, "score_of_first_answer": 0.13455228507518768, "max_score_of_first_answer": 0.96240234375, "min_score_of_first_answer": 0.00469970703125, "avg_score": 0.13455228507518768, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3918399069605958, "std_of_first_answer": 0.3918399069605958, "avg_entropy_of_first_answer": 1.13427734375, "lex_sim": 1.0}
{"question": "According to the proverb, what is the better part of valor?", "answer": ["Discretion"], "scores": [[0.822265625, 0.99951171875, 0.9990234375]], "normalized_score": [0.9363938570022583], "tokens": [["\u2581Dis", "cret", "ion"]], "entropy": [[0.6591796875, 0.00495147705078125, 0.009063720703125]], "avg_entropy": [0.2243982950846354], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Discretion", "score_of_biggest_cluster": 0.9363938570022583, "score_of_first_answer": 0.9363938570022583, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.822265625, "avg_score": 0.9363938570022583, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08343975900678821, "std_of_first_answer": 0.08343975900678821, "avg_entropy_of_first_answer": 0.2243982950846354, "lex_sim": 1.0}
{"question": "The major UK/European mobile network corp EE was originally branded?", "answer": ["Orange Personal Communications Services Limited"], "scores": [[0.392578125, 0.032745361328125, 0.41796875, 0.99853515625, 0.9990234375, 0.3857421875]], "normalized_score": [0.3569242060184479], "tokens": [["\u2581Orange", "\u2581Personal", "\u2581Communic", "ations", "\u2581Services", "\u2581Limited"]], "entropy": [[0.95849609375, 1.572265625, 1.966796875, 0.01134490966796875, 0.00775146484375, 1.4580078125]], "avg_entropy": [0.9957771301269531], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Everything Everywhere", "score_of_biggest_cluster": 0.3569242060184479, "score_of_first_answer": 0.3569242060184479, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.032745361328125, "avg_score": 0.3569242060184479, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3508690696459015, "std_of_first_answer": 0.3508690696459015, "avg_entropy_of_first_answer": 0.9957771301269531, "lex_sim": 1.0}
{"question": "Who played the part of 'Fitz' in the TV series 'Cracker'?", "answer": ["Robin McMorris ### Overview"], "scores": [[0.0010242462158203125, 0.54931640625, 0.01873779296875, 0.69775390625, 0.0, 0.0001552104949951172, 0.032684326171875]], "normalized_score": [0.0], "tokens": [["\u2581Robin", "\u2581Mc", "Mor", "ris", "\u2581###", "\u2581Over", "view"]], "entropy": [[1.02734375, 2.349609375, 2.9296875, 1.3662109375, 1.1689453125, 2.66796875, 3.015625]], "avg_entropy": [2.0750558035714284], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Anthony R. McMillan", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.69775390625, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2799802937781058, "std_of_first_answer": 0.2799802937781058, "avg_entropy_of_first_answer": 2.0750558035714284, "lex_sim": 1.0}
{"question": "What was the name of the container ship that foundered off the SW coast of England in Early 2007?", "answer": ["The Queen Mary II"], "scores": [[0.1102294921875, 0.004032135009765625, 0.67578125, 0.039215087890625]], "normalized_score": [0.05858319252729416], "tokens": [["\u2581The", "\u2581Queen", "\u2581Mary", "\u2581II"]], "entropy": [[2.759765625, 3.025390625, 1.6865234375, 0.40478515625]], "avg_entropy": [1.9691162109375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Neapel", "score_of_biggest_cluster": 0.05858319252729416, "score_of_first_answer": 0.05858319252729416, "max_score_of_first_answer": 0.67578125, "min_score_of_first_answer": 0.004032135009765625, "avg_score": 0.05858319252729416, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27316099962524654, "std_of_first_answer": 0.27316099962524654, "avg_entropy_of_first_answer": 1.9691162109375, "lex_sim": 1.0}
{"question": "\"Which company uses the advertising slogan \"\"Spend A Little, Live A Lot\"\"?\"", "answer": ["MoneySuperMarket"], "scores": [[0.00821685791015625, 0.58837890625, 0.74755859375, 0.8935546875, 0.99951171875]], "normalized_score": [0.31752902269363403], "tokens": [["\u2581M", "oney", "Super", "Mark", "et"]], "entropy": [[3.912109375, 1.0400390625, 0.99853515625, 0.364501953125, 0.0034275054931640625]], "avg_entropy": [1.2637226104736328], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Aldi S\u00fcd", "score_of_biggest_cluster": 0.31752902269363403, "score_of_first_answer": 0.31752902269363403, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.00821685791015625, "avg_score": 0.31752902269363403, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3483231561217853, "std_of_first_answer": 0.3483231561217853, "avg_entropy_of_first_answer": 1.2637226104736328, "lex_sim": 1.0}
{"question": "In which country would you find the temple of Angkor Wat?", "answer": ["Cambodia"], "scores": [[0.9580078125, 0.9990234375]], "normalized_score": [0.9783006906509399], "tokens": [["\u2581Camb", "odia"]], "entropy": [[0.32470703125, 0.00952911376953125]], "avg_entropy": [0.16711807250976562], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Srok khmer", "score_of_biggest_cluster": 0.9783006906509399, "score_of_first_answer": 0.9783006906509399, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.9580078125, "avg_score": 0.9783006906509399, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0205078125, "std_of_first_answer": 0.0205078125, "avg_entropy_of_first_answer": 0.16711807250976562, "lex_sim": 1.0}
{"question": "Which British politician became Member of Parliament for Epping in October 1924?", "answer": ["Stanley Baldwin ## Background"], "scores": [[0.00811004638671875, 0.99755859375, 1.0, 0.0, 1.2516975402832031e-06]], "normalized_score": [0.0], "tokens": [["\u2581Stanley", "\u2581Bald", "win", "\u2581##", "\u2581Background"]], "entropy": [[0.9736328125, 0.031341552734375, 0.00067138671875, 0.70361328125, 5.96875]], "avg_entropy": [1.535601806640625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tango (cat)", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4879849542386219, "std_of_first_answer": 0.4879849542386219, "avg_entropy_of_first_answer": 1.535601806640625, "lex_sim": 1.0}
{"question": "\"Who according to the Book of Genesis was described as \"\"A mighty hunter before the Lord\"\"?\"", "answer": ["Nimrod"], "scores": [[0.966796875, 0.9990234375, 0.9970703125]], "normalized_score": [0.9875192642211914], "tokens": [["\u2581N", "im", "rod"]], "entropy": [[0.238525390625, 0.00786590576171875, 0.026214599609375]], "avg_entropy": [0.09086863199869792], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nimrod (computing)", "score_of_biggest_cluster": 0.9875192642211914, "score_of_first_answer": 0.9875192642211914, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.966796875, "avg_score": 0.9875192642211914, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.014752954679198777, "std_of_first_answer": 0.014752954679198777, "avg_entropy_of_first_answer": 0.09086863199869792, "lex_sim": 1.0}
{"question": "What was the name given to Barbary pirates who raided ships in the Mediterranean and the south European coast from bases on the north coast of Africa?", "answer": ["Ottoman privateers ###Overview"], "scores": [[0.451904296875, 0.78515625, 0.0213775634765625, 0.98876953125, 0.0, 0.00015461444854736328, 0.073486328125]], "normalized_score": [0.0], "tokens": [["\u2581Ott", "oman", "\u2581private", "ers", "\u2581###", "Over", "view"]], "entropy": [[2.37109375, 0.59619140625, 1.857421875, 0.06732177734375, 0.9267578125, 3.181640625, 3.3203125]], "avg_entropy": [1.7601056780133928], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Corsairs", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.98876953125, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.38447244044958373, "std_of_first_answer": 0.38447244044958373, "avg_entropy_of_first_answer": 1.7601056780133928, "lex_sim": 1.0}
{"question": "What color golf balls are used on the golf course in Antarctica?", "answer": ["White golf balls #Sustainability: The future of fashion, today"], "scores": [[0.25732421875, 0.02783203125, 0.98828125, 8.940696716308594e-07, 0.00247955322265625, 0.00010699033737182617, 0.98486328125, 0.87060546875, 0.0019817352294921875, 0.056427001953125, 3.701448440551758e-05, 0.4541015625, 0.00769805908203125, 0.0254058837890625, 0.00015223026275634766]], "normalized_score": [0.008454172872006893], "tokens": [["\u2581White", "\u2581golf", "\u2581balls", "\u2581#", "S", "ust", "ain", "ability", ":", "\u2581The", "\u2581future", "\u2581of", "\u2581fashion", ",", "\u2581today"]], "entropy": [[2.556640625, 1.2119140625, 0.082763671875, 1.3134765625, 4.62109375, 1.818359375, 0.1510009765625, 0.60693359375, 2.234375, 4.4609375, 2.845703125, 1.1787109375, 2.509765625, 1.28125, 3.51171875]], "avg_entropy": [2.025642903645833], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Orange (album)", "score_of_biggest_cluster": 0.008454172872006893, "score_of_first_answer": 0.008454172872006893, "max_score_of_first_answer": 0.98828125, "min_score_of_first_answer": 8.940696716308594e-07, "avg_score": 0.008454172872006893, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.372398162203227, "std_of_first_answer": 0.372398162203227, "avg_entropy_of_first_answer": 2.025642903645833, "lex_sim": 1.0}
{"question": "Who was the renowned English botanist who accompanied Capt. James Cook on his first voyage in 1768 and is credited with introducing Eucalyptus, Mimosa and Acacia to Europe?", "answer": ["Sir Joseph Banks ###What is your favorite song by The Beatles?"], "scores": [[0.340576171875, 1.0, 0.998046875, 1.0, 0.0, 8.821487426757812e-06, 0.235595703125, 0.02716064453125, 0.0209503173828125, 0.0017986297607421875, 0.0027446746826171875, 0.2421875, 0.5009765625, 0.9990234375, 0.8505859375]], "normalized_score": [0.0], "tokens": [["\u2581Sir", "\u2581Joseph", "\u2581B", "anks", "\u2581###", "What", "\u2581is", "\u2581your", "\u2581favorite", "\u2581song", "\u2581by", "\u2581The", "\u2581Beat", "les", "?"]], "entropy": [[0.79638671875, 0.0008273124694824219, 0.0191497802734375, 0.00010323524475097656, 0.642578125, 3.81640625, 3.666015625, 2.34375, 3.091796875, 2.447265625, 1.3388671875, 3.978515625, 2.203125, 0.0101776123046875, 0.85400390625]], "avg_entropy": [1.6805979251861571], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sir Joseph Banks, 1st Baronet", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.418012615264896, "std_of_first_answer": 0.418012615264896, "avg_entropy_of_first_answer": 1.6805979251861571, "lex_sim": 1.0}
{"question": "What is the name of Bob Marley's greatest hits album?", "answer": ["Legend (Bob Marley & The Wailers album)"], "scores": [[0.9755859375, 0.98828125, 0.00574493408203125, 0.206787109375, 0.98974609375, 0.99951171875, 0.92822265625, 0.98876953125, 0.9990234375, 1.0, 0.9990234375, 0.74560546875, 0.99609375]], "normalized_score": [0.5761511921882629], "tokens": [["\u2581Leg", "end", "\u2581(", "Bob", "\u2581Mar", "ley", "\u2581&", "\u2581The", "\u2581W", "ail", "ers", "\u2581album", ")"]], "entropy": [[0.14697265625, 0.06707763671875, 0.73291015625, 1.5927734375, 0.07550048828125, 0.0027446746826171875, 0.406982421875, 0.0699462890625, 0.00951385498046875, 0.0004949569702148438, 0.0067138671875, 0.89794921875, 0.03363037109375]], "avg_entropy": [0.3110161561232347], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Legend (person)", "score_of_biggest_cluster": 0.5761511921882629, "score_of_first_answer": 0.5761511921882629, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.00574493408203125, "avg_score": 0.5761511921882629, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3191110392257096, "std_of_first_answer": 0.3191110392257096, "avg_entropy_of_first_answer": 0.3110161561232347, "lex_sim": 1.0}
{"question": "The 2014 'Bendgate' controversy/publicity concerned version 6 of which famous product?", "answer": ["Apple iPhone 6 Plus"], "scores": [[0.0748291015625, 0.83935546875, 0.96044921875, 1.0, 0.705078125]], "normalized_score": [0.5317966938018799], "tokens": [["\u2581Apple", "\u2581iPhone", "\u2581", "6", "\u2581Plus"]], "entropy": [[0.67529296875, 0.69384765625, 0.218994140625, 0.002197265625, 1.01171875]], "avg_entropy": [0.52041015625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bendgate", "score_of_biggest_cluster": 0.5317966938018799, "score_of_first_answer": 0.5317966938018799, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0748291015625, "avg_score": 0.5317966938018799, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33670384137279535, "std_of_first_answer": 0.33670384137279535, "avg_entropy_of_first_answer": 0.52041015625, "lex_sim": 1.0}
{"question": "Which soul singer /songwriter wrote the theme music for the 1971 film Shaft?", "answer": ["Isaac Hayes #1: The 5 Love Languages With Dr. Gary Chapman and Dr. Jennifer Thomas by The Love Language Couple"], "scores": [[0.9326171875, 0.99658203125, 1.0, 2.384185791015625e-07, 0.09857177734375, 0.006168365478515625, 0.045806884765625, 0.00115966796875, 0.026763916015625, 0.00011402368545532227, 0.994140625, 1.0, 2.6404857635498047e-05, 0.9638671875, 0.994140625, 0.86767578125, 0.9990234375, 0.9990234375, 0.01428985595703125, 0.07720947265625, 0.99609375, 0.93212890625, 0.9912109375, 0.127197265625, 0.00458526611328125, 0.0836181640625, 0.6884765625, 0.06817626953125, 0.02532958984375, 0.5888671875]], "normalized_score": [0.06927525997161865], "tokens": [["\u2581Isaac", "\u2581Hay", "es", "\u2581#", "1", ":", "\u2581The", "\u2581", "5", "\u2581Love", "\u2581L", "anguages", "\u2581With", "\u2581Dr", ".", "\u2581Gary", "\u2581Chap", "man", "\u2581and", "\u2581Dr", ".", "\u2581Jenn", "ifer", "\u2581Thomas", "\u2581by", "\u2581The", "\u2581Love", "\u2581Language", "\u2581Cou", "ple"]], "entropy": [[0.449462890625, 0.0309600830078125, 0.0026493072509765625, 0.64501953125, 4.76953125, 2.734375, 2.3828125, 0.344482421875, 1.6171875, 1.7529296875, 0.04998779296875, 0.0005140304565429688, 2.580078125, 0.306640625, 0.053741455078125, 0.70458984375, 0.01168060302734375, 0.0072174072265625, 2.974609375, 4.77734375, 0.0386962890625, 0.5283203125, 0.08209228515625, 2.724609375, 2.736328125, 3.27734375, 2.259765625, 2.267578125, 2.359375, 0.86767578125]], "avg_entropy": [1.4445865948994954], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Issac Hayes", "score_of_biggest_cluster": 0.06927525997161865, "score_of_first_answer": 0.06927525997161865, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.384185791015625e-07, "avg_score": 0.06927525997161865, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4545574238068784, "std_of_first_answer": 0.4545574238068784, "avg_entropy_of_first_answer": 1.4445865948994954, "lex_sim": 1.0}
{"question": "Where is the Arafura Sea?", "answer": ["between Australia and New Guinea"], "scores": [[0.24365234375, 0.260009765625, 0.9453125, 0.54052734375, 0.99609375]], "normalized_score": [0.5031421780586243], "tokens": [["\u2581between", "\u2581Australia", "\u2581and", "\u2581New", "\u2581Guinea"]], "entropy": [[2.884765625, 1.59765625, 0.2763671875, 1.345703125, 0.0283355712890625]], "avg_entropy": [1.2265655517578125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Between Australia and New Guinea", "score_of_biggest_cluster": 0.5031421780586243, "score_of_first_answer": 0.5031421780586243, "max_score_of_first_answer": 0.99609375, "min_score_of_first_answer": 0.24365234375, "avg_score": 0.5031421780586243, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.32317286183040994, "std_of_first_answer": 0.32317286183040994, "avg_entropy_of_first_answer": 1.2265655517578125, "lex_sim": 1.0}
{"question": "Also called The Roodee, which, officially, is Britain\u2019s oldest and shortest racecourse?", "answer": ["Chester Racecourse #78: C-I-T-Y - City"], "scores": [[0.95947265625, 0.99951171875, 0.7705078125, 0.99609375, 1.1324882507324219e-06, 0.0028629302978515625, 0.004367828369140625, 0.0005354881286621094, 0.0009698867797851562, 0.006504058837890625, 0.001972198486328125, 0.3857421875, 0.78515625, 0.98583984375, 0.97802734375, 0.00363922119140625, 0.01311492919921875]], "normalized_score": [0.025759020820260048], "tokens": [["\u2581Ch", "ester", "\u2581Race", "course", "\u2581#", "7", "8", ":", "\u2581C", "-", "I", "-", "T", "-", "Y", "\u2581-", "\u2581City"]], "entropy": [[0.2337646484375, 0.006046295166015625, 0.9169921875, 0.02734375, 1.2080078125, 5.17578125, 1.1669921875, 3.103515625, 4.484375, 4.9296875, 4.328125, 3.419921875, 1.185546875, 0.11761474609375, 0.1590576171875, 2.39453125, 5.1640625]], "avg_entropy": [2.236550948199104], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "County Borough of Chester", "score_of_biggest_cluster": 0.025759020820260048, "score_of_first_answer": 0.025759020820260048, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 1.1324882507324219e-06, "avg_score": 0.025759020820260048, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44744127488296775, "std_of_first_answer": 0.44744127488296775, "avg_entropy_of_first_answer": 2.236550948199104, "lex_sim": 1.0}
{"question": "How many chambers has the heart?", "answer": ["Four chambers: left and right atria, and left and right ventricles"], "scores": [[0.587890625, 0.224365234375, 1.0, 1.0, 0.05975341796875, 0.47216796875, 0.455810546875, 0.99951171875, 0.9990234375, 0.998046875, 0.266845703125, 0.9892578125, 0.99658203125, 1.0, 0.9990234375, 1.0, 0.99951171875, 0.9990234375]], "normalized_score": [0.6512144207954407], "tokens": [["\u2581Four", "\u2581ch", "amb", "ers", ":", "\u2581left", "\u2581and", "\u2581right", "\u2581at", "ria", ",", "\u2581and", "\u2581left", "\u2581and", "\u2581right", "\u2581vent", "ric", "les"]], "entropy": [[1.1533203125, 1.404296875, 0.0010852813720703125, 0.0016527175903320312, 1.2763671875, 1.296875, 0.71240234375, 0.00511932373046875, 0.0084075927734375, 0.01491546630859375, 0.71875, 0.06817626953125, 0.03009033203125, 0.0013647079467773438, 0.00665283203125, 0.0025272369384765625, 0.002777099609375, 0.01012420654296875]], "avg_entropy": [0.3730502658420139], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Four", "score_of_biggest_cluster": 0.6512144207954407, "score_of_first_answer": 0.6512144207954407, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.05975341796875, "avg_score": 0.6512144207954407, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3248675707545304, "std_of_first_answer": 0.3248675707545304, "avg_entropy_of_first_answer": 0.3730502658420139, "lex_sim": 1.0}
{"question": "Meryl Streep has won 2 Oscars for Best Actress but for which 1979 film did she win Best Supporting Actress?", "answer": ["Kramer vs. Kramer #InTheLabTV #BarackObama"], "scores": [[0.5576171875, 0.9931640625, 1.0, 0.91064453125, 0.693359375, 0.9970703125, 1.0, 1.0, 4.172325134277344e-07, 0.00028395652770996094, 0.0626220703125, 8.153915405273438e-05, 0.0005784034729003906, 0.2081298828125, 0.0002148151397705078, 0.0010862350463867188, 0.908203125, 0.99951171875]], "normalized_score": [0.034720905125141144], "tokens": [["\u2581K", "ram", "er", "\u2581vs", ".", "\u2581K", "ram", "er", "\u2581#", "In", "The", "Lab", "TV", "\u2581#", "Bar", "ack", "Ob", "ama"]], "entropy": [[1.970703125, 0.043670654296875, 0.0004954338073730469, 0.3857421875, 0.646484375, 0.0268402099609375, 0.00036263465881347656, 0.00025534629821777344, 1.0517578125, 3.689453125, 4.77734375, 4.20703125, 2.8671875, 2.4375, 5.0546875, 3.658203125, 0.6015625, 0.004978179931640625]], "avg_entropy": [1.7457921504974365], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kramer v kramer", "score_of_biggest_cluster": 0.034720905125141144, "score_of_first_answer": 0.034720905125141144, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 4.172325134277344e-07, "avg_score": 0.034720905125141144, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4493122734306335, "std_of_first_answer": 0.4493122734306335, "avg_entropy_of_first_answer": 1.7457921504974365, "lex_sim": 1.0}
{"question": "Which city is the capital of the Spanish region of Aragon?", "answer": ["Zaragoza #6 in Best Things To Do in Houston"], "scores": [[0.60595703125, 0.99560546875, 1.0, 4.172325134277344e-07, 0.0234527587890625, 0.0002493858337402344, 0.00010466575622558594, 0.1229248046875, 0.6953125, 0.99658203125, 0.99755859375, 1.7881393432617188e-07]], "normalized_score": [0.010734640061855316], "tokens": [["\u2581Zar", "ago", "za", "\u2581#", "6", "\u2581in", "\u2581Best", "\u2581Things", "\u2581To", "\u2581Do", "\u2581in", "\u2581Houston"]], "entropy": [[1.9765625, 0.0307159423828125, 0.0025177001953125, 0.9873046875, 4.93359375, 3.630859375, 2.9140625, 2.703125, 0.818359375, 0.033294677734375, 0.02301025390625, 0.572265625]], "avg_entropy": [1.5521392822265625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Zaragoza", "score_of_biggest_cluster": 0.010734640061855316, "score_of_first_answer": 0.010734640061855316, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.7881393432617188e-07, "avg_score": 0.010734640061855316, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44550919905505104, "std_of_first_answer": 0.44550919905505104, "avg_entropy_of_first_answer": 1.5521392822265625, "lex_sim": 1.0}
{"question": "What is the first name of Ms. Miranda, Electra, and Sandiego?", "answer": ["Orion"], "scores": [[0.061614990234375, 0.111083984375]], "normalized_score": [0.08273112773895264], "tokens": [["\u2581Or", "ion"]], "entropy": [[4.8046875, 1.986328125]], "avg_entropy": [3.3955078125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Les tringles des sistres tintaient", "score_of_biggest_cluster": 0.08273112773895264, "score_of_first_answer": 0.08273112773895264, "max_score_of_first_answer": 0.111083984375, "min_score_of_first_answer": 0.061614990234375, "avg_score": 0.08273112773895264, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0247344970703125, "std_of_first_answer": 0.0247344970703125, "avg_entropy_of_first_answer": 3.3955078125, "lex_sim": 1.0}
{"question": "Which Italian pasta's name means little spindles in English?", "answer": ["penne"], "scores": [[0.0106658935546875, 0.9892578125]], "normalized_score": [0.10271960496902466], "tokens": [["\u2581p", "enne"]], "entropy": [[2.703125, 0.07476806640625]], "avg_entropy": [1.388946533203125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fusili", "score_of_biggest_cluster": 0.10271960496902466, "score_of_first_answer": 0.10271960496902466, "max_score_of_first_answer": 0.9892578125, "min_score_of_first_answer": 0.0106658935546875, "avg_score": 0.10271960496902466, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.48929595947265625, "std_of_first_answer": 0.48929595947265625, "avg_entropy_of_first_answer": 1.388946533203125, "lex_sim": 1.0}
{"question": "Who was the Georgia Peach?", "answer": ["Ty Cobb"], "scores": [[0.974609375, 0.99609375, 0.99951171875]], "normalized_score": [0.9900099635124207], "tokens": [["\u2581Ty", "\u2581C", "obb"]], "entropy": [[0.21484375, 0.036651611328125, 0.006504058837890625]], "avg_entropy": [0.0859998067220052], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cobb, Ty", "score_of_biggest_cluster": 0.9900099635124207, "score_of_first_answer": 0.9900099635124207, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.974609375, "avg_score": 0.9900099635124207, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.01102213733941212, "std_of_first_answer": 0.01102213733941212, "avg_entropy_of_first_answer": 0.0859998067220052, "lex_sim": 1.0}
{"question": "Which branch of science has four fundamental principles: addition, subtraction, multiplication, and division.", "answer": ["Mathematics Chapter VIII."], "scores": [[0.5771484375, 0.99365234375, 0.0, 0.0016813278198242188, 0.0955810546875, 0.077880859375, 0.0001823902130126953, 0.9267578125, 0.0150604248046875]], "normalized_score": [0.0], "tokens": [["\u2581Mathemat", "ics", "\u2581Chapter", "\u2581VIII", ".", "\u2581The", "\u2581Art", "\u2581of", "\u2581War"]], "entropy": [[1.927734375, 0.04736328125, 0.96728515625, 3.205078125, 3.58984375, 4.8125, 4.8828125, 0.37451171875, 2.5]], "avg_entropy": [2.4785698784722223], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "+-*/", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.99365234375, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3932730528167926, "std_of_first_answer": 0.3932730528167926, "avg_entropy_of_first_answer": 2.4785698784722223, "lex_sim": 1.0}
{"question": "Which island is named after the world's largest bear?", "answer": ["Kodiak"], "scores": [[0.80029296875, 0.66796875, 0.994140625]], "normalized_score": [0.8099986910820007], "tokens": [["\u2581K", "od", "iak"]], "entropy": [[1.1845703125, 0.7275390625, 0.0467529296875]], "avg_entropy": [0.6529541015625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kodiak (disambiguation)", "score_of_biggest_cluster": 0.8099986910820007, "score_of_first_answer": 0.8099986910820007, "max_score_of_first_answer": 0.994140625, "min_score_of_first_answer": 0.66796875, "avg_score": 0.8099986910820007, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1339463842747171, "std_of_first_answer": 0.1339463842747171, "avg_entropy_of_first_answer": 0.6529541015625, "lex_sim": 1.0}
{"question": "Which pop group in October 1974 reached NumberOne in the UK charts with Sad, Sweet Dreamer?", "answer": ["Three Dog Night"], "scores": [[0.004222869873046875, 0.89111328125, 0.9931640625]], "normalized_score": [0.1551864892244339], "tokens": [["\u2581Three", "\u2581Dog", "\u2581Night"]], "entropy": [[3.20703125, 0.72216796875, 0.054779052734375]], "avg_entropy": [1.3279927571614583], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "SWEET SENSATION", "score_of_biggest_cluster": 0.1551864892244339, "score_of_first_answer": 0.1551864892244339, "max_score_of_first_answer": 0.9931640625, "min_score_of_first_answer": 0.004222869873046875, "avg_score": 0.1551864892244339, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4440962916381157, "std_of_first_answer": 0.4440962916381157, "avg_entropy_of_first_answer": 1.3279927571614583, "lex_sim": 1.0}
{"question": "Bandar Seri Begawan International airport is in which country?", "answer": ["BND"], "scores": [[0.0011873245239257812, 0.026885986328125]], "normalized_score": [0.005649989005178213], "tokens": [["\u2581B", "ND"]], "entropy": [[0.1575927734375, 4.71484375]], "avg_entropy": [2.43621826171875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Abode of Peace", "score_of_biggest_cluster": 0.005649989005178213, "score_of_first_answer": 0.005649989005178213, "max_score_of_first_answer": 0.026885986328125, "min_score_of_first_answer": 0.0011873245239257812, "avg_score": 0.005649989005178213, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01284933090209961, "std_of_first_answer": 0.01284933090209961, "avg_entropy_of_first_answer": 2.43621826171875, "lex_sim": 1.0}
{"question": "In which city was Michael Jackson born?", "answer": ["Gary, Indiana #535: \u201cIt seems to me that part of the function of a Christian is to bring order out of chaos."], "scores": [[0.986328125, 0.94482421875, 0.9755859375, 1.2516975402832031e-06, 0.0120697021484375, 0.0885009765625, 0.037353515625, 0.032257080078125, 0.0010747909545898438, 0.006237030029296875, 0.006298065185546875, 0.1356201171875, 0.65576171875, 0.82763671875, 2.866983413696289e-05, 0.990234375, 0.72998046875, 0.059906005859375, 0.99755859375, 0.10205078125, 0.0004127025604248047, 0.1995849609375, 0.91259765625, 0.52490234375, 0.007526397705078125, 0.87353515625, 0.99755859375, 0.94775390625, 0.99951171875, 0.11883544921875, 0.08148193359375, 0.054351806640625, 0.4501953125, 0.1767578125, 0.94921875, 0.024383544921875, 0.9990234375, 0.9912109375, 0.3017578125, 0.051788330078125, 0.455078125, 0.0117950439453125, 0.044525146484375, 0.9638671875, 0.03497314453125, 0.343994140625, 0.6181640625, 0.96533203125]], "normalized_score": [0.10264312475919724], "tokens": [["\u2581Gary", ",", "\u2581Indiana", "\u2581#", "5", "3", "5", ":", "\u2581\u201c", "It", "\u2581seems", "\u2581to", "\u2581me", "\u2581that", "\u2581part", "\u2581of", "\u2581the", "\u2581function", "\u2581of", "\u2581a", "\u2581Christian", "\u2581is", "\u2581to", "\u2581bring", "\u2581order", "\u2581out", "\u2581of", "\u2581cha", "os", ".", "\u2581To", "\u2581reduce", "\u2581the", "\u2581world", "\u2581to", "\u2581coh", "er", "ence", ".", "\u2581It", "\u2581is", "\u2581his", "\u2581business", "\u2581to", "\u2581be", "\u2581a", "\u2581real", "ist"]], "entropy": [[0.1331787109375, 0.275634765625, 0.1558837890625, 1.029296875, 5.28125, 3.34765625, 2.658203125, 3.625, 5.37109375, 4.79296875, 2.333984375, 3.234375, 1.2431640625, 1.05078125, 1.091796875, 0.0943603515625, 1.1943359375, 5.2890625, 0.0282745361328125, 1.7353515625, 4.71875, 4.46875, 0.413818359375, 2.673828125, 3.216796875, 0.5458984375, 0.0196685791015625, 0.275146484375, 0.0034656524658203125, 1.638671875, 4.26171875, 4.66015625, 1.759765625, 3.78125, 0.33544921875, 1.6669921875, 0.00933837890625, 0.056640625, 1.67578125, 2.412109375, 1.7685546875, 3.32421875, 2.3203125, 0.1966552734375, 4.0703125, 3.158203125, 2.755859375, 0.18359375]], "avg_entropy": [2.0903616348902383], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gary Indiana", "score_of_biggest_cluster": 0.10264312475919724, "score_of_first_answer": 0.10264312475919724, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 1.2516975402832031e-06, "avg_score": 0.10264312475919724, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4126902242631985, "std_of_first_answer": 0.4126902242631985, "avg_entropy_of_first_answer": 2.0903616348902383, "lex_sim": 1.0}
{"question": "The Collegiate Church of St Peter in London is more popularly known by what name?", "answer": ["Westminster Abbey"], "scores": [[0.99072265625, 0.98779296875, 0.9990234375, 0.9892578125, 0.99951171875]], "normalized_score": [0.9932491779327393], "tokens": [["\u2581West", "min", "ster", "\u2581Ab", "bey"]], "entropy": [[0.0740966796875, 0.07305908203125, 0.01036834716796875, 0.07574462890625, 0.00441741943359375]], "avg_entropy": [0.0475372314453125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Westminster Abbey", "score_of_biggest_cluster": 0.9932491779327393, "score_of_first_answer": 0.9932491779327393, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.98779296875, "avg_score": 0.9932491779327393, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.004992899621548522, "std_of_first_answer": 0.004992899621548522, "avg_entropy_of_first_answer": 0.0475372314453125, "lex_sim": 1.0}
{"question": "Denis Gabor won the 1971 Nobel Prize in Physics for which invention?", "answer": ["holography"], "scores": [[0.256103515625, 0.99951171875, 0.763671875, 0.99462890625]], "normalized_score": [0.664037823677063], "tokens": [["\u2581h", "olog", "raph", "y"]], "entropy": [[0.94140625, 0.00467681884765625, 0.61767578125, 0.040740966796875]], "avg_entropy": [0.4011249542236328], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hologrammatic", "score_of_biggest_cluster": 0.664037823677063, "score_of_first_answer": 0.664037823677063, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.256103515625, "avg_score": 0.664037823677063, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3025605902699209, "std_of_first_answer": 0.3025605902699209, "avg_entropy_of_first_answer": 0.4011249542236328, "lex_sim": 1.0}
{"question": "Which Hollywood actor was married to Uma Thurman 1998 - 2004?", "answer": ["Ethan Hawke"], "scores": [[0.99755859375, 0.9931640625, 0.9482421875, 1.0]], "normalized_score": [0.9845089316368103], "tokens": [["\u2581Eth", "an", "\u2581Haw", "ke"]], "entropy": [[0.025665283203125, 0.07080078125, 0.315673828125, 0.0003180503845214844]], "avg_entropy": [0.10311448574066162], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ETHAN HAWKE", "score_of_biggest_cluster": 0.9845089316368103, "score_of_first_answer": 0.9845089316368103, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.9482421875, "avg_score": 0.9845089316368103, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.021214612217473616, "std_of_first_answer": 0.021214612217473616, "avg_entropy_of_first_answer": 0.10311448574066162, "lex_sim": 1.0}
{"question": "Which Cricket broadcaster is famous for his observations of passing traffic?", "answer": ["Brian Johnston"], "scores": [[0.2108154296875, 0.94873046875, 1.0]], "normalized_score": [0.5848103761672974], "tokens": [["\u2581Brian", "\u2581John", "ston"]], "entropy": [[1.958984375, 0.348876953125, 0.001819610595703125]], "avg_entropy": [0.7698936462402344], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Henry Blofeld", "score_of_biggest_cluster": 0.5848103761672974, "score_of_first_answer": 0.5848103761672974, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.2108154296875, "avg_score": 0.5848103761672974, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36054888029357113, "std_of_first_answer": 0.36054888029357113, "avg_entropy_of_first_answer": 0.7698936462402344, "lex_sim": 1.0}
{"question": "According to Greek legend, which sorceress helped Jason to find the golden fleece and became his wife?", "answer": ["Medea"], "scores": [[0.97314453125, 1.0, 0.9736328125]], "normalized_score": [0.9821794629096985], "tokens": [["\u2581M", "ede", "a"]], "entropy": [[0.2413330078125, 0.000762939453125, 0.135009765625]], "avg_entropy": [0.125701904296875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Medea (Seneca)", "score_of_biggest_cluster": 0.9821794629096985, "score_of_first_answer": 0.9821794629096985, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.97314453125, "avg_score": 0.9821794629096985, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.012546284075912607, "std_of_first_answer": 0.012546284075912607, "avg_entropy_of_first_answer": 0.125701904296875, "lex_sim": 1.0}
{"question": "Which cartoon character is mentioned in the single \u2018Life on Mars\u2019 by David Bowie?", "answer": ["Rocket #####"], "scores": [[0.000652313232421875, 0.318115234375, 0.0]], "normalized_score": [0.0], "tokens": [["\u2581Ro", "cket", "\u2581#####"]], "entropy": [[4.4375, 1.88671875, 2.61328125]], "avg_entropy": [2.9791666666666665], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mickey Mouse Operation", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.318115234375, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14980744461340778, "std_of_first_answer": 0.14980744461340778, "avg_entropy_of_first_answer": 2.9791666666666665, "lex_sim": 1.0}
{"question": "Who founded the Jaguar motor company?", "answer": ["Sir William Lyons"], "scores": [[0.94873046875, 0.99951171875, 0.9716796875, 0.99951171875]], "normalized_score": [0.9796262979507446], "tokens": [["\u2581Sir", "\u2581William", "\u2581Ly", "ons"]], "entropy": [[0.31298828125, 0.006580352783203125, 0.1943359375, 0.0029850006103515625]], "avg_entropy": [0.12922239303588867], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sir William Lyons", "score_of_biggest_cluster": 0.9796262979507446, "score_of_first_answer": 0.9796262979507446, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.94873046875, "avg_score": 0.9796262979507446, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.021262321826652518, "std_of_first_answer": 0.021262321826652518, "avg_entropy_of_first_answer": 0.12922239303588867, "lex_sim": 1.0}
{"question": "How many heads did the Hydra have?", "answer": ["5-9 ###What is this?"], "scores": [[0.01318359375, 0.00946044921875, 0.140380859375, 0.7880859375, 2.384185791015625e-07, 8.45193862915039e-05, 0.343505859375, 0.0029392242431640625, 0.289794921875]], "normalized_score": [0.007567131891846657], "tokens": [["\u2581", "5", "-", "9", "\u2581###", "What", "\u2581is", "\u2581this", "?"]], "entropy": [[2.01171875, 1.3681640625, 2.662109375, 0.8349609375, 1.208984375, 3.1328125, 3.57421875, 2.26953125, 3.8984375]], "avg_entropy": [2.3289930555555554], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "9", "score_of_biggest_cluster": 0.007567131891846657, "score_of_first_answer": 0.007567131891846657, "max_score_of_first_answer": 0.7880859375, "min_score_of_first_answer": 2.384185791015625e-07, "avg_score": 0.007567131891846657, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25012017386505897, "std_of_first_answer": 0.25012017386505897, "avg_entropy_of_first_answer": 2.3289930555555554, "lex_sim": 1.0}
{"question": "Lenny Bruce was in what field of entertainment in the 1960s?", "answer": ["Comedian and social commentator"], "scores": [[0.57666015625, 0.5927734375, 0.9970703125, 0.08148193359375, 0.50341796875, 0.493896484375, 0.9912109375]], "normalized_score": [0.4906376898288727], "tokens": [["\u2581Com", "ed", "ian", "\u2581and", "\u2581social", "\u2581comment", "ator"]], "entropy": [[1.4873046875, 0.80126953125, 0.0250701904296875, 1.0498046875, 1.1416015625, 0.8017578125, 0.05889892578125]], "avg_entropy": [0.7665296282087054], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stand up comedy", "score_of_biggest_cluster": 0.4906376898288727, "score_of_first_answer": 0.4906376898288727, "max_score_of_first_answer": 0.9970703125, "min_score_of_first_answer": 0.08148193359375, "avg_score": 0.4906376898288727, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.29290768746045465, "std_of_first_answer": 0.29290768746045465, "avg_entropy_of_first_answer": 0.7665296282087054, "lex_sim": 1.0}
{"question": "What is the first name of James bond villain Dr No?", "answer": ["Dr No"], "scores": [[0.080810546875, 0.48828125]], "normalized_score": [0.19864106178283691], "tokens": [["\u2581Dr", "\u2581No"]], "entropy": [[1.5693359375, 1.0615234375]], "avg_entropy": [1.3154296875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gens Iulia", "score_of_biggest_cluster": 0.19864106178283691, "score_of_first_answer": 0.19864106178283691, "max_score_of_first_answer": 0.48828125, "min_score_of_first_answer": 0.080810546875, "avg_score": 0.19864106178283691, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2037353515625, "std_of_first_answer": 0.2037353515625, "avg_entropy_of_first_answer": 1.3154296875, "lex_sim": 1.0}
{"question": "Ninette de Valois was involved professionally in what activity?", "answer": ["Ballet"], "scores": [[0.4052734375, 0.9970703125]], "normalized_score": [0.635677695274353], "tokens": [["\u2581Bal", "let"]], "entropy": [[2.0546875, 0.0225372314453125]], "avg_entropy": [1.0386123657226562], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ballet", "score_of_biggest_cluster": 0.635677695274353, "score_of_first_answer": 0.635677695274353, "max_score_of_first_answer": 0.9970703125, "min_score_of_first_answer": 0.4052734375, "avg_score": 0.635677695274353, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2958984375, "std_of_first_answer": 0.2958984375, "avg_entropy_of_first_answer": 1.0386123657226562, "lex_sim": 1.0}
{"question": "What was the name of the island where Nelson Mandela was imprisoned for 18 years?", "answer": ["Robben Island"], "scores": [[0.98974609375, 0.9990234375, 0.9541015625]], "normalized_score": [0.9807643890380859], "tokens": [["\u2581Rob", "ben", "\u2581Island"]], "entropy": [[0.09619140625, 0.0112152099609375, 0.2462158203125]], "avg_entropy": [0.1178741455078125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Robbeneiland", "score_of_biggest_cluster": 0.9807643890380859, "score_of_first_answer": 0.9807643890380859, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.9541015625, "avg_score": 0.9807643890380859, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.019363701920527936, "std_of_first_answer": 0.019363701920527936, "avg_entropy_of_first_answer": 0.1178741455078125, "lex_sim": 1.0}
{"question": "Whose play about the Salem witch trials in the 17th century drew parallels to McCarthyism in the 1950s?", "answer": ["Arthur Miller #SaturdaySpotlight is on Kim Cox!"], "scores": [[0.90185546875, 0.9970703125, 2.980232238769531e-07, 0.0016679763793945312, 0.00034546852111816406, 8.106231689453125e-06, 0.640625, 0.66259765625, 0.0006957054138183594, 0.06829833984375, 5.7220458984375e-06, 0.0035533905029296875, 3.904104232788086e-05, 0.01349639892578125]], "normalized_score": [0.002443492179736495], "tokens": [["\u2581Arthur", "\u2581Miller", "\u2581#", "S", "aturday", "Sp", "ot", "light", "\u2581is", "\u2581on", "\u2581Kim", "\u2581C", "ox", "!"]], "entropy": [[0.391357421875, 0.024627685546875, 1.33984375, 1.8681640625, 0.16455078125, 0.349365234375, 2.12109375, 1.837890625, 2.4453125, 4.76171875, 3.087890625, 3.943359375, 0.50341796875, 2.765625]], "avg_entropy": [1.8288726806640625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Agnes Barley", "score_of_biggest_cluster": 0.002443492179736495, "score_of_first_answer": 0.002443492179736495, "max_score_of_first_answer": 0.9970703125, "min_score_of_first_answer": 2.980232238769531e-07, "avg_score": 0.002443492179736495, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36727954403411867, "std_of_first_answer": 0.36727954403411867, "avg_entropy_of_first_answer": 1.8288726806640625, "lex_sim": 1.0}
{"question": "What company advertised itself with the slogan \u201cWe\u2019ll leave a light on for you\u201d?", "answer": ["Motel 6"], "scores": [[0.291015625, 0.99755859375, 0.97607421875, 0.99951171875]], "normalized_score": [0.7295101881027222], "tokens": [["\u2581Mot", "el", "\u2581", "6"]], "entropy": [[2.791015625, 0.017669677734375, 0.1641845703125, 0.0033721923828125]], "avg_entropy": [0.7440605163574219], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Motel 6", "score_of_biggest_cluster": 0.7295101881027222, "score_of_first_answer": 0.7295101881027222, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.291015625, "avg_score": 0.7295101881027222, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.30326243460619795, "std_of_first_answer": 0.30326243460619795, "avg_entropy_of_first_answer": 0.7440605163574219, "lex_sim": 1.0}
{"question": "In which novel does Michael Henchard sell his wife and child for 5 guineas?", "answer": ["Far from the Madding Crowd"], "scores": [[0.00727081298828125, 0.701171875, 0.9970703125, 0.99560546875, 1.0, 0.99951171875, 1.0]], "normalized_score": [0.4698953926563263], "tokens": [["\u2581Far", "\u2581from", "\u2581the", "\u2581M", "adding", "\u2581Crow", "d"]], "entropy": [[1.1337890625, 0.6298828125, 0.0256195068359375, 0.03179931640625, 0.0012731552124023438, 0.0056610107421875, 0.00139617919921875]], "avg_entropy": [0.26134586334228516], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mayor of Casterbridge", "score_of_biggest_cluster": 0.4698953926563263, "score_of_first_answer": 0.4698953926563263, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.00727081298828125, "avg_score": 0.4698953926563263, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34509689365868207, "std_of_first_answer": 0.34509689365868207, "avg_entropy_of_first_answer": 0.26134586334228516, "lex_sim": 1.0}
{"question": "How was President Kennedy assassinated?", "answer": ["He was shot and killed by Lee Harvey Oswald."], "scores": [[0.05828857421875, 0.994140625, 0.1297607421875, 0.638671875, 0.96484375, 0.89306640625, 0.7734375, 0.99951171875, 0.99951171875, 0.99951171875, 1.0, 1.0, 0.53955078125, 1.9073486328125e-06, 0.005382537841796875, 7.671117782592773e-05, 0.00022554397583007812, 0.019256591796875, 0.17822265625, 0.0211334228515625, 0.93603515625, 0.0190582275390625, 0.01334381103515625, 0.0300445556640625]], "normalized_score": [0.06945613026618958], "tokens": [["\u2581He", "\u2581was", "\u2581shot", "\u2581and", "\u2581killed", "\u2581by", "\u2581Lee", "\u2581Har", "vey", "\u2581O", "sw", "ald", ".", "\u2581#", "4", "\u2581R", "ally", "\u2581for", "\u2581the", "\u2581C", "ure", "\u2581at", "\u2581P", "IR"]], "entropy": [[1.73046875, 0.047027587890625, 0.61474609375, 1.146484375, 0.1837158203125, 0.515625, 0.6640625, 0.0034847259521484375, 0.003879547119140625, 0.005397796630859375, 0.00020825862884521484, 0.0006070137023925781, 1.58203125, 0.833984375, 4.6796875, 3.11328125, 4.71875, 4.90234375, 5.2265625, 4.38671875, 0.281005859375, 4.234375, 5.6171875, 4.2265625]], "avg_entropy": [2.02992490430673], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Shot (unit)", "score_of_biggest_cluster": 0.06945613026618958, "score_of_first_answer": 0.06945613026618958, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.9073486328125e-06, "avg_score": 0.06945613026618958, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.442656396620445, "std_of_first_answer": 0.442656396620445, "avg_entropy_of_first_answer": 2.02992490430673, "lex_sim": 1.0}
{"question": "Who played Dr. Doug Ross on the TV show E.R.?", "answer": ["George Clooney"], "scores": [[0.7861328125, 0.99462890625, 0.99951171875]], "normalized_score": [0.92111736536026], "tokens": [["\u2581George", "\u2581Clo", "oney"]], "entropy": [[0.740234375, 0.05303955078125, 0.0043792724609375]], "avg_entropy": [0.2658843994140625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Max (pig)", "score_of_biggest_cluster": 0.92111736536026, "score_of_first_answer": 0.92111736536026, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.7861328125, "avg_score": 0.92111736536026, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09945686982549283, "std_of_first_answer": 0.09945686982549283, "avg_entropy_of_first_answer": 0.2658843994140625, "lex_sim": 1.0}
{"question": "In the English translations of the 'Asterix' comics, what is the name of the village leader, a middle-aged, obese man with red hair, pigtails and a huge moustache?", "answer": ["Asterix"], "scores": [[0.05401611328125, 0.99609375, 0.99951171875]], "normalized_score": [0.3774595856666565], "tokens": [["\u2581A", "ster", "ix"]], "entropy": [[1.9423828125, 0.035186767578125, 0.006256103515625]], "avg_entropy": [0.6612752278645834], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Unhygienix", "score_of_biggest_cluster": 0.3774595856666565, "score_of_first_answer": 0.3774595856666565, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.05401611328125, "avg_score": 0.3774595856666565, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44490746804209863, "std_of_first_answer": 0.44490746804209863, "avg_entropy_of_first_answer": 0.6612752278645834, "lex_sim": 1.0}
{"question": "Who is the oldest person to have a number one hit in Britain?", "answer": ["Phyllis Dillon #ChromeExtensions #GoogleChromeExtensions #Tutorials #Development"], "scores": [[0.010711669921875, 0.64990234375, 0.99462890625, 0.491455078125, 0.31396484375, 1.0251998901367188e-05, 0.0011796951293945312, 3.057718276977539e-05, 3.916025161743164e-05, 0.069580078125, 0.012420654296875, 0.252685546875, 0.98974609375, 0.0145263671875, 0.301025390625, 0.09869384765625, 1.52587890625e-05, 0.74560546875, 0.49462890625, 0.0007891654968261719, 0.09564208984375]], "normalized_score": [0.018862249329686165], "tokens": [["\u2581Ph", "yll", "is", "\u2581D", "illon", "\u2581#", "Ch", "rome", "Extensions", "\u2581#", "Google", "Ch", "rome", "Extensions", "\u2581#", "T", "utorial", "s", "\u2581#", "Develop", "ment"]], "entropy": [[4.14453125, 1.478515625, 0.03497314453125, 1.3955078125, 1.451171875, 2.2734375, 4.328125, 3.890625, 3.736328125, 2.142578125, 4.203125, 3.353515625, 0.0582275390625, 1.0888671875, 1.072265625, 4.4140625, 0.2939453125, 1.0400390625, 1.4052734375, 3.328125, 0.82421875]], "avg_entropy": [2.188450404575893], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Armstrong, Louis %22Pops%22", "score_of_biggest_cluster": 0.018862249329686165, "score_of_first_answer": 0.018862249329686165, "max_score_of_first_answer": 0.99462890625, "min_score_of_first_answer": 1.0251998901367188e-05, "avg_score": 0.018862249329686165, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3273669266311389, "std_of_first_answer": 0.3273669266311389, "avg_entropy_of_first_answer": 2.188450404575893, "lex_sim": 1.0}
{"question": "\u2018Before I Forget\u2019 is a 1981 Autobiography by which English actor?", "answer": ["James Forbes, Tony Hancock"], "scores": [[0.04473876953125, 0.0008115768432617188, 0.423828125, 0.07757568359375, 0.0020427703857421875, 0.72265625, 0.99658203125]], "normalized_score": [0.05613420903682709], "tokens": [["\u2581James", "\u2581For", "bes", ",", "\u2581Tony", "\u2581Han", "cock"]], "entropy": [[2.736328125, 2.162109375, 1.96875, 4.1640625, 2.916015625, 1.7802734375, 0.032440185546875]], "avg_entropy": [2.2514256068638394], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mason, James", "score_of_biggest_cluster": 0.05613420903682709, "score_of_first_answer": 0.05613420903682709, "max_score_of_first_answer": 0.99658203125, "min_score_of_first_answer": 0.0008115768432617188, "avg_score": 0.05613420903682709, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3718828606247105, "std_of_first_answer": 0.3718828606247105, "avg_entropy_of_first_answer": 2.2514256068638394, "lex_sim": 1.0}
{"question": "What was the first airship to cross the Atlantic, in 1919?", "answer": ["Roma"], "scores": [[0.0002872943878173828]], "normalized_score": [0.000287294271402061], "tokens": [["\u2581Roma"]], "entropy": [[1.3330078125]], "avg_entropy": [1.3330078125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "R34", "score_of_biggest_cluster": 0.000287294271402061, "score_of_first_answer": 0.000287294271402061, "max_score_of_first_answer": 0.0002872943878173828, "min_score_of_first_answer": 0.0002872943878173828, "avg_score": 0.000287294271402061, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.3330078125, "lex_sim": 1.0}
{"question": "What 2007 movie earned Eddie Murphy the Golden Raspberry for Worst Actor, Worst Supporting Actor, and Worst Supporting Actress?", "answer": ["Norbit"], "scores": [[0.96435546875, 0.99853515625]], "normalized_score": [0.9812965393066406], "tokens": [["\u2581Nor", "bit"]], "entropy": [[0.251953125, 0.0124053955078125]], "avg_entropy": [0.13217926025390625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Norbit", "score_of_biggest_cluster": 0.9812965393066406, "score_of_first_answer": 0.9812965393066406, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.96435546875, "avg_score": 0.9812965393066406, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.01708984375, "std_of_first_answer": 0.01708984375, "avg_entropy_of_first_answer": 0.13217926025390625, "lex_sim": 1.0}
{"question": "Who wrote the 1982 novel, \u2018The Prodigal Daughter\u2019?", "answer": ["Michael Innes"], "scores": [[0.005584716796875, 0.061553955078125, 0.99169921875]], "normalized_score": [0.0698573887348175], "tokens": [["\u2581Michael", "\u2581In", "nes"]], "entropy": [[3.955078125, 4.44921875, 0.056640625]], "avg_entropy": [2.8203125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "JEFFREY ARCHER", "score_of_biggest_cluster": 0.0698573887348175, "score_of_first_answer": 0.0698573887348175, "max_score_of_first_answer": 0.99169921875, "min_score_of_first_answer": 0.005584716796875, "avg_score": 0.0698573887348175, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45224435112586164, "std_of_first_answer": 0.45224435112586164, "avg_entropy_of_first_answer": 2.8203125, "lex_sim": 1.0}
{"question": "Who plays Chicago police officer Sharon Pogue in the 2001 film \u2018Angel Eyes\u2019?", "answer": ["Jennifer Lopez"], "scores": [[0.9775390625, 0.99951171875, 0.98828125, 0.99560546875, 0.998046875]], "normalized_score": [0.9917635917663574], "tokens": [["\u2581Jenn", "ifer", "\u2581L", "ope", "z"]], "entropy": [[0.17431640625, 0.00450897216796875, 0.10650634765625, 0.038909912109375, 0.0165863037109375]], "avg_entropy": [0.06816558837890625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jennifer J.Lo Lopez", "score_of_biggest_cluster": 0.9917635917663574, "score_of_first_answer": 0.9917635917663574, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.9775390625, "avg_score": 0.9917635917663574, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.008108409669784012, "std_of_first_answer": 0.008108409669784012, "avg_entropy_of_first_answer": 0.06816558837890625, "lex_sim": 1.0}
{"question": "Lawrence Sheriff founded which famous school in 1567?", "answer": ["Rugby School #8 of 31 things I have learned from my 40s"], "scores": [[0.95361328125, 0.96533203125, 1.7881393432617188e-07, 0.01340484619140625, 2.7298927307128906e-05, 0.2939453125, 0.01751708984375, 0.08319091796875, 2.205371856689453e-06, 0.042816162109375, 0.0131072998046875, 0.32177734375, 0.1334228515625, 0.1317138671875, 0.030670166015625, 0.05169677734375, 0.330810546875, 0.08355712890625]], "normalized_score": [0.016465669497847557], "tokens": [["\u2581Rugby", "\u2581School", "\u2581#", "8", "\u2581of", "\u2581", "3", "1", "\u2581things", "\u2581I", "\u2581have", "\u2581learned", "\u2581from", "\u2581my", "\u2581", "4", "0", "s"]], "entropy": [[0.34814453125, 0.1895751953125, 0.79248046875, 4.3203125, 1.38671875, 3.359375, 1.4619140625, 2.439453125, 2.052734375, 3.162109375, 2.228515625, 3.193359375, 3.205078125, 4.66015625, 4.4140625, 1.720703125, 2.53515625, 1.4228515625]], "avg_entropy": [2.3829277886284723], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rugby (disambiguation)", "score_of_biggest_cluster": 0.016465669497847557, "score_of_first_answer": 0.016465669497847557, "max_score_of_first_answer": 0.96533203125, "min_score_of_first_answer": 1.7881393432617188e-07, "avg_score": 0.016465669497847557, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2913713897012255, "std_of_first_answer": 0.2913713897012255, "avg_entropy_of_first_answer": 2.3829277886284723, "lex_sim": 1.0}
{"question": "Irish writer and poet Oscar Wilde, who died in 1900, is buried in which European country?", "answer": ["PPPPP"], "scores": [[0.0029544830322265625, 3.635883331298828e-06, 0.26708984375]], "normalized_score": [0.0014209639048203826], "tokens": [["\u2581P", "PP", "PP"]], "entropy": [[0.78759765625, 1.587890625, 4.109375]], "avg_entropy": [2.16162109375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "La Republique francaise", "score_of_biggest_cluster": 0.0014209639048203826, "score_of_first_answer": 0.0014209639048203826, "max_score_of_first_answer": 0.26708984375, "min_score_of_first_answer": 3.635883331298828e-06, "avg_score": 0.0014209639048203826, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1252159196095563, "std_of_first_answer": 0.1252159196095563, "avg_entropy_of_first_answer": 2.16162109375, "lex_sim": 1.0}
{"question": "\"Which pair of films starred Uma Thurman as \"\"the bride\"\", Daryl Hannah as her arch-nemesis and Lucy Liu as \"\"Cottonmouth\"\"?\"", "answer": ["Kill Bill series"], "scores": [[0.78173828125, 0.99560546875, 0.01371002197265625]], "normalized_score": [0.22015520930290222], "tokens": [["\u2581Kill", "\u2581Bill", "\u2581series"]], "entropy": [[1.0380859375, 0.03582763671875, 2.302734375]], "avg_entropy": [1.12554931640625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kill Bill: Volume 3", "score_of_biggest_cluster": 0.22015520930290222, "score_of_first_answer": 0.22015520930290222, "max_score_of_first_answer": 0.99560546875, "min_score_of_first_answer": 0.01371002197265625, "avg_score": 0.22015520930290222, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42160081822340834, "std_of_first_answer": 0.42160081822340834, "avg_entropy_of_first_answer": 1.12554931640625, "lex_sim": 1.0}
{"question": "What style of American Football founded in 2003 is a full-contact 7 a side game where uniforms consist of helmets, shoulder pads, elbow pads, knee pads, bras, and panties?", "answer": ["Women's American football"], "scores": [[0.391357421875, 0.98046875, 1.0, 0.25341796875, 0.1202392578125]], "normalized_score": [0.4107506573200226], "tokens": [["\u2581Women", "'", "s", "\u2581American", "\u2581football"]], "entropy": [[2.55859375, 0.1390380859375, 0.001064300537109375, 2.595703125, 0.5703125]], "avg_entropy": [1.1729423522949218], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lingerie Football League", "score_of_biggest_cluster": 0.4107506573200226, "score_of_first_answer": 0.4107506573200226, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.1202392578125, "avg_score": 0.4107506573200226, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.370303099951567, "std_of_first_answer": 0.370303099951567, "avg_entropy_of_first_answer": 1.1729423522949218, "lex_sim": 1.0}
{"question": "What is the general term for heat energy that has been drawnfrom under the ground?", "answer": ["Geothermal energy #BEYOUROWN WOMEN: LORENZA ROTA"], "scores": [[0.798828125, 0.96923828125, 0.9990234375, 0.990234375, 0.224609375, 2.980232238769531e-07, 0.0004940032958984375, 0.0001544952392578125, 0.0037670135498046875, 0.8076171875, 1.0, 0.0012760162353515625, 0.5927734375, 1.0, 0.005741119384765625, 0.0017423629760742188, 0.00232696533203125, 0.4169921875, 0.4248046875, 0.0256195068359375, 0.011260986328125, 0.029632568359375, 0.52490234375]], "normalized_score": [0.03399212658405304], "tokens": [["\u2581Ge", "oth", "erm", "al", "\u2581energy", "\u2581#", "B", "EY", "OU", "RO", "WN", "\u2581W", "OM", "EN", ":", "\u2581L", "OR", "EN", "Z", "A", "\u2581R", "OT", "A"]], "entropy": [[0.69775390625, 0.1883544921875, 0.00872802734375, 0.078857421875, 1.1943359375, 0.7626953125, 5.31640625, 4.7578125, 0.26904296875, 0.78125, 0.0020122528076171875, 2.3359375, 1.4697265625, 0.0017480850219726562, 1.962890625, 5.0703125, 3.712890625, 1.728515625, 2.529296875, 0.36669921875, 4.16796875, 3.259765625, 1.7294921875]], "avg_entropy": [1.8431518803472104], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Geothermic gradient", "score_of_biggest_cluster": 0.03399212658405304, "score_of_first_answer": 0.03399212658405304, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.980232238769531e-07, "avg_score": 0.03399212658405304, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40919163797251745, "std_of_first_answer": 0.40919163797251745, "avg_entropy_of_first_answer": 1.8431518803472104, "lex_sim": 1.0}
{"question": "The highest grossing hand-drawn animated film in history is?", "answer": ["USD 1.989 billion"], "scores": [[0.0001455545425415039, 0.31689453125, 0.728515625, 0.77001953125, 0.5498046875, 0.002899169921875, 0.4794921875, 0.0131378173828125, 0.91552734375]], "normalized_score": [0.08525198698043823], "tokens": [["\u2581US", "D", "\u2581", "1", ".", "9", "8", "9", "\u2581billion"]], "entropy": [[2.48046875, 2.298828125, 1.4912109375, 0.9609375, 1.4287109375, 1.1708984375, 1.697265625, 1.6025390625, 0.580078125]], "avg_entropy": [1.5234375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Simba and Nala's new-born cub", "score_of_biggest_cluster": 0.08525198698043823, "score_of_first_answer": 0.08525198698043823, "max_score_of_first_answer": 0.91552734375, "min_score_of_first_answer": 0.0001455545425415039, "avg_score": 0.08525198698043823, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3350038986445364, "std_of_first_answer": 0.3350038986445364, "avg_entropy_of_first_answer": 1.5234375, "lex_sim": 1.0}
{"question": "Mohenjo-daro and Harappa in present-day Pakistan were the most important city settlements of a Bronze Age civilization that is named after what major geographic feature?", "answer": ["Indus River Valley Civilization"], "scores": [[0.93603515625, 0.99755859375, 0.2200927734375, 0.52734375, 0.87451171875, 0.9736328125]], "normalized_score": [0.6722257733345032], "tokens": [["\u2581Ind", "us", "\u2581River", "\u2581Valley", "\u2581Civil", "ization"]], "entropy": [[0.409912109375, 0.0258331298828125, 1.501953125, 1.072265625, 0.521484375, 0.131591796875]], "avg_entropy": [0.6105066935221354], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lion River", "score_of_biggest_cluster": 0.6722257733345032, "score_of_first_answer": 0.6722257733345032, "max_score_of_first_answer": 0.99755859375, "min_score_of_first_answer": 0.2200927734375, "avg_score": 0.6722257733345032, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2862530210968115, "std_of_first_answer": 0.2862530210968115, "avg_entropy_of_first_answer": 0.6105066935221354, "lex_sim": 1.0}
{"question": "On which river is Shrewsbury?", "answer": ["Rive Severn"], "scores": [[0.0006184577941894531, 0.52294921875, 0.830078125, 1.0]], "normalized_score": [0.12800359725952148], "tokens": [["\u2581R", "ive", "\u2581Se", "vern"]], "entropy": [[1.5595703125, 2.078125, 1.197265625, 0.00018095970153808594]], "avg_entropy": [1.2087854743003845], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Seven Boar", "score_of_biggest_cluster": 0.12800359725952148, "score_of_first_answer": 0.12800359725952148, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0006184577941894531, "avg_score": 0.12800359725952148, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37999796766007404, "std_of_first_answer": 0.37999796766007404, "avg_entropy_of_first_answer": 1.2087854743003845, "lex_sim": 1.0}
{"question": "Which racecourse hosts the Welsh Grand National?", "answer": ["Chepstow Racecourse"], "scores": [[0.830078125, 1.0, 1.0, 1.0, 0.47900390625, 0.99169921875]], "normalized_score": [0.8563270568847656], "tokens": [["\u2581Che", "p", "st", "ow", "\u2581Race", "course"]], "entropy": [[0.9560546875, 0.0007219314575195312, 0.001049041748046875, 0.00228118896484375, 1.201171875, 0.059112548828125]], "avg_entropy": [0.37006521224975586], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chepstowe", "score_of_biggest_cluster": 0.8563270568847656, "score_of_first_answer": 0.8563270568847656, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.47900390625, "avg_score": 0.8563270568847656, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19100383920677894, "std_of_first_answer": 0.19100383920677894, "avg_entropy_of_first_answer": 0.37006521224975586, "lex_sim": 1.0}
{"question": "Clyde Tombaugh discovered which celestial object in our solar system in 1930?", "answer": ["Pluto"], "scores": [[0.97607421875, 0.9638671875]], "normalized_score": [0.9699515104293823], "tokens": [["\u2581Pl", "uto"]], "entropy": [[0.1976318359375, 0.209228515625]], "avg_entropy": [0.20343017578125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Operation PLUTO", "score_of_biggest_cluster": 0.9699515104293823, "score_of_first_answer": 0.9699515104293823, "max_score_of_first_answer": 0.97607421875, "min_score_of_first_answer": 0.9638671875, "avg_score": 0.9699515104293823, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.006103515625, "std_of_first_answer": 0.006103515625, "avg_entropy_of_first_answer": 0.20343017578125, "lex_sim": 1.0}
{"question": "'The Slave of Duty' is the alternative name for which Gilbert and Sullivan opera?", "answer": ["The Pirates of Penzance #TBT: The First Golf Major of the Year"], "scores": [[0.473876953125, 0.74365234375, 1.0, 0.998046875, 0.99951171875, 1.0, 1.0, 4.172325134277344e-07, 0.00010246038436889648, 0.927734375, 0.0012655258178710938, 0.07086181640625, 0.0015010833740234375, 0.0025577545166015625, 0.1871337890625, 0.0003681182861328125, 0.58935546875, 0.95849609375, 0.982421875]], "normalized_score": [0.04989864304661751], "tokens": [["\u2581The", "\u2581Pir", "ates", "\u2581of", "\u2581P", "enz", "ance", "\u2581#", "TB", "T", ":", "\u2581The", "\u2581First", "\u2581G", "olf", "\u2581Major", "\u2581of", "\u2581the", "\u2581Year"]], "entropy": [[1.7744140625, 0.84130859375, 0.0009813308715820312, 0.0186309814453125, 0.005161285400390625, 0.0018262863159179688, 0.000858306884765625, 0.8583984375, 5.33984375, 0.564453125, 2.1484375, 4.32421875, 2.703125, 4.71875, 2.255859375, 0.984375, 1.9462890625, 0.2000732421875, 0.10577392578125]], "avg_entropy": [1.515409369217722], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Pirates of Penzance", "score_of_biggest_cluster": 0.04989864304661751, "score_of_first_answer": 0.04989864304661751, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 4.172325134277344e-07, "avg_score": 0.04989864304661751, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44174989329742564, "std_of_first_answer": 0.44174989329742564, "avg_entropy_of_first_answer": 1.515409369217722, "lex_sim": 1.0}
{"question": "Which is the only English football team, in the top four divisions, to have a Latin word in its name?", "answer": ["VULG"], "scores": [[0.112060546875, 0.0009503364562988281, 0.77197265625]], "normalized_score": [0.04348211735486984], "tokens": [["\u2581V", "UL", "G"]], "entropy": [[4.51953125, 4.3984375, 1.1572265625]], "avg_entropy": [3.3583984375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Aston Villa", "score_of_biggest_cluster": 0.04348211735486984, "score_of_first_answer": 0.04348211735486984, "max_score_of_first_answer": 0.77197265625, "min_score_of_first_answer": 0.0009503364562988281, "avg_score": 0.04348211735486984, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34031111396275, "std_of_first_answer": 0.34031111396275, "avg_entropy_of_first_answer": 3.3583984375, "lex_sim": 1.0}
{"question": "The largest region in New Zealand is Canterbury. Which city is the seat of government?", "answer": ["It is a region, not a city."], "scores": [[0.007076263427734375, 0.908203125, 0.0187835693359375, 0.01023101806640625, 0.3662109375, 0.740234375, 0.95703125, 0.99560546875, 0.56982421875, 1.1920928955078125e-07, 1.2814998626708984e-05, 0.07525634765625, 5.5789947509765625e-05, 0.0116119384765625, 0.97705078125, 0.036407470703125, 0.00392913818359375, 0.82080078125, 0.9990234375]], "normalized_score": [0.02698325365781784], "tokens": [["\u2581It", "\u2581is", "\u2581a", "\u2581region", ",", "\u2581not", "\u2581a", "\u2581city", ".", "\u2581###", "Introduction", "\u2581to", "\u2581Computer", "\u2581Program", "ming", "\u2581in", "\u2581P", "ASC", "AL"]], "entropy": [[1.0771484375, 0.6015625, 2.107421875, 4.15234375, 2.22265625, 1.3251953125, 0.2337646484375, 0.041351318359375, 1.2607421875, 1.955078125, 4.60546875, 4.09375, 3.978515625, 2.693359375, 0.1759033203125, 3.83203125, 1.4228515625, 0.67626953125, 0.0104522705078125]], "avg_entropy": [1.9192561099403782], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Port Cooper Plains", "score_of_biggest_cluster": 0.02698325365781784, "score_of_first_answer": 0.02698325365781784, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 1.1920928955078125e-07, "avg_score": 0.02698325365781784, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4239123656834938, "std_of_first_answer": 0.4239123656834938, "avg_entropy_of_first_answer": 1.9192561099403782, "lex_sim": 1.0}
{"question": "What is the nickname of the frontiersman Nathaniel Poe, played by Daniel Day Lewis, in the 1992, film \u2018The Last of the Mohicans\u2019?", "answer": ["Hawkeye #679 - 2017-09-09"], "scores": [[0.9306640625, 0.99951171875, 1.0, 5.364418029785156e-07, 0.0089111328125, 0.061737060546875, 0.0225372314453125, 0.0006699562072753906, 0.128173828125, 0.233642578125, 0.2015380859375, 0.1783447265625, 0.09869384765625, 0.1546630859375, 0.6591796875, 0.15869140625, 0.974609375, 0.5029296875, 0.07275390625]], "normalized_score": [0.07301584631204605], "tokens": [["\u2581Haw", "key", "e", "\u2581#", "6", "7", "9", "\u2581-", "\u2581", "2", "0", "1", "7", "-", "0", "9", "-", "0", "9"]], "entropy": [[0.330078125, 0.00289154052734375, 0.0025310516357421875, 0.64697265625, 4.33984375, 3.029296875, 2.7578125, 2.76953125, 5.96875, 2.419921875, 3.17578125, 2.9609375, 2.50390625, 4.09765625, 1.14453125, 2.1953125, 0.1822509765625, 1.1474609375, 2.080078125]], "avg_entropy": [2.1976602453934517], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hawkeye", "score_of_biggest_cluster": 0.07301584631204605, "score_of_first_answer": 0.07301584631204605, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.364418029785156e-07, "avg_score": 0.07301584631204605, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3675843840948322, "std_of_first_answer": 0.3675843840948322, "avg_entropy_of_first_answer": 2.1976602453934517, "lex_sim": 1.0}
{"question": "What are the membranes enveloping the brain and spinal cord called ?", "answer": ["Meninx"], "scores": [[0.74853515625, 0.2122802734375]], "normalized_score": [0.39862167835235596], "tokens": [["\u2581Men", "inx"]], "entropy": [[0.7783203125, 2.9296875]], "avg_entropy": [1.85400390625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cerebral meninges", "score_of_biggest_cluster": 0.39862167835235596, "score_of_first_answer": 0.39862167835235596, "max_score_of_first_answer": 0.74853515625, "min_score_of_first_answer": 0.2122802734375, "avg_score": 0.39862167835235596, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.26812744140625, "std_of_first_answer": 0.26812744140625, "avg_entropy_of_first_answer": 1.85400390625, "lex_sim": 1.0}
{"question": "Which illness caused Edward VII's coronation to be delayed by six weeks?", "answer": ["Appendicitis"], "scores": [[0.38232421875, 0.98291015625, 0.99365234375]], "normalized_score": [0.7201009392738342], "tokens": [["\u2581Append", "ic", "itis"]], "entropy": [[0.97216796875, 0.11651611328125, 0.0521240234375]], "avg_entropy": [0.3802693684895833], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rumbling appendicitis", "score_of_biggest_cluster": 0.7201009392738342, "score_of_first_answer": 0.7201009392738342, "max_score_of_first_answer": 0.99365234375, "min_score_of_first_answer": 0.38232421875, "avg_score": 0.7201009392738342, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2856845461793003, "std_of_first_answer": 0.2856845461793003, "avg_entropy_of_first_answer": 0.3802693684895833, "lex_sim": 1.0}
{"question": "What is the former name of Kinshasa ?", "answer": ["L\u00e9opoldville #FashionFriday: A Conversation with Fashion Editor Lisa Aiken"], "scores": [[0.5908203125, 0.95947265625, 0.99951171875, 0.99951171875, 2.384185791015625e-07, 0.00255584716796875, 0.0004794597625732422, 0.05096435546875, 0.44384765625, 0.998046875, 0.003772735595703125, 0.0098114013671875, 0.000499725341796875, 0.984375, 0.9765625, 0.75927734375, 0.03485107421875, 0.95068359375, 0.006099700927734375, 0.00380706787109375, 0.8662109375, 0.98583984375]], "normalized_score": [0.05075377970933914], "tokens": [["\u2581L\u00e9", "op", "old", "ville", "\u2581#", "F", "ashion", "F", "rid", "ay", ":", "\u2581A", "\u2581Con", "vers", "ation", "\u2581with", "\u2581F", "ashion", "\u2581Editor", "\u2581Lisa", "\u2581A", "iken"]], "entropy": [[0.99169921875, 0.26904296875, 0.005298614501953125, 0.005260467529296875, 0.84228515625, 5.6484375, 4.390625, 3.76171875, 2.6796875, 0.017669677734375, 1.2705078125, 4.04296875, 3.6953125, 0.114013671875, 0.118896484375, 1.1142578125, 4.97265625, 0.419921875, 2.6328125, 4.65625, 1.0478515625, 0.13427734375]], "avg_entropy": [1.9468841552734375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Leopoldville", "score_of_biggest_cluster": 0.05075377970933914, "score_of_first_answer": 0.05075377970933914, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 2.384185791015625e-07, "avg_score": 0.05075377970933914, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45008226143238345, "std_of_first_answer": 0.45008226143238345, "avg_entropy_of_first_answer": 1.9468841552734375, "lex_sim": 1.0}
{"question": "The Battle of Asculum took place in 279 BC between the Romans and the Greeks in which the Romans lost 6,000 men, while the Greeks lost 3,500, including many of their officers. Who commanded the 'victorious' Greeks?", "answer": ["Pyrrhus of Epirus"], "scores": [[0.92431640625, 1.0, 1.0, 0.99560546875, 0.8662109375, 0.9970703125, 0.99267578125]], "normalized_score": [0.9667072296142578], "tokens": [["\u2581P", "yr", "r", "hus", "\u2581of", "\u2581Ep", "irus"]], "entropy": [[0.5126953125, 0.0004298686981201172, 0.0021762847900390625, 0.02911376953125, 0.5849609375, 0.0268402099609375, 0.0528564453125]], "avg_entropy": [0.17272468975612096], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pirro of Epirus", "score_of_biggest_cluster": 0.9667072296142578, "score_of_first_answer": 0.9667072296142578, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.8662109375, "avg_score": 0.9667072296142578, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.04859939251058145, "std_of_first_answer": 0.04859939251058145, "avg_entropy_of_first_answer": 0.17272468975612096, "lex_sim": 1.0}
{"question": "Which island was home to Odysseus?", "answer": ["Ithaca"], "scores": [[0.919921875, 0.99365234375, 0.9482421875]], "normalized_score": [0.9534580111503601], "tokens": [["\u2581I", "th", "aca"]], "entropy": [[0.6142578125, 0.0626220703125, 0.300537109375]], "avg_entropy": [0.3258056640625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ithake", "score_of_biggest_cluster": 0.9534580111503601, "score_of_first_answer": 0.9534580111503601, "max_score_of_first_answer": 0.99365234375, "min_score_of_first_answer": 0.919921875, "avg_score": 0.9534580111503601, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.030368668819546748, "std_of_first_answer": 0.030368668819546748, "avg_entropy_of_first_answer": 0.3258056640625, "lex_sim": 1.0}
{"question": "Name the UK budget holiday company specialising in Turkey and Greece which went bust in July 2010?", "answer": ["XL Holiday Group #4: D-Day\u2019s 75th Anniversary"], "scores": [[0.13623046875, 0.9951171875, 0.0712890625, 0.01132965087890625, 0.0687255859375, 5.960464477539062e-07, 0.005298614501953125, 0.0025196075439453125, 0.00048804283142089844, 0.003177642822265625, 0.0133056640625, 0.0008969306945800781, 0.9951171875, 0.1527099609375, 0.8173828125, 0.95703125, 0.990234375, 0.50244140625, 0.99951171875, 0.998046875]], "normalized_score": [0.038973987102508545], "tokens": [["\u2581X", "L", "\u2581Hol", "iday", "\u2581Group", "\u2581#", "4", ":", "\u2581D", "-", "Day", "\u2019", "s", "\u2581", "7", "5", "th", "\u2581An", "ni", "versary"]], "entropy": [[1.4501953125, 0.038238525390625, 1.0966796875, 0.0634765625, 2.26171875, 1.34375, 4.77734375, 2.98046875, 4.55078125, 5.25, 3.986328125, 4.0078125, 0.04150390625, 5.31640625, 0.77880859375, 0.2359619140625, 0.08197021484375, 1.04296875, 0.005252838134765625, 0.01763916015625]], "avg_entropy": [1.9663652420043944], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Goldtrail", "score_of_biggest_cluster": 0.038973987102508545, "score_of_first_answer": 0.038973987102508545, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 5.960464477539062e-07, "avg_score": 0.038973987102508545, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4395525105380786, "std_of_first_answer": 0.4395525105380786, "avg_entropy_of_first_answer": 1.9663652420043944, "lex_sim": 1.0}
{"question": "Which literary location did John Jarndyce own?", "answer": ["Bleak House"], "scores": [[0.7958984375, 0.982421875, 0.97021484375]], "normalized_score": [0.9120273590087891], "tokens": [["\u2581Ble", "ak", "\u2581House"]], "entropy": [[1.296875, 0.093017578125, 0.18017578125]], "avg_entropy": [0.5233561197916666], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Reverend Chadband", "score_of_biggest_cluster": 0.9120273590087891, "score_of_first_answer": 0.9120273590087891, "max_score_of_first_answer": 0.982421875, "min_score_of_first_answer": 0.7958984375, "avg_score": 0.9120273590087891, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08519664432402509, "std_of_first_answer": 0.08519664432402509, "avg_entropy_of_first_answer": 0.5233561197916666, "lex_sim": 1.0}
{"question": "Where is the strait of Strelasund?", "answer": ["Bod\u00f8"], "scores": [[0.002719879150390625, 0.109130859375]], "normalized_score": [0.017228543758392334], "tokens": [["\u2581Bod", "\u00f8"]], "entropy": [[2.953125, 1.8134765625]], "avg_entropy": [2.38330078125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mare Suevicum", "score_of_biggest_cluster": 0.017228543758392334, "score_of_first_answer": 0.017228543758392334, "max_score_of_first_answer": 0.109130859375, "min_score_of_first_answer": 0.002719879150390625, "avg_score": 0.017228543758392334, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05320549011230469, "std_of_first_answer": 0.05320549011230469, "avg_entropy_of_first_answer": 2.38330078125, "lex_sim": 1.0}
{"question": "In what US state is Mount Rushmore?", "answer": ["SD and KS"], "scores": [[0.00019288063049316406, 0.0005521774291992188, 0.040985107421875, 0.71484375]], "normalized_score": [0.00747396657243371], "tokens": [["\u2581SD", "\u2581and", "\u2581K", "S"]], "entropy": [[0.239501953125, 2.306640625, 2.603515625, 0.89697265625]], "avg_entropy": [1.51165771484375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "South dakota", "score_of_biggest_cluster": 0.00747396657243371, "score_of_first_answer": 0.00747396657243371, "max_score_of_first_answer": 0.71484375, "min_score_of_first_answer": 0.00019288063049316406, "avg_score": 0.00747396657243371, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3039657405245362, "std_of_first_answer": 0.3039657405245362, "avg_entropy_of_first_answer": 1.51165771484375, "lex_sim": 1.0}
{"question": "Which French composer and pupil of Saint-Saens is best known for his requiem?", "answer": ["Durufle #81 Post by shawross \u00bb Fri Jan 27, 2017 3:53 pm"], "scores": [[0.033355712890625, 0.9912109375, 0.859375, 3.4570693969726562e-06, 0.0123291015625, 0.01025390625, 5.960464477539063e-08, 0.00539398193359375, 0.0011682510375976562, 0.2083740234375, 0.00528717041015625, 0.282958984375, 0.08697509765625, 0.11181640625, 0.9990234375, 0.2978515625, 0.0164031982421875, 0.99951171875, 0.99169921875, 0.99951171875, 1.0, 0.5146484375, 0.71142578125, 0.9951171875, 0.061370849609375, 0.99951171875, 0.1524658203125, 0.10968017578125, 0.72705078125]], "normalized_score": [0.06344262510538101], "tokens": [["\u2581Dur", "uf", "le", "\u2581#", "8", "1", "\u2581Post", "\u2581by", "\u2581sh", "aw", "ross", "\u2581\u00bb", "\u2581Fri", "\u2581Jan", "\u2581", "2", "7", ",", "\u2581", "2", "0", "1", "7", "\u2581", "3", ":", "5", "3", "\u2581pm"]], "entropy": [[1.2294921875, 0.07275390625, 0.6357421875, 1.2939453125, 4.28125, 1.396484375, 1.2763671875, 2.787109375, 5.78515625, 3.84765625, 0.8525390625, 2.37890625, 2.453125, 2.49609375, 0.006702423095703125, 1.1708984375, 0.8740234375, 0.0066375732421875, 0.063232421875, 0.0038967132568359375, 0.0010232925415039062, 0.82958984375, 0.8076171875, 0.039306640625, 1.90234375, 0.004367828369140625, 1.7900390625, 2.302734375, 0.59423828125]], "avg_entropy": [1.42011284006053], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gabriel Faure", "score_of_biggest_cluster": 0.06344262510538101, "score_of_first_answer": 0.06344262510538101, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.06344262510538101, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42115190380864553, "std_of_first_answer": 0.42115190380864553, "avg_entropy_of_first_answer": 1.42011284006053, "lex_sim": 1.0}
{"question": "Which film finds Mike Myers deeply suspicious about his newly-married wife's skills with a meat cleaver?", "answer": ["So I Married an Axe Murder"], "scores": [[0.7333984375, 0.9658203125, 0.97021484375, 1.0, 0.78759765625, 0.99267578125, 0.99951171875, 0.99560546875, 1.0]], "normalized_score": [0.9327986836433411], "tokens": [["\u2581So", "\u2581I", "\u2581Mar", "ried", "\u2581an", "\u2581A", "xe", "\u2581Mur", "der"]], "entropy": [[1.3583984375, 0.28515625, 0.167724609375, 0.0009603500366210938, 0.5712890625, 0.0460205078125, 0.003719329833984375, 0.034820556640625, 0.0019683837890625]], "avg_entropy": [0.2744508319430881], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Harriet Michaels", "score_of_biggest_cluster": 0.9327986836433411, "score_of_first_answer": 0.9327986836433411, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.7333984375, "avg_score": 0.9327986836433411, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09665118652569707, "std_of_first_answer": 0.09665118652569707, "avg_entropy_of_first_answer": 0.2744508319430881, "lex_sim": 1.0}
{"question": "In the British monarchy, who succeeded Queen Anne to the throne?", "answer": ["King George I"], "scores": [[0.66552734375, 0.99560546875, 0.9912109375]], "normalized_score": [0.8692401051521301], "tokens": [["\u2581King", "\u2581George", "\u2581I"]], "entropy": [[0.81884765625, 0.037078857421875, 0.0675048828125]], "avg_entropy": [0.3078104654947917], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "George the First", "score_of_biggest_cluster": 0.8692401051521301, "score_of_first_answer": 0.8692401051521301, "max_score_of_first_answer": 0.99560546875, "min_score_of_first_answer": 0.66552734375, "avg_score": 0.8692401051521301, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.15457493103717904, "std_of_first_answer": 0.15457493103717904, "avg_entropy_of_first_answer": 0.3078104654947917, "lex_sim": 1.0}
{"question": "Who is the current presenter of BBC Radio 4 programme Desert Island Discs?", "answer": ["Kirsty Young, she has presented the programme since 2006."], "scores": [[0.416015625, 0.99609375, 1.0, 0.99951171875, 0.00418853759765625, 0.1474609375, 0.73974609375, 0.0143890380859375, 0.99169921875, 0.24462890625, 0.99853515625, 0.966796875, 0.99951171875, 1.0, 0.88720703125, 0.99755859375, 0.77880859375, 2.384185791015625e-06, 7.671117782592773e-05, 0.546875, 0.99609375, 0.2330322265625, 0.998046875, 0.0148468017578125, 0.0011625289916992188, 0.7431640625, 0.0014066696166992188, 0.0008258819580078125, 0.0306549072265625, 0.0268096923828125, 0.346435546875, 0.05816650390625, 0.1617431640625, 0.912109375, 0.96240234375, 0.990234375, 0.98388671875, 0.99755859375, 0.7109375, 0.47412109375, 0.998046875, 0.8828125, 0.98486328125, 0.99951171875, 0.118408203125, 0.0408935546875, 0.0909423828125, 0.96240234375]], "normalized_score": [0.1665409654378891], "tokens": [["\u2581K", "irst", "y", "\u2581Young", ",", "\u2581she", "\u2581has", "\u2581presented", "\u2581the", "\u2581programme", "\u2581since", "\u2581", "2", "0", "0", "6", ".", "\u2581#", "Me", "To", "o", "\u2581Mov", "ement", ":", "\u2581Act", "ress", "\u2581Kate", "\u2581Sh", "arma", "\u2581Acc", "uses", "\u2581Director", "\u2581Ali", "\u2581Abb", "as", "\u2581Z", "af", "ar", "\u2581of", "\u2581Sex", "ual", "\u2581Har", "ass", "ment", ",", "\u2581S", "ends", "\u2581Leg"]], "entropy": [[0.7431640625, 0.032012939453125, 6.92605972290039e-05, 0.006359100341796875, 0.83056640625, 2.06640625, 1.1591796875, 0.133056640625, 0.05865478515625, 0.865234375, 0.01551055908203125, 0.203857421875, 0.004566192626953125, 0.001529693603515625, 0.3603515625, 0.0196380615234375, 0.7177734375, 0.9443359375, 4.01953125, 2.3828125, 0.034759521484375, 2.61328125, 0.02069091796875, 3.224609375, 4.5078125, 0.99609375, 3.05859375, 1.779296875, 2.28125, 3.939453125, 0.69580078125, 4.671875, 3.072265625, 0.568359375, 0.1846923828125, 0.0718994140625, 0.09771728515625, 0.0182952880859375, 0.9326171875, 0.927734375, 0.01617431640625, 0.392578125, 0.0909423828125, 0.005161285400390625, 3.70703125, 5.79296875, 1.5419921875, 0.260498046875]], "avg_entropy": [1.251438630123933], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kirsty Young", "score_of_biggest_cluster": 0.1665409654378891, "score_of_first_answer": 0.1665409654378891, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.384185791015625e-06, "avg_score": 0.1665409654378891, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.42253811213446785, "std_of_first_answer": 0.42253811213446785, "avg_entropy_of_first_answer": 1.251438630123933, "lex_sim": 1.0}
{"question": "Name the ballet dancer who requested asylum in France while in Paris with the Kirov Ballet, in June 1961?", "answer": ["Rudolf Nureyev #ThrowbackThursday \u2013 \u2018The Fault in Our Stars\u2019 Film Review"], "scores": [[0.5537109375, 0.99658203125, 0.99560546875, 0.998046875, 1.0, 4.172325134277344e-07, 0.0002008676528930664, 0.98876953125, 0.069091796875, 0.99951171875, 0.998046875, 3.218650817871094e-05, 3.886222839355469e-05, 0.1104736328125, 0.004505157470703125, 0.00698089599609375, 0.90966796875, 0.98486328125, 0.9921875, 0.93359375, 0.0004832744598388672, 0.1392822265625]], "normalized_score": [0.04275164753198624], "tokens": [["\u2581Rudolf", "\u2581N", "ure", "ye", "v", "\u2581#", "Throw", "back", "Th", "urs", "day", "\u2581\u2013", "\u2581\u2018", "The", "\u2581F", "ault", "\u2581in", "\u2581Our", "\u2581Stars", "\u2019", "\u2581Film", "\u2581Review"]], "entropy": [[0.697265625, 0.03363037109375, 0.03271484375, 0.0165863037109375, 0.00115966796875, 0.68603515625, 4.68359375, 0.07763671875, 2.505859375, 0.0034332275390625, 0.018768310546875, 1.4296875, 4.7578125, 4.4140625, 4.3984375, 2.291015625, 0.475341796875, 0.08782958984375, 0.06610107421875, 0.330810546875, 4.28125, 3.626953125]], "avg_entropy": [1.5870902321555398], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nuriev", "score_of_biggest_cluster": 0.04275164753198624, "score_of_first_answer": 0.04275164753198624, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 4.172325134277344e-07, "avg_score": 0.04275164753198624, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46451443013804095, "std_of_first_answer": 0.46451443013804095, "avg_entropy_of_first_answer": 1.5870902321555398, "lex_sim": 1.0}
{"question": "Dassault, Embraer, Grob, and Sino Swearingen are makers of which high-end transport?", "answer": ["Business jet"], "scores": [[0.720703125, 0.11785888671875]], "normalized_score": [0.29144683480262756], "tokens": [["\u2581Business", "\u2581jet"]], "entropy": [[1.3232421875, 1.263671875]], "avg_entropy": [1.29345703125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Executive jets", "score_of_biggest_cluster": 0.29144683480262756, "score_of_first_answer": 0.29144683480262756, "max_score_of_first_answer": 0.720703125, "min_score_of_first_answer": 0.11785888671875, "avg_score": 0.29144683480262756, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.301422119140625, "std_of_first_answer": 0.301422119140625, "avg_entropy_of_first_answer": 1.29345703125, "lex_sim": 1.0}
{"question": "On what river is the town of Colchester", "answer": ["From the River Colne, near its confluence with the River Blackwater"], "scores": [[0.0008320808410644531, 0.309814453125, 0.62255859375, 0.67529296875, 0.99853515625, 0.040557861328125, 0.006031036376953125, 0.320068359375, 0.91064453125, 1.0, 1.0, 0.99609375, 0.9921875, 0.9560546875, 0.56982421875, 0.9990234375]], "normalized_score": [0.2993149161338806], "tokens": [["\u2581From", "\u2581the", "\u2581River", "\u2581Col", "ne", ",", "\u2581near", "\u2581its", "\u2581confl", "u", "ence", "\u2581with", "\u2581the", "\u2581River", "\u2581Black", "water"]], "entropy": [[1.3359375, 1.142578125, 2.193359375, 1.4130859375, 0.01216888427734375, 1.3271484375, 2.216796875, 1.80859375, 0.47265625, 0.0006928443908691406, 0.0004482269287109375, 0.031585693359375, 0.0557861328125, 0.263427734375, 1.583984375, 0.00800323486328125]], "avg_entropy": [0.8666408360004425], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Colne", "score_of_biggest_cluster": 0.2993149161338806, "score_of_first_answer": 0.2993149161338806, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0008320808410644531, "avg_score": 0.2993149161338806, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3813388167935405, "std_of_first_answer": 0.3813388167935405, "avg_entropy_of_first_answer": 0.8666408360004425, "lex_sim": 1.0}
{"question": "Fred Scuttle was one of the best-known characters of which British comedian?", "answer": ["Mr Fredrick Truman"], "scores": [[0.00293731689453125, 0.003749847412109375, 0.067626953125, 0.221435546875, 0.53662109375]], "normalized_score": [0.038850799202919006], "tokens": [["\u2581Mr", "\u2581Fred", "rick", "\u2581Tr", "uman"]], "entropy": [[3.650390625, 1.0068359375, 3.1015625, 4.046875, 2.171875]], "avg_entropy": [2.7955078125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Benny Hill", "score_of_biggest_cluster": 0.038850799202919006, "score_of_first_answer": 0.038850799202919006, "max_score_of_first_answer": 0.53662109375, "min_score_of_first_answer": 0.00293731689453125, "avg_score": 0.038850799202919006, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2015062172136846, "std_of_first_answer": 0.2015062172136846, "avg_entropy_of_first_answer": 2.7955078125, "lex_sim": 1.0}
{"question": "US merchant and statesman John Hancock is remembered for his large and stylish signature on which document?", "answer": ["The United States Declaration of Independence"], "scores": [[0.72998046875, 0.8916015625, 0.99658203125, 0.99853515625, 1.0, 0.99853515625, 0.998046875, 1.0]], "normalized_score": [0.946747362613678], "tokens": [["\u2581The", "\u2581United", "\u2581States", "\u2581Dec", "laration", "\u2581of", "\u2581Independ", "ence"]], "entropy": [[1.2392578125, 0.53515625, 0.0302276611328125, 0.01256561279296875, 0.0005059242248535156, 0.01062774658203125, 0.0184173583984375, 0.00032329559326171875]], "avg_entropy": [0.23088520765304565], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "American Independance", "score_of_biggest_cluster": 0.946747362613678, "score_of_first_answer": 0.946747362613678, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.72998046875, "avg_score": 0.946747362613678, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0908203125, "std_of_first_answer": 0.0908203125, "avg_entropy_of_first_answer": 0.23088520765304565, "lex_sim": 1.0}
{"question": "What number is Hurricane on the Beaufort Scale?", "answer": ["12"], "scores": [[0.6201171875, 0.9716796875, 0.97705078125]], "normalized_score": [0.8381170630455017], "tokens": [["\u2581", "1", "2"]], "entropy": [[1.9072265625, 0.1466064453125, 0.126708984375]], "avg_entropy": [0.7268473307291666], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "twelve", "score_of_biggest_cluster": 0.8381170630455017, "score_of_first_answer": 0.8381170630455017, "max_score_of_first_answer": 0.97705078125, "min_score_of_first_answer": 0.6201171875, "avg_score": 0.8381170630455017, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.16700852619672224, "std_of_first_answer": 0.16700852619672224, "avg_entropy_of_first_answer": 0.7268473307291666, "lex_sim": 1.0}
{"question": "Dr Gunther Van Hagens caused controversy on TV by performing which medical procedure live?", "answer": ["Autopsy"], "scores": [[0.285400390625, 1.0, 0.93408203125]], "normalized_score": [0.643595814704895], "tokens": [["\u2581Aut", "ops", "y"]], "entropy": [[2.142578125, 0.0022754669189453125, 0.26123046875]], "avg_entropy": [0.8020280202229818], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "AN AUTOPSY", "score_of_biggest_cluster": 0.643595814704895, "score_of_first_answer": 0.643595814704895, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.285400390625, "avg_score": 0.643595814704895, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3224533818514564, "std_of_first_answer": 0.3224533818514564, "avg_entropy_of_first_answer": 0.8020280202229818, "lex_sim": 1.0}
{"question": "What is the fifth event in the heptathlon", "answer": ["\u7231\u5947\u6735\u516d\u9879\u76ee"], "scores": [[0.456787109375, 8.344650268554688e-07, 0.172607421875, 0.76513671875, 0.0182037353515625, 0.004497528076171875, 0.541015625, 0.3251953125, 0.001708984375, 0.11663818359375, 0.07000732421875]], "normalized_score": [0.028510957956314087], "tokens": [["\u2581", "<0xE7>", "<0x88>", "<0xB1>", "\u5947", "<0xE6>", "<0x9C>", "<0xB5>", "\u516d", "\u9879", "\u76ee"]], "entropy": [[2.025390625, 0.93310546875, 3.44140625, 1.01171875, 4.8984375, 0.994140625, 2.150390625, 1.173828125, 4.02734375, 4.0703125, 3.4453125]], "avg_entropy": [2.56103515625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Longjumper", "score_of_biggest_cluster": 0.028510957956314087, "score_of_first_answer": 0.028510957956314087, "max_score_of_first_answer": 0.76513671875, "min_score_of_first_answer": 8.344650268554688e-07, "avg_score": 0.028510957956314087, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24968280162050638, "std_of_first_answer": 0.24968280162050638, "avg_entropy_of_first_answer": 2.56103515625, "lex_sim": 0.0}
{"question": "Which composer did pianist Clara Wieck marry in 1840?", "answer": ["Robert Schumann"], "scores": [[0.98388671875, 0.97607421875, 0.998046875]], "normalized_score": [0.9859607219696045], "tokens": [["\u2581Robert", "\u2581Sch", "umann"]], "entropy": [[0.155517578125, 0.162841796875, 0.01910400390625]], "avg_entropy": [0.11248779296875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Schumann, Robert Alexander", "score_of_biggest_cluster": 0.9859607219696045, "score_of_first_answer": 0.9859607219696045, "max_score_of_first_answer": 0.998046875, "min_score_of_first_answer": 0.97607421875, "avg_score": 0.9859607219696045, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.009094215523892574, "std_of_first_answer": 0.009094215523892574, "avg_entropy_of_first_answer": 0.11248779296875, "lex_sim": 1.0}
{"question": "Who in legend provided pie fillings for Mrs Lovett?", "answer": ["Sweeney Todd"], "scores": [[0.95654296875, 1.0, 0.99755859375, 0.998046875, 0.99951171875]], "normalized_score": [0.990185022354126], "tokens": [["\u2581S", "ween", "ey", "\u2581Tod", "d"]], "entropy": [[0.337158203125, 1.1265277862548828e-05, 0.017791748046875, 0.018890380859375, 0.006465911865234375]], "avg_entropy": [0.07606350183486939], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sweeny tod", "score_of_biggest_cluster": 0.990185022354126, "score_of_first_answer": 0.990185022354126, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.95654296875, "avg_score": 0.990185022354126, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.01691850493767614, "std_of_first_answer": 0.01691850493767614, "avg_entropy_of_first_answer": 0.07606350183486939, "lex_sim": 1.0}
{"question": "Who was the King of Libya who was overthrown by a military coupled by Colonel Qaddafi in 1969?", "answer": ["King Idris of Libya"], "scores": [[0.93017578125, 0.99609375, 0.99560546875, 0.050994873046875, 0.9990234375, 1.0]], "normalized_score": [0.6007251143455505], "tokens": [["\u2581King", "\u2581Id", "ris", "\u2581of", "\u2581Lib", "ya"]], "entropy": [[0.318359375, 0.03607177734375, 0.02935791015625, 1.267578125, 0.0085906982421875, 0.002079010009765625]], "avg_entropy": [0.2770061492919922], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Idris ibn 'Abd Allah", "score_of_biggest_cluster": 0.6007251143455505, "score_of_first_answer": 0.6007251143455505, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.050994873046875, "avg_score": 0.6007251143455505, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34865323015679606, "std_of_first_answer": 0.34865323015679606, "avg_entropy_of_first_answer": 0.2770061492919922, "lex_sim": 1.0}
{"question": "Which Lloyd Webber musical premiered in the US on 10th December 1993?", "answer": ["Stephen Ward"], "scores": [[0.01201629638671875, 0.60205078125]], "normalized_score": [0.0850553959608078], "tokens": [["\u2581Stephen", "\u2581Ward"]], "entropy": [[1.9052734375, 1.529296875]], "avg_entropy": [1.71728515625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sunset Blvd", "score_of_biggest_cluster": 0.0850553959608078, "score_of_first_answer": 0.0850553959608078, "max_score_of_first_answer": 0.60205078125, "min_score_of_first_answer": 0.01201629638671875, "avg_score": 0.0850553959608078, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2950172424316406, "std_of_first_answer": 0.2950172424316406, "avg_entropy_of_first_answer": 1.71728515625, "lex_sim": 1.0}
{"question": "Which is the only property on a traditional UK Monopoly board which is south of the River Thames?", "answer": ["Water Works"], "scores": [[0.226806640625, 0.6328125]], "normalized_score": [0.3788483440876007], "tokens": [["\u2581Water", "\u2581Works"]], "entropy": [[3.341796875, 0.900390625]], "avg_entropy": [2.12109375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "OLD KENT ROAD", "score_of_biggest_cluster": 0.3788483440876007, "score_of_first_answer": 0.3788483440876007, "max_score_of_first_answer": 0.6328125, "min_score_of_first_answer": 0.226806640625, "avg_score": 0.3788483440876007, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2030029296875, "std_of_first_answer": 0.2030029296875, "avg_entropy_of_first_answer": 2.12109375, "lex_sim": 1.0}
{"question": "A tropical plant that grows several feet high, with a large fleshy root that contains an acrid milk juice, poisonous in its natural form, produces from the juice, extracted under pressure and purified, a staple foodstuff called what?", "answer": ["Manioc"], "scores": [[0.59326171875, 0.463623046875, 0.99462890625]], "normalized_score": [0.6491685509681702], "tokens": [["\u2581Man", "i", "oc"]], "entropy": [[1.45703125, 0.78759765625, 0.0460205078125]], "avg_entropy": [0.7635498046875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Casave", "score_of_biggest_cluster": 0.6491685509681702, "score_of_first_answer": 0.6491685509681702, "max_score_of_first_answer": 0.99462890625, "min_score_of_first_answer": 0.463623046875, "avg_score": 0.6491685509681702, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2260454789878838, "std_of_first_answer": 0.2260454789878838, "avg_entropy_of_first_answer": 0.7635498046875, "lex_sim": 1.0}
{"question": "Who had a hit in 1955 with Rock Around the Clock?", "answer": ["Bill Haley & His Comets"], "scores": [[0.99072265625, 0.9873046875, 1.0, 0.99951171875, 0.890625, 0.97607421875, 0.9990234375, 1.0]], "normalized_score": [0.9797562956809998], "tokens": [["\u2581Bill", "\u2581H", "ale", "y", "\u2581&", "\u2581His", "\u2581Com", "ets"]], "entropy": [[0.08795166015625, 0.09893798828125, 0.0001766681671142578, 0.006580352783203125, 0.40576171875, 0.1383056640625, 0.007511138916015625, 0.0002899169921875]], "avg_entropy": [0.09318938851356506], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bill Haley's Comets", "score_of_biggest_cluster": 0.9797562956809998, "score_of_first_answer": 0.9797562956809998, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.890625, "avg_score": 0.9797562956809998, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03484412556604084, "std_of_first_answer": 0.03484412556604084, "avg_entropy_of_first_answer": 0.09318938851356506, "lex_sim": 1.0}
{"question": "What two word name is given to an 1887 demonstration in London against Irish repression, a 1905 massacre in St. Petersburg, and a 1972 shooting of civilians in Londonderry?", "answer": ["Bloody"], "scores": [[0.7939453125, 0.9990234375]], "normalized_score": [0.8906009197235107], "tokens": [["\u2581Blo", "ody"]], "entropy": [[1.1005859375, 0.00725555419921875]], "avg_entropy": [0.5539207458496094], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Blut Sonntag", "score_of_biggest_cluster": 0.8906009197235107, "score_of_first_answer": 0.8906009197235107, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.7939453125, "avg_score": 0.8906009197235107, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1025390625, "std_of_first_answer": 0.1025390625, "avg_entropy_of_first_answer": 0.5539207458496094, "lex_sim": 1.0}
{"question": "On which river does the Scottish city of Stirling lie?", "answer": ["Forth"], "scores": [[0.26806640625, 0.99951171875]], "normalized_score": [0.5176248550415039], "tokens": [["\u2581For", "th"]], "entropy": [[1.7314453125, 0.006748199462890625]], "avg_entropy": [0.8690967559814453], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Forth (disambiguation)", "score_of_biggest_cluster": 0.5176248550415039, "score_of_first_answer": 0.5176248550415039, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.26806640625, "avg_score": 0.5176248550415039, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36572265625, "std_of_first_answer": 0.36572265625, "avg_entropy_of_first_answer": 0.8690967559814453, "lex_sim": 1.0}
{"question": "In which country does The Girl From Ipanema live", "answer": ["Brazil"], "scores": [[0.775390625]], "normalized_score": [0.775390625], "tokens": [["\u2581Brazil"]], "entropy": [[1.1923828125]], "avg_entropy": [1.1923828125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brazil", "score_of_biggest_cluster": 0.775390625, "score_of_first_answer": 0.775390625, "max_score_of_first_answer": 0.775390625, "min_score_of_first_answer": 0.775390625, "avg_score": 0.775390625, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.1923828125, "lex_sim": 1.0}
{"question": "What is a reality TV show in which a group of people live in a large house watched by cameras, first broadcast in the Netherlands in 1999?", "answer": ["Big Brother Netherlands (Herkulenenvelden)|Big Brother Netherlands"], "scores": [[0.94775390625, 0.9951171875, 1.0, 0.0026950836181640625, 0.034759521484375, 0.0765380859375, 0.001125335693359375, 0.337646484375, 0.0206146240234375, 0.372802734375, 0.05718994140625, 0.00011903047561645508, 0.380615234375, 0.9921875, 0.9990234375, 0.208251953125]], "normalized_score": [0.08689248561859131], "tokens": [["\u2581Big", "\u2581Bro", "ther", "\u2581Netherlands", "\u2581(", "H", "erk", "ul", "enen", "vel", "den", ")|", "Big", "\u2581Bro", "ther", "\u2581Netherlands"]], "entropy": [[0.346923828125, 0.03912353515625, 0.0005488395690917969, 0.78271484375, 0.93505859375, 2.826171875, 0.5302734375, 3.50390625, 4.875, 3.0078125, 0.474609375, 0.55126953125, 3.4765625, 0.06634521484375, 0.00882720947265625, 2.888671875]], "avg_entropy": [1.5196137130260468], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Big Brother (film)", "score_of_biggest_cluster": 0.08689248561859131, "score_of_first_answer": 0.08689248561859131, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.00011903047561645508, "avg_score": 0.08689248561859131, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.413957890820185, "std_of_first_answer": 0.413957890820185, "avg_entropy_of_first_answer": 1.5196137130260468, "lex_sim": 1.0}
{"question": "In the Harry Potter books, what type of creature is Aragog?", "answer": ["Spiders with humanoid features #100DaysOfGod: See the First Trailer for Existential Thriller"], "scores": [[0.0032196044921875, 0.25927734375, 0.0004334449768066406, 0.0982666015625, 0.9853515625, 0.99951171875, 0.138671875, 7.748603820800781e-07, 0.0258026123046875, 0.107666015625, 0.1690673828125, 0.00011295080184936523, 0.994140625, 0.002361297607421875, 0.0012979507446289062, 0.0016641616821289062, 2.6226043701171875e-05, 0.1246337890625, 0.0001137852668762207, 0.0018358230590820312, 0.99853515625, 0.470703125, 0.0011577606201171875, 0.1995849609375, 0.9873046875, 0.453369140625, 0.99951171875, 1.0]], "normalized_score": [0.02188853919506073], "tokens": [["\u2581Sp", "iders", "\u2581with", "\u2581hum", "ano", "id", "\u2581features", "\u2581#", "1", "0", "0", "Days", "Of", "G", "od", ":", "\u2581See", "\u2581the", "\u2581First", "\u2581Tra", "iler", "\u2581for", "\u2581Ex", "ist", "ential", "\u2581Th", "r", "iller"]], "entropy": [[1.556640625, 2.771484375, 1.115234375, 2.91015625, 0.1014404296875, 0.0052642822265625, 3.1015625, 1.6025390625, 5.94140625, 3.974609375, 4.15234375, 1.580078125, 0.047882080078125, 1.8642578125, 2.662109375, 2.80078125, 4.94921875, 2.65234375, 4.9921875, 3.583984375, 0.01476287841796875, 2.333984375, 3.60546875, 1.5439453125, 0.0810546875, 1.5556640625, 0.0045013427734375, 0.0007734298706054688]], "avg_entropy": [2.1966313975197926], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Giant spider", "score_of_biggest_cluster": 0.02188853919506073, "score_of_first_answer": 0.02188853919506073, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 7.748603820800781e-07, "avg_score": 0.02188853919506073, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40710263203813013, "std_of_first_answer": 0.40710263203813013, "avg_entropy_of_first_answer": 2.1966313975197926, "lex_sim": 1.0}
{"question": "Which unit of the Roman army was one tenth of a legion?", "answer": ["Cohort (unit)"], "scores": [[0.49609375, 1.0, 0.91064453125, 0.07684326171875, 0.01509857177734375, 0.5537109375]], "normalized_score": [0.2573097348213196], "tokens": [["\u2581C", "oh", "ort", "\u2581(", "unit", ")"]], "entropy": [[1.775390625, 4.1961669921875e-05, 0.313720703125, 1.16015625, 3.833984375, 0.92822265625]], "avg_entropy": [1.3352527618408203], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cohort", "score_of_biggest_cluster": 0.2573097348213196, "score_of_first_answer": 0.2573097348213196, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.01509857177734375, "avg_score": 0.2573097348213196, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3731091449996784, "std_of_first_answer": 0.3731091449996784, "avg_entropy_of_first_answer": 1.3352527618408203, "lex_sim": 1.0}
{"question": "What is the German term for wine made from fully ripened grapes", "answer": ["Liebfraumilch #ThrowbackThursday ~ The Dark Hunter Series by Sherrilyn Kenyon"], "scores": [[0.0162506103515625, 0.90234375, 0.9970703125, 0.99951171875, 1.0, 1.4901161193847656e-06, 1.627206802368164e-05, 0.9814453125, 0.481201171875, 0.9990234375, 0.99853515625, 6.377696990966797e-06, 0.002765655517578125, 0.00018489360809326172, 6.854534149169922e-06, 0.0218048095703125, 0.253173828125, 0.82666015625, 0.99755859375, 0.99951171875, 1.0, 0.9970703125, 1.0, 1.0]], "normalized_score": [0.047445427626371384], "tokens": [["\u2581Lie", "bf", "raum", "il", "ch", "\u2581#", "Throw", "back", "Th", "urs", "day", "\u2581~", "\u2581The", "\u2581Dark", "\u2581Hunter", "\u2581Series", "\u2581by", "\u2581Sh", "err", "il", "yn", "\u2581Ken", "y", "on"]], "entropy": [[2.83984375, 0.51611328125, 0.0266876220703125, 0.00397491455078125, 0.0016946792602539062, 0.85595703125, 6.06640625, 0.1104736328125, 1.7529296875, 0.0089111328125, 0.01287841796875, 1.263671875, 2.517578125, 4.78515625, 1.681640625, 4.40625, 3.06640625, 1.2353515625, 0.0190277099609375, 0.0032825469970703125, 0.0004792213439941406, 0.0270843505859375, 0.0012063980102539062, 0.0017604827880859375]], "avg_entropy": [1.3001985748608906], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kabinett", "score_of_biggest_cluster": 0.047445427626371384, "score_of_first_answer": 0.047445427626371384, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.4901161193847656e-06, "avg_score": 0.047445427626371384, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45664602298600193, "std_of_first_answer": 0.45664602298600193, "avg_entropy_of_first_answer": 1.3001985748608906, "lex_sim": 1.0}
{"question": "In which American state is Camp David, the mountain retreat of the President of the U.S.A.?", "answer": ["Maryland"], "scores": [[0.97607421875]], "normalized_score": [0.97607421875], "tokens": [["\u2581Maryland"]], "entropy": [[0.1849365234375]], "avg_entropy": [0.1849365234375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Education in Maryland", "score_of_biggest_cluster": 0.97607421875, "score_of_first_answer": 0.97607421875, "max_score_of_first_answer": 0.97607421875, "min_score_of_first_answer": 0.97607421875, "avg_score": 0.97607421875, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.1849365234375, "lex_sim": 1.0}
{"question": "Who was the man behind The Chipmunks?", "answer": ["David Seville"], "scores": [[0.53125, 0.9580078125, 0.99658203125]], "normalized_score": [0.7974932193756104], "tokens": [["\u2581David", "\u2581Se", "ville"]], "entropy": [[0.9580078125, 0.3486328125, 0.0245361328125]], "avg_entropy": [0.4437255859375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "David Seville", "score_of_biggest_cluster": 0.7974932193756104, "score_of_first_answer": 0.7974932193756104, "max_score_of_first_answer": 0.99658203125, "min_score_of_first_answer": 0.53125, "avg_score": 0.7974932193756104, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.21085648099259072, "std_of_first_answer": 0.21085648099259072, "avg_entropy_of_first_answer": 0.4437255859375, "lex_sim": 1.0}
{"question": "Which Norwegian firm supposedly produced the first fleece fabrics in the 1960s?", "answer": ["Helly Hansen THE PETROLIANS HAVE THEIR PARIS AGREEMENT, AND NOW IT\u2019S TIME FOR THE TRUMP PEOPLE TO HAVE THEIR P"], "scores": [[0.1669921875, 0.9931640625, 0.9423828125, 0.9990234375, 5.960464477539063e-08, 0.012939453125, 0.0010881423950195312, 0.021575927734375, 0.0380859375, 0.00539398193359375, 0.08074951171875, 0.0014896392822265625, 0.047943115234375, 0.96435546875, 0.0249786376953125, 0.209716796875, 0.023773193359375, 0.019256591796875, 0.0145111083984375, 0.0005049705505371094, 0.83154296875, 0.98291015625, 0.9970703125, 0.036529541015625, 0.10302734375, 0.0758056640625, 0.81982421875, 0.109375, 0.00949859619140625, 0.99755859375, 0.3701171875, 0.94677734375, 0.46337890625, 0.385009765625, 0.0018711090087890625, 0.08367919921875, 0.268310546875, 0.033416748046875, 0.72021484375, 0.98974609375, 0.99951171875, 0.91845703125, 0.04119873046875, 0.80224609375, 0.99951171875, 0.8369140625, 0.90673828125, 0.10162353515625]], "normalized_score": [0.08749422430992126], "tokens": [["\u2581Hel", "ly", "\u2581Hans", "en", "\u2581THE", "\u2581P", "ET", "RO", "LI", "AN", "S", "\u2581H", "AV", "E", "\u2581THE", "IR", "\u2581P", "AR", "IS", "\u2581AG", "RE", "EM", "ENT", ",", "\u2581AND", "\u2581N", "OW", "\u2581IT", "\u2019", "S", "\u2581T", "IME", "\u2581FOR", "\u2581THE", "\u2581TR", "U", "MP", "\u2581P", "E", "OP", "LE", "\u2581TO", "\u2581H", "AV", "E", "\u2581THE", "IR", "\u2581P"]], "entropy": [[4.24609375, 0.0601806640625, 0.396240234375, 0.007579803466796875, 0.53857421875, 3.9921875, 2.119140625, 1.71875, 1.8388671875, 0.81298828125, 2.642578125, 2.10546875, 2.3984375, 0.1802978515625, 3.7109375, 3.158203125, 3.421875, 3.2890625, 2.78125, 4.0234375, 0.84912109375, 0.1192626953125, 0.0285186767578125, 3.771484375, 4.37890625, 3.240234375, 0.63671875, 2.875, 1.43359375, 0.0207061767578125, 2.54296875, 0.28369140625, 1.177734375, 2.41796875, 3.673828125, 1.9189453125, 0.89404296875, 4.0859375, 1.3134765625, 0.0767822265625, 0.006336212158203125, 0.59619140625, 3.865234375, 1.0048828125, 0.007328033447265625, 0.85791015625, 0.69091796875, 3.21484375]], "avg_entropy": [1.8630149364471436], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Helly Juell Hansen", "score_of_biggest_cluster": 0.08749422430992126, "score_of_first_answer": 0.08749422430992126, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.08749422430992126, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41807315124304456, "std_of_first_answer": 0.41807315124304456, "avg_entropy_of_first_answer": 1.8630149364471436, "lex_sim": 1.0}
{"question": "Beloved in 1999 was whose first movie since The Color Purple in 1985?", "answer": ["Favourite #672592 - 02/20/17 01:37 PM Re: Trump [Re: pdx rick]"], "scores": [[6.812810897827148e-05, 0.047027587890625, 0.60693359375, 0.8935546875, 9.298324584960938e-06, 0.00435638427734375, 0.0235443115234375, 0.033050537109375, 0.07659912109375, 0.04376220703125, 0.031280517578125, 0.0006642341613769531, 0.10552978515625, 0.107666015625, 0.039520263671875, 0.265625, 0.243896484375, 0.1395263671875, 0.779296875, 0.093994140625, 0.01534271240234375, 0.1944580078125, 0.336181640625, 0.09356689453125, 0.97607421875, 0.174072265625, 0.1046142578125, 0.3994140625, 0.00948333740234375, 0.9794921875, 7.915496826171875e-05, 0.06829833984375, 0.99951171875, 0.99951171875, 0.01163482666015625, 0.49072265625, 0.9248046875, 0.9375, 0.9990234375]], "normalized_score": [0.06722195446491241], "tokens": [["\u2581F", "av", "our", "ite", "\u2581#", "6", "7", "2", "5", "9", "2", "\u2581-", "\u2581", "0", "2", "/", "2", "0", "/", "1", "7", "\u2581", "0", "1", ":", "3", "7", "\u2581PM", "\u2581Re", ":", "\u2581Trump", "\u2581[", "Re", ":", "\u2581p", "dx", "\u2581r", "ick", "]"]], "entropy": [[1.3037109375, 4.4140625, 1.4150390625, 0.6689453125, 1.1884765625, 2.9140625, 3.09375, 3.18359375, 3.09765625, 2.462890625, 3.0078125, 3.064453125, 5.9296875, 1.7783203125, 2.537109375, 3.490234375, 1.4208984375, 2.244140625, 1.1435546875, 0.94189453125, 0.7333984375, 3.11328125, 1.49609375, 2.2890625, 0.1851806640625, 1.8544921875, 2.30078125, 2.142578125, 2.76171875, 0.15771484375, 5.72265625, 5.54296875, 0.00435638427734375, 0.00406646728515625, 6.015625, 2.28125, 0.564453125, 0.458251953125, 0.00797271728515625]], "avg_entropy": [2.2291332147060294], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Oprah", "score_of_biggest_cluster": 0.06722195446491241, "score_of_first_answer": 0.06722195446491241, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 9.298324584960938e-06, "avg_score": 0.06722195446491241, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3710868208768804, "std_of_first_answer": 0.3710868208768804, "avg_entropy_of_first_answer": 2.2291332147060294, "lex_sim": 1.0}
{"question": "Which modern musical instrument was developed from the shawm?", "answer": ["Oboe #Selma50: A Legacy of Courage and Resistance"], "scores": [[0.69287109375, 0.99658203125, 0.99658203125, 2.384185791015625e-07, 0.00440216064453125, 0.00930023193359375, 0.034515380859375, 0.00963592529296875, 0.1400146484375, 0.00235748291015625, 0.0122222900390625, 0.01947021484375, 0.99560546875, 0.9267578125, 0.85546875, 1.0, 0.225341796875, 0.16845703125, 0.912109375]], "normalized_score": [0.059248488396406174], "tokens": [["\u2581O", "bo", "e", "\u2581#", "S", "el", "ma", "5", "0", ":", "\u2581A", "\u2581Leg", "acy", "\u2581of", "\u2581Cou", "rage", "\u2581and", "\u2581Res", "istance"]], "entropy": [[1.375, 0.029052734375, 0.0300750732421875, 0.494384765625, 5.125, 5.45703125, 1.38671875, 4.31640625, 2.6875, 1.9814453125, 3.830078125, 5.78125, 0.0343017578125, 0.50439453125, 0.650390625, 0.0005140304565429688, 2.03515625, 2.779296875, 0.29931640625]], "avg_entropy": [2.0419638282374333], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Conservatoire oboe", "score_of_biggest_cluster": 0.059248488396406174, "score_of_first_answer": 0.059248488396406174, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.384185791015625e-07, "avg_score": 0.059248488396406174, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.435958689217486, "std_of_first_answer": 0.435958689217486, "avg_entropy_of_first_answer": 2.0419638282374333, "lex_sim": 1.0}
{"question": "Which prominent Irish Nationalist was hung for treason in 1916 due to his involvement in the Easter Uprising?", "answer": ["James Connolly"], "scores": [[0.05694580078125, 0.9658203125, 0.9990234375, 0.9951171875]], "normalized_score": [0.48356181383132935], "tokens": [["\u2581James", "\u2581Con", "n", "olly"]], "entropy": [[1.978515625, 0.2509765625, 0.00818634033203125, 0.03594970703125]], "avg_entropy": [0.5684070587158203], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Roger Casement", "score_of_biggest_cluster": 0.48356181383132935, "score_of_first_answer": 0.48356181383132935, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.05694580078125, "avg_score": 0.48356181383132935, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4027797719985263, "std_of_first_answer": 0.4027797719985263, "avg_entropy_of_first_answer": 0.5684070587158203, "lex_sim": 1.0}
{"question": "Whose playing of the part of Enrico Bandello in the film 'Little Caesar' brought him to stardom?", "answer": ["Edward G. Robinson"], "scores": [[0.9990234375, 0.9794921875, 0.7060546875, 0.99755859375]], "normalized_score": [0.9111468195915222], "tokens": [["\u2581Edward", "\u2581G", ".", "\u2581Robinson"]], "entropy": [[0.01007843017578125, 0.1854248046875, 0.64501953125, 0.0218963623046875]], "avg_entropy": [0.2156047821044922], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Emanuel Goldenberg", "score_of_biggest_cluster": 0.9111468195915222, "score_of_first_answer": 0.9111468195915222, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.7060546875, "avg_score": 0.9111468195915222, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.12406734380226374, "std_of_first_answer": 0.12406734380226374, "avg_entropy_of_first_answer": 0.2156047821044922, "lex_sim": 1.0}
{"question": "What name is given to a baby marsupial?", "answer": ["Joey #WCW WOMAN CRUSH WEDNESDAY: KRISTIN KREUK"], "scores": [[0.488525390625, 1.0, 1.7881393432617188e-07, 0.0004246234893798828, 0.000568389892578125, 0.26025390625, 0.000499725341796875, 0.072509765625, 0.269775390625, 0.1575927734375, 0.9990234375, 1.0, 0.90234375, 0.99951171875, 0.9990234375, 1.0, 0.9990234375, 0.01503753662109375, 0.01433563232421875, 0.01090240478515625, 0.98193359375, 0.095703125, 0.059112548828125, 0.83837890625, 0.994140625]], "normalized_score": [0.07528413087129593], "tokens": [["\u2581Jo", "ey", "\u2581#", "W", "C", "W", "\u2581W", "O", "MAN", "\u2581CR", "US", "H", "\u2581W", "ED", "N", "ES", "DAY", ":", "\u2581K", "RI", "ST", "IN", "\u2581K", "RE", "UK"]], "entropy": [[1.5673828125, 0.00038242340087890625, 1.119140625, 4.2109375, 2.02734375, 3.330078125, 2.50390625, 4.31640625, 2.12109375, 1.884765625, 0.0085296630859375, 0.000476837158203125, 0.4990234375, 0.004001617431640625, 0.010467529296875, 0.0012035369873046875, 0.00743865966796875, 2.595703125, 5.5625, 3.7109375, 0.12225341796875, 0.485595703125, 2.076171875, 0.67529296875, 0.04071044921875]], "avg_entropy": [1.5552697372436524], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joey (disambiguation)", "score_of_biggest_cluster": 0.07528413087129593, "score_of_first_answer": 0.07528413087129593, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.7881393432617188e-07, "avg_score": 0.07528413087129593, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44595800502318306, "std_of_first_answer": 0.44595800502318306, "avg_entropy_of_first_answer": 1.5552697372436524, "lex_sim": 1.0}
{"question": "The bacteria used in botox is also responsible for what disease", "answer": ["Clostridium botulinum"], "scores": [[0.05633544921875, 0.9990234375, 0.99755859375, 0.9853515625, 0.93505859375, 0.99853515625, 0.99951171875, 0.9970703125]], "normalized_score": [0.6901609301567078], "tokens": [["\u2581Clo", "str", "id", "ium", "\u2581bot", "ul", "in", "um"]], "entropy": [[1.404296875, 0.01104736328125, 0.02056884765625, 0.09906005859375, 0.35498046875, 0.01439666748046875, 0.005191802978515625, 0.0268707275390625]], "avg_entropy": [0.2420516014099121], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Botchalism", "score_of_biggest_cluster": 0.6901609301567078, "score_of_first_answer": 0.6901609301567078, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.05633544921875, "avg_score": 0.6901609301567078, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3086137085168922, "std_of_first_answer": 0.3086137085168922, "avg_entropy_of_first_answer": 0.2420516014099121, "lex_sim": 1.0}
{"question": "What is the oldest vegetable known to man", "answer": ["Asparagus"], "scores": [[0.2342529296875, 0.9990234375, 1.0, 0.99951171875]], "normalized_score": [0.6954439878463745], "tokens": [["\u2581As", "par", "ag", "us"]], "entropy": [[3.60546875, 0.012115478515625, 0.0005140304565429688, 0.006591796875]], "avg_entropy": [0.906172513961792], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pisum sativum", "score_of_biggest_cluster": 0.6954439878463745, "score_of_first_answer": 0.6954439878463745, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.2342529296875, "avg_score": 0.6954439878463745, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3313669557738109, "std_of_first_answer": 0.3313669557738109, "avg_entropy_of_first_answer": 0.906172513961792, "lex_sim": 1.0}
{"question": "The OK Corral is in which US town?", "answer": ["Tombstone"], "scores": [[0.939453125, 0.99560546875, 0.99951171875]], "normalized_score": [0.9778001308441162], "tokens": [["\u2581T", "omb", "stone"]], "entropy": [[0.5634765625, 0.0399169921875, 0.00733184814453125]], "avg_entropy": [0.20357513427734375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gravestone", "score_of_biggest_cluster": 0.9778001308441162, "score_of_first_answer": 0.9778001308441162, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.939453125, "avg_score": 0.9778001308441162, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.027437563831174957, "std_of_first_answer": 0.027437563831174957, "avg_entropy_of_first_answer": 0.20357513427734375, "lex_sim": 1.0}
{"question": "\"Which singer recently made a surprise return to the top of the UK charts after 29 years, with his album \"\"50\"\"?\"", "answer": ["Robbie Williams"], "scores": [[0.716796875, 0.97412109375, 0.98486328125]], "normalized_score": [0.8826631307601929], "tokens": [["\u2581Rob", "bie", "\u2581Williams"]], "entropy": [[1.4375, 0.15869140625, 0.1114501953125]], "avg_entropy": [0.5692138671875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rickard Astley", "score_of_biggest_cluster": 0.8826631307601929, "score_of_first_answer": 0.8826631307601929, "max_score_of_first_answer": 0.98486328125, "min_score_of_first_answer": 0.716796875, "avg_score": 0.8826631307601929, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12391338655482666, "std_of_first_answer": 0.12391338655482666, "avg_entropy_of_first_answer": 0.5692138671875, "lex_sim": 1.0}
{"question": "By what name was world champion boxer Walker Smith better known?", "answer": ["Smokin' Joe"], "scores": [[0.00250244140625, 0.99609375, 0.99609375, 0.9775390625, 0.74267578125]], "normalized_score": [0.28260478377342224], "tokens": [["\u2581Sm", "ok", "in", "'", "\u2581Joe"]], "entropy": [[1.5048828125, 0.0310516357421875, 0.0266265869140625, 0.16455078125, 0.95458984375]], "avg_entropy": [0.53634033203125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Walker Smith Junior", "score_of_biggest_cluster": 0.28260478377342224, "score_of_first_answer": 0.28260478377342224, "max_score_of_first_answer": 0.99609375, "min_score_of_first_answer": 0.00250244140625, "avg_score": 0.28260478377342224, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38248090200804347, "std_of_first_answer": 0.38248090200804347, "avg_entropy_of_first_answer": 0.53634033203125, "lex_sim": 1.0}
{"question": "Which Rugby League team plays home games at Derwent Park?", "answer": ["Workington Town #KnifeChallenge: Throwing a Knife at Someone is Considered a Terroristic Threat in Texas"], "scores": [[0.994140625, 1.0, 0.83837890625, 1.7881393432617188e-07, 0.0008134841918945312, 0.147705078125, 0.004123687744140625, 0.0026340484619140625, 0.049896240234375, 0.0016422271728515625, 0.0007829666137695312, 0.08111572265625, 0.7939453125, 0.66650390625, 0.01375579833984375, 0.998046875, 0.310302734375, 0.06573486328125, 0.85400390625, 0.278564453125, 0.0041351318359375, 0.9990234375, 0.335205078125, 0.00327301025390625, 0.96826171875, 0.309814453125, 0.98291015625, 0.99560546875, 1.0, 0.34326171875, 0.59326171875]], "normalized_score": [0.07421579211950302], "tokens": [["\u2581Work", "ington", "\u2581Town", "\u2581#", "K", "n", "ife", "Ch", "allenge", ":", "\u2581Th", "row", "ing", "\u2581a", "\u2581Kn", "ife", "\u2581at", "\u2581Some", "one", "\u2581is", "\u2581Consider", "ed", "\u2581a", "\u2581T", "error", "istic", "\u2581Th", "re", "at", "\u2581in", "\u2581Texas"]], "entropy": [[0.0565185546875, 0.0007886886596679688, 0.7470703125, 1.294921875, 4.828125, 4.91796875, 0.86669921875, 4.0703125, 0.65576171875, 1.994140625, 5.44921875, 3.10546875, 1.3212890625, 1.7275390625, 1.93359375, 0.01824951171875, 3.5625, 2.4765625, 0.908203125, 3.13671875, 2.24609375, 0.01021575927734375, 2.0078125, 1.876953125, 0.250244140625, 1.1142578125, 0.1326904296875, 0.038665771484375, 0.002140045166015625, 2.400390625, 1.720703125]], "avg_entropy": [1.7700586318969727], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Workington Town", "score_of_biggest_cluster": 0.07421579211950302, "score_of_first_answer": 0.07421579211950302, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.7881393432617188e-07, "avg_score": 0.07421579211950302, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4124186886700501, "std_of_first_answer": 0.4124186886700501, "avg_entropy_of_first_answer": 1.7700586318969727, "lex_sim": 1.0}
{"question": "January 17, 1929 saw the pride of Chester, Il, Elzie Segar debut what iconic cartoon character in a Thimble Theater strip?", "answer": ["Popeye #866: \u201cYou always do that.\u201d"], "scores": [[0.9599609375, 0.99853515625, 3.6597251892089844e-05, 0.007686614990234375, 0.017333984375, 0.032135009765625, 0.002506256103515625, 0.0021038055419921875, 0.0023345947265625, 0.0005817413330078125, 0.053131103515625, 0.09423828125, 0.003566741943359375]], "normalized_score": [0.010514953173696995], "tokens": [["\u2581Pope", "ye", "\u2581#", "8", "6", "6", ":", "\u2581\u201c", "You", "\u2581always", "\u2581do", "\u2581that", ".\u201d"]], "entropy": [[0.364501953125, 0.01232147216796875, 0.77880859375, 3.443359375, 1.9189453125, 2.046875, 2.478515625, 2.517578125, 2.8359375, 2.875, 4.0546875, 2.546875, 1.5615234375]], "avg_entropy": [2.110379145695613], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Popeye Cigarettes", "score_of_biggest_cluster": 0.010514953173696995, "score_of_first_answer": 0.010514953173696995, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 3.6597251892089844e-05, "avg_score": 0.010514953173696995, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.347309717412246, "std_of_first_answer": 0.347309717412246, "avg_entropy_of_first_answer": 2.110379145695613, "lex_sim": 1.0}
{"question": "How meny times have Liverpool won the Europion Cup?", "answer": ["6 times"], "scores": [[0.0295867919921875, 0.07830810546875, 0.47119140625]], "normalized_score": [0.10296759754419327], "tokens": [["\u2581", "6", "\u2581times"]], "entropy": [[2.541015625, 1.3818359375, 1.5]], "avg_entropy": [1.8076171875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "five", "score_of_biggest_cluster": 0.10296759754419327, "score_of_first_answer": 0.10296759754419327, "max_score_of_first_answer": 0.47119140625, "min_score_of_first_answer": 0.0295867919921875, "avg_score": 0.10296759754419327, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19769384010901608, "std_of_first_answer": 0.19769384010901608, "avg_entropy_of_first_answer": 1.8076171875, "lex_sim": 1.0}
{"question": "A harquebus/arquebus is an early form of?", "answer": ["Rifle"], "scores": [[0.188232421875, 0.98388671875, 0.97021484375]], "normalized_score": [0.5642901659011841], "tokens": [["\u2581R", "if", "le"]], "entropy": [[1.87890625, 0.134033203125, 0.188232421875]], "avg_entropy": [0.7337239583333334], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gun", "score_of_biggest_cluster": 0.5642901659011841, "score_of_first_answer": 0.5642901659011841, "max_score_of_first_answer": 0.98388671875, "min_score_of_first_answer": 0.188232421875, "avg_score": 0.5642901659011841, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.37189442773501025, "std_of_first_answer": 0.37189442773501025, "avg_entropy_of_first_answer": 0.7337239583333334, "lex_sim": 1.0}
{"question": "Which singer played the Goblin King Jareth in the film \u0091Labyrinth'?", "answer": ["David Bowie CHAPTER XXIII."], "scores": [[0.99560546875, 0.9970703125, 1.0, 0.0, 0.04278564453125, 0.226318359375, 0.100830078125, 0.023406982421875, 0.01393890380859375, 0.99658203125, 0.39111328125, 0.003192901611328125, 0.71484375, 0.7216796875, 0.002777099609375, 0.5126953125, 1.0, 1.0, 0.9755859375]], "normalized_score": [0.0], "tokens": [["\u2581David", "\u2581B", "owie", "\u2581CHAPTER", "\u2581XX", "III", ".", "\u2581M", "ONE", "Y", "\u2581AND", "\u2581WH", "AT", "\u2581IT", "\u2581RE", "PRE", "S", "ENT", "S"]], "entropy": [[0.04583740234375, 0.031982421875, 0.0005016326904296875, 0.67822265625, 3.498046875, 3.017578125, 3.49609375, 4.6796875, 4.04296875, 0.033905029296875, 2.4765625, 3.185546875, 0.7880859375, 0.9814453125, 1.2822265625, 1.0048828125, 0.0015811920166015625, 0.001209259033203125, 0.205078125]], "avg_entropy": [1.55007593255294], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hermione Farthingale", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42741653378651473, "std_of_first_answer": 0.42741653378651473, "avg_entropy_of_first_answer": 1.55007593255294, "lex_sim": 1.0}
{"question": "Which is the main sport played at the Basin Reserve in Wellington, New Zealand?", "answer": ["Cricket #MySexualHarassmentStory: How Can We Combat Sexual Harassment in India?"], "scores": [[0.8876953125, 0.99755859375, 3.5762786865234375e-07, 6.973743438720703e-05, 0.51025390625, 0.00041747093200683594, 0.0755615234375, 0.056854248046875, 0.8994140625, 0.5439453125, 0.78173828125, 0.00702667236328125, 0.0009250640869140625, 0.002285003662109375, 0.509765625, 0.01206207275390625, 0.9990234375, 0.3671875, 0.990234375, 0.85546875, 0.998046875, 0.9990234375, 0.3408203125, 0.05487060546875, 0.93798828125]], "normalized_score": [0.06658214330673218], "tokens": [["\u2581Cr", "icket", "\u2581#", "My", "S", "ex", "ual", "Har", "ass", "ment", "Story", ":", "\u2581How", "\u2581Can", "\u2581We", "\u2581Com", "bat", "\u2581Sex", "ual", "\u2581Har", "ass", "ment", "\u2581in", "\u2581India", "?"]], "entropy": [[0.53515625, 0.0212249755859375, 0.837890625, 5.17578125, 3.583984375, 0.9697265625, 3.255859375, 3.087890625, 0.346923828125, 0.81884765625, 0.91455078125, 0.81787109375, 2.87109375, 3.99609375, 2.46875, 3.6015625, 0.0103607177734375, 2.5703125, 0.0626220703125, 0.65625, 0.01468658447265625, 0.00687408447265625, 1.7763671875, 1.3740234375, 0.424072265625]], "avg_entropy": [1.6079510498046874], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cricket games", "score_of_biggest_cluster": 0.06658214330673218, "score_of_first_answer": 0.06658214330673218, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 3.5762786865234375e-07, "avg_score": 0.06658214330673218, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41330573413075583, "std_of_first_answer": 0.41330573413075583, "avg_entropy_of_first_answer": 1.6079510498046874, "lex_sim": 1.0}
{"question": "Who had a number one in 2011 with Grenade", "answer": ["Bruno Mars"], "scores": [[0.99072265625, 0.99658203125]], "normalized_score": [0.9936480522155762], "tokens": [["\u2581Bruno", "\u2581Mars"]], "entropy": [[0.102294921875, 0.033721923828125]], "avg_entropy": [0.0680084228515625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gypsy Love (album)", "score_of_biggest_cluster": 0.9936480522155762, "score_of_first_answer": 0.9936480522155762, "max_score_of_first_answer": 0.99658203125, "min_score_of_first_answer": 0.99072265625, "avg_score": 0.9936480522155762, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0029296875, "std_of_first_answer": 0.0029296875, "avg_entropy_of_first_answer": 0.0680084228515625, "lex_sim": 1.0}
{"question": "What is the main ingredient in borscht?", "answer": ["Beetroot ### What is SlowAI?"], "scores": [[0.83447265625, 0.7490234375, 0.83642578125, 0.0, 0.0019350051879882812, 0.70703125, 0.0001710653305053711, 0.006404876708984375, 0.00019633769989013672, 0.7216796875]], "normalized_score": [0.0], "tokens": [["\u2581Be", "et", "root", "\u2581###", "\u2581What", "\u2581is", "\u2581S", "low", "AI", "?"]], "entropy": [[0.7646484375, 0.66943359375, 0.65380859375, 0.83740234375, 2.22265625, 1.9560546875, 1.2197265625, 5.93359375, 4.92578125, 1.8935546875]], "avg_entropy": [2.107666015625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Beetroot", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.83642578125, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3859846037214511, "std_of_first_answer": 0.3859846037214511, "avg_entropy_of_first_answer": 2.107666015625, "lex_sim": 1.0}
{"question": "How many James Bond films did Pierce Brosnan do?", "answer": ["007/4"], "scores": [[0.1636962890625, 0.58056640625, 0.76416015625, 0.37158203125, 0.0013294219970703125, 0.0733642578125]], "normalized_score": [0.11750227212905884], "tokens": [["\u2581", "0", "0", "7", "/", "4"]], "entropy": [[2.111328125, 1.05078125, 1.236328125, 2.005859375, 2.400390625, 1.890625]], "avg_entropy": [1.7825520833333333], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "four", "score_of_biggest_cluster": 0.11750227212905884, "score_of_first_answer": 0.11750227212905884, "max_score_of_first_answer": 0.76416015625, "min_score_of_first_answer": 0.0013294219970703125, "avg_score": 0.11750227212905884, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.27521025720787085, "std_of_first_answer": 0.27521025720787085, "avg_entropy_of_first_answer": 1.7825520833333333, "lex_sim": 1.0}
{"question": "The equatorial Republic of Kiribati's 'Kiritimati' (Christmas) Island, among the first land to see each new day/year, is in which ocean?", "answer": ["Pacific Ocean #24 Day 4: Push-Ups for 22 Dead Children"], "scores": [[0.919921875, 0.2252197265625, 4.172325134277344e-07, 0.0081939697265625, 0.026397705078125, 6.902217864990234e-05, 0.158935546875, 0.051025390625, 0.00809478759765625, 0.00043010711669921875, 0.001392364501953125, 0.085693359375, 0.0229949951171875, 1.0, 0.005199432373046875, 0.2152099609375, 0.1944580078125, 0.034332275390625, 1.245737075805664e-05, 0.0036563873291015625]], "normalized_score": [0.0094751613214612], "tokens": [["\u2581Pacific", "\u2581Ocean", "\u2581#", "2", "4", "\u2581Day", "\u2581", "4", ":", "\u2581P", "ush", "-", "U", "ps", "\u2581for", "\u2581", "2", "2", "\u2581Dead", "\u2581Children"]], "entropy": [[0.58837890625, 1.287109375, 0.85888671875, 5.484375, 3.2890625, 3.8359375, 3.29296875, 1.935546875, 3.615234375, 4.3671875, 5.48828125, 4.078125, 0.7119140625, 0.0007948875427246094, 2.3203125, 4.43359375, 1.953125, 2.236328125, 3.02734375, 3.953125]], "avg_entropy": [2.8378815412521363], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pacific Basin", "score_of_biggest_cluster": 0.0094751613214612, "score_of_first_answer": 0.0094751613214612, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 4.172325134277344e-07, "avg_score": 0.0094751613214612, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.28110940274662816, "std_of_first_answer": 0.28110940274662816, "avg_entropy_of_first_answer": 2.8378815412521363, "lex_sim": 1.0}
{"question": "What is the former name of Helsingor, Denmark?", "answer": ["It lies by the sound"], "scores": [[0.007537841796875, 0.003360748291015625, 0.005199432373046875, 0.79541015625, 0.29150390625]], "normalized_score": [0.03140326589345932], "tokens": [["\u2581It", "\u2581lies", "\u2581by", "\u2581the", "\u2581sound"]], "entropy": [[3.48828125, 1.5048828125, 3.29296875, 1.228515625, 2.90234375]], "avg_entropy": [2.4833984375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Elsinor", "score_of_biggest_cluster": 0.03140326589345932, "score_of_first_answer": 0.03140326589345932, "max_score_of_first_answer": 0.79541015625, "min_score_of_first_answer": 0.003360748291015625, "avg_score": 0.03140326589345932, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3080324203377675, "std_of_first_answer": 0.3080324203377675, "avg_entropy_of_first_answer": 2.4833984375, "lex_sim": 1.0}
{"question": "The splanchnocranium refers to the bones of which defining part of the human body?", "answer": ["Thoracic vertebrae"], "scores": [[0.054046630859375, 0.6376953125, 0.99609375, 0.031402587890625, 0.93310546875, 0.923828125]], "normalized_score": [0.3123883903026581], "tokens": [["\u2581Thor", "ac", "ic", "\u2581verte", "bra", "e"]], "entropy": [[2.97265625, 0.7412109375, 0.03765869140625, 2.57421875, 0.25048828125, 0.37890625]], "avg_entropy": [1.1591898600260417], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Human Face", "score_of_biggest_cluster": 0.3123883903026581, "score_of_first_answer": 0.3123883903026581, "max_score_of_first_answer": 0.99609375, "min_score_of_first_answer": 0.031402587890625, "avg_score": 0.3123883903026581, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4073099908005538, "std_of_first_answer": 0.4073099908005538, "avg_entropy_of_first_answer": 1.1591898600260417, "lex_sim": 1.0}
{"question": "In the NATO phonetic alphabet, which word represents the letter I?", "answer": ["India"], "scores": [[0.9599609375]], "normalized_score": [0.9599609375], "tokens": [["\u2581India"]], "entropy": [[0.230712890625]], "avg_entropy": [0.230712890625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u0d2d\u0d3e\u0d30\u0d24 \u0d2e\u0d39\u0d3e\u0d30\u0d3e\u0d1c\u0d4d\u0d2f\u0d02", "score_of_biggest_cluster": 0.9599609375, "score_of_first_answer": 0.9599609375, "max_score_of_first_answer": 0.9599609375, "min_score_of_first_answer": 0.9599609375, "avg_score": 0.9599609375, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.230712890625, "lex_sim": 1.0}
{"question": "Cosa Nostra refers to what organization?", "answer": ["Mafia Siciliana"], "scores": [[0.02496337890625, 0.998046875, 0.99853515625, 0.0007109642028808594, 0.6845703125, 0.97802734375]], "normalized_score": [0.15097525715827942], "tokens": [["\u2581M", "af", "ia", "\u2581Sic", "il", "iana"]], "entropy": [[0.90087890625, 0.016204833984375, 0.012176513671875, 0.90966796875, 0.923828125, 0.145263671875]], "avg_entropy": [0.4846700032552083], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mob (crime)", "score_of_biggest_cluster": 0.15097525715827942, "score_of_first_answer": 0.15097525715827942, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.0007109642028808594, "avg_score": 0.15097525715827942, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.43892725772627134, "std_of_first_answer": 0.43892725772627134, "avg_entropy_of_first_answer": 0.4846700032552083, "lex_sim": 1.0}
{"question": "Robert Walden Cassotto achieved fame under what name?", "answer": ["Dean Paul Martin"], "scores": [[0.468994140625, 0.08428955078125, 0.8837890625]], "normalized_score": [0.3269113004207611], "tokens": [["\u2581Dean", "\u2581Paul", "\u2581Martin"]], "entropy": [[1.7109375, 0.591796875, 0.5029296875]], "avg_entropy": [0.9352213541666666], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bobby Darrin", "score_of_biggest_cluster": 0.3269113004207611, "score_of_first_answer": 0.3269113004207611, "max_score_of_first_answer": 0.8837890625, "min_score_of_first_answer": 0.08428955078125, "avg_score": 0.3269113004207611, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32647135622162, "std_of_first_answer": 0.32647135622162, "avg_entropy_of_first_answer": 0.9352213541666666, "lex_sim": 1.0}
{"question": "El Prat Airport serves which European city", "answer": ["PRAT"], "scores": [[2.6106834411621094e-05, 0.9091796875]], "normalized_score": [0.004871939774602652], "tokens": [["\u2581PR", "AT"]], "entropy": [[0.18701171875, 0.4716796875]], "avg_entropy": [0.329345703125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The weather in Barcelona", "score_of_biggest_cluster": 0.004871939774602652, "score_of_first_answer": 0.004871939774602652, "max_score_of_first_answer": 0.9091796875, "min_score_of_first_answer": 2.6106834411621094e-05, "avg_score": 0.004871939774602652, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4545767903327942, "std_of_first_answer": 0.4545767903327942, "avg_entropy_of_first_answer": 0.329345703125, "lex_sim": 1.0}
{"question": "What type of creature is a francolin?", "answer": ["A wild, ground-living chicken-like bird"], "scores": [[0.60986328125, 0.00023102760314941406, 0.0025119781494140625, 0.406494140625, 0.99267578125, 0.8876953125, 0.998046875, 0.00971221923828125, 1.0, 0.80810546875, 0.99951171875, 0.98828125]], "normalized_score": [0.17754308879375458], "tokens": [["\u2581A", "\u2581wild", ",", "\u2581ground", "-", "l", "iving", "\u2581ch", "icken", "-", "like", "\u2581bird"]], "entropy": [[1.591796875, 1.478515625, 1.7666015625, 1.7998046875, 0.053436279296875, 0.40625, 0.018218994140625, 0.96142578125, 0.0008630752563476562, 0.81103515625, 0.002773284912109375, 0.07745361328125]], "avg_entropy": [0.747347911198934], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Avians", "score_of_biggest_cluster": 0.17754308879375458, "score_of_first_answer": 0.17754308879375458, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.00023102760314941406, "avg_score": 0.17754308879375458, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4072965058603601, "std_of_first_answer": 0.4072965058603601, "avg_entropy_of_first_answer": 0.747347911198934, "lex_sim": 1.0}
{"question": "In 2015 Edgar Lungu became prime minister of?", "answer": ["Zambia"], "scores": [[0.98486328125, 0.99560546875, 0.9990234375]], "normalized_score": [0.9931457042694092], "tokens": [["\u2581Z", "amb", "ia"]], "entropy": [[0.1270751953125, 0.03662109375, 0.0094757080078125]], "avg_entropy": [0.0577239990234375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Zamibia", "score_of_biggest_cluster": 0.9931457042694092, "score_of_first_answer": 0.9931457042694092, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.98486328125, "avg_score": 0.9931457042694092, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.006033122719026901, "std_of_first_answer": 0.006033122719026901, "avg_entropy_of_first_answer": 0.0577239990234375, "lex_sim": 1.0}
{"question": "Which premiership manager upset Louis Van Gaal in February 2015, when he described his team as long ball united?", "answer": ["Louis Van Gaal"], "scores": [[0.1917724609375, 0.405517578125, 0.99951171875, 1.0]], "normalized_score": [0.528014600276947], "tokens": [["\u2581Louis", "\u2581Van", "\u2581Ga", "al"]], "entropy": [[3.30078125, 0.802734375, 0.00716400146484375, 0.0026454925537109375]], "avg_entropy": [1.0283312797546387], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sam Allardyce", "score_of_biggest_cluster": 0.528014600276947, "score_of_first_answer": 0.528014600276947, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.1917724609375, "avg_score": 0.528014600276947, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3586084272979537, "std_of_first_answer": 0.3586084272979537, "avg_entropy_of_first_answer": 1.0283312797546387, "lex_sim": 1.0}
{"question": "Richard Joseph Daley was the infamous mayor of which major US City in the 1960's?", "answer": ["Chicago"], "scores": [[0.9306640625]], "normalized_score": [0.9306640625], "tokens": [["\u2581Chicago"]], "entropy": [[0.51416015625]], "avg_entropy": [0.51416015625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chi-Beria", "score_of_biggest_cluster": 0.9306640625, "score_of_first_answer": 0.9306640625, "max_score_of_first_answer": 0.9306640625, "min_score_of_first_answer": 0.9306640625, "avg_score": 0.9306640625, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.51416015625, "lex_sim": 1.0}
{"question": "January 18, 1778 saw Captain James Cook discover what island group, which he named the Sandwich Islands, in honor of his sponsor, John Montagu, 4th Earl of Sandwich?", "answer": ["The Hawaiian Islands"], "scores": [[0.349853515625, 0.77197265625, 0.9453125, 0.8515625]], "normalized_score": [0.6828416585922241], "tokens": [["\u2581The", "\u2581Hawai", "ian", "\u2581Islands"]], "entropy": [[1.24609375, 1.1328125, 0.220703125, 0.5947265625]], "avg_entropy": [0.798583984375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hawaiian ridge", "score_of_biggest_cluster": 0.6828416585922241, "score_of_first_answer": 0.6828416585922241, "max_score_of_first_answer": 0.9453125, "min_score_of_first_answer": 0.349853515625, "avg_score": 0.6828416585922241, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22771119086423278, "std_of_first_answer": 0.22771119086423278, "avg_entropy_of_first_answer": 0.798583984375, "lex_sim": 1.0}
{"question": "In what modern-day country was the Norse settlement of Vinland?", "answer": ["North America"], "scores": [[0.0146331787109375, 0.9619140625]], "normalized_score": [0.11864171922206879], "tokens": [["\u2581North", "\u2581America"]], "entropy": [[1.857421875, 0.2208251953125]], "avg_entropy": [1.03912353515625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Canada", "score_of_biggest_cluster": 0.11864171922206879, "score_of_first_answer": 0.11864171922206879, "max_score_of_first_answer": 0.9619140625, "min_score_of_first_answer": 0.0146331787109375, "avg_score": 0.11864171922206879, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.47364044189453125, "std_of_first_answer": 0.47364044189453125, "avg_entropy_of_first_answer": 1.03912353515625, "lex_sim": 1.0}
{"question": "In 1990, George H. W. Bush famously said \u201cI do not like what. And I haven\u2019t liked it since I was a little kid and my mother made me eat it. And I\u2019m President of the United States and I\u2019m not going to eat any more what.\u201d?", "answer": ["Broccoli Whenever we're in an unfamiliar city, we like to take local walking tours."], "scores": [[0.0950927734375, 0.99267578125, 0.99951171875, 2.980232238769531e-07, 0.006244659423828125, 0.001834869384765625, 0.06390380859375, 0.433837890625, 0.185791015625, 0.002353668212890625, 0.0017213821411132812, 0.5966796875, 0.9990234375, 0.04620361328125, 0.50048828125, 0.376708984375, 0.197998046875, 0.98046875, 0.1279296875, 0.0001533031463623047, 0.1177978515625, 0.9765625, 1.0, 0.260009765625, 0.102783203125, 0.99267578125, 0.463623046875, 0.5810546875, 0.256591796875, 0.63916015625, 0.62744140625, 0.30810546875, 1.0, 0.99365234375, 0.09112548828125, 0.7099609375, 0.8681640625, 1.0, 0.90673828125, 0.82958984375, 0.29833984375, 0.9873046875, 0.931640625, 0.9736328125, 0.53662109375, 0.02197265625, 0.84130859375, 0.9853515625]], "normalized_score": [0.17427797615528107], "tokens": [["\u2581Bro", "cc", "oli", "\u2581When", "ever", "\u2581we", "'", "re", "\u2581in", "\u2581an", "\u2581un", "famil", "iar", "\u2581city", ",", "\u2581we", "\u2581like", "\u2581to", "\u2581take", "\u2581local", "\u2581walking", "\u2581t", "ours", ".", "\u2581Not", "\u2581only", "\u2581do", "\u2581they", "\u2581provide", "\u2581a", "\u2581great", "\u2581over", "view", "\u2581of", "\u2581a", "\u2581city", "'", "s", "\u2581history", "\u2581and", "\u2581culture", ",", "\u2581but", "\u2581they", "\u2581also", "\u2581lead", "\u2581us", "\u2581to"]], "entropy": [[0.90380859375, 0.048919677734375, 0.0034656524658203125, 0.90625, 3.134765625, 2.69140625, 3.4609375, 1.1376953125, 3.095703125, 1.6318359375, 2.962890625, 1.9482421875, 0.0080108642578125, 2.634765625, 2.029296875, 3.26953125, 3.13671875, 0.1617431640625, 3.740234375, 1.6298828125, 2.9453125, 0.1429443359375, 0.0010499954223632812, 1.3076171875, 2.7421875, 0.06475830078125, 1.2197265625, 1.1201171875, 1.6201171875, 1.28125, 1.708984375, 1.2734375, 0.0006594657897949219, 0.047943115234375, 0.42431640625, 0.96875, 0.54541015625, 0.0011882781982421875, 0.55712890625, 0.481201171875, 1.8955078125, 0.08544921875, 0.278076171875, 0.167724609375, 1.1328125, 2.037109375, 0.5517578125, 0.09912109375]], "avg_entropy": [1.3174533943335216], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brocolli", "score_of_biggest_cluster": 0.17427797615528107, "score_of_first_answer": 0.17427797615528107, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.980232238769531e-07, "avg_score": 0.17427797615528107, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38773067061582095, "std_of_first_answer": 0.38773067061582095, "avg_entropy_of_first_answer": 1.3174533943335216, "lex_sim": 1.0}
{"question": "Which wife of Henry VIII had already married twice before she became queen, and married for a fourth time after Henry's death?", "answer": ["Anne Boleyn, Catherine Parr"], "scores": [[0.06939697265625, 0.91162109375, 0.99951171875, 0.99853515625, 0.183349609375, 0.31396484375, 0.125244140625, 1.0]], "normalized_score": [0.3821888864040375], "tokens": [["\u2581Anne", "\u2581B", "ole", "yn", ",", "\u2581Catherine", "\u2581Par", "r"]], "entropy": [[1.275390625, 0.384521484375, 0.005802154541015625, 0.01267242431640625, 1.1376953125, 2.287109375, 0.7861328125, 0.00016427040100097656]], "avg_entropy": [0.7361860573291779], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Katheryn Parre", "score_of_biggest_cluster": 0.3821888864040375, "score_of_first_answer": 0.3821888864040375, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.06939697265625, "avg_score": 0.3821888864040375, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4081933969925321, "std_of_first_answer": 0.4081933969925321, "avg_entropy_of_first_answer": 0.7361860573291779, "lex_sim": 1.0}
{"question": "Who is the United Nations and Arab League envoy to Syria?", "answer": ["Kofi A. Annan"], "scores": [[0.007598876953125, 0.99951171875, 0.99951171875, 0.0247650146484375, 0.98388671875, 0.994140625, 1.0]], "normalized_score": [0.29266124963760376], "tokens": [["\u2581K", "of", "i", "\u2581A", ".", "\u2581Ann", "an"]], "entropy": [[1.7783203125, 0.005199432373046875, 0.0037555694580078125, 0.23388671875, 0.09075927734375, 0.0421142578125, 0.001087188720703125]], "avg_entropy": [0.3078746795654297], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kofi A Annan", "score_of_biggest_cluster": 0.29266124963760376, "score_of_first_answer": 0.29266124963760376, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.007598876953125, "avg_score": 0.29266124963760376, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.44242453625417133, "std_of_first_answer": 0.44242453625417133, "avg_entropy_of_first_answer": 0.3078746795654297, "lex_sim": 1.0}
{"question": "What Canadian-born teen idol of the 1950s-60s wrote the lyrics to Sinatra's song My Way?", "answer": ["PAUL ANKA"], "scores": [[0.00040268898010253906, 0.91552734375, 0.64208984375, 0.99267578125, 0.99951171875]], "normalized_score": [0.18800371885299683], "tokens": [["\u2581PA", "UL", "\u2581AN", "K", "A"]], "entropy": [[0.314453125, 0.4638671875, 1.412109375, 0.052764892578125, 0.005336761474609375]], "avg_entropy": [0.4497062683105469], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u0628\u0648\u0644 \u0645\u0635\u0637\u0641\u0649 \u0639\u0628\u062f\u064a \u0623\u0646\u0643\u0627", "score_of_biggest_cluster": 0.18800371885299683, "score_of_first_answer": 0.18800371885299683, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.00040268898010253906, "avg_score": 0.18800371885299683, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37791645800503987, "std_of_first_answer": 0.37791645800503987, "avg_entropy_of_first_answer": 0.4497062683105469, "lex_sim": 1.0}
{"question": "Who in 1975 became the first black man to win a Wimbledon singles title?", "answer": ["Arthur Ashe"], "scores": [[0.97607421875, 0.9609375, 1.0]], "normalized_score": [0.9788721799850464], "tokens": [["\u2581Arthur", "\u2581As", "he"]], "entropy": [[0.1658935546875, 0.230712890625, 0.0001285076141357422]], "avg_entropy": [0.13224498430887857], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Arthur Ashe, Jr.", "score_of_biggest_cluster": 0.9788721799850464, "score_of_first_answer": 0.9788721799850464, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.9609375, "avg_score": 0.9788721799850464, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.016081190424390067, "std_of_first_answer": 0.016081190424390067, "avg_entropy_of_first_answer": 0.13224498430887857, "lex_sim": 1.0}
{"question": "Who played Clayton Farlowe in Dallas", "answer": ["Eric Farlowe"], "scores": [[0.001644134521484375, 0.974609375, 0.9990234375, 0.97802734375]], "normalized_score": [0.19891779124736786], "tokens": [["\u2581Eric", "\u2581Far", "low", "e"]], "entropy": [[3.41796875, 0.1630859375, 0.01013946533203125, 0.128173828125]], "avg_entropy": [0.9298419952392578], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Howard Keel", "score_of_biggest_cluster": 0.19891779124736786, "score_of_first_answer": 0.19891779124736786, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.001644134521484375, "avg_score": 0.19891779124736786, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4254262251968305, "std_of_first_answer": 0.4254262251968305, "avg_entropy_of_first_answer": 0.9298419952392578, "lex_sim": 1.0}
{"question": "Whom did Andy Murray defeat in the Wimbledon men's singles final in 2013 ?", "answer": ["Novak Djokovic"], "scores": [[0.69189453125, 0.998046875, 0.98095703125, 0.99951171875, 0.998046875, 0.99951171875]], "normalized_score": [0.9366872310638428], "tokens": [["\u2581Nov", "ak", "\u2581Dj", "ok", "ov", "ic"]], "entropy": [[1.36328125, 0.0167236328125, 0.1265869140625, 0.007404327392578125, 0.0197906494140625, 0.00649261474609375]], "avg_entropy": [0.2567132314046224], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Novak Djokovi\u0107", "score_of_biggest_cluster": 0.9366872310638428, "score_of_first_answer": 0.9366872310638428, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.69189453125, "avg_score": 0.9366872310638428, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.11322955709633986, "std_of_first_answer": 0.11322955709633986, "avg_entropy_of_first_answer": 0.2567132314046224, "lex_sim": 1.0}
{"question": "In which James Bond film does actress Jane Seymour play Solitaire?", "answer": ["Live and Let Die (1973 film) #TalentRadar FA People\u2019s Team Of The Week #13: Eriksen, Dybala, Falsini & More"], "scores": [[0.91796875, 0.943359375, 0.96728515625, 0.99609375, 0.13134765625, 0.98828125, 1.0, 1.0, 0.99951171875, 0.01763916015625, 0.99462890625, 1.0728836059570312e-06, 0.00437164306640625, 5.364418029785156e-06, 0.20849609375, 0.006031036376953125, 0.74365234375, 0.0006389617919921875, 1.4543533325195312e-05, 0.0034198760986328125, 0.998046875, 0.041412353515625, 0.004917144775390625, 0.9482421875, 0.7763671875, 0.3095703125, 0.2039794921875, 0.14306640625, 0.2169189453125, 0.003986358642578125, 0.1607666015625, 0.99951171875, 1.0, 0.39990234375, 0.027740478515625, 0.99853515625, 0.998046875, 0.84130859375, 0.0780029296875, 9.495019912719727e-05, 0.5302734375, 0.32177734375, 0.319580078125]], "normalized_score": [0.0765138790011406], "tokens": [["\u2581Live", "\u2581and", "\u2581Let", "\u2581Die", "\u2581(", "1", "9", "7", "3", "\u2581film", ")", "\u2581#", "T", "al", "ent", "Rad", "ar", "\u2581FA", "\u2581People", "\u2019", "s", "\u2581Team", "\u2581Of", "\u2581The", "\u2581Week", "\u2581#", "1", "3", ":", "\u2581E", "ri", "ks", "en", ",", "\u2581Dy", "b", "ala", ",", "\u2581F", "als", "ini", "\u2581&", "\u2581More"]], "entropy": [[0.42138671875, 0.329345703125, 0.1500244140625, 0.03814697265625, 1.0810546875, 0.077880859375, 0.0012378692626953125, 0.0011110305786132812, 0.0028362274169921875, 0.11993408203125, 0.045989990234375, 0.6806640625, 4.3671875, 0.055633544921875, 2.689453125, 3.98828125, 0.76806640625, 2.76953125, 2.751953125, 4.45703125, 0.018585205078125, 3.10546875, 4.234375, 0.319580078125, 0.89306640625, 3.01171875, 3.521484375, 3.03515625, 3.005859375, 5.5, 1.5380859375, 0.0055694580078125, 0.002399444580078125, 3.71484375, 4.6953125, 0.0148162841796875, 0.01885986328125, 0.7353515625, 4.63671875, 1.2958984375, 1.6484375, 2.0546875, 2.84765625]], "avg_entropy": [1.7360623603643373], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Live and Let Die (disambiguation)", "score_of_biggest_cluster": 0.0765138790011406, "score_of_first_answer": 0.0765138790011406, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.0728836059570312e-06, "avg_score": 0.0765138790011406, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.425696016394846, "std_of_first_answer": 0.425696016394846, "avg_entropy_of_first_answer": 1.7360623603643373, "lex_sim": 1.0}
{"question": "Plaka is the old quarter of which European city?", "answer": ["Athina"], "scores": [[0.93408203125, 0.0022602081298828125]], "normalized_score": [0.04594800993800163], "tokens": [["\u2581Ath", "ina"]], "entropy": [[0.476318359375, 0.0595703125]], "avg_entropy": [0.2679443359375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Athina", "score_of_biggest_cluster": 0.04594800993800163, "score_of_first_answer": 0.04594800993800163, "max_score_of_first_answer": 0.93408203125, "min_score_of_first_answer": 0.0022602081298828125, "avg_score": 0.04594800993800163, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4659109115600586, "std_of_first_answer": 0.4659109115600586, "avg_entropy_of_first_answer": 0.2679443359375, "lex_sim": 1.0}
{"question": "Which two countries took part in the short Kargil War in 1999?", "answer": ["india-and-pakistan"], "scores": [[0.004711151123046875, 0.97265625, 0.0011777877807617188, 0.4052734375, 0.86962890625, 0.1976318359375, 0.9921875, 0.994140625]], "normalized_score": [0.1570875495672226], "tokens": [["\u2581ind", "ia", "-", "and", "-", "p", "ak", "istan"]], "entropy": [[0.34521484375, 0.199462890625, 0.8681640625, 1.9609375, 0.5791015625, 1.0146484375, 0.0672607421875, 0.048828125]], "avg_entropy": [0.6354522705078125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pakistan-India relation", "score_of_biggest_cluster": 0.1570875495672226, "score_of_first_answer": 0.1570875495672226, "max_score_of_first_answer": 0.994140625, "min_score_of_first_answer": 0.0011777877807617188, "avg_score": 0.1570875495672226, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4208728368680365, "std_of_first_answer": 0.4208728368680365, "avg_entropy_of_first_answer": 0.6354522705078125, "lex_sim": 1.0}
{"question": "How many squares are there in a standard 'Sudoku' grid?", "answer": ["81 #483805 - 03/04/08 10:06 AM Re: Rate the Dress [Re: Shadow]"], "scores": [[0.431640625, 0.8623046875, 0.9541015625, 3.635883331298828e-06, 0.0015192031860351562, 0.03253173828125, 0.0304412841796875, 0.039398193359375, 0.044097900390625, 0.045318603515625, 0.0010547637939453125, 0.31640625, 0.075927734375, 0.028594970703125, 0.7392578125, 0.367431640625, 0.1600341796875, 0.9130859375, 0.032012939453125, 0.09552001953125, 0.59619140625, 0.386962890625, 0.202880859375, 0.994140625, 0.1561279296875, 0.10821533203125, 0.576171875, 0.62841796875, 0.98779296875, 0.001491546630859375, 0.06573486328125, 0.34814453125, 0.00839996337890625, 2.4497509002685547e-05, 0.032318115234375, 0.837890625, 0.9970703125, 0.002117156982421875, 0.3271484375, 0.316162109375]], "normalized_score": [0.07159826904535294], "tokens": [["\u2581", "8", "1", "\u2581#", "4", "8", "3", "8", "0", "5", "\u2581-", "\u2581", "0", "3", "/", "0", "4", "/", "0", "8", "\u2581", "1", "0", ":", "0", "6", "\u2581AM", "\u2581Re", ":", "\u2581R", "ate", "\u2581the", "\u2581D", "ress", "\u2581[", "Re", ":", "\u2581Sh", "adow", "]"]], "entropy": [[1.6806640625, 0.496826171875, 0.31201171875, 0.9697265625, 5.73046875, 3.919921875, 4.12890625, 3.224609375, 3.39453125, 3.24609375, 2.9453125, 4.5234375, 1.85546875, 1.751953125, 0.978515625, 1.37890625, 2.1875, 0.53759765625, 0.73974609375, 2.1796875, 1.7470703125, 1.0859375, 2.15625, 0.0582275390625, 1.8271484375, 2.310546875, 1.9501953125, 1.798828125, 0.11614990234375, 4.94140625, 3.3984375, 3.556640625, 3.47265625, 0.0052032470703125, 1.84375, 1.259765625, 0.0298919677734375, 5.46484375, 3.271484375, 3.93359375]], "avg_entropy": [2.260247802734375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "eighty-one", "score_of_biggest_cluster": 0.07159826904535294, "score_of_first_answer": 0.07159826904535294, "max_score_of_first_answer": 0.9970703125, "min_score_of_first_answer": 3.635883331298828e-06, "avg_score": 0.07159826904535294, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.34454079043406033, "std_of_first_answer": 0.34454079043406033, "avg_entropy_of_first_answer": 2.260247802734375, "lex_sim": 1.0}
{"question": "Bill Clinton is fairly fluent in which language other than English?", "answer": ["Irish"], "scores": [[0.004558563232421875]], "normalized_score": [0.0045585623010993], "tokens": [["\u2581Irish"]], "entropy": [[1.9921875]], "avg_entropy": [1.9921875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "German", "score_of_biggest_cluster": 0.0045585623010993, "score_of_first_answer": 0.0045585623010993, "max_score_of_first_answer": 0.004558563232421875, "min_score_of_first_answer": 0.004558563232421875, "avg_score": 0.0045585623010993, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.9921875, "lex_sim": 1.0}
{"question": "Barnacles , crabs and lobsters belong to which class of marine life ?", "answer": ["Crustacea \\ \u02c8b\u0251\u02d0r-n\u0259k-lz \\"], "scores": [[0.70068359375, 0.9990234375, 0.99462890625, 0.481689453125, 9.47713851928711e-06, 0.0013437271118164062, 0.7392578125, 3.5762786865234375e-07, 0.1512451171875, 0.34765625, 0.0211181640625, 0.022369384765625, 0.80224609375, 0.90283203125, 0.00043010711669921875, 0.7802734375, 3.832578659057617e-05, 0.0029354095458984375, 0.207763671875]], "normalized_score": [0.023937063291668892], "tokens": [["\u2581Cr", "ust", "ace", "a", "\u2581\\", "\u2581", "\u02c8", "b", "\u0251", "\u02d0", "r", "-", "n", "\u0259", "k", "-", "l", "z", "\u2581\\"]], "entropy": [[1.5751953125, 0.01039886474609375, 0.052703857421875, 1.083984375, 0.87890625, 2.052734375, 1.576171875, 1.15234375, 3.53125, 1.4072265625, 1.486328125, 1.662109375, 0.748046875, 0.55810546875, 0.185302734375, 1.017578125, 1.904296875, 2.015625, 1.939453125]], "avg_entropy": [1.3072505750154193], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Crustacen", "score_of_biggest_cluster": 0.023937063291668892, "score_of_first_answer": 0.023937063291668892, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 3.5762786865234375e-07, "avg_score": 0.023937063291668892, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3850755181650715, "std_of_first_answer": 0.3850755181650715, "avg_entropy_of_first_answer": 1.3072505750154193, "lex_sim": 1.0}
{"question": "Which fruit is known in Portuguese as a marmelo, the first marmalades being made from this fruit?", "answer": ["Quince #MondayMotivation: Remember That Failure Is Just Feedback"], "scores": [[0.6201171875, 0.99658203125, 4.76837158203125e-07, 0.00707244873046875, 0.00013971328735351562, 0.99853515625, 0.90283203125, 0.2315673828125, 0.99853515625, 0.9990234375, 0.00434112548828125, 0.005451202392578125, 2.8133392333984375e-05, 0.190185546875, 0.98583984375, 0.5791015625, 0.0226593017578125, 0.039520263671875, 1.0, 0.9990234375]], "normalized_score": [0.04782690852880478], "tokens": [["\u2581Qu", "ince", "\u2581#", "M", "ond", "ay", "M", "ot", "iv", "ation", ":", "\u2581Remember", "\u2581That", "\u2581Fail", "ure", "\u2581Is", "\u2581Just", "\u2581Fe", "ed", "back"]], "entropy": [[1.4326171875, 0.033203125, 0.810546875, 5.98046875, 0.2744140625, 0.01611328125, 0.658203125, 1.4794921875, 0.01372528076171875, 0.0108184814453125, 1.2744140625, 4.58984375, 1.26953125, 3.267578125, 0.08544921875, 0.81298828125, 1.939453125, 1.6796875, 0.0009160041809082031, 0.007617950439453125]], "avg_entropy": [1.2818540811538697], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cydonia oblonga", "score_of_biggest_cluster": 0.04782690852880478, "score_of_first_answer": 0.04782690852880478, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 4.76837158203125e-07, "avg_score": 0.04782690852880478, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4468177427122463, "std_of_first_answer": 0.4468177427122463, "avg_entropy_of_first_answer": 1.2818540811538697, "lex_sim": 1.0}
{"question": "The technical term abecedarian refers to what form of organization?", "answer": ["Alphabetical or being first in order of time"], "scores": [[0.3662109375, 0.96630859375, 0.96875, 0.00666046142578125, 0.0174713134765625, 0.305908203125, 0.73583984375, 0.74462890625, 0.268798828125, 0.06317138671875]], "normalized_score": [0.20207616686820984], "tokens": [["\u2581Al", "phabet", "ical", "\u2581or", "\u2581being", "\u2581first", "\u2581in", "\u2581order", "\u2581of", "\u2581time"]], "entropy": [[2.724609375, 0.17822265625, 0.191650390625, 0.98876953125, 3.05078125, 1.40234375, 0.84326171875, 0.8876953125, 1.4560546875, 3.154296875]], "avg_entropy": [1.4877685546875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "List of letters in the english language", "score_of_biggest_cluster": 0.20207616686820984, "score_of_first_answer": 0.20207616686820984, "max_score_of_first_answer": 0.96875, "min_score_of_first_answer": 0.00666046142578125, "avg_score": 0.20207616686820984, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3602436143124514, "std_of_first_answer": 0.3602436143124514, "avg_entropy_of_first_answer": 1.4877685546875, "lex_sim": 1.0}
{"question": "Who was married to Whitney Houston from 1992 to 2007?", "answer": ["Bobby Brown ###What is \"Gigantomachy\"?"], "scores": [[0.97265625, 0.99365234375, 0.0, 3.0994415283203125e-06, 0.280517578125, 0.003246307373046875, 0.008453369140625, 0.0024509429931640625, 0.005359649658203125, 0.8046875, 0.802734375, 0.1357421875]], "normalized_score": [0.0], "tokens": [["\u2581Bobby", "\u2581Brown", "\u2581###", "What", "\u2581is", "\u2581\"", "G", "ig", "antom", "ach", "y", "\"?"]], "entropy": [[0.1551513671875, 0.061065673828125, 0.6875, 2.59375, 3.556640625, 2.1015625, 5.4765625, 5.34765625, 2.650390625, 0.6533203125, 0.49951171875, 0.78662109375]], "avg_entropy": [2.0474777221679688], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bobby brown", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.99365234375, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.40649577685534, "std_of_first_answer": 0.40649577685534, "avg_entropy_of_first_answer": 2.0474777221679688, "lex_sim": 1.0}
{"question": "In which Asian city is the 830 metres high Burj Khalifa?", "answer": ["Dubai (\u062f\u0628\u064a\u0651) Difference between revisions of \"User talk:ASDamick\""], "scores": [[0.95751953125, 0.99658203125, 0.00301361083984375, 0.0008091926574707031, 0.98291015625, 0.96826171875, 0.0645751953125, 0.41259765625, 2.384185791015625e-06, 1.6927719116210938e-05, 0.07147216796875, 0.000682830810546875, 1.0, 0.7529296875, 0.97119140625, 0.05999755859375, 0.5654296875, 0.98828125, 0.0061492919921875, 0.018798828125, 0.9267578125, 0.962890625, 0.18017578125]], "normalized_score": [0.058966588228940964], "tokens": [["\u2581Dub", "ai", "\u2581(", "\u062f", "\u0628", "\u064a", "\u0651", ")", "\u2581D", "ifference", "\u2581between", "\u2581revis", "ions", "\u2581of", "\u2581\"", "User", "\u2581talk", ":", "AS", "D", "am", "ick", "\""]], "entropy": [[0.363525390625, 0.029510498046875, 1.0224609375, 1.2958984375, 0.11138916015625, 0.1737060546875, 1.2197265625, 1.18359375, 1.33984375, 2.326171875, 2.869140625, 1.7802734375, 0.0005030632019042969, 1.45703125, 0.23681640625, 2.205078125, 0.9501953125, 0.091064453125, 5.08203125, 3.986328125, 0.47119140625, 0.33251953125, 0.79931640625]], "avg_entropy": [1.2751006872757622], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dubai (city)", "score_of_biggest_cluster": 0.058966588228940964, "score_of_first_answer": 0.058966588228940964, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.384185791015625e-06, "avg_score": 0.058966588228940964, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4406042610976778, "std_of_first_answer": 0.4406042610976778, "avg_entropy_of_first_answer": 1.2751006872757622, "lex_sim": 1.0}
{"question": "Michelangelo Merisi (or Amerighi) is the birth name of which artist?", "answer": ["Michelangelo #Budget2017-18: What Real Estate Sector Expects?"], "scores": [[0.28759765625, 0.99951171875, 0.99951171875, 2.384185791015625e-07, 0.00539398193359375, 0.00028133392333984375, 0.078857421875, 0.107421875, 0.99755859375, 0.0104217529296875, 0.039520263671875, 0.11138916015625, 0.477294921875, 0.9365234375, 0.02227783203125, 0.0027637481689453125, 8.940696716308594e-06, 0.5673828125, 0.9990234375, 0.287353515625, 0.8662109375, 0.258056640625, 0.99462890625, 0.9755859375, 0.205078125]], "normalized_score": [0.056787993758916855], "tokens": [["\u2581Michel", "ang", "elo", "\u2581#", "B", "ud", "get", "2", "0", "1", "7", "-", "1", "8", ":", "\u2581What", "\u2581Real", "\u2581Est", "ate", "\u2581S", "ector", "\u2581Ex", "pect", "s", "?"]], "entropy": [[3.43359375, 0.00563812255859375, 0.00322723388671875, 0.66552734375, 4.76953125, 2.966796875, 2.67578125, 3.4296875, 0.0252685546875, 0.11322021484375, 2.41796875, 2.25, 1.3671875, 0.32421875, 2.484375, 4.6875, 3.078125, 2.109375, 0.0090789794921875, 3.1015625, 0.45947265625, 2.248046875, 0.034637451171875, 0.1649169921875, 1.408203125]], "avg_entropy": [1.769317626953125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Morigi, Michaelangelo (Caravaggio)", "score_of_biggest_cluster": 0.056787993758916855, "score_of_first_answer": 0.056787993758916855, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 2.384185791015625e-07, "avg_score": 0.056787993758916855, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4113331672871527, "std_of_first_answer": 0.4113331672871527, "avg_entropy_of_first_answer": 1.769317626953125, "lex_sim": 1.0}
{"question": "Which national team beat Hungary 4-2 to win the 1938 FIFA World Cup?", "answer": ["It"], "scores": [[0.00991058349609375]], "normalized_score": [0.009910584427416325], "tokens": [["\u2581It"]], "entropy": [[1.7578125]], "avg_entropy": [1.7578125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Environment of Italy", "score_of_biggest_cluster": 0.009910584427416325, "score_of_first_answer": 0.009910584427416325, "max_score_of_first_answer": 0.00991058349609375, "min_score_of_first_answer": 0.00991058349609375, "avg_score": 0.009910584427416325, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.7578125, "lex_sim": 1.0}
{"question": "Which car company produces a model called Eos?", "answer": ["Volks \u00cdndice"], "scores": [[0.96923828125, 0.0, 0.0009479522705078125, 0.084228515625]], "normalized_score": [0.0], "tokens": [["\u2581Volks", "\u2581\u00cd", "nd", "ice"]], "entropy": [[0.163330078125, 0.05908203125, 4.09375, 1.48828125]], "avg_entropy": [1.45111083984375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Folksvagon", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.96923828125, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4088308242703992, "std_of_first_answer": 0.4088308242703992, "avg_entropy_of_first_answer": 1.45111083984375, "lex_sim": 1.0}
{"question": "On a standard dartboard, what number lies opposite 18?", "answer": ["20 #WPLongform, Food for Thought, Humor, Post A Day 2013, This is Life, tvaraj"], "scores": [[0.8681640625, 0.213623046875, 0.61767578125, 3.5762786865234375e-06, 0.00027179718017578125, 0.0005984306335449219, 0.0201873779296875, 0.9912109375, 0.005504608154296875, 6.401538848876953e-05, 0.029266357421875, 0.66748046875, 0.99951171875, 0.748046875, 0.002696990966796875, 0.345458984375, 0.8369140625, 0.0195159912109375, 0.58544921875, 0.98193359375, 0.89013671875, 1.0, 1.0, 0.99267578125, 0.20849609375, 0.96923828125, 0.409912109375, 0.9990234375, 0.9794921875, 0.317626953125, 0.363037109375, 0.0215911865234375, 0.99951171875]], "normalized_score": [0.10744680464267731], "tokens": [["\u2581", "2", "0", "\u2581#", "W", "PL", "ong", "form", ",", "\u2581Food", "\u2581for", "\u2581Th", "ought", ",", "\u2581Hum", "or", ",", "\u2581Post", "\u2581A", "\u2581Day", "\u2581", "2", "0", "1", "3", ",", "\u2581This", "\u2581is", "\u2581Life", ",", "\u2581t", "var", "aj"]], "entropy": [[0.935546875, 1.294921875, 0.9697265625, 0.66845703125, 5.9921875, 4.58984375, 3.119140625, 0.058074951171875, 1.646484375, 5.5625, 2.908203125, 1.314453125, 0.0040283203125, 1.0712890625, 5.0703125, 1.173828125, 0.9228515625, 4.69921875, 1.1513671875, 0.10076904296875, 0.37060546875, 0.0005178451538085938, 0.001041412353515625, 0.044158935546875, 0.7353515625, 0.1607666015625, 2.298828125, 0.009124755859375, 0.1705322265625, 0.7392578125, 1.37109375, 0.11474609375, 0.003643035888671875]], "avg_entropy": [1.493117332458496], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "seven", "score_of_biggest_cluster": 0.10744680464267731, "score_of_first_answer": 0.10744680464267731, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 3.5762786865234375e-06, "avg_score": 0.10744680464267731, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4108202478790633, "std_of_first_answer": 0.4108202478790633, "avg_entropy_of_first_answer": 1.493117332458496, "lex_sim": 1.0}
{"question": "\"In a famous speech by Winston Churchill on 4 June 1940 that began \"\"We shall fight in France, we shall fight on the seas and oceans\"\", he went on to say that \"\"we shall fight\"\" in the air, on the beaches, on the landing grounds, in the fields, in the streets, and where else?\"", "answer": ["I don't know"], "scores": [[0.002719879150390625, 0.01351165771484375, 0.96533203125, 0.99951171875, 0.79833984375]], "normalized_score": [0.12313519418239594], "tokens": [["\u2581I", "\u2581don", "'", "t", "\u2581know"]], "entropy": [[3.275390625, 3.119140625, 0.228515625, 0.005161285400390625, 0.98583984375]], "avg_entropy": [1.5228096008300782], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "In the hills", "score_of_biggest_cluster": 0.12313519418239594, "score_of_first_answer": 0.12313519418239594, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.002719879150390625, "avg_score": 0.12313519418239594, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45241493820890144, "std_of_first_answer": 0.45241493820890144, "avg_entropy_of_first_answer": 1.5228096008300782, "lex_sim": 1.0}
{"question": "Who captained the England rugby union team 59 times between 1988 and 1996?", "answer": ["Will Carling \\ Fishing \\ Fishing \\ Basic Techniques \\ Setting the Hook"], "scores": [[0.10626220703125, 0.990234375, 0.99951171875, 8.940696716308594e-07, 1.6748905181884766e-05, 0.0279083251953125, 0.045501708984375, 0.1549072265625, 0.62841796875, 0.2083740234375, 1.5020370483398438e-05, 0.00018703937530517578, 0.28857421875, 0.939453125, 0.56201171875, 0.0020389556884765625, 0.08673095703125, 0.8349609375]], "normalized_score": [0.022701352834701538], "tokens": [["\u2581Will", "\u2581Car", "ling", "\u2581\\", "\u2581Fish", "ing", "\u2581\\", "\u2581Fish", "ing", "\u2581\\", "\u2581Basic", "\u2581Te", "chni", "ques", "\u2581\\", "\u2581Setting", "\u2581the", "\u2581Hook"]], "entropy": [[2.806640625, 0.0771484375, 0.00299072265625, 0.67919921875, 2.291015625, 3.083984375, 3.962890625, 5.79296875, 2.20703125, 4.38671875, 2.4375, 4.1015625, 2.181640625, 0.35400390625, 1.94921875, 3.52734375, 2.623046875, 0.80859375]], "avg_entropy": [2.404083251953125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "William David Charles Carling", "score_of_biggest_cluster": 0.022701352834701538, "score_of_first_answer": 0.022701352834701538, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 8.940696716308594e-07, "avg_score": 0.022701352834701538, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.37452487087436026, "std_of_first_answer": 0.37452487087436026, "avg_entropy_of_first_answer": 2.404083251953125, "lex_sim": 1.0}
{"question": "January 23 saw the anniversary of the introduction of the Apple Macintosh. In what year did this event happen?", "answer": ["The Apple Macintosh was introduced on January 24, 1984."], "scores": [[0.005420684814453125, 0.242919921875, 0.9990234375, 0.99951171875, 1.0, 0.97216796875, 0.7294921875, 0.86962890625, 0.998046875, 0.9990234375, 1.0, 0.98876953125, 0.96044921875, 0.982421875, 1.0, 1.0, 0.99951171875, 1.0, 0.65869140625, 9.5367431640625e-07, 0.006313323974609375, 2.962350845336914e-05, 5.745887756347656e-05, 0.93017578125, 0.00036025047302246094, 0.80712890625, 6.973743438720703e-06, 0.0204925537109375, 0.0021800994873046875, 0.00223541259765625, 0.206787109375, 0.6708984375, 0.9609375, 0.95556640625, 1.0]], "normalized_score": [0.088173508644104], "tokens": [["\u2581The", "\u2581Apple", "\u2581Mac", "int", "osh", "\u2581was", "\u2581introduced", "\u2581on", "\u2581January", "\u2581", "2", "4", ",", "\u2581", "1", "9", "8", "4", ".", "\u2581#", "F", "arm", "Bo", "y", "Di", "aries", "\u2581::", "\u2581The", "\u2581First", "\u2581C", "ut", "\u2581is", "\u2581the", "\u2581Deep", "est"]], "entropy": [[2.302734375, 2.404296875, 0.00878143310546875, 0.005413055419921875, 0.0017871856689453125, 0.1767578125, 0.62744140625, 0.414306640625, 0.0167694091796875, 0.007678985595703125, 0.0010232925415039062, 0.063720703125, 0.191162109375, 0.11334228515625, 0.001102447509765625, 0.0003948211669921875, 0.0029315948486328125, 0.0017213821411132812, 1.0634765625, 1.640625, 4.0234375, 2.150390625, 2.033203125, 0.37255859375, 4.0078125, 0.5595703125, 1.8115234375, 5.62890625, 6.3359375, 3.87890625, 2.896484375, 1.33984375, 0.248291015625, 0.265625, 0.001384735107421875]], "avg_entropy": [1.2742669241768974], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1984", "score_of_biggest_cluster": 0.088173508644104, "score_of_first_answer": 0.088173508644104, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 9.5367431640625e-07, "avg_score": 0.088173508644104, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4355554990800815, "std_of_first_answer": 0.4355554990800815, "avg_entropy_of_first_answer": 1.2742669241768974, "lex_sim": 1.0}
{"question": "What is the name of the coronation anthem composed by Handel using texts from the King James Bible that has been sung at every British coronation service since 1727?", "answer": ["Zadok the priest"], "scores": [[0.8017578125, 0.99951171875, 0.99658203125, 0.998046875, 0.04205322265625]], "normalized_score": [0.5070594549179077], "tokens": [["\u2581Z", "ad", "ok", "\u2581the", "\u2581priest"]], "entropy": [[0.57763671875, 0.004695892333984375, 0.02532958984375, 0.0170745849609375, 0.18017578125]], "avg_entropy": [0.16098251342773437], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Zadok The Priest", "score_of_biggest_cluster": 0.5070594549179077, "score_of_first_answer": 0.5070594549179077, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.04205322265625, "avg_score": 0.5070594549179077, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.37064982393832563, "std_of_first_answer": 0.37064982393832563, "avg_entropy_of_first_answer": 0.16098251342773437, "lex_sim": 1.0}
{"question": "Which detective was created by G K Chesterton?", "answer": ["Father Brown (detective)"], "scores": [[0.953125, 0.99560546875, 0.0014333724975585938, 0.03668212890625, 0.99951171875, 1.0, 0.935546875]], "normalized_score": [0.24058693647384644], "tokens": [["\u2581Father", "\u2581Brown", "\u2581(", "det", "ect", "ive", ")"]], "entropy": [[0.384033203125, 0.0430908203125, 0.6923828125, 3.60546875, 0.005397796630859375, 0.0012798309326171875, 0.429931640625]], "avg_entropy": [0.7373692648751395], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Father Brown", "score_of_biggest_cluster": 0.24058693647384644, "score_of_first_answer": 0.24058693647384644, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0014333724975585938, "avg_score": 0.24058693647384644, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.43335268475144095, "std_of_first_answer": 0.43335268475144095, "avg_entropy_of_first_answer": 0.7373692648751395, "lex_sim": 1.0}
{"question": "Who founded the TV production company 'David Paradine Productions'?", "answer": ["Alistair Cooke"], "scores": [[0.006732940673828125, 0.66357421875, 1.0, 0.880859375, 0.99853515625]], "normalized_score": [0.3302728235721588], "tokens": [["\u2581Al", "ist", "air", "\u2581Co", "oke"]], "entropy": [[2.509765625, 1.6318359375, 0.0009946823120117188, 0.68603515625, 0.0149078369140625]], "avg_entropy": [0.9687078475952149], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "DAVID FROST", "score_of_biggest_cluster": 0.3302728235721588, "score_of_first_answer": 0.3302728235721588, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.006732940673828125, "avg_score": 0.3302728235721588, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3723668498988238, "std_of_first_answer": 0.3723668498988238, "avg_entropy_of_first_answer": 0.9687078475952149, "lex_sim": 1.0}
{"question": "What is the state capital of Ohio", "answer": ["Capital of Ohio"], "scores": [[0.0012645721435546875, 0.4677734375, 0.9833984375]], "normalized_score": [0.08347751945257187], "tokens": [["\u2581Capital", "\u2581of", "\u2581Ohio"]], "entropy": [[0.24560546875, 1.857421875, 0.1258544921875]], "avg_entropy": [0.7429606119791666], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Colombus", "score_of_biggest_cluster": 0.08347751945257187, "score_of_first_answer": 0.08347751945257187, "max_score_of_first_answer": 0.9833984375, "min_score_of_first_answer": 0.0012645721435546875, "avg_score": 0.08347751945257187, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40112156533083726, "std_of_first_answer": 0.40112156533083726, "avg_entropy_of_first_answer": 0.7429606119791666, "lex_sim": 1.0}
{"question": "Which famous painter's surname was Harmenzoon van Rijn which translated as son of Harmen, of the Rhine?", "answer": ["Rembrandt Proof Positive: How to Show that a Statement is True"], "scores": [[0.9423828125, 0.9921875, 0.9990234375, 0.0, 2.8908252716064453e-05, 0.75048828125, 0.01447296142578125, 0.00038170814514160156, 0.1676025390625, 0.0017900466918945312, 0.00884246826171875, 0.14111328125, 0.11517333984375, 0.99853515625, 0.884765625, 0.81884765625]], "normalized_score": [0.0], "tokens": [["\u2581Rem", "brand", "t", "\u2581Proof", "\u2581Pos", "itive", ":", "\u2581How", "\u2581to", "\u2581Show", "\u2581that", "\u2581a", "\u2581Stat", "ement", "\u2581is", "\u2581True"]], "entropy": [[0.36279296875, 0.0469970703125, 0.0110015869140625, 1.5185546875, 4.515625, 1.6796875, 3.505859375, 4.765625, 4.828125, 5.453125, 2.40234375, 4.62109375, 4.65234375, 0.01251983642578125, 0.58935546875, 0.88427734375]], "avg_entropy": [2.4905829429626465], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rembrandt", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4348409037121906, "std_of_first_answer": 0.4348409037121906, "avg_entropy_of_first_answer": 2.4905829429626465, "lex_sim": 1.0}
{"question": "In the US, where would a busboy work?", "answer": ["In a restaurant"], "scores": [[0.58349609375, 0.986328125, 0.9833984375]], "normalized_score": [0.8271728754043579], "tokens": [["\u2581In", "\u2581a", "\u2581restaurant"]], "entropy": [[1.1220703125, 0.087890625, 0.10113525390625]], "avg_entropy": [0.4370320638020833], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eateries", "score_of_biggest_cluster": 0.8271728754043579, "score_of_first_answer": 0.8271728754043579, "max_score_of_first_answer": 0.986328125, "min_score_of_first_answer": 0.58349609375, "avg_score": 0.8271728754043579, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.18921008694235206, "std_of_first_answer": 0.18921008694235206, "avg_entropy_of_first_answer": 0.4370320638020833, "lex_sim": 1.0}
{"question": "How many verses make up the national anthem God Save The Queen?", "answer": ["Three"], "scores": [[0.1251220703125]], "normalized_score": [0.1251220703125], "tokens": [["\u2581Three"]], "entropy": [[2.375]], "avg_entropy": [2.375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "3", "score_of_biggest_cluster": 0.1251220703125, "score_of_first_answer": 0.1251220703125, "max_score_of_first_answer": 0.1251220703125, "min_score_of_first_answer": 0.1251220703125, "avg_score": 0.1251220703125, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.375, "lex_sim": 1.0}
{"question": "Which group released an album in 1986 entitled \u2018The Queen is Dead\u2019?", "answer": ["Smiths"], "scores": [[0.0019931793212890625, 0.97314453125]], "normalized_score": [0.044041477143764496], "tokens": [["\u2581Smith", "s"]], "entropy": [[0.1776123046875, 0.1636962890625]], "avg_entropy": [0.170654296875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dale Hibbert", "score_of_biggest_cluster": 0.044041477143764496, "score_of_first_answer": 0.044041477143764496, "max_score_of_first_answer": 0.97314453125, "min_score_of_first_answer": 0.0019931793212890625, "avg_score": 0.044041477143764496, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.48557567596435547, "std_of_first_answer": 0.48557567596435547, "avg_entropy_of_first_answer": 0.170654296875, "lex_sim": 1.0}
{"question": "What was the pre-independence name of Malawi?", "answer": ["Nyasaland"], "scores": [[0.98095703125, 0.99462890625, 1.0]], "normalized_score": [0.991829514503479], "tokens": [["\u2581Ny", "as", "aland"]], "entropy": [[0.122802734375, 0.040679931640625, 0.0014553070068359375]], "avg_entropy": [0.05497932434082031], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "NYASALAND", "score_of_biggest_cluster": 0.991829514503479, "score_of_first_answer": 0.991829514503479, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.98095703125, "avg_score": 0.991829514503479, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.008016673408870282, "std_of_first_answer": 0.008016673408870282, "avg_entropy_of_first_answer": 0.05497932434082031, "lex_sim": 1.0}
{"question": "What instrument was originally called the trompette-saicueboute?", "answer": ["Trumpet"], "scores": [[0.393798828125, 0.998046875]], "normalized_score": [0.6269208192825317], "tokens": [["\u2581Trump", "et"]], "entropy": [[2.37109375, 0.019500732421875]], "avg_entropy": [1.1952972412109375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Harmonic glissando", "score_of_biggest_cluster": 0.6269208192825317, "score_of_first_answer": 0.6269208192825317, "max_score_of_first_answer": 0.998046875, "min_score_of_first_answer": 0.393798828125, "avg_score": 0.6269208192825317, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3021240234375, "std_of_first_answer": 0.3021240234375, "avg_entropy_of_first_answer": 1.1952972412109375, "lex_sim": 1.0}
{"question": "Which \u2018A\u2019 means in the open air?", "answer": ["Acarology ## Introduction"], "scores": [[0.042572021484375, 0.0031490325927734375, 0.0190582275390625, 1.1920928955078125e-07, 2.6404857635498047e-05]], "normalized_score": [0.00038113229675218463], "tokens": [["\u2581A", "car", "ology", "\u2581##", "\u2581Introduction"]], "entropy": [[3.869140625, 4.0234375, 3.26953125, 1.515625, 5.0]], "avg_entropy": [3.535546875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Al-fresco", "score_of_biggest_cluster": 0.00038113229675218463, "score_of_first_answer": 0.00038113229675218463, "max_score_of_first_answer": 0.042572021484375, "min_score_of_first_answer": 1.1920928955078125e-07, "avg_score": 0.00038113229675218463, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01640458267466599, "std_of_first_answer": 0.01640458267466599, "avg_entropy_of_first_answer": 3.535546875, "lex_sim": 1.0}
{"question": "Which celebrated battle was fought near Brno on 2nd December 1805?", "answer": ["Battle of Austerlitz"], "scores": [[0.137939453125, 0.9951171875, 0.96923828125, 1.0, 1.0, 0.99951171875]], "normalized_score": [0.7144362926483154], "tokens": [["\u2581Battle", "\u2581of", "\u2581Aust", "er", "l", "itz"]], "entropy": [[1.373046875, 0.0396728515625, 0.216552734375, 3.409385681152344e-05, 0.0012559890747070312, 0.002674102783203125]], "avg_entropy": [0.2722061077753703], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Austerlitz (disambiguation)", "score_of_biggest_cluster": 0.7144362926483154, "score_of_first_answer": 0.7144362926483154, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.137939453125, "avg_score": 0.7144362926483154, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3187632868580128, "std_of_first_answer": 0.3187632868580128, "avg_entropy_of_first_answer": 0.2722061077753703, "lex_sim": 1.0}
{"question": "Which train is the holder of the world speed record for steam locomotives?", "answer": ["35644"], "scores": [[0.037139892578125, 0.3173828125, 0.92138671875, 0.00763702392578125, 0.0180816650390625, 0.951171875]], "normalized_score": [0.10609980672597885], "tokens": [["\u2581", "3", "5", "6", "4", "4"]], "entropy": [[1.4443359375, 1.5810546875, 0.4755859375, 3.357421875, 3.31640625, 0.2744140625]], "avg_entropy": [1.7415364583333333], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mallard Duck", "score_of_biggest_cluster": 0.10609980672597885, "score_of_first_answer": 0.10609980672597885, "max_score_of_first_answer": 0.951171875, "min_score_of_first_answer": 0.00763702392578125, "avg_score": 0.10609980672597885, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4103507881102175, "std_of_first_answer": 0.4103507881102175, "avg_entropy_of_first_answer": 1.7415364583333333, "lex_sim": 1.0}
{"question": "Kurt Russell is the long term partner of which actress?", "answer": ["Goldie Hawn"], "scores": [[0.9814453125, 1.0, 0.9951171875, 1.0]], "normalized_score": [0.9941114783287048], "tokens": [["\u2581Gold", "ie", "\u2581Ha", "wn"]], "entropy": [[0.1571044921875, 0.0013170242309570312, 0.039764404296875, 0.000492095947265625]], "avg_entropy": [0.049669504165649414], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Goldie Hahn", "score_of_biggest_cluster": 0.9941114783287048, "score_of_first_answer": 0.9941114783287048, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.9814453125, "avg_score": 0.9941114783287048, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.007595873626027366, "std_of_first_answer": 0.007595873626027366, "avg_entropy_of_first_answer": 0.049669504165649414, "lex_sim": 1.0}
{"question": "The town of Keswick in Cumbria lies just north of which body of water?", "answer": ["Derwent Water"], "scores": [[0.89794921875, 0.99853515625, 0.9990234375, 0.11578369140625]], "normalized_score": [0.5674917101860046], "tokens": [["\u2581Der", "w", "ent", "\u2581Water"]], "entropy": [[0.69482421875, 0.01580810546875, 0.009674072265625, 0.5322265625]], "avg_entropy": [0.31313323974609375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Derwentwater", "score_of_biggest_cluster": 0.5674917101860046, "score_of_first_answer": 0.5674917101860046, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.11578369140625, "avg_score": 0.5674917101860046, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3700911439147221, "std_of_first_answer": 0.3700911439147221, "avg_entropy_of_first_answer": 0.31313323974609375, "lex_sim": 1.0}
{"question": "Fiji Day is celebrated in Fiji during which month of the year?", "answer": ["second month #WomenBoycottTwitter Day Two: A Recap"], "scores": [[0.0034313201904296875, 0.0482177734375, 1.3709068298339844e-06, 0.00021183490753173828, 0.0015974044799804688, 0.0006995201110839844, 0.76416015625, 0.99560546875, 0.75634765625, 1.0, 0.0160675048828125, 4.214048385620117e-05, 0.032989501953125, 0.009613037109375, 0.0003151893615722656, 0.99951171875]], "normalized_score": [0.009067919105291367], "tokens": [["\u2581second", "\u2581month", "\u2581#", "W", "omen", "Bo", "yc", "ott", "Tw", "itter", "\u2581Day", "\u2581Two", ":", "\u2581A", "\u2581Rec", "ap"]], "entropy": [[1.0361328125, 1.5029296875, 1.0068359375, 5.28125, 4.5390625, 3.01171875, 1.0859375, 0.0311431884765625, 1.3583984375, 0.002300262451171875, 3.37890625, 2.224609375, 3.40234375, 5.0546875, 4.140625, 0.0082855224609375]], "avg_entropy": [2.316572904586792], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ocotber", "score_of_biggest_cluster": 0.009067919105291367, "score_of_first_answer": 0.009067919105291367, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.3709068298339844e-06, "avg_score": 0.009067919105291367, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4191447164630858, "std_of_first_answer": 0.4191447164630858, "avg_entropy_of_first_answer": 2.316572904586792, "lex_sim": 1.0}
{"question": "Which British prime Minister wrote two novels 'Coningsby' and 'Sybil' in the 1840s?", "answer": ["Benjamin Disraeli"], "scores": [[0.896484375, 0.99609375, 0.9951171875, 1.0]], "normalized_score": [0.9709107279777527], "tokens": [["\u2581Benjamin", "\u2581Dis", "ra", "eli"]], "entropy": [[0.6640625, 0.036834716796875, 0.0364990234375, 0.0010519027709960938]], "avg_entropy": [0.18461203575134277], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Benjamin disreali", "score_of_biggest_cluster": 0.9709107279777527, "score_of_first_answer": 0.9709107279777527, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.896484375, "avg_score": 0.9709107279777527, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.04359328949934588, "std_of_first_answer": 0.04359328949934588, "avg_entropy_of_first_answer": 0.18461203575134277, "lex_sim": 1.0}
{"question": "Which Chilean socialist leader was deposed and killed by General Pinochet during a U.S. backed coup in 1973?", "answer": ["Salvador Allende"], "scores": [[0.341552734375, 0.99169921875, 0.9990234375]], "normalized_score": [0.6968475580215454], "tokens": [["\u2581Salvador", "\u2581All", "ende"]], "entropy": [[1.3369140625, 0.074462890625, 0.00959014892578125]], "avg_entropy": [0.47365570068359375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Allende Gossens", "score_of_biggest_cluster": 0.6968475580215454, "score_of_first_answer": 0.6968475580215454, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.341552734375, "avg_score": 0.6968475580215454, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3082228307920445, "std_of_first_answer": 0.3082228307920445, "avg_entropy_of_first_answer": 0.47365570068359375, "lex_sim": 1.0}
{"question": "Which fashion designer introduced the Polo label in 1967?", "answer": ["Ralph Lauren #SayHerName: Remembering Janessa Goldbeck"], "scores": [[0.884765625, 0.97509765625, 1.0, 1.3113021850585938e-06, 0.001087188720703125, 0.004718780517578125, 0.00028133392333984375, 0.99072265625, 0.0032520294189453125, 0.004150390625, 0.95751953125, 0.0033779144287109375, 0.0005478858947753906, 0.353759765625, 0.95263671875]], "normalized_score": [0.01880832016468048], "tokens": [["\u2581Ralph", "\u2581La", "uren", "\u2581#", "S", "ay", "Her", "Name", ":", "\u2581Remember", "ing", "\u2581Jan", "essa", "\u2581Gold", "beck"]], "entropy": [[0.81884765625, 0.1922607421875, 0.0019207000732421875, 0.470703125, 3.330078125, 5.03125, 2.349609375, 0.087646484375, 1.8193359375, 5.5234375, 0.34033203125, 4.171875, 1.2705078125, 1.6767578125, 0.358642578125]], "avg_entropy": [1.8295469919840495], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ralf Lauren", "score_of_biggest_cluster": 0.01880832016468048, "score_of_first_answer": 0.01880832016468048, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.3113021850585938e-06, "avg_score": 0.01880832016468048, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4588413823892504, "std_of_first_answer": 0.4588413823892504, "avg_entropy_of_first_answer": 1.8295469919840495, "lex_sim": 1.0}
{"question": "Who was the first professional cricketer to captain England?", "answer": ["Lord Hawke"], "scores": [[0.046661376953125, 0.392333984375, 0.99853515625]], "normalized_score": [0.2634262144565582], "tokens": [["\u2581Lord", "\u2581Haw", "ke"]], "entropy": [[3.51953125, 1.5048828125, 0.0139923095703125]], "avg_entropy": [1.6794687906901042], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sir leonard hutton", "score_of_biggest_cluster": 0.2634262144565582, "score_of_first_answer": 0.2634262144565582, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.046661376953125, "avg_score": 0.2634262144565582, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39342274464236404, "std_of_first_answer": 0.39342274464236404, "avg_entropy_of_first_answer": 1.6794687906901042, "lex_sim": 1.0}
{"question": "\"Where would you find a \"\"plimsoll line\"\"?\"", "answer": ["On a ship #876: What would the impact be on the world if people stopped believing in the concept of money?"], "scores": [[0.861328125, 0.6298828125, 0.90185546875, 5.960464477539063e-08, 0.01155853271484375, 0.01007843017578125, 0.005840301513671875, 1.0669231414794922e-05, 0.0001341104507446289, 0.1795654296875, 0.0032634735107421875, 3.457069396972656e-05, 0.391845703125, 0.06903076171875, 0.50634765625, 0.1644287109375, 0.6396484375, 0.003993988037109375, 0.4833984375, 0.0079803466796875, 0.9970703125, 0.9755859375, 0.140625, 0.78857421875, 0.99755859375, 0.2359619140625, 0.81298828125]], "normalized_score": [0.03420611843466759], "tokens": [["\u2581On", "\u2581a", "\u2581ship", "\u2581#", "8", "7", "6", ":", "\u2581What", "\u2581would", "\u2581the", "\u2581impact", "\u2581be", "\u2581on", "\u2581the", "\u2581world", "\u2581if", "\u2581people", "\u2581stopped", "\u2581bel", "ieving", "\u2581in", "\u2581the", "\u2581concept", "\u2581of", "\u2581money", "?"]], "entropy": [[0.56005859375, 0.7333984375, 0.4072265625, 1.576171875, 4.83203125, 0.278564453125, 0.87939453125, 1.091796875, 5.24609375, 2.201171875, 0.367919921875, 4.99609375, 1.166015625, 1.166015625, 2.826171875, 4.21875, 1.62890625, 3.759765625, 2.583984375, 3.27734375, 0.0252838134765625, 0.198486328125, 4.03125, 1.3046875, 0.026397705078125, 3.638671875, 0.84619140625]], "avg_entropy": [1.9951053195529513], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "On a ship", "score_of_biggest_cluster": 0.03420611843466759, "score_of_first_answer": 0.03420611843466759, "max_score_of_first_answer": 0.99755859375, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.03420611843466759, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3750501335746797, "std_of_first_answer": 0.3750501335746797, "avg_entropy_of_first_answer": 1.9951053195529513, "lex_sim": 1.0}
{"question": "What is secreted by the pancreas to regulate blood sugar levels?", "answer": ["Insulin"], "scores": [[0.8291015625, 0.99072265625, 0.9931640625]], "normalized_score": [0.9343873262405396], "tokens": [["\u2581Ins", "ul", "in"]], "entropy": [[0.65869140625, 0.07867431640625, 0.06072998046875]], "avg_entropy": [0.2660319010416667], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Insulin antagonists", "score_of_biggest_cluster": 0.9343873262405396, "score_of_first_answer": 0.9343873262405396, "max_score_of_first_answer": 0.9931640625, "min_score_of_first_answer": 0.8291015625, "avg_score": 0.9343873262405396, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0767708294686015, "std_of_first_answer": 0.0767708294686015, "avg_entropy_of_first_answer": 0.2660319010416667, "lex_sim": 1.0}
{"question": "In Harry Potter And The Goblet Of Fire, which dragon did Harry have to defeat?", "answer": ["The Hungarian Horntail ## What is it?"], "scores": [[0.072021484375, 0.93505859375, 0.99951171875, 0.9990234375, 1.0, 0.99853515625, 5.960464477539063e-08, 0.0001131892204284668, 0.419189453125, 0.0038318634033203125, 0.360595703125]], "normalized_score": [0.03833021596074104], "tokens": [["\u2581The", "\u2581Hung", "arian", "\u2581Hor", "nt", "ail", "\u2581##", "\u2581What", "\u2581is", "\u2581it", "?"]], "entropy": [[3.41015625, 0.50390625, 0.002964019775390625, 0.0105133056640625, 0.0012073516845703125, 0.01071929931640625, 0.9150390625, 5.296875, 3.4296875, 2.0, 3.314453125]], "avg_entropy": [1.7177746512673118], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ronan (Harry Potter)", "score_of_biggest_cluster": 0.03833021596074104, "score_of_first_answer": 0.03833021596074104, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.03833021596074104, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44045581871290546, "std_of_first_answer": 0.44045581871290546, "avg_entropy_of_first_answer": 1.7177746512673118, "lex_sim": 1.0}
{"question": "Who is the Vice President, and High Representative of the European Union for Foreign Affairs and Security Policy?", "answer": ["Josep Borrell"], "scores": [[0.92138671875, 0.9990234375, 0.99951171875, 0.99609375]], "normalized_score": [0.9784225225448608], "tokens": [["\u2581Jose", "p", "\u2581Bor", "rell"]], "entropy": [[0.460205078125, 0.010589599609375, 0.00701141357421875, 0.026519775390625]], "avg_entropy": [0.1260814666748047], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kathy Ashton", "score_of_biggest_cluster": 0.9784225225448608, "score_of_first_answer": 0.9784225225448608, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.92138671875, "avg_score": 0.9784225225448608, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.033290971222744666, "std_of_first_answer": 0.033290971222744666, "avg_entropy_of_first_answer": 0.1260814666748047, "lex_sim": 1.0}
{"question": "What do we call the selfless and virtuous practice of concern for the welfare of others?", "answer": ["Altruism"], "scores": [[0.875, 0.9990234375, 0.9951171875, 0.9931640625]], "normalized_score": [0.9640948176383972], "tokens": [["\u2581Al", "tr", "u", "ism"]], "entropy": [[0.73583984375, 0.01043701171875, 0.043487548828125, 0.0498046875]], "avg_entropy": [0.20989227294921875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Altruist", "score_of_biggest_cluster": 0.9640948176383972, "score_of_first_answer": 0.9640948176383972, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.875, "avg_score": 0.9640948176383972, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0523367122644619, "std_of_first_answer": 0.0523367122644619, "avg_entropy_of_first_answer": 0.20989227294921875, "lex_sim": 1.0}
{"question": "What was the fictional family with children called Greg, Peter, Bobby, Marcia, Jan and Cindy, that first came to US TV screens in 1969?", "answer": ["Brady Bunch #GivingTuesday: Support the work of Women Thrive Alliance"], "scores": [[0.293212890625, 1.0, 0.486083984375, 1.0, 2.980232238769531e-07, 0.0009312629699707031, 2.104043960571289e-05, 0.5849609375, 1.0, 0.99560546875, 0.01485443115234375, 0.046142578125, 0.0806884765625, 0.00951385498046875, 0.97314453125, 0.0007796287536621094, 0.0005784034729003906, 0.9443359375, 0.8447265625]], "normalized_score": [0.03339824452996254], "tokens": [["\u2581Br", "ady", "\u2581B", "unch", "\u2581#", "G", "iving", "T", "ues", "day", ":", "\u2581Support", "\u2581the", "\u2581work", "\u2581of", "\u2581Women", "\u2581Th", "rive", "\u2581Alliance"]], "entropy": [[0.904296875, 0.0011310577392578125, 1.5205078125, 0.00018644332885742188, 0.8544921875, 4.296875, 1.365234375, 1.546875, 0.002590179443359375, 0.03509521484375, 2.728515625, 4.7890625, 4.109375, 4.61328125, 0.168212890625, 4.00390625, 1.998046875, 0.261474609375, 0.669921875]], "avg_entropy": [1.7825832115976434], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kellys kids", "score_of_biggest_cluster": 0.03339824452996254, "score_of_first_answer": 0.03339824452996254, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.980232238769531e-07, "avg_score": 0.03339824452996254, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43603908551632276, "std_of_first_answer": 0.43603908551632276, "avg_entropy_of_first_answer": 1.7825832115976434, "lex_sim": 1.0}
{"question": "August 17, 1908 saw the opening of what public farmers market, the longest continually running public farmers market in the US?", "answer": ["Portland Farmers Market in Portland, Oregon"], "scores": [[0.061065673828125, 0.85205078125, 0.390380859375, 1.0, 0.97509765625, 0.0438232421875, 0.88427734375, 0.99951171875, 0.990234375, 0.98876953125]], "normalized_score": [0.4870474934577942], "tokens": [["\u2581Port", "land", "\u2581Farm", "ers", "\u2581Market", "\u2581in", "\u2581Port", "land", ",", "\u2581Oregon"]], "entropy": [[2.794921875, 0.90966796875, 1.916015625, 0.00013244152069091797, 0.1563720703125, 1.15625, 0.4951171875, 0.00439453125, 0.06890869140625, 0.07794189453125]], "avg_entropy": [0.7579722285270691], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sanitary Market", "score_of_biggest_cluster": 0.4870474934577942, "score_of_first_answer": 0.4870474934577942, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0438232421875, "avg_score": 0.4870474934577942, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37567620485812286, "std_of_first_answer": 0.37567620485812286, "avg_entropy_of_first_answer": 0.7579722285270691, "lex_sim": 1.0}
{"question": "What was the name of the scientist who discovered radiation belts around the Earth at a height of approximately 10,000 miles?", "answer": ["James Van Allen"], "scores": [[0.80126953125, 0.98779296875, 0.98974609375]], "normalized_score": [0.9218412041664124], "tokens": [["\u2581James", "\u2581Van", "\u2581Allen"]], "entropy": [[0.88330078125, 0.07843017578125, 0.060943603515625]], "avg_entropy": [0.3408915201822917], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "James Van Allen", "score_of_biggest_cluster": 0.9218412041664124, "score_of_first_answer": 0.9218412041664124, "max_score_of_first_answer": 0.98974609375, "min_score_of_first_answer": 0.80126953125, "avg_score": 0.9218412041664124, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08839194410622274, "std_of_first_answer": 0.08839194410622274, "avg_entropy_of_first_answer": 0.3408915201822917, "lex_sim": 1.0}
{"question": "The official Astronomical length of a ('Julian') year is how many days?", "answer": ["365.25 days"], "scores": [[0.8583984375, 0.9970703125, 0.99755859375, 0.998046875, 0.87939453125, 0.9970703125, 0.9970703125, 0.06939697265625]], "normalized_score": [0.6905338764190674], "tokens": [["\u2581", "3", "6", "5", ".", "2", "5", "\u2581days"]], "entropy": [[0.82763671875, 0.027801513671875, 0.0221099853515625, 0.018463134765625, 0.68017578125, 0.029815673828125, 0.025115966796875, 1.0673828125]], "avg_entropy": [0.3373126983642578], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "365.25", "score_of_biggest_cluster": 0.6905338764190674, "score_of_first_answer": 0.6905338764190674, "max_score_of_first_answer": 0.998046875, "min_score_of_first_answer": 0.06939697265625, "avg_score": 0.6905338764190674, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.29976065749982306, "std_of_first_answer": 0.29976065749982306, "avg_entropy_of_first_answer": 0.3373126983642578, "lex_sim": 1.0}
{"question": "Whose character ran a bookshop in the film 'Notting Hill'?", "answer": ["Anna Scott generous in giving"], "scores": [[0.09747314453125, 0.99755859375, 0.0, 0.0391845703125, 0.0021572113037109375, 0.013153076171875]], "normalized_score": [0.0], "tokens": [["\u2581Anna", "\u2581Scott", "\u2581gener", "ous", "\u2581in", "\u2581giving"]], "entropy": [[2.03515625, 0.0242767333984375, 1.6083984375, 3.734375, 4.31640625, 5.3359375]], "avg_entropy": [2.8424250284830728], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hugh Grant", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.99755859375, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3619627091477478, "std_of_first_answer": 0.3619627091477478, "avg_entropy_of_first_answer": 2.8424250284830728, "lex_sim": 1.0}
{"question": "What's missing: Amphibians, Fish, Mammals, Reptiles, Invertebrates?", "answer": ["Fish"], "scores": [[0.040496826171875]], "normalized_score": [0.0404968224465847], "tokens": [["\u2581Fish"]], "entropy": [[2.8515625]], "avg_entropy": [2.8515625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Avians", "score_of_biggest_cluster": 0.0404968224465847, "score_of_first_answer": 0.0404968224465847, "max_score_of_first_answer": 0.040496826171875, "min_score_of_first_answer": 0.040496826171875, "avg_score": 0.0404968224465847, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.8515625, "lex_sim": 1.0}
{"question": "Mary Surratt was executed having been found guilty of the conspiracy to assassinate who?", "answer": ["AbrahamL"], "scores": [[0.48583984375, 0.00019490718841552734]], "normalized_score": [0.009731069207191467], "tokens": [["\u2581Abraham", "L"]], "entropy": [[1.232421875, 0.24169921875]], "avg_entropy": [0.737060546875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Abaraham lincoln", "score_of_biggest_cluster": 0.009731069207191467, "score_of_first_answer": 0.009731069207191467, "max_score_of_first_answer": 0.48583984375, "min_score_of_first_answer": 0.00019490718841552734, "avg_score": 0.009731069207191467, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.24282246828079224, "std_of_first_answer": 0.24282246828079224, "avg_entropy_of_first_answer": 0.737060546875, "lex_sim": 1.0}
{"question": "In the US television series \u2018Friends\u2019, what is Joey\u2019s surname?", "answer": ["Tribbiani"], "scores": [[0.9931640625, 0.97900390625, 0.99951171875]], "normalized_score": [0.9905226826667786], "tokens": [["\u2581Trib", "b", "iani"]], "entropy": [[0.06500244140625, 0.12744140625, 0.003170013427734375]], "avg_entropy": [0.06520462036132812], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joey Tribiane", "score_of_biggest_cluster": 0.9905226826667786, "score_of_first_answer": 0.9905226826667786, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.97900390625, "avg_score": 0.9905226826667786, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.008572391980640425, "std_of_first_answer": 0.008572391980640425, "avg_entropy_of_first_answer": 0.06520462036132812, "lex_sim": 1.0}
{"question": "What is the project/brand name for Google's computerized spectacles?", "answer": ["Google Glass #101 Postby MaryStella \u00bb July 14th, 2017, 2:20 pm"], "scores": [[0.7939453125, 0.998046875, 0.99951171875, 5.960464477539063e-08, 0.01372528076171875, 0.09942626953125, 0.0634765625, 5.4836273193359375e-06, 2.6464462280273438e-05, 0.00012814998626708984, 0.00011688470840454102, 0.004756927490234375, 0.006534576416015625, 0.0022983551025390625, 0.990234375, 0.406494140625, 0.09326171875, 0.87060546875, 0.99658203125, 0.9873046875, 0.9951171875, 0.9990234375, 0.68408203125, 0.0858154296875, 0.98583984375, 0.990234375, 0.07965087890625, 0.99560546875, 0.2066650390625, 0.09466552734375, 0.67041015625]], "normalized_score": [0.04444609582424164], "tokens": [["\u2581Google", "\u2581G", "lass", "\u2581#", "1", "0", "1", "\u2581Post", "by", "\u2581Mary", "St", "ella", "\u2581\u00bb", "\u2581July", "\u2581", "1", "4", "th", ",", "\u2581", "2", "0", "1", "7", ",", "\u2581", "2", ":", "2", "0", "\u2581pm"]], "entropy": [[0.72607421875, 0.0172271728515625, 0.0048065185546875, 0.82666015625, 5.375, 3.44921875, 3.318359375, 2.9296875, 3.4453125, 6.30859375, 5.890625, 2.294921875, 5.51171875, 2.8359375, 0.09429931640625, 1.5576171875, 2.447265625, 0.430908203125, 0.0302276611328125, 0.1326904296875, 0.03729248046875, 0.008697509765625, 0.7509765625, 2.091796875, 0.1229248046875, 0.1090087890625, 1.9072265625, 0.042938232421875, 1.7998046875, 2.302734375, 0.74853515625]], "avg_entropy": [1.8564221782069052], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Optical glass", "score_of_biggest_cluster": 0.04444609582424164, "score_of_first_answer": 0.04444609582424164, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.04444609582424164, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44027953137545583, "std_of_first_answer": 0.44027953137545583, "avg_entropy_of_first_answer": 1.8564221782069052, "lex_sim": 1.0}
{"question": "Who wrote the short story on which the much admired film The Shaw - shank Redemption was based ?", "answer": ["Stephen King"], "scores": [[0.841796875, 0.98486328125]], "normalized_score": [0.9105244874954224], "tokens": [["\u2581Stephen", "\u2581King"]], "entropy": [[1.1943359375, 0.1197509765625]], "avg_entropy": [0.65704345703125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stephen King", "score_of_biggest_cluster": 0.9105244874954224, "score_of_first_answer": 0.9105244874954224, "max_score_of_first_answer": 0.98486328125, "min_score_of_first_answer": 0.841796875, "avg_score": 0.9105244874954224, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.071533203125, "std_of_first_answer": 0.071533203125, "avg_entropy_of_first_answer": 0.65704345703125, "lex_sim": 1.0}
{"question": "Which method of painting consists of applying pigment to wet plaster?", "answer": ["Fresco"], "scores": [[0.87158203125, 0.99853515625, 0.99560546875]], "normalized_score": [0.9533513188362122], "tokens": [["\u2581F", "res", "co"]], "entropy": [[0.59814453125, 0.01629638671875, 0.0367431640625]], "avg_entropy": [0.21706136067708334], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fresco painting", "score_of_biggest_cluster": 0.9533513188362122, "score_of_first_answer": 0.9533513188362122, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.87158203125, "avg_score": 0.9533513188362122, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.05916783291379201, "std_of_first_answer": 0.05916783291379201, "avg_entropy_of_first_answer": 0.21706136067708334, "lex_sim": 1.0}
{"question": "Of which country was Cleopatra the Queen", "answer": ["Ancient Egypt ###Who is the leader of the Nationalist Party in Malta?"], "scores": [[0.1649169921875, 0.998046875, 0.98681640625, 0.0, 2.0444393157958984e-05, 0.16796875, 0.53662109375, 0.026092529296875, 0.98583984375, 0.55224609375, 0.0130462646484375, 0.279052734375, 0.63818359375, 0.5830078125, 0.9501953125, 0.99951171875, 0.67333984375]], "normalized_score": [0.0], "tokens": [["\u2581An", "cient", "\u2581Egypt", "\u2581###", "Who", "\u2581is", "\u2581the", "\u2581leader", "\u2581of", "\u2581the", "\u2581National", "ist", "\u2581Party", "\u2581in", "\u2581Mal", "ta", "?"]], "entropy": [[0.85693359375, 0.02362060546875, 0.10650634765625, 0.98291015625, 2.951171875, 3.1484375, 3.08984375, 4.01171875, 0.1229248046875, 2.83203125, 4.31640625, 2.208984375, 1.3271484375, 1.212890625, 0.4169921875, 0.0031681060791015625, 1.5517578125]], "avg_entropy": [1.7154968486112707], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "EGY", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3808835714663349, "std_of_first_answer": 0.3808835714663349, "avg_entropy_of_first_answer": 1.7154968486112707, "lex_sim": 1.0}
{"question": "What collective noun is normally used to describe a group of finches or hummingbirds ?", "answer": ["A \"Flock\""], "scores": [[0.787109375, 0.67236328125, 0.004077911376953125, 0.99560546875, 0.97802734375]], "normalized_score": [0.29140904545783997], "tokens": [["\u2581A", "\u2581\"", "F", "lock", "\""]], "entropy": [[0.85205078125, 1.916015625, 2.765625, 0.0312347412109375, 0.1422119140625]], "avg_entropy": [1.1414276123046876], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A CHARM", "score_of_biggest_cluster": 0.29140904545783997, "score_of_first_answer": 0.29140904545783997, "max_score_of_first_answer": 0.99560546875, "min_score_of_first_answer": 0.004077911376953125, "avg_score": 0.29140904545783997, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3623677679790944, "std_of_first_answer": 0.3623677679790944, "avg_entropy_of_first_answer": 1.1414276123046876, "lex_sim": 1.0}
{"question": "Which German mathematician, physicist and astonomer (1777 - 1855) was involved in the first worldwide survey of the Earth's magnetic field and gives his name to a unit of magnetic induction ?", "answer": ["Carl Friedrich Gauss #MustDoMondays - Arts & Culture Edition (Apr 17-23, 2017)"], "scores": [[0.9580078125, 0.9853515625, 0.99658203125, 0.9990234375, 2.980232238769531e-07, 0.0028820037841796875, 0.000202178955078125, 0.0025768280029296875, 7.408857345581055e-05, 0.98388671875, 0.1190185546875, 0.00435638427734375, 7.492303848266602e-05, 0.2064208984375, 0.233642578125, 0.0084075927734375, 0.06427001953125, 0.0087738037109375, 0.07427978515625, 0.9326171875, 0.289794921875, 0.135009765625, 0.2110595703125, 0.7802734375, 0.986328125, 0.004451751708984375, 0.890625, 0.99951171875, 0.9990234375, 0.012420654296875, 0.9521484375, 0.984375]], "normalized_score": [0.051368679851293564], "tokens": [["\u2581Carl", "\u2581Friedrich", "\u2581G", "auss", "\u2581#", "M", "ust", "Do", "M", "ond", "ays", "\u2581-", "\u2581Arts", "\u2581&", "\u2581Culture", "\u2581Edition", "\u2581(", "A", "pr", "\u2581", "1", "7", "-", "2", "3", ",", "\u2581", "2", "0", "1", "7", ")"]], "entropy": [[0.29736328125, 0.120361328125, 0.0345458984375, 0.00791168212890625, 0.80078125, 4.9375, 4.92578125, 2.986328125, 0.09417724609375, 0.146240234375, 0.38134765625, 2.57421875, 5.64453125, 2.8046875, 1.7958984375, 3.037109375, 2.177734375, 4.140625, 2.177734375, 0.31689453125, 1.4638671875, 2.44921875, 1.6875, 0.86962890625, 0.0946044921875, 0.06097412109375, 0.4189453125, 0.006687164306640625, 0.0125885009765625, 0.07562255859375, 0.2529296875, 0.1280517578125]], "avg_entropy": [1.4663246870040894], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "GAUSS (program)", "score_of_biggest_cluster": 0.051368679851293564, "score_of_first_answer": 0.051368679851293564, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 2.980232238769531e-07, "avg_score": 0.051368679851293564, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44230034804683965, "std_of_first_answer": 0.44230034804683965, "avg_entropy_of_first_answer": 1.4663246870040894, "lex_sim": 1.0}
{"question": "What type of animal, according to its alternative name, is the Thylacine now thought to be extinct?", "answer": ["Tasmanian devil ###"], "scores": [[0.428466796875, 0.93408203125, 0.99951171875, 0.02008056640625, 0.0]], "normalized_score": [0.0], "tokens": [["\u2581Tas", "man", "ian", "\u2581devil", "\u2581###"]], "entropy": [[2.5078125, 0.253662109375, 0.005184173583984375, 1.1669921875, 0.88525390625]], "avg_entropy": [0.9637809753417969], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marsupial Wolf", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42909222446398554, "std_of_first_answer": 0.42909222446398554, "avg_entropy_of_first_answer": 0.9637809753417969, "lex_sim": 1.0}
{"question": "Apart from a battle, what did Nelson lose at Tenerife in 1797?", "answer": ["His right arm"], "scores": [[0.58544921875, 0.04669189453125, 0.99951171875]], "normalized_score": [0.30118927359580994], "tokens": [["\u2581His", "\u2581right", "\u2581arm"]], "entropy": [[2.255859375, 0.77685546875, 0.0034027099609375]], "avg_entropy": [1.0120391845703125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "His arm", "score_of_biggest_cluster": 0.30118927359580994, "score_of_first_answer": 0.30118927359580994, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.04669189453125, "avg_score": 0.30118927359580994, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3900958323469086, "std_of_first_answer": 0.3900958323469086, "avg_entropy_of_first_answer": 1.0120391845703125, "lex_sim": 1.0}
{"question": "What was the first name of the fictional character Gulliver, which was created by Jonathan Swift?", "answer": ["Lemuel Gulliver"], "scores": [[0.9755859375, 0.99853515625, 0.34033203125, 0.99853515625]], "normalized_score": [0.7585317492485046], "tokens": [["\u2581Lem", "uel", "\u2581Gul", "liver"]], "entropy": [[0.165771484375, 0.0158233642578125, 1.091796875, 0.013397216796875]], "avg_entropy": [0.3216972351074219], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lemuel", "score_of_biggest_cluster": 0.7585317492485046, "score_of_first_answer": 0.7585317492485046, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.34033203125, "avg_score": 0.7585317492485046, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2818536373510068, "std_of_first_answer": 0.2818536373510068, "avg_entropy_of_first_answer": 0.3216972351074219, "lex_sim": 1.0}
{"question": "The Earth's Equator is approximately how many millions of metres in length?", "answer": ["40,075,000 #Tech News: Sprint Corporation (NYSE: S) Surges 4.5% After Q1 Earnings"], "scores": [[0.5439453125, 0.9453125, 0.98388671875, 0.6943359375, 0.92041015625, 0.9267578125, 0.994140625, 0.395263671875, 0.998046875, 0.99951171875, 0.9833984375, 4.649162292480469e-06, 0.0005373954772949219, 0.0019102096557617188, 2.1517276763916016e-05, 0.06915283203125, 0.003864288330078125, 0.0112152099609375, 0.021697998046875, 0.1121826171875, 0.6494140625, 1.0, 1.0, 0.98193359375, 0.364501953125, 0.97119140625, 0.0037403106689453125, 0.0256805419921875, 0.7841796875, 0.2381591796875, 0.127685546875, 0.427001953125, 0.14013671875, 0.465576171875, 0.71630859375, 0.004032135009765625, 0.274169921875, 0.476806640625, 0.9931640625, 0.99755859375]], "normalized_score": [0.12351398915052414], "tokens": [["\u2581", "4", "0", ",", "0", "7", "5", ",", "0", "0", "0", "\u2581#", "T", "ech", "\u2581News", ":", "\u2581S", "print", "\u2581Corporation", "\u2581(", "N", "Y", "SE", ":", "\u2581S", ")", "\u2581S", "urg", "es", "\u2581", "4", ".", "5", "%", "\u2581After", "\u2581Q", "1", "\u2581E", "arn", "ings"]], "entropy": [[2.330078125, 0.279296875, 0.1236572265625, 1.1474609375, 0.339599609375, 0.381103515625, 0.0458984375, 2.05859375, 0.02020263671875, 0.0059661865234375, 0.098876953125, 1.955078125, 2.68359375, 5.09375, 4.19140625, 2.90625, 5.2578125, 3.3359375, 3.86328125, 3.138671875, 0.86083984375, 0.0010213851928710938, 0.002506256103515625, 0.126708984375, 0.77490234375, 0.17822265625, 3.908203125, 2.39453125, 0.60400390625, 2.744140625, 2.044921875, 2.05078125, 2.2890625, 1.9853515625, 1.384765625, 3.708984375, 1.62890625, 1.7197265625, 0.043426513671875, 0.0222015380859375]], "avg_entropy": [1.6932430982589721], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "forty", "score_of_biggest_cluster": 0.12351398915052414, "score_of_first_answer": 0.12351398915052414, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 4.649162292480469e-06, "avg_score": 0.12351398915052414, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40584862980100406, "std_of_first_answer": 0.40584862980100406, "avg_entropy_of_first_answer": 1.6932430982589721, "lex_sim": 1.0}
{"question": "\u201cTis Herself\u2019 published in 2005 was the autobiography of which Irish-American actress?", "answer": ["Herself"], "scores": [[0.0138092041015625, 0.99853515625, 1.0]], "normalized_score": [0.23979714512825012], "tokens": [["\u2581H", "ers", "elf"]], "entropy": [[3.447265625, 0.0172882080078125, 0.0019683837890625]], "avg_entropy": [1.1555074055989583], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Maureen FitzSimons", "score_of_biggest_cluster": 0.23979714512825012, "score_of_first_answer": 0.23979714512825012, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0138092041015625, "avg_score": 0.23979714512825012, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46454991748485286, "std_of_first_answer": 0.46454991748485286, "avg_entropy_of_first_answer": 1.1555074055989583, "lex_sim": 1.0}
{"question": "The Estonian, Arvo Part wrote a Cantus to mourn which English composer, who died in 1976?", "answer": ["Benjamin Britten #1 Post by Bob_San \u00bb Tue Feb 21, 2017 9:11 am"], "scores": [[0.9970703125, 0.99755859375, 0.99658203125, 1.0132789611816406e-06, 0.054046630859375, 2.2649765014648438e-06, 0.004638671875, 0.0008668899536132812, 0.31787109375, 0.00048065185546875, 0.2164306640625, 0.07611083984375, 0.99609375, 0.09600830078125, 0.99853515625, 0.197509765625, 0.0987548828125, 0.9951171875, 0.98828125, 0.9990234375, 1.0, 0.3642578125, 0.92236328125, 0.990234375, 0.08209228515625, 0.99853515625, 0.1502685546875, 0.0966796875, 0.468017578125]], "normalized_score": [0.08966153860092163], "tokens": [["\u2581Benjamin", "\u2581Brit", "ten", "\u2581#", "1", "\u2581Post", "\u2581by", "\u2581Bob", "_", "San", "\u2581\u00bb", "\u2581T", "ue", "\u2581Feb", "\u2581", "2", "1", ",", "\u2581", "2", "0", "1", "7", "\u2581", "9", ":", "1", "1", "\u2581am"]], "entropy": [[0.024688720703125, 0.0258941650390625, 0.029052734375, 0.775390625, 5.71875, 3.11328125, 2.814453125, 6.2734375, 4.14453125, 4.6953125, 1.658203125, 2.431640625, 0.029083251953125, 2.4765625, 0.01175689697265625, 0.92333984375, 1.521484375, 0.038482666015625, 0.08770751953125, 0.0081634521484375, 0.0008544921875, 0.69775390625, 0.28955078125, 0.08331298828125, 1.923828125, 0.01522064208984375, 1.794921875, 2.302734375, 0.70751953125]], "avg_entropy": [1.5385142359240302], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Britten, Benjamin", "score_of_biggest_cluster": 0.08966153860092163, "score_of_first_answer": 0.08966153860092163, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.0132789611816406e-06, "avg_score": 0.08966153860092163, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.43593959348071104, "std_of_first_answer": 0.43593959348071104, "avg_entropy_of_first_answer": 1.5385142359240302, "lex_sim": 1.0}
{"question": "\"In the 19th century, the term \"\"penny dreadful\"\" applied to what item?\"", "answer": ["Cheap fiction"], "scores": [[0.07110595703125, 0.99755859375, 0.040802001953125]], "normalized_score": [0.14250889420509338], "tokens": [["\u2581Che", "ap", "\u2581fiction"]], "entropy": [[2.298828125, 0.0207672119140625, 1.7138671875]], "avg_entropy": [1.3444875081380208], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Booke", "score_of_biggest_cluster": 0.14250889420509338, "score_of_first_answer": 0.14250889420509338, "max_score_of_first_answer": 0.99755859375, "min_score_of_first_answer": 0.040802001953125, "avg_score": 0.14250889420509338, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44404904555481417, "std_of_first_answer": 0.44404904555481417, "avg_entropy_of_first_answer": 1.3444875081380208, "lex_sim": 1.0}
{"question": "By what name was Peggy Hookham DBE better known?", "answer": ["Dame Margot Fonteyn"], "scores": [[0.34130859375, 0.9833984375, 0.20068359375, 0.99951171875, 0.99658203125, 1.0, 0.998046875]], "normalized_score": [0.6796149611473083], "tokens": [["\u2581D", "ame", "\u2581Mar", "got", "\u2581F", "onte", "yn"]], "entropy": [[3.015625, 0.1146240234375, 1.896484375, 0.0059967041015625, 0.0287628173828125, 0.0003829002380371094, 0.017669677734375]], "avg_entropy": [0.725649356842041], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Peggy Hookham", "score_of_biggest_cluster": 0.6796149611473083, "score_of_first_answer": 0.6796149611473083, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.20068359375, "avg_score": 0.6796149611473083, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3294930959784331, "std_of_first_answer": 0.3294930959784331, "avg_entropy_of_first_answer": 0.725649356842041, "lex_sim": 1.0}
{"question": "What is the name of the world's largest church, that was begun in 1450, finished in 1600 and consecrated by Pope Urban XIII in 1626?", "answer": ["St. Peter's Basilica"], "scores": [[0.6943359375, 0.8544921875, 0.96923828125, 0.97900390625, 1.0, 0.978515625, 0.99951171875, 1.0]], "normalized_score": [0.9281240701675415], "tokens": [["\u2581St", ".", "\u2581Peter", "'", "s", "\u2581Bas", "il", "ica"]], "entropy": [[1.4375, 0.487548828125, 0.18310546875, 0.1318359375, 0.00030612945556640625, 0.1522216796875, 0.0042266845703125, 0.0018482208251953125]], "avg_entropy": [0.2998241186141968], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Saint Peter's Basillica", "score_of_biggest_cluster": 0.9281240701675415, "score_of_first_answer": 0.9281240701675415, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.6943359375, "avg_score": 0.9281240701675415, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1012494875753646, "std_of_first_answer": 0.1012494875753646, "avg_entropy_of_first_answer": 0.2998241186141968, "lex_sim": 1.0}
{"question": "What country won 3 of the 5 Olympic gold medals awarded in baseball?", "answer": ["KOREA #MentalHealthMatters: What it means to have a mental health problem"], "scores": [[0.00023055076599121094, 0.07391357421875, 0.67626953125, 0.9794921875, 1.0609626770019531e-05, 0.000286102294921875, 6.288290023803711e-05, 0.65576171875, 1.0, 0.0792236328125, 0.99853515625, 0.00112152099609375, 0.01148223876953125, 0.0002536773681640625, 0.06402587890625, 0.88232421875, 0.126953125, 0.69873046875, 0.3779296875, 0.272216796875, 0.25146484375]], "normalized_score": [0.03223608434200287], "tokens": [["\u2581K", "O", "RE", "A", "\u2581#", "M", "ental", "He", "alth", "Mat", "ters", ":", "\u2581What", "\u2581it", "\u2581means", "\u2581to", "\u2581have", "\u2581a", "\u2581mental", "\u2581health", "\u2581problem"]], "entropy": [[2.046875, 4.4765625, 1.166015625, 0.153564453125, 1.09765625, 4.1875, 3.59765625, 2.1484375, 0.0012226104736328125, 0.48828125, 0.011199951171875, 0.982421875, 5.2578125, 2.94921875, 2.060546875, 0.65966796875, 2.162109375, 1.861328125, 3.0546875, 1.1005859375, 1.912109375]], "avg_entropy": [1.9702599843343098], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Republic of Cuba", "score_of_biggest_cluster": 0.03223608434200287, "score_of_first_answer": 0.03223608434200287, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.0609626770019531e-05, "avg_score": 0.03223608434200287, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3772214941180209, "std_of_first_answer": 0.3772214941180209, "avg_entropy_of_first_answer": 1.9702599843343098, "lex_sim": 1.0}
{"question": "Which band released the album 'Kid' in 2000?", "answer": ["MEGAME"], "scores": [[0.034515380859375, 0.0010280609130859375, 0.00020825862884521484]], "normalized_score": [0.0019478031899780035], "tokens": [["\u2581M", "EG", "AME"]], "entropy": [[3.837890625, 1.970703125, 0.88916015625]], "avg_entropy": [2.2325846354166665], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dead Air Space", "score_of_biggest_cluster": 0.0019478031899780035, "score_of_first_answer": 0.0019478031899780035, "max_score_of_first_answer": 0.034515380859375, "min_score_of_first_answer": 0.00020825862884521484, "avg_score": 0.0019478031899780035, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.01598280780752621, "std_of_first_answer": 0.01598280780752621, "avg_entropy_of_first_answer": 2.2325846354166665, "lex_sim": 1.0}
{"question": "What satellite, launched by NASA aboard a Delta rocket from Cape Canaveral on 10 July 1962, was built as part of a multi-national agreement between AT&T;, Bell Telephone Laboratories, NASA, the British General Post Office, and the French National PTT (Post, Telegraph & Telecom Office)?", "answer": ["Telstar ### Background"], "scores": [[0.77587890625, 0.99755859375, 0.0, 5.0067901611328125e-06]], "normalized_score": [0.0], "tokens": [["\u2581Tel", "star", "\u2581###", "\u2581Background"]], "entropy": [[0.65380859375, 0.0206451416015625, 1.248046875, 4.41015625]], "avg_entropy": [1.5831642150878906], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Telstar 11N", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.99755859375, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45023234111142524, "std_of_first_answer": 0.45023234111142524, "avg_entropy_of_first_answer": 1.5831642150878906, "lex_sim": 1.0}
{"question": "The inventors of Penicillin, Television, the Bank of England, the BBC, and the decimal point were?", "answer": ["Alexander Fleming, John Logie Baird, William Shakespeare, William Shakespeare, William Shakespeare 6 July 1976 \u2013 22 August 2017"], "scores": [[0.77734375, 0.99658203125, 1.0, 0.9990234375, 0.9365234375, 0.96875, 0.9951171875, 1.0, 0.99951171875, 1.0, 1.0, 0.99755859375, 0.958984375, 0.56005859375, 0.9736328125, 0.10235595703125, 0.129150390625, 0.94482421875, 0.1607666015625, 0.98681640625, 3.3736228942871094e-05, 0.0178680419921875, 0.0014591217041015625, 0.931640625, 0.99609375, 0.06011962890625, 0.032928466796875, 0.10986328125, 0.003276824951171875, 0.80908203125, 0.417724609375, 0.0911865234375, 0.08782958984375, 0.97998046875, 0.191650390625, 0.9970703125, 0.485595703125, 0.08660888671875]], "normalized_score": [0.24744091928005219], "tokens": [["\u2581Alexander", "\u2581F", "lem", "ing", ",", "\u2581John", "\u2581Log", "ie", "\u2581B", "air", "d", ",", "\u2581William", "\u2581Shakespeare", ",", "\u2581William", "\u2581Shakespeare", ",", "\u2581William", "\u2581Shakespeare", "\u2581", "6", "\u2581July", "\u2581", "1", "9", "7", "6", "\u2581\u2013", "\u2581", "2", "2", "\u2581August", "\u2581", "2", "0", "1", "7"]], "entropy": [[1.1328125, 0.037261962890625, 5.3882598876953125e-05, 0.009429931640625, 0.36669921875, 0.207275390625, 0.037139892578125, 0.002285003662109375, 0.0031566619873046875, 5.942583084106445e-05, 9.632110595703125e-05, 0.0233917236328125, 0.2724609375, 1.783203125, 0.2022705078125, 3.166015625, 2.03515625, 0.35888671875, 2.62109375, 0.11846923828125, 1.1083984375, 3.40234375, 3.513671875, 0.4521484375, 0.031646728515625, 1.3974609375, 1.7138671875, 1.576171875, 2.96484375, 1.302734375, 1.73046875, 2.349609375, 2.291015625, 0.1539306640625, 0.50244140625, 0.02642822265625, 1.06640625, 1.69921875]], "avg_entropy": [1.0436848543192212], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Scottish (disambiguation)", "score_of_biggest_cluster": 0.24744091928005219, "score_of_first_answer": 0.24744091928005219, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 3.3736228942871094e-05, "avg_score": 0.24744091928005219, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4226714917925911, "std_of_first_answer": 0.4226714917925911, "avg_entropy_of_first_answer": 1.0436848543192212, "lex_sim": 1.0}
{"question": "In the 2005 Julian Barnes novel \u2018Arthur and George\u2019, who is Arthur?", "answer": ["The main character"], "scores": [[0.005504608154296875, 0.1864013671875, 0.8515625]], "normalized_score": [0.09560135751962662], "tokens": [["\u2581The", "\u2581main", "\u2581character"]], "entropy": [[1.521484375, 2.677734375, 0.455810546875]], "avg_entropy": [1.5516764322916667], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A Conan Doyle", "score_of_biggest_cluster": 0.09560135751962662, "score_of_first_answer": 0.09560135751962662, "max_score_of_first_answer": 0.8515625, "min_score_of_first_answer": 0.005504608154296875, "avg_score": 0.09560135751962662, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3637729643591087, "std_of_first_answer": 0.3637729643591087, "avg_entropy_of_first_answer": 1.5516764322916667, "lex_sim": 1.0}
{"question": "An anchor hallmark on a valuable metal item denotes it was made in which city?", "answer": ["Edinburgh, Scotland"], "scores": [[0.44580078125, 0.31982421875, 0.91162109375]], "normalized_score": [0.5065498352050781], "tokens": [["\u2581Edinburgh", ",", "\u2581Scotland"]], "entropy": [[2.638671875, 1.095703125, 0.5576171875]], "avg_entropy": [1.4306640625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "County Borough of Birmingham", "score_of_biggest_cluster": 0.5065498352050781, "score_of_first_answer": 0.5065498352050781, "max_score_of_first_answer": 0.91162109375, "min_score_of_first_answer": 0.31982421875, "avg_score": 0.5065498352050781, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25453273072581883, "std_of_first_answer": 0.25453273072581883, "avg_entropy_of_first_answer": 1.4306640625, "lex_sim": 1.0}
{"question": "\u2018Hampsteads\u2019 is Cockney rhyming slang for what?", "answer": ["Teeth"], "scores": [[0.0501708984375, 0.99267578125]], "normalized_score": [0.2231668382883072], "tokens": [["\u2581Te", "eth"]], "entropy": [[4.04296875, 0.050018310546875]], "avg_entropy": [2.0464935302734375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tooth root", "score_of_biggest_cluster": 0.2231668382883072, "score_of_first_answer": 0.2231668382883072, "max_score_of_first_answer": 0.99267578125, "min_score_of_first_answer": 0.0501708984375, "avg_score": 0.2231668382883072, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.47125244140625, "std_of_first_answer": 0.47125244140625, "avg_entropy_of_first_answer": 2.0464935302734375, "lex_sim": 1.0}
{"question": "What did St. George slay?", "answer": ["Dragon"], "scores": [[0.14111328125]], "normalized_score": [0.14111328125], "tokens": [["\u2581Dragon"]], "entropy": [[1.79296875]], "avg_entropy": [1.79296875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Dragon (disambiguation)", "score_of_biggest_cluster": 0.14111328125, "score_of_first_answer": 0.14111328125, "max_score_of_first_answer": 0.14111328125, "min_score_of_first_answer": 0.14111328125, "avg_score": 0.14111328125, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.79296875, "lex_sim": 1.0}
{"question": "Who is the only actress to win 4 Oscars for Best Actress?", "answer": ["She was Katharine Hepburn"], "scores": [[0.00012093782424926758, 0.2041015625, 0.45849609375, 0.99951171875, 1.0, 0.98046875, 1.0, 0.99267578125]], "normalized_score": [0.24000577628612518], "tokens": [["\u2581She", "\u2581was", "\u2581Kath", "ar", "ine", "\u2581He", "p", "burn"]], "entropy": [[0.26220703125, 2.865234375, 2.779296875, 0.004497528076171875, 0.002193450927734375, 0.13916015625, 0.0014047622680664062, 0.056182861328125]], "avg_entropy": [0.7637721300125122], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Catherine Hepburn", "score_of_biggest_cluster": 0.24000577628612518, "score_of_first_answer": 0.24000577628612518, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.00012093782424926758, "avg_score": 0.24000577628612518, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3917816321940767, "std_of_first_answer": 0.3917816321940767, "avg_entropy_of_first_answer": 0.7637721300125122, "lex_sim": 1.0}
{"question": "The 1958 film \u2018Gigi\u2019 was based on the novel by which French author?", "answer": ["COCO before EDOUK"], "scores": [[0.0010309219360351562, 0.08251953125, 0.039703369140625, 0.00011116266250610352, 0.1341552734375, 0.00010687112808227539]], "normalized_score": [0.004186390433460474], "tokens": [["\u2581CO", "CO", "\u2581before", "\u2581E", "DO", "UK"]], "entropy": [[0.7138671875, 1.583984375, 3.4921875, 1.064453125, 4.08203125, 1.1484375]], "avg_entropy": [2.01416015625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sidonie-Gabrielle Colette", "score_of_biggest_cluster": 0.004186390433460474, "score_of_first_answer": 0.004186390433460474, "max_score_of_first_answer": 0.1341552734375, "min_score_of_first_answer": 0.00010687112808227539, "avg_score": 0.004186390433460474, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.050534803542988636, "std_of_first_answer": 0.050534803542988636, "avg_entropy_of_first_answer": 2.01416015625, "lex_sim": 1.0}
{"question": "What was the name of the character played by Ralph Fiennes in 'Schindler's List'?", "answer": ["Amon G\u00f6th"], "scores": [[0.97900390625, 1.0, 0.227294921875, 0.998046875]], "normalized_score": [0.6864853501319885], "tokens": [["\u2581Am", "on", "\u2581G\u00f6", "th"]], "entropy": [[0.1829833984375, 0.000904083251953125, 0.82666015625, 0.016326904296875]], "avg_entropy": [0.25671863555908203], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Amon Leopold G\u00f6th", "score_of_biggest_cluster": 0.6864853501319885, "score_of_first_answer": 0.6864853501319885, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.227294921875, "avg_score": 0.6864853501319885, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3313802002142984, "std_of_first_answer": 0.3313802002142984, "avg_entropy_of_first_answer": 0.25671863555908203, "lex_sim": 1.0}
{"question": "Who was assassinated by Hugh de Merville, William de Tracy, Reginald Fitzhurse and Richard le Breton?", "answer": ["Thomas Becket CHAPTER X. MEDI\u00c6VAL ART"], "scores": [[0.87158203125, 0.87890625, 1.0, 0.0, 0.0019445419311523438, 0.046173095703125, 0.007549285888671875, 0.0007023811340332031, 0.53076171875, 0.70263671875, 0.9970703125, 0.0233001708984375, 0.70458984375]], "normalized_score": [0.0], "tokens": [["\u2581Thomas", "\u2581Be", "cket", "\u2581CHAPTER", "\u2581X", ".", "\u2581M", "ED", "I", "\u00c6", "VAL", "\u2581AR", "T"]], "entropy": [[0.5732421875, 0.58642578125, 0.0005846023559570312, 1.04296875, 3.744140625, 4.171875, 4.73046875, 1.345703125, 1.31640625, 1.4482421875, 0.0221405029296875, 3.455078125, 0.7666015625]], "avg_entropy": [1.784913649925819], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Will no one rid me of this turbulent priest%3F", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41548139919456106, "std_of_first_answer": 0.41548139919456106, "avg_entropy_of_first_answer": 1.784913649925819, "lex_sim": 1.0}
{"question": "Which American singer's music publishing company was called Gladys Music?", "answer": ["Aretha Franklin"], "scores": [[0.02362060546875, 0.99755859375, 0.986328125]], "normalized_score": [0.2853756844997406], "tokens": [["\u2581Are", "tha", "\u2581Franklin"]], "entropy": [[4.04296875, 0.0180511474609375, 0.09576416015625]], "avg_entropy": [1.3855946858723958], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Elvis Aron Presley", "score_of_biggest_cluster": 0.2853756844997406, "score_of_first_answer": 0.2853756844997406, "max_score_of_first_answer": 0.99755859375, "min_score_of_first_answer": 0.02362060546875, "avg_score": 0.2853756844997406, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4564947482486865, "std_of_first_answer": 0.4564947482486865, "avg_entropy_of_first_answer": 1.3855946858723958, "lex_sim": 1.0}
{"question": "What Olympic event entails disqualification if both feet of a competitor are seen to be off the ground at the same time?", "answer": ["High jump ##### Winner of the Man Booker Prize 2019 ####"], "scores": [[0.279541015625, 0.79736328125, 0.0, 0.0002028942108154297, 0.0184783935546875, 0.1865234375, 0.5068359375, 9.876489639282227e-05, 0.76171875, 1.0, 0.9150390625, 0.1171875, 0.90380859375, 0.99951171875, 0.6630859375, 0.607421875, 0.0017843246459960938]], "normalized_score": [0.0], "tokens": [["\u2581High", "\u2581jump", "\u2581#####", "\u2581W", "inner", "\u2581of", "\u2581the", "\u2581Man", "\u2581Book", "er", "\u2581Prize", "\u2581", "2", "0", "1", "9", "\u2581####"]], "entropy": [[2.427734375, 0.75927734375, 0.8173828125, 2.400390625, 3.93359375, 3.66796875, 2.83203125, 3.462890625, 1.265625, 0.0014057159423828125, 0.370849609375, 1.1064453125, 0.345458984375, 0.00792694091796875, 0.767578125, 1.3232421875, 3.2421875]], "avg_entropy": [1.6901169945211971], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "5000 m walk", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3850742788036272, "std_of_first_answer": 0.3850742788036272, "avg_entropy_of_first_answer": 1.6901169945211971, "lex_sim": 1.0}
{"question": "Actor Norman Painting died in November 2009, which part in a log running radio series did he make his own?", "answer": ["The Giant Rat of Sumatra #BookReview: THE SECRET LIFE OF LUISA MONTEMAYOR by Janice Y.K."], "scores": [[0.033935546875, 0.0007100105285644531, 0.8388671875, 0.12347412109375, 0.99560546875, 0.99853515625, 0.9990234375, 3.0994415283203125e-06, 3.3020973205566406e-05, 0.01495361328125, 0.966796875, 0.0207366943359375, 0.004268646240234375, 0.0222015380859375, 0.83154296875, 0.96240234375, 0.1591796875, 0.89013671875, 1.0, 0.77783203125, 0.0025272369384765625, 0.056060791015625, 0.0129852294921875, 0.07464599609375, 0.11529541015625, 0.8984375, 0.325439453125, 0.9931640625, 0.9990234375, 0.1910400390625, 0.2841796875, 0.861328125, 0.115234375, 0.89697265625, 0.77880859375, 0.94677734375, 0.99609375]], "normalized_score": [0.11336418241262436], "tokens": [["\u2581The", "\u2581Gi", "ant", "\u2581Rat", "\u2581of", "\u2581Sum", "atra", "\u2581#", "Book", "Re", "view", ":", "\u2581THE", "\u2581SE", "CRE", "T", "\u2581L", "IF", "E", "\u2581OF", "\u2581L", "UI", "SA", "\u2581M", "ON", "TE", "MA", "Y", "OR", "\u2581by", "\u2581Jan", "ice", "\u2581Y", ".", "K", ".", "\u2581Lee"]], "entropy": [[2.185546875, 4.9140625, 0.6142578125, 3.27734375, 0.03533935546875, 0.01554107666015625, 0.01053619384765625, 0.78369140625, 4.1484375, 4.67578125, 0.17333984375, 1.8857421875, 3.703125, 3.642578125, 0.81103515625, 0.164794921875, 2.09765625, 0.431396484375, 0.0010423660278320312, 0.97216796875, 1.88671875, 2.935546875, 0.09246826171875, 1.5439453125, 2.546875, 0.52587890625, 2.12890625, 0.054290771484375, 0.01082611083984375, 2.09375, 3.74609375, 0.86376953125, 2.603515625, 0.466552734375, 0.6435546875, 0.24072265625, 0.027923583984375]], "avg_entropy": [1.539317698092074], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "PHIL ARCHER", "score_of_biggest_cluster": 0.11336418241262436, "score_of_first_answer": 0.11336418241262436, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 3.0994415283203125e-06, "avg_score": 0.11336418241262436, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42975901471086114, "std_of_first_answer": 0.42975901471086114, "avg_entropy_of_first_answer": 1.539317698092074, "lex_sim": 1.0}
{"question": "The 1950\u2019s Mau Mau uprising was against the British occupation of which country?", "answer": ["Kenya"], "scores": [[0.984375, 0.99560546875]], "normalized_score": [0.989974319934845], "tokens": [["\u2581Ken", "ya"]], "entropy": [[0.1409912109375, 0.0355224609375]], "avg_entropy": [0.0882568359375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kenyan", "score_of_biggest_cluster": 0.989974319934845, "score_of_first_answer": 0.989974319934845, "max_score_of_first_answer": 0.99560546875, "min_score_of_first_answer": 0.984375, "avg_score": 0.989974319934845, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.005615234375, "std_of_first_answer": 0.005615234375, "avg_entropy_of_first_answer": 0.0882568359375, "lex_sim": 1.0}
{"question": "The 1964 film A Shot In The Dark was the sequel to which film of the same year ?", "answer": ["Goldfinger"], "scores": [[0.6083984375, 0.99609375, 0.9990234375]], "normalized_score": [0.8459694385528564], "tokens": [["\u2581Gold", "f", "inger"]], "entropy": [[1.9560546875, 0.038055419921875, 0.0094451904296875]], "avg_entropy": [0.6678517659505209], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pink panther", "score_of_biggest_cluster": 0.8459694385528564, "score_of_first_answer": 0.8459694385528564, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.6083984375, "avg_score": 0.8459694385528564, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.18345575580090157, "std_of_first_answer": 0.18345575580090157, "avg_entropy_of_first_answer": 0.6678517659505209, "lex_sim": 1.0}
{"question": "What is a \u2018mullet\u2019 on a coat of arms?", "answer": ["Heraldic charge resembling a fish tail"], "scores": [[0.00946044921875, 0.99658203125, 0.64453125, 0.02349853515625, 0.0262451171875, 0.99755859375, 0.99951171875, 0.94140625, 0.61767578125, 0.31689453125]], "normalized_score": [0.2419879138469696], "tokens": [["\u2581Her", "ald", "ic", "\u2581charge", "\u2581res", "emb", "ling", "\u2581a", "\u2581fish", "\u2581tail"]], "entropy": [[1.0625, 0.03240966796875, 0.68798828125, 0.38623046875, 2.1953125, 0.0225067138671875, 0.0031890869140625, 0.32373046875, 2.4609375, 1.955078125]], "avg_entropy": [0.91298828125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stellar diameter", "score_of_biggest_cluster": 0.2419879138469696, "score_of_first_answer": 0.2419879138469696, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.00946044921875, "avg_score": 0.2419879138469696, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40816980727901964, "std_of_first_answer": 0.40816980727901964, "avg_entropy_of_first_answer": 0.91298828125, "lex_sim": 1.0}
{"question": "Lieutenant General James Thomas Brudenell, who commanded the Light Brigade of the British Army during the Crimean War, was the 7th Earl of what?", "answer": ["Cardigan"], "scores": [[0.9638671875, 1.0]], "normalized_score": [0.9817673563957214], "tokens": [["\u2581Card", "igan"]], "entropy": [[0.2509765625, 0.00234222412109375]], "avg_entropy": [0.12665939331054688], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cardigan", "score_of_biggest_cluster": 0.9817673563957214, "score_of_first_answer": 0.9817673563957214, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.9638671875, "avg_score": 0.9817673563957214, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.01806640625, "std_of_first_answer": 0.01806640625, "avg_entropy_of_first_answer": 0.12665939331054688, "lex_sim": 1.0}
{"question": "What is the sweet clear liquid by-product from soap manufacture, used in laxatives, explosives and antifreeze?", "answer": ["Glycerol #HedgeFunds: How will ESG factors shape the future of investing?"], "scores": [[0.96826171875, 1.0, 1.0, 0.7763671875, 5.960464477539063e-08, 0.0008397102355957031, 7.152557373046875e-07, 0.005115509033203125, 0.99853515625, 0.0345458984375, 0.007465362548828125, 0.004467010498046875, 0.00293731689453125, 0.0002276897430419922, 0.9990234375, 0.1610107421875, 0.04412841796875, 0.392333984375, 0.328125, 0.8955078125, 0.075927734375, 0.541015625, 0.86083984375]], "normalized_score": [0.02857130952179432], "tokens": [["\u2581G", "ly", "cer", "ol", "\u2581#", "H", "edge", "F", "und", "s", ":", "\u2581How", "\u2581will", "\u2581ES", "G", "\u2581factors", "\u2581shape", "\u2581the", "\u2581future", "\u2581of", "\u2581invest", "ing", "?"]], "entropy": [[0.2008056640625, 0.0015268325805664062, 0.0019426345825195312, 0.63720703125, 1.150390625, 4.10546875, 2.6171875, 0.47119140625, 0.01290130615234375, 3.330078125, 1.8173828125, 5.35546875, 3.1953125, 3.2265625, 0.0122528076171875, 3.373046875, 1.93359375, 2.4921875, 2.556640625, 0.580078125, 2.572265625, 0.98681640625, 0.68310546875]], "avg_entropy": [1.7962353747823965], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "C3H8O3", "score_of_biggest_cluster": 0.02857130952179432, "score_of_first_answer": 0.02857130952179432, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.02857130952179432, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.420651820832055, "std_of_first_answer": 0.420651820832055, "avg_entropy_of_first_answer": 1.7962353747823965, "lex_sim": 1.0}
{"question": "Name BP's catastrophic Gulf of Mexico oil rig, lost off the US Louisiana coast?", "answer": ["Deepwater Horizon"], "scores": [[0.970703125, 0.99169921875, 0.98876953125, 1.0]], "normalized_score": [0.987734854221344], "tokens": [["\u2581Deep", "water", "\u2581Hor", "izon"]], "entropy": [[0.1895751953125, 0.06268310546875, 0.081787109375, 0.00235748291015625]], "avg_entropy": [0.08410072326660156], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Deepwater Horizon rig", "score_of_biggest_cluster": 0.987734854221344, "score_of_first_answer": 0.987734854221344, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.970703125, "avg_score": 0.987734854221344, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.010692133027567476, "std_of_first_answer": 0.010692133027567476, "avg_entropy_of_first_answer": 0.08410072326660156, "lex_sim": 1.0}
{"question": "Luminol, a chemical that exhibits chemiluminescence, is often used by forensic examiners to try and detect trace amounts of what?", "answer": ["Blood"], "scores": [[0.900390625]], "normalized_score": [0.900390625], "tokens": [["\u2581Blood"]], "entropy": [[0.44921875]], "avg_entropy": [0.44921875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A blood", "score_of_biggest_cluster": 0.900390625, "score_of_first_answer": 0.900390625, "max_score_of_first_answer": 0.900390625, "min_score_of_first_answer": 0.900390625, "avg_score": 0.900390625, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.44921875, "lex_sim": 1.0}
{"question": "What name is given to a substance that accelerates a chemical reaction without itself being affected?", "answer": ["A catalyst"], "scores": [[0.2044677734375, 0.953125, 0.998046875]], "normalized_score": [0.579395592212677], "tokens": [["\u2581A", "\u2581catal", "yst"]], "entropy": [[1.568359375, 0.25341796875, 0.0174102783203125]], "avg_entropy": [0.6130625406901041], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Catalytes", "score_of_biggest_cluster": 0.579395592212677, "score_of_first_answer": 0.579395592212677, "max_score_of_first_answer": 0.998046875, "min_score_of_first_answer": 0.2044677734375, "avg_score": 0.579395592212677, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36397090973112345, "std_of_first_answer": 0.36397090973112345, "avg_entropy_of_first_answer": 0.6130625406901041, "lex_sim": 1.0}
{"question": "What is the name of the whirlpool in the Hebrides between Scarba and Jura, the third largest in the world?", "answer": ["Corryvreckan #HASHTAG CAMPAIGN FOR HUMAN RIGHTS"], "scores": [[0.7001953125, 1.0, 0.99462890625, 0.994140625, 0.9990234375, 5.364418029785156e-07, 0.0028171539306640625, 0.00016808509826660156, 0.98974609375, 0.99560546875, 6.079673767089844e-06, 0.01898193359375, 0.340087890625, 0.986328125, 0.0005140304565429688, 0.0164031982421875, 0.00872039794921875, 0.92529296875, 0.98193359375, 0.99853515625, 0.99755859375]], "normalized_score": [0.04984103515744209], "tokens": [["\u2581Cor", "ry", "vre", "ck", "an", "\u2581#", "H", "AS", "HT", "AG", "\u2581CA", "MP", "A", "IGN", "\u2581FOR", "\u2581H", "U", "MAN", "\u2581R", "IGHT", "S"]], "entropy": [[1.00390625, 0.001316070556640625, 0.049285888671875, 0.046234130859375, 0.00856781005859375, 1.224609375, 3.818359375, 2.44140625, 0.08636474609375, 0.037200927734375, 2.6640625, 3.0625, 3.107421875, 0.08624267578125, 1.8046875, 1.701171875, 1.71875, 0.28466796875, 0.1307373046875, 0.01434326171875, 0.0248565673828125]], "avg_entropy": [1.1103186834426153], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Coire Bhreacain", "score_of_biggest_cluster": 0.04984103515744209, "score_of_first_answer": 0.04984103515744209, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.364418029785156e-07, "avg_score": 0.04984103515744209, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4648411085702343, "std_of_first_answer": 0.4648411085702343, "avg_entropy_of_first_answer": 1.1103186834426153, "lex_sim": 1.0}
{"question": "What do antique dealers call old wooden utensils such as soupspoons, bowls etc", "answer": ["Woodenware"], "scores": [[0.09588623046875, 0.9072265625, 0.576171875]], "normalized_score": [0.368701308965683], "tokens": [["\u2581Wood", "en", "ware"]], "entropy": [[4.703125, 0.51953125, 1.9287109375]], "avg_entropy": [2.3837890625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Treen", "score_of_biggest_cluster": 0.368701308965683, "score_of_first_answer": 0.368701308965683, "max_score_of_first_answer": 0.9072265625, "min_score_of_first_answer": 0.09588623046875, "avg_score": 0.368701308965683, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.33309068515597634, "std_of_first_answer": 0.33309068515597634, "avg_entropy_of_first_answer": 2.3837890625, "lex_sim": 1.0}
{"question": "When it was a first performed at the Abbey Theatre in Dublin in 1907, which play caused rioting in the streets because of objections to the portrayal of rural Irish Catholic immorality?", "answer": ["The Playboy of the Western World #44765 \u2022 2014-06-28 09:28:38 UTC \u2022 Edited"], "scores": [[0.326171875, 0.79150390625, 0.982421875, 0.994140625, 0.998046875, 0.9990234375, 0.998046875, 1.1920928955078125e-07, 0.0032024383544921875, 0.07098388671875, 0.0247650146484375, 0.08245849609375, 0.08441162109375, 8.940696716308594e-07, 0.068115234375, 0.361083984375, 0.73974609375, 0.0697021484375, 0.0251617431640625, 0.85986328125, 0.89013671875, 0.03851318359375, 0.99609375, 0.259521484375, 0.0758056640625, 0.9541015625, 0.324951171875, 0.10906982421875, 0.99853515625, 0.189453125, 0.109130859375, 0.99072265625, 0.1881103515625, 0.108154296875, 0.0595703125, 0.048187255859375, 7.593631744384766e-05, 0.99267578125]], "normalized_score": [0.08735786378383636], "tokens": [["\u2581The", "\u2581Play", "boy", "\u2581of", "\u2581the", "\u2581Western", "\u2581World", "\u2581#", "4", "4", "7", "6", "5", "\u2581\u2022", "\u2581", "2", "0", "1", "4", "-", "0", "6", "-", "2", "8", "\u2581", "0", "9", ":", "2", "8", ":", "3", "8", "\u2581UTC", "\u2581\u2022", "\u2581Ed", "ited"]], "entropy": [[2.837890625, 0.7802734375, 0.136962890625, 0.04534912109375, 0.0189666748046875, 0.01012420654296875, 0.0186309814453125, 1.0869140625, 4.8828125, 3.705078125, 3.357421875, 3.103515625, 2.779296875, 2.8203125, 5.453125, 2.021484375, 1.4189453125, 0.95751953125, 1.486328125, 0.80126953125, 0.45556640625, 2.01953125, 0.038665771484375, 1.1767578125, 2.28515625, 0.30078125, 1.0029296875, 2.119140625, 0.016021728515625, 1.794921875, 2.294921875, 0.0784912109375, 1.7841796875, 2.306640625, 1.923828125, 1.4404296875, 3.568359375, 0.078125]], "avg_entropy": [1.6422807793868215], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sara Tansey", "score_of_biggest_cluster": 0.08735786378383636, "score_of_first_answer": 0.08735786378383636, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 1.1920928955078125e-07, "avg_score": 0.08735786378383636, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41345097308494494, "std_of_first_answer": 0.41345097308494494, "avg_entropy_of_first_answer": 1.6422807793868215, "lex_sim": 1.0}
{"question": "Name the Archbishop of Canterbury who was burnt at the stake in 1556?", "answer": ["Thomas Cranmer"], "scores": [[0.8740234375, 0.98095703125, 0.94677734375, 1.0]], "normalized_score": [0.9491945505142212], "tokens": [["\u2581Thomas", "\u2581C", "ran", "mer"]], "entropy": [[0.87939453125, 0.169677734375, 0.2310791015625, 0.001583099365234375]], "avg_entropy": [0.3204336166381836], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Thomas Cranmer", "score_of_biggest_cluster": 0.9491945505142212, "score_of_first_answer": 0.9491945505142212, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.8740234375, "avg_score": 0.9491945505142212, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.04806347071903791, "std_of_first_answer": 0.04806347071903791, "avg_entropy_of_first_answer": 0.3204336166381836, "lex_sim": 1.0}
{"question": "Kylie Minogue played Charlene Robinson in what soap opera?", "answer": ["Neighbors"], "scores": [[0.98974609375, 0.9990234375, 1.0, 0.27783203125]], "normalized_score": [0.7239697575569153], "tokens": [["\u2581Ne", "igh", "b", "ors"]], "entropy": [[0.081787109375, 0.008087158203125, 0.002582550048828125, 0.60400390625]], "avg_entropy": [0.17411518096923828], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "List of Neighbours characters (1986)", "score_of_biggest_cluster": 0.7239697575569153, "score_of_first_answer": 0.7239697575569153, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.27783203125, "avg_score": 0.7239697575569153, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3111126621998616, "std_of_first_answer": 0.3111126621998616, "avg_entropy_of_first_answer": 0.17411518096923828, "lex_sim": 1.0}
{"question": "What is the capital and largest city of the Islamic Republic of Mauritania?", "answer": ["Nouakchott CHAPTER 35 35 Then shall the King say unto them on his right hand, Come, ye blessed of my Father, inherit the kingdom prepared for you from the foundation of the world: 2"], "scores": [[0.982421875, 0.99267578125, 0.9931640625, 0.99658203125, 1.0, 0.0, 0.11614990234375, 0.0958251953125, 0.00714874267578125, 0.018341064453125, 0.0274810791015625, 0.483154296875, 0.0013551712036132812, 0.1849365234375, 0.96875, 0.60791015625, 0.875, 0.98095703125, 1.0, 0.9951171875, 0.99951171875, 0.99658203125, 1.0, 0.99609375, 0.98876953125, 0.98828125, 0.99951171875, 0.9990234375, 1.0, 0.99853515625, 0.998046875, 0.9990234375, 0.998046875, 0.9990234375, 0.998046875, 0.99658203125, 0.9990234375, 0.99951171875, 0.9990234375, 0.99755859375, 0.998046875, 0.99951171875, 0.99755859375, 0.99853515625, 1.0, 0.9541015625, 0.57568359375, 0.003559112548828125]], "normalized_score": [0.0], "tokens": [["\u2581N", "ou", "ak", "ch", "ott", "\u2581CHAPTER", "\u2581", "3", "5", "\u2581", "3", "5", "\u2581Then", "\u2581shall", "\u2581the", "\u2581King", "\u2581say", "\u2581unto", "\u2581them", "\u2581on", "\u2581his", "\u2581right", "\u2581hand", ",", "\u2581Come", ",", "\u2581ye", "\u2581bl", "essed", "\u2581of", "\u2581my", "\u2581Father", ",", "\u2581inherit", "\u2581the", "\u2581kingdom", "\u2581prepared", "\u2581for", "\u2581you", "\u2581from", "\u2581the", "\u2581foundation", "\u2581of", "\u2581the", "\u2581world", ":", "\u2581", "2"]], "entropy": [[0.18115234375, 0.061248779296875, 0.0574951171875, 0.0290985107421875, 0.0021915435791015625, 0.79150390625, 3.38671875, 1.888671875, 2.927734375, 3.568359375, 4.33203125, 2.158203125, 4.171875, 2.720703125, 0.2032470703125, 1.6806640625, 0.87939453125, 0.1103515625, 0.0022258758544921875, 0.038604736328125, 0.00388336181640625, 0.0238037109375, 0.001949310302734375, 0.035064697265625, 0.0924072265625, 0.0782470703125, 0.007076263427734375, 0.0117645263671875, 0.00023221969604492188, 0.0135955810546875, 0.01849365234375, 0.00789642333984375, 0.018218994140625, 0.01216888427734375, 0.0167388916015625, 0.0288543701171875, 0.00949859619140625, 0.0072784423828125, 0.00942230224609375, 0.02020263671875, 0.0180511474609375, 0.00418853759765625, 0.01873779296875, 0.01226806640625, 0.0015544891357421875, 0.312744140625, 1.482421875, 0.11004638671875]], "avg_entropy": [0.6576725939909617], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Noukchott", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3709073974011873, "std_of_first_answer": 0.3709073974011873, "avg_entropy_of_first_answer": 0.6576725939909617, "lex_sim": 1.0}
{"question": "Under what name is the American singer/songwriter Steveland Judkins better known?", "answer": ["Stevie Wonder"], "scores": [[0.9384765625, 0.99365234375, 0.93701171875]], "normalized_score": [0.9560214281082153], "tokens": [["\u2581Ste", "vie", "\u2581Wonder"]], "entropy": [[0.35791015625, 0.0450439453125, 0.305908203125]], "avg_entropy": [0.23628743489583334], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Steve Wonder", "score_of_biggest_cluster": 0.9560214281082153, "score_of_first_answer": 0.9560214281082153, "max_score_of_first_answer": 0.99365234375, "min_score_of_first_answer": 0.93701171875, "avg_score": 0.9560214281082153, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02636216355172354, "std_of_first_answer": 0.02636216355172354, "avg_entropy_of_first_answer": 0.23628743489583334, "lex_sim": 1.0}
{"question": "The Scottish football club Ferranti Thistle joined the Scottish Football League in 1974 as Meadowbank Thistle; in 1995 they relocated and changed their name to what?", "answer": ["Livingston ### Hackathons & Workshops"], "scores": [[0.96826171875, 0.986328125, 0.0, 7.367134094238281e-05, 0.0027313232421875, 0.0005970001220703125, 0.0277557373046875, 0.0005054473876953125, 0.003570556640625, 0.9599609375, 0.9990234375]], "normalized_score": [0.0], "tokens": [["\u2581Living", "ston", "\u2581###", "\u2581H", "ack", "ath", "ons", "\u2581&", "\u2581Work", "sh", "ops"]], "entropy": [[0.216064453125, 0.079833984375, 1.40234375, 2.626953125, 4.0, 2.94140625, 0.6142578125, 2.75, 5.390625, 0.297119140625, 0.00814056396484375]], "avg_entropy": [1.8478858254172585], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Livingston (disambiguation)", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46838065800648904, "std_of_first_answer": 0.46838065800648904, "avg_entropy_of_first_answer": 1.8478858254172585, "lex_sim": 1.0}
{"question": "On which island is the city of Knossos, once the capital of the ancient Minoan civilisation?", "answer": ["Crete #NHL #Penguins"], "scores": [[0.9677734375, 0.998046875, 5.960464477539062e-07, 0.004947662353515625, 8.58306884765625e-06, 0.18408203125, 0.01287078857421875, 0.0975341796875, 0.9990234375]], "normalized_score": [0.012120228260755539], "tokens": [["\u2581Cre", "te", "\u2581#", "N", "HL", "\u2581#", "P", "engu", "ins"]], "entropy": [[0.265380859375, 0.0188751220703125, 1.083984375, 4.27734375, 4.203125, 2.9921875, 5.296875, 4.50390625, 0.00955963134765625]], "avg_entropy": [2.51680416531033], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Krete", "score_of_biggest_cluster": 0.012120228260755539, "score_of_first_answer": 0.012120228260755539, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 5.960464477539062e-07, "avg_score": 0.012120228260755539, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.44601537819271475, "std_of_first_answer": 0.44601537819271475, "avg_entropy_of_first_answer": 2.51680416531033, "lex_sim": 1.0}
{"question": "In which field of the arts is Terry O'Neill a leading name?", "answer": ["Photography, Fine Arts, Architecture, Graphic Design, Computer Graphics, Fashion Design, Interior Design, Industrial Design, Furniture Design, Textile Design, Web Design, Animation, Illustration, Game Design, Advert"], "scores": [[0.9228515625, 0.978515625, 0.0019779205322265625, 0.00048804283142089844, 0.240966796875, 0.0950927734375, 0.034637451171875, 0.5625, 0.05645751953125, 0.998046875, 0.7451171875, 0.70068359375, 0.005153656005859375, 0.70751953125, 0.7734375, 0.04083251953125, 0.98046875, 0.50634765625, 0.912109375, 0.501953125, 0.99560546875, 0.9580078125, 0.95068359375, 0.6591796875, 0.99755859375, 0.9833984375, 0.95654296875, 0.008544921875, 0.96484375, 1.0, 0.9619140625, 0.9375, 0.66455078125, 0.99365234375, 0.88720703125, 0.966796875, 0.00817108154296875, 0.9150390625, 0.916015625, 0.34130859375, 1.0, 0.87548828125, 0.35498046875, 1.0, 0.96240234375, 0.01419830322265625, 0.91064453125, 0.95166015625, 0.06182861328125, 1.0]], "normalized_score": [0.3247242271900177], "tokens": [["\u2581Phot", "ography", ",", "\u2581Fine", "\u2581Arts", ",", "\u2581Architecture", ",", "\u2581Graph", "ic", "\u2581Design", ",", "\u2581Computer", "\u2581Graphics", ",", "\u2581F", "ashion", "\u2581Design", ",", "\u2581Inter", "ior", "\u2581Design", ",", "\u2581Indust", "rial", "\u2581Design", ",", "\u2581F", "urn", "iture", "\u2581Design", ",", "\u2581Text", "ile", "\u2581Design", ",", "\u2581Web", "\u2581Design", ",", "\u2581An", "imation", ",", "\u2581Ill", "ustration", ",", "\u2581Game", "\u2581Design", ",", "\u2581Ad", "vert"]], "entropy": [[0.40380859375, 0.11907958984375, 0.59814453125, 3.134765625, 1.791015625, 1.30078125, 4.03515625, 1.4091796875, 3.36328125, 0.0174407958984375, 0.89697265625, 1.05859375, 3.423828125, 1.2353515625, 0.98388671875, 3.353515625, 0.10882568359375, 1.142578125, 0.44384765625, 2.158203125, 0.03662109375, 0.24169921875, 0.27880859375, 1.521484375, 0.01861572265625, 0.11566162109375, 0.2469482421875, 2.744140625, 0.2127685546875, 0.0003044605255126953, 0.2176513671875, 0.33056640625, 1.869140625, 0.040191650390625, 0.46875, 0.201904296875, 3.306640625, 0.47998046875, 0.438720703125, 2.798828125, 0.00011217594146728516, 0.5869140625, 2.759765625, 0.0011358261108398438, 0.2147216796875, 2.943359375, 0.392822265625, 0.267822265625, 2.490234375, 0.001384735107421875]], "avg_entropy": [1.124119107723236], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pet photography", "score_of_biggest_cluster": 0.3247242271900177, "score_of_first_answer": 0.3247242271900177, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.00048804283142089844, "avg_score": 0.3247242271900177, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38135166706459217, "std_of_first_answer": 0.38135166706459217, "avg_entropy_of_first_answer": 1.124119107723236, "lex_sim": 1.0}
{"question": "Who was the lyre-playing muse of tragedy in Greek mythology?", "answer": ["Melpomene"], "scores": [[0.50244140625, 0.97705078125, 0.9990234375]], "normalized_score": [0.7886047959327698], "tokens": [["\u2581Mel", "pom", "ene"]], "entropy": [[2.3984375, 0.11407470703125, 0.00771331787109375]], "avg_entropy": [0.8400751749674479], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Melpom\u00e8ne", "score_of_biggest_cluster": 0.7886047959327698, "score_of_first_answer": 0.7886047959327698, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.50244140625, "avg_score": 0.7886047959327698, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.22908770038785178, "std_of_first_answer": 0.22908770038785178, "avg_entropy_of_first_answer": 0.8400751749674479, "lex_sim": 1.0}
{"question": "In what year was Band-Aid's Do They Know It's Christmas the UK Christmas chart-topping record (bonus point each for the years of reissue success by Band Aid II and Band Aid 20)?", "answer": ["1984, 2004, 2004"], "scores": [[0.66748046875, 0.9931640625, 0.998046875, 0.9990234375, 0.99951171875, 0.313720703125, 0.814453125, 0.908203125, 0.99853515625, 0.990234375, 0.99853515625, 0.90869140625, 0.45654296875, 0.9990234375, 0.99951171875, 0.97314453125, 0.9912109375]], "normalized_score": [0.8478744626045227], "tokens": [["\u2581", "1", "9", "8", "4", ",", "\u2581", "2", "0", "0", "4", ",", "\u2581", "2", "0", "0", "4"]], "entropy": [[1.3818359375, 0.05059814453125, 0.0206298828125, 0.00789642333984375, 0.00464630126953125, 1.810546875, 1.0224609375, 0.341796875, 0.016021728515625, 0.06854248046875, 0.01401519775390625, 0.474853515625, 0.77099609375, 0.00926971435546875, 0.003139495849609375, 0.156494140625, 0.0670166015625]], "avg_entropy": [0.3659270791446461], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1984", "score_of_biggest_cluster": 0.8478744626045227, "score_of_first_answer": 0.8478744626045227, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.313720703125, "avg_score": 0.8478744626045227, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20231937752394183, "std_of_first_answer": 0.20231937752394183, "avg_entropy_of_first_answer": 0.3659270791446461, "lex_sim": 1.0}
{"question": "Who directed the Noughties films One Day in September, The Last King of Scotland and Touching the Void?", "answer": ["Kevin Macdonald"], "scores": [[0.99560546875, 0.9873046875, 0.9970703125, 1.0]], "normalized_score": [0.9949839115142822], "tokens": [["\u2581Kevin", "\u2581Mac", "don", "ald"]], "entropy": [[0.032318115234375, 0.09771728515625, 0.0234222412109375, 0.0011186599731445312]], "avg_entropy": [0.03864407539367676], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kevin MacDonald (footballer", "score_of_biggest_cluster": 0.9949839115142822, "score_of_first_answer": 0.9949839115142822, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.9873046875, "avg_score": 0.9949839115142822, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.004713558246168902, "std_of_first_answer": 0.004713558246168902, "avg_entropy_of_first_answer": 0.03864407539367676, "lex_sim": 1.0}
{"question": "Mel Gibson (1990) and Kenneth Branagh (1996) have both played which character in movies?", "answer": ["William Wallace ##"], "scores": [[0.9375, 0.88916015625, 1.0, 0.0]], "normalized_score": [0.0], "tokens": [["\u2581William", "\u2581Wal", "lace", "\u2581##"]], "entropy": [[0.405029296875, 0.493896484375, 0.00014543533325195312, 0.94091796875]], "avg_entropy": [0.459997296333313], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Murder of Gonzago", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40988111804818045, "std_of_first_answer": 0.40988111804818045, "avg_entropy_of_first_answer": 0.459997296333313, "lex_sim": 1.0}
{"question": "When running around the athletics track, what does the sound of a bell indicate?", "answer": ["The end of the race"], "scores": [[0.68310546875, 0.43505859375, 0.99560546875, 0.487060546875, 0.4677734375]], "normalized_score": [0.5831069946289062], "tokens": [["\u2581The", "\u2581end", "\u2581of", "\u2581the", "\u2581race"]], "entropy": [[1.44921875, 1.1318359375, 0.03515625, 0.87548828125, 0.97021484375]], "avg_entropy": [0.8923828125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The last lap", "score_of_biggest_cluster": 0.5831069946289062, "score_of_first_answer": 0.5831069946289062, "max_score_of_first_answer": 0.99560546875, "min_score_of_first_answer": 0.43505859375, "avg_score": 0.5831069946289062, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20972065647823562, "std_of_first_answer": 0.20972065647823562, "avg_entropy_of_first_answer": 0.8923828125, "lex_sim": 1.0}
{"question": "What was the former name of The Home Guard", "answer": ["Neighbourhood Watch"], "scores": [[0.0003371238708496094, 0.62353515625, 0.62548828125, 0.998046875, 0.88330078125]], "normalized_score": [0.1632397174835205], "tokens": [["\u2581Ne", "igh", "bour", "hood", "\u2581Watch"]], "entropy": [[1.1162109375, 1.505859375, 0.79150390625, 0.0214996337890625, 0.6494140625]], "avg_entropy": [0.8168975830078125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Local Defence Volunteers", "score_of_biggest_cluster": 0.1632397174835205, "score_of_first_answer": 0.1632397174835205, "max_score_of_first_answer": 0.998046875, "min_score_of_first_answer": 0.0003371238708496094, "avg_score": 0.1632397174835205, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3452775233900052, "std_of_first_answer": 0.3452775233900052, "avg_entropy_of_first_answer": 0.8168975830078125, "lex_sim": 1.0}
{"question": "Where in the body is the cuboid bone?", "answer": ["ITERATION(S)- Chapter 2: The Shadow of the Unseen"], "scores": [[0.0004413127899169922, 5.4717063903808594e-05, 0.156982421875, 0.0017194747924804688, 0.112060546875, 0.00019168853759765625, 7.748603820800781e-07, 0.385009765625, 0.146728515625, 0.25, 0.1856689453125, 0.005615234375, 0.254150390625, 0.306884765625, 0.5458984375, 0.03509521484375, 0.3935546875]], "normalized_score": [0.0171276293694973], "tokens": [["\u2581I", "TER", "ATION", "(", "S", ")-", "\u2581Chapter", "\u2581", "2", ":", "\u2581The", "\u2581Sh", "adow", "\u2581of", "\u2581the", "\u2581Un", "seen"]], "entropy": [[2.173828125, 2.169921875, 3.958984375, 1.783203125, 2.822265625, 0.83203125, 6.0, 3.224609375, 1.9248046875, 3.208984375, 5.13671875, 6.62109375, 2.7734375, 3.361328125, 2.587890625, 4.7734375, 2.25]], "avg_entropy": [3.2707375919117645], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "FOOT", "score_of_biggest_cluster": 0.0171276293694973, "score_of_first_answer": 0.0171276293694973, "max_score_of_first_answer": 0.5458984375, "min_score_of_first_answer": 7.748603820800781e-07, "avg_score": 0.0171276293694973, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.16489441919615697, "std_of_first_answer": 0.16489441919615697, "avg_entropy_of_first_answer": 3.2707375919117645, "lex_sim": 1.0}
{"question": "Prior to the introduction of the Euro, which coin was equal to one-hundredth of a German Mark?", "answer": ["Pfennig"], "scores": [[0.736328125, 0.93408203125, 0.99658203125]], "normalized_score": [0.8817046880722046], "tokens": [["\u2581Pf", "enn", "ig"]], "entropy": [[1.1826171875, 0.283203125, 0.027740478515625]], "avg_entropy": [0.4978535970052083], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u20b0", "score_of_biggest_cluster": 0.8817046880722046, "score_of_first_answer": 0.8817046880722046, "max_score_of_first_answer": 0.99658203125, "min_score_of_first_answer": 0.736328125, "avg_score": 0.8817046880722046, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.11092788105367159, "std_of_first_answer": 0.11092788105367159, "avg_entropy_of_first_answer": 0.4978535970052083, "lex_sim": 1.0}
{"question": "Timothy Q Mouse, Mr Stork and Jim Crow are all characters in which Disney film?", "answer": ["Dumbo"], "scores": [[0.96728515625, 0.99951171875]], "normalized_score": [0.9832664132118225], "tokens": [["\u2581Dum", "bo"]], "entropy": [[0.2261962890625, 0.0059967041015625]], "avg_entropy": [0.11609649658203125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dumbo (film)", "score_of_biggest_cluster": 0.9832664132118225, "score_of_first_answer": 0.9832664132118225, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.96728515625, "avg_score": 0.9832664132118225, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.01611328125, "std_of_first_answer": 0.01611328125, "avg_entropy_of_first_answer": 0.11609649658203125, "lex_sim": 1.0}
{"question": "The excellent cut of beef known as \"hanger steak\" is taken from what part of the animal?", "answer": ["American Cutter"], "scores": [[0.0008454322814941406, 0.0108489990234375, 0.01078033447265625]], "normalized_score": [0.004624166525900364], "tokens": [["\u2581American", "\u2581C", "utter"]], "entropy": [[1.7783203125, 4.22265625, 3.263671875]], "avg_entropy": [3.0882161458333335], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Diafram", "score_of_biggest_cluster": 0.004624166525900364, "score_of_first_answer": 0.004624166525900364, "max_score_of_first_answer": 0.0108489990234375, "min_score_of_first_answer": 0.0008454322814941406, "avg_score": 0.004624166525900364, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.004699625799867321, "std_of_first_answer": 0.004699625799867321, "avg_entropy_of_first_answer": 3.0882161458333335, "lex_sim": 1.0}
{"question": "What was the name of the Argentine Navy cruiser which was torpedoed and sunk during the 1982 Falklands War?", "answer": ["General Belgrano #3: A Look Back at a Memorable 2018, a Review of the Market Forecasts, and Some Thoughts on the Future of the Economy and Investing"], "scores": [[0.89697265625, 0.9990234375, 1.0, 1.0, 3.159046173095703e-06, 0.11224365234375, 5.739927291870117e-05, 0.01245880126953125, 1.5676021575927734e-05, 0.0894775390625, 0.300537109375, 0.036285400390625, 0.00031280517578125, 0.9892578125, 0.99560546875, 0.006633758544921875, 0.284912109375, 0.42236328125, 0.2293701171875, 0.1373291015625, 0.029266357421875, 0.017822265625, 0.003173828125, 0.6240234375, 0.27880859375, 0.0006103515625, 0.00475311279296875, 0.96533203125, 0.55908203125, 0.210205078125, 0.64501953125, 0.0245819091796875, 0.1859130859375, 0.99755859375, 0.99853515625, 0.751953125, 0.418701171875, 0.322265625, 0.413818359375, 0.2734375, 0.0401611328125, 0.90185546875, 0.07281494140625, 0.0919189453125, 0.97998046875, 0.6650390625]], "normalized_score": [0.08240287005901337], "tokens": [["\u2581General", "\u2581Bel", "gr", "ano", "\u2581#", "3", ":", "\u2581A", "\u2581Look", "\u2581Back", "\u2581at", "\u2581a", "\u2581Mem", "or", "able", "\u2581", "2", "0", "1", "8", ",", "\u2581a", "\u2581Review", "\u2581of", "\u2581the", "\u2581Market", "\u2581Fore", "cast", "s", ",", "\u2581and", "\u2581Some", "\u2581Th", "ought", "s", "\u2581on", "\u2581the", "\u2581Future", "\u2581of", "\u2581the", "\u2581Econom", "y", "\u2581and", "\u2581In", "vest", "ing"]], "entropy": [[0.59814453125, 0.00807952880859375, 0.0015668869018554688, 0.0001062154769897461, 0.81103515625, 0.72021484375, 0.79345703125, 4.765625, 2.513671875, 1.8212890625, 2.587890625, 1.20703125, 3.63671875, 0.08294677734375, 0.0369873046875, 4.78125, 1.8173828125, 2.04296875, 1.9814453125, 2.17578125, 5.1328125, 5.9921875, 5.12109375, 1.9375, 4.015625, 3.123046875, 3.318359375, 0.2132568359375, 2.220703125, 3.0078125, 2.220703125, 3.111328125, 4.16796875, 0.021942138671875, 0.01544189453125, 0.92236328125, 2.6484375, 3.302734375, 2.51953125, 3.9609375, 4.44921875, 0.375, 2.88671875, 2.76953125, 0.11456298828125, 0.93212890625]], "avg_entropy": [2.1931421523508816], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Manuel Belgrano", "score_of_biggest_cluster": 0.08240287005901337, "score_of_first_answer": 0.08240287005901337, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 3.159046173095703e-06, "avg_score": 0.08240287005901337, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38173051187266904, "std_of_first_answer": 0.38173051187266904, "avg_entropy_of_first_answer": 2.1931421523508816, "lex_sim": 1.0}
{"question": "In 1940, the RAF made the first raid on which city?", "answer": ["Bremen"], "scores": [[0.004657745361328125, 0.9853515625]], "normalized_score": [0.06774598360061646], "tokens": [["\u2581Bre", "men"]], "entropy": [[2.845703125, 0.110595703125]], "avg_entropy": [1.4781494140625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Berlin-Zentrum", "score_of_biggest_cluster": 0.06774598360061646, "score_of_first_answer": 0.06774598360061646, "max_score_of_first_answer": 0.9853515625, "min_score_of_first_answer": 0.004657745361328125, "avg_score": 0.06774598360061646, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.49034690856933594, "std_of_first_answer": 0.49034690856933594, "avg_entropy_of_first_answer": 1.4781494140625, "lex_sim": 1.0}
{"question": "What is stevia a substitute for?", "answer": ["Aspartame"], "scores": [[0.0008764266967773438, 0.97021484375, 0.9619140625]], "normalized_score": [0.09352044016122818], "tokens": [["\u2581Asp", "art", "ame"]], "entropy": [[1.048828125, 0.2264404296875, 0.2154541015625]], "avg_entropy": [0.4969075520833333], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sugar lumps", "score_of_biggest_cluster": 0.09352044016122818, "score_of_first_answer": 0.09352044016122818, "max_score_of_first_answer": 0.97021484375, "min_score_of_first_answer": 0.0008764266967773438, "avg_score": 0.09352044016122818, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4550066186545294, "std_of_first_answer": 0.4550066186545294, "avg_entropy_of_first_answer": 0.4969075520833333, "lex_sim": 1.0}
{"question": "In terms of tonnage of cargo handled, which is the world's largest port?", "answer": ["Shanghai"], "scores": [[0.301025390625, 0.994140625]], "normalized_score": [0.547048032283783], "tokens": [["\u2581Shang", "hai"]], "entropy": [[2.478515625, 0.044097900390625]], "avg_entropy": [1.2613067626953125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rotterdam South", "score_of_biggest_cluster": 0.547048032283783, "score_of_first_answer": 0.547048032283783, "max_score_of_first_answer": 0.994140625, "min_score_of_first_answer": 0.301025390625, "avg_score": 0.547048032283783, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3465576171875, "std_of_first_answer": 0.3465576171875, "avg_entropy_of_first_answer": 1.2613067626953125, "lex_sim": 1.0}
{"question": "There are two forms of classical Japanese theatre, 'Noh' is mostly spoken, but what name is given to the singing and dancing form?", "answer": ["Kabuki #AskAlan: Is the Stock Market Too High Right Now?"], "scores": [[0.93115234375, 0.99755859375, 2.384185791015625e-07, 0.00235748291015625, 0.345458984375, 0.0009975433349609375, 0.02716064453125, 0.0005626678466796875, 0.0109100341796875, 0.111328125, 6.449222564697266e-05, 0.9345703125, 0.0007696151733398438, 0.9833984375, 0.60546875, 0.09832763671875, 0.99755859375, 0.93212890625]], "normalized_score": [0.02425369992852211], "tokens": [["\u2581Kab", "uki", "\u2581#", "A", "sk", "Al", "an", ":", "\u2581Is", "\u2581the", "\u2581Stock", "\u2581Market", "\u2581To", "o", "\u2581High", "\u2581Right", "\u2581Now", "?"]], "entropy": [[0.43017578125, 0.018157958984375, 0.6796875, 5.171875, 3.125, 5.08984375, 2.22265625, 1.48046875, 4.671875, 3.619140625, 5.48828125, 0.35205078125, 3.830078125, 0.11224365234375, 1.5302734375, 1.55859375, 0.0219879150390625, 0.456787109375]], "avg_entropy": [2.2143987019856772], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u6b4c\u821e\u4f0e", "score_of_biggest_cluster": 0.02425369992852211, "score_of_first_answer": 0.02425369992852211, "max_score_of_first_answer": 0.99755859375, "min_score_of_first_answer": 2.384185791015625e-07, "avg_score": 0.02425369992852211, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.432627442520256, "std_of_first_answer": 0.432627442520256, "avg_entropy_of_first_answer": 2.2143987019856772, "lex_sim": 1.0}
{"question": "The flag of which country prominently features 'The Eagle of Saladin' holding a scroll bearing the name of the country in Arabic?", "answer": ["Jordan"], "scores": [[0.186279296875]], "normalized_score": [0.186279296875], "tokens": [["\u2581Jordan"]], "entropy": [[2.4140625]], "avg_entropy": [2.4140625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "EGY", "score_of_biggest_cluster": 0.186279296875, "score_of_first_answer": 0.186279296875, "max_score_of_first_answer": 0.186279296875, "min_score_of_first_answer": 0.186279296875, "avg_score": 0.186279296875, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.4140625, "lex_sim": 1.0}
{"question": "Who was the first Norwegian group to top the UK charts", "answer": ["A-ha"], "scores": [[0.951171875, 0.9970703125, 0.98828125]], "normalized_score": [0.9786373376846313], "tokens": [["\u2581A", "-", "ha"]], "entropy": [[0.309326171875, 0.03411865234375, 0.07171630859375]], "avg_entropy": [0.13838704427083334], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ah-Ha", "score_of_biggest_cluster": 0.9786373376846313, "score_of_first_answer": 0.9786373376846313, "max_score_of_first_answer": 0.9970703125, "min_score_of_first_answer": 0.951171875, "avg_score": 0.9786373376846313, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.01989142723626587, "std_of_first_answer": 0.01989142723626587, "avg_entropy_of_first_answer": 0.13838704427083334, "lex_sim": 1.0}
{"question": "Why was Korean short-track speedskater Kim Dong-Sung disqualified from the men's 1500 metre final at the 2000 Olympics?", "answer": ["He impeded Canadian athlete Marc Gagnon"], "scores": [[0.270751953125, 0.1173095703125, 0.9912109375, 0.0262908935546875, 0.1636962890625, 1.0, 0.54736328125, 0.9990234375, 0.9912109375, 1.0]], "normalized_score": [0.38600531220436096], "tokens": [["\u2581He", "\u2581imp", "eded", "\u2581Canadian", "\u2581ath", "lete", "\u2581Marc", "\u2581G", "agn", "on"]], "entropy": [[3.23828125, 2.5390625, 0.07244873046875, 2.70703125, 1.3046875, 0.001331329345703125, 2.6171875, 0.01268768310546875, 0.060943603515625, 0.002490997314453125]], "avg_entropy": [1.255615234375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cross-tracking", "score_of_biggest_cluster": 0.38600531220436096, "score_of_first_answer": 0.38600531220436096, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0262908935546875, "avg_score": 0.38600531220436096, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40593569307233923, "std_of_first_answer": 0.40593569307233923, "avg_entropy_of_first_answer": 1.255615234375, "lex_sim": 1.0}
{"question": "Who played Grandad in the UK television series \u2018Only Fools and Horses\u2019?", "answer": ["Lennard Pierce"], "scores": [[0.93994140625, 0.99169921875, 0.99853515625, 0.2032470703125, 1.0]], "normalized_score": [0.7167598605155945], "tokens": [["\u2581L", "enn", "ard", "\u2581Pier", "ce"]], "entropy": [[0.41162109375, 0.0654296875, 0.01111602783203125, 0.97509765625, 8.845329284667969e-05]], "avg_entropy": [0.29267058372497556], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lennard Pearce", "score_of_biggest_cluster": 0.7167598605155945, "score_of_first_answer": 0.7167598605155945, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.2032470703125, "avg_score": 0.7167598605155945, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3125066794635277, "std_of_first_answer": 0.3125066794635277, "avg_entropy_of_first_answer": 0.29267058372497556, "lex_sim": 1.0}
{"question": "Thomas the Tank Engine pulled two coaches. One was Clarabel. What was the other?", "answer": ["Annie and Clarabel"], "scores": [[0.71923828125, 0.9970703125, 0.96240234375, 0.8115234375, 0.9990234375]], "normalized_score": [0.8903615474700928], "tokens": [["\u2581An", "nie", "\u2581and", "\u2581Clar", "abel"]], "entropy": [[1.1025390625, 0.0307159423828125, 0.211669921875, 1.0234375, 0.007781982421875]], "avg_entropy": [0.4752288818359375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Annie (film)", "score_of_biggest_cluster": 0.8903615474700928, "score_of_first_answer": 0.8903615474700928, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.71923828125, "avg_score": 0.8903615474700928, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.11278489385327337, "std_of_first_answer": 0.11278489385327337, "avg_entropy_of_first_answer": 0.4752288818359375, "lex_sim": 1.0}
{"question": "What was the first name of Mr. Ferrari, the founder of the car manufacturing company?", "answer": ["Enzo Ferrari"], "scores": [[0.99951171875, 0.99951171875, 0.034149169921875, 0.9990234375]], "normalized_score": [0.4296678900718689], "tokens": [["\u2581En", "zo", "\u2581Ferr", "ari"]], "entropy": [[0.00608062744140625, 0.004184722900390625, 0.814453125, 0.0112152099609375]], "avg_entropy": [0.2089834213256836], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Enzo", "score_of_biggest_cluster": 0.4296678900718689, "score_of_first_answer": 0.4296678900718689, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.034149169921875, "avg_score": 0.4296678900718689, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.41794381578385104, "std_of_first_answer": 0.41794381578385104, "avg_entropy_of_first_answer": 0.2089834213256836, "lex_sim": 1.0}
{"question": "In 1487, Portuguese navigator Bartholomew Diaz was the first European to sail past which landmark?", "answer": ["Cape of Good Hope \\*The current version of APA does not accept the use of the term \"sexual orientation\" or its use as a category in coding data."], "scores": [[0.599609375, 0.93017578125, 0.97412109375, 0.99267578125, 7.748603820800781e-07, 0.1583251953125, 0.0168914794921875, 6.556510925292969e-05, 0.004924774169921875, 0.96435546875, 0.0006971359252929688, 0.00010257959365844727, 0.0170745849609375, 0.990234375, 0.0070953369140625, 0.1575927734375, 0.1339111328125, 0.99462890625, 0.11944580078125, 0.2413330078125, 0.77392578125, 1.5914440155029297e-05, 0.18212890625, 0.460693359375, 0.82763671875, 0.07843017578125, 0.0165252685546875, 0.250244140625, 0.26953125, 0.68359375, 0.03216552734375, 0.3642578125, 0.008270263671875, 0.1534423828125, 0.486572265625, 0.032318115234375, 0.0003724098205566406, 0.12091064453125, 0.5439453125, 0.677734375, 0.8466796875, 0.91162109375, 0.98095703125, 0.06646728515625, 0.99853515625, 0.8671875, 0.31689453125, 0.09747314453125]], "normalized_score": [0.07385904341936111], "tokens": [["\u2581Cape", "\u2581of", "\u2581Good", "\u2581Hope", "\u2581\\", "*", "The", "\u2581current", "\u2581version", "\u2581of", "\u2581A", "PA", "\u2581does", "\u2581not", "\u2581accept", "\u2581the", "\u2581use", "\u2581of", "\u2581the", "\u2581term", "\u2581\"", "sex", "ual", "\u2581orientation", "\"", "\u2581or", "\u2581its", "\u2581use", "\u2581as", "\u2581a", "\u2581category", "\u2581in", "\u2581coding", "\u2581data", ".", "\u2581A", "IS", "\u2581uses", "\u2581the", "\u2581term", "\u2581\"", "sex", "ual", "\u2581att", "raction", "\"", "\u2581instead", ".\\"]], "entropy": [[1.4453125, 0.49853515625, 0.1622314453125, 0.06463623046875, 1.0146484375, 3.42578125, 3.53515625, 1.1171875, 3.384765625, 0.289794921875, 2.4453125, 0.261474609375, 1.541015625, 0.0872802734375, 2.416015625, 3.751953125, 4.30859375, 0.055816650390625, 3.978515625, 3.83203125, 1.306640625, 3.095703125, 1.3564453125, 2.67578125, 0.8359375, 2.640625, 1.69921875, 2.775390625, 1.603515625, 1.0224609375, 4.14453125, 2.443359375, 4.10546875, 3.498046875, 2.380859375, 3.6015625, 2.76171875, 4.95703125, 2.548828125, 1.7763671875, 0.982421875, 0.53125, 0.1455078125, 2.990234375, 0.0126190185546875, 0.75, 2.15625, 1.98046875]], "avg_entropy": [2.0498812993367515], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cape Point Nature Reserve", "score_of_biggest_cluster": 0.07385904341936111, "score_of_first_answer": 0.07385904341936111, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 7.748603820800781e-07, "avg_score": 0.07385904341936111, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37805425651558305, "std_of_first_answer": 0.37805425651558305, "avg_entropy_of_first_answer": 2.0498812993367515, "lex_sim": 1.0}
{"question": "Which Asian country has the Tugrik as its currency?", "answer": ["MONGO"], "scores": [[0.0035266876220703125, 0.3603515625, 0.98388671875]], "normalized_score": [0.10773235559463501], "tokens": [["\u2581M", "ON", "GO"]], "entropy": [[0.3955078125, 3.1484375, 0.1373291015625]], "avg_entropy": [1.2270914713541667], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mongolia Proper", "score_of_biggest_cluster": 0.10773235559463501, "score_of_first_answer": 0.10773235559463501, "max_score_of_first_answer": 0.98388671875, "min_score_of_first_answer": 0.0035266876220703125, "avg_score": 0.10773235559463501, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.40513727070717337, "std_of_first_answer": 0.40513727070717337, "avg_entropy_of_first_answer": 1.2270914713541667, "lex_sim": 1.0}
{"question": "Which country suspended its annual Antarctic Whale Hunt in February 2011, after activists from a US-based environmental group chased the mother ship?", "answer": ["Japan"], "scores": [[0.87109375]], "normalized_score": [0.87109375], "tokens": [["\u2581Japan"]], "entropy": [[0.69189453125]], "avg_entropy": [0.69189453125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u65e5\u672c\u570b", "score_of_biggest_cluster": 0.87109375, "score_of_first_answer": 0.87109375, "max_score_of_first_answer": 0.87109375, "min_score_of_first_answer": 0.87109375, "avg_score": 0.87109375, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.69189453125, "lex_sim": 1.0}
{"question": "Who was the last inmate of Spandau jail in Berlin?", "answer": ["Albert Speer #6 Georgia 80, Mercer 52"], "scores": [[0.0167236328125, 0.89404296875, 1.0, 3.516674041748047e-06, 0.00946807861328125, 5.364418029785156e-07, 0.047332763671875, 0.034759521484375, 0.2298583984375, 0.01910400390625, 1.3530254364013672e-05, 0.30810546875, 0.0149383544921875, 0.029266357421875, 0.071044921875]], "normalized_score": [0.009116798639297485], "tokens": [["\u2581Albert", "\u2581Spe", "er", "\u2581#", "6", "\u2581Georgia", "\u2581", "8", "0", ",", "\u2581Mer", "cer", "\u2581", "5", "2"]], "entropy": [[1.66796875, 0.72119140625, 0.0008435249328613281, 0.94482421875, 5.0703125, 2.96875, 5.65234375, 3.150390625, 2.9921875, 3.4765625, 4.671875, 2.580078125, 0.89306640625, 1.4140625, 3.61328125]], "avg_entropy": [2.6545158704121907], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hess, Rudolf", "score_of_biggest_cluster": 0.009116798639297485, "score_of_first_answer": 0.009116798639297485, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.364418029785156e-07, "avg_score": 0.009116798639297485, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3141619020582251, "std_of_first_answer": 0.3141619020582251, "avg_entropy_of_first_answer": 2.6545158704121907, "lex_sim": 1.0}
{"question": "What is the better known name of the English singer Yusuf Islam who converted to Islam in 1977 at the height of his fame?", "answer": ["Cat Stevens"], "scores": [[0.96728515625, 0.97216796875]], "normalized_score": [0.9697234630584717], "tokens": [["\u2581Cat", "\u2581Stevens"]], "entropy": [[0.2249755859375, 0.190673828125]], "avg_entropy": [0.20782470703125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cat stevens", "score_of_biggest_cluster": 0.9697234630584717, "score_of_first_answer": 0.9697234630584717, "max_score_of_first_answer": 0.97216796875, "min_score_of_first_answer": 0.96728515625, "avg_score": 0.9697234630584717, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.00244140625, "std_of_first_answer": 0.00244140625, "avg_entropy_of_first_answer": 0.20782470703125, "lex_sim": 1.0}
{"question": "Which US actor played the title role in the television police drama \u2018T J Hooker\u2019?", "answer": ["William Shatner #MeToo Movement Sheds Light on Sexual Harassment Claims Against Kirtan Artists"], "scores": [[0.96533203125, 0.98828125, 0.9990234375, 0.99951171875, 2.384185791015625e-07, 0.0003330707550048828, 0.235595703125, 0.984375, 0.0328369140625, 0.9990234375, 0.001544952392578125, 0.0015716552734375, 0.9775390625, 0.93408203125, 0.026885986328125, 0.95654296875, 0.28759765625, 0.99951171875, 0.9990234375, 0.00701141357421875, 0.99365234375, 0.6064453125, 0.99951171875, 0.00017952919006347656, 0.0233917236328125, 0.0087890625, 0.0158538818359375, 0.05413818359375]], "normalized_score": [0.06683789193630219], "tokens": [["\u2581William", "\u2581Sh", "at", "ner", "\u2581#", "Me", "To", "o", "\u2581Mov", "ement", "\u2581Sh", "eds", "\u2581Light", "\u2581on", "\u2581Sex", "ual", "\u2581Har", "ass", "ment", "\u2581Cla", "ims", "\u2581Again", "st", "\u2581K", "irt", "an", "\u2581Art", "ists"]], "entropy": [[0.1923828125, 0.109375, 0.007598876953125, 0.0049591064453125, 0.5615234375, 5.375, 3.859375, 0.1336669921875, 2.560546875, 0.01140594482421875, 3.654296875, 3.587890625, 0.1650390625, 0.30126953125, 2.794921875, 0.23974609375, 1.353515625, 0.00594329833984375, 0.00797271728515625, 2.359375, 0.043670654296875, 2.080078125, 0.005214691162109375, 2.416015625, 4.2109375, 0.50537109375, 2.513671875, 0.3681640625]], "avg_entropy": [1.4081760134015764], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gloria Rabinowitz", "score_of_biggest_cluster": 0.06683789193630219, "score_of_first_answer": 0.06683789193630219, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 2.384185791015625e-07, "avg_score": 0.06683789193630219, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4634080203714961, "std_of_first_answer": 0.4634080203714961, "avg_entropy_of_first_answer": 1.4081760134015764, "lex_sim": 1.0}
{"question": "John and David Lloyd represented Great Britain in the early 1970s at what sport?", "answer": ["They played badminton together."], "scores": [[0.01336669921875, 0.08465576171875, 0.169189453125, 0.99951171875, 1.0, 0.254150390625, 0.3720703125, 2.0265579223632812e-06, 3.4749507904052734e-05, 0.0007386207580566406, 0.2459716796875, 6.008148193359375e-05, 0.1947021484375, 0.00098419189453125, 0.9775390625, 0.59912109375, 0.0027484893798828125, 0.1781005859375, 0.99462890625, 0.60595703125, 0.09515380859375, 1.0, 0.99951171875, 0.9921875]], "normalized_score": [0.04934616759419441], "tokens": [["\u2581They", "\u2581played", "\u2581bad", "m", "inton", "\u2581together", ".", "\u2581#", "Book", "Re", "view", "\u2581::", "\u2581The", "\u2581Sec", "rets", "\u2581of", "\u2581Life", "\u2581and", "\u2581Death", "\u2581by", "\u2581Reb", "e", "cca", "\u2581Alexander"]], "entropy": [[2.99609375, 1.271484375, 2.833984375, 0.00677490234375, 0.00040984153747558594, 1.7822265625, 1.9326171875, 1.0625, 4.9375, 4.05859375, 1.2275390625, 2.951171875, 4.34765625, 5.546875, 0.150146484375, 1.4892578125, 3.939453125, 3.0, 0.052978515625, 1.556640625, 2.755859375, 0.00032067298889160156, 0.00366973876953125, 0.062347412109375]], "avg_entropy": [1.9985875288645427], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lawn tennis", "score_of_biggest_cluster": 0.04934616759419441, "score_of_first_answer": 0.04934616759419441, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.0265579223632812e-06, "avg_score": 0.04934616759419441, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4105895793224653, "std_of_first_answer": 0.4105895793224653, "avg_entropy_of_first_answer": 1.9985875288645427, "lex_sim": 1.0}
{"question": "On which river does Cardiff stand?", "answer": ["Severn"], "scores": [[0.01351165771484375, 0.99365234375]], "normalized_score": [0.1158701479434967], "tokens": [["\u2581Se", "vern"]], "entropy": [[1.5751953125, 0.06597900390625]], "avg_entropy": [0.820587158203125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Taff", "score_of_biggest_cluster": 0.1158701479434967, "score_of_first_answer": 0.1158701479434967, "max_score_of_first_answer": 0.99365234375, "min_score_of_first_answer": 0.01351165771484375, "avg_score": 0.1158701479434967, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4900703430175781, "std_of_first_answer": 0.4900703430175781, "avg_entropy_of_first_answer": 0.820587158203125, "lex_sim": 1.0}
{"question": "Where is the Headquarters of Boeing?", "answer": ["Seattle"], "scores": [[0.14306640625]], "normalized_score": [0.14306640625], "tokens": [["\u2581Seattle"]], "entropy": [[2.009765625]], "avg_entropy": [2.009765625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Seattle, Washington.", "score_of_biggest_cluster": 0.14306640625, "score_of_first_answer": 0.14306640625, "max_score_of_first_answer": 0.14306640625, "min_score_of_first_answer": 0.14306640625, "avg_score": 0.14306640625, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.009765625, "lex_sim": 1.0}
{"question": "\"Nicknamed the \"\"She Wolf of France\"\", Isabella the wife of Edward II, was from which French royal house?\"", "answer": ["Capetian"], "scores": [[0.87451171875, 0.99267578125, 0.64892578125]], "normalized_score": [0.8258908987045288], "tokens": [["\u2581Cap", "et", "ian"]], "entropy": [[0.6396484375, 0.051300048828125, 1.0478515625]], "avg_entropy": [0.5796000162760416], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Direct Capetians", "score_of_biggest_cluster": 0.8258908987045288, "score_of_first_answer": 0.8258908987045288, "max_score_of_first_answer": 0.99267578125, "min_score_of_first_answer": 0.64892578125, "avg_score": 0.8258908987045288, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1426011622845955, "std_of_first_answer": 0.1426011622845955, "avg_entropy_of_first_answer": 0.5796000162760416, "lex_sim": 1.0}
{"question": "Which Championship Football League club is based at the King Power Stadium?", "answer": ["Leicester Football Club"], "scores": [[0.99755859375, 0.9990234375, 1.0, 0.0001310110092163086, 0.99658203125]], "normalized_score": [0.16705787181854248], "tokens": [["\u2581Le", "ic", "ester", "\u2581Football", "\u2581Club"]], "entropy": [[0.025238037109375, 0.00676727294921875, 0.0007433891296386719, 0.07568359375, 0.0260162353515625]], "avg_entropy": [0.026889705657958986], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Leicester Fosse FC", "score_of_biggest_cluster": 0.16705787181854248, "score_of_first_answer": 0.16705787181854248, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0001310110092163086, "avg_score": 0.16705787181854248, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3992657335636153, "std_of_first_answer": 0.3992657335636153, "avg_entropy_of_first_answer": 0.026889705657958986, "lex_sim": 1.0}
{"question": "Which island produces Marsala wines?", "answer": ["Sicliy"], "scores": [[0.82958984375, 0.00011229515075683594, 0.98779296875]], "normalized_score": [0.0451471284031868], "tokens": [["\u2581Sic", "li", "y"]], "entropy": [[1.1103515625, 0.120849609375, 0.08709716796875]], "avg_entropy": [0.4394327799479167], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Trinacrian", "score_of_biggest_cluster": 0.0451471284031868, "score_of_first_answer": 0.0451471284031868, "max_score_of_first_answer": 0.98779296875, "min_score_of_first_answer": 0.00011229515075683594, "avg_score": 0.0451471284031868, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4331505185384856, "std_of_first_answer": 0.4331505185384856, "avg_entropy_of_first_answer": 0.4394327799479167, "lex_sim": 1.0}
{"question": "Which playing card is called the curse of Scotland", "answer": ["The Nine of Diamonds INTERVIEW: A.J."], "scores": [[0.91015625, 0.4287109375, 0.94287109375, 0.99951171875, 0.277587890625, 0.99951171875, 0.0, 0.00931549072265625, 0.024810791015625, 0.56396484375, 0.98291015625, 0.35400390625, 0.01418304443359375, 0.0007219314575195312, 0.00534820556640625, 0.80517578125, 0.08154296875, 0.87841796875, 0.0011358261108398438, 0.99951171875, 0.0036773681640625, 0.47216796875, 0.440185546875, 0.0019073486328125, 0.003467559814453125, 0.441162109375, 0.08831787109375, 1.0]], "normalized_score": [0.0], "tokens": [["\u2581The", "\u2581N", "ine", "\u2581of", "\u2581Diam", "onds", "\u2581IN", "TER", "V", "IE", "W", ":", "\u2581A", ".", "J", ".", "\u2581Ben", "za", "\u2581Tal", "ks", "\u2581His", "\u2581New", "\u2581Book", "\u2581And", "\u2581M", "ob", "\u2581Mov", "ies"]], "entropy": [[0.67236328125, 2.6015625, 0.345458984375, 0.004421234130859375, 0.7431640625, 0.004398345947265625, 1.361328125, 4.53515625, 1.5947265625, 1.0439453125, 0.10491943359375, 2.134765625, 4.06640625, 1.708984375, 2.732421875, 1.3955078125, 4.13671875, 0.57373046875, 2.423828125, 0.006954193115234375, 3.119140625, 2.974609375, 1.849609375, 1.6650390625, 3.796875, 1.1748046875, 3.166015625, 0.002109527587890625]], "avg_entropy": [1.7835344587053572], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nine of Diamonds", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.40165729567259123, "std_of_first_answer": 0.40165729567259123, "avg_entropy_of_first_answer": 1.7835344587053572, "lex_sim": 1.0}
{"question": "Which film director visited the deepest point on Earth in 2012 ?", "answer": ["James Cameron"], "scores": [[0.990234375, 0.99951171875, 1.0]], "normalized_score": [0.9965718984603882], "tokens": [["\u2581James", "\u2581Camer", "on"]], "entropy": [[0.094482421875, 0.003986358642578125, 0.0005621910095214844]], "avg_entropy": [0.03301032384236654], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "James cameron", "score_of_biggest_cluster": 0.9965718984603882, "score_of_first_answer": 0.9965718984603882, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.990234375, "avg_score": 0.9965718984603882, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.004492895098255944, "std_of_first_answer": 0.004492895098255944, "avg_entropy_of_first_answer": 0.03301032384236654, "lex_sim": 1.0}
{"question": "Ardbeg is whisky distillery on which Scottish island?", "answer": ["Islay"], "scores": [[0.97509765625, 0.8984375]], "normalized_score": [0.9359830617904663], "tokens": [["\u2581Is", "lay"]], "entropy": [[0.1927490234375, 0.342041015625]], "avg_entropy": [0.26739501953125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Isle of Islay", "score_of_biggest_cluster": 0.9359830617904663, "score_of_first_answer": 0.9359830617904663, "max_score_of_first_answer": 0.97509765625, "min_score_of_first_answer": 0.8984375, "avg_score": 0.9359830617904663, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.038330078125, "std_of_first_answer": 0.038330078125, "avg_entropy_of_first_answer": 0.26739501953125, "lex_sim": 1.0}
{"question": "Which comedian had the catch phrase 'It's all done in the best possible taste!'?", "answer": ["Benny Hill #52 Ancestors in 52 Weeks \u2013 Week 2: Lillian Mildred Swinney"], "scores": [[0.0645751953125, 1.0, 0.9990234375, 2.384185791015625e-07, 0.006175994873046875, 0.05645751953125, 1.3589859008789062e-05, 0.0056915283203125, 0.9794921875, 0.1767578125, 0.0740966796875, 0.98291015625, 0.29345703125, 0.99951171875, 0.9326171875, 1.0, 0.00865936279296875, 0.91943359375, 0.9921875, 0.247314453125, 0.082275390625, 0.0154571533203125, 0.330078125, 0.68017578125, 0.01016998291015625, 0.181396484375, 0.9990234375, 0.0032596588134765625, 0.11328125, 0.11474609375]], "normalized_score": [0.07241394370794296], "tokens": [["\u2581Ben", "ny", "\u2581Hill", "\u2581#", "5", "2", "\u2581An", "c", "est", "ors", "\u2581in", "\u2581", "5", "2", "\u2581We", "eks", "\u2581\u2013", "\u2581Week", "\u2581", "2", ":", "\u2581L", "ill", "ian", "\u2581M", "ild", "red", "\u2581Sw", "in", "ney"]], "entropy": [[2.74609375, 0.0004787445068359375, 0.0079498291015625, 0.70947265625, 5.75390625, 3.45703125, 3.548828125, 2.671875, 0.171142578125, 1.087890625, 3.099609375, 0.174072265625, 0.6259765625, 0.006153106689453125, 0.270751953125, 0.0024204254150390625, 1.349609375, 0.6767578125, 0.05291748046875, 1.7939453125, 2.4765625, 4.41796875, 2.666015625, 0.6484375, 5.3203125, 2.310546875, 0.01030731201171875, 5.484375, 2.216796875, 2.40234375]], "avg_entropy": [1.8720183054606119], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kenny Everett", "score_of_biggest_cluster": 0.07241394370794296, "score_of_first_answer": 0.07241394370794296, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.384185791015625e-07, "avg_score": 0.07241394370794296, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4255330507433093, "std_of_first_answer": 0.4255330507433093, "avg_entropy_of_first_answer": 1.8720183054606119, "lex_sim": 1.0}
{"question": "The 53.5 km Milford Track is a long-distance walk in which country?", "answer": ["New Zealand"], "scores": [[0.95849609375, 0.8779296875]], "normalized_score": [0.9173288345336914], "tokens": [["\u2581New", "\u2581Zealand"]], "entropy": [[0.33984375, 0.66943359375]], "avg_entropy": [0.504638671875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "N Z", "score_of_biggest_cluster": 0.9173288345336914, "score_of_first_answer": 0.9173288345336914, "max_score_of_first_answer": 0.95849609375, "min_score_of_first_answer": 0.8779296875, "avg_score": 0.9173288345336914, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.040283203125, "std_of_first_answer": 0.040283203125, "avg_entropy_of_first_answer": 0.504638671875, "lex_sim": 1.0}
{"question": "Who was the Roman goddess of Death?", "answer": ["\u3007\u3007 is a Roman goddess of death, Mors or Letum."], "scores": [[0.0032215118408203125, 0.000125885009765625, 0.97021484375, 0.66455078125, 0.365966796875, 0.99755859375, 0.95947265625, 0.0051422119140625, 0.1181640625, 0.08087158203125, 0.86474609375, 0.9541015625, 0.77880859375, 0.280517578125, 0.215087890625, 0.0208740234375, 0.8251953125, 0.051239013671875, 0.039306640625, 0.53271484375, 0.31591796875, 0.03607177734375, 0.0034694671630859375, 0.9248046875, 0.7685546875, 0.546875, 0.61279296875, 0.043182373046875, 0.8994140625, 0.99169921875, 0.89404296875, 0.99951171875, 0.1668701171875, 0.0390625, 0.7998046875, 1.0, 0.72607421875, 0.496337890625, 0.990234375, 0.048980712890625, 0.9990234375, 0.1541748046875, 0.047698974609375, 0.93212890625, 0.953125, 0.089599609375, 0.8994140625, 0.038360595703125, 0.035308837890625, 0.9404296875]], "normalized_score": [0.20706124603748322], "tokens": [["\u2581", "<0xE3>", "<0x80>", "<0x87>", "<0xE3>", "<0x80>", "<0x87>", "\u2581is", "\u2581a", "\u2581Roman", "\u2581god", "dess", "\u2581of", "\u2581death", ",", "\u2581M", "ors", "\u2581or", "\u2581Let", "um", ".", "\u2581In", "\u2581art", ",", "\u2581she", "\u2581is", "\u2581often", "\u2581shown", "\u2581as", "\u2581a", "\u2581wing", "ed", ",", "\u2581d", "rap", "ed", "\u2581figure", "\u2581with", "\u2581a", "\u2581tor", "ch", ",", "\u2581or", "\u2581as", "\u2581a", "\u2581gr", "im", "\u2581figure", "\u2581leading", "\u2581a"]], "entropy": [[3.021484375, 1.802734375, 0.1973876953125, 1.2119140625, 4.296875, 0.0258941650390625, 0.267822265625, 3.345703125, 2.119140625, 5.01171875, 0.91845703125, 0.3203125, 1.130859375, 1.5439453125, 1.8974609375, 2.43359375, 0.91796875, 2.58984375, 2.3359375, 1.3017578125, 2.0859375, 2.095703125, 2.517578125, 0.431884765625, 1.1171875, 0.96728515625, 1.39453125, 1.041015625, 0.53662109375, 0.07220458984375, 0.642578125, 0.003971099853515625, 2.25, 2.283203125, 0.8623046875, 0.0008220672607421875, 0.89404296875, 1.4150390625, 0.07659912109375, 2.666015625, 0.007595062255859375, 1.2314453125, 3.193359375, 0.435546875, 0.326904296875, 2.630859375, 0.34521484375, 1.078125, 1.0283203125, 0.29248046875]], "avg_entropy": [1.4123037338256836], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "LIBITINA", "score_of_biggest_cluster": 0.20706124603748322, "score_of_first_answer": 0.20706124603748322, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.000125885009765625, "avg_score": 0.20706124603748322, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39840280997396543, "std_of_first_answer": 0.39840280997396543, "avg_entropy_of_first_answer": 1.4123037338256836, "lex_sim": 1.0}
{"question": "Manchester United's Kevin Moran was the first player to he sent off in an FA Cup final, which Arsenal player was the second?", "answer": ["N/A"], "scores": [[0.044891357421875, 0.0154571533203125, 0.98974609375]], "normalized_score": [0.0882277861237526], "tokens": [["\u2581N", "/", "A"]], "entropy": [[3.5703125, 0.81201171875, 0.0904541015625]], "avg_entropy": [1.4909261067708333], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jose Antonio Reyes", "score_of_biggest_cluster": 0.0882277861237526, "score_of_first_answer": 0.0882277861237526, "max_score_of_first_answer": 0.98974609375, "min_score_of_first_answer": 0.0154571533203125, "avg_score": 0.0882277861237526, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.45250608155230426, "std_of_first_answer": 0.45250608155230426, "avg_entropy_of_first_answer": 1.4909261067708333, "lex_sim": 1.0}
{"question": "\"What type of towns often start with the letters \"\"Bad\"\" in Germany? ( Baden-baden, for example )\"", "answer": ["Spa town"], "scores": [[0.32470703125, 0.98046875, 0.050201416015625]], "normalized_score": [0.2518916726112366], "tokens": [["\u2581Sp", "a", "\u2581town"]], "entropy": [[2.830078125, 0.1259765625, 1.048828125]], "avg_entropy": [1.3349609375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Water town", "score_of_biggest_cluster": 0.2518916726112366, "score_of_first_answer": 0.2518916726112366, "max_score_of_first_answer": 0.98046875, "min_score_of_first_answer": 0.050201416015625, "avg_score": 0.2518916726112366, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3902668712634063, "std_of_first_answer": 0.3902668712634063, "avg_entropy_of_first_answer": 1.3349609375, "lex_sim": 1.0}
{"question": "Which British TV character set up a chain of shops called Grot?", "answer": ["Terry and June (TV series)"], "scores": [[0.0087432861328125, 0.1737060546875, 0.9970703125, 0.0330810546875, 0.175537109375, 0.85302734375, 0.9091796875]], "normalized_score": [0.18279655277729034], "tokens": [["\u2581Terry", "\u2581and", "\u2581June", "\u2581(", "TV", "\u2581series", ")"]], "entropy": [[2.74609375, 1.87109375, 0.033538818359375, 2.111328125, 2.7890625, 0.59326171875, 0.367919921875]], "avg_entropy": [1.5017569405691964], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Fall and Rise of Reginald Perrin (2009)", "score_of_biggest_cluster": 0.18279655277729034, "score_of_first_answer": 0.18279655277729034, "max_score_of_first_answer": 0.9970703125, "min_score_of_first_answer": 0.0087432861328125, "avg_score": 0.18279655277729034, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4127884363159043, "std_of_first_answer": 0.4127884363159043, "avg_entropy_of_first_answer": 1.5017569405691964, "lex_sim": 1.0}
{"question": "What number in Bingo is sometimes referred to as Heinz varieties?", "answer": ["7 #205 - The Mere Exposure Effect, Live in Chicago with Michael Covel on Trend Following, and More"], "scores": [[0.354248046875, 0.5234375, 1.5437602996826172e-05, 0.009307861328125, 0.0865478515625, 0.037567138671875, 0.0009150505065917969, 0.021331787109375, 0.00035381317138671875, 0.0007033348083496094, 0.2481689453125, 0.90576171875, 1.0, 0.998046875, 0.0263671875, 8.016824722290039e-05, 0.017974853515625, 0.00875091552734375, 0.0200347900390625, 0.00983428955078125, 0.01499176025390625, 0.99462890625, 0.91162109375, 0.95703125, 0.9765625, 0.99609375, 0.260986328125, 0.0635986328125, 0.00841522216796875]], "normalized_score": [0.03868483379483223], "tokens": [["\u2581", "7", "\u2581#", "2", "0", "5", "\u2581-", "\u2581The", "\u2581M", "ere", "\u2581Ex", "pos", "ure", "\u2581Effect", ",", "\u2581Live", "\u2581in", "\u2581Chicago", "\u2581with", "\u2581Michael", "\u2581Cov", "el", "\u2581on", "\u2581Tr", "end", "\u2581Following", ",", "\u2581and", "\u2581More"]], "entropy": [[2.513671875, 1.2177734375, 1.560546875, 3.53515625, 2.9140625, 2.9375, 2.29296875, 4.671875, 4.578125, 3.595703125, 4.46484375, 0.43359375, 0.00089263916015625, 0.016082763671875, 2.28515625, 5.2734375, 3.9375, 4.73046875, 2.634765625, 5.453125, 2.6171875, 0.037567138671875, 0.4775390625, 0.3291015625, 0.147216796875, 0.0290069580078125, 2.291015625, 5.546875, 5.30078125]], "avg_entropy": [2.614604818409887], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "57", "score_of_biggest_cluster": 0.03868483379483223, "score_of_first_answer": 0.03868483379483223, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.5437602996826172e-05, "avg_score": 0.03868483379483223, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41362935743398166, "std_of_first_answer": 0.41362935743398166, "avg_entropy_of_first_answer": 2.614604818409887, "lex_sim": 1.0}
{"question": "\"The culinary term \"\"Du Barry\"\", indicates the presence of which vegetable?\"", "answer": ["The culinary term \"Du Barry\" typically indicates the presence of a bell pepper."], "scores": [[0.018951416015625, 0.6708984375, 1.0, 0.99658203125, 0.6240234375, 0.99560546875, 0.99951171875, 0.9970703125, 0.98095703125, 0.01490020751953125, 0.262939453125, 0.978515625, 0.99169921875, 0.99951171875, 0.04248046875, 0.006122589111328125, 0.99951171875, 0.99951171875, 0.5400390625, 2.7418136596679688e-06, 0.0005574226379394531, 0.035125732421875, 0.165771484375, 0.99658203125, 0.9765625, 0.0008220672607421875, 0.378173828125, 1.0, 0.54736328125, 0.0025959014892578125, 0.05560302734375, 0.003421783447265625, 0.9853515625, 0.99853515625, 0.86181640625, 0.9775390625, 0.9970703125, 0.1488037109375, 0.009918212890625, 0.0033245086669921875, 0.876953125, 0.56298828125, 0.1890869140625, 0.0046234130859375, 0.05035400390625, 0.822265625, 0.113037109375, 0.0007495880126953125]], "normalized_score": [0.12269359081983566], "tokens": [["\u2581The", "\u2581cul", "inary", "\u2581term", "\u2581\"", "D", "u", "\u2581Barry", "\"", "\u2581typically", "\u2581indicates", "\u2581the", "\u2581presence", "\u2581of", "\u2581a", "\u2581bell", "\u2581pe", "pper", ".", "\u2581M", "ON", "TE", "\u2581C", "AR", "LO", "\u2581Y", "AC", "HT", "S", "\u2581AND", "\u2581S", "UST", "A", "IN", "AB", "IL", "ITY", ":", "\u2581THE", "\u2581ST", "ORY", "\u2581OF", "\u2581THE", "\u2581\u201c", "E", "CO", "\u2581T", "UN"]], "entropy": [[4.0703125, 2.095703125, 0.0005030632019042969, 0.0289154052734375, 0.7724609375, 0.040924072265625, 0.0033111572265625, 0.0232086181640625, 0.11273193359375, 1.05078125, 0.837890625, 0.114013671875, 0.060089111328125, 0.0034637451171875, 3.0234375, 4.1953125, 0.007335662841796875, 0.002765655517578125, 1.3662109375, 0.98486328125, 3.552734375, 3.568359375, 3.357421875, 0.02520751953125, 0.1522216796875, 2.693359375, 2.81640625, 0.0008435249328613281, 2.162109375, 3.080078125, 3.921875, 1.189453125, 0.12646484375, 0.016876220703125, 0.46337890625, 0.1544189453125, 0.025634765625, 4.515625, 5.0390625, 3.40234375, 0.70947265625, 1.7783203125, 3.015625, 3.853515625, 4.515625, 0.955078125, 2.244140625, 0.90966796875]], "avg_entropy": [1.6049907207489014], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Purple cauliflower", "score_of_biggest_cluster": 0.12269359081983566, "score_of_first_answer": 0.12269359081983566, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.7418136596679688e-06, "avg_score": 0.12269359081983566, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43535678723564364, "std_of_first_answer": 0.43535678723564364, "avg_entropy_of_first_answer": 1.6049907207489014, "lex_sim": 1.0}
{"question": "Which ex-World Heavyweight Champion boxer signed up for Celebrity Big Brother 2014?", "answer": ["George Foreman \\r\\nDr."], "scores": [[0.054840087890625, 0.9697265625, 0.998046875, 8.52346420288086e-06, 0.01471710205078125, 0.69775390625, 0.99462890625, 2.1457672119140625e-06, 0.6796875, 0.00030159950256347656, 2.8133392333984375e-05, 0.03369140625, 0.003753662109375, 0.00021219253540039062, 0.90380859375, 0.08209228515625, 0.1375732421875, 0.2578125, 0.00313568115234375, 0.49609375, 0.144287109375, 0.43505859375, 0.08453369140625, 1.0, 0.37109375, 0.86376953125, 0.8759765625, 1.0, 0.2276611328125, 0.30419921875, 0.291259765625, 0.2154541015625, 0.055145263671875, 0.0008335113525390625, 0.07421875, 0.931640625, 0.96533203125, 0.83349609375, 0.039306640625, 0.94091796875, 0.95361328125, 0.98876953125, 1.0, 0.99951171875, 0.9892578125, 0.9892578125, 0.220458984375]], "normalized_score": [0.09472253173589706], "tokens": [["\u2581George", "\u2581For", "eman", "\u2581\\", "r", "\\", "n", "Dr", ".", "\u2581R", "iba", "udo", "\u2581has", "\u2581test", "ified", "\u2581before", "\u2581Congress", "\u2581and", "\u2581serves", "\u2581on", "\u2581several", "\u2581national", "\u2581bo", "ards", "\u2581and", "\u2581comm", "itte", "es", ".", "\u2581He", "\u2581has", "\u2581been", "\u2581an", "\u2581elected", "\u2581Fellow", "\u2581of", "\u2581the", "\u2581American", "\u2581Association", "\u2581for", "\u2581the", "\u2581Ad", "van", "cement", "\u2581of", "\u2581Science", "\u2581since"]], "entropy": [[1.9873046875, 0.2410888671875, 0.0192718505859375, 0.8798828125, 2.748046875, 0.8408203125, 0.042327880859375, 1.1865234375, 1.5849609375, 2.83984375, 2.263671875, 2.751953125, 2.859375, 4.8984375, 0.6357421875, 2.341796875, 2.125, 2.41796875, 3.703125, 0.8125, 2.08984375, 2.904296875, 3.193359375, 0.00228118896484375, 2.3984375, 0.89013671875, 0.38720703125, 0.0010662078857421875, 2.474609375, 3.142578125, 1.9833984375, 3.322265625, 3.376953125, 2.548828125, 2.150390625, 0.40869140625, 0.25537109375, 1.0048828125, 1.37109375, 0.289794921875, 0.3291015625, 0.08526611328125, 0.0007877349853515625, 0.00424957275390625, 0.0693359375, 0.08599853515625, 1.5703125]], "avg_entropy": [1.5642591435858544], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Evander Hollyfield", "score_of_biggest_cluster": 0.09472253173589706, "score_of_first_answer": 0.09472253173589706, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.1457672119140625e-06, "avg_score": 0.09472253173589706, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4137699996207943, "std_of_first_answer": 0.4137699996207943, "avg_entropy_of_first_answer": 1.5642591435858544, "lex_sim": 1.0}
{"question": "What is the Latin for wolf spider?", "answer": ["Lycosidae"], "scores": [[0.289794921875, 0.92822265625, 0.9794921875]], "normalized_score": [0.6410835385322571], "tokens": [["\u2581Ly", "cos", "idae"]], "entropy": [[3.48828125, 0.283203125, 0.14013671875]], "avg_entropy": [1.3038736979166667], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Www.lycos.com", "score_of_biggest_cluster": 0.6410835385322571, "score_of_first_answer": 0.6410835385322571, "max_score_of_first_answer": 0.9794921875, "min_score_of_first_answer": 0.289794921875, "avg_score": 0.6410835385322571, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3137410211312636, "std_of_first_answer": 0.3137410211312636, "avg_entropy_of_first_answer": 1.3038736979166667, "lex_sim": 1.0}
{"question": "What was the title of the musical based on the TV programme 'X Factor?", "answer": ["I Can't Sing!"], "scores": [[0.73486328125, 0.69873046875, 0.99853515625, 1.0, 0.99951171875, 0.841796875]], "normalized_score": [0.8692522048950195], "tokens": [["\u2581I", "\u2581Can", "'", "t", "\u2581Sing", "!"]], "entropy": [[0.9599609375, 1.185546875, 0.01529693603515625, 0.0004417896270751953, 0.0041961669921875, 0.5791015625]], "avg_entropy": [0.4574240446090698], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The X Factor Musical", "score_of_biggest_cluster": 0.8692522048950195, "score_of_first_answer": 0.8692522048950195, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.69873046875, "avg_score": 0.8692522048950195, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.12787314371431618, "std_of_first_answer": 0.12787314371431618, "avg_entropy_of_first_answer": 0.4574240446090698, "lex_sim": 1.0}
{"question": "Major asshat Idi Amin Dada was the president of what African nation from 1971 to 1979?", "answer": ["ISO 3166-1:UG"], "scores": [[1.7881393432617188e-07, 0.472412109375, 0.137451171875, 0.986328125, 0.998046875, 0.9990234375, 0.97705078125, 0.95166015625, 0.021820068359375, 0.208740234375]], "normalized_score": [0.09299255907535553], "tokens": [["\u2581ISO", "\u2581", "3", "1", "6", "6", "-", "1", ":", "UG"]], "entropy": [[0.332275390625, 3.080078125, 1.103515625, 0.1138916015625, 0.0203857421875, 0.0119476318359375, 0.1878662109375, 0.254638671875, 2.4609375, 2.2109375]], "avg_entropy": [0.9776473999023437], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ugandese", "score_of_biggest_cluster": 0.09299255907535553, "score_of_first_answer": 0.09299255907535553, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 1.7881393432617188e-07, "avg_score": 0.09299255907535553, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42476166518295394, "std_of_first_answer": 0.42476166518295394, "avg_entropy_of_first_answer": 0.9776473999023437, "lex_sim": 1.0}
{"question": "Leslie Poles were the Christian names of which English novelist, born in 1895?", "answer": ["LESPOLES"], "scores": [[0.0017490386962890625, 0.041595458984375, 2.8908252716064453e-05, 0.65283203125]], "normalized_score": [0.006087192799896002], "tokens": [["\u2581L", "ES", "PO", "LES"]], "entropy": [[3.8203125, 1.041015625, 0.85107421875, 1.58984375]], "avg_entropy": [1.8255615234375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "L.P. HARTLEY", "score_of_biggest_cluster": 0.006087192799896002, "score_of_first_answer": 0.006087192799896002, "max_score_of_first_answer": 0.65283203125, "min_score_of_first_answer": 2.8908252716064453e-05, "avg_score": 0.006087192799896002, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2769239067484264, "std_of_first_answer": 0.2769239067484264, "avg_entropy_of_first_answer": 1.8255615234375, "lex_sim": 1.0}
{"question": "Which Australian prime minister was said to have tweaked the Queen's bra strap", "answer": ["That is a myth."], "scores": [[0.00270843505859375, 0.42041015625, 0.72119140625, 0.01425933837890625, 0.1475830078125, 7.152557373046875e-07, 6.091594696044922e-05, 0.94287109375, 0.642578125, 0.99951171875, 0.9931640625, 0.03143310546875, 0.1124267578125, 9.047985076904297e-05, 0.64794921875, 0.990234375, 0.0034275054931640625, 0.11273193359375, 0.5517578125, 0.99951171875]], "normalized_score": [0.04622004181146622], "tokens": [["\u2581That", "\u2581is", "\u2581a", "\u2581myth", ".", "\u2581#", "Throw", "back", "Th", "urs", "day", ":", "\u2581The", "\u2581E", "volution", "\u2581of", "\u2581G", "aming", "\u2581Cons", "oles"]], "entropy": [[3.76171875, 1.427734375, 0.97705078125, 3.025390625, 1.33984375, 1.216796875, 4.46484375, 0.32080078125, 1.6884765625, 0.00611114501953125, 0.04779052734375, 2.23828125, 4.890625, 2.00390625, 1.7353515625, 0.0743408203125, 4.33203125, 2.283203125, 2.10546875, 0.003101348876953125]], "avg_entropy": [1.8971433639526367], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paul John Keating", "score_of_biggest_cluster": 0.04622004181146622, "score_of_first_answer": 0.04622004181146622, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 7.152557373046875e-07, "avg_score": 0.04622004181146622, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40396032801050674, "std_of_first_answer": 0.40396032801050674, "avg_entropy_of_first_answer": 1.8971433639526367, "lex_sim": 1.0}
{"question": "English novelist Barbara McCorquodale was better known by what name?", "answer": ["Jean Plaidy"], "scores": [[0.0207366943359375, 0.9345703125, 1.0, 1.0]], "normalized_score": [0.3731108605861664], "tokens": [["\u2581Jean", "\u2581Pla", "id", "y"]], "entropy": [[2.220703125, 0.4111328125, 0.00020444393157958984, 0.0004897117614746094]], "avg_entropy": [0.6581325232982635], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Barbara Hamilton Cartland", "score_of_biggest_cluster": 0.3731108605861664, "score_of_first_answer": 0.3731108605861664, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0207366943359375, "avg_score": 0.3731108605861664, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.415449095357943, "std_of_first_answer": 0.415449095357943, "avg_entropy_of_first_answer": 0.6581325232982635, "lex_sim": 1.0}
{"question": "Which is the only US state to begin with the letter \u2018U\u2019?", "answer": ["None exist \\r\\nKathy is a long-time resident of Berkeley and an experienced activist and organizer, with a background in environmental and social justice work."], "scores": [[0.2314453125, 0.00566864013671875, 7.152557373046875e-07, 0.00018739700317382812, 0.830078125, 0.95166015625, 5.716085433959961e-05, 0.0009431838989257812, 0.1800537109375, 0.0122528076171875, 0.10821533203125, 0.0003693103790283203, 0.389892578125, 0.40087890625, 0.298828125, 0.931640625, 0.00032329559326171875, 0.87109375, 0.123291015625, 0.0278472900390625, 0.035888671875, 0.01263427734375, 0.99853515625, 0.056060791015625, 0.61181640625, 0.998046875, 0.07733154296875, 0.12335205078125, 0.87841796875, 0.1927490234375, 0.92822265625, 0.1240234375, 0.39599609375, 0.77880859375, 0.9912109375, 0.07183837890625, 0.8623046875, 0.60693359375, 0.29833984375, 0.56201171875, 0.266357421875, 0.267333984375, 0.9482421875, 0.004322052001953125, 0.05364990234375, 0.99951171875, 0.374267578125, 0.11572265625]], "normalized_score": [0.08261733502149582], "tokens": [["\u2581None", "\u2581exist", "\u2581\\", "r", "\\", "n", "K", "ath", "y", "\u2581is", "\u2581a", "\u2581long", "-", "time", "\u2581resident", "\u2581of", "\u2581Ber", "keley", "\u2581and", "\u2581an", "\u2581experienced", "\u2581activ", "ist", "\u2581and", "\u2581organ", "izer", ",", "\u2581with", "\u2581a", "\u2581background", "\u2581in", "\u2581environmental", "\u2581and", "\u2581social", "\u2581justice", "\u2581work", ".", "\u2581She", "\u2581has", "\u2581been", "\u2581a", "\u2581leader", "\u2581in", "\u2581numerous", "\u2581progress", "ive", "\u2581causes", "\u2581over"]], "entropy": [[1.4033203125, 1.6455078125, 1.6865234375, 2.134765625, 0.7431640625, 0.364501953125, 2.90625, 4.046875, 2.123046875, 5.08984375, 3.525390625, 4.6796875, 2.748046875, 2.37890625, 4.19921875, 0.4365234375, 5.04296875, 0.73291015625, 1.7861328125, 4.3046875, 3.625, 5.2890625, 0.01194000244140625, 2.169921875, 2.271484375, 0.01508331298828125, 1.5283203125, 3.33984375, 0.7490234375, 2.705078125, 0.348876953125, 3.701171875, 2.19140625, 1.21484375, 0.06402587890625, 1.388671875, 0.6298828125, 2.05859375, 2.22265625, 1.986328125, 2.31640625, 2.857421875, 0.311767578125, 0.8955078125, 2.353515625, 0.00344085693359375, 1.8037109375, 1.439453125]], "avg_entropy": [2.1139731407165527], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Utah (State)", "score_of_biggest_cluster": 0.08261733502149582, "score_of_first_answer": 0.08261733502149582, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 7.152557373046875e-07, "avg_score": 0.08261733502149582, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3688985500056595, "std_of_first_answer": 0.3688985500056595, "avg_entropy_of_first_answer": 2.1139731407165527, "lex_sim": 1.0}
{"question": "Which island group in the South Atlantic was once known as the Sebald Islands?", "answer": ["Falkland Islands"], "scores": [[0.42822265625, 0.99951171875, 0.9306640625, 0.85009765625]], "normalized_score": [0.7628335952758789], "tokens": [["\u2581F", "alk", "land", "\u2581Islands"]], "entropy": [[1.697265625, 0.0030345916748046875, 0.258544921875, 0.66552734375]], "avg_entropy": [0.6560931205749512], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Malvinas/Falklands", "score_of_biggest_cluster": 0.7628335952758789, "score_of_first_answer": 0.7628335952758789, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.42822265625, "avg_score": 0.7628335952758789, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.22225444075028042, "std_of_first_answer": 0.22225444075028042, "avg_entropy_of_first_answer": 0.6560931205749512, "lex_sim": 1.0}
{"question": "Which singer was nicknamed the Walrus of Love?", "answer": ["Singer-songwriters' record sales #236 / Mendocino"], "scores": [[0.00728607177734375, 0.69384765625, 0.05364990234375, 0.986328125, 0.0008029937744140625, 0.421630859375, 0.0023708343505859375, 0.00098419189453125, 0.07598876953125, 0.00010114908218383789, 0.149658203125, 0.053314208984375, 0.03619384765625, 0.00044155120849609375, 0.000762939453125, 0.0006604194641113281, 0.086181640625, 0.9931640625]], "normalized_score": [0.016576755791902542], "tokens": [["\u2581S", "inger", "-", "song", "writ", "ers", "'", "\u2581record", "\u2581sales", "\u2581#", "2", "3", "6", "\u2581/", "\u2581M", "end", "oc", "ino"]], "entropy": [[3.177734375, 1.30078125, 4.1640625, 0.10345458984375, 0.022491455078125, 1.748046875, 2.923828125, 4.171875, 2.10546875, 3.87109375, 2.89453125, 3.677734375, 4.12109375, 4.38671875, 4.73828125, 4.55859375, 1.201171875, 0.0594482421875]], "avg_entropy": [2.734800550672743], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Walrus Of Love", "score_of_biggest_cluster": 0.016576755791902542, "score_of_first_answer": 0.016576755791902542, "max_score_of_first_answer": 0.9931640625, "min_score_of_first_answer": 0.00010114908218383789, "avg_score": 0.016576755791902542, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.329619735106662, "std_of_first_answer": 0.329619735106662, "avg_entropy_of_first_answer": 2.734800550672743, "lex_sim": 1.0}
{"question": "What is the capital of the US state of Georgia?", "answer": ["\u7c73 #IrishAmerica: Fashion Fusion"], "scores": [[0.0016345977783203125, 1.1205673217773438e-05, 5.0067901611328125e-06, 8.45193862915039e-05, 0.0009150505065917969, 0.215087890625, 0.0001932382583618164, 0.01490020751953125, 0.00449371337890625, 0.00191497802734375, 0.002635955810546875, 0.2705078125]], "normalized_score": [0.0013158804504200816], "tokens": [["\u2581", "\u7c73", "\u2581#", "I", "r", "ish", "America", ":", "\u2581F", "ashion", "\u2581F", "usion"]], "entropy": [[0.81103515625, 2.744140625, 4.26171875, 3.744140625, 3.615234375, 4.16015625, 4.82421875, 3.7421875, 4.88671875, 3.51953125, 4.53125, 3.294921875]], "avg_entropy": [3.6779378255208335], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marthasville, GA", "score_of_biggest_cluster": 0.0013158804504200816, "score_of_first_answer": 0.0013158804504200816, "max_score_of_first_answer": 0.2705078125, "min_score_of_first_answer": 5.0067901611328125e-06, "avg_score": 0.0013158804504200816, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09028451673945183, "std_of_first_answer": 0.09028451673945183, "avg_entropy_of_first_answer": 3.6779378255208335, "lex_sim": 1.0}
{"question": "Captain of the Flying Dustman, what was the name of Captain Pugwash's mortal enemy?", "answer": ["Cut-throat Dear Friend & Supporter of the Gospel,"], "scores": [[0.77685546875, 0.99560546875, 0.9091796875, 0.471923828125, 1.0, 1.0, 1.7285346984863281e-06, 0.00036025047302246094, 5.543231964111328e-06, 0.00016939640045166016, 0.00010603666305541992, 0.59814453125, 0.319091796875, 0.366455078125, 0.006664276123046875, 0.01084136962890625, 0.2227783203125]], "normalized_score": [0.020681258291006088], "tokens": [["\u2581C", "ut", "-", "th", "ro", "at", "\u2581D", "ear", "\u2581Friend", "\u2581&", "\u2581Supp", "orter", "\u2581of", "\u2581the", "\u2581G", "ospel", ","]], "entropy": [[0.95703125, 0.037841796875, 0.435546875, 0.70361328125, 0.0015392303466796875, 0.00012117624282836914, 0.1387939453125, 3.71484375, 2.833984375, 2.298828125, 4.3203125, 1.1689453125, 2.576171875, 2.96875, 3.728515625, 4.12109375, 3.357421875]], "avg_entropy": [1.962550278972177], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cut-throat Jake", "score_of_biggest_cluster": 0.020681258291006088, "score_of_first_answer": 0.020681258291006088, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.7285346984863281e-06, "avg_score": 0.020681258291006088, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3958842237858816, "std_of_first_answer": 0.3958842237858816, "avg_entropy_of_first_answer": 1.962550278972177, "lex_sim": 1.0}
{"question": "Who became the host of the UK television game show Blankety Blank in 1984?", "answer": ["Terence Alexander"], "scores": [[0.000751495361328125, 0.95263671875, 0.6220703125]], "normalized_score": [0.07636558264493942], "tokens": [["\u2581Ter", "ence", "\u2581Alexander"]], "entropy": [[0.82861328125, 0.252197265625, 2.36328125]], "avg_entropy": [1.1480305989583333], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Les Dawson", "score_of_biggest_cluster": 0.07636558264493942, "score_of_first_answer": 0.07636558264493942, "max_score_of_first_answer": 0.95263671875, "min_score_of_first_answer": 0.000751495361328125, "avg_score": 0.07636558264493942, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3946020076506539, "std_of_first_answer": 0.3946020076506539, "avg_entropy_of_first_answer": 1.1480305989583333, "lex_sim": 1.0}
{"question": "What was made and repaired by a Wainwright?", "answer": ["Roads and tracks in the British Lake District"], "scores": [[0.005565643310546875, 0.0626220703125, 0.337158203125, 0.0675048828125, 0.01447296142578125, 0.22998046875, 0.01398468017578125, 6.890296936035156e-05, 0.98681640625]], "normalized_score": [0.03082231618463993], "tokens": [["\u2581Ro", "ads", "\u2581and", "\u2581tracks", "\u2581in", "\u2581the", "\u2581British", "\u2581Lake", "\u2581District"]], "entropy": [[1.9873046875, 0.25244140625, 1.513671875, 3.177734375, 1.541015625, 2.26953125, 1.7724609375, 0.69775390625, 0.095458984375]], "avg_entropy": [1.4785970052083333], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wagon", "score_of_biggest_cluster": 0.03082231618463993, "score_of_first_answer": 0.03082231618463993, "max_score_of_first_answer": 0.98681640625, "min_score_of_first_answer": 6.890296936035156e-05, "avg_score": 0.03082231618463993, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30208058417646494, "std_of_first_answer": 0.30208058417646494, "avg_entropy_of_first_answer": 1.4785970052083333, "lex_sim": 1.0}
{"question": "The king of what country became Iceland's ruler by treaty in 1262?", "answer": ["Haakon Haakonsson of Norway PHIL JAYHANIAN"], "scores": [[0.025421142578125, 0.99755859375, 0.99853515625, 0.006320953369140625, 0.99951171875, 0.6376953125, 0.9609375, 0.310302734375, 0.9990234375, 3.5762786865234375e-07, 0.0077362060546875, 0.0033283233642578125, 0.0008883476257324219, 0.8740234375, 0.9833984375, 0.4052734375, 0.96044921875, 0.006198883056640625, 0.0278167724609375]], "normalized_score": [0.0627647414803505], "tokens": [["\u2581Ha", "ak", "on", "\u2581Ha", "ak", "ons", "son", "\u2581of", "\u2581Norway", "\u2581P", "H", "IL", "\u2581J", "A", "Y", "H", "AN", "I", "AN"]], "entropy": [[0.767578125, 0.023284912109375, 0.01568603515625, 0.460693359375, 0.0039520263671875, 0.689453125, 0.19677734375, 1.45703125, 0.0108642578125, 0.9609375, 4.453125, 1.890625, 0.82275390625, 0.70361328125, 0.1104736328125, 2.689453125, 0.260498046875, 1.3662109375, 2.427734375]], "avg_entropy": [1.0163550125925165], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Norv\u00e8ge", "score_of_biggest_cluster": 0.0627647414803505, "score_of_first_answer": 0.0627647414803505, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 3.5762786865234375e-07, "avg_score": 0.0627647414803505, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44579093791873375, "std_of_first_answer": 0.44579093791873375, "avg_entropy_of_first_answer": 1.0163550125925165, "lex_sim": 1.0}
{"question": "Who had hits with Go Now and Nights in White Satin?", "answer": ["The Moody Blues"], "scores": [[0.99169921875, 0.99609375, 0.99951171875, 0.9970703125]], "normalized_score": [0.9960897564888], "tokens": [["\u2581The", "\u2581Mo", "ody", "\u2581Blues"]], "entropy": [[0.0772705078125, 0.04193115234375, 0.0035400390625, 0.025238037109375]], "avg_entropy": [0.03699493408203125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Moody Blues", "score_of_biggest_cluster": 0.9960897564888, "score_of_first_answer": 0.9960897564888, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.99169921875, "avg_score": 0.9960897564888, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.00282613205634527, "std_of_first_answer": 0.00282613205634527, "avg_entropy_of_first_answer": 0.03699493408203125, "lex_sim": 1.0}
{"question": "In Kipling's 'The Jungle Book' what kind of creature is Hathi?", "answer": ["Elephant"], "scores": [[0.168212890625, 0.9951171875, 0.99267578125]], "normalized_score": [0.5497691035270691], "tokens": [["\u2581Ele", "ph", "ant"]], "entropy": [[1.8095703125, 0.04168701171875, 0.045623779296875]], "avg_entropy": [0.632293701171875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Elephant (disambiguation)", "score_of_biggest_cluster": 0.5497691035270691, "score_of_first_answer": 0.5497691035270691, "max_score_of_first_answer": 0.9951171875, "min_score_of_first_answer": 0.168212890625, "avg_score": 0.5497691035270691, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38923225495448033, "std_of_first_answer": 0.38923225495448033, "avg_entropy_of_first_answer": 0.632293701171875, "lex_sim": 1.0}
{"question": "Which admiral commanded the British fleet sent to repel the Spanish Armada in 1588?", "answer": ["\u590f\u5a01\u5937\u5927 \\\");\"; var dataplay = $(\".video-player\").data(\"videoplayer\"); if (typeof dataplay == 'object') { dataplay.pause(); }"], "scores": [[0.0006284713745117188, 3.5762786865234375e-07, 0.258544921875, 0.99951171875, 0.94873046875, 0.98046875, 0.97412109375, 1.0, 0.0028820037841796875, 2.5212764739990234e-05, 7.569789886474609e-06, 0.005527496337890625, 0.475341796875, 0.00605010986328125, 0.05767822265625, 0.447998046875, 0.1741943359375, 0.005519866943359375, 0.034393310546875, 0.74658203125, 0.2091064453125, 0.48828125, 0.3525390625, 0.9970703125, 0.03948974609375, 0.849609375, 0.85400390625, 0.99462890625, 0.9462890625, 0.379638671875, 0.70263671875, 0.054840087890625, 0.54345703125, 1.0, 0.9951171875, 0.077392578125, 0.6328125, 0.466064453125, 0.99462890625, 0.7822265625, 0.74267578125, 1.0, 0.99658203125, 0.97998046875, 0.0117645263671875, 0.98193359375, 0.9619140625, 0.66845703125]], "normalized_score": [0.1291503608226776], "tokens": [["\u2581", "\u590f", "<0xE5>", "<0xA8>", "<0x81>", "<0xE5>", "<0xA4>", "<0xB7>", "\u5927", "\u2581\\", "\");", "\";", "\u2581var", "\u2581dat", "ap", "lay", "\u2581=", "\u2581$(\".", "video", "-", "player", "\").", "data", "(\"", "v", "ide", "op", "layer", "\");", "\u2581if", "\u2581(", "typeof", "\u2581dat", "ap", "lay", "\u2581==", "\u2581'", "object", "')", "\u2581{", "\u2581dat", "ap", "lay", ".", "p", "ause", "();", "\u2581}"]], "entropy": [[1.720703125, 2.517578125, 4.07421875, 0.0032596588134765625, 0.257568359375, 0.1693115234375, 0.1378173828125, 0.0016107559204101562, 4.390625, 4.5078125, 2.404296875, 3.111328125, 3.51171875, 4.859375, 2.16015625, 2.69140625, 2.62890625, 3.6796875, 5.18359375, 1.359375, 3.22265625, 1.626953125, 3.3125, 0.0232391357421875, 2.771484375, 0.66162109375, 0.90380859375, 0.039398193359375, 0.279296875, 2.0078125, 0.92236328125, 0.4677734375, 2.083984375, 0.00025582313537597656, 0.037933349609375, 1.333984375, 0.92333984375, 0.99951171875, 0.036712646484375, 0.78125, 1.2880859375, 0.000865936279296875, 0.026885986328125, 0.138671875, 3.62890625, 0.138916015625, 0.2322998046875, 1.263671875]], "avg_entropy": [1.6359277417262394], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Earl of Effingham", "score_of_biggest_cluster": 0.1291503608226776, "score_of_first_answer": 0.1291503608226776, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 3.5762786865234375e-07, "avg_score": 0.1291503608226776, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4014448426682594, "std_of_first_answer": 0.4014448426682594, "avg_entropy_of_first_answer": 1.6359277417262394, "lex_sim": 1.0}
{"question": "In what country is 'Mt. Everest'?", "answer": ["In Nepal, on the border with Tibet"], "scores": [[0.0025310516357421875, 0.83984375, 0.99951171875, 0.1893310546875, 0.56396484375, 0.99609375, 0.98681640625, 0.94287109375, 0.662109375, 1.0]], "normalized_score": [0.4114982485771179], "tokens": [["\u2581In", "\u2581Nep", "al", ",", "\u2581on", "\u2581the", "\u2581border", "\u2581with", "\u2581Tib", "et"]], "entropy": [[0.350830078125, 0.54150390625, 0.0031490325927734375, 1.537109375, 2.205078125, 0.03216552734375, 0.09588623046875, 0.259765625, 0.7060546875, 0.002285003662109375]], "avg_entropy": [0.5733827590942383], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nepol", "score_of_biggest_cluster": 0.4114982485771179, "score_of_first_answer": 0.4114982485771179, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0025310516357421875, "avg_score": 0.4114982485771179, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34525649405123393, "std_of_first_answer": 0.34525649405123393, "avg_entropy_of_first_answer": 0.5733827590942383, "lex_sim": 1.0}
{"question": "Where does Dame Edna Everage come from in Australia", "answer": ["Melbourne, Victoria #Sustainability and the Circular Economy: A Conversation with Emanuel Pi\u00f1eiro-Lo\u0301pez"], "scores": [[0.48046875, 0.64306640625, 0.78271484375, 1.1920928955078125e-07, 0.0019044876098632812, 6.628036499023438e-05, 0.857421875, 0.249267578125, 0.0043182373046875, 0.049957275390625, 0.11456298828125, 0.99951171875, 0.958984375, 1.0, 0.0838623046875, 0.09637451171875, 0.0111083984375, 0.97900390625, 0.99365234375, 0.783203125, 0.003307342529296875, 0.0019292831420898438, 0.01654052734375, 0.2491455078125, 0.92529296875, 0.98388671875, 0.00614166259765625, 0.232666015625, 0.6533203125, 0.990234375]], "normalized_score": [0.07049889117479324], "tokens": [["\u2581Melbourne", ",", "\u2581Victoria", "\u2581#", "S", "ust", "ain", "ability", "\u2581and", "\u2581the", "\u2581Circ", "ular", "\u2581Econom", "y", ":", "\u2581A", "\u2581Con", "vers", "ation", "\u2581with", "\u2581E", "manuel", "\u2581Pi", "\u00f1", "e", "iro", "-", "Lo", "\u0301", "pez"]], "entropy": [[2.09765625, 0.97607421875, 0.77490234375, 1.1171875, 5.16796875, 4.1875, 0.69775390625, 0.7158203125, 2.994140625, 4.5390625, 2.888671875, 0.002658843994140625, 0.254638671875, 0.0001385211944580078, 2.443359375, 4.70703125, 4.984375, 0.1331787109375, 0.043548583984375, 1.162109375, 4.55078125, 2.705078125, 4.921875, 3.38671875, 0.50927734375, 0.11767578125, 2.912109375, 3.32421875, 2.234375, 0.07684326171875]], "avg_entropy": [2.1542243083318073], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Moonee Ponds West Primary School", "score_of_biggest_cluster": 0.07049889117479324, "score_of_first_answer": 0.07049889117479324, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.1920928955078125e-07, "avg_score": 0.07049889117479324, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4141923570105958, "std_of_first_answer": 0.4141923570105958, "avg_entropy_of_first_answer": 2.1542243083318073, "lex_sim": 1.0}
{"question": "Which play by Sir Peter Shaffer was made into a film with Tom Hulce in the title role?", "answer": ["Amadeus"], "scores": [[0.92578125, 1.0, 0.998046875]], "normalized_score": [0.9739868640899658], "tokens": [["\u2581Ama", "de", "us"]], "entropy": [[0.38525390625, 0.0002295970916748047, 0.0164794921875]], "avg_entropy": [0.1339876651763916], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "AMADEUS", "score_of_biggest_cluster": 0.9739868640899658, "score_of_first_answer": 0.9739868640899658, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.92578125, "avg_score": 0.9739868640899658, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03453590419238241, "std_of_first_answer": 0.03453590419238241, "avg_entropy_of_first_answer": 0.1339876651763916, "lex_sim": 1.0}
{"question": "For which 1960 film did Billy Wilder become the first person to win three Oscars for the same film?", "answer": ["The Apartment"], "scores": [[0.69091796875, 0.97900390625, 1.0]], "normalized_score": [0.8778163194656372], "tokens": [["\u2581The", "\u2581A", "partment"]], "entropy": [[1.3125, 0.2115478515625, 0.0016508102416992188]], "avg_entropy": [0.5085662206013998], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Apartment", "score_of_biggest_cluster": 0.8778163194656372, "score_of_first_answer": 0.8778163194656372, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.69091796875, "avg_score": 0.8778163194656372, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.14101459553284967, "std_of_first_answer": 0.14101459553284967, "avg_entropy_of_first_answer": 0.5085662206013998, "lex_sim": 1.0}
{"question": "Fremantle prison, which was closed in 1991 and is now a heritage site, is in which country?", "answer": ["Western Australia"], "scores": [[0.16943359375, 0.97314453125]], "normalized_score": [0.40605834126472473], "tokens": [["\u2581Western", "\u2581Australia"]], "entropy": [[0.85302734375, 0.1939697265625]], "avg_entropy": [0.52349853515625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Australia (Commonwealth realm)", "score_of_biggest_cluster": 0.40605834126472473, "score_of_first_answer": 0.40605834126472473, "max_score_of_first_answer": 0.97314453125, "min_score_of_first_answer": 0.16943359375, "avg_score": 0.40605834126472473, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40185546875, "std_of_first_answer": 0.40185546875, "avg_entropy_of_first_answer": 0.52349853515625, "lex_sim": 1.0}
{"question": "At 1900 the largest fishing port in the world was?", "answer": ["The port of Fraserburgh, Scotland #2020\u62a5\u544a\uff0c#\u4e2d\u56fd\uff0c#\u5236\u9020\u4e1a\uff0c#\u5168\u7403\u5316"], "scores": [[0.0281982421875, 0.1871337890625, 0.97900390625, 0.0037059783935546875, 0.434326171875, 0.98828125, 0.9921875, 0.87890625, 0.91064453125, 8.344650268554688e-07, 0.00934600830078125, 0.2255859375, 0.07708740234375, 0.54638671875, 1.615285873413086e-05, 0.00530242919921875, 0.00508880615234375, 0.276123046875, 0.0018510818481445312, 0.25048828125, 0.003757476806640625, 0.7587890625, 0.0521240234375, 0.7578125, 0.002941131591796875, 0.8876953125, 0.8466796875, 0.548828125, 0.98681640625, 0.0156707763671875, 0.99951171875, 0.024383544921875]], "normalized_score": [0.06105417013168335], "tokens": [["\u2581The", "\u2581port", "\u2581of", "\u2581F", "ras", "er", "burgh", ",", "\u2581Scotland", "\u2581#", "2", "0", "2", "0", "<0xE6>", "<0x8A>", "<0xA5>", "\u544a", "\uff0c", "#", "\u4e2d", "\u56fd", "\uff0c", "#", "\u5236", "\u9020", "\u4e1a", "\uff0c", "#", "\u5168", "\u7403", "\u5316"]], "entropy": [[4.6171875, 2.75390625, 0.1502685546875, 4.02734375, 2.953125, 0.0853271484375, 0.05023193359375, 0.53564453125, 0.498291015625, 1.056640625, 5.58203125, 3.4375, 3.40234375, 2.17578125, 3.701171875, 2.515625, 1.4443359375, 2.484375, 2.138671875, 4.8359375, 5.49609375, 1.556640625, 4.109375, 1.9150390625, 4.17578125, 0.6474609375, 0.962890625, 1.939453125, 0.1436767578125, 4.31640625, 0.00464630126953125, 2.998046875]], "avg_entropy": [2.397226572036743], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Grimsby", "score_of_biggest_cluster": 0.06105417013168335, "score_of_first_answer": 0.06105417013168335, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 8.344650268554688e-07, "avg_score": 0.06105417013168335, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40096298026374255, "std_of_first_answer": 0.40096298026374255, "avg_entropy_of_first_answer": 2.397226572036743, "lex_sim": 1.0}
{"question": "\"What celebrity chef says \"\"Bam\"\"?\"", "answer": ["Guy Fieri"], "scores": [[0.255615234375, 0.99755859375, 1.0]], "normalized_score": [0.6341252326965332], "tokens": [["\u2581Guy", "\u2581F", "ieri"]], "entropy": [[0.83203125, 0.026397705078125, 0.0009937286376953125]], "avg_entropy": [0.28647422790527344], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Emeril", "score_of_biggest_cluster": 0.6341252326965332, "score_of_first_answer": 0.6341252326965332, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.255615234375, "avg_score": 0.6341252326965332, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3503323165668284, "std_of_first_answer": 0.3503323165668284, "avg_entropy_of_first_answer": 0.28647422790527344, "lex_sim": 1.0}
{"question": "On what apparatus would a male gymnast traditionally perform moves such as scissors, moores, spindels and flairs?", "answer": ["Horse"], "scores": [[0.055023193359375, 0.2093505859375]], "normalized_score": [0.10732723772525787], "tokens": [["\u2581Hor", "se"]], "entropy": [[2.1015625, 0.51904296875]], "avg_entropy": [1.310302734375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gymnastics pommel horse", "score_of_biggest_cluster": 0.10732723772525787, "score_of_first_answer": 0.10732723772525787, "max_score_of_first_answer": 0.2093505859375, "min_score_of_first_answer": 0.055023193359375, "avg_score": 0.10732723772525787, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0771636962890625, "std_of_first_answer": 0.0771636962890625, "avg_entropy_of_first_answer": 1.310302734375, "lex_sim": 1.0}
{"question": "Whose single I saw mommy kissing Santa Claus sold three million copies in 1952 ?", "answer": ["The Browns #1."], "scores": [[0.0699462890625, 0.019927978515625, 0.986328125, 2.372264862060547e-05, 0.409423828125, 0.008209228515625, 0.0015115737915039062, 0.0002570152282714844, 0.0010347366333007812, 0.10797119140625, 0.002597808837890625, 0.041900634765625, 0.00724029541015625, 0.96240234375, 0.053802490234375, 0.99853515625]], "normalized_score": [0.018534038215875626], "tokens": [["\u2581The", "\u2581Brow", "ns", "\u2581#", "1", ".", "\u2581To", "xic", "\u2581Parent", "ing", "\u2581Can", "\u2581Harm", "\u2581Ad", "ult", "\u2581K", "ids"]], "entropy": [[3.517578125, 4.01953125, 0.1070556640625, 1.623046875, 3.033203125, 3.2421875, 2.970703125, 2.93359375, 2.576171875, 3.751953125, 3.2578125, 3.654296875, 3.369140625, 0.183837890625, 1.9091796875, 0.01387786865234375]], "avg_entropy": [2.5101981163024902], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bwana Jimmy", "score_of_biggest_cluster": 0.018534038215875626, "score_of_first_answer": 0.018534038215875626, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 2.372264862060547e-05, "avg_score": 0.018534038215875626, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37443063965720635, "std_of_first_answer": 0.37443063965720635, "avg_entropy_of_first_answer": 2.5101981163024902, "lex_sim": 1.0}
{"question": "Who composed the opera Cosi Fan Tutte?", "answer": ["Wolfgang Amadeus Mozart #789 \u2013 1964 3c Washington, coil"], "scores": [[0.8798828125, 0.9951171875, 1.0, 1.0, 0.998046875, 0.99951171875, 2.980232238769531e-07, 0.00849151611328125, 0.07958984375, 0.04364013671875, 2.5987625122070312e-05, 0.1514892578125, 0.451416015625, 0.050628662109375, 0.071533203125, 0.1602783203125, 0.01763916015625, 0.0186767578125, 0.0002493858337402344, 0.003459930419921875, 0.1295166015625, 0.005809783935546875, 0.98974609375]], "normalized_score": [0.0375816710293293], "tokens": [["\u2581Wolfgang", "\u2581Ama", "de", "us", "\u2581Moz", "art", "\u2581#", "7", "8", "9", "\u2581\u2013", "\u2581", "1", "9", "6", "4", "\u2581", "3", "c", "\u2581Washington", ",", "\u2581co", "il"]], "entropy": [[0.470947265625, 0.037750244140625, 0.00014662742614746094, 0.0024166107177734375, 0.01605224609375, 0.006183624267578125, 0.73291015625, 4.953125, 3.375, 3.73828125, 3.162109375, 5.69921875, 1.9052734375, 2.20703125, 3.51953125, 2.99609375, 3.89453125, 4.015625, 3.458984375, 5.2265625, 4.4296875, 4.2890625, 0.07708740234375]], "avg_entropy": [2.5310265810593315], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Joannes Chrysostomus Wolfgangus Theophilus Mozart", "score_of_biggest_cluster": 0.0375816710293293, "score_of_first_answer": 0.0375816710293293, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.980232238769531e-07, "avg_score": 0.0375816710293293, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4275425791688802, "std_of_first_answer": 0.4275425791688802, "avg_entropy_of_first_answer": 2.5310265810593315, "lex_sim": 1.0}
{"question": "Which force is nothing more than the bending of space and time?", "answer": ["Gravitational force"], "scores": [[0.94189453125, 0.501953125, 0.91845703125, 0.99267578125, 0.41357421875]], "normalized_score": [0.7082996368408203], "tokens": [["\u2581Gra", "v", "it", "ational", "\u2581force"]], "entropy": [[0.421630859375, 0.787109375, 0.351318359375, 0.06719970703125, 1.251953125]], "avg_entropy": [0.57584228515625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gravitational physics", "score_of_biggest_cluster": 0.7082996368408203, "score_of_first_answer": 0.7082996368408203, "max_score_of_first_answer": 0.99267578125, "min_score_of_first_answer": 0.41357421875, "avg_score": 0.7082996368408203, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.24443138935407313, "std_of_first_answer": 0.24443138935407313, "avg_entropy_of_first_answer": 0.57584228515625, "lex_sim": 1.0}
{"question": "Which is heaviest of all naturally-occurring elements?", "answer": ["Ununoctium #IWSG: Moving Forward"], "scores": [[0.054901123046875, 0.271484375, 1.0, 0.99658203125, 7.748603820800781e-07, 5.233287811279297e-05, 2.384185791015625e-06, 0.02105712890625, 0.021240234375, 1.5914440155029297e-05, 0.96044921875, 0.307861328125, 0.99658203125]], "normalized_score": [0.009118285030126572], "tokens": [["\u2581Un", "uno", "ct", "ium", "\u2581#", "I", "WS", "G", ":", "\u2581Mov", "ing", "\u2581For", "ward"]], "entropy": [[2.8203125, 1.40234375, 0.0014696121215820312, 0.03570556640625, 1.4189453125, 0.587890625, 4.81640625, 3.615234375, 3.744140625, 4.6484375, 0.2093505859375, 2.63671875, 0.0261688232421875]], "avg_entropy": [1.997163405785194], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "234U", "score_of_biggest_cluster": 0.009118285030126572, "score_of_first_answer": 0.009118285030126572, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 7.748603820800781e-07, "avg_score": 0.009118285030126572, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43252833601873086, "std_of_first_answer": 0.43252833601873086, "avg_entropy_of_first_answer": 1.997163405785194, "lex_sim": 1.0}
{"question": "Art Garfunkel trained for which profession although he didn't qualify?", "answer": ["Aromatherapist"], "scores": [[0.01007843017578125, 0.00814056396484375, 0.57373046875, 0.55419921875, 0.998046875]], "normalized_score": [0.12109171599149704], "tokens": [["\u2581A", "rom", "ather", "ap", "ist"]], "entropy": [[5.1015625, 4.890625, 1.076171875, 0.81787109375, 0.016204833984375]], "avg_entropy": [2.380487060546875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Master Builder (occupation)", "score_of_biggest_cluster": 0.12109171599149704, "score_of_first_answer": 0.12109171599149704, "max_score_of_first_answer": 0.998046875, "min_score_of_first_answer": 0.00814056396484375, "avg_score": 0.12109171599149704, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37763842920059515, "std_of_first_answer": 0.37763842920059515, "avg_entropy_of_first_answer": 2.380487060546875, "lex_sim": 1.0}
{"question": "What is vellum parchment made from", "answer": ["animal skin often calf or sheep #Moscow #Russia"], "scores": [[0.37451171875, 0.72998046875, 0.00019180774688720703, 0.14453125, 0.99853515625, 0.52001953125, 0.49267578125, 5.960464477539062e-07, 0.00038743019104003906, 4.8041343688964844e-05, 0.9375, 0.1275634765625, 0.1951904296875, 0.9599609375]], "normalized_score": [0.03001653216779232], "tokens": [["\u2581animal", "\u2581skin", "\u2581often", "\u2581cal", "f", "\u2581or", "\u2581sheep", "\u2581#", "M", "osc", "ow", "\u2581#", "Russ", "ia"]], "entropy": [[2.12890625, 0.75146484375, 1.4228515625, 1.650390625, 0.01568603515625, 1.2021484375, 1.1162109375, 1.44140625, 4.046875, 3.912109375, 0.350830078125, 3.916015625, 5.34765625, 0.1759033203125]], "avg_entropy": [1.9627467564174108], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Calfskin", "score_of_biggest_cluster": 0.03001653216779232, "score_of_first_answer": 0.03001653216779232, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 5.960464477539062e-07, "avg_score": 0.03001653216779232, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3696676130434319, "std_of_first_answer": 0.3696676130434319, "avg_entropy_of_first_answer": 1.9627467564174108, "lex_sim": 1.0}
{"question": "In what year did Kurt Waldheim become UN General Secretary; Bloody Sunday happened in Derry, NI; President Nixon visited China.", "answer": ["year 1972 #TBT to When Obama\u2019s Proposal for Free Community College Fell Flat"], "scores": [[6.699562072753906e-05, 0.2232666015625, 0.8896484375, 0.97412109375, 0.99560546875, 0.1337890625, 5.9604644775390625e-06, 9.40561294555664e-05, 0.8046875, 0.0009760856628417969, 0.00104522705078125, 7.081031799316406e-05, 0.97216796875, 0.003086090087890625, 0.99951171875, 0.0008993148803710938, 0.888671875, 0.61669921875, 0.405029296875, 0.1705322265625, 0.9072265625, 0.96630859375, 0.009979248046875, 0.83349609375, 0.30126953125, 1.0]], "normalized_score": [0.04547077789902687], "tokens": [["\u2581year", "\u2581", "1", "9", "7", "2", "\u2581#", "TB", "T", "\u2581to", "\u2581When", "\u2581Ob", "ama", "\u2019", "s", "\u2581Pro", "pos", "al", "\u2581for", "\u2581Free", "\u2581Community", "\u2581College", "\u2581F", "ell", "\u2581Fl", "at"]], "entropy": [[1.736328125, 1.859375, 0.57080078125, 0.205078125, 0.0347900390625, 0.7177734375, 1.10546875, 6.03125, 1.181640625, 1.4130859375, 4.65625, 5.203125, 0.2064208984375, 4.26171875, 0.005252838134765625, 6.05078125, 0.60986328125, 0.6884765625, 2.345703125, 3.900390625, 0.495849609375, 0.1890869140625, 3.38671875, 0.74560546875, 1.4970703125, 0.0005259513854980469]], "avg_entropy": [1.8884011598733754], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and seventy-two", "score_of_biggest_cluster": 0.04547077789902687, "score_of_first_answer": 0.04547077789902687, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.9604644775390625e-06, "avg_score": 0.04547077789902687, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.42367800596168687, "std_of_first_answer": 0.42367800596168687, "avg_entropy_of_first_answer": 1.8884011598733754, "lex_sim": 1.0}
{"question": "In November 2008, RMS Queen Elizabeth II will depart Southamptonon her last voyage. To which city will she travel?", "answer": ["New York City"], "scores": [[0.428955078125, 0.9873046875, 0.7919921875]], "normalized_score": [0.6948024034500122], "tokens": [["\u2581New", "\u2581York", "\u2581City"]], "entropy": [[2.587890625, 0.0936279296875, 0.7529296875]], "avg_entropy": [1.1448160807291667], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dubai (city)", "score_of_biggest_cluster": 0.6948024034500122, "score_of_first_answer": 0.6948024034500122, "max_score_of_first_answer": 0.9873046875, "min_score_of_first_answer": 0.428955078125, "avg_score": 0.6948024034500122, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.23134803068797086, "std_of_first_answer": 0.23134803068797086, "avg_entropy_of_first_answer": 1.1448160807291667, "lex_sim": 1.0}
{"question": "From which fruit is Slivovitz made?", "answer": ["Plum #BookReview: The Black Hour by Lori Rader-Day (@LoriRaderDay) @GalleryBooks"], "scores": [[0.9501953125, 0.9580078125, 5.960464477539063e-08, 1.1920928955078125e-06, 0.003543853759765625, 0.2318115234375, 0.2237548828125, 0.2269287109375, 0.0032482147216796875, 0.002414703369140625, 0.43701171875, 0.9267578125, 0.81494140625, 0.9365234375, 0.9951171875, 1.0, 0.99951171875, 0.99951171875, 0.0038471221923828125, 0.6064453125, 0.8955078125, 0.994140625, 0.97021484375, 0.99755859375, 0.96435546875, 0.07611083984375, 0.0029010772705078125, 0.64306640625, 0.98974609375, 1.0]], "normalized_score": [0.10192468017339706], "tokens": [["\u2581Pl", "um", "\u2581#", "Book", "Re", "view", ":", "\u2581The", "\u2581Black", "\u2581H", "our", "\u2581by", "\u2581L", "ori", "\u2581R", "ader", "-", "Day", "\u2581(@", "L", "ori", "R", "ader", "Day", ")", "\u2581@", "G", "allery", "Book", "s"]], "entropy": [[0.205322265625, 0.196044921875, 0.77734375, 5.484375, 4.62890625, 0.650390625, 2.474609375, 3.205078125, 5.40234375, 3.244140625, 1.4931640625, 0.425537109375, 1.1435546875, 0.50244140625, 0.03997802734375, 0.0015344619750976562, 0.005168914794921875, 0.004756927490234375, 2.166015625, 2.13671875, 0.69189453125, 0.04937744140625, 0.159423828125, 0.0189361572265625, 0.2164306640625, 1.98828125, 4.1953125, 1.197265625, 0.07659912109375, 0.00220489501953125]], "avg_entropy": [1.42610502243042], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Prunus subg. Prunus", "score_of_biggest_cluster": 0.10192468017339706, "score_of_first_answer": 0.10192468017339706, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.10192468017339706, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.42246163087682015, "std_of_first_answer": 0.42246163087682015, "avg_entropy_of_first_answer": 1.42610502243042, "lex_sim": 1.0}
{"question": "Amr Shabana from Egypt is famous for playing which sport?", "answer": ["Squash"], "scores": [[0.9345703125, 0.998046875, 0.9951171875]], "normalized_score": [0.9754665493965149], "tokens": [["\u2581S", "qu", "ash"]], "entropy": [[0.443359375, 0.0189361572265625, 0.045013427734375]], "avg_entropy": [0.16910298665364584], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Squash (disambiguation)", "score_of_biggest_cluster": 0.9754665493965149, "score_of_first_answer": 0.9754665493965149, "max_score_of_first_answer": 0.998046875, "min_score_of_first_answer": 0.9345703125, "avg_score": 0.9754665493965149, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.029257062068517345, "std_of_first_answer": 0.029257062068517345, "avg_entropy_of_first_answer": 0.16910298665364584, "lex_sim": 1.0}
{"question": "\"\"\"Getting to Know You\"\" is a song from which film?\"", "answer": ["\"\"Getting to Know You\"\" is a song from the 1951 Disney movie \"The King and I.\""], "scores": [[0.046234130859375, 0.1668701171875, 0.9990234375, 0.98046875, 0.99658203125, 0.9990234375, 0.99462890625, 0.97021484375, 0.97705078125, 0.97119140625, 0.99560546875, 0.63720703125, 0.78955078125, 0.9990234375, 1.0, 0.9755859375, 0.9990234375, 0.34423828125, 0.1396484375, 0.49609375, 0.99658203125, 0.99951171875, 0.998046875, 0.998046875, 0.466064453125]], "normalized_score": [0.6648972630500793], "tokens": [["\u2581\"\"", "Get", "ting", "\u2581to", "\u2581Know", "\u2581You", "\"\"", "\u2581is", "\u2581a", "\u2581song", "\u2581from", "\u2581the", "\u2581", "1", "9", "5", "1", "\u2581Disney", "\u2581movie", "\u2581\"", "The", "\u2581King", "\u2581and", "\u2581I", ".\""]], "entropy": [[1.435546875, 1.0380859375, 0.01114654541015625, 0.12481689453125, 0.0255126953125, 0.00832366943359375, 0.03997802734375, 0.20751953125, 0.1295166015625, 0.2120361328125, 0.0426025390625, 1.0087890625, 0.9150390625, 0.00984954833984375, 0.000682830810546875, 0.125244140625, 0.01076507568359375, 1.732421875, 1.2509765625, 1.572265625, 0.03265380859375, 0.0081024169921875, 0.015289306640625, 0.01776123046875, 1.2021484375]], "avg_entropy": [0.44708297729492186], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "King & i", "score_of_biggest_cluster": 0.6648972630500793, "score_of_first_answer": 0.6648972630500793, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.046234130859375, "avg_score": 0.6648972630500793, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3140868647599432, "std_of_first_answer": 0.3140868647599432, "avg_entropy_of_first_answer": 0.44708297729492186, "lex_sim": 1.0}
{"question": "In 2006, who became the UK's first woman Secretary of State for Foreign and Commonwealth Affairs?", "answer": ["PRIME MINISTER"], "scores": [[1.5735626220703125e-05, 0.002704620361328125, 0.99365234375, 0.89794921875, 0.986328125, 0.9951171875, 0.98974609375]], "normalized_score": [0.08672173321247101], "tokens": [["\u2581P", "RI", "ME", "\u2581M", "IN", "IST", "ER"]], "entropy": [[0.2108154296875, 5.08203125, 0.06060791015625, 0.6376953125, 0.10693359375, 0.044342041015625, 0.065185546875]], "avg_entropy": [0.8868015834263393], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Margaret Becket", "score_of_biggest_cluster": 0.08672173321247101, "score_of_first_answer": 0.08672173321247101, "max_score_of_first_answer": 0.9951171875, "min_score_of_first_answer": 1.5735626220703125e-05, "avg_score": 0.08672173321247101, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4398823305322597, "std_of_first_answer": 0.4398823305322597, "avg_entropy_of_first_answer": 0.8868015834263393, "lex_sim": 1.0}
{"question": "Smoky, Leaf-nosed and Vesper are all species of which creature?", "answer": ["Caledonian crow species."], "scores": [[0.0017747879028320312, 0.005153656005859375, 0.93896484375, 0.00797271728515625, 0.003376007080078125, 0.0665283203125, 2.3245811462402344e-06, 0.0013828277587890625, 1.800060272216797e-05, 0.00037217140197753906, 0.0035839080810546875, 0.07269287109375, 0.00844573974609375, 0.00010561943054199219, 0.033416748046875, 0.004558563232421875, 0.0257568359375, 0.0809326171875, 0.00449371337890625, 0.9287109375, 3.892183303833008e-05, 0.8310546875, 1.0, 0.05615234375, 0.36279296875, 0.9765625, 0.09197998046875, 0.9990234375, 0.057159423828125, 0.84228515625, 0.46484375, 0.83447265625]], "normalized_score": [0.019180726259946823], "tokens": [["\u2581C", "aled", "onian", "\u2581crow", "\u2581species", ".", "\u2581#", "O", "oni", "Con", "ver", "ts", ":", "\u2581Che", "f", "\u2581Michael", "\u2581O", "\u2019", "Ne", "ill", "\u2581Em", "bra", "ces", "\u2581the", "\u2581Power", "\u2581of", "\u2581O", "oni", "\u2581P", "izza", "i", "olo"]], "entropy": [[2.263671875, 4.34765625, 0.337646484375, 3.38671875, 1.0654296875, 1.3173828125, 0.46142578125, 5.46875, 4.37890625, 4.47265625, 3.09765625, 2.388671875, 3.779296875, 5.765625, 2.5, 6.34375, 4.01953125, 0.791015625, 0.101806640625, 0.391845703125, 4.91796875, 0.93798828125, 0.00028324127197265625, 1.490234375, 3.724609375, 0.1680908203125, 4.125, 0.00836944580078125, 1.037109375, 0.5400390625, 2.44140625, 0.68115234375]], "avg_entropy": [2.398490458726883], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bat Conservation", "score_of_biggest_cluster": 0.019180726259946823, "score_of_first_answer": 0.019180726259946823, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.3245811462402344e-06, "avg_score": 0.019180726259946823, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3875142732324538, "std_of_first_answer": 0.3875142732324538, "avg_entropy_of_first_answer": 2.398490458726883, "lex_sim": 1.0}
{"question": "What hero from Greek mythology was killed by a discus thrown by Apollo, who then named a flower after him, made from his spilled blood?", "answer": ["Hyacinth"], "scores": [[0.62744140625, 0.859375, 0.99951171875]], "normalized_score": [0.8137941956520081], "tokens": [["\u2581Hy", "ac", "inth"]], "entropy": [[1.4609375, 0.46923828125, 0.004314422607421875]], "avg_entropy": [0.644830067952474], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hyacinth", "score_of_biggest_cluster": 0.8137941956520081, "score_of_first_answer": 0.8137941956520081, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.62744140625, "avg_score": 0.8137941956520081, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1534303350042525, "std_of_first_answer": 0.1534303350042525, "avg_entropy_of_first_answer": 0.644830067952474, "lex_sim": 1.0}
{"question": "The childhood disease 'Rickets' is caused by a lack of which vitamin?", "answer": ["Vitamin D #846 The Wrecking Crew!"], "scores": [[0.36474609375, 0.994140625, 0.955078125, 3.5762786865234375e-07, 0.0073089599609375, 0.045745849609375, 0.0252532958984375, 8.702278137207031e-06, 0.0007290840148925781, 0.007354736328125, 0.06805419921875, 0.99169921875, 1.0, 0.021942138671875, 0.000400543212890625, 0.9345703125, 0.0012722015380859375, 0.80419921875, 0.02130126953125, 0.998046875]], "normalized_score": [0.021801337599754333], "tokens": [["\u2581Vit", "amin", "\u2581D", "\u2581#", "8", "4", "6", "\u2581The", "\u2581W", "reck", "ing", "\u2581C", "rew", "!", "\u2581by", "\u2581Thomas", "\u2581S", ".", "\u2581Flow", "ers"]], "entropy": [[1.78125, 0.0462646484375, 0.332763671875, 0.89453125, 4.61328125, 2.48828125, 2.16015625, 1.7490234375, 5.73046875, 2.0546875, 1.0947265625, 0.07940673828125, 0.00041103363037109375, 2.810546875, 2.80078125, 0.388916015625, 4.015625, 1.4697265625, 4.61328125, 0.0182037353515625]], "avg_entropy": [1.9571166515350342], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Vitamin D1", "score_of_biggest_cluster": 0.021801337599754333, "score_of_first_answer": 0.021801337599754333, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 3.5762786865234375e-07, "avg_score": 0.021801337599754333, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44268715867226566, "std_of_first_answer": 0.44268715867226566, "avg_entropy_of_first_answer": 1.9571166515350342, "lex_sim": 1.0}
{"question": "Neal Foulds, John Dunning and Mark Williams are associated with which sport?", "answer": ["Snooker #57 The Cure with Simon Hodgson"], "scores": [[0.72802734375, 0.8203125, 0.97509765625, 2.980232238769531e-07, 0.004833221435546875, 0.047393798828125, 2.8431415557861328e-05, 0.00762176513671875, 0.0001169443130493164, 0.0006356239318847656, 0.0070343017578125, 0.003345489501953125, 0.004669189453125, 0.9892578125, 0.55615234375]], "normalized_score": [0.008051115088164806], "tokens": [["\u2581S", "no", "oker", "\u2581#", "5", "7", "\u2581The", "\u2581C", "ure", "\u2581with", "\u2581Simon", "\u2581H", "od", "g", "son"]], "entropy": [[1.3017578125, 0.491943359375, 0.1458740234375, 0.828125, 5.078125, 3.46484375, 3.123046875, 5.265625, 2.2109375, 3.26171875, 5.69921875, 0.935546875, 2.76171875, 0.0689697265625, 0.818359375]], "avg_entropy": [2.363720703125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lightning break", "score_of_biggest_cluster": 0.008051115088164806, "score_of_first_answer": 0.008051115088164806, "max_score_of_first_answer": 0.9892578125, "min_score_of_first_answer": 2.980232238769531e-07, "avg_score": 0.008051115088164806, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3914893528282032, "std_of_first_answer": 0.3914893528282032, "avg_entropy_of_first_answer": 2.363720703125, "lex_sim": 1.0}
{"question": "What was introduced by the Volstead Act?", "answer": ["Prohibition of alcohol in the United States"], "scores": [[0.826171875, 0.9990234375, 0.99853515625, 0.3837890625, 0.7177734375, 0.99951171875, 0.233154296875, 0.99072265625, 0.88525390625, 0.99853515625]], "normalized_score": [0.7355130910873413], "tokens": [["\u2581Pro", "hib", "ition", "\u2581of", "\u2581alco", "hol", "\u2581in", "\u2581the", "\u2581United", "\u2581States"]], "entropy": [[0.794921875, 0.0117034912109375, 0.0145721435546875, 1.5986328125, 0.7568359375, 0.004207611083984375, 1.4873046875, 0.06939697265625, 0.434326171875, 0.0128631591796875]], "avg_entropy": [0.5184764862060547], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alcohol prohibition in the United States", "score_of_biggest_cluster": 0.7355130910873413, "score_of_first_answer": 0.7355130910873413, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.233154296875, "avg_score": 0.7355130910873413, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2652880259032816, "std_of_first_answer": 0.2652880259032816, "avg_entropy_of_first_answer": 0.5184764862060547, "lex_sim": 1.0}
{"question": "Which Pacific nation consists of over 800 islands and islets, the largest being Viti Levu?", "answer": ["FIJI"], "scores": [[0.00705718994140625, 0.9375, 0.95947265625]], "normalized_score": [0.18515919148921967], "tokens": [["\u2581FI", "J", "I"]], "entropy": [[0.5087890625, 0.25830078125, 0.275634765625]], "avg_entropy": [0.3475748697916667], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fiji (disambiguation)", "score_of_biggest_cluster": 0.18515919148921967, "score_of_first_answer": 0.18515919148921967, "max_score_of_first_answer": 0.95947265625, "min_score_of_first_answer": 0.00705718994140625, "avg_score": 0.18515919148921967, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44388459972738026, "std_of_first_answer": 0.44388459972738026, "avg_entropy_of_first_answer": 0.3475748697916667, "lex_sim": 1.0}
{"question": "Can you name the singer of the title track for '1971 - Diamonds Are Forever'?", "answer": ["Shirley Bassey"], "scores": [[0.96337890625, 1.0, 0.99951171875, 0.998046875, 1.0]], "normalized_score": [0.9920811653137207], "tokens": [["\u2581Sh", "ir", "ley", "\u2581Bas", "sey"]], "entropy": [[0.260009765625, 0.0004239082336425781, 0.002696990966796875, 0.015716552734375, 0.00046181678771972656]], "avg_entropy": [0.055861806869506835], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dame Shirley Bassey DBE", "score_of_biggest_cluster": 0.9920811653137207, "score_of_first_answer": 0.9920811653137207, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.96337890625, "avg_score": 0.9920811653137207, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.014422079198513753, "std_of_first_answer": 0.014422079198513753, "avg_entropy_of_first_answer": 0.055861806869506835, "lex_sim": 1.0}
{"question": "\u2018Quicksilver\u2019 is another name for which metallic element?", "answer": ["mercury"], "scores": [[0.63720703125, 0.99951171875, 0.9892578125]], "normalized_score": [0.8572865128517151], "tokens": [["\u2581mer", "cur", "y"]], "entropy": [[0.701171875, 0.00812530517578125, 0.0946044921875]], "avg_entropy": [0.26796722412109375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mercury", "score_of_biggest_cluster": 0.8572865128517151, "score_of_first_answer": 0.8572865128517151, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.63720703125, "avg_score": 0.8572865128517151, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1684272285180991, "std_of_first_answer": 0.1684272285180991, "avg_entropy_of_first_answer": 0.26796722412109375, "lex_sim": 1.0}
{"question": "Which King of Macedon a great military leader was reputedly never defeated in battle?", "answer": ["Alexander the Great (356 \u2013 323 BC) #WeAreAllTania: A Reflection on the Murder of a Trans Woman of Color"], "scores": [[0.9306640625, 0.8525390625, 0.94140625, 0.01371002197265625, 0.1246337890625, 0.99658203125, 0.99951171875, 0.028076171875, 0.78662109375, 1.0, 1.0, 1.0, 0.83349609375, 0.8857421875, 7.3909759521484375e-06, 6.496906280517578e-05, 0.043060302734375, 0.14208984375, 0.01141357421875, 4.696846008300781e-05, 0.002960205078125, 0.023895263671875, 2.7179718017578125e-05, 0.81982421875, 0.80224609375, 0.364501953125, 0.038848876953125, 0.98291015625, 0.9677734375, 0.040771484375, 0.451904296875, 0.5263671875, 0.405517578125, 0.99169921875]], "normalized_score": [0.08934390544891357], "tokens": [["\u2581Alexander", "\u2581the", "\u2581Great", "\u2581(", "3", "5", "6", "\u2581\u2013", "\u2581", "3", "2", "3", "\u2581BC", ")", "\u2581#", "We", "Are", "All", "T", "ania", ":", "\u2581A", "\u2581Ref", "lection", "\u2581on", "\u2581the", "\u2581Mur", "der", "\u2581of", "\u2581a", "\u2581Trans", "\u2581Woman", "\u2581of", "\u2581Color"]], "entropy": [[0.4560546875, 0.685546875, 0.287353515625, 0.7978515625, 1.880859375, 0.0279693603515625, 0.00421905517578125, 1.640625, 0.53369140625, 0.0015058517456054688, 0.001995086669921875, 0.0020771026611328125, 0.619140625, 0.464111328125, 0.98681640625, 4.98828125, 4.3984375, 4.26953125, 2.8671875, 3.38671875, 2.53515625, 5.29296875, 3.435546875, 0.88623046875, 0.98388671875, 3.765625, 4.8828125, 0.0953369140625, 0.227294921875, 0.65576171875, 3.23046875, 1.6591796875, 2.25390625, 0.0615234375]], "avg_entropy": [1.7136962273541618], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u039c\u03ad\u03b3\u03b1\u03c2 \u1f08\u03bb\u03ad\u03be\u03b1\u03bd\u03b4\u03c1\u03bf\u03c2", "score_of_biggest_cluster": 0.08934390544891357, "score_of_first_answer": 0.08934390544891357, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 7.3909759521484375e-06, "avg_score": 0.08934390544891357, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4229939912552778, "std_of_first_answer": 0.4229939912552778, "avg_entropy_of_first_answer": 1.7136962273541618, "lex_sim": 1.0}
{"question": "What are the names of Donald Duck's three nephews?", "answer": ["Huey, Dewey, and Louie #TwinFlames: The Path to Union - Part One"], "scores": [[0.98193359375, 1.0, 1.0, 0.99755859375, 0.9931640625, 1.0, 1.0, 0.9736328125, 0.98486328125, 1.0, 1.0, 8.344650268554688e-07, 0.0038814544677734375, 0.0003294944763183594, 0.00452423095703125, 0.63232421875, 0.00576019287109375, 0.026763916015625, 0.00033736228942871094, 0.80126953125, 0.291748046875, 0.04296875, 0.007442474365234375, 0.056549072265625]], "normalized_score": [0.07354103773832321], "tokens": [["\u2581H", "ue", "y", ",", "\u2581De", "we", "y", ",", "\u2581and", "\u2581Lou", "ie", "\u2581#", "T", "win", "Fl", "ames", ":", "\u2581The", "\u2581Path", "\u2581to", "\u2581Union", "\u2581-", "\u2581Part", "\u2581One"]], "entropy": [[0.1663818359375, 0.0004138946533203125, 0.0018110275268554688, 0.022064208984375, 0.049346923828125, 0.0001360177993774414, 5.3048133850097656e-05, 0.124755859375, 0.080810546875, 0.001953125, 0.00021719932556152344, 0.79296875, 5.02734375, 0.08819580078125, 3.833984375, 0.8828125, 2.65234375, 6.16796875, 5.42578125, 0.91015625, 3.333984375, 3.1328125, 5.17578125, 1.263671875]], "avg_entropy": [1.6306562026341755], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Huey, Dewey and Louie", "score_of_biggest_cluster": 0.07354103773832321, "score_of_first_answer": 0.07354103773832321, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 8.344650268554688e-07, "avg_score": 0.07354103773832321, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4638794797482464, "std_of_first_answer": 0.4638794797482464, "avg_entropy_of_first_answer": 1.6306562026341755, "lex_sim": 1.0}
{"question": "Which quadrilateral has one pair of parallel edges of unequal length?", "answer": ["Rhombus #174."], "scores": [[0.032379150390625, 0.98193359375, 0.98681640625, 5.364418029785156e-07, 0.01512908935546875, 0.0758056640625, 0.0589599609375, 0.0012454986572265625, 2.086162567138672e-06, 0.0018787384033203125, 0.99267578125]], "normalized_score": [0.0076841311529278755], "tokens": [["\u2581Rh", "omb", "us", "\u2581#", "1", "7", "4", ".", "\u2581James", "\u2581Camer", "on"]], "entropy": [[2.857421875, 0.0994873046875, 0.1009521484375, 0.87548828125, 5.1796875, 3.443359375, 2.892578125, 0.359130859375, 4.10546875, 4.41015625, 0.044677734375]], "avg_entropy": [2.215309836647727], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Trapezium (disambiguation)", "score_of_biggest_cluster": 0.0076841311529278755, "score_of_first_answer": 0.0076841311529278755, "max_score_of_first_answer": 0.99267578125, "min_score_of_first_answer": 5.364418029785156e-07, "avg_score": 0.0076841311529278755, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4299769275915844, "std_of_first_answer": 0.4299769275915844, "avg_entropy_of_first_answer": 2.215309836647727, "lex_sim": 1.0}
{"question": "Ingvar Kamprad, one of the world's richest men, founded which company in the 1940's?", "answer": ["IKEA"], "scores": [[0.9755859375, 0.94091796875, 0.99951171875]], "normalized_score": [0.9717063903808594], "tokens": [["\u2581I", "KE", "A"]], "entropy": [[0.2232666015625, 0.2314453125, 0.004367828369140625]], "avg_entropy": [0.15302658081054688], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ikea International Group", "score_of_biggest_cluster": 0.9717063903808594, "score_of_first_answer": 0.9717063903808594, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.94091796875, "avg_score": 0.9717063903808594, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.024054425799138475, "std_of_first_answer": 0.024054425799138475, "avg_entropy_of_first_answer": 0.15302658081054688, "lex_sim": 1.0}
{"question": "What type of food in the United States are Brown Bobbies", "answer": ["Brown Bobbies are not a type of food in the United States."], "scores": [[0.263427734375, 0.6171875, 0.99951171875, 1.0, 0.88134765625, 0.349365234375, 0.99072265625, 0.71044921875, 1.0, 0.99951171875, 0.93505859375, 1.0, 0.97998046875, 0.9990234375, 0.94091796875, 0.31005859375, 0.99951171875, 0.7880859375, 0.837890625, 0.97021484375, 0.8896484375, 0.97998046875, 0.7197265625, 0.85693359375, 0.99462890625, 0.99755859375, 2.980232238769531e-07, 0.0007982254028320312, 0.05584716796875, 0.07989501953125, 0.01219940185546875, 0.01456451416015625, 0.0006799697875976562, 0.01059722900390625, 0.0004954338073730469, 0.0023365020751953125, 0.43603515625, 0.47119140625, 0.76806640625]], "normalized_score": [0.16914041340351105], "tokens": [["\u2581Brown", "\u2581Bob", "b", "ies", "\u2581are", "\u2581not", "\u2581a", "\u2581type", "\u2581of", "\u2581food", "\u2581in", "\u2581the", "\u2581United", "\u2581States", ".", "\u2581Could", "\u2581you", "\u2581please", "\u2581provide", "\u2581more", "\u2581context", "\u2581or", "\u2581clarify", "\u2581your", "\u2581question", "?", "\u2581#", "6", "7", "3", ":", "\u2581The", "\u2581P", "ill", "\u2581by", "\u2581Lor", "etta", "\u2581Lyn", "n"]], "entropy": [[3.857421875, 1.66015625, 0.0032062530517578125, 0.0021820068359375, 0.60693359375, 0.71923828125, 0.06488037109375, 1.1259765625, 0.0019321441650390625, 0.0050048828125, 0.338134765625, 0.0013360977172851562, 0.10906982421875, 0.00991058349609375, 0.286376953125, 1.919921875, 0.00615692138671875, 0.56982421875, 0.6279296875, 0.1800537109375, 0.352783203125, 0.119140625, 1.072265625, 0.5419921875, 0.040283203125, 0.0189056396484375, 0.50537109375, 4.0234375, 3.38671875, 3.345703125, 3.123046875, 4.89453125, 5.16015625, 4.3671875, 2.890625, 5.13671875, 1.9248046875, 2.630859375, 0.55078125]], "avg_entropy": [1.4405374037913787], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hanson Gregory", "score_of_biggest_cluster": 0.16914041340351105, "score_of_first_answer": 0.16914041340351105, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.980232238769531e-07, "avg_score": 0.16914041340351105, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40224200070617194, "std_of_first_answer": 0.40224200070617194, "avg_entropy_of_first_answer": 1.4405374037913787, "lex_sim": 1.0}
{"question": "What was James Herbert\u2019s first book, published in 1974?", "answer": ["Rats"], "scores": [[0.1046142578125, 1.0]], "normalized_score": [0.3234412968158722], "tokens": [["\u2581R", "ats"]], "entropy": [[1.48828125, 0.0006222724914550781]], "avg_entropy": [0.7444517612457275], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\ud83d\udc00", "score_of_biggest_cluster": 0.3234412968158722, "score_of_first_answer": 0.3234412968158722, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.1046142578125, "avg_score": 0.3234412968158722, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.44769287109375, "std_of_first_answer": 0.44769287109375, "avg_entropy_of_first_answer": 0.7444517612457275, "lex_sim": 1.0}
{"question": "In 1703, Isaac Newton succeeded which of his rivals as President of the Royal Society ?", "answer": ["Edmond Halley"], "scores": [[0.84033203125, 0.99951171875, 0.99951171875, 0.99951171875]], "normalized_score": [0.9570919275283813], "tokens": [["\u2581Ed", "mond", "\u2581Hal", "ley"]], "entropy": [[0.92919921875, 0.00384521484375, 0.007293701171875, 0.00345611572265625]], "avg_entropy": [0.2359485626220703], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Robert Hook", "score_of_biggest_cluster": 0.9570919275283813, "score_of_first_answer": 0.9570919275283813, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.84033203125, "avg_score": 0.9570919275283813, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06892682657073414, "std_of_first_answer": 0.06892682657073414, "avg_entropy_of_first_answer": 0.2359485626220703, "lex_sim": 1.0}
{"question": "On Red Nose Day 2001 who won Celebrity Big Brother?", "answer": ["N/ANOTA"], "scores": [[0.0021038055419921875, 0.841796875, 0.0001595020294189453, 0.3935546875, 0.000469207763671875]], "normalized_score": [0.008779504336416721], "tokens": [["\u2581N", "/", "AN", "OT", "A"]], "entropy": [[4.3046875, 1.0517578125, 0.0848388671875, 1.220703125, 2.24609375]], "avg_entropy": [1.7816162109375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dee, Jack", "score_of_biggest_cluster": 0.008779504336416721, "score_of_first_answer": 0.008779504336416721, "max_score_of_first_answer": 0.841796875, "min_score_of_first_answer": 0.0001595020294189453, "avg_score": 0.008779504336416721, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33374885936878346, "std_of_first_answer": 0.33374885936878346, "avg_entropy_of_first_answer": 1.7816162109375, "lex_sim": 1.0}
{"question": "A swanee whistle was used to provide the voices of whom, in an animated tv series?", "answer": ["Rescue Rangers"], "scores": [[0.0007176399230957031, 0.194580078125, 0.95263671875, 0.99267578125]], "normalized_score": [0.10719756036996841], "tokens": [["\u2581Res", "cue", "\u2581R", "angers"]], "entropy": [[3.921875, 1.9375, 0.384765625, 0.049346923828125]], "avg_entropy": [1.5733718872070312], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Soup Dragon", "score_of_biggest_cluster": 0.10719756036996841, "score_of_first_answer": 0.10719756036996841, "max_score_of_first_answer": 0.99267578125, "min_score_of_first_answer": 0.0007176399230957031, "avg_score": 0.10719756036996841, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4430662535102047, "std_of_first_answer": 0.4430662535102047, "avg_entropy_of_first_answer": 1.5733718872070312, "lex_sim": 1.0}
{"question": "In September 1645 after the surrender of which city, was Prince Rupert dismissed by Charles I?", "answer": ["Portsmouth"], "scores": [[0.0007891654968261719, 0.9873046875, 0.99853515625]], "normalized_score": [0.09197312593460083], "tokens": [["\u2581Port", "sm", "outh"]], "entropy": [[1.73046875, 0.11151123046875, 0.01556396484375]], "avg_entropy": [0.6191813151041666], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bristol (England)", "score_of_biggest_cluster": 0.09197312593460083, "score_of_first_answer": 0.09197312593460083, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.0007891654968261719, "avg_score": 0.09197312593460083, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46771739575725896, "std_of_first_answer": 0.46771739575725896, "avg_entropy_of_first_answer": 0.6191813151041666, "lex_sim": 1.0}
{"question": "In which country is Lake Como?", "answer": ["Italy"], "scores": [[0.93310546875]], "normalized_score": [0.93310546875], "tokens": [["\u2581Italy"]], "entropy": [[0.418701171875]], "avg_entropy": [0.418701171875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Environment of Italy", "score_of_biggest_cluster": 0.93310546875, "score_of_first_answer": 0.93310546875, "max_score_of_first_answer": 0.93310546875, "min_score_of_first_answer": 0.93310546875, "avg_score": 0.93310546875, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.418701171875, "lex_sim": 1.0}
{"question": "Who is the author of the Twilight series of best-selling books?", "answer": ["Stephenie Meyer"], "scores": [[0.98046875, 0.9970703125, 0.99365234375]], "normalized_score": [0.9903711676597595], "tokens": [["\u2581Stephen", "ie", "\u2581Meyer"]], "entropy": [[0.160888671875, 0.0316162109375, 0.055389404296875]], "avg_entropy": [0.08263142903645833], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Meyer, Stephenie", "score_of_biggest_cluster": 0.9903711676597595, "score_of_first_answer": 0.9903711676597595, "max_score_of_first_answer": 0.9970703125, "min_score_of_first_answer": 0.98046875, "avg_score": 0.9903711676597595, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.007157758277114591, "std_of_first_answer": 0.007157758277114591, "avg_entropy_of_first_answer": 0.08263142903645833, "lex_sim": 1.0}
{"question": "In which country is the Sierra Madre mountain range, with its highest peak at Cerro Mohinora?", "answer": ["Nicaragua"], "scores": [[0.099853515625, 0.99462890625, 0.994140625]], "normalized_score": [0.4621937870979309], "tokens": [["\u2581Nic", "ar", "agua"]], "entropy": [[1.9375, 0.043853759765625, 0.04827880859375]], "avg_entropy": [0.676544189453125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mexican Republic", "score_of_biggest_cluster": 0.4621937870979309, "score_of_first_answer": 0.4621937870979309, "max_score_of_first_answer": 0.99462890625, "min_score_of_first_answer": 0.099853515625, "avg_score": 0.4621937870979309, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4216861223549775, "std_of_first_answer": 0.4216861223549775, "avg_entropy_of_first_answer": 0.676544189453125, "lex_sim": 1.0}
{"question": "Retailer Fortnum and Mason opened its first new UK store for more than 300 years at which London station in November 2013?", "answer": ["King's Cross"], "scores": [[0.376220703125, 0.95849609375, 1.0, 0.99658203125]], "normalized_score": [0.7742594480514526], "tokens": [["\u2581King", "'", "s", "\u2581Cross"]], "entropy": [[2.3984375, 0.2705078125, 0.0023632049560546875, 0.0277252197265625]], "avg_entropy": [0.6747584342956543], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Saint Pancras railway station", "score_of_biggest_cluster": 0.7742594480514526, "score_of_first_answer": 0.7742594480514526, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.376220703125, "avg_score": 0.7742594480514526, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2641233393878249, "std_of_first_answer": 0.2641233393878249, "avg_entropy_of_first_answer": 0.6747584342956543, "lex_sim": 1.0}
{"question": "What is the name of pigment that gives plants and leaves their colors and also absorbs light in the blue and red portions of the spectrum?", "answer": ["Chlorophyll"], "scores": [[0.82177734375, 0.9892578125, 0.99951171875, 0.98974609375]], "normalized_score": [0.9469866156578064], "tokens": [["\u2581Ch", "lor", "oph", "yll"]], "entropy": [[0.60205078125, 0.07403564453125, 0.004871368408203125, 0.061187744140625]], "avg_entropy": [0.18553638458251953], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "C55H70O6N4Mg", "score_of_biggest_cluster": 0.9469866156578064, "score_of_first_answer": 0.9469866156578064, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.82177734375, "avg_score": 0.9469866156578064, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.07418451021335518, "std_of_first_answer": 0.07418451021335518, "avg_entropy_of_first_answer": 0.18553638458251953, "lex_sim": 1.0}
{"question": "In which English city is the HQ of the Ordnance Survey?", "answer": ["Southampton"], "scores": [[0.98681640625, 1.0, 1.0]], "normalized_score": [0.9955860376358032], "tokens": [["\u2581South", "am", "pton"]], "entropy": [[0.12152099609375, 0.0027332305908203125, 0.0007562637329101562]], "avg_entropy": [0.04167016347249349], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Southampton", "score_of_biggest_cluster": 0.9955860376358032, "score_of_first_answer": 0.9955860376358032, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.98681640625, "avg_score": 0.9955860376358032, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.00621480569402239, "std_of_first_answer": 0.00621480569402239, "avg_entropy_of_first_answer": 0.04167016347249349, "lex_sim": 1.0}
{"question": "Which cartoon character created by Al Capp lives in Dog Patch?", "answer": ["Li'l Abner"], "scores": [[0.7392578125, 0.994140625, 1.0, 0.99658203125, 1.0]], "normalized_score": [0.9396180510520935], "tokens": [["\u2581Li", "'", "l", "\u2581Ab", "ner"]], "entropy": [[1.236328125, 0.051300048828125, 0.0013341903686523438, 0.029510498046875, 0.0019702911376953125]], "avg_entropy": [0.26408863067626953], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "L'ABNER", "score_of_biggest_cluster": 0.9396180510520935, "score_of_first_answer": 0.9396180510520935, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.7392578125, "avg_score": 0.9396180510520935, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1033928369634549, "std_of_first_answer": 0.1033928369634549, "avg_entropy_of_first_answer": 0.26408863067626953, "lex_sim": 1.0}
{"question": "Last week saw the passing of Bob Guccione, founder and publisher of what adult magazine, as well as producing that well known high art film, Caligula?", "answer": ["Penthouse magazine and Caligula (film)"], "scores": [[0.9306640625, 0.99609375, 0.007171630859375, 0.047454833984375, 0.9423828125, 0.998046875, 0.98974609375, 0.146484375, 0.9501953125, 0.99658203125]], "normalized_score": [0.36391595005989075], "tokens": [["\u2581Pent", "house", "\u2581magazine", "\u2581and", "\u2581Cal", "ig", "ula", "\u2581(", "film", ")"]], "entropy": [[0.41259765625, 0.034881591796875, 0.6650390625, 0.9091796875, 0.302490234375, 0.0150299072265625, 0.06597900390625, 1.568359375, 0.2626953125, 0.0273284912109375]], "avg_entropy": [0.4263580322265625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Penthouse", "score_of_biggest_cluster": 0.36391595005989075, "score_of_first_answer": 0.36391595005989075, "max_score_of_first_answer": 0.998046875, "min_score_of_first_answer": 0.007171630859375, "avg_score": 0.36391595005989075, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4165541676004884, "std_of_first_answer": 0.4165541676004884, "avg_entropy_of_first_answer": 0.4263580322265625, "lex_sim": 1.0}
{"question": "What company purchased Skype in 2011?", "answer": ["Microsoft"], "scores": [[0.98291015625]], "normalized_score": [0.98291015625], "tokens": [["\u2581Microsoft"]], "entropy": [[0.1680908203125]], "avg_entropy": [0.1680908203125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "'Microsoft Corp", "score_of_biggest_cluster": 0.98291015625, "score_of_first_answer": 0.98291015625, "max_score_of_first_answer": 0.98291015625, "min_score_of_first_answer": 0.98291015625, "avg_score": 0.98291015625, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.1680908203125, "lex_sim": 1.0}
{"question": "What is the technical term for the hollow at the back of the knee, sometimes called the knee pit?", "answer": ["Popliteal fossa"], "scores": [[0.86572265625, 0.99853515625, 0.99951171875, 1.0, 0.484619140625, 1.0]], "normalized_score": [0.8649448752403259], "tokens": [["\u2581Pop", "l", "ite", "al", "\u2581f", "ossa"]], "entropy": [[0.7587890625, 0.01503753662109375, 0.003414154052734375, 0.0013322830200195312, 1.6025390625, 0.0013637542724609375]], "avg_entropy": [0.3970793088277181], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The popliteal fossa", "score_of_biggest_cluster": 0.8649448752403259, "score_of_first_answer": 0.8649448752403259, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.484619140625, "avg_score": 0.8649448752403259, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.18836312402304337, "std_of_first_answer": 0.18836312402304337, "avg_entropy_of_first_answer": 0.3970793088277181, "lex_sim": 1.0}
{"question": "Steve Martin and which other actor hosted the 2010 Academy Awards?", "answer": ["Alec Baldwin Transcriber's Note:"], "scores": [[0.9931640625, 1.0, 0.99853515625, 1.0, 0.0, 0.00878143310546875, 0.9296875, 0.08917236328125, 0.99951171875, 0.55810546875, 0.97802734375]], "normalized_score": [0.0], "tokens": [["\u2581Ale", "c", "\u2581Bald", "win", "\u2581Trans", "cri", "ber", "'", "s", "\u2581Note", ":"]], "entropy": [[0.058563232421875, 0.0003046989440917969, 0.01715087890625, 0.0017862319946289062, 0.68701171875, 3.001953125, 0.430419921875, 2.982421875, 0.00539398193359375, 0.95654296875, 0.18798828125]], "avg_entropy": [0.7572306286204945], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Alec Baldwin", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4193010458289443, "std_of_first_answer": 0.4193010458289443, "avg_entropy_of_first_answer": 0.7572306286204945, "lex_sim": 1.0}
{"question": "Who succeeded Brezhnev as Soviet premier?", "answer": ["Yuri Andropov #MondayMorningMoment: The Magic Of Community"], "scores": [[0.95068359375, 0.96240234375, 0.96484375, 0.9990234375, 1.0, 2.3245811462402344e-06, 0.0005822181701660156, 0.003818511962890625, 0.970703125, 0.061248779296875, 0.99853515625, 0.06695556640625, 0.0017461776733398438, 0.07012939453125, 0.067138671875, 0.0003294944763183594, 0.0195159912109375, 0.00024116039276123047]], "normalized_score": [0.029150357469916344], "tokens": [["\u2581Y", "uri", "\u2581And", "rop", "ov", "\u2581#", "M", "ond", "ay", "Mor", "ning", "M", "oment", ":", "\u2581The", "\u2581Magic", "\u2581Of", "\u2581Community"]], "entropy": [[0.45068359375, 0.1719970703125, 0.202392578125, 0.00830841064453125, 0.001995086669921875, 1.009765625, 4.44921875, 1.3583984375, 0.157470703125, 2.91796875, 0.0113525390625, 2.279296875, 0.91552734375, 2.44140625, 4.12109375, 5.3359375, 1.365234375, 4.41796875]], "avg_entropy": [1.7564453548855252], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yuri V. Andropov", "score_of_biggest_cluster": 0.029150357469916344, "score_of_first_answer": 0.029150357469916344, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.3245811462402344e-06, "avg_score": 0.029150357469916344, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.46462954608637674, "std_of_first_answer": 0.46462954608637674, "avg_entropy_of_first_answer": 1.7564453548855252, "lex_sim": 1.0}
{"question": "What were Benjy and Laska, which were sent into space in 1958?", "answer": ["Dogs"], "scores": [[0.246337890625, 0.9990234375]], "normalized_score": [0.49608197808265686], "tokens": [["\u2581D", "ogs"]], "entropy": [[3.71875, 0.00981903076171875]], "avg_entropy": [1.8642845153808594], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\ud83d\udc01", "score_of_biggest_cluster": 0.49608197808265686, "score_of_first_answer": 0.49608197808265686, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.246337890625, "avg_score": 0.49608197808265686, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3763427734375, "std_of_first_answer": 0.3763427734375, "avg_entropy_of_first_answer": 1.8642845153808594, "lex_sim": 1.0}
{"question": "Which patriotic Irving Berlin song includes the line  From the mountains to the prairie to the oceans white with foam ?", "answer": ["God Bless America"], "scores": [[0.1353759765625, 0.998046875, 1.0, 0.99853515625]], "normalized_score": [0.6060577034950256], "tokens": [["\u2581God", "\u2581B", "less", "\u2581America"]], "entropy": [[0.9189453125, 0.018218994140625, 0.0011119842529296875, 0.0146942138671875]], "avg_entropy": [0.23824262619018555], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "God Bless America (Celine Dion song)", "score_of_biggest_cluster": 0.6060577034950256, "score_of_first_answer": 0.6060577034950256, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.1353759765625, "avg_score": 0.6060577034950256, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3739005340058761, "std_of_first_answer": 0.3739005340058761, "avg_entropy_of_first_answer": 0.23824262619018555, "lex_sim": 1.0}
{"question": "In Greek mythology who was the Queen of the Gods, also the wife and sister of Zeus?", "answer": ["Hera"], "scores": [[0.98681640625, 1.0]], "normalized_score": [0.9933863282203674], "tokens": [["\u2581H", "era"]], "entropy": [[0.126220703125, 0.0009713172912597656]], "avg_entropy": [0.06359601020812988], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hera (particle accelerator)", "score_of_biggest_cluster": 0.9933863282203674, "score_of_first_answer": 0.9933863282203674, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.98681640625, "avg_score": 0.9933863282203674, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.006591796875, "std_of_first_answer": 0.006591796875, "avg_entropy_of_first_answer": 0.06359601020812988, "lex_sim": 1.0}
{"question": "Gentoo, Adelie and Macaroni are all species of which bird?", "answer": ["Penguin #FictionFriday: Together"], "scores": [[0.95068359375, 0.97998046875, 0.85302734375, 4.172325134277344e-07, 0.00574493408203125, 0.0006427764892578125, 0.0955810546875, 0.259033203125, 0.99853515625, 0.0269927978515625, 0.0017576217651367188, 0.0037784576416015625]], "normalized_score": [0.02047433890402317], "tokens": [["\u2581P", "engu", "in", "\u2581#", "F", "iction", "F", "rid", "ay", ":", "\u2581T", "ogether"]], "entropy": [[0.3486328125, 0.1514892578125, 0.4267578125, 0.7236328125, 4.828125, 3.40625, 2.5859375, 2.271484375, 0.0126953125, 1.63671875, 4.2734375, 3.376953125]], "avg_entropy": [2.003509521484375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Penquins", "score_of_biggest_cluster": 0.02047433890402317, "score_of_first_answer": 0.02047433890402317, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 4.172325134277344e-07, "avg_score": 0.02047433890402317, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.42942044311094624, "std_of_first_answer": 0.42942044311094624, "avg_entropy_of_first_answer": 2.003509521484375, "lex_sim": 1.0}
{"question": "Which band released a 1999 album entitled \u2018The Man Who\u2019?", "answer": ["Travis"], "scores": [[0.951171875, 0.990234375]], "normalized_score": [0.9705066084861755], "tokens": [["\u2581Tra", "vis"]], "entropy": [[0.41552734375, 0.060577392578125]], "avg_entropy": [0.2380523681640625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Travis", "score_of_biggest_cluster": 0.9705066084861755, "score_of_first_answer": 0.9705066084861755, "max_score_of_first_answer": 0.990234375, "min_score_of_first_answer": 0.951171875, "avg_score": 0.9705066084861755, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.01953125, "std_of_first_answer": 0.01953125, "avg_entropy_of_first_answer": 0.2380523681640625, "lex_sim": 1.0}
{"question": "In June 1971, who was the first indigenous Australian to become a member of the Parliament of Australia?", "answer": ["Yindjibarndi tribe leader"], "scores": [[0.0025196075439453125, 0.0111541748046875, 0.904296875, 0.958984375, 0.998046875, 0.9267578125, 0.99951171875, 0.0016078948974609375, 0.10504150390625]], "normalized_score": [0.11600874364376068], "tokens": [["\u2581Y", "ind", "j", "ib", "ar", "nd", "i", "\u2581tribe", "\u2581leader"]], "entropy": [[2.486328125, 3.91796875, 0.578125, 0.295654296875, 0.0207366943359375, 0.490478515625, 0.00601959228515625, 2.830078125, 2.21875]], "avg_entropy": [1.4271265665690105], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Neville Bonner", "score_of_biggest_cluster": 0.11600874364376068, "score_of_first_answer": 0.11600874364376068, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.0016078948974609375, "avg_score": 0.11600874364376068, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46262086222489907, "std_of_first_answer": 0.46262086222489907, "avg_entropy_of_first_answer": 1.4271265665690105, "lex_sim": 1.0}
{"question": "\"What animal did Will Smith's character use to commit suicide in the movie \"\"7 Pounds\"\"?\"", "answer": ["No information or references found in provided knowledge cutoff that match the specific question."], "scores": [[0.003910064697265625, 0.031768798828125, 0.04669189453125, 0.0012073516845703125, 0.40771484375, 0.018157958984375, 0.0016393661499023438, 0.062286376953125, 0.342529296875, 0.68994140625, 0.0152130126953125, 0.11053466796875, 0.533203125, 0.07586669921875, 0.2354736328125, 0.56689453125]], "normalized_score": [0.056683968752622604], "tokens": [["\u2581No", "\u2581information", "\u2581or", "\u2581references", "\u2581found", "\u2581in", "\u2581provided", "\u2581knowledge", "\u2581cut", "off", "\u2581that", "\u2581match", "\u2581the", "\u2581specific", "\u2581question", "."]], "entropy": [[3.138671875, 2.869140625, 2.962890625, 2.787109375, 1.9951171875, 1.83984375, 1.4453125, 2.30859375, 1.9326171875, 0.86572265625, 2.33203125, 3.158203125, 1.4189453125, 2.349609375, 2.4453125, 1.998046875]], "avg_entropy": [2.240447998046875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Box jellyfish", "score_of_biggest_cluster": 0.056683968752622604, "score_of_first_answer": 0.056683968752622604, "max_score_of_first_answer": 0.68994140625, "min_score_of_first_answer": 0.0012073516845703125, "avg_score": 0.056683968752622604, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2276108061035039, "std_of_first_answer": 0.2276108061035039, "avg_entropy_of_first_answer": 2.240447998046875, "lex_sim": 1.0}
{"question": "On which island in the North Sea did both St Aidan and St Cuthbert live?", "answer": ["Lindisfarne #AskAlliance April 11, 2017"], "scores": [[0.52587890625, 0.998046875, 0.9990234375, 0.99853515625, 7.152557373046875e-07, 0.0016298294067382812, 0.2320556640625, 0.000995635986328125, 0.0026683807373046875, 1.430511474609375e-06, 0.78564453125, 0.385986328125, 0.09722900390625, 0.75830078125, 0.970703125, 0.9775390625, 0.99462890625, 0.038970947265625, 0.136962890625]], "normalized_score": [0.047073040157556534], "tokens": [["\u2581Lind", "isf", "ar", "ne", "\u2581#", "A", "sk", "Al", "liance", "\u2581April", "\u2581", "1", "1", ",", "\u2581", "2", "0", "1", "7"]], "entropy": [[1.54296875, 0.015350341796875, 0.0107574462890625, 0.01189422607421875, 1.2197265625, 4.77734375, 2.78125, 5.36328125, 2.57421875, 1.876953125, 1.2802734375, 1.533203125, 2.490234375, 1.001953125, 0.2178955078125, 0.11859130859375, 0.04364013671875, 0.1827392578125, 1.3212890625]], "avg_entropy": [1.4928191335577714], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lindisfarne", "score_of_biggest_cluster": 0.047073040157556534, "score_of_first_answer": 0.047073040157556534, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 7.152557373046875e-07, "avg_score": 0.047073040157556534, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4234126939839411, "std_of_first_answer": 0.4234126939839411, "avg_entropy_of_first_answer": 1.4928191335577714, "lex_sim": 1.0}
{"question": "Had war not intervened, which city would have hosted the 1916 Olympics?", "answer": ["Berlin #62: The Lost City of Atlantis Found?"], "scores": [[0.98583984375, 2.980232238769531e-07, 0.001918792724609375, 0.04107666015625, 0.0022258758544921875, 0.024200439453125, 8.046627044677734e-06, 0.484619140625, 0.368408203125, 0.74951171875, 1.0, 0.99951171875, 1.5139579772949219e-05, 0.06396484375, 0.01751708984375, 0.01383209228515625]], "normalized_score": [0.012406378984451294], "tokens": [["\u2581Berlin", "\u2581#", "6", "2", ":", "\u2581The", "\u2581Lost", "\u2581City", "\u2581of", "\u2581Atl", "ant", "is", "\u2581Found", "?", "\u2581", "0"]], "entropy": [[0.129638671875, 0.89794921875, 5.890625, 3.931640625, 3.587890625, 5.5703125, 2.63671875, 2.251953125, 2.640625, 1.4951171875, 0.0008521080017089844, 0.005260467529296875, 3.119140625, 3.404296875, 3.578125, 3.67578125]], "avg_entropy": [2.6759954392910004], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Berlin-Zentrum", "score_of_biggest_cluster": 0.012406378984451294, "score_of_first_answer": 0.012406378984451294, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.980232238769531e-07, "avg_score": 0.012406378984451294, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39515765864175695, "std_of_first_answer": 0.39515765864175695, "avg_entropy_of_first_answer": 2.6759954392910004, "lex_sim": 1.0}
{"question": "The flag of the Great Socialist People's Libyan Arab Jamahiriya was what single colour?", "answer": ["Red"], "scores": [[0.3642578125]], "normalized_score": [0.3642578125], "tokens": [["\u2581Red"]], "entropy": [[1.9423828125]], "avg_entropy": [1.9423828125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Greenishly", "score_of_biggest_cluster": 0.3642578125, "score_of_first_answer": 0.3642578125, "max_score_of_first_answer": 0.3642578125, "min_score_of_first_answer": 0.3642578125, "avg_score": 0.3642578125, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.9423828125, "lex_sim": 1.0}
{"question": "Which anti-conformist movement in 1950's fashion began in New York's West Village and was characterised by black slim trousers , thick sweaters and untucked shirts ?", "answer": ["Beatnik movement Painting with Dots: Art of Yayoi Kusama"], "scores": [[0.5419921875, 0.6650390625, 0.0169677734375, 4.76837158203125e-07, 0.00033974647521972656, 0.0011701583862304688, 0.004123687744140625, 0.001068115234375, 0.2122802734375, 0.086181640625, 0.007213592529296875, 0.049468994140625, 0.90087890625, 0.9873046875, 0.99951171875, 0.9921875, 0.9990234375, 0.99267578125]], "normalized_score": [0.038139890879392624], "tokens": [["\u2581Beat", "nik", "\u2581movement", "\u2581P", "ain", "ting", "\u2581with", "\u2581D", "ots", ":", "\u2581Art", "\u2581of", "\u2581Y", "ay", "oi", "\u2581K", "us", "ama"]], "entropy": [[1.974609375, 1.29296875, 1.232421875, 0.81298828125, 2.548828125, 0.479248046875, 4.16015625, 4.99609375, 3.1953125, 4.015625, 4.21875, 2.681640625, 0.73486328125, 0.08636474609375, 0.0034847259521484375, 0.061431884765625, 0.0104522705078125, 0.0523681640625]], "avg_entropy": [1.808755980597602], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Beatnick", "score_of_biggest_cluster": 0.038139890879392624, "score_of_first_answer": 0.038139890879392624, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 4.76837158203125e-07, "avg_score": 0.038139890879392624, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43811285451636384, "std_of_first_answer": 0.43811285451636384, "avg_entropy_of_first_answer": 1.808755980597602, "lex_sim": 1.0}
{"question": "The Southern Alps can be found in which country?", "answer": ["New Zealand"], "scores": [[0.81201171875, 0.94970703125]], "normalized_score": [0.8781647086143494], "tokens": [["\u2581New", "\u2581Zealand"]], "entropy": [[1.10546875, 0.356201171875]], "avg_entropy": [0.7308349609375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "N Z", "score_of_biggest_cluster": 0.8781647086143494, "score_of_first_answer": 0.8781647086143494, "max_score_of_first_answer": 0.94970703125, "min_score_of_first_answer": 0.81201171875, "avg_score": 0.8781647086143494, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06884765625, "std_of_first_answer": 0.06884765625, "avg_entropy_of_first_answer": 0.7308349609375, "lex_sim": 1.0}
{"question": "What instrument was played by jazz musician Bix Beiderbeck?", "answer": ["Cornet #PsychicPredictions for 2020"], "scores": [[0.72216796875, 0.96533203125, 1.7881393432617188e-06, 0.0005064010620117188, 0.0014562606811523438, 0.56396484375, 0.004245758056640625, 0.01308441162109375, 0.5107421875, 0.9892578125, 0.0006756782531738281, 0.498046875, 0.8671875, 0.9609375, 0.95458984375, 0.044677734375]], "normalized_score": [0.04365566372871399], "tokens": [["\u2581Cor", "net", "\u2581#", "P", "s", "ych", "ic", "Pred", "ict", "ions", "\u2581for", "\u2581", "2", "0", "2", "0"]], "entropy": [[1.1064453125, 0.2164306640625, 0.880859375, 3.77734375, 4.05859375, 2.912109375, 2.640625, 4.0078125, 0.9580078125, 0.08453369140625, 1.7548828125, 2.296875, 0.5986328125, 0.28955078125, 0.27734375, 1.4228515625]], "avg_entropy": [1.7051811218261719], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Trumpet.", "score_of_biggest_cluster": 0.04365566372871399, "score_of_first_answer": 0.04365566372871399, "max_score_of_first_answer": 0.9892578125, "min_score_of_first_answer": 1.7881393432617188e-06, "avg_score": 0.04365566372871399, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4107904955304291, "std_of_first_answer": 0.4107904955304291, "avg_entropy_of_first_answer": 1.7051811218261719, "lex_sim": 1.0}
{"question": "\"Who is a South African serial killer who committed the \"\"ABC Murders\"\" between 1994 and 1995, with over 30 victims?\"", "answer": ["Hector Pieterson #DXCLUSIVE: G.L.O drops new single \"U Already Know\""], "scores": [[0.007427215576171875, 0.290771484375, 0.97412109375, 0.97314453125, 2.980232238769531e-07, 0.0031414031982421875, 0.0002338886260986328, 0.00011873245239257812, 0.11865234375, 0.88134765625, 0.03515625, 0.00421142578125, 0.002391815185546875, 0.007114410400390625, 0.92578125, 0.6181640625, 1.329183578491211e-05, 0.98681640625, 0.166259765625, 0.37353515625, 0.62890625, 0.0042572021484375, 0.0027637481689453125, 0.994140625, 0.91845703125, 0.94580078125]], "normalized_score": [0.027987098321318626], "tokens": [["\u2581H", "ector", "\u2581Piet", "erson", "\u2581#", "D", "X", "CL", "US", "IVE", ":", "\u2581G", ".", "L", ".", "O", "\u2581dro", "ps", "\u2581new", "\u2581single", "\u2581\"", "U", "\u2581Al", "ready", "\u2581Know", "\""]], "entropy": [[5.0703125, 1.857421875, 0.240966796875, 0.1282958984375, 1.322265625, 5.80078125, 4.84375, 3.720703125, 2.96875, 0.495361328125, 2.43359375, 4.55859375, 3.326171875, 4.01953125, 0.65673828125, 2.203125, 0.39697265625, 0.0733642578125, 3.033203125, 2.58984375, 1.841796875, 5.4140625, 3.677734375, 0.049163818359375, 0.39501953125, 0.35791015625]], "avg_entropy": [2.364439744215745], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Moses Sithole", "score_of_biggest_cluster": 0.027987098321318626, "score_of_first_answer": 0.027987098321318626, "max_score_of_first_answer": 0.994140625, "min_score_of_first_answer": 2.980232238769531e-07, "avg_score": 0.027987098321318626, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41757332959850724, "std_of_first_answer": 0.41757332959850724, "avg_entropy_of_first_answer": 2.364439744215745, "lex_sim": 1.0}
{"question": "\"\"\"Fawlty Towers\"\" was written by John Cleese and Connie Booth, who played main characters. Which character did Connie Booth play?\"", "answer": ["Basil's wife, Spanish waitress Manuel's love interest and later wife Proof Positive: The Story of the First Tank Crews"], "scores": [[0.81689453125, 0.9990234375, 0.68798828125, 1.0, 0.94091796875, 0.7001953125, 0.0003705024719238281, 0.703125, 0.98974609375, 0.06988525390625, 0.90380859375, 1.0, 0.3447265625, 0.9970703125, 0.32080078125, 0.141357421875, 0.341064453125, 0.0, 0.00048422813415527344, 0.9970703125, 0.056060791015625, 0.00738525390625, 4.231929779052734e-06, 0.64892578125, 0.045257568359375, 0.005710601806640625, 0.0009012222290039062, 0.0039215087890625, 0.1358642578125, 0.98291015625, 0.2095947265625]], "normalized_score": [0.0], "tokens": [["\u2581Bas", "il", "'", "s", "\u2581wife", ",", "\u2581Spanish", "\u2581wait", "ress", "\u2581Manuel", "'", "s", "\u2581love", "\u2581interest", "\u2581and", "\u2581later", "\u2581wife", "\u2581Proof", "\u2581Pos", "itive", ":", "\u2581The", "\u2581Story", "\u2581of", "\u2581the", "\u2581First", "\u2581T", "ank", "\u2581C", "rew", "s"]], "entropy": [[0.92529296875, 0.0125579833984375, 0.78759765625, 7.206201553344727e-05, 0.405029296875, 1.07421875, 1.03515625, 1.396484375, 0.058990478515625, 2.37890625, 0.529296875, 0.0010309219360351562, 1.3935546875, 0.0273590087890625, 1.498046875, 3.515625, 2.533203125, 1.2021484375, 3.642578125, 0.024566650390625, 3.73828125, 3.298828125, 4.17578125, 0.8955078125, 0.998046875, 4.69140625, 5.1015625, 3.13671875, 4.0390625, 0.1461181640625, 3.361328125]], "avg_entropy": [1.8072373347897683], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Polly", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41123323904038717, "std_of_first_answer": 0.41123323904038717, "avg_entropy_of_first_answer": 1.8072373347897683, "lex_sim": 1.0}
{"question": "The Florentine Girdle was a type of what?", "answer": ["Armor"], "scores": [[0.1129150390625, 0.76806640625]], "normalized_score": [0.2944931983947754], "tokens": [["\u2581Arm", "or"]], "entropy": [[4.4453125, 0.634765625]], "avg_entropy": [2.5400390625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Girdle of chastity", "score_of_biggest_cluster": 0.2944931983947754, "score_of_first_answer": 0.2944931983947754, "max_score_of_first_answer": 0.76806640625, "min_score_of_first_answer": 0.1129150390625, "avg_score": 0.2944931983947754, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32757568359375, "std_of_first_answer": 0.32757568359375, "avg_entropy_of_first_answer": 2.5400390625, "lex_sim": 1.0}
{"question": "The sinking of what battleship in Havana harbor on Feb 15, 1898 led the United States to declare war on Spain on April 25?", "answer": ["Battleship Maine (ACR-1) #VitalFactz: 13th Anniversary - The Game's \"Doctor's Advocate\" Album Released"], "scores": [[0.0181121826171875, 0.99951171875, 0.99609375, 0.9443359375, 0.0014591217041015625, 0.047698974609375, 0.99365234375, 0.93310546875, 0.99853515625, 0.9892578125, 6.079673767089844e-06, 7.724761962890625e-05, 4.5239925384521484e-05, 0.0253448486328125, 0.0015649795532226562, 0.08935546875, 0.10693359375, 0.35009765625, 0.012420654296875, 0.1302490234375, 0.00017654895782470703, 0.9853515625, 0.98486328125, 0.004573822021484375, 0.053497314453125, 5.3822994232177734e-05, 0.01393890380859375, 0.984375, 0.3271484375, 0.00010675191879272461, 0.02618408203125, 0.998046875, 1.0, 0.99853515625, 0.99951171875, 1.0, 0.99169921875, 0.134033203125, 0.537109375, 0.99755859375]], "normalized_score": [0.05622607842087746], "tokens": [["\u2581Batt", "les", "hip", "\u2581Maine", "\u2581(", "AC", "R", "-", "1", ")", "\u2581#", "V", "ital", "Fact", "z", ":", "\u2581", "1", "3", "th", "\u2581An", "ni", "versary", "\u2581-", "\u2581The", "\u2581Game", "'", "s", "\u2581\"", "Do", "ctor", "'", "s", "\u2581Adv", "oc", "ate", "\"", "\u2581Album", "\u2581Re", "leased"]], "entropy": [[1.328125, 0.005634307861328125, 0.028717041015625, 0.387451171875, 0.75048828125, 3.998046875, 0.043243408203125, 0.35205078125, 0.0138702392578125, 0.08392333984375, 0.712890625, 1.978515625, 3.68359375, 4.39453125, 2.484375, 2.58984375, 3.8359375, 1.626953125, 1.7822265625, 3.99609375, 3.951171875, 0.1353759765625, 0.12359619140625, 1.6650390625, 4.5078125, 3.0390625, 3.794921875, 0.1273193359375, 3.8671875, 0.3466796875, 0.8359375, 0.0197601318359375, 0.0006947517395019531, 0.01300048828125, 0.006191253662109375, 0.001453399658203125, 0.0711669921875, 2.62109375, 2.11328125, 0.021453857421875]], "avg_entropy": [1.5334677815437316], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "USS Maine", "score_of_biggest_cluster": 0.05622607842087746, "score_of_first_answer": 0.05622607842087746, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 6.079673767089844e-06, "avg_score": 0.05622607842087746, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4598489767503535, "std_of_first_answer": 0.4598489767503535, "avg_entropy_of_first_answer": 1.5334677815437316, "lex_sim": 1.0}
{"question": "Aung San Suu Kyi is a controversial leader in which country?", "answer": ["Burma/Myanmar"], "scores": [[0.1260986328125, 0.99755859375, 0.06829833984375, 0.99267578125, 1.0, 0.9990234375]], "normalized_score": [0.4519325792789459], "tokens": [["\u2581Bur", "ma", "/", "My", "an", "mar"]], "entropy": [[0.50146484375, 0.020751953125, 1.3984375, 0.051605224609375, 0.001949310302734375, 0.01114654541015625]], "avg_entropy": [0.33089256286621094], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Economic sanctions against Myanmar", "score_of_biggest_cluster": 0.4519325792789459, "score_of_first_answer": 0.4519325792789459, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.06829833984375, "avg_score": 0.4519325792789459, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42465289450095217, "std_of_first_answer": 0.42465289450095217, "avg_entropy_of_first_answer": 0.33089256286621094, "lex_sim": 1.0}
{"question": "Two all beef patties, special sauce, lettuce, cheese, pickles, onions on a sesame seed bun describes what?", "answer": ["A Big Mac, a type of hamburger sold by the fast food restaurant chain McDonald's"], "scores": [[0.81689453125, 0.11224365234375, 0.9833984375, 0.10186767578125, 0.9267578125, 0.5888671875, 0.99951171875, 0.80224609375, 1.0, 1.0, 0.286376953125, 0.96875, 0.2442626953125, 0.97998046875, 0.53466796875, 0.61328125, 0.9765625, 0.970703125, 0.998046875, 0.9912109375, 1.0]], "normalized_score": [0.6389107704162598], "tokens": [["\u2581A", "\u2581Big", "\u2581Mac", ",", "\u2581a", "\u2581type", "\u2581of", "\u2581h", "amb", "urger", "\u2581sold", "\u2581by", "\u2581the", "\u2581fast", "\u2581food", "\u2581restaurant", "\u2581chain", "\u2581Mc", "Donald", "'", "s"]], "entropy": [[0.9619140625, 1.8935546875, 0.106201171875, 1.759765625, 0.3916015625, 1.5244140625, 0.003025054931640625, 0.6962890625, 1.7464160919189453e-05, 0.0017719268798828125, 1.9287109375, 0.1470947265625, 0.6865234375, 0.129150390625, 0.69873046875, 0.68994140625, 0.11761474609375, 0.158447265625, 0.0175933837890625, 0.06378173828125, 1.6868114471435547e-05]], "avg_entropy": [0.5702933356875465], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Big Mac Sauce", "score_of_biggest_cluster": 0.6389107704162598, "score_of_first_answer": 0.6389107704162598, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.10186767578125, "avg_score": 0.6389107704162598, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3124946295618434, "std_of_first_answer": 0.3124946295618434, "avg_entropy_of_first_answer": 0.5702933356875465, "lex_sim": 1.0}
{"question": "Which superhero film, produced by Marvel Studios and written and directed by Joss whedon, was the highest grossing film world-wide in 2012 ?", "answer": ["The Avengers (2012 film) #BookBlitz: The Girl with the Crystal Mind by Jason Tesar"], "scores": [[0.7724609375, 0.95947265625, 0.9970703125, 0.042999267578125, 0.93408203125, 1.0, 1.0, 1.0, 0.64013671875, 0.9970703125, 3.5762786865234375e-06, 7.927417755126953e-06, 1.0073184967041016e-05, 0.058837890625, 0.0017805099487304688, 0.123046875, 0.0010128021240234375, 0.177734375, 0.68408203125, 0.0261688232421875, 1.0, 0.0030364990234375, 0.363525390625, 0.0028896331787109375, 0.047576904296875, 0.85107421875]], "normalized_score": [0.0469510592520237], "tokens": [["\u2581The", "\u2581Aven", "gers", "\u2581(", "2", "0", "1", "2", "\u2581film", ")", "\u2581#", "Book", "Bl", "itz", ":", "\u2581The", "\u2581Girl", "\u2581with", "\u2581the", "\u2581Cry", "stal", "\u2581Mind", "\u2581by", "\u2581Jason", "\u2581T", "esar"]], "entropy": [[1.0390625, 0.376220703125, 0.0270233154296875, 0.96923828125, 0.328125, 0.0014247894287109375, 0.0006422996520996094, 0.001483917236328125, 0.7578125, 0.026275634765625, 0.7294921875, 4.14453125, 4.44140625, 1.4345703125, 2.673828125, 4.87890625, 5.7734375, 2.83984375, 1.470703125, 2.61328125, 0.0005364418029785156, 2.744140625, 2.546875, 4.43359375, 4.40234375, 1.0830078125]], "avg_entropy": [1.912992550776555], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Avengers (cartoon)", "score_of_biggest_cluster": 0.0469510592520237, "score_of_first_answer": 0.0469510592520237, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 3.5762786865234375e-06, "avg_score": 0.0469510592520237, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43485815939204564, "std_of_first_answer": 0.43485815939204564, "avg_entropy_of_first_answer": 1.912992550776555, "lex_sim": 1.0}
{"question": "Who won the award for the Best Actor at the 2015 BAFTA film awards", "answer": ["BDS #40DaysOfDating is the latest (and maybe greatest) social media experiment"], "scores": [[2.6941299438476562e-05, 0.0016145706176757812, 2.562999725341797e-05, 0.01065826416015625, 0.10986328125, 2.6226043701171875e-06, 0.16259765625, 0.05023193359375, 0.136474609375, 0.0031414031982421875, 0.0290374755859375, 0.0157470703125, 0.0011196136474609375, 0.07647705078125, 0.01537322998046875, 0.10906982421875, 0.80517578125, 0.06390380859375, 0.396484375, 0.70654296875]], "normalized_score": [0.01266889926046133], "tokens": [["\u2581B", "DS", "\u2581#", "4", "0", "Days", "Of", "D", "ating", "\u2581is", "\u2581the", "\u2581latest", "\u2581(", "and", "\u2581maybe", "\u2581greatest", ")", "\u2581social", "\u2581media", "\u2581experiment"]], "entropy": [[0.25830078125, 2.94921875, 3.1796875, 5.6328125, 3.619140625, 2.736328125, 2.958984375, 4.1015625, 2.9609375, 3.740234375, 2.03125, 5.19921875, 3.8515625, 3.9375, 3.232421875, 2.373046875, 0.73046875, 4.08984375, 0.89208984375, 1.5322265625]], "avg_entropy": [3.000341796875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eddie redmayne", "score_of_biggest_cluster": 0.01266889926046133, "score_of_first_answer": 0.01266889926046133, "max_score_of_first_answer": 0.80517578125, "min_score_of_first_answer": 2.6226043701171875e-06, "avg_score": 0.01266889926046133, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.22637197088685068, "std_of_first_answer": 0.22637197088685068, "avg_entropy_of_first_answer": 3.000341796875, "lex_sim": 1.0}
{"question": "Who was the star of the 1940 satirical film 'The Great Dictator'?", "answer": ["Charlie #1020: Noctilucent Clouds"], "scores": [[0.96533203125, 1.7881393432617188e-07, 0.0478515625, 0.05291748046875, 0.048736572265625, 0.080078125, 0.001201629638671875, 0.00312042236328125, 0.0014247894287109375, 0.1201171875, 0.1495361328125, 0.2352294921875, 0.26416015625, 0.8720703125]], "normalized_score": [0.022564426064491272], "tokens": [["\u2581Charlie", "\u2581#", "1", "0", "2", "0", ":", "\u2581No", "ct", "il", "uc", "ent", "\u2581Cloud", "s"]], "entropy": [[0.1837158203125, 0.04449462890625, 4.65625, 3.052734375, 3.12890625, 3.560546875, 2.541015625, 5.91796875, 5.1328125, 1.857421875, 1.6396484375, 1.9345703125, 2.06640625, 0.900390625]], "avg_entropy": [2.6154915945870534], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Roy Export Company Establishment", "score_of_biggest_cluster": 0.022564426064491272, "score_of_first_answer": 0.022564426064491272, "max_score_of_first_answer": 0.96533203125, "min_score_of_first_answer": 1.7881393432617188e-07, "avg_score": 0.022564426064491272, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30370699807441803, "std_of_first_answer": 0.30370699807441803, "avg_entropy_of_first_answer": 2.6154915945870534, "lex_sim": 1.0}
{"question": "On the human body, a keloid is a type of what?", "answer": ["Scar"], "scores": [[0.63720703125, 0.96484375]], "normalized_score": [0.7840951681137085], "tokens": [["\u2581Sc", "ar"]], "entropy": [[1.201171875, 0.2110595703125]], "avg_entropy": [0.70611572265625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Uloid", "score_of_biggest_cluster": 0.7840951681137085, "score_of_first_answer": 0.7840951681137085, "max_score_of_first_answer": 0.96484375, "min_score_of_first_answer": 0.63720703125, "avg_score": 0.7840951681137085, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.163818359375, "std_of_first_answer": 0.163818359375, "avg_entropy_of_first_answer": 0.70611572265625, "lex_sim": 1.0}
{"question": "Who did Betty Boothroyd replace as Speaker of the House of Commons on April 27th 1992?", "answer": ["Bernard Weatherill"], "scores": [[0.849609375, 0.9658203125, 0.99853515625, 0.9970703125]], "normalized_score": [0.9507168531417847], "tokens": [["\u2581Bernard", "\u2581We", "ather", "ill"]], "entropy": [[0.68701171875, 0.1885986328125, 0.0108184814453125, 0.0222930908203125]], "avg_entropy": [0.22718048095703125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Baron Weatherill", "score_of_biggest_cluster": 0.9507168531417847, "score_of_first_answer": 0.9507168531417847, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.849609375, "avg_score": 0.9507168531417847, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.060970058057587526, "std_of_first_answer": 0.060970058057587526, "avg_entropy_of_first_answer": 0.22718048095703125, "lex_sim": 1.0}
{"question": "Which is the most powerful chess piece?", "answer": ["Queen"], "scores": [[0.391845703125]], "normalized_score": [0.391845703125], "tokens": [["\u2581Queen"]], "entropy": [[2.09765625]], "avg_entropy": [2.09765625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Quene", "score_of_biggest_cluster": 0.391845703125, "score_of_first_answer": 0.391845703125, "max_score_of_first_answer": 0.391845703125, "min_score_of_first_answer": 0.391845703125, "avg_score": 0.391845703125, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.09765625, "lex_sim": 1.0}
{"question": "In humans, Gorham disease affects which part of the body?", "answer": ["Bones"], "scores": [[0.358642578125, 0.7255859375]], "normalized_score": [0.5101235508918762], "tokens": [["\u2581B", "ones"]], "entropy": [[2.474609375, 0.61279296875]], "avg_entropy": [1.543701171875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bone lining cell", "score_of_biggest_cluster": 0.5101235508918762, "score_of_first_answer": 0.5101235508918762, "max_score_of_first_answer": 0.7255859375, "min_score_of_first_answer": 0.358642578125, "avg_score": 0.5101235508918762, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.1834716796875, "std_of_first_answer": 0.1834716796875, "avg_entropy_of_first_answer": 1.543701171875, "lex_sim": 1.0}
{"question": "Which large American bird (Gymnogyps) has been reintroduced to the wild after breeding in captivity in 1991?", "answer": ["Whooping Crane"], "scores": [[0.40380859375, 0.7197265625, 0.99609375, 0.21826171875, 0.962890625]], "normalized_score": [0.5712675452232361], "tokens": [["\u2581Who", "op", "ing", "\u2581C", "rane"]], "entropy": [[1.091796875, 0.61279296875, 0.038055419921875, 0.859375, 0.1658935546875]], "avg_entropy": [0.553582763671875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "(California) Condor", "score_of_biggest_cluster": 0.5712675452232361, "score_of_first_answer": 0.5712675452232361, "max_score_of_first_answer": 0.99609375, "min_score_of_first_answer": 0.21826171875, "avg_score": 0.5712675452232361, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.30627948852568093, "std_of_first_answer": 0.30627948852568093, "avg_entropy_of_first_answer": 0.553582763671875, "lex_sim": 1.0}
{"question": "What kind of an animal is a wireworm?", "answer": ["Nematode worms (Rhabditida) feeding on plant roots"], "scores": [[0.0159149169921875, 0.984375, 0.94140625, 0.092529296875, 0.99951171875, 0.1483154296875, 0.003055572509765625, 0.037139892578125, 0.25244140625, 0.96728515625, 0.69384765625, 0.8115234375, 0.00313568115234375, 0.407470703125, 0.98046875, 0.70361328125, 0.98291015625]], "normalized_score": [0.2088424563407898], "tokens": [["\u2581N", "emat", "ode", "\u2581w", "orm", "s", "\u2581(", "R", "hab", "dit", "ida", ")", "\u2581feed", "ing", "\u2581on", "\u2581plant", "\u2581roots"]], "entropy": [[2.330078125, 0.1353759765625, 0.291748046875, 1.546875, 0.004528045654296875, 1.111328125, 1.3681640625, 3.427734375, 1.9130859375, 0.1475830078125, 0.8134765625, 0.966796875, 1.3017578125, 0.89892578125, 0.147705078125, 1.2265625, 0.11749267578125]], "avg_entropy": [1.0440716462976791], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Beetle", "score_of_biggest_cluster": 0.2088424563407898, "score_of_first_answer": 0.2088424563407898, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.003055572509765625, "avg_score": 0.2088424563407898, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40751888235294836, "std_of_first_answer": 0.40751888235294836, "avg_entropy_of_first_answer": 1.0440716462976791, "lex_sim": 1.0}
{"question": "What is the generic word for plants that grow in water or water-logged conditions?", "answer": ["\u6c34\u7acb\u82b1"], "scores": [[0.00033354759216308594, 0.0038547515869140625, 0.00567626953125, 0.399169921875]], "normalized_score": [0.007346727419644594], "tokens": [["\u2581", "\u6c34", "\u7acb", "\u82b1"]], "entropy": [[1.5341796875, 3.328125, 2.810546875, 3.591796875]], "avg_entropy": [2.816162109375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hydrophyte", "score_of_biggest_cluster": 0.007346727419644594, "score_of_first_answer": 0.007346727419644594, "max_score_of_first_answer": 0.399169921875, "min_score_of_first_answer": 0.00033354759216308594, "avg_score": 0.007346727419644594, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17143257662064884, "std_of_first_answer": 0.17143257662064884, "avg_entropy_of_first_answer": 2.816162109375, "lex_sim": 0.0}
{"question": "Whose 1977-79 albums, Low, \u201cHeroes\u201d and Lodger, are known as his Berlin Trilogy?", "answer": ["Bowie"], "scores": [[0.0236358642578125, 0.9697265625]], "normalized_score": [0.15139459073543549], "tokens": [["\u2581B", "owie"]], "entropy": [[0.390380859375, 0.276123046875]], "avg_entropy": [0.333251953125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hermione Farthingale", "score_of_biggest_cluster": 0.15139459073543549, "score_of_first_answer": 0.15139459073543549, "max_score_of_first_answer": 0.9697265625, "min_score_of_first_answer": 0.0236358642578125, "avg_score": 0.15139459073543549, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.47304534912109375, "std_of_first_answer": 0.47304534912109375, "avg_entropy_of_first_answer": 0.333251953125, "lex_sim": 1.0}
{"question": "What colour is Stephanie's hair in the children's TV series LazyTown?", "answer": ["Pumpkin"], "scores": [[0.2017822265625, 0.00036644935607910156, 0.974609375]], "normalized_score": [0.041614286601543427], "tokens": [["\u2581P", "ump", "kin"]], "entropy": [[3.45703125, 0.1121826171875, 0.210693359375]], "avg_entropy": [1.2599690755208333], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pink", "score_of_biggest_cluster": 0.041614286601543427, "score_of_first_answer": 0.041614286601543427, "max_score_of_first_answer": 0.974609375, "min_score_of_first_answer": 0.00036644935607910156, "avg_score": 0.041614286601543427, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4199179013459298, "std_of_first_answer": 0.4199179013459298, "avg_entropy_of_first_answer": 1.2599690755208333, "lex_sim": 1.0}
{"question": "In which country is the most northerly point on mainland Africa?", "answer": ["Dahomey Gap"], "scores": [[0.0013637542724609375, 0.173828125, 0.7578125, 0.9990234375, 0.347412109375, 0.99560546875]], "normalized_score": [0.198985293507576], "tokens": [["\u2581D", "ah", "ome", "y", "\u2581G", "ap"]], "entropy": [[3.58984375, 4.546875, 1.439453125, 0.011749267578125, 1.5498046875, 0.032806396484375]], "avg_entropy": [1.86175537109375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ISO 3166-1:TN", "score_of_biggest_cluster": 0.198985293507576, "score_of_first_answer": 0.198985293507576, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.0013637542724609375, "avg_score": 0.198985293507576, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3930261634073235, "std_of_first_answer": 0.3930261634073235, "avg_entropy_of_first_answer": 1.86175537109375, "lex_sim": 1.0}
{"question": "\"What country's \"\"Kingdom Tower\"\" is planned to be more than a kilometer tall when it is completed in 2018?\"", "answer": ["Saudi Arabia"], "scores": [[0.8525390625, 0.96728515625, 0.9775390625, 0.99755859375]], "normalized_score": [0.9469680786132812], "tokens": [["\u2581Sa", "udi", "\u2581Arab", "ia"]], "entropy": [[1.0029296875, 0.148681640625, 0.1497802734375, 0.0221710205078125]], "avg_entropy": [0.3308906555175781], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Saudia Arabia", "score_of_biggest_cluster": 0.9469680786132812, "score_of_first_answer": 0.9469680786132812, "max_score_of_first_answer": 0.99755859375, "min_score_of_first_answer": 0.8525390625, "avg_score": 0.9469680786132812, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.05659325032511244, "std_of_first_answer": 0.05659325032511244, "avg_entropy_of_first_answer": 0.3308906555175781, "lex_sim": 1.0}
{"question": "Who is the narrator on the UK television show The Only Way Is Essex?", "answer": ["Letitia Dean"], "scores": [[0.001621246337890625, 0.9912109375, 0.99951171875, 0.75]], "normalized_score": [0.18630139529705048], "tokens": [["\u2581Let", "it", "ia", "\u2581Dean"]], "entropy": [[3.611328125, 0.06951904296875, 0.007297515869140625, 1.396484375]], "avg_entropy": [1.2711572647094727], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Denise van Outen", "score_of_biggest_cluster": 0.18630139529705048, "score_of_first_answer": 0.18630139529705048, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.001621246337890625, "avg_score": 0.18630139529705048, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40740425469140923, "std_of_first_answer": 0.40740425469140923, "avg_entropy_of_first_answer": 1.2711572647094727, "lex_sim": 1.0}
{"question": "What is the name of the heroine in the story of 'Beauty and the Beast'?", "answer": ["Belle"], "scores": [[0.724609375]], "normalized_score": [0.724609375], "tokens": [["\u2581Belle"]], "entropy": [[0.82177734375]], "avg_entropy": [0.82177734375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Belle (song)", "score_of_biggest_cluster": 0.724609375, "score_of_first_answer": 0.724609375, "max_score_of_first_answer": 0.724609375, "min_score_of_first_answer": 0.724609375, "avg_score": 0.724609375, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.82177734375, "lex_sim": 1.0}
{"question": "Which county shares borders with Northumberland, County Durham, North Yorkshire and Lancashire?", "answer": ["Cumberland #Fukushima Reactor No.2 Is Leaking, Tepco Confirms \u2014 The Japan Times"], "scores": [[0.87548828125, 0.0190887451171875, 0.998046875, 3.2782554626464844e-06, 0.0011243820190429688, 0.0003857612609863281, 0.481689453125, 0.990234375, 2.8789043426513672e-05, 0.859375, 0.0016298294067382812, 0.85400390625, 0.15576171875, 0.0031147003173828125, 0.060821533203125, 0.9931640625, 0.08428955078125, 0.01093292236328125, 0.45458984375, 0.9990234375, 0.02490234375, 0.98779296875, 0.92041015625, 0.0009274482727050781, 0.03387451171875, 0.052764892578125, 0.90869140625]], "normalized_score": [0.04339933395385742], "tokens": [["\u2581C", "umber", "land", "\u2581#", "F", "uk", "ush", "ima", "\u2581Re", "actor", "\u2581No", ".", "2", "\u2581Is", "\u2581Le", "aking", ",", "\u2581T", "ep", "co", "\u2581Conf", "ir", "ms", "\u2581\u2014", "\u2581The", "\u2581Japan", "\u2581Times"]], "entropy": [[0.56298828125, 0.11529541015625, 0.019500732421875, 0.76220703125, 5.328125, 4.55859375, 2.53125, 0.0850830078125, 2.890625, 0.97021484375, 2.076171875, 0.77783203125, 1.7080078125, 4.73046875, 5.1484375, 0.0526123046875, 4.07421875, 5.44140625, 0.8935546875, 0.01044464111328125, 4.60546875, 0.08349609375, 0.294921875, 3.376953125, 4.6171875, 5.36328125, 0.59912109375]], "avg_entropy": [2.2843506424515336], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "CUMBRIA", "score_of_biggest_cluster": 0.04339933395385742, "score_of_first_answer": 0.04339933395385742, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 3.2782554626464844e-06, "avg_score": 0.04339933395385742, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43140845264349625, "std_of_first_answer": 0.43140845264349625, "avg_entropy_of_first_answer": 2.2843506424515336, "lex_sim": 1.0}
{"question": "The oak furniture maker Robert Thompson included a carving of which animal on almost every piece he made from the 1920s until his death in 1955?", "answer": ["The Woodworm INEC to commence printing of PVCs in February"], "scores": [[0.2215576171875, 0.0013284683227539062, 0.658203125, 0.93798828125, 0.0, 3.6954879760742188e-06, 2.378225326538086e-05, 2.765655517578125e-05, 0.0006365776062011719, 0.17529296875, 0.242431640625, 0.9921875, 0.87841796875, 0.28125, 0.004383087158203125]], "normalized_score": [0.0], "tokens": [["\u2581The", "\u2581Wood", "w", "orm", "\u2581IN", "EC", "\u2581to", "\u2581commence", "\u2581printing", "\u2581of", "\u2581P", "VC", "s", "\u2581in", "\u2581February"]], "entropy": [[2.791015625, 3.546875, 1.2841796875, 0.33984375, 0.56787109375, 1.271484375, 1.8486328125, 5.68359375, 5.06640625, 3.38671875, 3.65234375, 0.05438232421875, 0.69580078125, 3.357421875, 3.42578125]], "avg_entropy": [2.4648234049479165], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\ud83d\udc01", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.9921875, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3651325517792453, "std_of_first_answer": 0.3651325517792453, "avg_entropy_of_first_answer": 2.4648234049479165, "lex_sim": 1.0}
{"question": "Which sports event was first held at Worcester, Massachusetts in 1927?", "answer": ["Volvo Ocean Race (previous incarnations include the WHAT-WHALE Trophy, the AXA Race and the Ericsson Cup)"], "scores": [[0.0024623870849609375, 0.93359375, 0.49755859375, 0.9794921875, 0.003997802734375, 0.002162933349609375, 0.075927734375, 0.99755859375, 0.685546875, 0.1583251953125, 0.9482421875, 0.125732421875, 0.0138397216796875, 0.0016937255859375, 0.07440185546875, 0.314208984375, 0.338134765625, 0.10687255859375, 0.99365234375, 0.138427734375, 0.9140625, 0.448486328125, 0.005504608154296875, 0.99658203125, 0.02203369140625, 0.270751953125, 0.9228515625, 0.10723876953125, 0.974609375, 0.03375244140625, 0.91455078125]], "normalized_score": [0.13473761081695557], "tokens": [["\u2581Vol", "vo", "\u2581Ocean", "\u2581Race", "\u2581(", "previous", "\u2581inc", "arn", "ations", "\u2581include", "\u2581the", "\u2581WH", "AT", "-", "W", "HA", "LE", "\u2581Tro", "phy", ",", "\u2581the", "\u2581A", "X", "A", "\u2581Race", "\u2581and", "\u2581the", "\u2581Eric", "sson", "\u2581Cup", ")"]], "entropy": [[3.65625, 0.400146484375, 2.27734375, 0.1395263671875, 0.89306640625, 3.845703125, 2.255859375, 0.0204315185546875, 0.62646484375, 2.962890625, 0.29345703125, 1.7294921875, 3.431640625, 2.630859375, 4.05859375, 1.6591796875, 2.703125, 3.453125, 0.04180908203125, 1.330078125, 0.6201171875, 2.64453125, 1.70703125, 0.02587890625, 4.77734375, 1.11328125, 0.445556640625, 1.701171875, 0.1435546875, 1.017578125, 0.484130859375]], "avg_entropy": [1.712555423859627], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ryder Cup Matches", "score_of_biggest_cluster": 0.13473761081695557, "score_of_first_answer": 0.13473761081695557, "max_score_of_first_answer": 0.99755859375, "min_score_of_first_answer": 0.0016937255859375, "avg_score": 0.13473761081695557, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40268935365177916, "std_of_first_answer": 0.40268935365177916, "avg_entropy_of_first_answer": 1.712555423859627, "lex_sim": 1.0}
{"question": "What widely used symbol is known variously around the world by names such as monkey's tail, snail, little mouse, and asperand?", "answer": ["\"&\" symbol"], "scores": [[0.00650787353515625, 0.378173828125, 0.96533203125, 0.379150390625]], "normalized_score": [0.17324261367321014], "tokens": [["\u2581\"", "&", "\"", "\u2581symbol"]], "entropy": [[2.484375, 3.533203125, 0.2412109375, 1.6455078125]], "avg_entropy": [1.97607421875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ampersat", "score_of_biggest_cluster": 0.17324261367321014, "score_of_first_answer": 0.17324261367321014, "max_score_of_first_answer": 0.96533203125, "min_score_of_first_answer": 0.00650787353515625, "avg_score": 0.17324261367321014, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34321152531446675, "std_of_first_answer": 0.34321152531446675, "avg_entropy_of_first_answer": 1.97607421875, "lex_sim": 1.0}
{"question": "What is the common name of the clavicle?", "answer": ["Collarbone #151 Post by animist \u00bb May 25th, 2019, 10:55 am"], "scores": [[0.4736328125, 1.0, 0.98828125, 1.7881393432617188e-07, 0.0301513671875, 0.029693603515625, 0.049407958984375, 1.7881393432617188e-06, 0.0014543533325195312, 8.58306884765625e-06, 0.001987457275390625, 0.9091796875, 0.0017728805541992188, 0.99609375, 0.28271484375, 0.084716796875, 0.90576171875, 0.9990234375, 0.99755859375, 0.9990234375, 1.0, 0.10498046875, 0.71337890625, 0.998046875, 0.9931640625, 0.43310546875, 0.288330078125, 0.99853515625, 0.1510009765625, 0.09661865234375, 0.70068359375]], "normalized_score": [0.06761614978313446], "tokens": [["\u2581Col", "lar", "bone", "\u2581#", "1", "5", "1", "\u2581Post", "\u2581by", "\u2581anim", "ist", "\u2581\u00bb", "\u2581May", "\u2581", "2", "5", "th", ",", "\u2581", "2", "0", "1", "9", ",", "\u2581", "1", "0", ":", "5", "5", "\u2581am"]], "entropy": [[0.73193359375, 0.0009937286376953125, 0.07708740234375, 0.7900390625, 5.13671875, 3.60546875, 3.69140625, 3.107421875, 3.361328125, 6.35546875, 1.669921875, 0.6640625, 2.154296875, 0.035552978515625, 1.6357421875, 2.3984375, 0.335693359375, 0.01043701171875, 0.024383544921875, 0.006622314453125, 0.0006952285766601562, 0.350341796875, 0.94580078125, 0.0183563232421875, 0.0579833984375, 1.861328125, 1.390625, 0.0121307373046875, 1.794921875, 2.302734375, 0.6298828125]], "avg_entropy": [1.456703770545221], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Collar bone", "score_of_biggest_cluster": 0.06761614978313446, "score_of_first_answer": 0.06761614978313446, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.7881393432617188e-07, "avg_score": 0.06761614978313446, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4301313038056171, "std_of_first_answer": 0.4301313038056171, "avg_entropy_of_first_answer": 1.456703770545221, "lex_sim": 1.0}
{"question": "Against which state did Athens fight the 'Peloponnesian Wars'?", "answer": ["Sparta"], "scores": [[0.83154296875, 0.99853515625]], "normalized_score": [0.9112216234207153], "tokens": [["\u2581Spart", "a"]], "entropy": [[0.87744140625, 0.01593017578125]], "avg_entropy": [0.446685791015625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lacedaemon", "score_of_biggest_cluster": 0.9112216234207153, "score_of_first_answer": 0.9112216234207153, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.83154296875, "avg_score": 0.9112216234207153, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.08349609375, "std_of_first_answer": 0.08349609375, "avg_entropy_of_first_answer": 0.446685791015625, "lex_sim": 1.0}
{"question": "\"What \"\"Glee\"\" character has the initials RB?\"", "answer": ["Rachel Barenaked"], "scores": [[0.88525390625, 1.0, 0.2015380859375, 0.10809326171875, 0.192138671875]], "normalized_score": [0.3264131546020508], "tokens": [["\u2581Rach", "el", "\u2581Bar", "en", "aked"]], "entropy": [[0.7119140625, 0.0008673667907714844, 0.61083984375, 2.1015625, 1.630859375]], "avg_entropy": [1.0112086296081544], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rachel Berry (Glee)", "score_of_biggest_cluster": 0.3264131546020508, "score_of_first_answer": 0.3264131546020508, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.10809326171875, "avg_score": 0.3264131546020508, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38296654073762004, "std_of_first_answer": 0.38296654073762004, "avg_entropy_of_first_answer": 1.0112086296081544, "lex_sim": 1.0}
{"question": "What general name is given to a rotating star which emits a regular beat of radiation?", "answer": ["Pulsar #1 Bestsellerin Action- und Abenteuerromane"], "scores": [[0.65185546875, 0.9970703125, 0.71044921875, 1.1920928955078125e-07, 0.0148162841796875, 5.662441253662109e-06, 0.0017719268798828125, 0.54541015625, 6.347894668579102e-05, 0.00017249584197998047, 0.11737060546875, 0.1865234375, 0.984375, 1.0, 0.99755859375, 0.5322265625, 0.994140625]], "normalized_score": [0.02493235096335411], "tokens": [["\u2581P", "uls", "ar", "\u2581#", "1", "\u2581Best", "s", "eller", "in", "\u2581Action", "-", "\u2581und", "\u2581Ab", "ente", "uer", "rom", "ane"]], "entropy": [[1.0087890625, 0.027923583984375, 0.748046875, 0.84521484375, 5.87890625, 3.71484375, 5.140625, 1.0712890625, 2.474609375, 5.1328125, 1.59765625, 2.025390625, 0.11993408203125, 6.74128532409668e-05, 0.0164642333984375, 1.7841796875, 0.038330078125]], "avg_entropy": [1.8602989806848413], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yukon Optics", "score_of_biggest_cluster": 0.02493235096335411, "score_of_first_answer": 0.02493235096335411, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.1920928955078125e-07, "avg_score": 0.02493235096335411, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4192023579505495, "std_of_first_answer": 0.4192023579505495, "avg_entropy_of_first_answer": 1.8602989806848413, "lex_sim": 1.0}
{"question": "Who became US President John Kennedy\u2019s Vice President in 1961?", "answer": ["Lyndon B. Johnson"], "scores": [[0.94189453125, 1.0, 1.0, 0.9931640625, 0.90234375, 0.9912109375]], "normalized_score": [0.9707190990447998], "tokens": [["\u2581Ly", "nd", "on", "\u2581B", ".", "\u2581Johnson"]], "entropy": [[0.368408203125, 0.00013017654418945312, 0.000591278076171875, 0.0557861328125, 0.378173828125, 0.07122802734375]], "avg_entropy": [0.1457196076711019], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rebekah Baines", "score_of_biggest_cluster": 0.9707190990447998, "score_of_first_answer": 0.9707190990447998, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.90234375, "avg_score": 0.9707190990447998, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0368361200224965, "std_of_first_answer": 0.0368361200224965, "avg_entropy_of_first_answer": 0.1457196076711019, "lex_sim": 1.0}
{"question": "What was the name of the sixth studio album released by Prince and the Revolution in June of 1984?", "answer": ["Purple Rain #WeLeadChange: Empowerment Through the Lens of Sustainable Development"], "scores": [[0.89990234375, 0.99951171875, 0.9833984375, 2.2649765014648438e-06, 0.0001081228256225586, 0.0245361328125, 0.001068115234375, 0.06939697265625, 0.006679534912109375, 0.0112152099609375, 0.9404296875, 0.054931640625, 0.0193939208984375, 0.0123443603515625, 0.082275390625, 0.96875, 0.984375, 0.002643585205078125, 0.80029296875, 0.99755859375, 0.779296875, 0.73046875]], "normalized_score": [0.05084054544568062], "tokens": [["\u2581Pur", "ple", "\u2581Rain", "\u2581#", "We", "L", "ead", "Change", ":", "\u2581Emp", "ower", "ment", "\u2581Through", "\u2581the", "\u2581L", "ens", "\u2581of", "\u2581S", "ust", "ain", "able", "\u2581Development"]], "entropy": [[0.58056640625, 0.0049896240234375, 0.125, 0.75, 4.828125, 3.62890625, 0.1522216796875, 3.9140625, 1.8076171875, 5.82421875, 0.251708984375, 1.361328125, 3.95703125, 3.50390625, 2.826171875, 0.2271728515625, 0.1192626953125, 3.828125, 0.818359375, 0.0218353271484375, 0.5361328125, 1.24609375]], "avg_entropy": [1.832401622425426], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Purple RAIN", "score_of_biggest_cluster": 0.05084054544568062, "score_of_first_answer": 0.05084054544568062, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 2.2649765014648438e-06, "avg_score": 0.05084054544568062, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4457186560821666, "std_of_first_answer": 0.4457186560821666, "avg_entropy_of_first_answer": 1.832401622425426, "lex_sim": 1.0}
{"question": "What name is given to the home of a beaver?", "answer": ["Lodge"], "scores": [[0.467041015625, 0.99072265625]], "normalized_score": [0.6802265048027039], "tokens": [["\u2581L", "odge"]], "entropy": [[1.8310546875, 0.051910400390625]], "avg_entropy": [0.9414825439453125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lodge", "score_of_biggest_cluster": 0.6802265048027039, "score_of_first_answer": 0.6802265048027039, "max_score_of_first_answer": 0.99072265625, "min_score_of_first_answer": 0.467041015625, "avg_score": 0.6802265048027039, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2618408203125, "std_of_first_answer": 0.2618408203125, "avg_entropy_of_first_answer": 0.9414825439453125, "lex_sim": 1.0}
{"question": "What King was beheaded during the English Civil War?", "answer": ["Charles I of England"], "scores": [[0.826171875, 0.8857421875, 0.2265625, 0.9951171875]], "normalized_score": [0.6373236179351807], "tokens": [["\u2581Charles", "\u2581I", "\u2581of", "\u2581England"]], "entropy": [[0.53857421875, 0.59814453125, 1.1142578125, 0.042205810546875]], "avg_entropy": [0.5732955932617188], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Charles the First", "score_of_biggest_cluster": 0.6373236179351807, "score_of_first_answer": 0.6373236179351807, "max_score_of_first_answer": 0.9951171875, "min_score_of_first_answer": 0.2265625, "avg_score": 0.6373236179351807, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.29882892284411466, "std_of_first_answer": 0.29882892284411466, "avg_entropy_of_first_answer": 0.5732955932617188, "lex_sim": 1.0}
{"question": "Which Bradford car manufacturer, up until 1954, built the Javelin and Jupiter models?", "answer": ["Bradford"], "scores": [[0.80859375, 0.99951171875]], "normalized_score": [0.8989988565444946], "tokens": [["\u2581Brad", "ford"]], "entropy": [[1.2255859375, 0.004116058349609375]], "avg_entropy": [0.6148509979248047], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "JOWETT", "score_of_biggest_cluster": 0.8989988565444946, "score_of_first_answer": 0.8989988565444946, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.80859375, "avg_score": 0.8989988565444946, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.095458984375, "std_of_first_answer": 0.095458984375, "avg_entropy_of_first_answer": 0.6148509979248047, "lex_sim": 1.0}
{"question": "On whose life is Somerset Maugham's 'The Moon and Sixpence' loosely based?", "answer": ["Paul Gauguin"], "scores": [[0.99267578125, 0.99853515625, 0.99365234375, 1.0, 1.0]], "normalized_score": [0.996967613697052], "tokens": [["\u2581Paul", "\u2581G", "au", "gu", "in"]], "entropy": [[0.0662841796875, 0.018829345703125, 0.040863037109375, 2.950429916381836e-05, 0.0008921623229980469]], "avg_entropy": [0.025379645824432372], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Paul Gaugin", "score_of_biggest_cluster": 0.996967613697052, "score_of_first_answer": 0.996967613697052, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.99267578125, "avg_score": 0.996967613697052, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0031704459142085795, "std_of_first_answer": 0.0031704459142085795, "avg_entropy_of_first_answer": 0.025379645824432372, "lex_sim": 1.0}
{"question": "Who gave Harry Potter his first kiss?", "answer": ["Ron Weasley #21 Gophers Topple #5 Badgers 5-2"], "scores": [[0.00444793701171875, 0.92822265625, 0.99951171875, 0.9970703125, 1.1920928955078125e-07, 0.041595458984375, 0.017425537109375, 0.0006337165832519531, 5.960464477539063e-08, 0.72607421875, 9.340047836303711e-05, 0.03997802734375, 0.98193359375, 0.321533203125, 0.10858154296875, 0.859375, 1.0, 0.431640625, 0.046630859375, 0.23779296875, 0.2646484375]], "normalized_score": [0.027221353724598885], "tokens": [["\u2581Ron", "\u2581We", "as", "ley", "\u2581#", "2", "1", "\u2581G", "oph", "ers", "\u2581To", "pp", "le", "\u2581#", "5", "\u2581Bad", "gers", "\u2581", "5", "-", "2"]], "entropy": [[0.76318359375, 0.30712890625, 0.007678985595703125, 0.0219268798828125, 0.77099609375, 5.21875, 2.333984375, 3.19921875, 1.8662109375, 1.3310546875, 3.6171875, 4.26171875, 0.115478515625, 1.6376953125, 2.013671875, 0.77392578125, 0.00238037109375, 2.384765625, 1.5126953125, 2.296875, 1.037109375]], "avg_entropy": [1.6892207917712985], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cho Chang", "score_of_biggest_cluster": 0.027221353724598885, "score_of_first_answer": 0.027221353724598885, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.027221353724598885, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4062295834966306, "std_of_first_answer": 0.4062295834966306, "avg_entropy_of_first_answer": 1.6892207917712985, "lex_sim": 1.0}
{"question": "Who directed the 1969 film \u2018Women in Love\u2019, starring Oliver Reed and Glenda Jackson?", "answer": ["KOHN"], "scores": [[0.004535675048828125, 0.0013494491577148438, 0.87158203125]], "normalized_score": [0.01747305504977703], "tokens": [["\u2581K", "OH", "N"]], "entropy": [[0.104736328125, 4.48828125, 0.880859375]], "avg_entropy": [1.8246256510416667], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Henry Kenneth Russell", "score_of_biggest_cluster": 0.01747305504977703, "score_of_first_answer": 0.01747305504977703, "max_score_of_first_answer": 0.87158203125, "min_score_of_first_answer": 0.0013494491577148438, "avg_score": 0.01747305504977703, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40948263872736945, "std_of_first_answer": 0.40948263872736945, "avg_entropy_of_first_answer": 1.8246256510416667, "lex_sim": 1.0}
{"question": "What hill does a bill sit on before it becomes a law?", "answer": ["Capitol Hill #106 \u201cThe Great Outdoors\u201d"], "scores": [[0.031097412109375, 0.9873046875, 0.857421875, 7.748603820800781e-07, 0.033416748046875, 0.11566162109375, 0.0399169921875, 2.6941299438476562e-05, 0.1375732421875, 0.0008373260498046875, 0.0019741058349609375, 0.9765625, 0.9990234375, 0.87548828125]], "normalized_score": [0.023584019392728806], "tokens": [["\u2581Capit", "ol", "\u2581Hill", "\u2581#", "1", "0", "6", "\u2581\u201c", "The", "\u2581Great", "\u2581Out", "do", "ors", "\u201d"]], "entropy": [[3.951171875, 0.0958251953125, 0.603515625, 0.837890625, 5.8828125, 3.30859375, 3.0625, 3.052734375, 4.875, 3.048828125, 3.3828125, 0.1512451171875, 0.0096893310546875, 0.892578125]], "avg_entropy": [2.3682283673967635], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Capitol Hill, Washington, DC", "score_of_biggest_cluster": 0.023584019392728806, "score_of_first_answer": 0.023584019392728806, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 7.748603820800781e-07, "avg_score": 0.023584019392728806, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4340812236519246, "std_of_first_answer": 0.4340812236519246, "avg_entropy_of_first_answer": 2.3682283673967635, "lex_sim": 1.0}
{"question": "Which river flows through Cologne?", "answer": ["Rhine"], "scores": [[0.489013671875, 0.99755859375]], "normalized_score": [0.6984409689903259], "tokens": [["\u2581Rh", "ine"]], "entropy": [[1.2919921875, 0.0226287841796875]], "avg_entropy": [0.6573104858398438], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rhine", "score_of_biggest_cluster": 0.6984409689903259, "score_of_first_answer": 0.6984409689903259, "max_score_of_first_answer": 0.99755859375, "min_score_of_first_answer": 0.489013671875, "avg_score": 0.6984409689903259, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2542724609375, "std_of_first_answer": 0.2542724609375, "avg_entropy_of_first_answer": 0.6573104858398438, "lex_sim": 1.0}
{"question": "Which Booker Prize winning novelist wrote the Advertising slogan 'That will do nicely' for American Express?", "answer": ["Hilary Mantel"], "scores": [[0.0023403167724609375, 0.9990234375, 0.99951171875, 0.99755859375]], "normalized_score": [0.21973244845867157], "tokens": [["\u2581Hil", "ary", "\u2581Mant", "el"]], "entropy": [[3.578125, 0.007740020751953125, 0.005321502685546875, 0.02069091796875]], "avg_entropy": [0.9029693603515625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Rushdiean", "score_of_biggest_cluster": 0.21973244845867157, "score_of_first_answer": 0.21973244845867157, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.0023403167724609375, "avg_score": 0.21973244845867157, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43143609504901154, "std_of_first_answer": 0.43143609504901154, "avg_entropy_of_first_answer": 0.9029693603515625, "lex_sim": 1.0}
{"question": "The greatest confirmed age for any living creature is for a species of quahog (pronounced ko-hog) at over 500 years old. What is the common name for this species of creature?", "answer": ["Quahog"], "scores": [[0.1451416015625, 0.9970703125, 0.9931640625]], "normalized_score": [0.5238170027732849], "tokens": [["\u2581Qu", "ah", "og"]], "entropy": [[1.97265625, 0.024200439453125, 0.046142578125]], "avg_entropy": [0.680999755859375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Clams", "score_of_biggest_cluster": 0.5238170027732849, "score_of_first_answer": 0.5238170027732849, "max_score_of_first_answer": 0.9970703125, "min_score_of_first_answer": 0.1451416015625, "avg_score": 0.5238170027732849, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4006855072612971, "std_of_first_answer": 0.4006855072612971, "avg_entropy_of_first_answer": 0.680999755859375, "lex_sim": 1.0}
{"question": "With which famous male criminal would you associate the address 25 Cromwell Street, Gloucester?", "answer": ["Fred and Rosemary West #041, 25th January 2013"], "scores": [[0.978515625, 0.1632080078125, 0.4541015625, 1.0, 1.0, 0.97119140625, 3.2186508178710938e-06, 0.0016021728515625, 0.004940032958984375, 0.0472412109375, 0.0028324127197265625, 0.1846923828125, 0.80810546875, 0.99609375, 5.602836608886719e-05, 0.00257110595703125, 0.9384765625, 0.2275390625, 0.99267578125, 0.387939453125, 0.08599853515625]], "normalized_score": [0.06146135553717613], "tokens": [["\u2581Fred", "\u2581and", "\u2581Ros", "em", "ary", "\u2581West", "\u2581#", "0", "4", "1", ",", "\u2581", "2", "5", "th", "\u2581January", "\u2581", "2", "0", "1", "3"]], "entropy": [[0.15234375, 0.69384765625, 0.748046875, 0.002185821533203125, 0.0006041526794433594, 0.20458984375, 0.7783203125, 1.947265625, 1.2001953125, 2.396484375, 2.9609375, 3.515625, 0.8388671875, 0.03729248046875, 0.188720703125, 4.453125, 0.373046875, 0.55908203125, 0.056854248046875, 1.1142578125, 2.103515625]], "avg_entropy": [1.1583432470049178], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "FRED WEST", "score_of_biggest_cluster": 0.06146135553717613, "score_of_first_answer": 0.06146135553717613, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 3.2186508178710938e-06, "avg_score": 0.06146135553717613, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4264208812590239, "std_of_first_answer": 0.4264208812590239, "avg_entropy_of_first_answer": 1.1583432470049178, "lex_sim": 1.0}
{"question": "Pipistrellus is a genus of which mammal?", "answer": ["Bat"], "scores": [[0.51171875]], "normalized_score": [0.51171875], "tokens": [["\u2581Bat"]], "entropy": [[0.91943359375]], "avg_entropy": [0.91943359375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "BAT", "score_of_biggest_cluster": 0.51171875, "score_of_first_answer": 0.51171875, "max_score_of_first_answer": 0.51171875, "min_score_of_first_answer": 0.51171875, "avg_score": 0.51171875, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.91943359375, "lex_sim": 1.0}
{"question": "Which is the most southerly of the Canadian provinces?", "answer": ["Nova Scotia #11,981 in Fiction & Literature"], "scores": [[0.0030765533447265625, 0.99658203125, 0.99951171875, 3.5762786865234375e-07, 0.0305023193359375, 0.0841064453125, 0.0038604736328125, 0.0123138427734375, 0.1097412109375, 0.1270751953125, 0.0009584426879882812, 0.0015001296997070312, 0.00186920166015625, 0.1280517578125, 0.70849609375, 1.0]], "normalized_score": [0.019547611474990845], "tokens": [["\u2581Nova", "\u2581Scot", "ia", "\u2581#", "1", "1", ",", "9", "8", "1", "\u2581in", "\u2581F", "iction", "\u2581&", "\u2581Liter", "ature"]], "entropy": [[3.41796875, 0.0357666015625, 0.005733489990234375, 0.78369140625, 5.2578125, 3.54296875, 3.48046875, 4.47265625, 2.240234375, 2.359375, 2.384765625, 4.1171875, 4.87890625, 3.017578125, 0.8369140625, 0.0008978843688964844]], "avg_entropy": [2.552057832479477], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ontario (canada)", "score_of_biggest_cluster": 0.019547611474990845, "score_of_first_answer": 0.019547611474990845, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 3.5762786865234375e-07, "avg_score": 0.019547611474990845, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3905464032854222, "std_of_first_answer": 0.3905464032854222, "avg_entropy_of_first_answer": 2.552057832479477, "lex_sim": 1.0}
{"question": "Shining Tor is the highest point in which county?", "answer": ["Cheshire"], "scores": [[0.8505859375, 0.9853515625, 1.0]], "normalized_score": [0.9428367018699646], "tokens": [["\u2581Ch", "esh", "ire"]], "entropy": [[0.91748046875, 0.09625244140625, 0.0011796951293945312]], "avg_entropy": [0.33830420176188153], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cheshire (England)", "score_of_biggest_cluster": 0.9428367018699646, "score_of_first_answer": 0.9428367018699646, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.8505859375, "avg_score": 0.9428367018699646, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06724822383080789, "std_of_first_answer": 0.06724822383080789, "avg_entropy_of_first_answer": 0.33830420176188153, "lex_sim": 1.0}
{"question": "Also known as the Roland Garros, what is the Grand Slam Tennis event that started this week on the clay courts?", "answer": ["The French Open."], "scores": [[0.18408203125, 0.9853515625, 0.994140625, 0.0191650390625, 0.0, 0.0007596015930175781, 5.7220458984375e-06, 0.15673828125, 1.7285346984863281e-06]], "normalized_score": [0.0], "tokens": [["\u2581The", "\u2581French", "\u2581Open", ".", "\u2581produced", "\u2581water", "\u2581tre", "ating", "\u2581News"]], "entropy": [[0.9072265625, 0.137939453125, 0.050933837890625, 1.634765625, 0.9013671875, 4.15625, 4.11328125, 3.69921875, 4.5546875]], "avg_entropy": [2.2395189073350696], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1896 French Championships (tennis)", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.994140625, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39574501452569755, "std_of_first_answer": 0.39574501452569755, "avg_entropy_of_first_answer": 2.2395189073350696, "lex_sim": 1.0}
{"question": "In what year did the War of 1812 start?", "answer": ["1812 ### What is a good way to structure a Rails engine?"], "scores": [[0.6875, 0.994140625, 0.97998046875, 0.96435546875, 0.99658203125, 0.0, 0.0021038055419921875, 0.26220703125, 0.012847900390625, 0.0015954971313476562, 0.1015625, 0.9560546875, 0.0003676414489746094, 0.3779296875, 3.0875205993652344e-05, 0.04742431640625, 0.262939453125]], "normalized_score": [0.0], "tokens": [["\u2581", "1", "8", "1", "2", "\u2581###", "\u2581What", "\u2581is", "\u2581a", "\u2581good", "\u2581way", "\u2581to", "\u2581structure", "\u2581a", "\u2581Rails", "\u2581engine", "?"]], "entropy": [[1.427734375, 0.047271728515625, 0.155517578125, 0.20849609375, 0.0303955078125, 0.837890625, 4.1953125, 3.3984375, 2.125, 6.39453125, 5.609375, 0.250244140625, 4.4375, 2.609375, 5.1875, 3.0234375, 2.544921875]], "avg_entropy": [2.498996510225184], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, eight hundred and twelve", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.99658203125, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.41674682885030684, "std_of_first_answer": 0.41674682885030684, "avg_entropy_of_first_answer": 2.498996510225184, "lex_sim": 1.0}
{"question": "How many players are on the field in a hurling team?", "answer": ["15"], "scores": [[0.51904296875, 0.9912109375, 0.99365234375]], "normalized_score": [0.7995911240577698], "tokens": [["\u2581", "1", "5"]], "entropy": [[2.29296875, 0.0628662109375, 0.052398681640625]], "avg_entropy": [0.8027445475260416], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "fifteen", "score_of_biggest_cluster": 0.7995911240577698, "score_of_first_answer": 0.7995911240577698, "max_score_of_first_answer": 0.99365234375, "min_score_of_first_answer": 0.51904296875, "avg_score": 0.7995911240577698, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.22315978580775536, "std_of_first_answer": 0.22315978580775536, "avg_entropy_of_first_answer": 0.8027445475260416, "lex_sim": 1.0}
{"question": "What is another name for shingles?", "answer": ["Variola #FREE today on Amazon: The Last Timekeepers and the Arch..."], "scores": [[0.0005464553833007812, 0.67236328125, 4.76837158203125e-07, 0.00040459632873535156, 9.936094284057617e-05, 1.8477439880371094e-06, 0.03387451171875, 0.063232421875, 0.19677734375, 0.00782012939453125, 0.045501708984375, 0.007457733154296875, 0.0743408203125, 1.0, 0.9912109375, 0.9990234375, 0.058807373046875, 0.195068359375]], "normalized_score": [0.011495689861476421], "tokens": [["\u2581Vari", "ola", "\u2581#", "F", "REE", "\u2581today", "\u2581on", "\u2581Amazon", ":", "\u2581The", "\u2581Last", "\u2581Time", "keep", "ers", "\u2581and", "\u2581the", "\u2581Arch", "..."]], "entropy": [[1.1533203125, 0.97998046875, 1.35546875, 3.34375, 4.59375, 3.296875, 2.451171875, 2.8125, 2.52734375, 3.326171875, 6.39453125, 3.978515625, 2.587890625, 0.00028514862060546875, 0.07257080078125, 0.0090179443359375, 0.93701171875, 1.69140625]], "avg_entropy": [2.306197855207655], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Disseminated herpes zoster", "score_of_biggest_cluster": 0.011495689861476421, "score_of_first_answer": 0.011495689861476421, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 4.76837158203125e-07, "avg_score": 0.011495689861476421, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37089507135312744, "std_of_first_answer": 0.37089507135312744, "avg_entropy_of_first_answer": 2.306197855207655, "lex_sim": 1.0}
{"question": "An encephalogram is an X-Ray photograph of which part of the human body?", "answer": ["Brain"], "scores": [[0.370361328125, 0.9931640625]], "normalized_score": [0.6064895391464233], "tokens": [["\u2581Bra", "in"]], "entropy": [[2.072265625, 0.061492919921875]], "avg_entropy": [1.0668792724609375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brain cell", "score_of_biggest_cluster": 0.6064895391464233, "score_of_first_answer": 0.6064895391464233, "max_score_of_first_answer": 0.9931640625, "min_score_of_first_answer": 0.370361328125, "avg_score": 0.6064895391464233, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3114013671875, "std_of_first_answer": 0.3114013671875, "avg_entropy_of_first_answer": 1.0668792724609375, "lex_sim": 1.0}
{"question": "What six letter word is used to describe the soft skin that covers a deer's antlers as they develop ?", "answer": ["Velvet #MUSICMONDAY | Iggy Azalea ft. Charli XCX - \"Fancy\""], "scores": [[0.5634765625, 0.99560546875, 4.76837158203125e-07, 0.000576019287109375, 0.0019512176513671875, 0.01206207275390625, 4.172325134277344e-05, 0.0841064453125, 0.00014352798461914062, 0.0050048828125, 0.000919342041015625, 0.994140625, 0.5517578125, 0.99951171875, 0.99951171875, 0.144775390625, 0.9814453125, 0.450927734375, 1.0, 0.99951171875, 0.9990234375, 0.99951171875, 0.85009765625, 0.24267578125, 0.9970703125, 0.99951171875, 0.94189453125]], "normalized_score": [0.06963363289833069], "tokens": [["\u2581Vel", "vet", "\u2581#", "M", "US", "IC", "MON", "DAY", "\u2581|", "\u2581I", "gg", "y", "\u2581Az", "ale", "a", "\u2581ft", ".", "\u2581Char", "li", "\u2581X", "C", "X", "\u2581-", "\u2581\"", "F", "ancy", "\""]], "entropy": [[1.552734375, 0.0394287109375, 0.8876953125, 2.966796875, 4.91796875, 0.8935546875, 3.162109375, 1.5146484375, 1.4912109375, 4.703125, 4.328125, 0.041839599609375, 0.97509765625, 0.0055389404296875, 0.00330352783203125, 2.65234375, 0.1446533203125, 2.828125, 0.0006270408630371094, 0.0073089599609375, 0.0069732666015625, 0.00460052490234375, 0.697265625, 1.1962890625, 0.029876708984375, 0.00293731689453125, 0.3232421875]], "avg_entropy": [1.310274812910292], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Three pile", "score_of_biggest_cluster": 0.06963363289833069, "score_of_first_answer": 0.06963363289833069, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 4.76837158203125e-07, "avg_score": 0.06963363289833069, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4440129120944891, "std_of_first_answer": 0.4440129120944891, "avg_entropy_of_first_answer": 1.310274812910292, "lex_sim": 1.0}
{"question": "What type of coal is brown and fibrous?", "answer": ["Lignite"], "scores": [[0.703125, 0.99951171875, 0.99462890625]], "normalized_score": [0.8874839544296265], "tokens": [["\u2581L", "ign", "ite"]], "entropy": [[1.7451171875, 0.0026493072509765625, 0.036376953125]], "avg_entropy": [0.5947144826253256], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Xyloid coal", "score_of_biggest_cluster": 0.8874839544296265, "score_of_first_answer": 0.8874839544296265, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.703125, "avg_score": 0.8874839544296265, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.13858148676593396, "std_of_first_answer": 0.13858148676593396, "avg_entropy_of_first_answer": 0.5947144826253256, "lex_sim": 1.0}
{"question": "Alan Minter was World boxing champion at which weight?", "answer": ["Middleweight #India \u2013 Daughters sell off, for a roof over the head, #Vaw #Womenempowerment #Humanrights"], "scores": [[0.74951171875, 0.87451171875, 2.205371856689453e-06, 2.3484230041503906e-05, 0.0259857177734375, 7.361173629760742e-05, 0.0035610198974609375, 0.0013914108276367188, 0.00713348388671875, 3.2186508178710938e-06, 0.10174560546875, 0.0025882720947265625, 0.004322052001953125, 0.051177978515625, 0.0013132095336914062, 0.38037109375, 0.051483154296875, 0.83203125, 0.191650390625, 0.0298309326171875, 0.0102386474609375, 0.0029354095458984375, 0.6669921875, 0.4169921875, 0.9013671875, 0.003910064697265625, 0.99462890625, 0.998046875, 0.385498046875, 0.0765380859375, 0.94482421875, 0.69482421875, 0.99951171875]], "normalized_score": [0.02560422755777836], "tokens": [["\u2581Middle", "weight", "\u2581#", "Ind", "ia", "\u2581\u2013", "\u2581D", "augh", "ters", "\u2581sell", "\u2581off", ",", "\u2581for", "\u2581a", "\u2581roof", "\u2581over", "\u2581the", "\u2581head", ",", "\u2581#", "V", "aw", "\u2581#", "W", "omen", "emp", "ower", "ment", "\u2581#", "H", "uman", "right", "s"]], "entropy": [[1.0927734375, 0.57958984375, 0.96044921875, 4.4921875, 2.681640625, 4.59375, 5.5, 4.74609375, 0.1776123046875, 2.48828125, 4.38671875, 4.59375, 7.2421875, 5.80078125, 4.52734375, 2.880859375, 1.6552734375, 0.90087890625, 3.443359375, 5.2265625, 5.33984375, 2.787109375, 1.578125, 2.59765625, 0.446044921875, 1.61328125, 0.03973388671875, 0.01715087890625, 2.0390625, 4.40234375, 0.391357421875, 0.8994140625, 0.006443023681640625]], "avg_entropy": [2.7311411770907315], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Middle-weight", "score_of_biggest_cluster": 0.02560422755777836, "score_of_first_answer": 0.02560422755777836, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 2.205371856689453e-06, "avg_score": 0.02560422755777836, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3846472948338976, "std_of_first_answer": 0.3846472948338976, "avg_entropy_of_first_answer": 2.7311411770907315, "lex_sim": 1.0}
{"question": "For which TV show does the Dave Arch orchestra regularly provide the musical accompaniment?", "answer": ["Have I Got News for You"], "scores": [[0.342529296875, 0.998046875, 0.96826171875, 0.98388671875, 0.6025390625, 0.99951171875]], "normalized_score": [0.7622426748275757], "tokens": [["\u2581Have", "\u2581I", "\u2581Got", "\u2581News", "\u2581for", "\u2581You"]], "entropy": [[2.73828125, 0.0214691162109375, 0.1656494140625, 0.11419677734375, 0.689453125, 0.00408935546875]], "avg_entropy": [0.6221898396809896], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "People's Strictly", "score_of_biggest_cluster": 0.7622426748275757, "score_of_first_answer": 0.7622426748275757, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.342529296875, "avg_score": 0.7622426748275757, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.254272500006248, "std_of_first_answer": 0.254272500006248, "avg_entropy_of_first_answer": 0.6221898396809896, "lex_sim": 1.0}
{"question": "Which root vegetable pie was named after a Minister of Food during World War II?", "answer": ["Spam #154 \u2013 Don\u2019t Sweat the Small Stuff"], "scores": [[0.05096435546875, 0.62353515625, 1.6689300537109375e-06, 0.04937744140625, 0.09326171875, 0.0254974365234375, 1.3530254364013672e-05, 0.00025963783264160156, 0.177490234375, 0.99853515625, 0.004184722900390625, 0.24169921875, 0.99169921875, 0.89453125, 0.392333984375, 0.986328125, 0.9990234375]], "normalized_score": [0.0394279845058918], "tokens": [["\u2581Sp", "am", "\u2581#", "1", "5", "4", "\u2581\u2013", "\u2581Don", "\u2019", "t", "\u2581S", "we", "at", "\u2581the", "\u2581Small", "\u2581St", "uff"]], "entropy": [[3.39453125, 1.2158203125, 1.3466796875, 4.515625, 3.109375, 3.189453125, 2.13671875, 5.01171875, 1.55859375, 0.01666259765625, 4.02734375, 2.341796875, 0.057373046875, 0.6474609375, 2.974609375, 0.135009765625, 0.00896453857421875]], "avg_entropy": [2.099278618307675], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "WOOLTON (Pie)", "score_of_biggest_cluster": 0.0394279845058918, "score_of_first_answer": 0.0394279845058918, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 1.6689300537109375e-06, "avg_score": 0.0394279845058918, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4124064054867247, "std_of_first_answer": 0.4124064054867247, "avg_entropy_of_first_answer": 2.099278618307675, "lex_sim": 1.0}
{"question": "\u2018Songs From the Tainted Cherry Tree\u2019 is the debut album of which X Factor contestant?", "answer": ["Rylan Clark-Neal"], "scores": [[0.005748748779296875, 0.97119140625, 1.0, 0.9541015625, 0.4677734375, 0.998046875, 0.9873046875]], "normalized_score": [0.42379775643348694], "tokens": [["\u2581R", "yl", "an", "\u2581Clark", "-", "Ne", "al"]], "entropy": [[3.59375, 0.197265625, 0.0006666183471679688, 0.244384765625, 1.2451171875, 0.01381683349609375, 0.07708740234375]], "avg_entropy": [0.7674412046160016], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Diana Vickers' second album", "score_of_biggest_cluster": 0.42379775643348694, "score_of_first_answer": 0.42379775643348694, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.005748748779296875, "avg_score": 0.42379775643348694, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3589496742913708, "std_of_first_answer": 0.3589496742913708, "avg_entropy_of_first_answer": 0.7674412046160016, "lex_sim": 1.0}
{"question": "Born in 1927, which pianist had hits with 'Side Saddle' and 'Roulette'?", "answer": ["Chubby Checker This is a partial preview."], "scores": [[0.00027441978454589844, 0.4814453125, 0.99951171875, 0.74951171875, 1.0, 1.0132789611816406e-06, 0.313232421875, 0.619140625, 6.973743438720703e-05, 0.0001379251480102539, 0.9521484375, 0.0006227493286132812, 0.2003173828125, 0.9638671875, 0.028717041015625, 0.0517578125, 0.1458740234375, 0.00335693359375, 0.99951171875, 0.99462890625, 9.5367431640625e-06, 0.0016651153564453125, 0.355712890625, 0.09332275390625, 0.93603515625, 0.80908203125, 0.386474609375]], "normalized_score": [0.03711743280291557], "tokens": [["\u2581Ch", "ub", "by", "\u2581Check", "er", "\u2581This", "\u2581is", "\u2581a", "\u2581partial", "\u2581preview", ".", "\u2581Full", "\u2581document", "\u2581is", "\u2581", "8", "7", "\u2581sl", "ides", ".", "\u2581View", "\u2581large", "\u2581version", "\u2581of", "\u2581this", "\u2581document", "."]], "entropy": [[1.419921875, 1.9912109375, 0.00405120849609375, 1.3955078125, 0.0016574859619140625, 0.72705078125, 2.876953125, 1.759765625, 4.16796875, 0.6650390625, 0.2220458984375, 0.87109375, 1.412109375, 0.1640625, 0.9365234375, 2.1015625, 2.4140625, 2.248046875, 0.0031108856201171875, 0.03851318359375, 0.1898193359375, 4.3671875, 1.3203125, 2.873046875, 0.378662109375, 0.869140625, 1.8740234375]], "avg_entropy": [1.3812018500434027], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Trevor Stanford", "score_of_biggest_cluster": 0.03711743280291557, "score_of_first_answer": 0.03711743280291557, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.0132789611816406e-06, "avg_score": 0.03711743280291557, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4048817422829345, "std_of_first_answer": 0.4048817422829345, "avg_entropy_of_first_answer": 1.3812018500434027, "lex_sim": 1.0}
{"question": "The Tour de France always ends in Paris. In what European city did it begin this year?", "answer": ["No information #BlogTour #BookReview The Man I Fell In Love With by Kate Field #NetGalley"], "scores": [[0.23876953125, 0.1287841796875, 5.960464477539063e-08, 0.0013828277587890625, 8.7738037109375e-05, 0.006061553955078125, 0.41455078125, 0.05072021484375, 0.012603759765625, 0.048614501953125, 0.96240234375, 0.01317596435546875, 0.005191802978515625, 0.00504302978515625, 0.041839599609375, 0.87255859375, 0.52734375, 0.99853515625, 0.98095703125, 0.72216796875, 0.81103515625, 0.048553466796875, 0.177734375, 0.275146484375, 0.947265625, 0.998046875, 0.9970703125]], "normalized_score": [0.055775199085474014], "tokens": [["\u2581No", "\u2581information", "\u2581#", "B", "log", "T", "our", "\u2581#", "Book", "Re", "view", "\u2581The", "\u2581Man", "\u2581I", "\u2581F", "ell", "\u2581In", "\u2581Love", "\u2581With", "\u2581by", "\u2581Kate", "\u2581Field", "\u2581#", "Net", "G", "al", "ley"]], "entropy": [[3.96484375, 4.01171875, 2.728515625, 4.6640625, 3.240234375, 4.375, 2.322265625, 2.388671875, 4.921875, 3.150390625, 0.2177734375, 2.32421875, 6.359375, 2.638671875, 2.05078125, 0.560546875, 0.74267578125, 0.0120086669921875, 0.111572265625, 1.5400390625, 0.869140625, 3.04296875, 1.73828125, 2.685546875, 0.2498779296875, 0.017486572265625, 0.0237274169921875]], "avg_entropy": [2.2574915002893516], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Principaut\u00e9 de Monaco", "score_of_biggest_cluster": 0.055775199085474014, "score_of_first_answer": 0.055775199085474014, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.055775199085474014, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40532470806464344, "std_of_first_answer": 0.40532470806464344, "avg_entropy_of_first_answer": 2.2574915002893516, "lex_sim": 1.0}
{"question": "Which company produces the 'Magentis' car model?", "answer": ["I don't know."], "scores": [[0.128662109375, 0.0286102294921875, 0.9775390625, 0.9990234375, 0.6435546875, 0.2269287109375, 1.1920928955078125e-06, 0.003093719482421875, 7.408857345581055e-05, 0.0002689361572265625, 0.1541748046875, 0.99951171875, 2.2411346435546875e-05]], "normalized_score": [0.012204191647469997], "tokens": [["\u2581I", "\u2581don", "'", "t", "\u2581know", ".", "\u2581#", "2", "\u2581D", "irt", "\u2581bi", "ke", "\u2581action"]], "entropy": [[3.84765625, 1.7109375, 0.158447265625, 0.007785797119140625, 0.8369140625, 2.265625, 2.484375, 4.80859375, 2.9453125, 5.00390625, 2.349609375, 0.0059051513671875, 3.587890625]], "avg_entropy": [2.3086891174316406], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "K. I. A.", "score_of_biggest_cluster": 0.012204191647469997, "score_of_first_answer": 0.012204191647469997, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 1.1920928955078125e-06, "avg_score": 0.012204191647469997, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4045251316626199, "std_of_first_answer": 0.4045251316626199, "avg_entropy_of_first_answer": 2.3086891174316406, "lex_sim": 1.0}
{"question": "How many cross-stars feature in the Subaru automotive badge logo?", "answer": ["Six"], "scores": [[0.25390625]], "normalized_score": [0.25390625], "tokens": [["\u2581Six"]], "entropy": [[1.53515625]], "avg_entropy": [1.53515625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Six", "score_of_biggest_cluster": 0.25390625, "score_of_first_answer": 0.25390625, "max_score_of_first_answer": 0.25390625, "min_score_of_first_answer": 0.25390625, "avg_score": 0.25390625, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.53515625, "lex_sim": 1.0}
{"question": "Name the year - Norfolk farmer Tony Martin shootsand kills a 16 year old burglar in his farmhouse, UK fuel protesters almost bring the country to a standstill and Paula Yates and Robin Day both sadly die ?", "answer": ["2000 #88: The Unspoken War - A Story of Tibet / Julie Randol"], "scores": [[0.371337890625, 0.98876953125, 0.99609375, 0.99755859375, 0.998046875, 5.245208740234375e-06, 0.002819061279296875, 0.0034236907958984375, 0.00109100341796875, 0.04315185546875, 0.00101470947265625, 0.0003135204315185547, 0.958984375, 0.0024623870849609375, 0.04840087890625, 0.06475830078125, 0.0045623779296875, 0.9091796875, 0.0088043212890625, 0.9990234375, 0.0017375946044921875, 0.0001285076141357422, 0.50732421875, 0.001834869384765625, 0.010040283203125]], "normalized_score": [0.019394543021917343], "tokens": [["\u2581", "2", "0", "0", "0", "\u2581#", "8", "8", ":", "\u2581The", "\u2581Un", "sp", "oken", "\u2581War", "\u2581-", "\u2581A", "\u2581Story", "\u2581of", "\u2581Tib", "et", "\u2581/", "\u2581Jul", "ie", "\u2581Rand", "ol"]], "entropy": [[1.84375, 0.07696533203125, 0.04010009765625, 0.0265960693359375, 0.0172576904296875, 1.3671875, 5.59375, 1.34765625, 2.046875, 5.76171875, 4.046875, 3.38671875, 0.25830078125, 4.53515625, 3.185546875, 5.70703125, 5.26953125, 0.61328125, 3.88671875, 0.01085662841796875, 3.10546875, 5.890625, 1.9736328125, 5.1015625, 3.458984375]], "avg_entropy": [2.7420858764648437], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "2000", "score_of_biggest_cluster": 0.019394543021917343, "score_of_first_answer": 0.019394543021917343, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 5.245208740234375e-06, "avg_score": 0.019394543021917343, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.42893782908543265, "std_of_first_answer": 0.42893782908543265, "avg_entropy_of_first_answer": 2.7420858764648437, "lex_sim": 1.0}
{"question": "Mr Worldly Wisemen appears in which 17th Century book?", "answer": ["Pilgrim's Progress #GivingTuesday is here!"], "scores": [[0.1314697265625, 0.986328125, 0.9326171875, 0.96826171875, 1.0, 0.94970703125, 8.940696716308594e-07, 0.00028204917907714844, 0.003093719482421875, 0.01081085205078125, 0.96533203125, 0.9775390625, 0.0015163421630859375, 0.00016736984252929688, 0.88134765625, 0.08648681640625, 0.6611328125, 0.1636962890625, 0.96240234375, 0.63427734375, 0.8310546875, 0.451171875, 0.9111328125, 0.3955078125, 0.1737060546875, 0.9990234375, 1.0, 1.0, 0.113525390625, 0.3291015625, 0.95751953125, 0.998046875, 0.9951171875, 0.09844970703125]], "normalized_score": [0.14980025589466095], "tokens": [["\u2581Pil", "gr", "im", "'", "s", "\u2581Progress", "\u2581#", "G", "iving", "T", "ues", "day", "\u2581is", "\u2581here", "!", "\u2581Help", "\u2581us", "\u2581reach", "\u2581our", "\u2581goal", "\u2581of", "\u2581raising", "\u2581$", "5", ",", "0", "0", "0", "\u2581in", "\u2581", "2", "4", "\u2581hours", "."]], "entropy": [[2.6328125, 0.11761474609375, 0.276611328125, 0.2266845703125, 0.001255035400390625, 0.252197265625, 1.203125, 5.0, 4.4609375, 4.0546875, 0.268310546875, 0.1588134765625, 2.416015625, 1.99609375, 0.68115234375, 3.412109375, 1.568359375, 3.0703125, 0.240966796875, 1.291015625, 0.73291015625, 1.3935546875, 0.50146484375, 1.666015625, 0.53466796875, 0.0089111328125, 0.0003228187561035156, 0.0010480880737304688, 1.3076171875, 1.5517578125, 0.23388671875, 0.0195465087890625, 0.037506103515625, 0.98779296875]], "avg_entropy": [1.2442963964798872], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Delectable Mountains", "score_of_biggest_cluster": 0.14980025589466095, "score_of_first_answer": 0.14980025589466095, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 8.940696716308594e-07, "avg_score": 0.14980025589466095, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4145398391496067, "std_of_first_answer": 0.4145398391496067, "avg_entropy_of_first_answer": 1.2442963964798872, "lex_sim": 1.0}
{"question": "The 1827 Battle of Navarino in which the Ottoman/Egyptian armada was destroyed by a combined British, French and Russian navy is notable for being the last battle to be fought with sailing ships. It is a conflict in the war of independence of which country?", "answer": ["Greece's War of Independence"], "scores": [[0.94384765625, 0.007671356201171875, 0.99951171875, 0.14892578125, 0.99609375, 0.98974609375, 1.0]], "normalized_score": [0.376005083322525], "tokens": [["\u2581Greece", "'", "s", "\u2581War", "\u2581of", "\u2581Independ", "ence"]], "entropy": [[0.333251953125, 0.86572265625, 0.007373809814453125, 1.5546875, 0.0325927734375, 0.066650390625, 0.0016984939575195312]], "avg_entropy": [0.40885393960135324], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yunanistan", "score_of_biggest_cluster": 0.376005083322525, "score_of_first_answer": 0.376005083322525, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.007671356201171875, "avg_score": 0.376005083322525, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4121136342822927, "std_of_first_answer": 0.4121136342822927, "avg_entropy_of_first_answer": 0.40885393960135324, "lex_sim": 1.0}
{"question": "Which cartoon character lived in Bunkerton Castle?", "answer": ["Dennis the Menace #WeAreAllUofL: How to talk to kids about a campus shooting"], "scores": [[0.035491943359375, 0.98681640625, 0.9970703125, 0.99951171875, 1.3709068298339844e-06, 0.00011354684829711914, 0.18310546875, 0.01023101806640625, 0.006793975830078125, 0.0010118484497070312, 0.035003662109375, 0.0011110305786132812, 0.01189422607421875, 0.039520263671875, 0.0010385513305664062, 0.64453125, 0.03863525390625, 0.99951171875, 0.982421875, 0.017547607421875, 0.0036754608154296875, 0.5234375]], "normalized_score": [0.023198336362838745], "tokens": [["\u2581Dennis", "\u2581the", "\u2581Men", "ace", "\u2581#", "We", "Are", "All", "U", "of", "L", ":", "\u2581How", "\u2581to", "\u2581talk", "\u2581to", "\u2581k", "ids", "\u2581about", "\u2581a", "\u2581campus", "\u2581shooting"]], "entropy": [[3.88671875, 0.092041015625, 0.025360107421875, 0.0039520263671875, 0.7841796875, 6.16796875, 4.25390625, 2.01953125, 3.341796875, 2.755859375, 2.826171875, 2.310546875, 5.51953125, 2.99609375, 5.625, 1.2451171875, 2.900390625, 0.00540924072265625, 0.137939453125, 3.8046875, 4.2578125, 2.017578125]], "avg_entropy": [2.589890566739169], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lord Snooty", "score_of_biggest_cluster": 0.023198336362838745, "score_of_first_answer": 0.023198336362838745, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 1.3709068298339844e-06, "avg_score": 0.023198336362838745, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4116960334831608, "std_of_first_answer": 0.4116960334831608, "avg_entropy_of_first_answer": 2.589890566739169, "lex_sim": 1.0}
{"question": "Bad Guys, You Give a Little Love & So You Wanna be a Boxer are all songs from which musical?", "answer": ["The musical is: ANN #NYPD Officers Ambushed and Shot in the Bronx."], "scores": [[0.11700439453125, 0.3076171875, 0.90087890625, 0.016754150390625, 0.11260986328125, 0.306640625, 1.7881393432617188e-07, 0.0013608932495117188, 0.0095672607421875, 0.0005192756652832031, 3.5762786865234375e-06, 0.9912109375, 4.827976226806641e-06, 0.623046875, 0.20068359375, 0.0033092498779296875, 0.99853515625, 0.268798828125, 0.36962890625, 0.880859375, 1.0, 0.066162109375]], "normalized_score": [0.023023616522550583], "tokens": [["\u2581The", "\u2581musical", "\u2581is", ":", "\u2581A", "NN", "\u2581#", "N", "Y", "PD", "\u2581Offic", "ers", "\u2581Amb", "ushed", "\u2581and", "\u2581Sh", "ot", "\u2581in", "\u2581the", "\u2581Bron", "x", "."]], "entropy": [[3.791015625, 2.962890625, 0.62109375, 2.4453125, 3.751953125, 2.447265625, 0.0284576416015625, 4.84765625, 2.87109375, 3.90625, 3.220703125, 0.0911865234375, 4.7734375, 0.76513671875, 3.328125, 0.23974609375, 0.0172271728515625, 2.796875, 2.298828125, 0.6142578125, 0.0020904541015625, 3.05859375]], "avg_entropy": [2.2217816439541904], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bugsy Malone", "score_of_biggest_cluster": 0.023023616522550583, "score_of_first_answer": 0.023023616522550583, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.7881393432617188e-07, "avg_score": 0.023023616522550583, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37441114100606854, "std_of_first_answer": 0.37441114100606854, "avg_entropy_of_first_answer": 2.2217816439541904, "lex_sim": 1.0}
{"question": "In which city's shipyard was Solidarity set up?", "answer": ["Gdansk"], "scores": [[0.88671875, 0.443359375, 0.96533203125]], "normalized_score": [0.7240015268325806], "tokens": [["\u2581G", "d", "ansk"]], "entropy": [[0.78271484375, 0.73046875, 0.186279296875]], "avg_entropy": [0.5664876302083334], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "D\u00e0nzica", "score_of_biggest_cluster": 0.7240015268325806, "score_of_first_answer": 0.7240015268325806, "max_score_of_first_answer": 0.96533203125, "min_score_of_first_answer": 0.443359375, "avg_score": 0.7240015268325806, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.22978324019290752, "std_of_first_answer": 0.22978324019290752, "avg_entropy_of_first_answer": 0.5664876302083334, "lex_sim": 1.0}
{"question": "What is Volkswagen's all-time best-selling car model?", "answer": ["Golf ###Overview"], "scores": [[0.48046875, 0.99951171875, 0.0, 7.826089859008789e-05, 0.14599609375]], "normalized_score": [0.0], "tokens": [["\u2581G", "olf", "\u2581###", "Over", "view"]], "entropy": [[1.2041015625, 0.005062103271484375, 0.8330078125, 3.83203125, 2.236328125]], "avg_entropy": [1.622106170654297], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Volkswagen Super Beetle", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3800994352588819, "std_of_first_answer": 0.3800994352588819, "avg_entropy_of_first_answer": 1.622106170654297, "lex_sim": 1.0}
{"question": "Arthur Dent and Ford Prefect first appear in which book?", "answer": ["The Hitchhiker's Guide to the Galaxy #CoverReveal: The Way Weburn by Chani Lynn Feener"], "scores": [[0.83984375, 0.98095703125, 0.998046875, 0.95654296875, 0.9892578125, 0.99267578125, 1.0, 0.9970703125, 0.994140625, 0.9990234375, 0.99853515625, 1.0, 1.1920928955078125e-06, 0.00029087066650390625, 0.003936767578125, 0.0029449462890625, 0.7216796875, 0.92431640625, 0.012969970703125, 0.24462890625, 1.7583370208740234e-05, 8.940696716308594e-07, 5.960464477539062e-07, 0.0002841949462890625, 0.003643035888671875, 0.006908416748046875, 0.5908203125, 0.97412109375, 0.888671875, 0.9697265625]], "normalized_score": [0.037828121334314346], "tokens": [["\u2581The", "\u2581H", "itch", "h", "iker", "'", "s", "\u2581Guide", "\u2581to", "\u2581the", "\u2581Gal", "axy", "\u2581#", "C", "over", "Re", "ve", "al", ":", "\u2581The", "\u2581Way", "\u2581Web", "urn", "\u2581by", "\u2581Ch", "ani", "\u2581Lyn", "n", "\u2581Fe", "ener"]], "entropy": [[0.76318359375, 0.16162109375, 0.0207061767578125, 0.227294921875, 0.0621337890625, 0.057342529296875, 0.0005965232849121094, 0.0256195068359375, 0.044097900390625, 0.00998687744140625, 0.01152801513671875, 0.0010480880737304688, 0.9345703125, 3.46484375, 4.10546875, 3.626953125, 1.224609375, 0.29052734375, 1.93359375, 4.5703125, 3.068359375, 1.7998046875, 4.71875, 0.37646484375, 5.37109375, 3.962890625, 2.087890625, 0.2420654296875, 0.78564453125, 0.210693359375]], "avg_entropy": [1.4719898382822671], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hitchhiker's Guide to the Galaxy", "score_of_biggest_cluster": 0.037828121334314346, "score_of_first_answer": 0.037828121334314346, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.960464477539062e-07, "avg_score": 0.037828121334314346, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4563210465285823, "std_of_first_answer": 0.4563210465285823, "avg_entropy_of_first_answer": 1.4719898382822671, "lex_sim": 1.0}
{"question": "Which actress is the mother of actress Carrie Fisher?", "answer": ["Debbie Reynolds #CoverReveal: \"Ghosted\" by J. M. Darhower"], "scores": [[0.9765625, 0.99853515625, 0.99951171875, 1.0, 5.960464477539063e-08, 0.004261016845703125, 3.6716461181640625e-05, 0.00029921531677246094, 0.35009765625, 0.953125, 0.0264434814453125, 0.1778564453125, 0.0007619857788085938, 0.00534820556640625, 0.01165008544921875, 0.383544921875, 0.861328125, 0.047332763671875, 0.80029296875, 0.0860595703125, 0.98583984375, 0.3291015625, 0.99462890625, 0.99951171875]], "normalized_score": [0.05034756287932396], "tokens": [["\u2581Deb", "bie", "\u2581Reyn", "olds", "\u2581#", "C", "over", "Re", "ve", "al", ":", "\u2581\"", "G", "host", "ed", "\"", "\u2581by", "\u2581J", ".", "\u2581M", ".", "\u2581Dar", "h", "ower"]], "entropy": [[0.211181640625, 0.01207733154296875, 0.002834320068359375, 0.0018777847290039062, 0.73876953125, 5.87890625, 1.798828125, 2.724609375, 2.390625, 0.19775390625, 2.62109375, 3.5234375, 3.0703125, 4.3828125, 3.4921875, 2.091796875, 0.978515625, 1.580078125, 1.3125, 1.2109375, 0.111572265625, 2.96484375, 0.046600341796875, 0.007602691650390625]], "avg_entropy": [1.7229897578557332], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Debbie Reynolds", "score_of_biggest_cluster": 0.05034756287932396, "score_of_first_answer": 0.05034756287932396, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.05034756287932396, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.43715240345844214, "std_of_first_answer": 0.43715240345844214, "avg_entropy_of_first_answer": 1.7229897578557332, "lex_sim": 1.0}
{"question": "One of the Wonders of the Modern World, The Catacombs of Alexandria are in which country?", "answer": ["Egypt"], "scores": [[0.935546875]], "normalized_score": [0.935546875], "tokens": [["\u2581Egypt"]], "entropy": [[0.53369140625]], "avg_entropy": [0.53369140625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "EGY", "score_of_biggest_cluster": 0.935546875, "score_of_first_answer": 0.935546875, "max_score_of_first_answer": 0.935546875, "min_score_of_first_answer": 0.935546875, "avg_score": 0.935546875, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.53369140625, "lex_sim": 1.0}
{"question": "\"Which car company, in 1986, comissioned \"\"Cars\"\", a series of artworks to track the evolution of its designs, from Andy Warhol?\"", "answer": ["I'm sorry, that information is not accurate."], "scores": [[0.0004596710205078125, 0.26953125, 0.9853515625, 0.88525390625, 0.96240234375, 0.0098724365234375, 0.7802734375, 0.919921875, 0.8095703125, 0.161865234375, 0.904296875, 0.0411376953125, 0.78076171875, 0.61474609375, 0.1746826171875, 0.5009765625, 0.9853515625, 0.9990234375, 0.261474609375, 0.99951171875, 0.1337890625, 0.99951171875, 0.99951171875, 0.01026153564453125, 0.9677734375, 0.99951171875, 0.9990234375, 0.9990234375, 0.99267578125, 0.82080078125, 0.9990234375, 0.99853515625, 0.998046875, 0.85546875, 0.99853515625, 1.0, 1.0, 1.0, 1.0, 0.9697265625, 0.0, 1.8477439880371094e-06, 0.00026226043701171875, 0.0034332275390625, 0.953125]], "normalized_score": [0.0], "tokens": [["\u2581I", "'", "m", "\u2581sorry", ",", "\u2581that", "\u2581information", "\u2581is", "\u2581not", "\u2581accurate", ".", "\u2581It", "\u2581is", "\u2581not", "\u2581clear", "\u2581which", "\u2581car", "\u2581company", "\u2581commission", "ed", "\u2581\"", "C", "ars", ",\"", "\u2581a", "\u2581series", "\u2581of", "\u2581art", "works", "\u2581from", "\u2581Andy", "\u2581War", "hol", "\u2581in", "\u2581", "1", "9", "8", "6", ".", "\u2581Tags", "\u2581>", "\u2581M", "umb", "ai"]], "entropy": [[2.3359375, 2.615234375, 0.09930419921875, 0.493408203125, 0.1947021484375, 0.7568359375, 0.923828125, 0.43212890625, 0.6533203125, 2.1796875, 0.48974609375, 2.466796875, 0.80322265625, 1.4169921875, 1.615234375, 1.0283203125, 0.09234619140625, 0.00965118408203125, 1.9765625, 0.003627777099609375, 1.6708984375, 0.00524139404296875, 0.0023822784423828125, 0.10015869140625, 0.1998291015625, 0.004253387451171875, 0.00907135009765625, 0.0087432861328125, 0.04815673828125, 0.6904296875, 0.00811767578125, 0.01043701171875, 0.01526641845703125, 0.52294921875, 0.01090240478515625, 4.291534423828125e-05, 0.00021266937255859375, 0.0015039443969726562, 0.0011262893676757812, 0.18994140625, 1.638671875, 1.27734375, 5.26171875, 4.26171875, 0.2783203125]], "avg_entropy": [0.8178738911946615], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Mercedes benz", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39960727809093194, "std_of_first_answer": 0.39960727809093194, "avg_entropy_of_first_answer": 0.8178738911946615, "lex_sim": 1.0}
{"question": "Which movie starred Michael Caine as a murderous transvestite psychiatrist?", "answer": ["Dressed to Kill (1980 film) #BookReview: The Chalk Man by C.J."], "scores": [[0.52734375, 0.97216796875, 1.0, 0.9814453125, 0.9990234375, 0.181640625, 0.994140625, 1.0, 1.0, 0.99951171875, 0.0160980224609375, 0.99560546875, 3.039836883544922e-06, 7.271766662597656e-06, 0.01007843017578125, 0.55615234375, 0.0699462890625, 0.07763671875, 0.0009288787841796875, 0.0791015625, 0.04010009765625, 0.814453125, 0.990234375, 0.798828125, 0.68310546875, 0.98974609375, 0.9580078125, 1.0, 1.0]], "normalized_score": [0.1496855914592743], "tokens": [["\u2581D", "ress", "ed", "\u2581to", "\u2581Kill", "\u2581(", "1", "9", "8", "0", "\u2581film", ")", "\u2581#", "Book", "Re", "view", ":", "\u2581The", "\u2581Ch", "alk", "\u2581Man", "\u2581by", "\u2581C", ".", "J", ".", "\u2581T", "ud", "or"]], "entropy": [[1.134765625, 0.14794921875, 0.0024776458740234375, 0.1131591796875, 0.00994110107421875, 1.064453125, 0.050079345703125, 0.0009756088256835938, 0.001628875732421875, 0.004161834716796875, 0.11212158203125, 0.0404052734375, 0.7451171875, 4.3984375, 4.05078125, 0.76220703125, 2.205078125, 4.125, 6.09765625, 3.509765625, 0.64501953125, 1.1123046875, 0.08380126953125, 0.65283203125, 0.75048828125, 0.076904296875, 0.28271484375, 0.00018417835235595703, 0.00058746337890625]], "avg_entropy": [1.1096895850937942], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dressed to Kill", "score_of_biggest_cluster": 0.1496855914592743, "score_of_first_answer": 0.1496855914592743, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 3.039836883544922e-06, "avg_score": 0.1496855914592743, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4284812213171822, "std_of_first_answer": 0.4284812213171822, "avg_entropy_of_first_answer": 1.1096895850937942, "lex_sim": 1.0}
{"question": "The left side of the heart pumps blood to all parts of the body, but the right side pumps blood to which organ in particular?", "answer": ["Lungs #BookReview: The Secret Keeper by Kate Morton - 4.5 Wine Glasses"], "scores": [[0.02691650390625, 0.95947265625, 5.960464477539063e-08, 1.8894672393798828e-05, 0.005306243896484375, 0.26904296875, 0.2017822265625, 0.1575927734375, 0.0176849365234375, 0.005706787109375, 0.99951171875, 1.0, 0.77880859375, 0.939453125, 1.0, 0.998046875, 0.056884765625, 0.02325439453125, 0.138916015625, 0.2841796875, 0.96142578125, 6.270408630371094e-05, 0.9296875, 0.998046875, 1.0, 0.91748046875]], "normalized_score": [0.0666339248418808], "tokens": [["\u2581L", "ungs", "\u2581#", "Book", "Re", "view", ":", "\u2581The", "\u2581Secret", "\u2581K", "ee", "per", "\u2581by", "\u2581Kate", "\u2581Mort", "on", "\u2581-", "\u2581", "4", ".", "5", "\u2581W", "ine", "\u2581G", "lass", "es"]], "entropy": [[1.513671875, 0.22900390625, 0.791015625, 6.05859375, 4.12109375, 0.74755859375, 2.447265625, 3.13671875, 5.6796875, 2.74609375, 0.00616455078125, 0.0006570816040039062, 1.1142578125, 0.32421875, 0.0010919570922851562, 0.01541900634765625, 2.654296875, 3.490234375, 2.173828125, 2.1640625, 0.2529296875, 1.7099609375, 0.53173828125, 0.016693115234375, 0.000164031982421875, 0.420166015625]], "avg_entropy": [1.6287148549006536], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pulmonary surgical procedures", "score_of_biggest_cluster": 0.0666339248418808, "score_of_first_answer": 0.0666339248418808, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.0666339248418808, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44292416325136486, "std_of_first_answer": 0.44292416325136486, "avg_entropy_of_first_answer": 1.6287148549006536, "lex_sim": 1.0}
{"question": "The Japanese condiment wasabi is produced from what part of the the wasabi plant?", "answer": ["Root"], "scores": [[0.141357421875]], "normalized_score": [0.141357421875], "tokens": [["\u2581Root"]], "entropy": [[2.705078125]], "avg_entropy": [2.705078125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Root (botany)", "score_of_biggest_cluster": 0.141357421875, "score_of_first_answer": 0.141357421875, "max_score_of_first_answer": 0.141357421875, "min_score_of_first_answer": 0.141357421875, "avg_score": 0.141357421875, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.705078125, "lex_sim": 1.0}
{"question": "Who won the BDO World Professional Darts Championship in 2009. (his nickname is The Count')?", "answer": ["Ted Hankey **Spoiler-Free** 'Avengers: Endgame' Theories, Predictions, and Speculation"], "scores": [[0.367431640625, 0.46875, 1.0, 6.556510925292969e-07, 0.001590728759765625, 0.60302734375, 0.89013671875, 0.00159454345703125, 0.1356201171875, 0.89013671875, 0.00021064281463623047, 0.0004398822784423828, 0.04541015625, 0.99609375, 0.99609375, 0.677734375, 1.0, 0.984375, 0.0001252889633178711, 0.06329345703125, 0.033203125, 0.62158203125, 1.0, 0.9990234375, 0.8310546875, 0.9130859375, 0.81298828125, 1.0, 0.310302734375]], "normalized_score": [0.09391261637210846], "tokens": [["\u2581Ted", "\u2581Han", "key", "\u2581**", "S", "po", "iler", "-", "Free", "**", "\u2581'", "A", "ven", "gers", ":", "\u2581End", "game", "'", "\u2581The", "ories", ",", "\u2581Pred", "ict", "ions", ",", "\u2581and", "\u2581Spe", "cul", "ation"]], "entropy": [[2.021484375, 1.8701171875, 0.00157928466796875, 1.2001953125, 3.6171875, 2.359375, 0.6328125, 2.232421875, 4.7578125, 0.85302734375, 1.4365234375, 2.232421875, 5.046875, 0.0304107666015625, 0.035003662109375, 0.81494140625, 0.0015544891357421875, 0.11962890625, 1.806640625, 4.3515625, 2.603515625, 1.810546875, 0.0013837814331054688, 0.0104217529296875, 0.6337890625, 0.52685546875, 1.1474609375, 0.00018215179443359375, 0.6318359375]], "avg_entropy": [1.475433349609375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ted Hankey", "score_of_biggest_cluster": 0.09391261637210846, "score_of_first_answer": 0.09391261637210846, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 6.556510925292969e-07, "avg_score": 0.09391261637210846, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41393828344088496, "std_of_first_answer": 0.41393828344088496, "avg_entropy_of_first_answer": 1.475433349609375, "lex_sim": 1.0}
{"question": "Which European country has the same flag as Chad - a blue, yellow and red tricolour?", "answer": ["North Macedonia"], "scores": [[0.0038661956787109375, 0.435791015625, 0.99951171875, 0.99951171875]], "normalized_score": [0.20255103707313538], "tokens": [["\u2581North", "\u2581Mac", "ed", "onia"]], "entropy": [[3.47265625, 1.775390625, 0.00385284423828125, 0.00446319580078125]], "avg_entropy": [1.3140907287597656], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ISO 3166-1:RO", "score_of_biggest_cluster": 0.20255103707313538, "score_of_first_answer": 0.20255103707313538, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.0038661956787109375, "avg_score": 0.20255103707313538, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4186840341753048, "std_of_first_answer": 0.4186840341753048, "avg_entropy_of_first_answer": 1.3140907287597656, "lex_sim": 1.0}
{"question": "Nouakchott is the capital of which Islamic African republic?", "answer": ["Islamic Republic of Mauritania"], "scores": [[0.005645751953125, 0.9970703125, 0.9013671875, 0.99609375, 0.99951171875, 1.0, 1.0]], "normalized_score": [0.46980714797973633], "tokens": [["\u2581Islam", "ic", "\u2581Republic", "\u2581of", "\u2581Maur", "it", "ania"]], "entropy": [[0.344970703125, 0.02703857421875, 0.61669921875, 0.0303955078125, 0.005390167236328125, 0.0017347335815429688, 0.0015926361083984375]], "avg_entropy": [0.14683164869035994], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Domain of Moor", "score_of_biggest_cluster": 0.46980714797973633, "score_of_first_answer": 0.46980714797973633, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.005645751953125, "avg_score": 0.46980714797973633, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3434151788083086, "std_of_first_answer": 0.3434151788083086, "avg_entropy_of_first_answer": 0.14683164869035994, "lex_sim": 1.0}
{"question": "Which English producer and screenwriter is responsible for the creation of the fictional school Grange Hill ?", "answer": ["Phil Redmond Tags: MSCI | FTSE | US | Europe | Stock"], "scores": [[0.82275390625, 0.99755859375, 0.98095703125, 0.0, 0.9677734375, 6.628036499023438e-05, 0.0005488395690917969, 0.02545166015625, 0.0006260871887207031, 0.006561279296875, 0.047821044921875, 0.97998046875, 0.77978515625, 0.0178070068359375, 0.097412109375, 0.007781982421875, 0.91064453125, 0.0008692741394042969]], "normalized_score": [0.0], "tokens": [["\u2581Phil", "\u2581Red", "mond", "\u2581Tags", ":", "\u2581M", "SC", "I", "\u2581|", "\u2581F", "T", "SE", "\u2581|", "\u2581US", "\u2581|", "\u2581Europe", "\u2581|", "\u2581Stock"]], "entropy": [[1.21875, 0.0247802734375, 0.148193359375, 0.6767578125, 0.2763671875, 3.40625, 4.26171875, 4.0, 2.009765625, 5.4453125, 4.8671875, 0.1614990234375, 0.99462890625, 5.10546875, 2.9296875, 5.34375, 0.60546875, 4.69921875]], "avg_entropy": [2.5652669270833335], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Phil REDMOND", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.99755859375, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4427682295306705, "std_of_first_answer": 0.4427682295306705, "avg_entropy_of_first_answer": 2.5652669270833335, "lex_sim": 1.0}
{"question": "A warm sunny period in Autumn (Fall) in the northern hemisphere is called an?", "answer": ["Indian Summer ###H1###Agriculture Industry in India"], "scores": [[0.9658203125, 0.24951171875, 0.0, 0.00021374225616455078, 0.05377197265625, 0.1219482421875, 0.96044921875, 0.0010013580322265625, 1.0371208190917969e-05, 0.6806640625, 0.76416015625, 0.0006155967712402344, 0.96142578125, 0.06024169921875, 0.423828125]], "normalized_score": [0.0], "tokens": [["\u2581Indian", "\u2581Summer", "\u2581###", "H", "1", "##", "#", "A", "gr", "icult", "ure", "\u2581Indust", "ry", "\u2581in", "\u2581India"]], "entropy": [[0.298095703125, 1.099609375, 0.81787109375, 3.59375, 4.1640625, 3.16796875, 0.26708984375, 1.6572265625, 2.484375, 1.283203125, 0.59326171875, 3.5546875, 0.19482421875, 4.19140625, 2.8984375]], "avg_entropy": [2.017724609375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Indian Summer", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.9658203125, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38790623945885705, "std_of_first_answer": 0.38790623945885705, "avg_entropy_of_first_answer": 2.017724609375, "lex_sim": 1.0}
{"question": "Bryan Ferry was the lead singer of which 1970\u2019s band?", "answer": ["Roxy Music #MeToo and Workplace Respect: A Compliance and Human Resources Perspective"], "scores": [[0.99560546875, 0.99951171875, 0.994140625, 3.5762786865234375e-07, 1.1444091796875e-05, 0.72509765625, 0.99755859375, 0.0008988380432128906, 8.255243301391602e-05, 0.5537109375, 0.003253936767578125, 0.78955078125, 0.06622314453125, 0.12164306640625, 0.0006670951843261719, 0.77978515625, 0.0667724609375, 0.10284423828125, 0.82177734375, 0.16357421875, 0.99951171875]], "normalized_score": [0.037593841552734375], "tokens": [["\u2581Ro", "xy", "\u2581Music", "\u2581#", "Me", "To", "o", "\u2581and", "\u2581Work", "place", "\u2581Res", "pect", ":", "\u2581A", "\u2581Comp", "liance", "\u2581and", "\u2581Human", "\u2581Resources", "\u2581Pers", "pective"]], "entropy": [[0.049346923828125, 0.0034160614013671875, 0.0467529296875, 0.73486328125, 4.73046875, 1.6982421875, 0.0249481201171875, 2.93359375, 4.2890625, 2.302734375, 3.81640625, 0.6767578125, 3.55078125, 4.44140625, 3.2734375, 0.9521484375, 3.28125, 2.544921875, 0.65625, 0.91259765625, 0.0029296875]], "avg_entropy": [1.9486816951206751], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Roxy Music", "score_of_biggest_cluster": 0.037593841552734375, "score_of_first_answer": 0.037593841552734375, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 3.5762786865234375e-07, "avg_score": 0.037593841552734375, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4230233994014418, "std_of_first_answer": 0.4230233994014418, "avg_entropy_of_first_answer": 1.9486816951206751, "lex_sim": 1.0}
{"question": "Used frequently in laboratories what is the common name of Drosophila?", "answer": ["Fly (genus) #75 - The Best of the First 100: Finding Hope and Purpose in the Face of Tragedy"], "scores": [[0.17236328125, 0.007740020751953125, 0.11041259765625, 0.99462890625, 0.13427734375, 6.41942024230957e-05, 0.0002732276916503906, 0.0521240234375, 0.00016760826110839844, 0.01116943359375, 1.341104507446289e-05, 0.1376953125, 0.2039794921875, 0.0004401206970214844, 0.970703125, 0.98095703125, 0.9970703125, 0.9990234375, 0.0186309814453125, 0.0038814544677734375, 0.0011205673217773438, 0.0004105567932128906, 0.1466064453125, 0.0745849609375, 1.0, 0.79296875, 0.28076171875, 0.09149169921875, 0.99951171875, 0.008453369140625, 0.98486328125, 0.9931640625]], "normalized_score": [0.03788475692272186], "tokens": [["\u2581Fly", "\u2581(", "gen", "us", ")", "\u2581#", "7", "5", "\u2581-", "\u2581The", "\u2581Best", "\u2581of", "\u2581the", "\u2581First", "\u2581", "1", "0", "0", ":", "\u2581F", "inding", "\u2581Hope", "\u2581and", "\u2581Pur", "pose", "\u2581in", "\u2581the", "\u2581Face", "\u2581of", "\u2581T", "rag", "edy"]], "entropy": [[1.20703125, 0.8427734375, 3.154296875, 0.0401611328125, 0.6435546875, 0.8232421875, 2.919921875, 3.779296875, 2.671875, 3.72265625, 2.646484375, 4.7421875, 4.078125, 2.564453125, 0.251708984375, 0.1258544921875, 0.0248565673828125, 0.01042938232421875, 2.640625, 3.693359375, 2.2734375, 4.05859375, 1.7841796875, 3.78515625, 0.0006184577941894531, 1.1708984375, 2.86328125, 3.220703125, 0.006206512451171875, 3.03125, 0.10235595703125, 0.046966552734375]], "avg_entropy": [1.9664544016122818], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fruitfly", "score_of_biggest_cluster": 0.03788475692272186, "score_of_first_answer": 0.03788475692272186, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.341104507446289e-05, "avg_score": 0.03788475692272186, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4260850921193387, "std_of_first_answer": 0.4260850921193387, "avg_entropy_of_first_answer": 1.9664544016122818, "lex_sim": 1.0}
{"question": "Which English cathedral was damaged by an earthquake in the middle Ages", "answer": ["Lincoln Cathedral"], "scores": [[0.4228515625, 0.9111328125, 0.9990234375]], "normalized_score": [0.7274141907691956], "tokens": [["\u2581Lincoln", "\u2581C", "athedral"]], "entropy": [[2.08203125, 0.44189453125, 0.01003265380859375]], "avg_entropy": [0.8446528116861979], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Our Lady of the Undercroft", "score_of_biggest_cluster": 0.7274141907691956, "score_of_first_answer": 0.7274141907691956, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.4228515625, "avg_score": 0.7274141907691956, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.25344676640392877, "std_of_first_answer": 0.25344676640392877, "avg_entropy_of_first_answer": 0.8446528116861979, "lex_sim": 1.0}
{"question": "The first civilians executed for espionage in the United States for passing atomic bomb secrets to the Soviets, what couple were given the electric chair in 1953?", "answer": ["Julius and Ethel Rosenberg"], "scores": [[0.52001953125, 0.96533203125, 0.99755859375, 0.99951171875, 0.99853515625, 0.98828125]], "normalized_score": [0.8890873789787292], "tokens": [["\u2581Julius", "\u2581and", "\u2581Eth", "el", "\u2581Ros", "enberg"]], "entropy": [[0.7470703125, 0.180419921875, 0.024200439453125, 0.005359649658203125, 0.01325225830078125, 0.06451416015625]], "avg_entropy": [0.1724694569905599], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ethel Greenglass Rosenburg", "score_of_biggest_cluster": 0.8890873789787292, "score_of_first_answer": 0.8890873789787292, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.52001953125, "avg_score": 0.8890873789787292, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.17548858391588204, "std_of_first_answer": 0.17548858391588204, "avg_entropy_of_first_answer": 0.1724694569905599, "lex_sim": 1.0}
{"question": "Who wrote 'Confessions of an English Opium Eater'?", "answer": ["Thomas De Quincey"], "scores": [[0.994140625, 0.4990234375, 0.99560546875, 0.9912109375, 0.99951171875]], "normalized_score": [0.8668062090873718], "tokens": [["\u2581Thomas", "\u2581De", "\u2581Qu", "ince", "y"]], "entropy": [[0.05218505859375, 0.7119140625, 0.0295562744140625, 0.050262451171875, 0.0031948089599609375]], "avg_entropy": [0.16942253112792968], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Thomas Penson De Quincey", "score_of_biggest_cluster": 0.8668062090873718, "score_of_first_answer": 0.8668062090873718, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.4990234375, "avg_score": 0.8668062090873718, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.19845552137348826, "std_of_first_answer": 0.19845552137348826, "avg_entropy_of_first_answer": 0.16942253112792968, "lex_sim": 1.0}
{"question": "Fez is an African city which gives its name to a type of hat. In which country is it?", "answer": ["Morocco"], "scores": [[0.76953125, 0.9921875, 0.99853515625]], "normalized_score": [0.9135404825210571], "tokens": [["\u2581Mor", "oc", "co"]], "entropy": [[0.77001953125, 0.048553466796875, 0.01409149169921875]], "avg_entropy": [0.27755482991536456], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ISO 3166-1:MA", "score_of_biggest_cluster": 0.9135404825210571, "score_of_first_answer": 0.9135404825210571, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.76953125, "avg_score": 0.9135404825210571, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.10648885569323384, "std_of_first_answer": 0.10648885569323384, "avg_entropy_of_first_answer": 0.27755482991536456, "lex_sim": 1.0}
{"question": "The river Danube flows into which sea?", "answer": ["Black Sea"], "scores": [[0.71728515625, 0.58837890625]], "normalized_score": [0.6496425867080688], "tokens": [["\u2581Black", "\u2581Sea"]], "entropy": [[1.1240234375, 1.1611328125]], "avg_entropy": [1.142578125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Marea Neagr\u0103", "score_of_biggest_cluster": 0.6496425867080688, "score_of_first_answer": 0.6496425867080688, "max_score_of_first_answer": 0.71728515625, "min_score_of_first_answer": 0.58837890625, "avg_score": 0.6496425867080688, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.064453125, "std_of_first_answer": 0.064453125, "avg_entropy_of_first_answer": 1.142578125, "lex_sim": 1.0}
{"question": "In which US city do the basketball team known as the 'Trail Blazers' play their home games?", "answer": ["Portland #MeToo Movement Leads To Arrest Of CEO Of Nike\u2019s Japanese Arm"], "scores": [[0.986328125, 0.99609375, 1.1920928955078125e-07, 0.00018024444580078125, 0.427978515625, 0.994140625, 0.1400146484375, 0.99755859375, 0.0002720355987548828, 0.003021240234375, 0.1334228515625, 0.021697998046875, 0.99853515625, 0.384521484375, 0.006443023681640625, 0.99267578125, 0.0535888671875, 0.0286865234375, 0.2406005859375, 0.0095977783203125, 0.97021484375, 0.01593017578125, 0.300048828125]], "normalized_score": [0.045260827988386154], "tokens": [["\u2581Port", "land", "\u2581#", "Me", "To", "o", "\u2581Mov", "ement", "\u2581Le", "ads", "\u2581To", "\u2581Ar", "rest", "\u2581Of", "\u2581CE", "O", "\u2581Of", "\u2581N", "ike", "\u2019", "s", "\u2581Japanese", "\u2581Arm"]], "entropy": [[0.125732421875, 0.033203125, 0.8203125, 3.169921875, 3.26953125, 0.0513916015625, 2.58984375, 0.0210113525390625, 3.98828125, 0.221923828125, 2.685546875, 4.8515625, 0.0122222900390625, 1.607421875, 3.8515625, 0.047454833984375, 3.400390625, 5.76953125, 2.39453125, 3.751953125, 0.176513671875, 3.9765625, 1.8212890625]], "avg_entropy": [2.114682404891304], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Portland Oregon/Lore", "score_of_biggest_cluster": 0.045260827988386154, "score_of_first_answer": 0.045260827988386154, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 1.1920928955078125e-07, "avg_score": 0.045260827988386154, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4222683946575845, "std_of_first_answer": 0.4222683946575845, "avg_entropy_of_first_answer": 2.114682404891304, "lex_sim": 1.0}
{"question": "For which art form is Henri Cartier-Bresson best known?", "answer": ["Photography #90DaysOfUnity: Nigeria\u2019s Unity is Non-Negotiable, Says Lai Mohammed"], "scores": [[0.8974609375, 0.98388671875, 1.1920928955078125e-07, 0.0012044906616210938, 0.0323486328125, 3.987550735473633e-05, 0.9521484375, 0.0004398822784423828, 0.08807373046875, 0.033447265625, 3.421306610107422e-05, 0.84033203125, 0.006374359130859375, 0.9970703125, 0.2442626953125, 0.051910400390625, 0.13818359375, 0.99658203125, 0.8466796875, 1.0, 1.0, 1.0, 0.037872314453125, 0.4765625, 0.9970703125, 0.003498077392578125, 0.990234375, 0.97412109375, 0.98876953125]], "normalized_score": [0.059755366295576096], "tokens": [["\u2581Phot", "ography", "\u2581#", "9", "0", "Days", "Of", "Un", "ity", ":", "\u2581Niger", "ia", "\u2019", "s", "\u2581Unity", "\u2581is", "\u2581Non", "-", "N", "eg", "oti", "able", ",", "\u2581S", "ays", "\u2581La", "i", "\u2581Moh", "ammed"]], "entropy": [[0.517578125, 0.09619140625, 0.60400390625, 3.892578125, 3.765625, 3.44921875, 0.290283203125, 4.3203125, 4.02734375, 2.130859375, 5.078125, 0.5869140625, 2.916015625, 0.024810791015625, 3.642578125, 2.619140625, 3.703125, 0.0295867919921875, 0.441650390625, 0.0004949569702148438, 0.0009074211120605469, 0.00028133392333984375, 2.787109375, 3.064453125, 0.027587890625, 1.8759765625, 0.0672607421875, 0.1654052734375, 0.0809326171875]], "avg_entropy": [1.7312534430931354], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pet photography", "score_of_biggest_cluster": 0.059755366295576096, "score_of_first_answer": 0.059755366295576096, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.1920928955078125e-07, "avg_score": 0.059755366295576096, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4542083446110034, "std_of_first_answer": 0.4542083446110034, "avg_entropy_of_first_answer": 1.7312534430931354, "lex_sim": 1.0}
{"question": "What was the first name of the character played by John Travolta in Saturday Night Fever.", "answer": ["Tony"], "scores": [[0.98583984375]], "normalized_score": [0.98583984375], "tokens": [["\u2581Tony"]], "entropy": [[0.126953125]], "avg_entropy": [0.126953125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tony", "score_of_biggest_cluster": 0.98583984375, "score_of_first_answer": 0.98583984375, "max_score_of_first_answer": 0.98583984375, "min_score_of_first_answer": 0.98583984375, "avg_score": 0.98583984375, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 0.126953125, "lex_sim": 1.0}
{"question": "Anosmia is the lack of which of the senses in humans?", "answer": ["Smell #AskAlan"], "scores": [[0.6064453125, 0.97900390625, 1.0132789611816406e-06, 0.00394439697265625, 0.0266265869140625, 0.0015859603881835938, 0.01593017578125]], "normalized_score": [0.007694121915847063], "tokens": [["\u2581Sm", "ell", "\u2581#", "A", "sk", "Al", "an"]], "entropy": [[1.4453125, 0.11322021484375, 0.78173828125, 5.14453125, 4.390625, 4.81640625, 1.2080078125]], "avg_entropy": [2.5571201869419644], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Smell (disambiguation)", "score_of_biggest_cluster": 0.007694121915847063, "score_of_first_answer": 0.007694121915847063, "max_score_of_first_answer": 0.97900390625, "min_score_of_first_answer": 1.0132789611816406e-06, "avg_score": 0.007694121915847063, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36761763109975615, "std_of_first_answer": 0.36761763109975615, "avg_entropy_of_first_answer": 2.5571201869419644, "lex_sim": 1.0}
{"question": "Which letter of the Greek alphabet when written looks like a capital P ?", "answer": ["Pi"], "scores": [[0.240478515625]], "normalized_score": [0.240478515625], "tokens": [["\u2581Pi"]], "entropy": [[1.7626953125]], "avg_entropy": [1.7626953125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u03a1", "score_of_biggest_cluster": 0.240478515625, "score_of_first_answer": 0.240478515625, "max_score_of_first_answer": 0.240478515625, "min_score_of_first_answer": 0.240478515625, "avg_score": 0.240478515625, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.7626953125, "lex_sim": 1.0}
{"question": "Which huge global communications brand is derived from a local word for a marten?", "answer": ["Fox \\ Fishing \\ Fishing Tips \\ How to Choose the Right Fishing Rod Reel Combo"], "scores": [[0.06573486328125, 7.271766662597656e-06, 0.002552032470703125, 0.0011491775512695312, 0.042327880859375, 0.0078887939453125, 0.06976318359375, 0.061676025390625, 0.00024366378784179688, 0.498291015625, 0.03564453125, 0.92041015625, 0.0026798248291015625, 0.9990234375, 0.81787109375, 0.322021484375, 0.85595703125, 0.99072265625, 0.301025390625, 0.00548553466796875, 0.998046875, 0.95068359375, 0.75927734375]], "normalized_score": [0.05352286249399185], "tokens": [["\u2581Fox", "\u2581\\", "\u2581Fish", "ing", "\u2581\\", "\u2581Fish", "ing", "\u2581T", "ips", "\u2581\\", "\u2581How", "\u2581to", "\u2581Cho", "ose", "\u2581the", "\u2581Right", "\u2581Fish", "ing", "\u2581Rod", "\u2581Re", "el", "\u2581Com", "bo"]], "entropy": [[2.904296875, 1.302734375, 4.52734375, 0.77587890625, 3.33203125, 5.63671875, 2.3515625, 4.03125, 0.154541015625, 1.7724609375, 3.205078125, 0.371826171875, 0.482666015625, 0.01071929931640625, 0.9208984375, 0.8623046875, 0.814453125, 0.07086181640625, 1.6611328125, 2.19921875, 0.018707275390625, 0.289306640625, 0.65087890625]], "avg_entropy": [1.6672552357549253], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nokia Corporation", "score_of_biggest_cluster": 0.05352286249399185, "score_of_first_answer": 0.05352286249399185, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 7.271766662597656e-06, "avg_score": 0.05352286249399185, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40999405925781574, "std_of_first_answer": 0.40999405925781574, "avg_entropy_of_first_answer": 1.6672552357549253, "lex_sim": 1.0}
{"question": "Which serial killer hanged himself in Wakefield Prison in January 2004, four years after being found guilty of 15 murders?", "answer": ["Ian Brady"], "scores": [[0.2171630859375, 0.94873046875, 1.0]], "normalized_score": [0.5906220078468323], "tokens": [["\u2581Ian", "\u2581Br", "ady"]], "entropy": [[1.6865234375, 0.345703125, 8.26120376586914e-05]], "avg_entropy": [0.6774363915125529], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Doctor shipman", "score_of_biggest_cluster": 0.5906220078468323, "score_of_first_answer": 0.5906220078468323, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.2171630859375, "avg_score": 0.5906220078468323, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35756165504841003, "std_of_first_answer": 0.35756165504841003, "avg_entropy_of_first_answer": 0.6774363915125529, "lex_sim": 1.0}
{"question": "Which Oscar winning song came from the film Pocahontas", "answer": ["Colors of the Wind"], "scores": [[0.1612548828125, 0.99072265625, 0.9990234375, 0.98828125]], "normalized_score": [0.6302027702331543], "tokens": [["\u2581Colors", "\u2581of", "\u2581the", "\u2581Wind"]], "entropy": [[0.818359375, 0.05560302734375, 0.006900787353515625, 0.06719970703125]], "avg_entropy": [0.2370157241821289], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Colours of the Wind", "score_of_biggest_cluster": 0.6302027702331543, "score_of_first_answer": 0.6302027702331543, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.1612548828125, "avg_score": 0.6302027702331543, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3600378286065944, "std_of_first_answer": 0.3600378286065944, "avg_entropy_of_first_answer": 0.2370157241821289, "lex_sim": 1.0}
{"question": "In the fairy tale Thumbelina, what creature wants the miniature girl to marry her son ?", "answer": ["Field Mouse"], "scores": [[0.0182647705078125, 0.40625]], "normalized_score": [0.08613978326320648], "tokens": [["\u2581Field", "\u2581Mouse"]], "entropy": [[1.9462890625, 1.7607421875]], "avg_entropy": [1.853515625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "TOAD", "score_of_biggest_cluster": 0.08613978326320648, "score_of_first_answer": 0.08613978326320648, "max_score_of_first_answer": 0.40625, "min_score_of_first_answer": 0.0182647705078125, "avg_score": 0.08613978326320648, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.19399261474609375, "std_of_first_answer": 0.19399261474609375, "avg_entropy_of_first_answer": 1.853515625, "lex_sim": 1.0}
{"question": "The White Rose Shopping Centre is in which English city?", "answer": ["BLACKBURN"], "scores": [[0.0024967193603515625, 0.01312255859375, 0.92138671875, 0.465087890625, 0.94384765625]], "normalized_score": [0.10579212009906769], "tokens": [["\u2581B", "L", "ACK", "B", "URN"]], "entropy": [[1.611328125, 4.80078125, 0.404296875, 0.93994140625, 0.254150390625]], "avg_entropy": [1.602099609375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Leeds, UK", "score_of_biggest_cluster": 0.10579212009906769, "score_of_first_answer": 0.10579212009906769, "max_score_of_first_answer": 0.94384765625, "min_score_of_first_answer": 0.0024967193603515625, "avg_score": 0.10579212009906769, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41366622286018784, "std_of_first_answer": 0.41366622286018784, "avg_entropy_of_first_answer": 1.602099609375, "lex_sim": 1.0}
{"question": "Which TV comedy had characters McLaren, Blanco & Warren", "answer": ["Bottom"], "scores": [[0.1895751953125, 0.9111328125]], "normalized_score": [0.4156057834625244], "tokens": [["\u2581B", "ottom"]], "entropy": [[3.685546875, 0.5537109375]], "avg_entropy": [2.11962890625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Porridge", "score_of_biggest_cluster": 0.4156057834625244, "score_of_first_answer": 0.4156057834625244, "max_score_of_first_answer": 0.9111328125, "min_score_of_first_answer": 0.1895751953125, "avg_score": 0.4156057834625244, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36077880859375, "std_of_first_answer": 0.36077880859375, "avg_entropy_of_first_answer": 2.11962890625, "lex_sim": 1.0}
{"question": "What is the capital of the state of Oregon?", "answer": ["Salem"], "scores": [[0.88671875, 1.0]], "normalized_score": [0.9416574239730835], "tokens": [["\u2581S", "alem"]], "entropy": [[0.83251953125, 0.0021877288818359375]], "avg_entropy": [0.41735363006591797], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Salem", "score_of_biggest_cluster": 0.9416574239730835, "score_of_first_answer": 0.9416574239730835, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.88671875, "avg_score": 0.9416574239730835, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.056640625, "std_of_first_answer": 0.056640625, "avg_entropy_of_first_answer": 0.41735363006591797, "lex_sim": 1.0}
{"question": "\"Who wrote the TV series \"\"Pennies From Heaven\"\", and \"\"The Singing Detective\"\"?\"", "answer": ["Dennis Potter"], "scores": [[0.9970703125, 0.9990234375, 0.99951171875]], "normalized_score": [0.998534619808197], "tokens": [["\u2581Dennis", "\u2581Pot", "ter"]], "entropy": [[0.03082275390625, 0.01218414306640625, 0.004634857177734375]], "avg_entropy": [0.015880584716796875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dennis C. G. Potter", "score_of_biggest_cluster": 0.998534619808197, "score_of_first_answer": 0.998534619808197, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.9970703125, "avg_score": 0.998534619808197, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0010548080563814878, "std_of_first_answer": 0.0010548080563814878, "avg_entropy_of_first_answer": 0.015880584716796875, "lex_sim": 1.0}
{"question": "Which of the founders of United Artists had the last name nearest the beginning of the alphabet?", "answer": ["Charles B. Mintz"], "scores": [[0.4130859375, 0.023406982421875, 0.91796875, 0.7421875, 0.74658203125, 0.99755859375]], "normalized_score": [0.41221511363983154], "tokens": [["\u2581Charles", "\u2581B", ".", "\u2581M", "int", "z"]], "entropy": [[2.17578125, 1.076171875, 0.67041015625, 1.8583984375, 0.72119140625, 0.0181732177734375]], "avg_entropy": [1.086687723795573], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chaplin", "score_of_biggest_cluster": 0.41221511363983154, "score_of_first_answer": 0.41221511363983154, "max_score_of_first_answer": 0.99755859375, "min_score_of_first_answer": 0.023406982421875, "avg_score": 0.41221511363983154, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33130892572903115, "std_of_first_answer": 0.33130892572903115, "avg_entropy_of_first_answer": 1.086687723795573, "lex_sim": 1.0}
{"question": "Where was the first penal colony in Australia situated", "answer": ["First FleetArrived A Wizard to Make My Dreams"], "scores": [[0.00429534912109375, 0.2432861328125, 0.99365234375, 0.00045800209045410156, 0.411376953125, 0.391357421875, 6.502866744995117e-05, 0.00012409687042236328, 0.00015366077423095703, 0.0026950836181640625, 0.0673828125, 0.00714111328125, 0.04656982421875, 0.9638671875]], "normalized_score": [0.013318710960447788], "tokens": [["\u2581First", "\u2581Fle", "et", "Ar", "riv", "ed", "\u2581A", "\u2581W", "izard", "\u2581to", "\u2581Make", "\u2581My", "\u2581Dream", "s"]], "entropy": [[2.609375, 1.546875, 0.040252685546875, 3.07421875, 1.361328125, 0.90966796875, 3.095703125, 2.73828125, 4.12890625, 4.46875, 5.44140625, 4.00390625, 4.015625, 0.249755859375]], "avg_entropy": [2.691717965262277], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Botany Bay", "score_of_biggest_cluster": 0.013318710960447788, "score_of_first_answer": 0.013318710960447788, "max_score_of_first_answer": 0.99365234375, "min_score_of_first_answer": 6.502866744995117e-05, "avg_score": 0.013318710960447788, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33847214946643467, "std_of_first_answer": 0.33847214946643467, "avg_entropy_of_first_answer": 2.691717965262277, "lex_sim": 1.0}
{"question": "What building, erected at Sydenham near London in 1834 in 200 acres of grounds and given to the public in 1920, was gutted by fire in 1936?", "answer": ["Penge Poultry Gardens"], "scores": [[0.0014667510986328125, 0.97998046875, 0.005069732666015625, 0.0008454322814941406, 0.99658203125, 0.10992431640625, 0.9990234375]], "normalized_score": [0.04895887151360512], "tokens": [["\u2581Pen", "ge", "\u2581P", "oul", "try", "\u2581Gard", "ens"]], "entropy": [[1.3974609375, 0.1875, 3.130859375, 3.326171875, 0.02447509765625, 3.287109375, 0.01061248779296875]], "avg_entropy": [1.623455592564174], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Crystal Palace", "score_of_biggest_cluster": 0.04895887151360512, "score_of_first_answer": 0.04895887151360512, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.0008454322814941406, "avg_score": 0.04895887151360512, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4776622182033612, "std_of_first_answer": 0.4776622182033612, "avg_entropy_of_first_answer": 1.623455592564174, "lex_sim": 1.0}
{"question": "Which organisation was founded in 1863 after the publication of the book 'A Memory Of Solferino' by the Swiss businessman Henri Dunant?", "answer": ["International Committee of the Red Cross"], "scores": [[0.630859375, 0.95361328125, 0.9931640625, 0.994140625, 0.9970703125, 0.99462890625]], "normalized_score": [0.9155739545822144], "tokens": [["\u2581International", "\u2581Committee", "\u2581of", "\u2581the", "\u2581Red", "\u2581Cross"]], "entropy": [[1.14453125, 0.310791015625, 0.051849365234375, 0.04522705078125, 0.0275115966796875, 0.04608154296875]], "avg_entropy": [0.27099863688151044], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Red Cross", "score_of_biggest_cluster": 0.9155739545822144, "score_of_first_answer": 0.9155739545822144, "max_score_of_first_answer": 0.9970703125, "min_score_of_first_answer": 0.630859375, "avg_score": 0.9155739545822144, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.13340179751899459, "std_of_first_answer": 0.13340179751899459, "avg_entropy_of_first_answer": 0.27099863688151044, "lex_sim": 1.0}
{"question": "Sadler\u2019s Wells Ballet and the Vic-Wells Ballet were the previous names of which British ballet company?", "answer": ["Royal Ballet"], "scores": [[0.09820556640625, 0.94775390625, 0.9990234375]], "normalized_score": [0.4530391991138458], "tokens": [["\u2581Royal", "\u2581Bal", "let"]], "entropy": [[0.8173828125, 0.301513671875, 0.01013946533203125]], "avg_entropy": [0.37634531656901044], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Royal Ballet dancers", "score_of_biggest_cluster": 0.4530391991138458, "score_of_first_answer": 0.4530391991138458, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.09820556640625, "avg_score": 0.4530391991138458, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4130958704233925, "std_of_first_answer": 0.4130958704233925, "avg_entropy_of_first_answer": 0.37634531656901044, "lex_sim": 1.0}
{"question": "\"\"\"Wouldn't it be nice\"\", \"\"God only knows\"\" and \"\"Sloop John B\"\" were songs from which 1966 album?\"", "answer": ["Pet Sounds"], "scores": [[0.8701171875, 0.96142578125, 1.0]], "normalized_score": [0.9422464370727539], "tokens": [["\u2581Pet", "\u2581S", "ounds"]], "entropy": [[0.763671875, 0.29150390625, 0.002170562744140625]], "avg_entropy": [0.3524487813313802], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pet Sounds", "score_of_biggest_cluster": 0.9422464370727539, "score_of_first_answer": 0.9422464370727539, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.8701171875, "avg_score": 0.9422464370727539, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.05446178540722695, "std_of_first_answer": 0.05446178540722695, "avg_entropy_of_first_answer": 0.3524487813313802, "lex_sim": 1.0}
{"question": "Which word completes the name of the rock band 'Half Man Half...'?", "answer": ["Biscuit"], "scores": [[0.89697265625, 0.9970703125, 1.0]], "normalized_score": [0.9634628891944885], "tokens": [["\u2581B", "isc", "uit"]], "entropy": [[0.68701171875, 0.0263214111328125, 0.000156402587890625]], "avg_entropy": [0.23782984415690103], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nuclear launch codes", "score_of_biggest_cluster": 0.9634628891944885, "score_of_first_answer": 0.9634628891944885, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.89697265625, "avg_score": 0.9634628891944885, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.047891958749496755, "std_of_first_answer": 0.047891958749496755, "avg_entropy_of_first_answer": 0.23782984415690103, "lex_sim": 1.0}
{"question": "Artemisinin and derivatives are drugs found to act especially well on which disease?", "answer": ["Malaria"], "scores": [[0.79052734375, 0.93212890625]], "normalized_score": [0.8584132790565491], "tokens": [["\u2581Mal", "aria"]], "entropy": [[0.98486328125, 0.309326171875]], "avg_entropy": [0.6470947265625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Malariologist", "score_of_biggest_cluster": 0.8584132790565491, "score_of_first_answer": 0.8584132790565491, "max_score_of_first_answer": 0.93212890625, "min_score_of_first_answer": 0.79052734375, "avg_score": 0.8584132790565491, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.07080078125, "std_of_first_answer": 0.07080078125, "avg_entropy_of_first_answer": 0.6470947265625, "lex_sim": 1.0}
{"question": "\"Who directed the film \"\"Gladiator\"\", starring Russell Crowe?\"", "answer": ["Ridley Scott Draft legislation on trade mark registration amendment act 2020 and the trade mark rules, 2020, has been notified"], "scores": [[0.97216796875, 0.998046875, 0.99951171875, 0.97900390625, 2.980232238769531e-07, 6.854534149169922e-05, 6.151199340820312e-05, 0.96923828125, 0.004848480224609375, 0.0011348724365234375, 0.034942626953125, 0.0218353271484375, 0.002529144287109375, 0.67236328125, 0.74462890625, 0.096435546875, 0.29248046875, 0.7763671875, 0.9833984375, 0.1722412109375, 0.25732421875, 0.01355743408203125, 0.0628662109375, 0.048431396484375, 0.83642578125, 0.58642578125, 0.11322021484375, 0.9189453125, 0.99853515625, 0.9990234375, 0.98388671875, 0.9326171875, 0.050079345703125, 0.035186767578125, 0.85546875, 0.08526611328125, 0.99365234375]], "normalized_score": [0.08917965739965439], "tokens": [["\u2581R", "id", "ley", "\u2581Scott", "\u2581D", "raft", "\u2581legisl", "ation", "\u2581on", "\u2581trade", "\u2581mark", "\u2581registration", "\u2581am", "end", "ment", "\u2581act", "\u2581", "2", "0", "2", "0", "\u2581and", "\u2581the", "\u2581trade", "\u2581mark", "\u2581rules", ",", "\u2581", "2", "0", "2", "0", ",", "\u2581has", "\u2581been", "\u2581not", "ified"]], "entropy": [[0.296875, 0.0213623046875, 0.0047607421875, 0.1612548828125, 0.6669921875, 3.49609375, 3.4296875, 0.1724853515625, 3.125, 5.3984375, 3.662109375, 4.30859375, 4.171875, 1.009765625, 0.77392578125, 3.587890625, 3.259765625, 0.6787109375, 0.1336669921875, 1.2685546875, 1.6279296875, 3.705078125, 5.65625, 5.19921875, 0.66162109375, 2.07421875, 0.748046875, 0.336181640625, 0.01030731201171875, 0.0078582763671875, 0.10589599609375, 0.273193359375, 3.546875, 5.546875, 1.0537109375, 3.611328125, 0.054412841796875]], "avg_entropy": [1.9958597131677576], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gertrude Bell (film)", "score_of_biggest_cluster": 0.08917965739965439, "score_of_first_answer": 0.08917965739965439, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 2.980232238769531e-07, "avg_score": 0.08917965739965439, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4289407786513341, "std_of_first_answer": 0.4289407786513341, "avg_entropy_of_first_answer": 1.9958597131677576, "lex_sim": 1.0}
{"question": "Which country produces the most of the world's olive oil?", "answer": ["025"], "scores": [[0.0011444091796875, 0.0152130126953125, 0.04217529296875, 0.04852294921875]], "normalized_score": [0.013738851994276047], "tokens": [["\u2581", "0", "2", "5"]], "entropy": [[0.73193359375, 3.177734375, 3.470703125, 3.751953125]], "avg_entropy": [2.7830810546875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Islands of Spain", "score_of_biggest_cluster": 0.013738851994276047, "score_of_first_answer": 0.013738851994276047, "max_score_of_first_answer": 0.04852294921875, "min_score_of_first_answer": 0.0011444091796875, "avg_score": 0.013738851994276047, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.019369748085338277, "std_of_first_answer": 0.019369748085338277, "avg_entropy_of_first_answer": 2.7830810546875, "lex_sim": 1.0}
{"question": "\"Which Scottish writer was nicknamed \"\"The Great Unknown\"\"?\"", "answer": ["Sir Arthur Conan Doyle A New Year, A New Start: Why It\u2019s Time to End the Debate Over Tattoos in the Workplace"], "scores": [[0.15625, 0.5146484375, 0.99560546875, 1.0, 0.990234375, 0.9990234375, 1.6987323760986328e-05, 2.4199485778808594e-05, 0.004749298095703125, 0.029144287109375, 0.084716796875, 0.7783203125, 0.12353515625, 0.056732177734375, 0.002429962158203125, 0.0263214111328125, 0.1285400390625, 0.9931640625, 0.2388916015625, 0.49462890625, 0.00363922119140625, 0.525390625, 0.0112152099609375, 0.9775390625, 0.474365234375, 0.011810302734375, 0.0220947265625, 0.8271484375, 0.5185546875, 0.89794921875, 0.90185546875, 0.994140625]], "normalized_score": [0.09478284418582916], "tokens": [["\u2581Sir", "\u2581Arthur", "\u2581Con", "an", "\u2581Do", "yle", "\u2581A", "\u2581New", "\u2581Year", ",", "\u2581A", "\u2581New", "\u2581Start", ":", "\u2581Why", "\u2581It", "\u2019", "s", "\u2581Time", "\u2581to", "\u2581End", "\u2581the", "\u2581Deb", "ate", "\u2581Over", "\u2581T", "atto", "os", "\u2581in", "\u2581the", "\u2581Work", "place"]], "entropy": [[2.68359375, 0.888671875, 0.036285400390625, 0.0024261474609375, 0.072021484375, 0.00711822509765625, 0.81298828125, 2.623046875, 2.7734375, 3.580078125, 5.01953125, 1.5107421875, 3.92578125, 3.501953125, 4.66796875, 2.548828125, 1.01171875, 0.04534912109375, 2.38671875, 1.0693359375, 3.97265625, 2.98046875, 5.234375, 0.12841796875, 1.529296875, 5.484375, 3.609375, 0.47265625, 2.18359375, 0.5712890625, 0.464111328125, 0.04486083984375]], "avg_entropy": [2.05759596824646], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sir Walter Scotts's", "score_of_biggest_cluster": 0.09478284418582916, "score_of_first_answer": 0.09478284418582916, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.6987323760986328e-05, "avg_score": 0.09478284418582916, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40615901744613964, "std_of_first_answer": 0.40615901744613964, "avg_entropy_of_first_answer": 2.05759596824646, "lex_sim": 1.0}
{"question": "The fictional character \u2018Disco Stu\u2019 appears in which US cartoon tv Series?", "answer": ["Futurama"], "scores": [[0.09490966796875, 0.99609375, 0.99951171875]], "normalized_score": [0.45547667145729065], "tokens": [["\u2581Fut", "ur", "ama"]], "entropy": [[3.21875, 0.031707763671875, 0.00485992431640625]], "avg_entropy": [1.0851058959960938], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Promulent", "score_of_biggest_cluster": 0.45547667145729065, "score_of_first_answer": 0.45547667145729065, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.09490966796875, "avg_score": 0.45547667145729065, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4256301605977928, "std_of_first_answer": 0.4256301605977928, "avg_entropy_of_first_answer": 1.0851058959960938, "lex_sim": 1.0}
{"question": "Vestiphobia is the irrational fear of which items?", "answer": ["Clothing"], "scores": [[0.6572265625, 0.9375]], "normalized_score": [0.7849521636962891], "tokens": [["\u2581Cl", "othing"]], "entropy": [[1.7626953125, 0.23876953125]], "avg_entropy": [1.000732421875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Accouterment", "score_of_biggest_cluster": 0.7849521636962891, "score_of_first_answer": 0.7849521636962891, "max_score_of_first_answer": 0.9375, "min_score_of_first_answer": 0.6572265625, "avg_score": 0.7849521636962891, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.14013671875, "std_of_first_answer": 0.14013671875, "avg_entropy_of_first_answer": 1.000732421875, "lex_sim": 1.0}
{"question": "By tradition, what country leads the procession of nations in the Olympics?", "answer": ["GRE Tags: News, Entertainment, Local"], "scores": [[0.00033283233642578125, 0.04681396484375, 2.980232238769531e-07, 0.89404296875, 7.593631744384766e-05, 0.4287109375, 0.003582000732421875, 0.88427734375, 0.0034465789794921875]], "normalized_score": [0.004846588708460331], "tokens": [["\u2581G", "RE", "\u2581Tags", ":", "\u2581News", ",", "\u2581Entertainment", ",", "\u2581Local"]], "entropy": [[0.28662109375, 3.7734375, 2.552734375, 0.76904296875, 4.28515625, 3.1875, 5.44140625, 0.572265625, 4.77734375]], "avg_entropy": [2.8495008680555554], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yunanistan", "score_of_biggest_cluster": 0.004846588708460331, "score_of_first_answer": 0.004846588708460331, "max_score_of_first_answer": 0.89404296875, "min_score_of_first_answer": 2.980232238769531e-07, "avg_score": 0.004846588708460331, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3650134117273609, "std_of_first_answer": 0.3650134117273609, "avg_entropy_of_first_answer": 2.8495008680555554, "lex_sim": 1.0}
{"question": "\"The song \"\"Diamonds Are a Girl's Best Friend\"\" was famously performed by Marilyn Monroe in which 1953 film?\"", "answer": ["Gentlemen Prefer Blondes"], "scores": [[0.95556640625, 0.9970703125, 0.97900390625, 0.98974609375, 1.0, 0.998046875, 0.99462890625, 1.0]], "normalized_score": [0.989152729511261], "tokens": [["\u2581Gent", "le", "men", "\u2581Pre", "fer", "\u2581Bl", "ond", "es"]], "entropy": [[0.340087890625, 0.030609130859375, 0.10565185546875, 0.09124755859375, 0.0010766983032226562, 0.0176849365234375, 0.03387451171875, 0.0005364418029785156]], "avg_entropy": [0.07759612798690796], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gentlemens prefer blondes", "score_of_biggest_cluster": 0.989152729511261, "score_of_first_answer": 0.989152729511261, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.95556640625, "avg_score": 0.989152729511261, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.014310889540774262, "std_of_first_answer": 0.014310889540774262, "avg_entropy_of_first_answer": 0.07759612798690796, "lex_sim": 1.0}
{"question": "\u2018Lofsongur\u2019 is the national anthem of which country?", "answer": ["Lofsongur is not a national anthem."], "scores": [[0.04241943359375, 0.88232421875, 0.994140625, 0.998046875, 0.78564453125, 0.802734375, 0.5947265625, 0.84326171875, 0.9990234375, 0.9990234375, 0.10687255859375, 0.29150390625, 0.0011510848999023438, 0.12322998046875, 0.018890380859375, 0.140625, 0.08740234375, 0.093505859375, 0.0015764236450195312, 0.99560546875, 0.54541015625, 0.0137939453125, 0.27392578125, 0.9921875, 0.04412841796875, 0.1693115234375, 0.73828125, 0.0009765625, 0.0149383544921875, 0.0123138427734375, 0.8623046875, 0.89990234375]], "normalized_score": [0.14794592559337616], "tokens": [["\u2581L", "of", "song", "ur", "\u2581is", "\u2581not", "\u2581a", "\u2581national", "\u2581anth", "em", ".", "\u2581It", "\u2581seems", "\u2581like", "\u2581the", "\u2581text", "\u2581is", "\u2581not", "\u2581meaning", "ful", ".", "\u2581It", "\u2581could", "\u2581be", "\u2581an", "\u2581error", "\u2581or", "\u2581random", "\u2581letters", "/", "words", "."]], "entropy": [[2.53125, 0.7197265625, 0.05169677734375, 0.01971435546875, 0.97119140625, 0.55810546875, 0.80419921875, 0.66943359375, 0.00739288330078125, 0.0076904296875, 0.923828125, 2.0546875, 0.98974609375, 0.86669921875, 1.091796875, 2.30078125, 1.224609375, 2.04296875, 2.171875, 0.032958984375, 1.3388671875, 1.7216796875, 2.119140625, 0.0655517578125, 0.9736328125, 3.14453125, 0.974609375, 1.3818359375, 1.7734375, 0.66259765625, 0.6875, 0.5947265625]], "avg_entropy": [1.1087019443511963], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "IJsland", "score_of_biggest_cluster": 0.14794592559337616, "score_of_first_answer": 0.14794592559337616, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.0009765625, "avg_score": 0.14794592559337616, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4054637351214753, "std_of_first_answer": 0.4054637351214753, "avg_entropy_of_first_answer": 1.1087019443511963, "lex_sim": 1.0}
{"question": "\"Which American writer chronicled the lives of the Glass family in works such as \"\"Franny and Zooey\"\", and \"\"Raise the Roofbeam, Carpenters\"\"?\"", "answer": ["Hail, hail rock 'n' roll"], "scores": [[0.0002875328063964844, 0.00968170166015625, 0.09747314453125, 0.12164306640625, 0.9931640625, 0.02655029296875, 0.368896484375, 0.9892578125, 0.833984375, 0.990234375]], "normalized_score": [0.11012380570173264], "tokens": [["\u2581H", "ail", ",", "\u2581ha", "il", "\u2581rock", "\u2581'", "n", "'", "\u2581roll"]], "entropy": [[0.95068359375, 5.06640625, 4.2109375, 6.08984375, 0.056060791015625, 2.18359375, 2.927734375, 0.10174560546875, 0.51123046875, 0.09619140625]], "avg_entropy": [2.2194427490234374], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Salingerian", "score_of_biggest_cluster": 0.11012380570173264, "score_of_first_answer": 0.11012380570173264, "max_score_of_first_answer": 0.9931640625, "min_score_of_first_answer": 0.0002875328063964844, "avg_score": 0.11012380570173264, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42880672230277256, "std_of_first_answer": 0.42880672230277256, "avg_entropy_of_first_answer": 2.2194427490234374, "lex_sim": 1.0}
{"question": "What is the secret identity of Penrod \u2018Penry\u2019 Pooch?", "answer": ["Archie #VeteranOfTheDay Army Veteran Robert E. McCallum Jr."], "scores": [[0.00732421875, 0.8837890625, 0.00013267993927001953, 1.3113021850585938e-06, 2.8252601623535156e-05, 0.984375, 0.0059967041015625, 0.3115234375, 0.057098388671875, 0.0002665519714355469, 0.2047119140625, 0.99853515625, 1.0, 0.004192352294921875, 0.031707763671875, 0.78125, 0.01430511474609375, 0.0025768280029296875, 0.26123046875, 0.1285400390625, 0.64990234375]], "normalized_score": [0.02197873592376709], "tokens": [["\u2581Arch", "ie", "\u2581#", "V", "eter", "an", "Of", "The", "Day", "\u2581Army", "\u2581V", "eter", "an", "\u2581Robert", "\u2581E", ".", "\u2581Mc", "Call", "um", "\u2581Jr", "."]], "entropy": [[4.62890625, 0.47021484375, 1.61328125, 1.12109375, 3.14453125, 0.1075439453125, 2.00390625, 3.744140625, 3.4453125, 1.6064453125, 3.341796875, 0.01220703125, 0.0015201568603515625, 4.375, 5.6171875, 1.4736328125, 5.4375, 2.783203125, 2.771484375, 2.833984375, 1.3720703125]], "avg_entropy": [2.4716648828415644], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hong Kong Phooie", "score_of_biggest_cluster": 0.02197873592376709, "score_of_first_answer": 0.02197873592376709, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.3113021850585938e-06, "avg_score": 0.02197873592376709, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38431433479342425, "std_of_first_answer": 0.38431433479342425, "avg_entropy_of_first_answer": 2.4716648828415644, "lex_sim": 1.0}
{"question": "\"Ravel's \"\"Bolero\"\", pop music riffs and (often) suspense music in film soundtracks are examples of what?\"", "answer": ["Bolero is an orchestral piece by Maurice Ravel, and these other examples are examples of the usage of the structural device known as the \"riff\" in music."], "scores": [[0.0667724609375, 0.87353515625, 0.01593017578125, 0.470703125, 0.09674072265625, 0.99755859375, 1.0, 0.919921875, 0.87451171875, 0.693359375, 0.9990234375, 1.0, 0.68505859375, 0.322265625, 0.018280029296875, 0.72314453125, 0.546875, 0.9560546875, 0.525390625, 0.9990234375, 0.3310546875, 0.004787445068359375, 0.99560546875, 0.46826171875, 0.0015115737915039062, 0.97705078125, 0.57275390625, 0.359375, 0.99609375, 0.421875, 0.7421875, 0.37548828125, 0.95458984375, 0.60546875, 0.8388671875, 0.8974609375, 0.8896484375, 0.0055999755859375, 0.049652099609375, 0.9462890625, 0.7060546875, 0.8095703125, 0.98046875, 1.0, 0.98193359375, 0.95556640625, 0.693359375, 0.63720703125, 0.7890625, 0.4775390625]], "normalized_score": [0.3916564881801605], "tokens": [["\u2581Bol", "ero", "\u2581is", "\u2581an", "\u2581orch", "est", "ral", "\u2581piece", "\u2581by", "\u2581Maurice", "\u2581R", "avel", ",", "\u2581and", "\u2581these", "\u2581other", "\u2581examples", "\u2581are", "\u2581examples", "\u2581of", "\u2581the", "\u2581usage", "\u2581of", "\u2581the", "\u2581struct", "ural", "\u2581device", "\u2581known", "\u2581as", "\u2581the", "\u2581\"", "r", "iff", "\"", "\u2581in", "\u2581music", ".", "\u2581In", "\u2581pop", "\u2581music", ",", "\u2581a", "\u2581r", "iff", "\u2581is", "\u2581a", "\u2581short", ",", "\u2581repeated", "\u2581pattern"]], "entropy": [[3.990234375, 0.6728515625, 2.244140625, 1.01953125, 0.77734375, 0.019744873046875, 0.0011472702026367188, 0.49462890625, 0.69580078125, 0.896484375, 0.010009765625, 0.00011819601058959961, 0.9951171875, 2.76953125, 2.52734375, 1.1044921875, 1.0849609375, 0.291748046875, 1.416015625, 0.006961822509765625, 2.763671875, 2.181640625, 0.039154052734375, 1.5634765625, 2.671875, 0.1334228515625, 1.6103515625, 1.333984375, 0.03369140625, 0.99658203125, 1.18359375, 2.185546875, 0.266845703125, 1.263671875, 0.71728515625, 0.479248046875, 0.49462890625, 1.369140625, 1.44140625, 0.23876953125, 0.7900390625, 0.70556640625, 0.10394287109375, 0.0013399124145507812, 0.126953125, 0.2265625, 1.482421875, 1.3359375, 0.76708984375, 1.5]], "avg_entropy": [1.020520933866501], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Riff driven", "score_of_biggest_cluster": 0.3916564881801605, "score_of_first_answer": 0.3916564881801605, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0015115737915039062, "avg_score": 0.3916564881801605, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3366974409019883, "std_of_first_answer": 0.3366974409019883, "avg_entropy_of_first_answer": 1.020520933866501, "lex_sim": 1.0}
{"question": "What is the name used in North America for a man who makes his living by felling trees?", "answer": ["Lumberjack"], "scores": [[0.364990234375, 0.99658203125, 0.99267578125]], "normalized_score": [0.712088406085968], "tokens": [["\u2581L", "umber", "jack"]], "entropy": [[1.76171875, 0.03277587890625, 0.057830810546875]], "avg_entropy": [0.6174418131510416], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Loggers", "score_of_biggest_cluster": 0.712088406085968, "score_of_first_answer": 0.712088406085968, "max_score_of_first_answer": 0.99658203125, "min_score_of_first_answer": 0.364990234375, "avg_score": 0.712088406085968, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2968188003974856, "std_of_first_answer": 0.2968188003974856, "avg_entropy_of_first_answer": 0.6174418131510416, "lex_sim": 1.0}
{"question": "Who hit six sixes in one over at 2007 Cricket World Cup in the West Indies?", "answer": ["Yuvraj Singh #WPLongform, Food for Thought, This is Life"], "scores": [[0.67919921875, 0.99072265625, 0.9970703125, 0.99951171875, 1.6689300537109375e-06, 0.0008440017700195312, 1.0609626770019531e-05, 0.060211181640625, 0.982421875, 0.013519287109375, 4.410743713378906e-06, 0.0655517578125, 0.4521484375, 0.99951171875, 0.82177734375, 0.0224456787109375, 0.8486328125, 0.1312255859375]], "normalized_score": [0.032997529953718185], "tokens": [["\u2581Y", "uv", "raj", "\u2581Singh", "\u2581#", "W", "PL", "ong", "form", ",", "\u2581Food", "\u2581for", "\u2581Th", "ought", ",", "\u2581This", "\u2581is", "\u2581Life"]], "entropy": [[1.3271484375, 0.0657958984375, 0.0261077880859375, 0.006793975830078125, 1.4765625, 2.853515625, 4.05078125, 5.015625, 0.09967041015625, 2.662109375, 3.93359375, 3.396484375, 2.134765625, 0.0034885406494140625, 1.0751953125, 5.00390625, 0.98974609375, 3.353515625]], "avg_entropy": [2.0819336573282876], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Herschelle Gibbs", "score_of_biggest_cluster": 0.032997529953718185, "score_of_first_answer": 0.032997529953718185, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 1.6689300537109375e-06, "avg_score": 0.032997529953718185, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.43541537929451285, "std_of_first_answer": 0.43541537929451285, "avg_entropy_of_first_answer": 2.0819336573282876, "lex_sim": 1.0}
{"question": "Marie Antoinette, Queen of France and Navarre, was born in which country?", "answer": ["Austria-Hungary"], "scores": [[0.8837890625, 0.1705322265625, 0.99658203125, 0.99609375, 0.99951171875]], "normalized_score": [0.6838348507881165], "tokens": [["\u2581Austria", "-", "H", "ung", "ary"]], "entropy": [[0.75341796875, 1.1044921875, 0.0269317626953125, 0.032958984375, 0.0038127899169921875]], "avg_entropy": [0.3843227386474609], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Republic of Austria", "score_of_biggest_cluster": 0.6838348507881165, "score_of_first_answer": 0.6838348507881165, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.1705322265625, "avg_score": 0.6838348507881165, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3224034277165794, "std_of_first_answer": 0.3224034277165794, "avg_entropy_of_first_answer": 0.3843227386474609, "lex_sim": 1.0}
{"question": "Which comedian, whose real name was Charles Springall, reached number one in 1958 with 'Splish, Splash'?", "answer": ["Comic Dickie Henderson"], "scores": [[0.006000518798828125, 0.0546875, 0.01971435546875, 0.99658203125, 0.33447265625, 1.0]], "normalized_score": [0.11366377770900726], "tokens": [["\u2581Com", "ic", "\u2581Dick", "ie", "\u2581Hend", "erson"]], "entropy": [[2.884765625, 0.2607421875, 4.09375, 0.03302001953125, 1.55078125, 0.0013036727905273438]], "avg_entropy": [1.4707271258036296], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "CHARLIE DRAKE", "score_of_biggest_cluster": 0.11366377770900726, "score_of_first_answer": 0.11366377770900726, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.006000518798828125, "avg_score": 0.11366377770900726, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.435751374197777, "std_of_first_answer": 0.435751374197777, "avg_entropy_of_first_answer": 1.4707271258036296, "lex_sim": 1.0}
{"question": "What passes between the sun and earth to make a solar eclipse?", "answer": ["Moon"], "scores": [[0.65771484375]], "normalized_score": [0.65771484375], "tokens": [["\u2581Moon"]], "entropy": [[1.3486328125]], "avg_entropy": [1.3486328125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sol 3a", "score_of_biggest_cluster": 0.65771484375, "score_of_first_answer": 0.65771484375, "max_score_of_first_answer": 0.65771484375, "min_score_of_first_answer": 0.65771484375, "avg_score": 0.65771484375, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 1.3486328125, "lex_sim": 1.0}
{"question": "Which country in 2001 became the first to decriminalize recreational drug use?", "answer": ["In the Netherlands, possession of small amounts of drugs for personal use is technically decriminalized, but not fully legalized #VitalFactz: 22nd Anniversary Of The Fugees' \"The Sc"], "scores": [[0.00460052490234375, 0.07196044921875, 0.261962890625, 0.2783203125, 0.3056640625, 0.269287109375, 0.70654296875, 0.98779296875, 0.98681640625, 0.9013671875, 1.0, 0.5439453125, 0.9990234375, 0.9951171875, 0.47509765625, 0.025482177734375, 1.0, 0.33203125, 1.0, 0.9931640625, 0.61669921875, 0.7119140625, 0.681640625, 0.313720703125, 0.9921875, 0.50830078125, 5.960464477539063e-08, 0.00067901611328125, 0.009033203125, 0.0119476318359375, 0.00479888916015625, 0.09527587890625, 0.0355224609375, 0.68701171875, 0.004085540771484375, 0.062255859375, 0.00022709369659423828, 0.99267578125, 0.99853515625, 0.01010894775390625, 0.446044921875, 0.0008897781372070312, 0.09259033203125, 1.0, 0.79541015625, 0.272705078125, 0.99072265625, 1.0]], "normalized_score": [0.12378738820552826], "tokens": [["\u2581In", "\u2581the", "\u2581Netherlands", ",", "\u2581possession", "\u2581of", "\u2581small", "\u2581amounts", "\u2581of", "\u2581dru", "gs", "\u2581for", "\u2581personal", "\u2581use", "\u2581is", "\u2581techn", "ically", "\u2581dec", "riminal", "ized", ",", "\u2581but", "\u2581not", "\u2581fully", "\u2581legal", "ized", "\u2581#", "V", "ital", "Fact", "z", ":", "\u2581", "2", "2", "nd", "\u2581An", "ni", "versary", "\u2581Of", "\u2581The", "\u2581F", "uge", "es", "'", "\u2581\"", "The", "\u2581Sc"]], "entropy": [[2.263671875, 2.080078125, 3.26171875, 1.5693359375, 2.431640625, 0.7626953125, 1.05859375, 0.07073974609375, 0.08282470703125, 0.5283203125, 5.364418029785156e-05, 1.44921875, 0.00690460205078125, 0.035064697265625, 1.15625, 1.9736328125, 0.00017690658569335938, 1.7763671875, 0.0011606216430664062, 0.041473388671875, 1.171875, 1.0419921875, 1.3974609375, 1.4384765625, 0.060089111328125, 1.02734375, 0.62109375, 5.015625, 2.912109375, 4.50390625, 1.4833984375, 1.275390625, 2.998046875, 1.2529296875, 0.50439453125, 3.583984375, 2.828125, 0.06561279296875, 0.01166534423828125, 1.7744140625, 2.80078125, 2.755859375, 3.474609375, 0.001407623291015625, 1.0634765625, 2.5234375, 0.08306884765625, 0.0001289844512939453]], "avg_entropy": [1.4212630192438762], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Portoga\u0142o", "score_of_biggest_cluster": 0.12378738820552826, "score_of_first_answer": 0.12378738820552826, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.12378738820552826, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40220275056860577, "std_of_first_answer": 0.40220275056860577, "avg_entropy_of_first_answer": 1.4212630192438762, "lex_sim": 1.0}
{"question": "What is the title of Stephen King\u2019s first novel published under his own name?", "answer": ["Carrie For years, I\u2019ve been drawn to quotes from famous authors about the importance of reading."], "scores": [[0.56005859375, 0.99853515625, 2.384185791015625e-07, 0.01139068603515625, 0.470703125, 0.398193359375, 0.06390380859375, 0.418212890625, 0.52734375, 0.0015001296997070312, 0.9921875, 9.131431579589844e-05, 0.343994140625, 0.0445556640625, 0.1783447265625, 0.300537109375, 0.61767578125, 0.09918212890625, 0.99267578125, 0.74658203125, 0.70947265625, 0.10498046875, 0.837890625, 0.001644134521484375, 0.4130859375, 0.9990234375, 0.1368408203125, 0.07171630859375, 0.90283203125, 0.9794921875, 0.61279296875, 0.439208984375, 0.436279296875, 0.84765625]], "normalized_score": [0.1299772709608078], "tokens": [["\u2581Car", "rie", "\u2581For", "\u2581years", ",", "\u2581I", "\u2019", "ve", "\u2581been", "\u2581drawn", "\u2581to", "\u2581quotes", "\u2581from", "\u2581famous", "\u2581authors", "\u2581about", "\u2581the", "\u2581importance", "\u2581of", "\u2581reading", ".", "\u2581Here", "\u2581are", "\u2581my", "\u2581favor", "ites", ".", "\u2581Do", "\u2581you", "\u2581have", "\u2581any", "\u2581to", "\u2581add", "?"]], "entropy": [[1.275390625, 0.01383209228515625, 0.876953125, 3.94140625, 1.833984375, 2.71484375, 2.802734375, 0.69091796875, 2.8125, 4.62890625, 0.068115234375, 2.876953125, 2.123046875, 4.3515625, 2.73828125, 2.455078125, 1.234375, 3.263671875, 0.0555419921875, 1.349609375, 1.2001953125, 3.486328125, 0.52294921875, 1.4111328125, 1.8935546875, 0.0096893310546875, 1.216796875, 4.01953125, 0.441162109375, 0.1314697265625, 0.873046875, 2.076171875, 0.81787109375, 0.48486328125]], "avg_entropy": [1.785072551054113], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Carrie (disambiguation)", "score_of_biggest_cluster": 0.1299772709608078, "score_of_first_answer": 0.1299772709608078, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 2.384185791015625e-07, "avg_score": 0.1299772709608078, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3481931045844002, "std_of_first_answer": 0.3481931045844002, "avg_entropy_of_first_answer": 1.785072551054113, "lex_sim": 1.0}
{"question": "Chi is the Chinese year of what?", "answer": ["Ox"], "scores": [[0.01436614990234375]], "normalized_score": [0.014366152696311474], "tokens": [["\u2581Ox"]], "entropy": [[3.6640625]], "avg_entropy": [3.6640625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Cock (disambiguation)", "score_of_biggest_cluster": 0.014366152696311474, "score_of_first_answer": 0.014366152696311474, "max_score_of_first_answer": 0.01436614990234375, "min_score_of_first_answer": 0.01436614990234375, "avg_score": 0.014366152696311474, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 3.6640625, "lex_sim": 1.0}
{"question": "What nationality was 19th Century author Hans Christian Anderson?", "answer": ["Danish"], "scores": [[0.994140625, 1.0]], "normalized_score": [0.9970660209655762], "tokens": [["\u2581Dan", "ish"]], "entropy": [[0.056671142578125, 0.0015344619750976562]], "avg_entropy": [0.029102802276611328], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Danish", "score_of_biggest_cluster": 0.9970660209655762, "score_of_first_answer": 0.9970660209655762, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.994140625, "avg_score": 0.9970660209655762, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.0029296875, "std_of_first_answer": 0.0029296875, "avg_entropy_of_first_answer": 0.029102802276611328, "lex_sim": 1.0}
{"question": "Limburger cheese is made from the milk of which animal?", "answer": ["Cattle"], "scores": [[0.42578125, 0.30859375]], "normalized_score": [0.36248233914375305], "tokens": [["\u2581C", "attle"]], "entropy": [[2.046875, 0.74560546875]], "avg_entropy": [1.396240234375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\ud83d\udc04", "score_of_biggest_cluster": 0.36248233914375305, "score_of_first_answer": 0.36248233914375305, "max_score_of_first_answer": 0.42578125, "min_score_of_first_answer": 0.30859375, "avg_score": 0.36248233914375305, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.05859375, "std_of_first_answer": 0.05859375, "avg_entropy_of_first_answer": 1.396240234375, "lex_sim": 1.0}
{"question": "The cathedral in which British city is known as \u2018The Ship of the Fens\u2019?", "answer": ["Ely Cathedral"], "scores": [[0.955078125, 1.0, 0.853515625, 0.9990234375]], "normalized_score": [0.9499626159667969], "tokens": [["\u2581E", "ly", "\u2581C", "athedral"]], "entropy": [[0.28125, 0.0017900466918945312, 0.58447265625, 0.010223388671875]], "avg_entropy": [0.21943402290344238], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ely", "score_of_biggest_cluster": 0.9499626159667969, "score_of_first_answer": 0.9499626159667969, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.853515625, "avg_score": 0.9499626159667969, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.05963181619450333, "std_of_first_answer": 0.05963181619450333, "avg_entropy_of_first_answer": 0.21943402290344238, "lex_sim": 1.0}
{"question": "Who was Pope during World War Two?", "answer": ["Pope Pius XII"], "scores": [[0.93115234375, 0.986328125, 0.99853515625, 0.97900390625]], "normalized_score": [0.9734137654304504], "tokens": [["\u2581Pope", "\u2581P", "ius", "\u2581XII"]], "entropy": [[0.281982421875, 0.11328125, 0.01288604736328125, 0.135986328125]], "avg_entropy": [0.1360340118408203], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pope Puis XII", "score_of_biggest_cluster": 0.9734137654304504, "score_of_first_answer": 0.9734137654304504, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.93115234375, "avg_score": 0.9734137654304504, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02556695598014193, "std_of_first_answer": 0.02556695598014193, "avg_entropy_of_first_answer": 0.1360340118408203, "lex_sim": 1.0}
{"question": "What is the name of the villain Azal\u2019s gargoyle servant in the BBC science fiction series Doctor Who?", "answer": ["Abzorbaloff"], "scores": [[0.0011262893676757812, 0.0982666015625, 0.9892578125, 0.86572265625]], "normalized_score": [0.09867019951343536], "tokens": [["\u2581Ab", "zor", "bal", "off"]], "entropy": [[3.75390625, 2.587890625, 0.07269287109375, 0.75244140625]], "avg_entropy": [1.7917327880859375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bok (crater)", "score_of_biggest_cluster": 0.09867019951343536, "score_of_first_answer": 0.09867019951343536, "max_score_of_first_answer": 0.9892578125, "min_score_of_first_answer": 0.0011262893676757812, "avg_score": 0.09867019951343536, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44239985403205356, "std_of_first_answer": 0.44239985403205356, "avg_entropy_of_first_answer": 1.7917327880859375, "lex_sim": 1.0}
{"question": "Which metal is created by the 'Bessemer Process'?", "answer": ["Iron #4 Re: Re: Re: Re: The Riddle of Innovation"], "scores": [[0.72216796875, 4.76837158203125e-07, 0.11846923828125, 4.309415817260742e-05, 0.000606536865234375, 0.004741668701171875, 0.60546875, 0.65087890625, 0.9912109375, 0.6943359375, 0.99267578125, 0.0014085769653320312, 0.0013599395751953125, 0.453369140625, 0.292236328125, 0.00012218952178955078, 0.00560760498046875, 0.8291015625]], "normalized_score": [0.019681019708514214], "tokens": [["\u2581Iron", "\u2581#", "4", "\u2581Re", ":", "\u2581Re", ":", "\u2581Re", ":", "\u2581Re", ":", "\u2581The", "\u2581R", "iddle", "\u2581of", "\u2581In", "nov", "ation"]], "entropy": [[1.3701171875, 0.8935546875, 4.421875, 2.552734375, 2.69140625, 6.5078125, 2.65234375, 2.583984375, 0.08721923828125, 2.517578125, 0.07281494140625, 2.060546875, 5.64453125, 2.849609375, 2.638671875, 1.26953125, 4.55078125, 0.93115234375]], "avg_entropy": [2.5720147026909723], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Steel", "score_of_biggest_cluster": 0.019681019708514214, "score_of_first_answer": 0.019681019708514214, "max_score_of_first_answer": 0.99267578125, "min_score_of_first_answer": 4.76837158203125e-07, "avg_score": 0.019681019708514214, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3730004318118813, "std_of_first_answer": 0.3730004318118813, "avg_entropy_of_first_answer": 2.5720147026909723, "lex_sim": 1.0}
{"question": "Which journalist and author won the fifth series of I'm A Celebrity, Get Me Out Of Here in 2005, beating Cannon and Ball, Jimmy Osmond and Sheree Murphy amongst others?", "answer": ["Carol Thatcher #KenyanChicks: Twitter Users Decry Sexism In Hashtag Trend"], "scores": [[0.0277252197265625, 0.89453125, 0.99951171875, 1.0, 5.960464477539063e-08, 0.001972198486328125, 0.0228729248046875, 0.002651214599609375, 0.004852294921875, 0.00020945072174072266, 0.00466156005859375, 0.0007367134094238281, 0.003276824951171875, 0.0016946792602539062, 0.896484375, 0.010345458984375, 0.275634765625, 0.06341552734375, 0.0048980712890625, 1.0, 0.990234375, 0.1981201171875, 0.9951171875]], "normalized_score": [0.01960618421435356], "tokens": [["\u2581Carol", "\u2581Th", "atch", "er", "\u2581#", "K", "en", "yan", "Ch", "icks", ":", "\u2581Twitter", "\u2581Users", "\u2581Dec", "ry", "\u2581Sex", "ism", "\u2581In", "\u2581Has", "ht", "ag", "\u2581Tr", "end"]], "entropy": [[3.1484375, 0.411865234375, 0.0034122467041015625, 0.0003731250762939453, 0.73291015625, 5.875, 4.875, 4.1796875, 4.58984375, 3.353515625, 2.94140625, 6.08984375, 5.58203125, 4.14453125, 0.46826171875, 4.30859375, 1.068359375, 1.93359375, 5.0546875, 0.002490997314453125, 0.056640625, 3.51171875, 0.0377197265625]], "avg_entropy": [2.7117358186970586], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Carole Thatcher", "score_of_biggest_cluster": 0.01960618421435356, "score_of_first_answer": 0.01960618421435356, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.01960618421435356, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4331013499460827, "std_of_first_answer": 0.4331013499460827, "avg_entropy_of_first_answer": 2.7117358186970586, "lex_sim": 1.0}
{"question": "How did Alice get into Wonderland?", "answer": ["Through a rabbit hole"], "scores": [[0.35595703125, 0.9833984375, 0.837890625, 0.99951171875, 0.95751953125]], "normalized_score": [0.775621771812439], "tokens": [["\u2581Through", "\u2581a", "\u2581rabb", "it", "\u2581hole"]], "entropy": [[1.6669921875, 0.10687255859375, 0.83349609375, 0.003265380859375, 0.1976318359375]], "avg_entropy": [0.561651611328125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Down a rabbit hole", "score_of_biggest_cluster": 0.775621771812439, "score_of_first_answer": 0.775621771812439, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.35595703125, "avg_score": 0.775621771812439, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.24218013629178312, "std_of_first_answer": 0.24218013629178312, "avg_entropy_of_first_answer": 0.561651611328125, "lex_sim": 1.0}
{"question": "Elvis Presley\u2019s manager, Andreas Cornelis van Kujik, was better known by what name?", "answer": ["Col. Tom Parker"], "scores": [[0.359375, 0.8125, 0.9814453125, 0.99951171875]], "normalized_score": [0.7315707206726074], "tokens": [["\u2581Col", ".", "\u2581Tom", "\u2581Parker"]], "entropy": [[1.107421875, 0.677734375, 0.10992431640625, 0.004791259765625]], "avg_entropy": [0.47496795654296875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Colonel Parker", "score_of_biggest_cluster": 0.7315707206726074, "score_of_first_answer": 0.7315707206726074, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.359375, "avg_score": 0.7315707206726074, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.25810742126719327, "std_of_first_answer": 0.25810742126719327, "avg_entropy_of_first_answer": 0.47496795654296875, "lex_sim": 1.0}
{"question": "Who was the British monarch when Theodore Roosevelt was the US President?", "answer": ["Queen Victoria was the British monarch during the presidency of Theodore Roosevelt, though she had passed away in 1901 and was succeeded by her son, King Edward VII."], "scores": [[0.09686279296875, 0.96533203125, 0.03515625, 0.779296875, 0.6826171875, 0.94775390625, 0.0210113525390625, 0.425537109375, 0.69482421875, 0.9951171875, 0.99951171875, 0.9970703125, 0.99951171875, 0.99951171875, 1.0, 1.0, 1.0, 0.037872314453125, 0.0030803680419921875, 0.47412109375, 0.638671875, 0.58984375, 0.99951171875, 0.958984375, 0.890625, 1.0, 1.0, 1.0, 1.0, 0.73486328125, 0.5087890625, 0.998046875, 0.99951171875, 0.962890625, 0.83740234375, 0.2001953125, 0.80810546875, 1.0, 0.99951171875, 0.97412109375]], "normalized_score": [0.5288798809051514], "tokens": [["\u2581Queen", "\u2581Victoria", "\u2581was", "\u2581the", "\u2581British", "\u2581monarch", "\u2581during", "\u2581the", "\u2581presiden", "cy", "\u2581of", "\u2581The", "odore", "\u2581Ro", "ose", "vel", "t", ",", "\u2581though", "\u2581she", "\u2581had", "\u2581passed", "\u2581away", "\u2581in", "\u2581", "1", "9", "0", "1", "\u2581and", "\u2581was", "\u2581succeeded", "\u2581by", "\u2581her", "\u2581son", ",", "\u2581King", "\u2581Edward", "\u2581VII", "."]], "entropy": [[1.302734375, 0.2037353515625, 1.08984375, 0.65625, 0.865234375, 0.2266845703125, 0.591796875, 0.9794921875, 1.2587890625, 0.031829833984375, 0.0028362274169921875, 0.027374267578125, 0.0028095245361328125, 0.004817962646484375, 0.0011892318725585938, 0.0015039443969726562, 2.3663043975830078e-05, 0.63525390625, 1.630859375, 1.84765625, 0.9345703125, 0.71142578125, 0.0030231475830078125, 0.24853515625, 0.35205078125, 2.6047229766845703e-05, 0.00013685226440429688, 0.00017750263214111328, 0.0022182464599609375, 0.80615234375, 1.0849609375, 0.018218994140625, 0.0031528472900390625, 0.1741943359375, 0.44921875, 0.97802734375, 0.491455078125, 0.00164794921875, 0.002857208251953125, 0.1444091796875]], "avg_entropy": [0.4441793382167816], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Eduard VII", "score_of_biggest_cluster": 0.5288798809051514, "score_of_first_answer": 0.5288798809051514, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0030803680419921875, "avg_score": 0.5288798809051514, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.33289082205608744, "std_of_first_answer": 0.33289082205608744, "avg_entropy_of_first_answer": 0.4441793382167816, "lex_sim": 1.0}
{"question": "According to the Quran, a man is allowed to have as many as four what?", "answer": ["Wives"], "scores": [[0.71044921875, 0.98681640625]], "normalized_score": [0.8373069763183594], "tokens": [["\u2581W", "ives"]], "entropy": [[1.1611328125, 0.079833984375]], "avg_entropy": [0.6204833984375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Wifers", "score_of_biggest_cluster": 0.8373069763183594, "score_of_first_answer": 0.8373069763183594, "max_score_of_first_answer": 0.98681640625, "min_score_of_first_answer": 0.71044921875, "avg_score": 0.8373069763183594, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.13818359375, "std_of_first_answer": 0.13818359375, "avg_entropy_of_first_answer": 0.6204833984375, "lex_sim": 1.0}
{"question": "Who plays Mrs. Hudson in the BBC tv series \u2018Sherlock\u2019?", "answer": ["Ustinov"], "scores": [[0.365966796875, 0.04791259765625, 0.984375, 0.98583984375]], "normalized_score": [0.3611724078655243], "tokens": [["\u2581U", "st", "in", "ov"]], "entropy": [[0.87353515625, 3.75390625, 0.136474609375, 0.0838623046875]], "avg_entropy": [1.211944580078125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Una Stubbs", "score_of_biggest_cluster": 0.3611724078655243, "score_of_first_answer": 0.3611724078655243, "max_score_of_first_answer": 0.98583984375, "min_score_of_first_answer": 0.04791259765625, "avg_score": 0.3611724078655243, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4050078139146448, "std_of_first_answer": 0.4050078139146448, "avg_entropy_of_first_answer": 1.211944580078125, "lex_sim": 1.0}
{"question": "What colour is the number 10 on the door of 10 Downing Street? White, Yellow or Brown?", "answer": ["White."], "scores": [[0.5732421875, 0.07421875, 4.947185516357422e-05, 0.8759765625, 4.649162292480469e-06, 9.310245513916016e-05, 0.0015974044799804688, 0.0091552734375, 0.472900390625, 0.0440673828125, 0.006389617919921875, 0.1185302734375, 0.6552734375, 0.0113067626953125, 0.005382537841796875, 0.97119140625, 0.008514404296875, 0.18115234375, 0.998046875]], "normalized_score": [0.020793728530406952], "tokens": [["\u2581White", ".", "\u2581#", "1", "\u2581The", "\u2581L", "one", "\u2581F", "ighter", ":", "\u2581I", "'", "m", "\u2581Not", "\u2581Do", "ing", "\u2581this", "\u2581Al", "one"]], "entropy": [[1.7236328125, 1.0439453125, 0.67431640625, 0.66259765625, 0.0643310546875, 2.1640625, 5.04296875, 2.96875, 2.896484375, 4.5234375, 5.90625, 5.03515625, 1.068359375, 4.96484375, 4.9921875, 0.183349609375, 2.416015625, 2.84765625, 0.018585205078125]], "avg_entropy": [2.5893121016652962], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "White (Colour)", "score_of_biggest_cluster": 0.020793728530406952, "score_of_first_answer": 0.020793728530406952, "max_score_of_first_answer": 0.998046875, "min_score_of_first_answer": 4.649162292480469e-06, "avg_score": 0.020793728530406952, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.35713901131405956, "std_of_first_answer": 0.35713901131405956, "avg_entropy_of_first_answer": 2.5893121016652962, "lex_sim": 1.0}
{"question": "What is the name of the alternative medicine that uses essential oils?", "answer": ["Aromatherapy #Humor #Science #Bacteria #Fungus"], "scores": [[0.91845703125, 0.9990234375, 0.91748046875, 0.94189453125, 5.960464477539063e-08, 0.001155853271484375, 0.0008649826049804688, 0.5869140625, 0.046295166015625, 0.40478515625, 0.0179901123046875, 0.609375, 0.002033233642578125, 0.000202178955078125, 0.8134765625, 0.5595703125, 0.031036376953125, 0.41015625, 0.0635986328125]], "normalized_score": [0.03763417899608612], "tokens": [["\u2581A", "rom", "ather", "apy", "\u2581#", "H", "um", "or", "\u2581#", "S", "cience", "\u2581#", "B", "acter", "ia", "\u2581#", "F", "ung", "us"]], "entropy": [[0.452880859375, 0.00852203369140625, 0.449951171875, 0.31640625, 0.7998046875, 5.73046875, 2.431640625, 1.6728515625, 2.052734375, 3.275390625, 0.57568359375, 1.4296875, 3.951171875, 0.931640625, 0.64697265625, 1.658203125, 5.20703125, 3.02734375, 0.443359375]], "avg_entropy": [1.8453549836811267], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Reed diffuser", "score_of_biggest_cluster": 0.03763417899608612, "score_of_first_answer": 0.03763417899608612, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.03763417899608612, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38232863239234105, "std_of_first_answer": 0.38232863239234105, "avg_entropy_of_first_answer": 1.8453549836811267, "lex_sim": 1.0}
{"question": "\"Who, in 1791, wrote \"\"The Life of Samuel Johnson\"\"?\"", "answer": ["James Boswell ### Introduction"], "scores": [[0.98974609375, 0.9951171875, 0.99755859375, 0.0, 3.3974647521972656e-05]], "normalized_score": [0.0], "tokens": [["\u2581James", "\u2581Bos", "well", "\u2581###", "\u2581Introduction"]], "entropy": [[0.100830078125, 0.048553466796875, 0.02093505859375, 0.79150390625, 2.5]], "avg_entropy": [0.692364501953125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bozzy", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.99755859375, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.48702569070855056, "std_of_first_answer": 0.48702569070855056, "avg_entropy_of_first_answer": 0.692364501953125, "lex_sim": 1.0}
{"question": "English playwright Chrisopher Marlowe coined the phrase 'The face that launched a thousand ships' in his tragedy Doctor Faustus when referring to which mythical Greek figure?", "answer": ["Helen of Troy #1 NEW YORK TIMES BESTSELLER \u2022 The compelling, inspiring, and comically sublime story of one man\u2019s coming-of-age, set during the twil"], "scores": [[0.998046875, 0.93310546875, 0.97265625, 0.99853515625, 1.7881393432617188e-07, 0.05230712890625, 9.834766387939453e-06, 0.01446533203125, 0.982421875, 0.99951171875, 0.387939453125, 1.0, 0.9990234375, 0.9912109375, 1.0, 0.94970703125, 0.99853515625, 0.908203125, 0.04376220703125, 0.0870361328125, 0.0156402587890625, 0.98291015625, 0.73291015625, 0.99560546875, 0.99951171875, 1.0, 0.99951171875, 0.89697265625, 0.97021484375, 0.52978515625, 0.99951171875, 0.98779296875, 0.99853515625, 0.232421875, 0.80126953125, 0.56005859375, 1.0, 0.9140625, 0.982421875, 1.0, 0.9990234375, 0.99658203125, 0.98779296875, 0.99169921875, 1.0, 0.99755859375, 0.99169921875, 0.98046875]], "normalized_score": [0.3621641993522644], "tokens": [["\u2581Helen", "\u2581of", "\u2581T", "roy", "\u2581#", "1", "\u2581NEW", "\u2581Y", "OR", "K", "\u2581T", "IME", "S", "\u2581BE", "ST", "SE", "LL", "ER", "\u2581\u2022", "\u2581The", "\u2581comp", "elling", ",", "\u2581insp", "iring", ",", "\u2581and", "\u2581com", "ically", "\u2581sub", "lime", "\u2581story", "\u2581of", "\u2581one", "\u2581man", "\u2019", "s", "\u2581coming", "-", "of", "-", "age", ",", "\u2581set", "\u2581during", "\u2581the", "\u2581tw", "il"]], "entropy": [[0.0145416259765625, 0.325439453125, 0.1734619140625, 0.01428985595703125, 0.72900390625, 5.48046875, 3.1015625, 4.2578125, 0.12042236328125, 0.005283355712890625, 2.662109375, 0.0013790130615234375, 0.01103973388671875, 0.08251953125, 1.7940998077392578e-05, 0.2100830078125, 0.01314544677734375, 0.38525390625, 3.349609375, 3.052734375, 3.4375, 0.102294921875, 1.0712890625, 0.03973388671875, 0.005809783935546875, 0.0007791519165039062, 0.005397796630859375, 0.46875, 0.1629638671875, 2.33984375, 0.003238677978515625, 0.07958984375, 0.0123291015625, 3.626953125, 0.54345703125, 1.05859375, 0.00146484375, 0.5185546875, 0.0953369140625, 0.0008559226989746094, 0.01220703125, 0.028228759765625, 0.09375, 0.0806884765625, 0.0015048980712890625, 0.0202789306640625, 0.086669921875, 0.09954833984375]], "avg_entropy": [0.7914123001197974], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Helena Troy", "score_of_biggest_cluster": 0.3621641993522644, "score_of_first_answer": 0.3621641993522644, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.7881393432617188e-07, "avg_score": 0.3621641993522644, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.35268446687728144, "std_of_first_answer": 0.35268446687728144, "avg_entropy_of_first_answer": 0.7914123001197974, "lex_sim": 1.0}
{"question": "What was the name of the Prime Minister of Egypt who was assassinated in 1910?", "answer": ["Boutros Boutros-Ghali #IStandWithAhmed: Activists Rally Around Muslim Teen Arrested for Clock"], "scores": [[0.69091796875, 0.99853515625, 0.96826171875, 0.0031681060791015625, 0.98681640625, 0.99951171875, 0.98974609375, 0.99609375, 1.0, 1.0, 1.3113021850585938e-06, 0.0009274482727050781, 0.00012695789337158203, 0.92333984375, 0.006069183349609375, 0.98193359375, 0.01218414306640625, 0.0003452301025390625, 0.1328125, 0.00531768798828125, 0.95654296875, 0.29052734375, 0.99951171875, 0.007450103759765625, 0.10302734375, 0.9912109375, 0.50244140625, 0.99853515625, 0.521484375, 0.2022705078125, 1.0]], "normalized_score": [0.09826312214136124], "tokens": [["\u2581B", "out", "ros", "\u2581B", "out", "ros", "-", "G", "h", "ali", "\u2581#", "I", "Stand", "With", "Ah", "med", ":", "\u2581Activ", "ists", "\u2581R", "ally", "\u2581Ar", "ound", "\u2581Muslim", "\u2581Te", "en", "\u2581Arr", "ested", "\u2581for", "\u2581C", "lock"]], "entropy": [[1.8798828125, 0.0178985595703125, 0.1549072265625, 1.0302734375, 0.11260986328125, 0.006855010986328125, 0.0716552734375, 0.033294677734375, 0.001750946044921875, 0.002307891845703125, 0.83984375, 4.84765625, 4.54296875, 0.430908203125, 3.3359375, 0.1513671875, 2.056640625, 5.375, 1.0927734375, 5.19140625, 0.26220703125, 2.30859375, 0.004222869873046875, 3.099609375, 3.931640625, 0.059112548828125, 2.548828125, 0.01160430908203125, 1.7119140625, 1.9296875, 0.002109527587890625]], "avg_entropy": [1.5175957218293221], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Boutros Ghali", "score_of_biggest_cluster": 0.09826312214136124, "score_of_first_answer": 0.09826312214136124, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.3113021850585938e-06, "avg_score": 0.09826312214136124, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4445039738941195, "std_of_first_answer": 0.4445039738941195, "avg_entropy_of_first_answer": 1.5175957218293221, "lex_sim": 1.0}
{"question": "Who was the narrator in the novel `Moby Dick'?", "answer": ["Capt."], "scores": [[0.00017333030700683594, 0.720703125, 0.8701171875, 1.0]], "normalized_score": [0.10210619866847992], "tokens": [["\u2581Capt", ".", "\u2581A", "hab"]], "entropy": [[0.64208984375, 1.04296875, 0.75, 0.0003180503845214844]], "avg_entropy": [0.6088441610336304], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Kedemah", "score_of_biggest_cluster": 0.10210619866847992, "score_of_first_answer": 0.10210619866847992, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.00017333030700683594, "avg_score": 0.10210619866847992, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38671854433957853, "std_of_first_answer": 0.38671854433957853, "avg_entropy_of_first_answer": 0.6088441610336304, "lex_sim": 1.0}
{"question": "\u201cAnd Another Thing...\u201d by Eoin Colfer, published in 2009, eight years after the death of the author of the first five, is described as the sixth in which trilogy of books?", "answer": ["Hitchhiker's Guide to the Galaxy"], "scores": [[0.483154296875, 0.98876953125, 0.98046875, 0.94287109375, 0.94677734375, 1.0, 0.91650390625, 0.9130859375, 0.9970703125, 0.9951171875, 1.0]], "normalized_score": [0.9083073139190674], "tokens": [["\u2581H", "itch", "h", "iker", "'", "s", "\u2581Guide", "\u2581to", "\u2581the", "\u2581Gal", "axy"]], "entropy": [[1.3251953125, 0.07464599609375, 0.124755859375, 0.2293701171875, 0.298095703125, 0.0026493072509765625, 0.40771484375, 0.4208984375, 0.023345947265625, 0.037445068359375, 0.0009775161743164062]], "avg_entropy": [0.26773582805286755], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hitchhiker's Guide to the Galaxy", "score_of_biggest_cluster": 0.9083073139190674, "score_of_first_answer": 0.9083073139190674, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.483154296875, "avg_score": 0.9083073139190674, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.14293288366014115, "std_of_first_answer": 0.14293288366014115, "avg_entropy_of_first_answer": 0.26773582805286755, "lex_sim": 1.0}
{"question": "In April 1963, Lester Bowles Pearson became Prime Minister of which country?", "answer": ["ISO 3166-1:AF"], "scores": [[9.5367431640625e-07, 0.289794921875, 0.090087890625, 0.9873046875, 0.99462890625, 0.9990234375, 0.98779296875, 0.96240234375, 0.0220489501953125, 9.709596633911133e-05]], "normalized_score": [0.04673536494374275], "tokens": [["\u2581ISO", "\u2581", "3", "1", "6", "6", "-", "1", ":", "AF"]], "entropy": [[1.11328125, 3.83203125, 1.4013671875, 0.1080322265625, 0.039093017578125, 0.00897979736328125, 0.1107177734375, 0.2215576171875, 1.7919921875, 2.693359375]], "avg_entropy": [1.1320411682128906], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Canada", "score_of_biggest_cluster": 0.04673536494374275, "score_of_first_answer": 0.04673536494374275, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 9.5367431640625e-07, "avg_score": 0.04673536494374275, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4596039768719332, "std_of_first_answer": 0.4596039768719332, "avg_entropy_of_first_answer": 1.1320411682128906, "lex_sim": 1.0}
{"question": "In what year did Marlene Dietrich become a US citizen?", "answer": ["0x00000000 #AskAlan: Election Day and the Rise of Independent Voters"], "scores": [[0.174072265625, 0.006439208984375, 0.0005555152893066406, 0.189208984375, 0.42578125, 0.52978515625, 0.439697265625, 0.41552734375, 0.63818359375, 0.65625, 0.517578125, 3.159046173095703e-06, 0.00334930419921875, 0.0099945068359375, 0.0002567768096923828, 0.246337890625, 0.0693359375, 0.0014944076538085938, 0.0006604194641113281, 0.2279052734375, 0.0008034706115722656, 0.223388671875, 0.00189208984375, 0.68896484375, 0.953125, 0.0015277862548828125, 0.15625, 0.748046875, 0.9990234375]], "normalized_score": [0.037480324506759644], "tokens": [["\u2581", "0", "x", "0", "0", "0", "0", "0", "0", "0", "0", "\u2581#", "A", "sk", "Al", "an", ":", "\u2581E", "lection", "\u2581Day", "\u2581and", "\u2581the", "\u2581R", "ise", "\u2581of", "\u2581Independent", "\u2581V", "ot", "ers"]], "entropy": [[1.7265625, 0.10052490234375, 2.0703125, 2.849609375, 2.72265625, 2.244140625, 1.91796875, 2.6953125, 1.671875, 1.5673828125, 1.466796875, 2.57421875, 4.4453125, 3.515625, 4.12109375, 3.3515625, 4.55859375, 4.66796875, 4.45703125, 3.44921875, 3.0703125, 5.83203125, 6.2265625, 2.021484375, 0.252197265625, 4.64453125, 1.9541015625, 0.73583984375, 0.01152801513671875]], "avg_entropy": [2.7904260569605333], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and forty-one", "score_of_biggest_cluster": 0.037480324506759644, "score_of_first_answer": 0.037480324506759644, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 3.159046173095703e-06, "avg_score": 0.037480324506759644, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3075172847499467, "std_of_first_answer": 0.3075172847499467, "avg_entropy_of_first_answer": 2.7904260569605333, "lex_sim": 1.0}
{"question": "Which play is featured in the film The Producers?", "answer": ["Springtime for Hitler #BringBackOurGirls: Nigerian Protesters Demand Action for Missing Schoolgirls"], "scores": [[0.55517578125, 0.94677734375, 0.98291015625, 0.9931640625, 4.172325134277344e-07, 0.0012712478637695312, 0.0014047622680664062, 0.8798828125, 0.5390625, 0.99755859375, 0.9853515625, 0.9970703125, 0.000988006591796875, 0.005283355712890625, 0.57177734375, 0.0001424551010131836, 0.4443359375, 0.282958984375, 0.998046875, 0.49658203125, 0.0101470947265625, 0.115234375, 1.0, 0.3544921875, 0.998046875, 0.99951171875]], "normalized_score": [0.09390045702457428], "tokens": [["\u2581Spring", "time", "\u2581for", "\u2581Hitler", "\u2581#", "B", "ring", "Back", "O", "ur", "G", "irls", ":", "\u2581Niger", "ian", "\u2581Protest", "ers", "\u2581Dem", "and", "\u2581Action", "\u2581for", "\u2581Miss", "ing", "\u2581School", "g", "irls"]], "entropy": [[1.54296875, 0.366943359375, 0.1070556640625, 0.06817626953125, 0.884765625, 5.23046875, 4.79296875, 0.6142578125, 1.904296875, 0.025146484375, 0.1309814453125, 0.0254058837890625, 1.255859375, 4.73828125, 0.70703125, 3.03125, 2.404296875, 2.9921875, 0.018585205078125, 1.4501953125, 3.15625, 0.919921875, 0.0007958412170410156, 0.8046875, 0.01404571533203125, 0.00696563720703125]], "avg_entropy": [1.4305303463569055], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Springtime for Hitler", "score_of_biggest_cluster": 0.09390045702457428, "score_of_first_answer": 0.09390045702457428, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 4.172325134277344e-07, "avg_score": 0.09390045702457428, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4139262795094611, "std_of_first_answer": 0.4139262795094611, "avg_entropy_of_first_answer": 1.4305303463569055, "lex_sim": 1.0}
{"question": "Who released a 2011 album entitled \u2018Born This Way\u2019?", "answer": ["Lady Gaga"], "scores": [[0.9921875, 0.98046875, 0.99951171875]], "normalized_score": [0.9906915426254272], "tokens": [["\u2581Lady", "\u2581G", "aga"]], "entropy": [[0.0853271484375, 0.11456298828125, 0.0033092498779296875]], "avg_entropy": [0.06773312886555989], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "@ladygaga", "score_of_biggest_cluster": 0.9906915426254272, "score_of_first_answer": 0.9906915426254272, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.98046875, "avg_score": 0.9906915426254272, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.00784295820518018, "std_of_first_answer": 0.00784295820518018, "avg_entropy_of_first_answer": 0.06773312886555989, "lex_sim": 1.0}
{"question": "Comprising around two-thirds of the Earth's mass , what is found between the core of the Earth and its crust ?", "answer": ["Mantle layers."], "scores": [[0.496826171875, 0.99462890625, 2.4557113647460938e-05, 0.0175628662109375, 0.0005769729614257812, 3.933906555175781e-06, 4.965066909790039e-05, 0.330810546875, 0.57666015625, 0.95947265625, 1.0, 0.0002263784408569336, 0.0245513916015625, 0.00107574462890625, 0.0014295578002929688, 0.94091796875, 0.0029048919677734375, 0.87255859375, 1.0, 1.0, 0.032684326171875, 0.260986328125, 0.9873046875, 0.0031452178955078125, 0.81494140625, 1.0, 1.0, 0.10784912109375, 0.9931640625, 0.270751953125, 0.001888275146484375, 0.900390625, 0.998046875, 0.9521484375, 0.1561279296875, 0.97265625, 1.0, 1.0, 0.0043182373046875, 0.09710693359375, 0.96484375, 1.0, 1.0, 0.0214385986328125, 0.1746826171875]], "normalized_score": [0.07870146632194519], "tokens": [["\u2581Mant", "le", "\u2581layers", ".", "\u2581", "<0xE8>", "<0xB6>", "<0xA8>", "<0xE5>", "<0x8B>", "<0xA2>", "\u751f", "\u6d3b", "-", "\u65e5", "\u5e38", "\u5b78", "<0xE7>", "<0xBF>", "<0x92>", "<0xE8>", "<0x88>", "<0x87>", "\u5b78", "<0xE7>", "<0xBF>", "<0x92>", "\u751f", "\u6d3b", "-", "\u97f3", "<0xE6>", "<0xA8>", "<0x82>", "\u5b78", "<0xE7>", "<0xBF>", "<0x92>", "+", "\u97f3", "<0xE6>", "<0xA8>", "<0x82>", "\u6559", "\u5b78"]], "entropy": [[1.818359375, 0.050872802734375, 0.92333984375, 0.943359375, 0.7890625, 3.93359375, 2.826171875, 2.09375, 1.4423828125, 0.34130859375, 0.0014247894287109375, 2.580078125, 1.216796875, 2.810546875, 6.09765625, 0.39697265625, 2.482421875, 0.79541015625, 1.1324882507324219e-05, 2.0205974578857422e-05, 3.76171875, 2.201171875, 0.07452392578125, 3.85546875, 1.154296875, 0.0017957687377929688, 4.45246696472168e-05, 3.966796875, 0.059661865234375, 2.771484375, 4.37109375, 0.62451171875, 0.0163116455078125, 0.1934814453125, 3.923828125, 0.200927734375, 5.3048133850097656e-05, 1.4841556549072266e-05, 2.943359375, 5.09765625, 0.27490234375, 0.002040863037109375, 1.0788440704345703e-05, 4.05078125, 0.5810546875]], "avg_entropy": [1.5926784780290393], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "THE MANTLE", "score_of_biggest_cluster": 0.07870146632194519, "score_of_first_answer": 0.07870146632194519, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 3.933906555175781e-06, "avg_score": 0.07870146632194519, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.44701261164171235, "std_of_first_answer": 0.44701261164171235, "avg_entropy_of_first_answer": 1.5926784780290393, "lex_sim": 1.0}
{"question": "Which early aviator flew in a plane christened Jason?", "answer": ["Lt. Frank T. Coffyn"], "scores": [[0.05078125, 0.71533203125, 0.8740234375, 0.146240234375, 0.229736328125, 0.849609375, 0.496337890625, 0.97216796875, 0.99267578125]], "normalized_score": [0.423055499792099], "tokens": [["\u2581L", "t", ".", "\u2581Frank", "\u2581T", ".", "\u2581C", "off", "yn"]], "entropy": [[3.7265625, 1.482421875, 0.931640625, 3.794921875, 3.013671875, 0.591796875, 2.1171875, 0.1934814453125, 0.05377197265625]], "avg_entropy": [1.76727294921875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Amy Johnston", "score_of_biggest_cluster": 0.423055499792099, "score_of_first_answer": 0.423055499792099, "max_score_of_first_answer": 0.99267578125, "min_score_of_first_answer": 0.05078125, "avg_score": 0.423055499792099, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34931439885059257, "std_of_first_answer": 0.34931439885059257, "avg_entropy_of_first_answer": 1.76727294921875, "lex_sim": 1.0}
{"question": "Which Frenchman, who never learned to speak Swedish, became King Charles XIV John of Sweden and Norway?", "answer": ["Jean-Baptiste Bernadotte"], "scores": [[0.9951171875, 0.52978515625, 0.99951171875, 0.98193359375, 0.9931640625, 0.9677734375, 1.0, 0.99951171875]], "normalized_score": [0.916333019733429], "tokens": [["\u2581Jean", "-", "B", "apt", "iste", "\u2581Bern", "ad", "otte"]], "entropy": [[0.04486083984375, 0.921875, 0.004589080810546875, 0.1273193359375, 0.04931640625, 0.1494140625, 0.001987457275390625, 0.00490570068359375]], "avg_entropy": [0.16303348541259766], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "King Charles XIV", "score_of_biggest_cluster": 0.916333019733429, "score_of_first_answer": 0.916333019733429, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.52978515625, "avg_score": 0.916333019733429, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.15288876586469857, "std_of_first_answer": 0.15288876586469857, "avg_entropy_of_first_answer": 0.16303348541259766, "lex_sim": 1.0}
{"question": "In \u2018Othello\u2019, what is the name of Iago\u2019s wife?", "answer": ["Emilia"], "scores": [[0.98193359375, 0.9912109375]], "normalized_score": [0.9865613579750061], "tokens": [["\u2581Em", "ilia"]], "entropy": [[0.16845703125, 0.05804443359375]], "avg_entropy": [0.113250732421875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Emilia", "score_of_biggest_cluster": 0.9865613579750061, "score_of_first_answer": 0.9865613579750061, "max_score_of_first_answer": 0.9912109375, "min_score_of_first_answer": 0.98193359375, "avg_score": 0.9865613579750061, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.004638671875, "std_of_first_answer": 0.004638671875, "avg_entropy_of_first_answer": 0.113250732421875, "lex_sim": 1.0}
{"question": "Born in 1947, James Jewel Osterberg is a rock/punk pop singer better known by what name?", "answer": ["\u62bd\u8c61\u547d\u540d"], "scores": [[0.001003265380859375, 4.303455352783203e-05, 0.07666015625, 0.5849609375, 0.62255859375, 0.0009570121765136719, 0.9912109375]], "normalized_score": [0.019680118188261986], "tokens": [["\u2581", "<0xE6>", "<0x8A>", "<0xBD>", "\u8c61", "\u547d", "\u540d"]], "entropy": [[0.7080078125, 3.060546875, 3.66796875, 1.65234375, 2.56640625, 4.1875, 0.07489013671875]], "avg_entropy": [2.273951939174107], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "James Osterberg", "score_of_biggest_cluster": 0.019680118188261986, "score_of_first_answer": 0.019680118188261986, "max_score_of_first_answer": 0.9912109375, "min_score_of_first_answer": 4.303455352783203e-05, "avg_score": 0.019680118188261986, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3736316845059723, "std_of_first_answer": 0.3736316845059723, "avg_entropy_of_first_answer": 2.273951939174107, "lex_sim": 0.0}
{"question": "Who played the title role in the 2008 film 'The Curious Case Of Benjamin Button'?", "answer": ["Brad Pitt"], "scores": [[0.9775390625, 0.99169921875]], "normalized_score": [0.9845936894416809], "tokens": [["\u2581Brad", "\u2581Pitt"]], "entropy": [[0.2142333984375, 0.055328369140625]], "avg_entropy": [0.1347808837890625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bradley Pitt", "score_of_biggest_cluster": 0.9845936894416809, "score_of_first_answer": 0.9845936894416809, "max_score_of_first_answer": 0.99169921875, "min_score_of_first_answer": 0.9775390625, "avg_score": 0.9845936894416809, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.007080078125, "std_of_first_answer": 0.007080078125, "avg_entropy_of_first_answer": 0.1347808837890625, "lex_sim": 1.0}
{"question": "What country did the Reagan administration illegally sell arms to?", "answer": ["A country in Central America"], "scores": [[0.0026149749755859375, 0.1346435546875, 0.67529296875, 0.41845703125, 0.9892578125]], "normalized_score": [0.15798690915107727], "tokens": [["\u2581A", "\u2581country", "\u2581in", "\u2581Central", "\u2581America"]], "entropy": [[1.43359375, 3.55859375, 1.650390625, 1.6494140625, 0.079833984375]], "avg_entropy": [1.674365234375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ir\u0101n", "score_of_biggest_cluster": 0.15798690915107727, "score_of_first_answer": 0.15798690915107727, "max_score_of_first_answer": 0.9892578125, "min_score_of_first_answer": 0.0026149749755859375, "avg_score": 0.15798690915107727, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3583230662747225, "std_of_first_answer": 0.3583230662747225, "avg_entropy_of_first_answer": 1.674365234375, "lex_sim": 1.0}
{"question": "What plant do we often call the 'Busy Lizzie'?", "answer": ["Impatiens"], "scores": [[0.9912109375, 0.99072265625, 1.0]], "normalized_score": [0.9939687252044678], "tokens": [["\u2581Imp", "ati", "ens"]], "entropy": [[0.07342529296875, 0.0667724609375, 0.0009179115295410156]], "avg_entropy": [0.04703855514526367], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Balsamine", "score_of_biggest_cluster": 0.9939687252044678, "score_of_first_answer": 0.9939687252044678, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.99072265625, "avg_score": 0.9939687252044678, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.004262956007066665, "std_of_first_answer": 0.004262956007066665, "avg_entropy_of_first_answer": 0.04703855514526367, "lex_sim": 1.0}
{"question": "What movie took place on the moon of Pandora?", "answer": ["Avatar (2009 film)"], "scores": [[0.92529296875, 0.99951171875, 0.068359375, 0.97119140625, 1.0, 0.99951171875, 0.99951171875, 0.17626953125, 0.9970703125]], "normalized_score": [0.6045126914978027], "tokens": [["\u2581Av", "atar", "\u2581(", "2", "0", "0", "9", "\u2581film", ")"]], "entropy": [[0.499267578125, 0.005130767822265625, 0.95556640625, 0.1929931640625, 0.00270843505859375, 0.004222869873046875, 0.0029773712158203125, 0.66064453125, 0.0266265869140625]], "avg_entropy": [0.2611264122856988], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Avat\u0101ra", "score_of_biggest_cluster": 0.6045126914978027, "score_of_first_answer": 0.6045126914978027, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.068359375, "avg_score": 0.6045126914978027, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3601155781944718, "std_of_first_answer": 0.3601155781944718, "avg_entropy_of_first_answer": 0.2611264122856988, "lex_sim": 1.0}
{"question": "What is the capital of Algeria", "answer": ["Oran-Algir #59: Derek Sivers - An Unconventional Guide to Becoming a Millionaire [podcast] 0"], "scores": [[0.0005345344543457031, 0.95947265625, 0.0004029273986816406, 0.06982421875, 0.032623291015625, 1.5497207641601562e-06, 0.002079010009765625, 0.022552490234375, 0.0017757415771484375, 0.0018596649169921875, 0.0003509521484375, 0.00734710693359375, 0.6298828125, 0.030059814453125, 0.1654052734375, 0.042877197265625, 0.76708984375, 0.99951171875, 0.99853515625, 0.12115478515625, 0.953125, 0.063232421875, 0.9970703125, 0.9345703125, 0.0054473876953125, 1.0, 0.99169921875, 0.0258941650390625, 0.0007376670837402344, 0.9990234375, 0.427734375, 0.0243682861328125, 0.013153076171875]], "normalized_score": [0.03625533729791641], "tokens": [["\u2581O", "ran", "-", "Alg", "ir", "\u2581#", "5", "9", ":", "\u2581D", "erek", "\u2581S", "ivers", "\u2581-", "\u2581An", "\u2581Un", "con", "vent", "ional", "\u2581Guide", "\u2581to", "\u2581Be", "coming", "\u2581a", "\u2581Mill", "ion", "aire", "\u2581[", "pod", "cast", "]", "\u2581", "0"]], "entropy": [[0.2296142578125, 0.344970703125, 1.310546875, 3.619140625, 2.734375, 1.578125, 4.9140625, 2.802734375, 3.1796875, 5.22265625, 3.412109375, 5.37890625, 2.265625, 3.90625, 5.03515625, 1.6767578125, 1.3271484375, 0.00421905517578125, 0.01305389404296875, 1.568359375, 0.2198486328125, 0.779296875, 0.0276947021484375, 0.329833984375, 1.6298828125, 0.00260162353515625, 0.054443359375, 2.54296875, 4.16796875, 0.00885772705078125, 2.2265625, 2.6875, 2.2265625]], "avg_entropy": [2.04325820460464], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Algiers, AL", "score_of_biggest_cluster": 0.03625533729791641, "score_of_first_answer": 0.03625533729791641, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.5497207641601562e-06, "avg_score": 0.03625533729791641, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4277133254828391, "std_of_first_answer": 0.4277133254828391, "avg_entropy_of_first_answer": 2.04325820460464, "lex_sim": 1.0}
{"question": "In January 1971, Idi Amin deposed Milton Obote in a coup to become President of which country?", "answer": ["Uganda"], "scores": [[0.98583984375, 0.9990234375, 0.99462890625]], "normalized_score": [0.993148922920227], "tokens": [["\u2581U", "g", "anda"]], "entropy": [[0.151123046875, 0.011505126953125, 0.037384033203125]], "avg_entropy": [0.06667073567708333], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ugandese", "score_of_biggest_cluster": 0.993148922920227, "score_of_first_answer": 0.993148922920227, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.98583984375, "avg_score": 0.993148922920227, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.005480943437657141, "std_of_first_answer": 0.005480943437657141, "avg_entropy_of_first_answer": 0.06667073567708333, "lex_sim": 1.0}
{"question": "What were the names of the founder of British company JCB?", "answer": ["Joseph Cyril Bamford"], "scores": [[0.98486328125, 0.99365234375, 1.0, 0.99560546875, 1.0, 1.0]], "normalized_score": [0.9956719875335693], "tokens": [["\u2581Joseph", "\u2581Cy", "ril", "\u2581B", "am", "ford"]], "entropy": [[0.09942626953125, 0.05731201171875, 0.00261688232421875, 0.0438232421875, 0.00025272369384765625, 0.0005688667297363281]], "avg_entropy": [0.033999999364217125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "J C Bamford (person)", "score_of_biggest_cluster": 0.9956719875335693, "score_of_first_answer": 0.9956719875335693, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.98486328125, "avg_score": 0.9956719875335693, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.005433005120597175, "std_of_first_answer": 0.005433005120597175, "avg_entropy_of_first_answer": 0.033999999364217125, "lex_sim": 1.0}
{"question": "Bring on the Empty Horses was the sequel to which other David Niven Autobiography/Memoir ?", "answer": ["The Moon's a Balloon"], "scores": [[0.78955078125, 0.9609375, 0.9716796875, 1.0, 0.95068359375, 0.99755859375, 0.99755859375, 1.0]], "normalized_score": [0.9559581875801086], "tokens": [["\u2581The", "\u2581Moon", "'", "s", "\u2581a", "\u2581Bal", "lo", "on"]], "entropy": [[0.734375, 0.327880859375, 0.167236328125, 3.516674041748047e-05, 0.20166015625, 0.0209503173828125, 0.023193359375, 0.00021708011627197266]], "avg_entropy": [0.18444353342056274], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Moon's a Balloon", "score_of_biggest_cluster": 0.9559581875801086, "score_of_first_answer": 0.9559581875801086, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.78955078125, "avg_score": 0.9559581875801086, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.06640804512739101, "std_of_first_answer": 0.06640804512739101, "avg_entropy_of_first_answer": 0.18444353342056274, "lex_sim": 1.0}
{"question": "Who were the notorious Friars of St Francis of Wycombe?", "answer": ["It is generally believed that the \"Friars of St Francis of Wycombe\" mentioned in the song are a reference to the Pythons themselves, and the line is a play on the phrase \"the Knights Who Say Ni\" earlier in"], "scores": [[0.06414794921875, 0.90283203125, 0.00015246868133544922, 0.50048828125, 0.93798828125, 0.6708984375, 0.2205810546875, 0.476318359375, 0.9990234375, 0.994140625, 0.99072265625, 0.677734375, 0.9912109375, 1.0, 1.0, 1.0, 0.994140625, 0.04052734375, 0.9404296875, 0.4189453125, 0.03582763671875, 0.03778076171875, 0.57080078125, 0.8544921875, 0.99951171875, 0.77783203125, 0.07080078125, 0.66943359375, 0.93408203125, 0.84033203125, 0.55322265625, 0.193115234375, 0.3046875, 0.7138671875, 0.7998046875, 0.40966796875, 0.2369384765625, 0.7626953125, 0.9404296875, 0.1441650390625, 0.99658203125, 0.1756591796875, 0.77783203125, 1.0, 0.8017578125, 0.98876953125, 0.8232421875, 0.1265869140625, 0.0299530029296875, 1.0]], "normalized_score": [0.3902266025543213], "tokens": [["\u2581It", "\u2581is", "\u2581generally", "\u2581believed", "\u2581that", "\u2581the", "\u2581\"", "Fri", "ars", "\u2581of", "\u2581St", "\u2581Francis", "\u2581of", "\u2581Wy", "com", "be", "\"", "\u2581mentioned", "\u2581in", "\u2581the", "\u2581song", "\u2581are", "\u2581a", "\u2581reference", "\u2581to", "\u2581the", "\u2581P", "yth", "ons", "\u2581themselves", ",", "\u2581and", "\u2581the", "\u2581line", "\u2581is", "\u2581a", "\u2581play", "\u2581on", "\u2581the", "\u2581phrase", "\u2581\"", "the", "\u2581Kn", "ights", "\u2581Who", "\u2581Say", "\u2581Ni", "\"", "\u2581earlier", "\u2581in"]], "entropy": [[3.20703125, 0.52685546875, 1.205078125, 1.591796875, 0.29248046875, 1.41796875, 2.044921875, 0.8505859375, 0.009613037109375, 0.0384521484375, 0.06268310546875, 0.64599609375, 0.056243896484375, 0.0015516281127929688, 0.0018215179443359375, 0.0004229545593261719, 0.042205810546875, 1.99609375, 0.332763671875, 2.853515625, 2.822265625, 0.72412109375, 1.513671875, 0.76708984375, 0.00402069091796875, 0.89794921875, 3.052734375, 1.724609375, 0.2841796875, 0.46826171875, 0.904296875, 1.767578125, 1.7978515625, 1.4208984375, 0.82080078125, 1.873046875, 2.150390625, 0.55419921875, 0.2509765625, 2.16796875, 0.0302886962890625, 3.07421875, 1.41015625, 4.3392181396484375e-05, 0.521484375, 0.07171630859375, 0.7060546875, 1.416015625, 0.91552734375, 0.0015773773193359375]], "avg_entropy": [1.025841522216797], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Phoenix Common Room", "score_of_biggest_cluster": 0.3902266025543213, "score_of_first_answer": 0.3902266025543213, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.00015246868133544922, "avg_score": 0.3902266025543213, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35673551787603175, "std_of_first_answer": 0.35673551787603175, "avg_entropy_of_first_answer": 1.025841522216797, "lex_sim": 1.0}
{"question": "Gjetost is the national cheese of which country?", "answer": ["Norway #86 \u2013 Talkin\u2019 Hockey with Bill Mc Guiness"], "scores": [[0.951171875, 5.364418029785156e-07, 0.006290435791015625, 0.00945281982421875, 2.0325183868408203e-05, 0.006656646728515625, 0.019500732421875, 0.00018107891082763672, 0.266357421875, 7.56978988647461e-05, 0.173095703125, 0.00464630126953125, 0.0171661376953125, 0.00020551681518554688, 0.005462646484375]], "normalized_score": [0.0027780476957559586], "tokens": [["\u2581Norway", "\u2581#", "8", "6", "\u2581\u2013", "\u2581T", "alk", "in", "\u2019", "\u2581Hockey", "\u2581with", "\u2581Bill", "\u2581Mc", "\u2581Gu", "iness"]], "entropy": [[0.338134765625, 0.57861328125, 4.66796875, 1.5439453125, 2.138671875, 5.421875, 5.53125, 3.19140625, 1.525390625, 2.462890625, 2.744140625, 5.55859375, 4.078125, 2.125, 0.9921875]], "avg_entropy": [2.859879557291667], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Norv\u00e8ge", "score_of_biggest_cluster": 0.0027780476957559586, "score_of_first_answer": 0.0027780476957559586, "max_score_of_first_answer": 0.951171875, "min_score_of_first_answer": 5.364418029785156e-07, "avg_score": 0.0027780476957559586, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.24005195302422128, "std_of_first_answer": 0.24005195302422128, "avg_entropy_of_first_answer": 2.859879557291667, "lex_sim": 1.0}
{"question": "What was the occupation of St Peter, the leader of the Apostles?", "answer": ["Fisherman"], "scores": [[0.67578125, 0.9609375]], "normalized_score": [0.8058433532714844], "tokens": [["\u2581Fish", "erman"]], "entropy": [[1.212890625, 0.226806640625]], "avg_entropy": [0.7198486328125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fisher girls", "score_of_biggest_cluster": 0.8058433532714844, "score_of_first_answer": 0.8058433532714844, "max_score_of_first_answer": 0.9609375, "min_score_of_first_answer": 0.67578125, "avg_score": 0.8058433532714844, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.142578125, "std_of_first_answer": 0.142578125, "avg_entropy_of_first_answer": 0.7198486328125, "lex_sim": 1.0}
{"question": "Pearl is composed of minute crystals of which compound?", "answer": ["Calcium carbonate (CaCO3)"], "scores": [[0.83935546875, 0.93701171875, 1.0, 0.89697265625, 0.9951171875, 0.18994140625, 0.978515625, 0.9833984375, 0.97021484375, 0.98974609375]], "normalized_score": [0.8110830187797546], "tokens": [["\u2581Cal", "ci", "um", "\u2581carbon", "ate", "\u2581(", "Ca", "CO", "3", ")"]], "entropy": [[0.90283203125, 0.26904296875, 0.0005278587341308594, 0.4765625, 0.0430908203125, 1.0458984375, 0.13037109375, 0.093505859375, 0.216552734375, 0.07061767578125]], "avg_entropy": [0.3249001979827881], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "E170 (E number)", "score_of_biggest_cluster": 0.8110830187797546, "score_of_first_answer": 0.8110830187797546, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.18994140625, "avg_score": 0.8110830187797546, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.2344277691246657, "std_of_first_answer": 0.2344277691246657, "avg_entropy_of_first_answer": 0.3249001979827881, "lex_sim": 1.0}
{"question": "What brand name are Chuck Taylor sneakers sold under?", "answer": ["Converse"], "scores": [[0.9833984375, 0.99951171875]], "normalized_score": [0.9914223551750183], "tokens": [["\u2581Con", "verse"]], "entropy": [[0.12939453125, 0.0047607421875]], "avg_entropy": [0.06707763671875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Converse", "score_of_biggest_cluster": 0.9914223551750183, "score_of_first_answer": 0.9914223551750183, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.9833984375, "avg_score": 0.9914223551750183, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.008056640625, "std_of_first_answer": 0.008056640625, "avg_entropy_of_first_answer": 0.06707763671875, "lex_sim": 1.0}
{"question": "In which 1969 movie did Michael Caine appear with his brother Stanley.", "answer": ["He who gets slapped \\*\\*1."], "scores": [[0.0005083084106445312, 0.00344085693359375, 0.005817413330078125, 0.94091796875, 1.0, 1.6689300537109375e-06, 0.084716796875, 0.99560546875, 0.11273193359375, 0.0005168914794921875, 0.34619140625, 0.0003597736358642578, 0.0005536079406738281, 0.000186920166015625, 0.91943359375, 0.032867431640625, 0.25390625, 0.99951171875, 0.99951171875, 0.70263671875, 0.11309814453125, 0.0684814453125, 5.960464477539063e-08, 0.00041747093200683594, 0.135498046875, 0.0011758804321289062, 0.419921875, 0.08319091796875, 0.05712890625, 0.99951171875, 0.9755859375, 0.3984375, 0.85400390625, 0.99609375, 0.0001939535140991211]], "normalized_score": [0.025671331211924553], "tokens": [["\u2581He", "\u2581who", "\u2581gets", "\u2581sla", "pped", "\u2581\\", "*\\", "*", "1", ".", "**", "\u2581Who", "\u2581will", "\u2581host", "\u2581the", "\u2581next", "\u2581O", "sc", "ars", "?", "\u2581\\", "*", "\u2581CHAPTER", "\u2581V", ".", "\u2581AD", "V", "AN", "TA", "GE", "S", "\u2581AND", "\u2581DIS", "AD", "..."]], "entropy": [[1.453125, 2.595703125, 1.0234375, 0.366455078125, 5.644559860229492e-05, 1.1484375, 2.90625, 0.035552978515625, 3.05078125, 0.213134765625, 3.078125, 3.658203125, 3.853515625, 3.9140625, 0.62841796875, 2.173828125, 3.5390625, 0.00838470458984375, 0.006664276123046875, 1.5029296875, 2.119140625, 0.36669921875, 1.85546875, 2.20703125, 3.193359375, 4.62109375, 2.43359375, 0.348876953125, 0.392333984375, 0.0032806396484375, 0.1968994140625, 1.9111328125, 0.966796875, 0.037078857421875, 0.07086181640625]], "avg_entropy": [1.5965650064604624], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Italian Job (1969 film)", "score_of_biggest_cluster": 0.025671331211924553, "score_of_first_answer": 0.025671331211924553, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.025671331211924553, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41426681010421285, "std_of_first_answer": 0.41426681010421285, "avg_entropy_of_first_answer": 1.5965650064604624, "lex_sim": 1.0}
{"question": "Au is the chemical symbol of what element?", "answer": ["Gold #1000Speak: We Stand United Against Unkindness"], "scores": [[0.9033203125, 5.364418029785156e-07, 0.0015316009521484375, 0.07183837890625, 0.051025390625, 0.0699462890625, 3.5762786865234375e-07, 0.68310546875, 0.032012939453125, 0.0113983154296875, 0.2880859375, 0.0218963623046875, 0.3671875, 1.0, 0.0003476142883300781, 0.96142578125, 0.97314453125]], "normalized_score": [0.019970128312706947], "tokens": [["\u2581Gold", "\u2581#", "1", "0", "0", "0", "Spe", "ak", ":", "\u2581We", "\u2581Stand", "\u2581United", "\u2581Again", "st", "\u2581Un", "kind", "ness"]], "entropy": [[0.3671875, 1.01953125, 1.390625, 1.7880859375, 2.029296875, 2.341796875, 2.60546875, 0.88037109375, 2.271484375, 4.87109375, 2.705078125, 1.01171875, 2.20703125, 0.00133514404296875, 1.0615234375, 0.253662109375, 0.159912109375]], "avg_entropy": [1.586188372443704], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gold compound", "score_of_biggest_cluster": 0.019970128312706947, "score_of_first_answer": 0.019970128312706947, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 3.5762786865234375e-07, "avg_score": 0.019970128312706947, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39464594997936875, "std_of_first_answer": 0.39464594997936875, "avg_entropy_of_first_answer": 1.586188372443704, "lex_sim": 1.0}
{"question": "What is the name of Kevin the teenager's friend played by Kathy Burke in a series of sketches with Harry Enfield?", "answer": ["Pauline (Kevin and Percy) #1 Post by wander \u00bb Sat Jan 13, 2018 12:18 pm"], "scores": [[0.215087890625, 0.99853515625, 0.02813720703125, 0.064453125, 0.98583984375, 0.1455078125, 0.0118255615234375, 0.203857421875, 0.9375, 4.172325134277344e-07, 0.05059814453125, 1.0073184967041016e-05, 0.0003902912139892578, 0.00058746337890625, 0.0128173828125, 0.0008068084716796875, 0.119384765625, 0.12646484375, 0.99755859375, 0.418212890625, 0.1417236328125, 0.99853515625, 0.97998046875, 0.9990234375, 1.0, 0.9931640625, 0.99951171875, 0.9912109375, 0.3876953125, 0.2171630859375, 0.99853515625, 0.1453857421875, 0.115966796875, 0.6044921875]], "normalized_score": [0.07981378585100174], "tokens": [["\u2581Paul", "ine", "\u2581(", "Ke", "vin", "\u2581and", "\u2581Per", "cy", ")", "\u2581#", "1", "\u2581Post", "\u2581by", "\u2581w", "ander", "\u2581\u00bb", "\u2581Sat", "\u2581Jan", "\u2581", "1", "3", ",", "\u2581", "2", "0", "1", "8", "\u2581", "1", "2", ":", "1", "8", "\u2581pm"]], "entropy": [[3.830078125, 0.01316070556640625, 1.21875, 4.265625, 0.116455078125, 0.72705078125, 0.822265625, 1.3974609375, 0.3017578125, 0.7080078125, 5.71875, 2.8984375, 2.7734375, 4.44921875, 5.5234375, 1.583984375, 2.298828125, 2.55859375, 0.0196533203125, 1.154296875, 1.92578125, 0.01192474365234375, 0.145751953125, 0.0108795166015625, 0.002758026123046875, 0.044525146484375, 0.005352020263671875, 0.0836181640625, 1.939453125, 1.3974609375, 0.0170440673828125, 1.810546875, 2.302734375, 0.73095703125]], "avg_entropy": [1.553177553064683], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Perry (drink)", "score_of_biggest_cluster": 0.07981378585100174, "score_of_first_answer": 0.07981378585100174, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 4.172325134277344e-07, "avg_score": 0.07981378585100174, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4264085281762916, "std_of_first_answer": 0.4264085281762916, "avg_entropy_of_first_answer": 1.553177553064683, "lex_sim": 1.0}
{"question": "The Oroville Dam is in which US state?", "answer": ["California #28: The Trend of \u2018Microlearning\u2019 in Training and Development"], "scores": [[0.9619140625, 1.7881393432617188e-07, 0.01275634765625, 0.050262451171875, 0.0024814605712890625, 0.226806640625, 0.00015676021575927734, 0.0004954338073730469, 0.0229034423828125, 0.00012385845184326172, 0.0210418701171875, 0.0491943359375, 0.03790283203125, 0.9833984375, 0.53369140625, 0.007694244384765625, 0.71142578125, 0.87939453125]], "normalized_score": [0.01492810994386673], "tokens": [["\u2581California", "\u2581#", "2", "8", ":", "\u2581The", "\u2581Tr", "end", "\u2581of", "\u2581\u2018", "M", "icro", "learning", "\u2019", "\u2581in", "\u2581Training", "\u2581and", "\u2581Development"]], "entropy": [[0.282470703125, 0.6240234375, 5.1796875, 2.953125, 3.205078125, 4.5625, 2.740234375, 2.900390625, 2.72265625, 5.80859375, 5.66015625, 3.96875, 3.708984375, 0.1412353515625, 2.357421875, 2.7578125, 1.6025390625, 0.708984375]], "avg_entropy": [2.882480197482639], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Taxifornia", "score_of_biggest_cluster": 0.01492810994386673, "score_of_first_answer": 0.01492810994386673, "max_score_of_first_answer": 0.9833984375, "min_score_of_first_answer": 1.7881393432617188e-07, "avg_score": 0.01492810994386673, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36437441322202613, "std_of_first_answer": 0.36437441322202613, "avg_entropy_of_first_answer": 2.882480197482639, "lex_sim": 1.0}
{"question": "Which Scot did Bob Crow succeed in 2001 as General Secretary of the RMT Union?", "answer": ["Jim Chalmers"], "scores": [[0.1768798828125, 0.0025844573974609375, 0.99853515625]], "normalized_score": [0.07699640840291977], "tokens": [["\u2581Jim", "\u2581Chal", "mers"]], "entropy": [[3.244140625, 3.93359375, 0.01302337646484375]], "avg_entropy": [2.3969192504882812], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Jimmy Knapp", "score_of_biggest_cluster": 0.07699640840291977, "score_of_first_answer": 0.07699640840291977, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.0025844573974609375, "avg_score": 0.07699640840291977, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4342828159714874, "std_of_first_answer": 0.4342828159714874, "avg_entropy_of_first_answer": 2.3969192504882812, "lex_sim": 1.0}
{"question": "What is the name for a rotating rod with oblong lobes sticking out of it, that is fitted in a car's engine block?", "answer": ["Piston"], "scores": [[0.177001953125, 0.99755859375, 0.98583984375]], "normalized_score": [0.5583513975143433], "tokens": [["\u2581P", "ist", "on"]], "entropy": [[2.791015625, 0.0242919921875, 0.0821533203125]], "avg_entropy": [0.9658203125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Camshaft", "score_of_biggest_cluster": 0.5583513975143433, "score_of_first_answer": 0.5583513975143433, "max_score_of_first_answer": 0.99755859375, "min_score_of_first_answer": 0.177001953125, "avg_score": 0.5583513975143433, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.384081771225007, "std_of_first_answer": 0.384081771225007, "avg_entropy_of_first_answer": 0.9658203125, "lex_sim": 1.0}
{"question": "Which US President has a statue in Parliament Square?", "answer": ["Abraham Lincoln"], "scores": [[0.51318359375, 0.966796875]], "normalized_score": [0.7043750882148743], "tokens": [["\u2581Abraham", "\u2581Lincoln"]], "entropy": [[1.642578125, 0.219970703125]], "avg_entropy": [0.9312744140625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Kennedy", "score_of_biggest_cluster": 0.7043750882148743, "score_of_first_answer": 0.7043750882148743, "max_score_of_first_answer": 0.966796875, "min_score_of_first_answer": 0.51318359375, "avg_score": 0.7043750882148743, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.226806640625, "std_of_first_answer": 0.226806640625, "avg_entropy_of_first_answer": 0.9312744140625, "lex_sim": 1.0}
{"question": "What is the title of the 1944 film starring Barbara Stanwyck as the wife who seduces an insurance salesman into killing her husband?", "answer": ["Double Indemnity (film)"], "scores": [[0.421630859375, 0.90771484375, 0.99951171875, 0.99951171875, 1.0, 0.01910400390625, 0.1368408203125, 0.9697265625]], "normalized_score": [0.42005470395088196], "tokens": [["\u2581Double", "\u2581Ind", "em", "n", "ity", "\u2581(", "film", ")"]], "entropy": [[1.541015625, 0.64208984375, 0.005420684814453125, 0.0035839080810546875, 2.574920654296875e-05, 0.83837890625, 1.2275390625, 0.1544189453125]], "avg_entropy": [0.5515590906143188], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Double indemnity (disambiguation)", "score_of_biggest_cluster": 0.42005470395088196, "score_of_first_answer": 0.42005470395088196, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.01910400390625, "avg_score": 0.42005470395088196, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39384425710348203, "std_of_first_answer": 0.39384425710348203, "avg_entropy_of_first_answer": 0.5515590906143188, "lex_sim": 1.0}
{"question": "In Rowing, who received gold medals for the Women's coxless pair?", "answer": ["Sarah Winckless"], "scores": [[0.032135009765625, 0.61181640625, 0.9951171875, 1.0]], "normalized_score": [0.3739972710609436], "tokens": [["\u2581Sarah", "\u2581Win", "ck", "less"]], "entropy": [[3.279296875, 2.666015625, 0.051788330078125, 0.0014820098876953125]], "avg_entropy": [1.499645709991455], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Helen Glover and Heather Stanning", "score_of_biggest_cluster": 0.3739972710609436, "score_of_first_answer": 0.3739972710609436, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.032135009765625, "avg_score": 0.3739972710609436, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39510739589121024, "std_of_first_answer": 0.39510739589121024, "avg_entropy_of_first_answer": 1.499645709991455, "lex_sim": 1.0}
{"question": "Which artist painted 'The Laughing Cavalier'?", "answer": ["Franz Hals"], "scores": [[0.23779296875, 0.9912109375, 1.0]], "normalized_score": [0.6177152991294861], "tokens": [["\u2581Franz", "\u2581H", "als"]], "entropy": [[1.375, 0.09130859375, 0.00042057037353515625]], "avg_entropy": [0.4889097213745117], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Frans Hals", "score_of_biggest_cluster": 0.6177152991294861, "score_of_first_answer": 0.6177152991294861, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.23779296875, "avg_score": 0.6177152991294861, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.35725425769050495, "std_of_first_answer": 0.35725425769050495, "avg_entropy_of_first_answer": 0.4889097213745117, "lex_sim": 1.0}
{"question": "Which actor plays Cate Blanchett's father in the 2004 film \"The Missing\", a story set in 19th century New Mexico where the father works with estranged daughter Blanchett to get her kidnapped child back?", "answer": ["Tommy Lee Jones #ThrowbackThursday: How to Choose a Web Hosting Provider"], "scores": [[0.83056640625, 1.0, 0.99951171875, 1.1920928955078125e-07, 0.00012230873107910156, 0.98779296875, 0.208984375, 0.99951171875, 0.99609375, 0.00171661376953125, 0.00103759765625, 0.08544921875, 0.022491455078125, 1.0, 0.124755859375, 0.00942230224609375, 0.94091796875, 0.82666015625, 0.5107421875, 0.99951171875]], "normalized_score": [0.06760294735431671], "tokens": [["\u2581Tommy", "\u2581Lee", "\u2581Jones", "\u2581#", "Throw", "back", "Th", "urs", "day", ":", "\u2581How", "\u2581to", "\u2581Cho", "ose", "\u2581a", "\u2581Web", "\u2581Host", "ing", "\u2581Pro", "vider"]], "entropy": [[0.97802734375, 0.0013990402221679688, 0.0029239654541015625, 0.77099609375, 5.2734375, 0.08050537109375, 2.5234375, 0.0031375885009765625, 0.035675048828125, 1.9365234375, 4.62890625, 3.138671875, 5.39453125, 0.002376556396484375, 0.95263671875, 5.43359375, 0.35302734375, 0.80078125, 1.322265625, 0.0031566619873046875]], "avg_entropy": [1.6818005084991454], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "List of awards and nominations received by Tommy Lee Jones", "score_of_biggest_cluster": 0.06760294735431671, "score_of_first_answer": 0.06760294735431671, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.1920928955078125e-07, "avg_score": 0.06760294735431671, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4467316777987731, "std_of_first_answer": 0.4467316777987731, "avg_entropy_of_first_answer": 1.6818005084991454, "lex_sim": 1.0}
{"question": "Which English poet and dramatist was jailed for murder of the actor Gabriel Spencer after a duel in 1598?", "answer": ["Christopher Marlowe"], "scores": [[0.87841796875, 0.9931640625, 0.99951171875, 0.99365234375]], "normalized_score": [0.9647974371910095], "tokens": [["\u2581Christopher", "\u2581Mar", "low", "e"]], "entropy": [[0.640625, 0.06134033203125, 0.003101348876953125, 0.04290771484375]], "avg_entropy": [0.18699359893798828], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Benjamin Jonson", "score_of_biggest_cluster": 0.9647974371910095, "score_of_first_answer": 0.9647974371910095, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.87841796875, "avg_score": 0.9647974371910095, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.050734718718684645, "std_of_first_answer": 0.050734718718684645, "avg_entropy_of_first_answer": 0.18699359893798828, "lex_sim": 1.0}
{"question": "Which group recorded the 1976 album 'Rastaman Vibration'?", "answer": ["Bob Marley & The Wailers"], "scores": [[0.66650390625, 0.97021484375, 0.99951171875, 0.5283203125, 0.9482421875, 0.998046875, 1.0, 0.99951171875]], "normalized_score": [0.8682639002799988], "tokens": [["\u2581Bob", "\u2581Mar", "ley", "\u2581&", "\u2581The", "\u2581W", "ail", "ers"]], "entropy": [[0.7421875, 0.18896484375, 0.003734588623046875, 0.76416015625, 0.2147216796875, 0.016845703125, 9.679794311523438e-05, 0.003726959228515625]], "avg_entropy": [0.24180477857589722], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Bob Marley and the Wailers (band)", "score_of_biggest_cluster": 0.8682639002799988, "score_of_first_answer": 0.8682639002799988, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.5283203125, "avg_score": 0.8682639002799988, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.17260409814637168, "std_of_first_answer": 0.17260409814637168, "avg_entropy_of_first_answer": 0.24180477857589722, "lex_sim": 1.0}
{"question": "Pistol and Boo, who became internationally famous in 2015, were what?", "answer": ["Cats"], "scores": [[0.44580078125, 0.99560546875]], "normalized_score": [0.6662144660949707], "tokens": [["\u2581C", "ats"]], "entropy": [[2.56640625, 0.03192138671875]], "avg_entropy": [1.299163818359375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chocolate Yorkshire Terrier", "score_of_biggest_cluster": 0.6662144660949707, "score_of_first_answer": 0.6662144660949707, "max_score_of_first_answer": 0.99560546875, "min_score_of_first_answer": 0.44580078125, "avg_score": 0.6662144660949707, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27490234375, "std_of_first_answer": 0.27490234375, "avg_entropy_of_first_answer": 1.299163818359375, "lex_sim": 1.0}
{"question": "Who had a Too Legit To Quit Tour?", "answer": ["MC Hammer In the world of business, communication is key."], "scores": [[0.966796875, 0.99658203125, 1.0, 1.1324882507324219e-06, 0.255859375, 0.00010514259338378906, 0.853515625, 0.01015472412109375, 0.638671875, 0.000438690185546875, 0.299560546875, 0.5693359375, 0.52880859375, 0.00951385498046875, 0.035430908203125, 0.3740234375, 0.8359375, 0.0048980712890625, 0.006473541259765625, 0.64404296875, 0.9990234375, 0.9755859375, 0.21875, 0.409912109375, 0.595703125, 0.796875, 0.3671875, 0.62939453125, 0.44775390625, 0.822265625, 0.93017578125, 1.0, 0.1063232421875, 0.62646484375, 0.1678466796875, 0.99853515625, 0.379638671875, 0.1409912109375, 0.99853515625, 0.51806640625, 0.2978515625, 0.91748046875, 0.9736328125, 0.138916015625, 0.97998046875, 0.04608154296875, 0.99853515625, 0.058990478515625]], "normalized_score": [0.17902708053588867], "tokens": [["\u2581MC", "\u2581Ham", "mer", "\u2581In", "\u2581the", "\u2581world", "\u2581of", "\u2581business", ",", "\u2581communication", "\u2581is", "\u2581key", ".", "\u2581From", "\u2581sharing", "\u2581ideas", "\u2581to", "\u2581creating", "\u2581effective", "\u2581strateg", "ies", ",", "\u2581it", "\u2581is", "\u2581important", "\u2581to", "\u2581have", "\u2581a", "\u2581clear", "\u2581and", "\u2581conc", "ise", "\u2581way", "\u2581of", "\u2581communic", "ating", "\u2581with", "\u2581colle", "agues", ",", "\u2581partners", ",", "\u2581and", "\u2581customers", ".", "\u2581However", ",", "\u2581communication"]], "entropy": [[0.258544921875, 0.031005859375, 0.001796722412109375, 0.85546875, 1.5517578125, 2.12109375, 0.92724609375, 4.5078125, 1.6044921875, 4.5390625, 2.087890625, 1.9443359375, 1.5302734375, 4.328125, 4.74609375, 1.228515625, 0.603515625, 3.021484375, 2.369140625, 1.3876953125, 0.0101165771484375, 0.1475830078125, 2.8046875, 1.4775390625, 1.2529296875, 0.6845703125, 2.298828125, 1.4697265625, 2.603515625, 0.83203125, 0.45458984375, 0.0013980865478515625, 1.2236328125, 0.76220703125, 2.357421875, 0.01140594482421875, 1.4912109375, 1.328125, 0.0121612548828125, 0.89599609375, 1.359375, 0.310791015625, 0.1661376953125, 0.91357421875, 0.130615234375, 3.7734375, 0.0141143798828125, 2.80859375]], "avg_entropy": [1.4842013915379841], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "M. C. Hammer", "score_of_biggest_cluster": 0.17902708053588867, "score_of_first_answer": 0.17902708053588867, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.1324882507324219e-06, "avg_score": 0.17902708053588867, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3721678211848351, "std_of_first_answer": 0.3721678211848351, "avg_entropy_of_first_answer": 1.4842013915379841, "lex_sim": 1.0}
{"question": "What was is the name of Penelope Pitstop\u2019s car in the children\u2019s tv cartoon series \u2018Wacky Races\u2019?", "answer": ["The Compact Pussycat"], "scores": [[0.99072265625, 0.85400390625, 0.8974609375, 0.9951171875, 0.99560546875, 0.99658203125, 0.962890625]], "normalized_score": [0.9545149207115173], "tokens": [["\u2581The", "\u2581Comp", "act", "\u2581P", "uss", "y", "cat"]], "entropy": [[0.0782470703125, 0.9677734375, 0.6962890625, 0.04510498046875, 0.046478271484375, 0.033599853515625, 0.201416015625]], "avg_entropy": [0.29555838448660715], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "PROFESSOR PAT PENDING", "score_of_biggest_cluster": 0.9545149207115173, "score_of_first_answer": 0.9545149207115173, "max_score_of_first_answer": 0.99658203125, "min_score_of_first_answer": 0.85400390625, "avg_score": 0.9545149207115173, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.053222976223644976, "std_of_first_answer": 0.053222976223644976, "avg_entropy_of_first_answer": 0.29555838448660715, "lex_sim": 1.0}
{"question": "\"Meaning \"\"double step\u201d in Spanish, which dance actually originated in southern France but is modelled after the sound, drama, and movement of the Spanish bullfight?\"", "answer": ["Flamenco Tags: Miami, MLB, NBA, NCAA, NFL"], "scores": [[0.347412109375, 0.92333984375, 0.998046875, 0.0, 0.86962890625, 1.3709068298339844e-06, 0.4619140625, 2.193450927734375e-05, 0.446044921875, 0.64208984375, 0.00562286376953125, 0.82080078125, 0.00498199462890625, 0.70458984375, 0.29541015625]], "normalized_score": [0.0], "tokens": [["\u2581Fl", "amen", "co", "\u2581Tags", ":", "\u2581Miami", ",", "\u2581ML", "B", ",", "\u2581NBA", ",", "\u2581NCAA", ",", "\u2581NFL"]], "entropy": [[2.62109375, 0.305908203125, 0.0173797607421875, 0.8017578125, 0.92822265625, 3.99609375, 3.51953125, 6.078125, 2.3828125, 2.203125, 6.6640625, 1.0849609375, 5.5390625, 1.419921875, 4.50390625]], "avg_entropy": [2.8043975830078125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pasodoble (dance)", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.998046875, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3627435758527373, "std_of_first_answer": 0.3627435758527373, "avg_entropy_of_first_answer": 2.8043975830078125, "lex_sim": 1.0}
{"question": "How many US states are prefixed by the word New?", "answer": ["5"], "scores": [[0.0243377685546875, 0.61376953125]], "normalized_score": [0.12222021073102951], "tokens": [["\u2581", "5"]], "entropy": [[2.833984375, 1.421875]], "avg_entropy": [2.1279296875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "four", "score_of_biggest_cluster": 0.12222021073102951, "score_of_first_answer": 0.12222021073102951, "max_score_of_first_answer": 0.61376953125, "min_score_of_first_answer": 0.0243377685546875, "avg_score": 0.12222021073102951, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.29471588134765625, "std_of_first_answer": 0.29471588134765625, "avg_entropy_of_first_answer": 2.1279296875, "lex_sim": 1.0}
{"question": "British MP, Diane Abbott, learned to play which musical instrument in the tv series \u2018Play It Again\u2019?", "answer": ["Ukulele"], "scores": [[0.00859832763671875, 0.97021484375, 0.9990234375]], "normalized_score": [0.20274610817432404], "tokens": [["\u2581Uk", "ule", "le"]], "entropy": [[3.521484375, 0.2174072265625, 0.0088348388671875]], "avg_entropy": [1.2492421468098958], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Pianos", "score_of_biggest_cluster": 0.20274610817432404, "score_of_first_answer": 0.20274610817432404, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.00859832763671875, "avg_score": 0.20274610817432404, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.46025091675729685, "std_of_first_answer": 0.46025091675729685, "avg_entropy_of_first_answer": 1.2492421468098958, "lex_sim": 1.0}
{"question": "Following that year's April Budget, Alderman Sir Cuthbert Ackroyd, who later became Lord Mayor of London, bought the first what on 1 November 1956?", "answer": ["Mini"], "scores": [[0.30712890625, 0.9951171875]], "normalized_score": [0.5528374910354614], "tokens": [["\u2581M", "ini"]], "entropy": [[3.375, 0.042388916015625]], "avg_entropy": [1.7086944580078125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ERNIE", "score_of_biggest_cluster": 0.5528374910354614, "score_of_first_answer": 0.5528374910354614, "max_score_of_first_answer": 0.9951171875, "min_score_of_first_answer": 0.30712890625, "avg_score": 0.5528374910354614, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.343994140625, "std_of_first_answer": 0.343994140625, "avg_entropy_of_first_answer": 1.7086944580078125, "lex_sim": 1.0}
{"question": "Which river forms almost all the boundary between Devon and Cornwall", "answer": ["River Tamar ##"], "scores": [[0.1881103515625, 0.98681640625, 0.9873046875, 0.0]], "normalized_score": [0.0], "tokens": [["\u2581River", "\u2581T", "amar", "\u2581##"]], "entropy": [[1.2822265625, 0.09735107421875, 0.0950927734375, 0.84326171875]], "avg_entropy": [0.5794830322265625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tamar (Bible) (disambiguation)", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.9873046875, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4514286943123076, "std_of_first_answer": 0.4514286943123076, "avg_entropy_of_first_answer": 0.5794830322265625, "lex_sim": 1.0}
{"question": "In the USA in 1912, Theodore Roosevelt formed the Progressive Party when he lost the Republican nomination to William Howard Taft. What was the party also known as?", "answer": ["Bull Moose Party"], "scores": [[0.88525390625, 0.88427734375, 0.99951171875, 0.94873046875]], "normalized_score": [0.9282111525535583], "tokens": [["\u2581Bull", "\u2581Mo", "ose", "\u2581Party"]], "entropy": [[0.51318359375, 0.459716796875, 0.00554656982421875, 0.2509765625]], "avg_entropy": [0.3073558807373047], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Progressive Party 1912 (United States)", "score_of_biggest_cluster": 0.9282111525535583, "score_of_first_answer": 0.9282111525535583, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.88427734375, "avg_score": 0.9282111525535583, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.04815143899322138, "std_of_first_answer": 0.04815143899322138, "avg_entropy_of_first_answer": 0.3073558807373047, "lex_sim": 1.0}
{"question": "In which decade were ZIP codes introduced in the USA?", "answer": ["1960s--70s #11."], "scores": [[0.333740234375, 0.98974609375, 0.9853515625, 0.85986328125, 0.9560546875, 0.94580078125, 8.940696716308594e-06, 0.016204833984375, 0.9365234375, 0.9833984375, 1.430511474609375e-06, 0.03546142578125, 0.004085540771484375, 0.01947021484375, 0.0002505779266357422, 0.0007433891296386719, 0.02593994140625, 0.0005207061767578125, 0.368896484375]], "normalized_score": [0.023647716268897057], "tokens": [["\u2581", "1", "9", "6", "0", "s", "--", "7", "0", "s", "\u2581#", "1", "1", ".", "\u2581H", "iring", "\u2581the", "\u2581Right", "\u2581People"]], "entropy": [[1.8056640625, 0.0772705078125, 0.12103271484375, 0.61328125, 0.191650390625, 0.281005859375, 0.9326171875, 4.0390625, 0.361328125, 0.1201171875, 0.97802734375, 4.0625, 0.59716796875, 3.443359375, 2.990234375, 4.1640625, 5.3515625, 5.48046875, 2.2265625]], "avg_entropy": [1.991419741981908], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sixties Revolution", "score_of_biggest_cluster": 0.023647716268897057, "score_of_first_answer": 0.023647716268897057, "max_score_of_first_answer": 0.98974609375, "min_score_of_first_answer": 1.430511474609375e-06, "avg_score": 0.023647716268897057, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4390188831084887, "std_of_first_answer": 0.4390188831084887, "avg_entropy_of_first_answer": 1.991419741981908, "lex_sim": 1.0}
{"question": "What are listed across the top of the Lincoln Memorial on the back of the $5 bill?", "answer": ["In GOD WE TRUST"], "scores": [[0.148193359375, 0.109375, 0.84033203125, 0.927734375, 1.0, 0.99267578125, 0.9951171875]], "normalized_score": [0.5346176028251648], "tokens": [["\u2581In", "\u2581G", "OD", "\u2581W", "E", "\u2581TR", "UST"]], "entropy": [[3.0625, 2.908203125, 0.77783203125, 0.3193359375, 0.0017595291137695312, 0.05975341796875, 0.043670654296875]], "avg_entropy": [1.0247220993041992], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "All 50 states", "score_of_biggest_cluster": 0.5346176028251648, "score_of_first_answer": 0.5346176028251648, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.109375, "avg_score": 0.5346176028251648, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37527128708990787, "std_of_first_answer": 0.37527128708990787, "avg_entropy_of_first_answer": 1.0247220993041992, "lex_sim": 1.0}
{"question": "Who wrote the BBC tv serial drama \u2018The Singing Detective\u2019 (1986)?", "answer": ["AUDIENCE: SPARK"], "scores": [[0.0006046295166015625, 0.007251739501953125, 0.4677734375, 0.9306640625, 0.99267578125, 0.87890625, 0.00022912025451660156, 0.05157470703125, 0.70556640625]], "normalized_score": [0.06217510625720024], "tokens": [["\u2581A", "U", "DI", "EN", "CE", ":", "\u2581SP", "AR", "K"]], "entropy": [[0.85546875, 5.5234375, 2.5859375, 0.307861328125, 0.0623779296875, 0.80126953125, 1.8583984375, 2.111328125, 1.0068359375]], "avg_entropy": [1.6792127821180556], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Dennis C. G. Potter", "score_of_biggest_cluster": 0.06217510625720024, "score_of_first_answer": 0.06217510625720024, "max_score_of_first_answer": 0.99267578125, "min_score_of_first_answer": 0.00022912025451660156, "avg_score": 0.06217510625720024, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4128653522978683, "std_of_first_answer": 0.4128653522978683, "avg_entropy_of_first_answer": 1.6792127821180556, "lex_sim": 1.0}
{"question": "Which Scottish league team play their home game at Victoria Park and have the nickname The Staggies'?", "answer": ["CAPPY"], "scores": [[0.12646484375, 0.005870819091796875, 0.0024776458740234375]], "normalized_score": [0.012252815999090672], "tokens": [["\u2581C", "AP", "PY"]], "entropy": [[2.052734375, 0.482421875, 1.701171875]], "avg_entropy": [1.412109375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Ross County (disambiguation)", "score_of_biggest_cluster": 0.012252815999090672, "score_of_first_answer": 0.012252815999090672, "max_score_of_first_answer": 0.12646484375, "min_score_of_first_answer": 0.0024776458740234375, "avg_score": 0.012252815999090672, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05766498807276165, "std_of_first_answer": 0.05766498807276165, "avg_entropy_of_first_answer": 1.412109375, "lex_sim": 1.0}
{"question": "\"In what year did Glen Campbell have a hit with \"\"It's Only Make Believe\"\"?\"", "answer": ["1958 Tags: biodiversity, carbon, climate change, conservation, deforestation, ecology, forests, habitat, logging, sustainability, trees"], "scores": [[0.2203369140625, 0.9921875, 0.97900390625, 0.1279296875, 0.58203125, 0.0, 0.7607421875, 0.0001347064971923828, 0.0004038810729980469, 0.8876953125, 0.56689453125, 0.01320648193359375, 0.20068359375, 0.1898193359375, 0.3818359375, 0.9462890625, 0.5283203125, 0.974609375, 0.421630859375, 0.99951171875, 1.0, 1.0, 0.98974609375, 0.7353515625, 0.39013671875, 0.9970703125, 0.9892578125, 0.03863525390625, 0.9990234375, 0.98779296875, 0.185791015625, 0.46875, 0.10687255859375, 0.98193359375, 0.10040283203125, 1.0, 0.70751953125, 0.60888671875, 0.22509765625]], "normalized_score": [0.0], "tokens": [["\u2581", "1", "9", "5", "8", "\u2581Tags", ":", "\u2581b", "iod", "iversity", ",", "\u2581carbon", ",", "\u2581climate", "\u2581change", ",", "\u2581conservation", ",", "\u2581def", "or", "est", "ation", ",", "\u2581e", "col", "ogy", ",", "\u2581for", "ests", ",", "\u2581habitat", ",", "\u2581logging", ",", "\u2581sust", "ain", "ability", ",", "\u2581trees"]], "entropy": [[1.642578125, 0.06329345703125, 0.1695556640625, 0.5546875, 1.099609375, 1.0458984375, 1.380859375, 4.3828125, 4.48828125, 0.65283203125, 2.873046875, 6.453125, 2.58984375, 5.05078125, 1.13671875, 0.355224609375, 2.470703125, 0.1805419921875, 2.134765625, 0.004619598388671875, 0.002376556396484375, 0.0022869110107421875, 0.080322265625, 1.4462890625, 0.91064453125, 0.025360107421875, 0.0831298828125, 2.734375, 0.01184844970703125, 0.0919189453125, 2.44921875, 1.185546875, 3.224609375, 0.115234375, 3.244140625, 0.00022101402282714844, 0.619140625, 1.474609375, 2.298828125]], "avg_entropy": [1.5058943369449713], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "one thousand, nine hundred and seventy", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.37914322835923525, "std_of_first_answer": 0.37914322835923525, "avg_entropy_of_first_answer": 1.5058943369449713, "lex_sim": 1.0}
{"question": "Bill Gold whose most noted creations are for films as varied as Casablanca, A Clockwork Orange, Mystic River and The Sting is renowned for what type of work?", "answer": ["Bill Gold is a graphic designer."], "scores": [[0.0097808837890625, 0.98828125, 0.845703125, 0.40576171875, 0.6240234375, 1.0, 0.98583984375, 0.159912109375]], "normalized_score": [0.36661019921302795], "tokens": [["\u2581Bill", "\u2581Gold", "\u2581is", "\u2581a", "\u2581graph", "ic", "\u2581designer", "."]], "entropy": [[2.58203125, 0.088134765625, 0.62158203125, 0.85546875, 1.5732421875, 0.0018262863159179688, 0.09503173828125, 2.001953125]], "avg_entropy": [0.977408766746521], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Poster designs", "score_of_biggest_cluster": 0.36661019921302795, "score_of_first_answer": 0.36661019921302795, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0097808837890625, "avg_score": 0.36661019921302795, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3698537955638534, "std_of_first_answer": 0.3698537955638534, "avg_entropy_of_first_answer": 0.977408766746521, "lex_sim": 1.0}
{"question": "Who won the Formula 1 World Championship in 1992?", "answer": ["Nigel Mansell #Reverb10: December 17 \u2013 1/1/10 & Beyond."], "scores": [[0.99267578125, 0.95166015625, 0.99853515625, 0.99365234375, 0.99365234375, 5.781650543212891e-06, 0.000110626220703125, 0.00012600421905517578, 0.00850677490234375, 0.285888671875, 0.023284912109375, 0.2861328125, 0.53662109375, 0.341552734375, 0.02490234375, 0.0183258056640625, 0.11663818359375, 0.255859375, 0.0004093647003173828, 0.3330078125, 0.86376953125, 0.86083984375, 0.034698486328125, 0.0159912109375, 0.1380615234375, 0.99951171875, 0.038421630859375, 0.583984375, 0.03656005859375, 0.841796875, 0.8359375, 0.99951171875, 0.9990234375, 0.283447265625, 0.92333984375, 0.9990234375, 1.0, 0.97607421875, 0.91943359375, 0.67822265625, 0.486572265625, 0.91845703125, 0.99560546875, 0.939453125, 0.99951171875, 0.9990234375, 0.0116729736328125]], "normalized_score": [0.15558481216430664], "tokens": [["\u2581N", "ig", "el", "\u2581Mans", "ell", "\u2581#", "Re", "verb", "1", "0", ":", "\u2581December", "\u2581", "1", "7", "\u2581\u2013", "\u2581", "1", "/", "1", "/", "1", "0", "\u2581&", "\u2581Bey", "ond", ".", "\u2581What", "\u2581are", "\u2581you", "\u2581letting", "\u2581go", "\u2581of", "\u2581in", "\u2581", "2", "0", "1", "0", "?", "\u2581What", "\u2581are", "\u2581you", "\u2581looking", "\u2581forward", "\u2581to", "?\""]], "entropy": [[0.07183837890625, 0.313232421875, 0.015350341796875, 0.06829833984375, 0.04254150390625, 0.90478515625, 5.18359375, 3.951171875, 2.986328125, 2.181640625, 2.36328125, 3.94921875, 1.560546875, 1.8994140625, 2.537109375, 1.6279296875, 4.68359375, 0.98046875, 1.03125, 1.2431640625, 0.81201171875, 0.6669921875, 0.304443359375, 3.43359375, 2.13671875, 0.00519561767578125, 1.94921875, 2.162109375, 2.09375, 0.59521484375, 0.751953125, 0.0053253173828125, 0.0084686279296875, 1.580078125, 0.31982421875, 0.00646209716796875, 0.0018138885498046875, 0.1239013671875, 0.324462890625, 0.91796875, 2.169921875, 0.40966796875, 0.033172607421875, 0.44384765625, 0.003643035888671875, 0.00812530517578125, 0.85595703125]], "avg_entropy": [1.270608496158681], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Nigel Mansell", "score_of_biggest_cluster": 0.15558481216430664, "score_of_first_answer": 0.15558481216430664, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.781650543212891e-06, "avg_score": 0.15558481216430664, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.41616153452678056, "std_of_first_answer": 0.41616153452678056, "avg_entropy_of_first_answer": 1.270608496158681, "lex_sim": 1.0}
{"question": "\"What organisation was founded in 1831 by James Winston, Samuel James Arnold, Samuel Beazley, Sir Andrew Francis Barnard, and Francis Mills to \"\"tend to the regeneration of the Drama\"\"?\"", "answer": ["The Royal Shakespeare Company"], "scores": [[0.8935546875, 0.013214111328125, 0.86328125, 0.97412109375]], "normalized_score": [0.31566837430000305], "tokens": [["\u2581The", "\u2581Royal", "\u2581Shakespeare", "\u2581Company"]], "entropy": [[0.751953125, 0.87890625, 0.7333984375, 0.174072265625]], "avg_entropy": [0.63458251953125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Garrick", "score_of_biggest_cluster": 0.31566837430000305, "score_of_first_answer": 0.31566837430000305, "max_score_of_first_answer": 0.97412109375, "min_score_of_first_answer": 0.013214111328125, "avg_score": 0.31566837430000305, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39056439905668583, "std_of_first_answer": 0.39056439905668583, "avg_entropy_of_first_answer": 0.63458251953125, "lex_sim": 1.0}
{"question": "Which element takes its name from the Persian for 'gold coloured'?", "answer": ["IRIDIUM"], "scores": [[0.000583648681640625, 0.9287109375, 0.9833984375, 0.99951171875]], "normalized_score": [0.15192784368991852], "tokens": [["\u2581IR", "ID", "I", "UM"]], "entropy": [[2.64453125, 0.300048828125, 0.1324462890625, 0.006397247314453125]], "avg_entropy": [0.7708559036254883], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Zirconium salts", "score_of_biggest_cluster": 0.15192784368991852, "score_of_first_answer": 0.15192784368991852, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.000583648681640625, "avg_score": 0.15192784368991852, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42082252595767683, "std_of_first_answer": 0.42082252595767683, "avg_entropy_of_first_answer": 0.7708559036254883, "lex_sim": 1.0}
{"question": "Who was king of England from 1042 to 1066?", "answer": ["Edward the Confessor"], "scores": [[0.818359375, 0.9755859375, 0.9833984375, 0.99951171875]], "normalized_score": [0.9411996603012085], "tokens": [["\u2581Edward", "\u2581the", "\u2581Conf", "essor"]], "entropy": [[0.73388671875, 0.181884765625, 0.1273193359375, 0.005313873291015625]], "avg_entropy": [0.2621011734008789], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "EDWARD the CONFESSOR", "score_of_biggest_cluster": 0.9411996603012085, "score_of_first_answer": 0.9411996603012085, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.818359375, "avg_score": 0.9411996603012085, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.07317247251779435, "std_of_first_answer": 0.07317247251779435, "avg_entropy_of_first_answer": 0.2621011734008789, "lex_sim": 1.0}
{"question": "In which Charles Dickens novel does the character Mr M Choakumchild appear?", "answer": ["\u3007ODD"], "scores": [[0.0526123046875, 2.682209014892578e-06, 0.9560546875, 0.83544921875, 0.0018911361694335938, 0.00021827220916748047]], "normalized_score": [0.005997230764478445], "tokens": [["\u2581", "<0xE3>", "<0x80>", "<0x87>", "O", "DD"]], "entropy": [[2.62109375, 0.4375, 0.284423828125, 0.72900390625, 4.49609375, 1.578125]], "avg_entropy": [1.6910400390625], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Josiah Bounderby", "score_of_biggest_cluster": 0.005997230764478445, "score_of_first_answer": 0.005997230764478445, "max_score_of_first_answer": 0.9560546875, "min_score_of_first_answer": 2.682209014892578e-06, "avg_score": 0.005997230764478445, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4176710197350599, "std_of_first_answer": 0.4176710197350599, "avg_entropy_of_first_answer": 1.6910400390625, "lex_sim": 1.0}
{"question": "Which monarch said 'Will no-one rid me of this turbulent priest'?", "answer": ["King Henry II #TBT, #ThrowBackJesus"], "scores": [[0.8427734375, 0.97021484375, 0.9912109375, 1.1920928955078125e-07, 3.0338764190673828e-05, 0.701171875, 0.0021800994873046875, 0.2496337890625, 0.4375, 0.0177154541015625, 2.199411392211914e-05, 0.53125, 0.98681640625]], "normalized_score": [0.020308559760451317], "tokens": [["\u2581King", "\u2581Henry", "\u2581II", "\u2581#", "TB", "T", ",", "\u2581#", "Throw", "Back", "J", "es", "us"]], "entropy": [[0.69775390625, 0.1773681640625, 0.0692138671875, 1.2041015625, 4.4609375, 1.8857421875, 2.548828125, 4.34765625, 2.41796875, 0.0899658203125, 1.0986328125, 2.0078125, 0.08160400390625]], "avg_entropy": [1.6221219576322115], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Henry II", "score_of_biggest_cluster": 0.020308559760451317, "score_of_first_answer": 0.020308559760451317, "max_score_of_first_answer": 0.9912109375, "min_score_of_first_answer": 1.1920928955078125e-07, "avg_score": 0.020308559760451317, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.40284894973182983, "std_of_first_answer": 0.40284894973182983, "avg_entropy_of_first_answer": 1.6221219576322115, "lex_sim": 1.0}
{"question": "What is the smallest Canadian province", "answer": ["Prince Edward Island (PE)"], "scores": [[0.7607421875, 0.99853515625, 0.99560546875, 0.1544189453125, 0.69873046875, 0.0452880859375]], "normalized_score": [0.39320018887519836], "tokens": [["\u2581Prince", "\u2581Edward", "\u2581Island", "\u2581(", "PE", ")"]], "entropy": [[0.9755859375, 0.01531982421875, 0.035247802734375, 1.31640625, 0.9638671875, 0.196533203125]], "avg_entropy": [0.5838267008463541], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "P.E.I", "score_of_biggest_cluster": 0.39320018887519836, "score_of_first_answer": 0.39320018887519836, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.0452880859375, "avg_score": 0.39320018887519836, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.37786542032238074, "std_of_first_answer": 0.37786542032238074, "avg_entropy_of_first_answer": 0.5838267008463541, "lex_sim": 1.0}
{"question": "A Nychthemeron is a period of how many consecutive hours?", "answer": ["24 hours Driving in Fuerteventura"], "scores": [[0.1925048828125, 0.97802734375, 0.9951171875, 0.30029296875, 5.364418029785156e-07, 0.00011283159255981445, 0.9091796875, 0.01081085205078125, 0.0012178421020507812, 0.015716552734375, 0.99853515625, 0.99951171875]], "normalized_score": [0.03046593628823757], "tokens": [["\u2581", "2", "4", "\u2581hours", "\u2581D", "riv", "ing", "\u2581in", "\u2581F", "uerte", "vent", "ura"]], "entropy": [[1.810546875, 0.1466064453125, 0.043304443359375, 1.6201171875, 1.056640625, 4.01953125, 0.60791015625, 3.470703125, 4.4296875, 3.59375, 0.01305389404296875, 0.0060577392578125]], "avg_entropy": [1.7348257700602214], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "24", "score_of_biggest_cluster": 0.03046593628823757, "score_of_first_answer": 0.03046593628823757, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 5.364418029785156e-07, "avg_score": 0.03046593628823757, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4533370734102656, "std_of_first_answer": 0.4533370734102656, "avg_entropy_of_first_answer": 1.7348257700602214, "lex_sim": 1.0}
{"question": "What is the title of The Beatles first feature film, released in 1964?", "answer": ["A Hard Day's Night"], "scores": [[0.82861328125, 0.96875, 0.9755859375, 0.9951171875, 0.99951171875, 0.98974609375]], "normalized_score": [0.957567036151886], "tokens": [["\u2581A", "\u2581Hard", "\u2581Day", "'", "s", "\u2581Night"]], "entropy": [[0.87353515625, 0.2314453125, 0.1419677734375, 0.044464111328125, 0.0035305023193359375, 0.078125]], "avg_entropy": [0.22884464263916016], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A hard day's night", "score_of_biggest_cluster": 0.957567036151886, "score_of_first_answer": 0.957567036151886, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.82861328125, "avg_score": 0.957567036151886, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.05952588145873409, "std_of_first_answer": 0.05952588145873409, "avg_entropy_of_first_answer": 0.22884464263916016, "lex_sim": 1.0}
{"question": "During the Falklands War which ship was the British flagship?", "answer": ["HMS Invincible"], "scores": [[0.837890625, 0.046630859375, 0.99951171875, 1.0]], "normalized_score": [0.444541335105896], "tokens": [["\u2581HMS", "\u2581Inv", "inci", "ble"]], "entropy": [[0.80029296875, 0.412841796875, 0.0038166046142578125, 0.0005221366882324219]], "avg_entropy": [0.30436837673187256], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "HMS Hermes", "score_of_biggest_cluster": 0.444541335105896, "score_of_first_answer": 0.444541335105896, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.046630859375, "avg_score": 0.444541335105896, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.39491990746405115, "std_of_first_answer": 0.39491990746405115, "avg_entropy_of_first_answer": 0.30436837673187256, "lex_sim": 1.0}
{"question": "In the 1950s which tennis player entered the Wimbledon Ladies Singles three times winning oh each occasion?", "answer": ["Maureen Connolly #FearlessWomen: The Female Activists Whose Strength and Bravery Inspire Us All"], "scores": [[0.326416015625, 0.99951171875, 1.0, 0.97314453125, 0.99755859375, 0.9873046875, 5.960464477539063e-08, 0.0012292861938476562, 0.0007352828979492188, 0.794921875, 0.0033435821533203125, 0.26806640625, 0.0005288124084472656, 0.04595947265625, 0.01552581787109375, 0.99365234375, 9.21487808227539e-05, 0.9345703125, 0.01123809814453125, 0.97265625, 0.014312744140625, 0.003192901611328125, 0.1253662109375, 0.650390625, 0.9970703125, 0.223388671875, 0.9951171875, 0.00911712646484375, 0.87060546875, 0.0631103515625]], "normalized_score": [0.051065053790807724], "tokens": [["\u2581Ma", "ure", "en", "\u2581Con", "n", "olly", "\u2581#", "F", "ear", "less", "W", "omen", ":", "\u2581The", "\u2581Fem", "ale", "\u2581Activ", "ists", "\u2581Wh", "ose", "\u2581Str", "ength", "\u2581and", "\u2581Bra", "very", "\u2581In", "sp", "ire", "\u2581Us", "\u2581All"]], "entropy": [[2.3828125, 0.005340576171875, 0.0004525184631347656, 0.2626953125, 0.017425537109375, 0.08782958984375, 0.93994140625, 3.380859375, 4.953125, 1.2265625, 3.759765625, 0.93798828125, 2.025390625, 4.6953125, 4.50390625, 0.04119873046875, 3.380859375, 0.26123046875, 2.921875, 0.2049560546875, 1.4306640625, 0.487060546875, 1.517578125, 1.6337890625, 0.02581787109375, 1.69140625, 0.0321044921875, 0.057647705078125, 0.56982421875, 3.337890625]], "avg_entropy": [1.5591103394826253], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Maureen Connolly Brinker", "score_of_biggest_cluster": 0.051065053790807724, "score_of_first_answer": 0.051065053790807724, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.051065053790807724, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.44324071093283074, "std_of_first_answer": 0.44324071093283074, "avg_entropy_of_first_answer": 1.5591103394826253, "lex_sim": 1.0}
{"question": "Ursus arctos horribilis is the scientific name of which animal?", "answer": ["Grizzly Bear"], "scores": [[0.58349609375, 0.9990234375, 1.0, 0.9990234375, 0.225830078125]], "normalized_score": [0.666490912437439], "tokens": [["\u2581G", "ri", "zz", "ly", "\u2581Bear"]], "entropy": [[1.43359375, 0.0093536376953125, 7.730722427368164e-05, 0.00980377197265625, 0.90185546875]], "avg_entropy": [0.4709367871284485], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Silvertip Bear", "score_of_biggest_cluster": 0.666490912437439, "score_of_first_answer": 0.666490912437439, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.225830078125, "avg_score": 0.666490912437439, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3125202935305293, "std_of_first_answer": 0.3125202935305293, "avg_entropy_of_first_answer": 0.4709367871284485, "lex_sim": 1.0}
{"question": "The Humber bridge forms part of which A road?", "answer": ["A15."], "scores": [[0.423583984375, 0.82421875, 0.87451171875, 0.025238037109375, 1.7881393432617188e-06, 0.005970001220703125, 0.007404327392578125, 0.061431884765625, 0.17919921875, 2.4437904357910156e-06, 0.033538818359375, 0.0025177001953125, 0.041717529296875, 0.258544921875, 1.0, 0.09722900390625, 0.00952911376953125, 0.053375244140625, 0.97265625, 0.95068359375]], "normalized_score": [0.03102434240281582], "tokens": [["\u2581A", "1", "5", ".", "\u2581#", "N", "Y", "F", "W", "\u2581Fe", "atures", "\u2581D", "iverse", "\u2581Mod", "els", "\u2581and", "\u2581Creative", "\u2581Coll", "abor", "ations"]], "entropy": [[1.8349609375, 0.61962890625, 0.67431640625, 1.0634765625, 0.57080078125, 6.0859375, 1.556640625, 4.17578125, 3.341796875, 1.6767578125, 1.666015625, 4.50390625, 3.857421875, 3.095703125, 0.0004699230194091797, 2.91796875, 1.79296875, 2.900390625, 0.1339111328125, 0.207763671875]], "avg_entropy": [2.1338308691978454], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "A15", "score_of_biggest_cluster": 0.03102434240281582, "score_of_first_answer": 0.03102434240281582, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.7881393432617188e-06, "avg_score": 0.03102434240281582, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3806379993090029, "std_of_first_answer": 0.3806379993090029, "avg_entropy_of_first_answer": 2.1338308691978454, "lex_sim": 1.0}
{"question": "The moons of which planet are named after Shakespearian characters?", "answer": ["Uranus"], "scores": [[0.86328125, 1.0, 0.9990234375]], "normalized_score": [0.9518663883209229], "tokens": [["\u2581U", "ran", "us"]], "entropy": [[0.814453125, 0.00113677978515625, 0.0097503662109375]], "avg_entropy": [0.27511342366536456], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Magnetosphere of Uranus", "score_of_biggest_cluster": 0.9518663883209229, "score_of_first_answer": 0.9518663883209229, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.86328125, "avg_score": 0.9518663883209229, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.06422089634237621, "std_of_first_answer": 0.06422089634237621, "avg_entropy_of_first_answer": 0.27511342366536456, "lex_sim": 1.0}
{"question": "What kind of singing, from the Italian for Manner of the Church, features singers without any musical accompaniment?", "answer": ["Gregorian chant 13 June 2007 tbs.admin News (EN)"], "scores": [[0.90625, 0.99658203125, 0.998046875, 0.390380859375, 2.384185791015625e-05, 0.08343505859375, 0.0283050537109375, 1.4185905456542969e-05, 0.8486328125, 0.2139892578125, 0.89208984375, 0.2352294921875, 0.07415771484375, 0.00010567903518676758, 5.906820297241211e-05, 0.0960693359375, 0.00713348388671875, 7.62939453125e-06, 0.007659912109375, 0.00411224365234375, 0.8359375]], "normalized_score": [0.01895909570157528], "tokens": [["\u2581Greg", "or", "ian", "\u2581chant", "\u2581", "1", "3", "\u2581June", "\u2581", "2", "0", "0", "7", "\u2581t", "bs", ".", "admin", "\u2581News", "\u2581(", "EN", ")"]], "entropy": [[0.57763671875, 0.0335693359375, 0.020782470703125, 1.3193359375, 0.98974609375, 4.45703125, 2.880859375, 3.08984375, 0.84912109375, 0.70166015625, 0.5986328125, 1.0869140625, 2.26953125, 3.716796875, 1.7978515625, 4.59375, 4.6484375, 1.986328125, 5.55078125, 5.578125, 0.9384765625]], "avg_entropy": [2.270724341982887], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Acappella", "score_of_biggest_cluster": 0.01895909570157528, "score_of_first_answer": 0.01895909570157528, "max_score_of_first_answer": 0.998046875, "min_score_of_first_answer": 7.62939453125e-06, "avg_score": 0.01895909570157528, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3913188826051235, "std_of_first_answer": 0.3913188826051235, "avg_entropy_of_first_answer": 2.270724341982887, "lex_sim": 1.0}
{"question": "\u2018Steps in Time\u2019 is a 1959 autobiography by which American singer/actor?", "answer": ["Fred Astaire"], "scores": [[0.93896484375, 0.99462890625, 0.99951171875, 1.0]], "normalized_score": [0.9829344749450684], "tokens": [["\u2581Fred", "\u2581A", "sta", "ire"]], "entropy": [[0.46923828125, 0.04998779296875, 0.00331878662109375, 0.00079345703125]], "avg_entropy": [0.13083457946777344], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Fred Astair", "score_of_biggest_cluster": 0.9829344749450684, "score_of_first_answer": 0.9829344749450684, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.93896484375, "avg_score": 0.9829344749450684, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.02566932891274623, "std_of_first_answer": 0.02566932891274623, "avg_entropy_of_first_answer": 0.13083457946777344, "lex_sim": 1.0}
{"question": "This quiz is printed on A4 paper which has a length of 297mm. How wide, in millimetres, is A4 paper?", "answer": ["A4 paper is 210mm by 297mm."], "scores": [[0.07958984375, 0.99267578125, 0.97900390625, 0.72216796875, 0.9609375, 0.99951171875, 0.9814453125, 0.99169921875, 0.7880859375, 0.0008025169372558594, 0.98779296875, 0.9990234375, 0.99951171875, 1.0, 0.99658203125, 0.406982421875, 1.1920928955078125e-07, 3.4570693969726562e-06]], "normalized_score": [0.10987219959497452], "tokens": [["\u2581A", "4", "\u2581paper", "\u2581is", "\u2581", "2", "1", "0", "mm", "\u2581by", "\u2581", "2", "9", "7", "mm", ".", "\u2581###", "Background"]], "entropy": [[0.77490234375, 0.0740966796875, 0.1456298828125, 0.8759765625, 0.2783203125, 0.00566864013671875, 0.09423828125, 0.055938720703125, 0.69873046875, 0.61962890625, 0.1011962890625, 0.01158905029296875, 0.0033721923828125, 0.0015115737915039062, 0.0243988037109375, 1.580078125, 1.017578125, 5.18359375]], "avg_entropy": [0.6414693726433648], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "two hundred and ten", "score_of_biggest_cluster": 0.10987219959497452, "score_of_first_answer": 0.10987219959497452, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.1920928955078125e-07, "avg_score": 0.10987219959497452, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3992255123393251, "std_of_first_answer": 0.3992255123393251, "avg_entropy_of_first_answer": 0.6414693726433648, "lex_sim": 1.0}
{"question": "What nationality is Louis Oosthuizen the 2010 Open Golf Champion?", "answer": ["South African #NewRelease #KindleUnlimited #Giveaway - Love and Other Games by M.J. Fields"], "scores": [[0.93115234375, 0.9873046875, 1.1324882507324219e-06, 0.00027060508728027344, 2.9802322387695312e-06, 0.1256103515625, 0.00015103816986083984, 0.99609375, 0.837890625, 0.99951171875, 0.68408203125, 0.00878143310546875, 0.0914306640625, 0.97509765625, 0.0125274658203125, 0.0011081695556640625, 0.049163818359375, 0.2320556640625, 0.157470703125, 0.564453125, 0.00157928466796875, 0.3486328125, 0.09197998046875, 0.99609375, 0.018280029296875, 0.8876953125]], "normalized_score": [0.03746374323964119], "tokens": [["\u2581South", "\u2581African", "\u2581#", "New", "Release", "\u2581#", "Kind", "le", "Un", "limited", "\u2581#", "G", "ive", "away", "\u2581-", "\u2581Love", "\u2581and", "\u2581Other", "\u2581Games", "\u2581by", "\u2581M", ".", "J", ".", "\u2581Field", "s"]], "entropy": [[0.330322265625, 0.09320068359375, 0.72119140625, 5.73046875, 3.583984375, 2.40625, 5.59765625, 0.0438232421875, 0.98974609375, 0.006229400634765625, 1.27734375, 3.830078125, 1.96484375, 0.177490234375, 2.576171875, 5.69140625, 3.75, 4.36328125, 4.234375, 1.6337890625, 1.8115234375, 2.361328125, 3.419921875, 0.041168212890625, 4.57421875, 0.53466796875]], "avg_entropy": [2.3747876974252553], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "South africa", "score_of_biggest_cluster": 0.03746374323964119, "score_of_first_answer": 0.03746374323964119, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 1.1324882507324219e-06, "avg_score": 0.03746374323964119, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.41340696587157333, "std_of_first_answer": 0.41340696587157333, "avg_entropy_of_first_answer": 2.3747876974252553, "lex_sim": 1.0}
{"question": "Louis Smith won an individual silver medal in gymnastics at the 2012 London Olympics. For what discipline?", "answer": ["pommel horse \\o/ This has to be the best fanart of Alex & Sadiq made by an amazing artist!"], "scores": [[0.3896484375, 0.9970703125, 0.986328125, 0.9755859375, 5.304813385009766e-06, 0.000865936279296875, 0.81689453125, 1.3232231140136719e-05, 0.00238800048828125, 0.0059051513671875, 0.82373046875, 0.46630859375, 0.222412109375, 1.6510486602783203e-05, 0.01345062255859375, 0.264404296875, 0.001216888427734375, 0.0023193359375, 0.00904083251953125, 0.004810333251953125, 0.78125, 0.0031909942626953125, 0.38037109375, 0.036163330078125, 0.038421630859375, 0.99462890625, 0.68310546875, 0.1612548828125]], "normalized_score": [0.0276483204215765], "tokens": [["\u2581p", "omm", "el", "\u2581horse", "\u2581\\", "o", "/", "\u2581This", "\u2581has", "\u2581to", "\u2581be", "\u2581the", "\u2581best", "\u2581fan", "art", "\u2581of", "\u2581Alex", "\u2581&", "\u2581S", "adi", "q", "\u2581made", "\u2581by", "\u2581an", "\u2581amaz", "ing", "\u2581artist", "!"]], "entropy": [[0.939453125, 0.021942138671875, 0.102783203125, 0.1708984375, 0.7861328125, 2.962890625, 0.7578125, 0.93603515625, 3.484375, 2.017578125, 0.80078125, 2.53125, 2.99609375, 4.22265625, 3.779296875, 2.685546875, 4.4296875, 4.5234375, 5.4765625, 4.38671875, 1.3271484375, 3.23828125, 2.279296875, 3.779296875, 3.02734375, 0.038055419921875, 1.9443359375, 4.0234375]], "avg_entropy": [2.416754586356027], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Gymnastics pommel horse", "score_of_biggest_cluster": 0.0276483204215765, "score_of_first_answer": 0.0276483204215765, "max_score_of_first_answer": 0.9970703125, "min_score_of_first_answer": 5.304813385009766e-06, "avg_score": 0.0276483204215765, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3808479415615506, "std_of_first_answer": 0.3808479415615506, "avg_entropy_of_first_answer": 2.416754586356027, "lex_sim": 1.0}
{"question": "After 23 years on the air, America's Most Wanted will be winding down. Who has been the host of the series since its inception?", "answer": ["John Walsh"], "scores": [[0.9326171875, 0.96337890625, 1.0]], "normalized_score": [0.9649397134780884], "tokens": [["\u2581John", "\u2581Wal", "sh"]], "entropy": [[0.61279296875, 0.307373046875, 0.00011289119720458984]], "avg_entropy": [0.30675963560740155], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Walsh, John", "score_of_biggest_cluster": 0.9649397134780884, "score_of_first_answer": 0.9649397134780884, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.9326171875, "avg_score": 0.9649397134780884, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.02754356401595792, "std_of_first_answer": 0.02754356401595792, "avg_entropy_of_first_answer": 0.30675963560740155, "lex_sim": 1.0}
{"question": "October 27, 1858 saw the birth of what totally bad assed US president, the 26th, who spent time as a North Dakota rancher before becoming assistant Secretary of the Navy in 1899?", "answer": ["Theodore \"T.R.\""], "scores": [[0.316650390625, 0.99755859375, 0.1566162109375, 0.958984375, 0.30078125, 0.9931640625, 0.98681640625, 0.9765625, 0.99951171875, 0.99853515625, 1.0, 0.36376953125, 0.8583984375]], "normalized_score": [0.6570804715156555], "tokens": [["\u2581The", "odore", "\u2581\"", "T", ".", "R", ".\"", "\u2581Ro", "ose", "vel", "t", "\u2581Jr", "."]], "entropy": [[2.830078125, 0.0186767578125, 0.70361328125, 0.2357177734375, 0.76123046875, 0.044464111328125, 0.08905029296875, 0.1727294921875, 0.0039215087890625, 0.01082611083984375, 0.00107574462890625, 1.4208984375, 0.5517578125]], "avg_entropy": [0.5264646089993991], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "T Ros", "score_of_biggest_cluster": 0.6570804715156555, "score_of_first_answer": 0.6570804715156555, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.1566162109375, "avg_score": 0.6570804715156555, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.32329318267727375, "std_of_first_answer": 0.32329318267727375, "avg_entropy_of_first_answer": 0.5264646089993991, "lex_sim": 1.0}
{"question": "Who originally looked after the Blue Peter garden", "answer": ["John Noakes Chapter 1: The Prophet\u2019s Life & Family"], "scores": [[0.2183837890625, 0.9599609375, 0.9990234375, 0.0, 0.353271484375, 0.369873046875, 0.11541748046875, 0.10162353515625, 0.0004069805145263672, 0.0958251953125, 0.892578125, 0.00867462158203125, 0.99853515625, 0.01690673828125, 0.010467529296875, 0.0276641845703125]], "normalized_score": [0.0], "tokens": [["\u2581John", "\u2581No", "akes", "\u2581Chapter", "\u2581", "1", ":", "\u2581The", "\u2581Pro", "ph", "et", "\u2019", "s", "\u2581Life", "\u2581&", "\u2581Family"]], "entropy": [[3.49609375, 0.281005859375, 0.0113067626953125, 1.005859375, 2.875, 1.8173828125, 3.568359375, 5.01953125, 5.79296875, 2.197265625, 0.450927734375, 4.03125, 0.0176849365234375, 5.25, 3.2890625, 3.53125]], "avg_entropy": [2.664684295654297], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Percy Thrower", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3862517712277523, "std_of_first_answer": 0.3862517712277523, "avg_entropy_of_first_answer": 2.664684295654297, "lex_sim": 1.0}
{"question": "\"The name of what tennis star was used by a commentator for the the hole cards Ace\u2013King (unsuited) of Texas Hold 'em as they meet the criteria of \"\"looks great but never wins\"\"?\"", "answer": ["Roger Federer #148541 - 07/20/07 01:39 AM Re: Help!"], "scores": [[0.433837890625, 0.98828125, 0.98291015625, 2.8014183044433594e-06, 0.0518798828125, 0.026458740234375, 0.01171875, 0.01387786865234375, 0.053497314453125, 0.061767578125, 0.0009975433349609375, 0.083740234375, 0.06549072265625, 0.053009033203125, 0.2086181640625, 0.287841796875, 0.1591796875, 0.80712890625, 0.049560546875, 0.125732421875, 0.0960693359375, 0.34765625, 0.11468505859375, 0.974609375, 0.1845703125, 0.09033203125, 0.026153564453125, 0.00024127960205078125, 0.96142578125, 0.0002627372741699219, 0.01678466796875, 0.0034275054931640625, 0.0021572113037109375, 0.0595703125, 0.00335693359375, 0.353515625, 0.99560546875, 0.051849365234375, 0.69873046875, 0.9990234375, 1.0, 0.004497528076171875, 0.01456451416015625, 0.005687713623046875, 0.80029296875]], "normalized_score": [0.05156290903687477], "tokens": [["\u2581Roger", "\u2581Feder", "er", "\u2581#", "1", "4", "8", "5", "4", "1", "\u2581-", "\u2581", "0", "7", "/", "2", "0", "/", "0", "7", "\u2581", "0", "1", ":", "3", "9", "\u2581AM", "\u2581Re", ":", "\u2581Help", "!", "\u2581Need", "\u2581advice", "\u2581from", "\u2581married", "\u2581cou", "ples", "!", "\u2581[", "Re", ":", "\u2581C", "J", "N", "]"]], "entropy": [[2.6484375, 0.100830078125, 0.1195068359375, 0.92578125, 5.35546875, 3.529296875, 2.75, 2.873046875, 3.107421875, 2.875, 2.931640625, 6.21484375, 2.25, 3.00390625, 3.375, 1.3935546875, 2.30859375, 0.90478515625, 0.7890625, 2.224609375, 2.966796875, 1.6787109375, 2.2578125, 0.2027587890625, 1.802734375, 2.3125, 1.4677734375, 3.150390625, 0.3017578125, 5.1796875, 3.4296875, 3.966796875, 4.85546875, 3.013671875, 4.57421875, 2.537109375, 0.035247802734375, 3.154296875, 1.759765625, 0.008270263671875, 0.0025424957275390625, 5.1328125, 5.5390625, 2.943359375, 1.29296875]], "avg_entropy": [2.5165997399224174], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u00c1nna Serg\u00e9yevna K\u00farnikova", "score_of_biggest_cluster": 0.05156290903687477, "score_of_first_answer": 0.05156290903687477, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.8014183044433594e-06, "avg_score": 0.05156290903687477, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3633529410014296, "std_of_first_answer": 0.3633529410014296, "avg_entropy_of_first_answer": 2.5165997399224174, "lex_sim": 1.0}
{"question": "Which footballer scored a record six international hat- tricks for England?", "answer": ["Sir Bobby Charlton"], "scores": [[0.59521484375, 0.5791015625, 0.99609375, 1.0, 1.0]], "normalized_score": [0.8075059652328491], "tokens": [["\u2581Sir", "\u2581Bobby", "\u2581Char", "l", "ton"]], "entropy": [[1.8466796875, 1.8046875, 0.036285400390625, 0.00112152099609375, 0.0012292861938476562]], "avg_entropy": [0.7380006790161133], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Greavsie", "score_of_biggest_cluster": 0.8075059652328491, "score_of_first_answer": 0.8075059652328491, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.5791015625, "avg_score": 0.8075059652328491, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.20168188536837459, "std_of_first_answer": 0.20168188536837459, "avg_entropy_of_first_answer": 0.7380006790161133, "lex_sim": 1.0}
{"question": "What is the residence of the Duke of Devonshire?", "answer": ["\uff24\uff35\uff2b\uff25\uff0f\uff24\uff25\uff36\uff2f\uff2e\uff33\uff28\uff29\uff32\uff25"], "scores": [[0.0015211105346679688, 0.0035400390625, 0.7724609375, 0.003589630126953125, 0.85205078125, 0.9990234375, 0.92431640625, 0.99951171875, 1.0, 0.8759765625, 0.99755859375, 1.0, 0.99951171875, 0.32568359375, 1.0, 0.0007767677307128906, 0.97802734375, 1.0, 0.83349609375, 1.0, 1.0, 0.9951171875, 0.9990234375, 1.0, 0.99755859375, 0.99951171875, 1.0, 0.99462890625, 0.99951171875, 1.0, 0.998046875, 0.99951171875, 1.0, 0.99169921875, 0.99951171875, 1.0, 0.99951171875, 0.990234375, 1.0, 0.99755859375, 0.99951171875, 1.0, 0.9697265625, 0.9990234375, 1.0, 1.0]], "normalized_score": [0.5565868616104126], "tokens": [["\u2581", "<0xEF>", "<0xBC>", "<0xA4>", "<0xEF>", "<0xBC>", "<0xB5>", "<0xEF>", "<0xBC>", "<0xAB>", "<0xEF>", "<0xBC>", "<0xA5>", "<0xEF>", "<0xBC>", "<0x8F>", "<0xEF>", "<0xBC>", "<0xA4>", "<0xEF>", "<0xBC>", "<0xA5>", "<0xEF>", "<0xBC>", "<0xB6>", "<0xEF>", "<0xBC>", "<0xAF>", "<0xEF>", "<0xBC>", "<0xAE>", "<0xEF>", "<0xBC>", "<0xB3>", "<0xEF>", "<0xBC>", "<0xA8>", "<0xEF>", "<0xBC>", "<0xA9>", "<0xEF>", "<0xBC>", "<0xB2>", "<0xEF>", "<0xBC>", "<0xA5>"]], "entropy": [[0.40283203125, 2.3515625, 1.005859375, 1.466796875, 0.7333984375, 0.007656097412109375, 0.424560546875, 0.004253387451171875, 0.0010004043579101562, 0.52978515625, 0.0223236083984375, 1.2993812561035156e-05, 0.0037441253662109375, 1.248046875, 4.106760025024414e-05, 2.056640625, 0.191162109375, 0.00013327598571777344, 0.8037109375, 0.00258636474609375, 0.00010526180267333984, 0.039459228515625, 0.0097503662109375, 6.031990051269531e-05, 0.0246429443359375, 0.00677490234375, 3.600120544433594e-05, 0.045166015625, 0.0036563873291015625, 5.739927291870117e-05, 0.0212554931640625, 0.006488800048828125, 1.2278556823730469e-05, 0.07708740234375, 0.0046539306640625, 5.781650543212891e-06, 0.007099151611328125, 0.08062744140625, 2.3484230041503906e-05, 0.02227783203125, 0.005573272705078125, 0.0001761913299560547, 0.1915283203125, 0.00981903076171875, 3.224611282348633e-05, 0.0014495849609375]], "avg_entropy": [0.25682447526765906], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chatsworth Library", "score_of_biggest_cluster": 0.5565868616104126, "score_of_first_answer": 0.5565868616104126, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0007767677307128906, "avg_score": 0.5565868616104126, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.2912351722644879, "std_of_first_answer": 0.2912351722644879, "avg_entropy_of_first_answer": 0.25682447526765906, "lex_sim": 0.0}
{"question": "Of which US state is Springfield the capital?", "answer": ["EGOP THE LATEST ON DRAFT 28/06/2012"], "scores": [[0.0036373138427734375, 0.00016295909881591797, 0.0019350051879882812, 2.5033950805664062e-06, 0.00461578369140625, 0.0261993408203125, 0.5322265625, 0.01727294921875, 0.0003216266632080078, 0.04486083984375, 0.10809326171875, 0.0177764892578125, 0.22216796875, 0.025238037109375, 0.0750732421875, 0.4033203125, 0.03826904296875, 0.95849609375, 0.50732421875, 0.7861328125, 0.0806884765625, 0.055694580078125]], "normalized_score": [0.0230883602052927], "tokens": [["\u2581E", "G", "OP", "\u2581THE", "\u2581L", "ATE", "ST", "\u2581ON", "\u2581D", "RA", "FT", "\u2581", "2", "8", "/", "0", "6", "/", "2", "0", "1", "2"]], "entropy": [[1.908203125, 0.525390625, 1.5849609375, 3.470703125, 3.4453125, 3.513671875, 2.390625, 4.40234375, 0.7412109375, 4.890625, 1.765625, 4.3515625, 2.279296875, 3.158203125, 4.55859375, 2.205078125, 1.884765625, 0.34521484375, 1.4453125, 0.82177734375, 1.068359375, 1.953125]], "avg_entropy": [2.395907315340909], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Transportation in Illinois", "score_of_biggest_cluster": 0.0230883602052927, "score_of_first_answer": 0.0230883602052927, "max_score_of_first_answer": 0.95849609375, "min_score_of_first_answer": 2.5033950805664062e-06, "avg_score": 0.0230883602052927, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.27194877172592746, "std_of_first_answer": 0.27194877172592746, "avg_entropy_of_first_answer": 2.395907315340909, "lex_sim": 1.0}
{"question": "After an earlier experiment by AOL's QuantumLink Serial what was the episodic online story (1995-7) which first included paid advertising banners and product placement?", "answer": ["Online soap opera"], "scores": [[0.0023288726806640625, 0.320068359375, 0.9775390625]], "normalized_score": [0.08998585492372513], "tokens": [["\u2581Online", "\u2581soap", "\u2581opera"]], "entropy": [[3.404296875, 3.314453125, 0.1553955078125]], "avg_entropy": [2.2913818359375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The Spot, or thespot.com", "score_of_biggest_cluster": 0.08998585492372513, "score_of_first_answer": 0.08998585492372513, "max_score_of_first_answer": 0.9775390625, "min_score_of_first_answer": 0.0023288726806640625, "avg_score": 0.08998585492372513, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4061008387635223, "std_of_first_answer": 0.4061008387635223, "avg_entropy_of_first_answer": 2.2913818359375, "lex_sim": 1.0}
{"question": "\"Where would one find \"\"Bachmann's bundle\"\"?\"", "answer": ["I'm sorry, but I'm not able to understand the context of your question."], "scores": [[0.00482940673828125, 0.6171875, 0.99853515625, 0.91650390625, 0.97607421875, 0.32763671875, 0.92724609375, 0.13330078125, 0.99951171875, 0.8525390625, 0.1480712890625, 0.99951171875, 0.07440185546875, 0.60498046875, 0.72021484375, 0.75927734375, 0.7509765625, 0.9892578125, 0.9765625, 0.60693359375, 0.99853515625, 0.8876953125, 0.9697265625, 0.9560546875, 0.97998046875, 0.70068359375, 0.148681640625, 0.99072265625, 0.9990234375, 0.8427734375, 0.978515625, 0.9658203125]], "normalized_score": [0.5618922114372253], "tokens": [["\u2581I", "'", "m", "\u2581sorry", ",", "\u2581but", "\u2581I", "'", "m", "\u2581not", "\u2581able", "\u2581to", "\u2581understand", "\u2581the", "\u2581context", "\u2581of", "\u2581your", "\u2581question", ".", "\u2581Could", "\u2581you", "\u2581please", "\u2581provide", "\u2581more", "\u2581information", "\u2581or", "\u2581re", "phr", "ase", "\u2581your", "\u2581question", "?"]], "entropy": [[3.1640625, 1.4111328125, 0.01213836669921875, 0.343994140625, 0.1292724609375, 0.78369140625, 0.410888671875, 1.529296875, 0.0034503936767578125, 0.460693359375, 1.1982421875, 0.005153656005859375, 1.759765625, 1.0888671875, 1.2470703125, 0.6103515625, 0.80029296875, 0.07904052734375, 0.1588134765625, 0.94384765625, 0.01189422607421875, 0.3720703125, 0.1785888671875, 0.2227783203125, 0.12420654296875, 0.8642578125, 1.0166015625, 0.0665283203125, 0.00766754150390625, 0.5869140625, 0.1273193359375, 0.181884765625]], "avg_entropy": [0.6218993067741394], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Four chambered heart", "score_of_biggest_cluster": 0.5618922114372253, "score_of_first_answer": 0.5618922114372253, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.00482940673828125, "avg_score": 0.5618922114372253, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.31736133266905375, "std_of_first_answer": 0.31736133266905375, "avg_entropy_of_first_answer": 0.6218993067741394, "lex_sim": 1.0}
{"question": "What is the darkly common name for the plague which killed 30-50% of Europe between 1347-52?", "answer": ["Black Death"], "scores": [[0.57958984375, 0.78076171875]], "normalized_score": [0.6726972460746765], "tokens": [["\u2581Black", "\u2581Death"]], "entropy": [[1.58203125, 0.99951171875]], "avg_entropy": [1.290771484375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Black death", "score_of_biggest_cluster": 0.6726972460746765, "score_of_first_answer": 0.6726972460746765, "max_score_of_first_answer": 0.78076171875, "min_score_of_first_answer": 0.57958984375, "avg_score": 0.6726972460746765, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.1005859375, "std_of_first_answer": 0.1005859375, "avg_entropy_of_first_answer": 1.290771484375, "lex_sim": 1.0}
{"question": "Which King of England is buried in Gloucester Cathedral?", "answer": ["Henry I"], "scores": [[0.10919189453125, 0.11663818359375]], "normalized_score": [0.11285363882780075], "tokens": [["\u2581Henry", "\u2581I"]], "entropy": [[1.1591796875, 2.013671875]], "avg_entropy": [1.58642578125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Edward II, King of the English", "score_of_biggest_cluster": 0.11285363882780075, "score_of_first_answer": 0.11285363882780075, "max_score_of_first_answer": 0.11663818359375, "min_score_of_first_answer": 0.10919189453125, "avg_score": 0.11285363882780075, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.00372314453125, "std_of_first_answer": 0.00372314453125, "avg_entropy_of_first_answer": 1.58642578125, "lex_sim": 1.0}
{"question": "Who was European Footballer of the Year for 1973 and 1974?", "answer": ["\u3014Taxi\u3015 G. BAIXE #MeetTheMaker: Linda from Ecoegg"], "scores": [[0.0174560546875, 2.562999725341797e-06, 0.708984375, 0.0145416259765625, 0.0015697479248046875, 0.00289154052734375, 0.9501953125, 1.0, 0.998046875, 0.061798095703125, 0.01166534423828125, 0.08544921875, 7.236003875732422e-05, 0.00569915771484375, 0.1103515625, 0.00077056884765625, 0.00023090839385986328, 0.0005364418029785156, 0.0545654296875, 0.016632080078125, 0.01348876953125, 0.068359375, 0.0026111602783203125, 0.0206146240234375, 0.0003788471221923828, 0.00421905517578125, 0.43310546875, 0.00688934326171875, 0.9130859375]], "normalized_score": [0.012707669287919998], "tokens": [["\u2581", "<0xE3>", "<0x80>", "<0x94>", "T", "axi", "<0xE3>", "<0x80>", "<0x95>", "\u2581G", ".", "\u2581B", "A", "IX", "E", "\u2581#", "Me", "et", "The", "M", "aker", ":", "\u2581L", "inda", "\u2581from", "\u2581E", "co", "eg", "g"]], "entropy": [[3.083984375, 0.99365234375, 1.3251953125, 0.353759765625, 4.70703125, 3.62890625, 0.4365234375, 0.0003883838653564453, 0.01763916015625, 4.78515625, 3.431640625, 4.3828125, 2.615234375, 1.7958984375, 2.57421875, 4.09375, 5.08203125, 4.1171875, 4.9453125, 3.1171875, 3.0859375, 4.21875, 4.91796875, 5.015625, 5.31640625, 3.6640625, 3.255859375, 4.26171875, 0.45703125]], "avg_entropy": [3.0924437703757452], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hendrik Johannes Cruijff", "score_of_biggest_cluster": 0.012707669287919998, "score_of_first_answer": 0.012707669287919998, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.562999725341797e-06, "avg_score": 0.012707669287919998, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3429503257491419, "std_of_first_answer": 0.3429503257491419, "avg_entropy_of_first_answer": 3.0924437703757452, "lex_sim": 1.0}
{"question": "What is added to a martini to make a 'Bronx' cocktail?", "answer": ["A shot of maraschino liqueur #MeToo comes to India: Women lead charge against harassment at the workplace"], "scores": [[0.06768798828125, 0.6884765625, 0.9951171875, 0.10675048828125, 0.99853515625, 1.0, 0.9990234375, 0.96826171875, 0.974609375, 0.99951171875, 1.1920928955078125e-07, 0.0001531839370727539, 0.0750732421875, 0.99755859375, 1.0669231414794922e-05, 0.4765625, 2.765655517578125e-05, 0.034210205078125, 0.0110626220703125, 0.00033402442932128906, 0.08538818359375, 0.873046875, 0.148193359375, 0.99951171875, 0.9873046875, 0.01532745361328125, 0.026458740234375, 0.6044921875, 0.99462890625]], "normalized_score": [0.055035192519426346], "tokens": [["\u2581A", "\u2581shot", "\u2581of", "\u2581mar", "as", "ch", "ino", "\u2581li", "que", "ur", "\u2581#", "Me", "To", "o", "\u2581comes", "\u2581to", "\u2581India", ":", "\u2581Women", "\u2581lead", "\u2581charge", "\u2581against", "\u2581har", "ass", "ment", "\u2581at", "\u2581the", "\u2581work", "place"]], "entropy": [[2.97265625, 1.8916015625, 0.038055419921875, 3.9765625, 0.01751708984375, 0.00025582313537597656, 0.00818634033203125, 0.19970703125, 0.12548828125, 0.003753662109375, 0.90185546875, 5.60546875, 2.517578125, 0.02581787109375, 2.83984375, 2.466796875, 0.71826171875, 3.84375, 4.62109375, 2.994140625, 2.546875, 0.72509765625, 1.375, 0.003108978271484375, 0.08160400390625, 3.453125, 3.671875, 2.294921875, 0.040313720703125]], "avg_entropy": [1.7227693837264488], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Orange juice", "score_of_biggest_cluster": 0.055035192519426346, "score_of_first_answer": 0.055035192519426346, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 1.1920928955078125e-07, "avg_score": 0.055035192519426346, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4483579046979223, "std_of_first_answer": 0.4483579046979223, "avg_entropy_of_first_answer": 1.7227693837264488, "lex_sim": 1.0}
{"question": "Which musical features the nurse Nellie Forbush?", "answer": ["South Pacific (musical)"], "scores": [[0.9111328125, 0.99169921875, 0.0033206939697265625, 0.52294921875, 1.0, 0.98974609375]], "normalized_score": [0.3403002917766571], "tokens": [["\u2581South", "\u2581Pacific", "\u2581(", "mus", "ical", ")"]], "entropy": [[0.50439453125, 0.0703125, 0.78515625, 2.080078125, 0.0020885467529296875, 0.0792236328125]], "avg_entropy": [0.5868755976359049], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "The South Pacific", "score_of_biggest_cluster": 0.3403002917766571, "score_of_first_answer": 0.3403002917766571, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0033206939697265625, "avg_score": 0.3403002917766571, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.36795491607140024, "std_of_first_answer": 0.36795491607140024, "avg_entropy_of_first_answer": 0.5868755976359049, "lex_sim": 1.0}
{"question": "Give the surname of the classical music piano soloist sisters, Katia and Marielle?", "answer": ["Labeque"], "scores": [[0.833984375, 0.9921875, 0.99853515625]], "normalized_score": [0.9383647441864014], "tokens": [["\u2581L", "abe", "que"]], "entropy": [[0.90673828125, 0.06402587890625, 0.01326751708984375]], "avg_entropy": [0.32801055908203125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Katia and Marielle Lab\u00e8que", "score_of_biggest_cluster": 0.9383647441864014, "score_of_first_answer": 0.9383647441864014, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.833984375, "avg_score": 0.9383647441864014, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.07611795021881, "std_of_first_answer": 0.07611795021881, "avg_entropy_of_first_answer": 0.32801055908203125, "lex_sim": 1.0}
{"question": "When was the first road speed limit set in the UK for powered vehicles?", "answer": ["20 mph"], "scores": [[0.61328125, 0.0065155029296875, 0.380859375, 0.1265869140625, 0.96875]], "normalized_score": [0.1795538067817688], "tokens": [["\u2581", "2", "0", "\u2581m", "ph"]], "entropy": [[2.03515625, 0.17138671875, 2.21875, 3.126953125, 0.1810302734375]], "avg_entropy": [1.5466552734375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "1861", "score_of_biggest_cluster": 0.1795538067817688, "score_of_first_answer": 0.1795538067817688, "max_score_of_first_answer": 0.96875, "min_score_of_first_answer": 0.0065155029296875, "avg_score": 0.1795538067817688, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.345564364424723, "std_of_first_answer": 0.345564364424723, "avg_entropy_of_first_answer": 1.5466552734375, "lex_sim": 1.0}
{"question": "Which car manufacturer produces models called Concerto, Inspire, Legend, Torneo, etc?", "answer": ["Proton Drama, Indie"], "scores": [[0.576171875, 0.9970703125, 2.8014183044433594e-06, 0.00025844573974609375, 0.14501953125, 0.00028228759765625, 0.0004856586456298828]], "normalized_score": [0.0036277659237384796], "tokens": [["\u2581Pro", "ton", "\u2581D", "rama", ",", "\u2581Ind", "ie"]], "entropy": [[1.642578125, 0.022613525390625, 0.6533203125, 4.12109375, 2.275390625, 2.826171875, 2.68359375]], "avg_entropy": [2.032108851841518], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Honda Technologies", "score_of_biggest_cluster": 0.0036277659237384796, "score_of_first_answer": 0.0036277659237384796, "max_score_of_first_answer": 0.9970703125, "min_score_of_first_answer": 2.8014183044433594e-06, "avg_score": 0.0036277659237384796, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.363489851926931, "std_of_first_answer": 0.363489851926931, "avg_entropy_of_first_answer": 2.032108851841518, "lex_sim": 1.0}
{"question": "The religious order of Poor Ladies of San Damiano has what more common name?", "answer": ["Franciscan Sisters of the Poor"], "scores": [[0.1865234375, 0.6748046875, 0.051788330078125, 0.99951171875, 0.5498046875, 0.4521484375, 0.9912109375, 1.0]], "normalized_score": [0.4474024772644043], "tokens": [["\u2581Francis", "can", "\u2581Sister", "s", "\u2581of", "\u2581the", "\u2581Po", "or"]], "entropy": [[2.330078125, 0.64453125, 1.87890625, 0.00566864013671875, 1.0068359375, 1.6806640625, 0.0704345703125, 9.357929229736328e-06]], "avg_entropy": [0.9521410241723061], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Clarissine", "score_of_biggest_cluster": 0.4474024772644043, "score_of_first_answer": 0.4474024772644043, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.051788330078125, "avg_score": 0.4474024772644043, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34869953207557125, "std_of_first_answer": 0.34869953207557125, "avg_entropy_of_first_answer": 0.9521410241723061, "lex_sim": 1.0}
{"question": "If you travel due south from Detroit, Michigan, what is the first country you reach?", "answer": ["Canada \\ \u02c8pr\u00e4-t\u0259-\u02c8ti-bi-\u0259l, \u02c8pr\u00e4s-t\u0259- ; \u02c8pr\u0251\u02d0t\u0259(\u028a)-"], "scores": [[0.82763671875, 1.6689300537109375e-06, 0.0006513595581054688, 0.0019369125366210938, 2.980232238769531e-07, 0.037017822265625, 0.4541015625, 0.255859375, 0.8837890625, 0.152099609375, 0.281005859375, 0.0019254684448242188, 0.95556640625, 0.0002498626708984375, 0.904296875, 0.8388671875, 0.08978271484375, 0.1771240234375, 0.2783203125, 0.9306640625, 0.94873046875, 0.0021839141845703125, 0.81396484375, 0.1746826171875, 0.9794921875, 0.8330078125, 0.031646728515625, 0.54150390625, 0.8603515625, 0.75244140625, 0.5107421875, 0.98779296875, 0.6484375, 0.9599609375, 0.049346923828125, 0.09014892578125, 0.20556640625]], "normalized_score": [0.08280273526906967], "tokens": [["\u2581Canada", "\u2581\\", "\u2581", "\u02c8", "pr", "\u00e4", "-", "t", "\u0259", "-", "\u02c8", "ti", "-", "bi", "-", "\u0259", "l", ",", "\u2581", "\u02c8", "pr", "\u00e4s", "-", "t", "\u0259", "-", "\u2581;", "\u2581", "\u02c8", "pr", "\u0251", "\u02d0", "t", "\u0259", "(", "\u028a", ")-"]], "entropy": [[0.9853515625, 0.94970703125, 2.26171875, 3.3046875, 0.7412109375, 2.171875, 2.78125, 1.521484375, 0.7451171875, 2.400390625, 2.689453125, 4.03515625, 0.326416015625, 2.091796875, 0.61376953125, 0.78076171875, 2.431640625, 3.1796875, 1.814453125, 0.45361328125, 0.393798828125, 1.2197265625, 0.822265625, 2.783203125, 0.2027587890625, 0.61669921875, 2.849609375, 2.671875, 0.9130859375, 1.091796875, 2.154296875, 0.08740234375, 1.2431640625, 0.27587890625, 2.048828125, 0.87255859375, 0.70556640625]], "avg_entropy": [1.5468123152449325], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Canada", "score_of_biggest_cluster": 0.08280273526906967, "score_of_first_answer": 0.08280273526906967, "max_score_of_first_answer": 0.98779296875, "min_score_of_first_answer": 2.980232238769531e-07, "avg_score": 0.08280273526906967, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3825233635285651, "std_of_first_answer": 0.3825233635285651, "avg_entropy_of_first_answer": 1.5468123152449325, "lex_sim": 1.0}
{"question": "In the lyrics of \u2018Hello, Hello, Who\u2019s Your Lady Friend?\u2019, Margate is mentioned in the verse.", "answer": ["Margate is a seaside town in Kent, England."], "scores": [[0.1270751953125, 0.99951171875, 0.78759765625, 0.95751953125, 0.46484375, 1.0, 1.0, 0.9892578125, 0.625, 0.28759765625, 0.99169921875, 0.96630859375, 0.857421875, 0.11419677734375, 0.018890380859375, 0.7998046875, 0.2039794921875, 0.98291015625, 0.99853515625, 0.1617431640625, 0.18603515625, 0.97265625, 0.404296875, 1.0, 0.625, 0.99755859375, 0.1326904296875, 0.8173828125, 0.98046875, 1.0, 0.92431640625, 0.56982421875, 0.52685546875, 0.9677734375, 0.99951171875, 1.0, 0.8798828125, 1.0, 0.94921875, 0.740234375, 0.87158203125, 0.84765625, 1.0, 0.99365234375, 0.9990234375, 0.04833984375, 0.99951171875, 1.0, 0.8798828125, 1.0]], "normalized_score": [0.6014518737792969], "tokens": [["\u2581Marg", "ate", "\u2581is", "\u2581a", "\u2581se", "as", "ide", "\u2581town", "\u2581in", "\u2581Kent", ",", "\u2581England", ".", "\u2581The", "\u2581town", "\u2581is", "\u2581located", "\u2581on", "\u2581the", "\u2581coast", "\u2581and", "\u2581is", "\u2581a", "\u2581popular", "\u2581destination", "\u2581for", "\u2581hol", "iday", "m", "akers", ".", "\u2581It", "\u2581is", "\u2581known", "\u2581for", "\u2581its", "\u2581be", "aches", ",", "\u2581am", "usement", "\u2581arc", "ades", ",", "\u2581and", "\u2581se", "as", "ide", "\u2581attra", "ctions"]], "entropy": [[3.623046875, 0.004146575927734375, 0.81640625, 0.288330078125, 1.2763671875, 0.0003447532653808594, 0.00010281801223754883, 0.061767578125, 0.93017578125, 0.79638671875, 0.059906005859375, 0.23486328125, 0.6279296875, 1.630859375, 1.8193359375, 0.87255859375, 1.515625, 0.11163330078125, 0.012298583984375, 1.7470703125, 0.64501953125, 0.1402587890625, 0.92041015625, 0.0026302337646484375, 1.03515625, 0.0164031982421875, 0.72412109375, 0.479736328125, 0.1082763671875, 0.00011056661605834961, 0.370361328125, 1.4228515625, 0.74658203125, 0.1810302734375, 0.004428863525390625, 0.0011196136474609375, 0.45068359375, 0.0006346702575683594, 0.20361328125, 1.52734375, 0.389404296875, 0.59033203125, 3.618001937866211e-05, 0.041961669921875, 0.00937652587890625, 3.2109375, 0.006591796875, 0.0017032623291015625, 0.544921875, 3.987550735473633e-05]], "avg_entropy": [0.6041046237945557], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Brighthelmstone", "score_of_biggest_cluster": 0.6014518737792969, "score_of_first_answer": 0.6014518737792969, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.018890380859375, "avg_score": 0.6014518737792969, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3237881970753554, "std_of_first_answer": 0.3237881970753554, "avg_entropy_of_first_answer": 0.6041046237945557, "lex_sim": 1.0}
{"question": "What is the capital of Togo?", "answer": ["Lome #RPGaDay2019 Day 3: Which RPG do you enjoy using as an instant filler?"], "scores": [[0.9775390625, 0.89794921875, 5.960464477539063e-08, 0.0009479522705078125, 0.00012922286987304688, 0.0008521080017089844, 0.354248046875, 0.1693115234375, 0.98681640625, 0.0292816162109375, 0.70947265625, 0.0011663436889648438, 0.931640625, 0.10699462890625, 0.13037109375, 0.07891845703125, 0.434326171875, 1.0, 0.1954345703125, 0.99267578125, 0.59423828125, 0.185302734375, 0.297119140625, 0.1409912109375, 0.0006093978881835938, 0.040191650390625, 0.98193359375, 0.47705078125]], "normalized_score": [0.058855850249528885], "tokens": [["\u2581L", "ome", "\u2581#", "R", "PG", "a", "Day", "2", "0", "1", "9", "\u2581Day", "\u2581", "3", ":", "\u2581Which", "\u2581R", "PG", "\u2581do", "\u2581you", "\u2581enjoy", "\u2581using", "\u2581as", "\u2581an", "\u2581instant", "\u2581fill", "er", "?"]], "entropy": [[0.243896484375, 0.43505859375, 0.8271484375, 4.91796875, 5.01953125, 3.623046875, 2.236328125, 2.00390625, 0.095947265625, 0.1455078125, 0.88037109375, 1.703125, 0.369384765625, 1.6982421875, 2.068359375, 3.0390625, 3.0546875, 0.0003848075866699219, 2.55078125, 0.0498046875, 1.396484375, 1.5166015625, 2.169921875, 0.482177734375, 2.166015625, 3.13671875, 0.1007080078125, 1.818359375]], "avg_entropy": [1.7053403684071131], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lome", "score_of_biggest_cluster": 0.058855850249528885, "score_of_first_answer": 0.058855850249528885, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.058855850249528885, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38252666652626777, "std_of_first_answer": 0.38252666652626777, "avg_entropy_of_first_answer": 1.7053403684071131, "lex_sim": 1.0}
{"question": "What is the meaning of the Arabic word 'Hezbollah', the Shi'ite Muslim group?", "answer": ["Party of God"], "scores": [[0.779296875, 0.998046875, 0.94384765625]], "normalized_score": [0.9020942449569702], "tokens": [["\u2581Party", "\u2581of", "\u2581God"]], "entropy": [[0.86279296875, 0.0169830322265625, 0.22412109375]], "avg_entropy": [0.3679656982421875], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Khizbollah", "score_of_biggest_cluster": 0.9020942449569702, "score_of_first_answer": 0.9020942449569702, "max_score_of_first_answer": 0.998046875, "min_score_of_first_answer": 0.779296875, "avg_score": 0.9020942449569702, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.09301497933313733, "std_of_first_answer": 0.09301497933313733, "avg_entropy_of_first_answer": 0.3679656982421875, "lex_sim": 1.0}
{"question": "Which family left Cliveden House in Buckinghamshire shortly after the Profumo Affair of the 1960s?", "answer": ["Astor family #TheDress debate is real and it\u2019s a prime example of the power of colour perception"], "scores": [[0.56005859375, 0.90478515625, 0.312255859375, 1.7881393432617188e-07, 0.003932952880859375, 0.0106201171875, 7.510185241699219e-06, 9.489059448242188e-05, 0.04693603515625, 0.004673004150390625, 0.05133056640625, 0.2073974609375, 0.03662109375, 0.99755859375, 0.053009033203125, 0.01776123046875, 0.98779296875, 0.9462890625, 0.25537109375, 0.26123046875, 0.96875, 0.00014412403106689453, 0.63916015625, 0.99951171875]], "normalized_score": [0.030360527336597443], "tokens": [["\u2581Ast", "or", "\u2581family", "\u2581#", "The", "D", "ress", "\u2581debate", "\u2581is", "\u2581real", "\u2581and", "\u2581it", "\u2019", "s", "\u2581a", "\u2581prime", "\u2581example", "\u2581of", "\u2581the", "\u2581power", "\u2581of", "\u2581colour", "\u2581per", "ception"]], "entropy": [[1.1572265625, 0.315673828125, 1.255859375, 0.7265625, 5.1875, 2.7734375, 0.5224609375, 3.28125, 3.4609375, 4.1640625, 2.619140625, 4.81640625, 2.15234375, 0.0246734619140625, 4.5703125, 5.01953125, 0.11224365234375, 0.34228515625, 2.0703125, 3.943359375, 0.2158203125, 3.630859375, 1.4658203125, 0.004352569580078125]], "avg_entropy": [2.2430179913838706], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "ASTOR (disambiguation)", "score_of_biggest_cluster": 0.030360527336597443, "score_of_first_answer": 0.030360527336597443, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 1.7881393432617188e-07, "avg_score": 0.030360527336597443, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3969475238087956, "std_of_first_answer": 0.3969475238087956, "avg_entropy_of_first_answer": 2.2430179913838706, "lex_sim": 1.0}
{"question": "Darwin is the capital of which Australian Territory?", "answer": ["005160"], "scores": [[0.001979827880859375, 0.0743408203125, 0.1334228515625, 0.0305023193359375, 0.0799560546875, 0.0285797119140625, 0.068359375]], "normalized_score": [0.03692357987165451], "tokens": [["\u2581", "0", "0", "5", "1", "6", "0"]], "entropy": [[2.771484375, 2.330078125, 3.46484375, 3.32421875, 3.5703125, 3.240234375, 3.83203125]], "avg_entropy": [3.219029017857143], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Northern Territory", "score_of_biggest_cluster": 0.03692357987165451, "score_of_first_answer": 0.03692357987165451, "max_score_of_first_answer": 0.1334228515625, "min_score_of_first_answer": 0.001979827880859375, "avg_score": 0.03692357987165451, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.04015544410553391, "std_of_first_answer": 0.04015544410553391, "avg_entropy_of_first_answer": 3.219029017857143, "lex_sim": 1.0}
{"question": "Name Microsoft's hands-free gaming system launched in November 2010, a made-up word alluding to joining?", "answer": ["Kinect #1 Post by The Sparrow \u00bb Thu Feb 08, 2018 2:25 pm"], "scores": [[0.9423828125, 0.9931640625, 0.9970703125, 1.1920928955078125e-07, 0.01557159423828125, 3.4570693969726562e-06, 0.0120697021484375, 0.002658843994140625, 0.0241241455078125, 0.0675048828125, 0.12359619140625, 0.10015869140625, 0.1702880859375, 0.9970703125, 0.08819580078125, 0.99951171875, 0.283447265625, 0.2445068359375, 0.9990234375, 0.9892578125, 0.99951171875, 0.99951171875, 0.9755859375, 0.998046875, 0.9951171875, 0.072998046875, 0.998046875, 0.1961669921875, 0.10009765625, 0.54638671875]], "normalized_score": [0.10373587161302567], "tokens": [["\u2581K", "ine", "ct", "\u2581#", "1", "\u2581Post", "\u2581by", "\u2581The", "\u2581S", "par", "row", "\u2581\u00bb", "\u2581Th", "u", "\u2581Feb", "\u2581", "0", "8", ",", "\u2581", "2", "0", "1", "8", "\u2581", "2", ":", "2", "5", "\u2581pm"]], "entropy": [[0.3349609375, 0.054656982421875, 0.0302886962890625, 0.765625, 5.6875, 3.6796875, 3.5078125, 5.89453125, 6.3046875, 3.1796875, 2.037109375, 4.1171875, 2.365234375, 0.0322265625, 2.462890625, 0.005550384521484375, 1.08984375, 1.763671875, 0.009735107421875, 0.08782958984375, 0.005390167236328125, 0.00374603271484375, 0.11871337890625, 0.017913818359375, 0.050048828125, 1.9580078125, 0.0179901123046875, 1.8046875, 2.302734375, 0.69921875]], "avg_entropy": [1.679638926188151], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Xbox kinect", "score_of_biggest_cluster": 0.10373587161302567, "score_of_first_answer": 0.10373587161302567, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 1.1920928955078125e-07, "avg_score": 0.10373587161302567, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4434564304885491, "std_of_first_answer": 0.4434564304885491, "avg_entropy_of_first_answer": 1.679638926188151, "lex_sim": 1.0}
{"question": "What was the surname of the Alice - the inspiration of Alice in Wonderland?", "answer": ["Liddell"], "scores": [[0.8662109375, 0.98388671875, 0.99853515625]], "normalized_score": [0.9476414322853088], "tokens": [["\u2581L", "idd", "ell"]], "entropy": [[1.02734375, 0.10174560546875, 0.01007080078125]], "avg_entropy": [0.3797200520833333], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Lidell", "score_of_biggest_cluster": 0.9476414322853088, "score_of_first_answer": 0.9476414322853088, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 0.8662109375, "avg_score": 0.9476414322853088, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.05922824502792107, "std_of_first_answer": 0.05922824502792107, "avg_entropy_of_first_answer": 0.3797200520833333, "lex_sim": 1.0}
{"question": "The film '10 things I hate about you', was inspired by which of Shakespeare's plays?", "answer": ["The Taming of the Shrew"], "scores": [[0.7724609375, 0.9873046875, 0.9990234375, 0.99267578125, 0.9951171875, 0.99462890625, 1.0]], "normalized_score": [0.9594783186912537], "tokens": [["\u2581The", "\u2581T", "aming", "\u2581of", "\u2581the", "\u2581Sh", "rew"]], "entropy": [[1.0810546875, 0.1265869140625, 0.0089111328125, 0.050537109375, 0.035552978515625, 0.044769287109375, 0.00028133392333984375]], "avg_entropy": [0.19252763475690568], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Hortensio", "score_of_biggest_cluster": 0.9594783186912537, "score_of_first_answer": 0.9594783186912537, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.7724609375, "avg_score": 0.9594783186912537, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0778965314669509, "std_of_first_answer": 0.0778965314669509, "avg_entropy_of_first_answer": 0.19252763475690568, "lex_sim": 1.0}
{"question": "What object was unveiled outside John Lennon Airport, Liverpool, in 2005?", "answer": ["Statue of John Lennon #FightFoodWaste: The Importance of Composting During the Holidays"], "scores": [[0.2127685546875, 0.97509765625, 0.96240234375, 0.91064453125, 0.9931640625, 0.99951171875, 1.0, 2.384185791015625e-07, 0.00478363037109375, 0.0017747879028320312, 0.0596923828125, 0.00012201070785522461, 0.457275390625, 0.99462890625, 0.002239227294921875, 0.0201416015625, 0.00493621826171875, 0.9931640625, 0.97021484375, 0.0095367431640625, 0.9736328125, 0.98779296875, 3.516674041748047e-05, 0.37939453125, 0.39453125, 0.74267578125, 0.99951171875]], "normalized_score": [0.06511116772890091], "tokens": [["\u2581Stat", "ue", "\u2581of", "\u2581John", "\u2581L", "enn", "on", "\u2581#", "F", "ight", "F", "ood", "W", "aste", ":", "\u2581The", "\u2581Import", "ance", "\u2581of", "\u2581Com", "post", "ing", "\u2581During", "\u2581the", "\u2581Hol", "id", "ays"]], "entropy": [[2.041015625, 0.1395263671875, 0.2073974609375, 0.485595703125, 0.054901123046875, 0.004756927490234375, 0.0025196075439453125, 0.609375, 5.078125, 5.3515625, 2.126953125, 3.529296875, 2.451171875, 0.043212890625, 1.5537109375, 5.56640625, 6.57421875, 0.04339599609375, 0.157958984375, 2.546875, 0.1409912109375, 0.0989990234375, 2.8046875, 2.61328125, 3.154296875, 0.59912109375, 0.006458282470703125]], "avg_entropy": [1.777252267908167], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Yellow Submarine", "score_of_biggest_cluster": 0.06511116772890091, "score_of_first_answer": 0.06511116772890091, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.384185791015625e-07, "avg_score": 0.06511116772890091, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.44431340825481536, "std_of_first_answer": 0.44431340825481536, "avg_entropy_of_first_answer": 1.777252267908167, "lex_sim": 1.0}
{"question": "\"What is \"\"Too much money chasing too few goods\"\"?\"", "answer": ["Inflation"], "scores": [[0.250244140625, 0.9990234375, 0.9990234375]], "normalized_score": [0.6297551989555359], "tokens": [["\u2581In", "fl", "ation"]], "entropy": [[1.443359375, 0.0127105712890625, 0.01177215576171875]], "avg_entropy": [0.48928070068359375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Inflation rate", "score_of_biggest_cluster": 0.6297551989555359, "score_of_first_answer": 0.6297551989555359, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 0.250244140625, "avg_score": 0.6297551989555359, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.352977945621605, "std_of_first_answer": 0.352977945621605, "avg_entropy_of_first_answer": 0.48928070068359375, "lex_sim": 1.0}
{"question": "Who recorded the original version of the song When A Man Loves A Woman ?", "answer": ["Percy Sledge #4 in Best School Districts in Arizona"], "scores": [[0.97314453125, 0.99951171875, 0.97900390625, 0.9990234375, 2.0265579223632812e-06, 0.006832122802734375, 0.0193023681640625, 4.5299530029296875e-06, 1.239776611328125e-05, 0.0012331008911132812, 0.92236328125, 0.388671875, 0.0057830810546875]], "normalized_score": [0.01104730274528265], "tokens": [["\u2581Per", "cy", "\u2581S", "ledge", "\u2581#", "4", "\u2581in", "\u2581Best", "\u2581School", "\u2581District", "s", "\u2581in", "\u2581Arizona"]], "entropy": [[0.272216796875, 0.00589752197265625, 0.116455078125, 0.01033782958984375, 0.6318359375, 3.91015625, 3.12890625, 1.380859375, 3.109375, 4.90625, 0.5244140625, 2.76953125, 4.15625]], "avg_entropy": [1.9171142578125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Percy sledge", "score_of_biggest_cluster": 0.01104730274528265, "score_of_first_answer": 0.01104730274528265, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 2.0265579223632812e-06, "avg_score": 0.01104730274528265, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4597826579151708, "std_of_first_answer": 0.4597826579151708, "avg_entropy_of_first_answer": 1.9171142578125, "lex_sim": 1.0}
{"question": "In betting, what odds are known as a \u2018Carpet\u2019?", "answer": ["100/1 ## Introduction"], "scores": [[0.5966796875, 0.5166015625, 0.87255859375, 0.91943359375, 0.1114501953125, 0.95947265625, 0.0, 5.543231964111328e-06]], "normalized_score": [0.0], "tokens": [["\u2581", "1", "0", "0", "/", "1", "\u2581##", "\u2581Introduction"]], "entropy": [[2.23046875, 1.6181640625, 0.7158203125, 0.478271484375, 2.09765625, 0.1846923828125, 1.1298828125, 5.69921875]], "avg_entropy": [1.7692718505859375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "3 to 1", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.95947265625, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.38509037161103127, "std_of_first_answer": 0.38509037161103127, "avg_entropy_of_first_answer": 1.7692718505859375, "lex_sim": 1.0}
{"question": "Kagoshima international airport is in which country?", "answer": ["Japan Preface: The Problem of Evil is a classic challenge to the existence of God."], "scores": [[0.90087890625, 0.0, 0.0004496574401855469, 0.09063720703125, 0.025970458984375, 1.7285346984863281e-06, 0.63623046875, 0.08465576171875, 0.9990234375, 0.033111572265625, 0.27099609375, 0.50927734375, 0.0255889892578125, 0.9404296875, 0.8740234375, 0.71875, 0.9697265625, 0.7763671875, 0.66650390625, 0.47314453125, 0.224853515625, 0.473876953125, 0.81396484375, 0.9873046875, 0.8935546875, 0.9267578125, 0.99609375, 0.9873046875, 0.69873046875, 0.97705078125, 0.98046875, 0.85693359375, 0.88818359375, 0.9970703125, 0.98779296875, 0.98681640625, 0.9677734375, 0.99267578125, 0.85009765625, 0.9423828125, 0.99169921875, 0.9931640625, 1.0, 0.6484375, 0.99755859375, 0.9931640625, 0.014801025390625, 1.0]], "normalized_score": [0.0], "tokens": [["\u2581Japan", "\u2581Pref", "ace", ":", "\u2581The", "\u2581Problem", "\u2581of", "\u2581Ev", "il", "\u2581is", "\u2581a", "\u2581classic", "\u2581challenge", "\u2581to", "\u2581the", "\u2581existence", "\u2581of", "\u2581God", ".", "\u2581It", "\u2581is", "\u2581the", "\u2581argument", "\u2581that", "\u2581the", "\u2581existence", "\u2581of", "\u2581evil", "\u2581in", "\u2581the", "\u2581world", "\u2581is", "\u2581in", "compatible", "\u2581with", "\u2581the", "\u2581existence", "\u2581of", "\u2581an", "\u2581all", "-", "power", "ful", ",", "\u2581all", "-", "lo", "ving"]], "entropy": [[0.58984375, 0.671875, 0.07080078125, 3.0390625, 3.287109375, 4.51953125, 2.08203125, 4.8046875, 0.01093292236328125, 3.216796875, 1.6630859375, 2.384765625, 2.529296875, 0.436279296875, 0.7509765625, 1.5947265625, 0.1844482421875, 0.87109375, 1.251953125, 1.8447265625, 3.123046875, 1.462890625, 0.73291015625, 0.08935546875, 0.58935546875, 0.31884765625, 0.0272369384765625, 0.09033203125, 0.955078125, 0.12005615234375, 0.11187744140625, 0.76416015625, 0.460205078125, 0.022247314453125, 0.0706787109375, 0.0870361328125, 0.2127685546875, 0.044281005859375, 0.489990234375, 0.261474609375, 0.053741455078125, 0.052398681640625, 0.00074005126953125, 0.69970703125, 0.02099609375, 0.041412353515625, 0.345458984375, 0.0006775856018066406]], "avg_entropy": [0.9802705148855845], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "\u65e5\u672c\u570b", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.3673598536215685, "std_of_first_answer": 0.3673598536215685, "avg_entropy_of_first_answer": 0.9802705148855845, "lex_sim": 1.0}
{"question": "On September 28th, NASA announced that what had been detected on Mars?", "answer": ["Water ice"], "scores": [[0.4169921875, 0.29931640625]], "normalized_score": [0.35328829288482666], "tokens": [["\u2581Water", "\u2581ice"]], "entropy": [[2.693359375, 1.931640625]], "avg_entropy": [2.3125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Flowing WATER", "score_of_biggest_cluster": 0.35328829288482666, "score_of_first_answer": 0.35328829288482666, "max_score_of_first_answer": 0.4169921875, "min_score_of_first_answer": 0.29931640625, "avg_score": 0.35328829288482666, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.058837890625, "std_of_first_answer": 0.058837890625, "avg_entropy_of_first_answer": 2.3125, "lex_sim": 1.0}
{"question": "What is the name of the village druid in the comic strip \u2018Asterix\u2019?", "answer": ["Getafix"], "scores": [[0.814453125, 0.99853515625, 0.99951171875]], "normalized_score": [0.9332666993141174], "tokens": [["\u2581G", "eta", "fix"]], "entropy": [[1.2197265625, 0.013153076171875, 0.004528045654296875]], "avg_entropy": [0.41246922810872394], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Unhygienix", "score_of_biggest_cluster": 0.9332666993141174, "score_of_first_answer": 0.9332666993141174, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 0.814453125, "avg_score": 0.9332666993141174, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.08700819311623598, "std_of_first_answer": 0.08700819311623598, "avg_entropy_of_first_answer": 0.41246922810872394, "lex_sim": 1.0}
{"question": "How many Von Trapp children were there in the Sound of Music?", "answer": ["Seven #NewRelease & Review - Wanderlust by @bethrinyue"], "scores": [[0.734375, 8.344650268554688e-07, 4.827976226806641e-06, 1.4722347259521484e-05, 0.0017852783203125, 0.001155853271484375, 0.049652099609375, 0.005344390869140625, 0.031768798828125, 0.19580078125, 0.9990234375, 0.68359375, 0.61181640625, 0.01013946533203125, 0.09649658203125, 0.003009796142578125, 0.72509765625, 0.0020694732666015625]], "normalized_score": [0.009305880405008793], "tokens": [["\u2581Seven", "\u2581#", "New", "Release", "\u2581&", "\u2581Review", "\u2581-", "\u2581W", "ander", "l", "ust", "\u2581by", "\u2581@", "b", "eth", "r", "iny", "ue"]], "entropy": [[0.8984375, 1.5576171875, 5.0625, 5.37109375, 2.62890625, 4.0703125, 3.1796875, 5.03515625, 4.01171875, 1.5146484375, 0.00811004638671875, 1.62109375, 2.03125, 4.49609375, 2.0703125, 2.36328125, 1.48828125, 1.9404296875]], "avg_entropy": [2.741607242160373], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "seven", "score_of_biggest_cluster": 0.009305880405008793, "score_of_first_answer": 0.009305880405008793, "max_score_of_first_answer": 0.9990234375, "min_score_of_first_answer": 8.344650268554688e-07, "avg_score": 0.009305880405008793, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3332232029825313, "std_of_first_answer": 0.3332232029825313, "avg_entropy_of_first_answer": 2.741607242160373, "lex_sim": 1.0}
{"question": "What is the colour of the bull of an archery target?", "answer": ["Black"], "scores": [[0.052490234375]], "normalized_score": [0.0524902381002903], "tokens": [["\u2581Black"]], "entropy": [[2.833984375]], "avg_entropy": [2.833984375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "GOLD (disambiguation)", "score_of_biggest_cluster": 0.0524902381002903, "score_of_first_answer": 0.0524902381002903, "max_score_of_first_answer": 0.052490234375, "min_score_of_first_answer": 0.052490234375, "avg_score": 0.0524902381002903, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.0, "std_of_first_answer": 0.0, "avg_entropy_of_first_answer": 2.833984375, "lex_sim": 1.0}
{"question": "In which sea is the The Grenadines island chain?", "answer": ["\u83f2\u5f8b\u8cd3\u897f\u73ed\u7259\u5df4\u5398\u5cf6\u5dbc"], "scores": [[0.0007390975952148438, 2.777576446533203e-05, 0.054962158203125, 0.98828125, 0.281005859375, 0.9990234375, 0.99853515625, 0.994140625, 0.99951171875, 0.99609375, 0.09912109375, 0.58740234375, 0.8486328125, 0.99951171875, 0.998046875, 0.9765625, 0.998046875, 0.04339599609375, 0.2822265625, 0.7607421875, 0.984375, 0.7490234375, 0.046417236328125, 0.4609375, 1.0]], "normalized_score": [0.2591642141342163], "tokens": [["\u2581", "<0xE8>", "<0x8F>", "<0xB2>", "<0xE5>", "<0xBE>", "<0x8B>", "<0xE8>", "<0xB3>", "<0x93>", "\u897f", "<0xE7>", "<0x8F>", "<0xAD>", "<0xE7>", "<0x89>", "<0x99>", "\u5df4", "<0xE5>", "<0x8E>", "<0x98>", "\u5cf6", "<0xE5>", "<0xB6>", "<0xBC>"]], "entropy": [[1.3623046875, 1.75, 3.23828125, 0.0794677734375, 4.1640625, 0.00829315185546875, 0.01456451416015625, 0.06488037109375, 0.00717926025390625, 0.0335693359375, 4.06640625, 2.287109375, 0.61376953125, 0.0044403076171875, 0.0213623046875, 0.1678466796875, 0.0193023681640625, 4.19921875, 2.74609375, 1.0439453125, 0.1092529296875, 0.64453125, 2.37109375, 1.76171875, 0.0011911392211914062]], "avg_entropy": [1.2311954116821289], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Carribean Islands", "score_of_biggest_cluster": 0.2591642141342163, "score_of_first_answer": 0.2591642141342163, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 2.777576446533203e-05, "avg_score": 0.2591642141342163, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4014712356462541, "std_of_first_answer": 0.4014712356462541, "avg_entropy_of_first_answer": 1.2311954116821289, "lex_sim": 0.0}
{"question": "Which Irish politician was known as 'The Liberator'?", "answer": ["Daniel O'Connell"], "scores": [[0.97216796875, 0.99169921875, 0.9912109375, 0.99365234375, 1.0]], "normalized_score": [0.9897018074989319], "tokens": [["\u2581Daniel", "\u2581O", "'", "Con", "nell"]], "entropy": [[0.2115478515625, 0.07666015625, 0.07305908203125, 0.04608154296875, 0.0016021728515625]], "avg_entropy": [0.0817901611328125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "'monster meetings'", "score_of_biggest_cluster": 0.9897018074989319, "score_of_first_answer": 0.9897018074989319, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.97216796875, "avg_score": 0.9897018074989319, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.009331155613743166, "std_of_first_answer": 0.009331155613743166, "avg_entropy_of_first_answer": 0.0817901611328125, "lex_sim": 1.0}
{"question": "What is the state capital of the US state of New Jersey?", "answer": ["Trenton"], "scores": [[0.98193359375, 0.9970703125, 0.9931640625]], "normalized_score": [0.9907018542289734], "tokens": [["\u2581Tr", "ent", "on"]], "entropy": [[0.1778564453125, 0.0301361083984375, 0.0621337890625]], "avg_entropy": [0.0900421142578125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Trenton", "score_of_biggest_cluster": 0.9907018542289734, "score_of_first_answer": 0.9907018542289734, "max_score_of_first_answer": 0.9970703125, "min_score_of_first_answer": 0.98193359375, "avg_score": 0.9907018542289734, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.006416146922014005, "std_of_first_answer": 0.006416146922014005, "avg_entropy_of_first_answer": 0.0900421142578125, "lex_sim": 1.0}
{"question": "Who hosted the 2006 Academy Awards ceremony?", "answer": ["JOHNANPR \\ Fishing \\ Fishing regulations \\ South Dakota Fishing \\ South Dakota Fishing Regulations and Licensing Requirements"], "scores": [[0.00978851318359375, 0.0019083023071289062, 0.5224609375, 0.00045800209045410156, 0.0008788108825683594, 1.4722347259521484e-05, 1.1324882507324219e-06, 0.00026416778564453125, 0.023406982421875, 0.1103515625, 0.72998046875, 0.00013828277587890625, 0.9365234375, 0.08837890625, 0.0009126663208007812, 0.118896484375, 0.9970703125, 0.037994384765625, 0.99560546875, 0.52099609375, 0.315673828125, 0.96826171875, 0.99951171875, 0.572265625, 0.99560546875, 0.73388671875, 0.99560546875, 0.0028171539306640625, 0.01116180419921875, 0.12030029296875, 0.99853515625, 0.202880859375, 0.9970703125, 0.99755859375]], "normalized_score": [0.04403011500835419], "tokens": [["\u2581J", "OH", "N", "AN", "PR", "\u2581\\", "\u2581Fish", "ing", "\u2581\\", "\u2581Fish", "ing", "\u2581reg", "ulations", "\u2581\\", "\u2581South", "\u2581Dak", "ota", "\u2581Fish", "ing", "\u2581\\", "\u2581South", "\u2581Dak", "ota", "\u2581Fish", "ing", "\u2581Reg", "ulations", "\u2581and", "\u2581Lic", "ens", "ing", "\u2581Re", "quire", "ments"]], "entropy": [[3.111328125, 4.0703125, 0.8134765625, 3.74609375, 2.8671875, 3.77734375, 2.201171875, 3.52734375, 5.72265625, 6.15625, 2.02734375, 5.546875, 0.359130859375, 3.673828125, 3.974609375, 3.48046875, 0.021759033203125, 2.478515625, 0.042755126953125, 1.767578125, 4.11328125, 0.224609375, 0.003849029541015625, 1.646484375, 0.044189453125, 1.4697265625, 0.03765869140625, 1.9404296875, 2.021484375, 0.403564453125, 0.01312255859375, 2.458984375, 0.021026611328125, 0.0203857421875]], "avg_entropy": [2.1701418932746437], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stew beef", "score_of_biggest_cluster": 0.04403011500835419, "score_of_first_answer": 0.04403011500835419, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 1.1324882507324219e-06, "avg_score": 0.04403011500835419, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.42468646535531984, "std_of_first_answer": 0.42468646535531984, "avg_entropy_of_first_answer": 2.1701418932746437, "lex_sim": 1.0}
{"question": "Which island is famous for its carved stone statues?", "answer": ["Easter Island #200 Money is the Root of All Evil"], "scores": [[0.9833984375, 1.0, 0.97607421875, 5.960464477539063e-08, 0.0169525146484375, 0.18359375, 0.1090087890625, 2.4139881134033203e-05, 0.0009756088256835938, 0.0017528533935546875, 0.115234375, 4.2498111724853516e-05, 0.9619140625, 0.498779296875, 0.822265625, 0.99755859375]], "normalized_score": [0.021100696176290512], "tokens": [["\u2581E", "aster", "\u2581Island", "\u2581#", "2", "0", "0", "\u2581M", "oney", "\u2581is", "\u2581the", "\u2581Root", "\u2581of", "\u2581All", "\u2581Ev", "il"]], "entropy": [[0.14453125, 0.00032138824462890625, 0.1478271484375, 0.90283203125, 5.2578125, 3.275390625, 2.62890625, 3.06640625, 4.47265625, 2.998046875, 5.12109375, 5.82421875, 0.291015625, 0.9013671875, 1.205078125, 0.017913818359375]], "avg_entropy": [2.265963613986969], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Chilean Polynesia", "score_of_biggest_cluster": 0.021100696176290512, "score_of_first_answer": 0.021100696176290512, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.021100696176290512, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.43629992942129325, "std_of_first_answer": 0.43629992942129325, "avg_entropy_of_first_answer": 2.265963613986969, "lex_sim": 1.0}
{"question": "Who was the next British Prime Minister after Arthur Balfour?", "answer": ["Robert Cecil"], "scores": [[0.340576171875, 0.293212890625, 1.0]], "normalized_score": [0.4639442563056946], "tokens": [["\u2581Robert", "\u2581Cec", "il"]], "entropy": [[2.240234375, 2.033203125, 1.329183578491211e-05]], "avg_entropy": [1.424483597278595], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Sir Henry Campbell-Bannerman", "score_of_biggest_cluster": 0.4639442563056946, "score_of_first_answer": 0.4639442563056946, "max_score_of_first_answer": 1.0, "min_score_of_first_answer": 0.293212890625, "avg_score": 0.4639442563056946, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.3225990086587483, "std_of_first_answer": 0.3225990086587483, "avg_entropy_of_first_answer": 1.424483597278595, "lex_sim": 1.0}
{"question": "In Greek mythology, which virgin goddess of the hearth, home and cooking gave up her seat as one of the Twelve Olympians to tend to the sacred flame on Mount Olympus, for Dionysus?", "answer": ["Vesta"], "scores": [[0.007472991943359375, 0.9697265625]], "normalized_score": [0.08512788265943527], "tokens": [["\u2581V", "esta"]], "entropy": [[0.177734375, 0.14990234375]], "avg_entropy": [0.163818359375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "HESTIA", "score_of_biggest_cluster": 0.08512788265943527, "score_of_first_answer": 0.08512788265943527, "max_score_of_first_answer": 0.9697265625, "min_score_of_first_answer": 0.007472991943359375, "avg_score": 0.08512788265943527, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4811267852783203, "std_of_first_answer": 0.4811267852783203, "avg_entropy_of_first_answer": 0.163818359375, "lex_sim": 1.0}
{"question": "Which preparation still in use today, was known in 4th century Egypt? Then, it was made from salt, pepper, iris flowers and mint leaves, but now mostly comprises Sodium Fluoride.", "answer": ["Toothpaste"], "scores": [[0.90380859375, 0.9921875, 0.96435546875]], "normalized_score": [0.9527284502983093], "tokens": [["\u2581To", "oth", "paste"]], "entropy": [[0.6083984375, 0.07196044921875, 0.239501953125]], "avg_entropy": [0.3066202799479167], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Tooth pastes", "score_of_biggest_cluster": 0.9527284502983093, "score_of_first_answer": 0.9527284502983093, "max_score_of_first_answer": 0.9921875, "min_score_of_first_answer": 0.90380859375, "avg_score": 0.9527284502983093, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.03689531302017897, "std_of_first_answer": 0.03689531302017897, "avg_entropy_of_first_answer": 0.3066202799479167, "lex_sim": 1.0}
{"question": "Which river meets the sea at Fleetwood on Lancashire's Fylde coast?", "answer": ["Wyre ##The Book on Not Reading"], "scores": [[0.50341796875, 0.99853515625, 5.960464477539063e-08, 0.004802703857421875, 3.2782554626464844e-06, 0.0005779266357421875, 0.0002522468566894531, 0.024871826171875]], "normalized_score": [0.0010693952208384871], "tokens": [["\u2581Wy", "re", "\u2581##", "The", "\u2581Book", "\u2581on", "\u2581Not", "\u2581Reading"]], "entropy": [[1.6435546875, 0.01546478271484375, 0.85302734375, 6.31640625, 3.22265625, 1.111328125, 5.45703125, 3.689453125]], "avg_entropy": [2.7886152267456055], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Borough of Wyre", "score_of_biggest_cluster": 0.0010693952208384871, "score_of_first_answer": 0.0010693952208384871, "max_score_of_first_answer": 0.99853515625, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.0010693952208384871, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.34597498601035637, "std_of_first_answer": 0.34597498601035637, "avg_entropy_of_first_answer": 2.7886152267456055, "lex_sim": 1.0}
{"question": "What is the (traditionally wooden) housing or stand which contains a ship's compass?", "answer": ["Binnacle #TechTuesday: Top 5 Tech Innovations of 2015"], "scores": [[0.8427734375, 0.99951171875, 0.99951171875, 1.1920928955078125e-07, 0.0012960433959960938, 0.0038852691650390625, 0.068115234375, 0.0626220703125, 0.98974609375, 0.052825927734375, 0.0002942085266113281, 0.73583984375, 0.50048828125, 0.0682373046875, 0.9609375, 0.2337646484375, 0.7685546875, 0.97705078125, 0.51171875, 0.294921875, 0.98095703125, 0.97216796875, 0.254638671875, 0.03778076171875]], "normalized_score": [0.09340808540582657], "tokens": [["\u2581B", "inn", "acle", "\u2581#", "T", "ech", "T", "ues", "day", ":", "\u2581Top", "\u2581", "5", "\u2581Te", "ch", "\u2581In", "nov", "ations", "\u2581of", "\u2581", "2", "0", "1", "5"]], "entropy": [[0.81005859375, 0.00522613525390625, 0.004421234130859375, 0.73046875, 4.90234375, 4.89453125, 3.9375, 1.6044921875, 0.06341552734375, 1.75, 3.080078125, 1.6943359375, 1.1005859375, 4.71484375, 0.1842041015625, 3.5625, 0.705078125, 0.1328125, 2.005859375, 0.9306640625, 0.09942626953125, 0.1356201171875, 0.6337890625, 1.1025390625]], "avg_entropy": [1.6160330772399902], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Billade", "score_of_biggest_cluster": 0.09340808540582657, "score_of_first_answer": 0.09340808540582657, "max_score_of_first_answer": 0.99951171875, "min_score_of_first_answer": 1.1920928955078125e-07, "avg_score": 0.09340808540582657, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.40848739040389076, "std_of_first_answer": 0.40848739040389076, "avg_entropy_of_first_answer": 1.6160330772399902, "lex_sim": 1.0}
{"question": "What colour/color is Mr Bump in Hargreaves' Mr Men series of children's cartoon books?", "answer": ["Yellow/yellow #ThrowbackThirty: 5 Most Read Bella Naija Features of 2014"], "scores": [[0.0711669921875, 0.990234375, 0.0439453125, 0.2132568359375, 3.3974647521972656e-06, 5.960464477539063e-08, 0.9814453125, 0.89013671875, 7.450580596923828e-06, 0.0074920654296875, 0.06488037109375, 0.01198577880859375, 0.0037708282470703125, 0.004558563232421875, 0.01378631591796875, 1.5497207641601562e-06, 0.20166015625, 0.9755859375, 0.04364013671875, 0.9794921875, 0.50439453125, 0.358154296875, 0.9970703125, 0.998046875, 0.38720703125, 0.0305023193359375]], "normalized_score": [0.02126987837255001], "tokens": [["\u2581Y", "ellow", "/", "yellow", "\u2581#", "Throw", "back", "Th", "irty", ":", "\u2581", "5", "\u2581Most", "\u2581Read", "\u2581B", "ella", "\u2581Na", "ija", "\u2581Fe", "atures", "\u2581of", "\u2581", "2", "0", "1", "4"]], "entropy": [[2.708984375, 0.07489013671875, 0.92529296875, 0.9404296875, 0.7861328125, 0.85693359375, 0.1136474609375, 0.69384765625, 0.0031719207763671875, 2.494140625, 3.60546875, 1.763671875, 4.68359375, 4.046875, 3.44140625, 1.0146484375, 2.46875, 0.190673828125, 3.375, 0.1259765625, 1.9638671875, 1.05078125, 0.0222320556640625, 0.021728515625, 0.7734375, 1.9736328125]], "avg_entropy": [1.543046731215257], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Blue", "score_of_biggest_cluster": 0.02126987837255001, "score_of_first_answer": 0.02126987837255001, "max_score_of_first_answer": 0.998046875, "min_score_of_first_answer": 5.960464477539063e-08, "avg_score": 0.02126987837255001, "D_var": 0.0, "rouge_correct_first_answer": false, "std_of_biggest_cluster": 0.4071932517418169, "std_of_first_answer": 0.4071932517418169, "avg_entropy_of_first_answer": 1.543046731215257, "lex_sim": 1.0}
{"question": "Which US state is nicknamed both 'the Colourful State' and 'the Land of Enchantment'?", "answer": ["New Mexico"], "scores": [[0.98779296875, 0.9951171875]], "normalized_score": [0.9914482831954956], "tokens": [["\u2581New", "\u2581Mexico"]], "entropy": [[0.11651611328125, 0.047210693359375]], "avg_entropy": [0.0818634033203125], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "State of New Mexico", "score_of_biggest_cluster": 0.9914482831954956, "score_of_first_answer": 0.9914482831954956, "max_score_of_first_answer": 0.9951171875, "min_score_of_first_answer": 0.98779296875, "avg_score": 0.9914482831954956, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.003662109375, "std_of_first_answer": 0.003662109375, "avg_entropy_of_first_answer": 0.0818634033203125, "lex_sim": 1.0}
{"question": "Italy is divided into how many regions?", "answer": ["20 regions ### What\u2019s Up Next"], "scores": [[0.1693115234375, 0.962890625, 0.98828125, 0.50146484375, 0.0, 5.805492401123047e-05, 3.0934810638427734e-05, 0.99462890625, 0.00038814544677734375, 0.005748748779296875]], "normalized_score": [0.0], "tokens": [["\u2581", "2", "0", "\u2581regions", "\u2581###", "\u2581What", "\u2019", "s", "\u2581Up", "\u2581Next"]], "entropy": [[0.7060546875, 0.1746826171875, 0.0721435546875, 1.3203125, 1.056640625, 3.56640625, 2.931640625, 0.052734375, 2.611328125, 3.181640625]], "avg_entropy": [1.5673583984375], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "twenty", "score_of_biggest_cluster": 0.0, "score_of_first_answer": 0.0, "max_score_of_first_answer": 0.99462890625, "min_score_of_first_answer": 0.0, "avg_score": 0.0, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.43137469204739387, "std_of_first_answer": 0.43137469204739387, "avg_entropy_of_first_answer": 1.5673583984375, "lex_sim": 1.0}
{"question": "In 1985 who became the first man to run the 1500 metres in less than 3 mins 30 secs?", "answer": ["Steve Cram"], "scores": [[0.0218048095703125, 0.9638671875, 0.99609375]], "normalized_score": [0.27560678124427795], "tokens": [["\u2581Steve", "\u2581C", "ram"]], "entropy": [[2.416015625, 0.204345703125, 0.0338134765625]], "avg_entropy": [0.8847249348958334], "clusters": [[0]], "scores_of_each_cluster": [1.0], "semantic_entropy": -0.0, "num_clusters": 1, "proportion_of_biggest_clusters": 1.0, "rouge_correct_of_biggest_cluster": false, "gold_answer": "Stephen Cram", "score_of_biggest_cluster": 0.27560678124427795, "score_of_first_answer": 0.27560678124427795, "max_score_of_first_answer": 0.99609375, "min_score_of_first_answer": 0.0218048095703125, "avg_score": 0.27560678124427795, "D_var": 0.0, "rouge_correct_first_answer": true, "std_of_biggest_cluster": 0.4518799022705906, "std_of_first_answer": 0.4518799022705906, "avg_entropy_of_first_answer": 0.8847249348958334, "lex_sim": 1.0}
